{"id": "1705.06224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Practical Processing of Mobile Sensor Data for Continual Deep Learning Predictions", "abstract": "We present a practical approach for processing mobile sensor time series data for continual deep learning predictions. The approach comprises data cleaning, normalization, capping, time-based compression, and finally classification with a recurrent neural network. We demonstrate the effectiveness of the approach in a case study with 279 participants. On the basis of sparse sensor events, the network continually predicts whether the participants would attend to a notification within 10 minutes. Compared to a random baseline, the classifier achieves a 40% performance increase (AUC of 0.702) on a withheld test set. This approach allows to forgo resource-intensive, domain-specific, error-prone feature engineering, which may drastically increase the applicability of machine learning to mobile phone sensor data.", "histories": [["v1", "Wed, 17 May 2017 15:55:53 GMT  (448kb,D)", "http://arxiv.org/abs/1705.06224v1", "6 pages, 3 figures, 3 tables"]], "COMMENTS": "6 pages, 3 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["kleomenis katevas", "ilias leontiadis", "martin pielot", "joan serr\\`a"], "accepted": false, "id": "1705.06224"}, "pdf": {"name": "1705.06224.pdf", "metadata": {"source": "CRF", "title": "Practical Processing of Mobile Sensor Data for Continual Deep Learning Predictions", "authors": ["Kleomenis Katevas", "Ilias Leontiadis", "Martin Pielot", "Joan Serr\u00e0"], "emails": ["k.katevas@qmul.ac.uk", "name.surname@telefonica.com", "permissions@acm.org."], "sections": [{"heading": null, "text": "Tags Mobile Sensing; Recurrent Neural Networks; Push Notifications; Sensor Data Processing"}, {"heading": "1. BACKGROUND AND MOTIVATION", "text": "It is indeed the case that we will be able to go in search of a solution that is capable of finding a solution that meets the needs of the people."}, {"heading": "2. DEEP LEARNING PIPELINE", "text": "A deep neural network [2] is a set of fully interconnected layers of units (nodes) that are able to map an input vector (raw data) into an output vector (e.g. inferred classes). An essential difference to traditional machine learning is that deep networks, rather than using manually created functions as input, are able to use raw data (e.g. images, audio, text).An RNN is a special type of deep network that uses sequential data as input [2, 3].RNNs can be stateful, i.e. they have an internal memory that allows them to remember past information.The most commonly used RNN architecture is the so-called long-term short-term memory (LSTM) network [5]. It has repeatedly proven to be one of the most powerful approaches to sequence modeling."}, {"heading": "2.1 Prediction / Ground Truth", "text": "RNs typically learn from a continuous series of events and ground truth labels. However, in the case of mobile sensor data, ground truth labels can be sparse. In our case study, for example, ground truth is the comparatively rare event that one cares about notification (only 1.45% of samples contain ground truth labels), so it is necessary to continuously predict while the collected sensor data stream is aggregated and train the model to do so, even if there is no continuous ground truth labels in the learning phase."}, {"heading": "2.2 Sensor Data Collection", "text": "Mobile sensor data can be divided into continuous, where the sampling rate is set (e.g. accelerometer, light, etc.) and event-driven, where new data is reported when an event occurs (e.g. when battery level drops, a notification is received, etc.) If high precision continuous sensor technology is not required, this data can be converted into periodic by aggregating the data into user-defined time intervals (e.g. average and maximum acceleration every 10 minutes). Table 1 describes the periodic and event-driven sensors in our case study."}, {"heading": "2.3 Normalization and Capping", "text": "Our input consists of real sensor values and single-line encoded vectors. Before feeding the input into the network, we normalize it by re-scaling all elements so that they are between 0 and 1. In the sensor data, we typically find strongly distorted long tail distributions. We empirically tested various thresholds above which the values are capped, and ended up using the 95th percentile of the input data. The timestamp of each data input can be confusing for the RNN, as the value increases steadily over time (usually in epoch format, i.e. milliseconds since January 1, 1970), replacing it with the time delta, the time difference in minutes between the current sensor event and the previous sensor event. By capping the value to 60 minutes, we also avoid outliers in situations where the device is switched off for a certain time or the battery is empty."}, {"heading": "2.4 Fusing Sensors and Ground Truth", "text": "RNNs are typically designed for synchronous data (e.g. audio, text, time series). While some of the sensors are sampled at regular intervals, most of the mobile data inputs are event-driven, so they have irregular outbursts (see Table 1 for some examples). In addition to introducing asynchronicity, event-driven inputs also result in extremely sparse input vectors. We organize the data in the form of sensor events stored in a two-dimensional matrix (Fig. 1). Each line represents a sensor event (Si), while each column represents a sensor measurement (x). Ground Truth Labels are also represented as a column in the matrix (y), with w = 0 used when no ground truth labels are available. Since mobile phone sensors can be both asynchronous and event-based, not all sensors in each time step can provide Ground and Truth with 5 and Labels, so we provide the problem between 0 and 0."}, {"heading": "2.5 Structuring the Data for Training", "text": "The data is structured along several dimensions: Input example: Each sample i contains the input data of a single instance for a single user: a tupel Si = (xi, yi, wi), where xi is a sensor data value, yi contains the ground truth label, and wi contains the weight of that sample, used in the error or loss function. Note that not all samples contain a ground truth label (Fig. 1). Sequences: To train RNNs, we must provide each user with a time-controlled sequence of input samples, which are used to build an internal state that determines how past events affect future time frames. They are also used to propagate the error in training the RNN [2] backwards. The number of steps to perform this reproduction in time (sequence length) is a parameter of the model. Batches: Modern Deep Learning Techniques allow us to train a network by combining multiple batches into one."}, {"heading": "3. PERFORMANCE IMPROVEMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Time-Based Sparse Data Compression", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3.2 Sample weights", "text": "In practice, the weights represent the contribution of each sample to the loss function. However, in our case, the weights are used not only to compensate for the different labels, but also for a more important task. As described, most of the samples generated only contain sensor values; there are very few samples that contain labeled data. However, even if there are unlabeled sensor values, all samples should go through the RNN as this further updates the internal RNN states. In other words, even if we do not want to make a prediction at the time of step t, this sample could affect a future time step t + i. An additional advantage is that by entering each sequence, the network will make a prediction for each input, and in fact we want to train the network in this way. In the example of Figure 2, we see that the entire square input is considered as input to determine the frequency of the network, which we do not influence by input i-weight factors, but only learn in reverse ways, like the network."}, {"heading": "4. CASE STUDY: PREDICTING REACTIVENESS TO NOTIFICATIONS", "text": "Notifications are notifications that attempt to draw the attention of the mobile phone user to new content, such as unread emails or so-called network activities. Although notifications help to avoid missing important content [16], they can have a significant negative impact. Notifications disrupt and impair work performance even when they are not visited [18]. Constant notifications can have a negative impact on well-being [9] by causing symptoms of hyperactivity and inattention. At the same time, notifications are essential for people to keep up with expectations of responsiveness [16]. Therefore, the research community is exploring ways to reduce the negative impact of notifications. An approach followed by the community is to predict how reactive a user would react to a notification [20] in order to provide smart ways to handle them. In this section, we present a case study predicting whether a user will respond to a notification within a time window of 10 minutes (or one minute)."}, {"heading": "4.1 Data collection", "text": "Our data set includes usage logs of 279 Android phone users for an average of four weeks in the summer of 2016. Participants ranged in age from 18 to 66 years (M = 37.7, SD = 11.1), with a gender balance (52.7% female and 47.3% male), and data was collected via an app that ran in the background while passively collecting extensive sensor data on the context and use of the user's phone. Participants registered the app to wait for notifications and accessibility events, allowing them to log what notifications the participants received and how much time they spent on the corresponding app.Table 1 presents a list of all sensors used in this study. Based on the time stamp of each entry, we extracted some simple information such as the time delta (explained on September 2), the day of the week (1-7), the hour of the day (0-23), and a compatible data set that includes the date day, or whether the 1 is a current date or not."}, {"heading": "4.2 Data Analysis", "text": "For analysis, we divided the 4-week sequential data set into training (first two weeks), validation (3rd week), known test (4th week), and unknown test (4th week).The difference between the two sets of tests is that the unknown test set includes 22 new users that the model has never seen before. Following the pipeline described in Section 2, we applied uniform encoding to all categorical sensors, replaced all NaN values with zeros, applied normalization, and capping, and finally applied time-based compression to the datasets.To implement our model, we used Keras v2.0.3 [1] with Theano v0.9 [19]. As a single layer, we used a fully networked, time-distributed linear layer with 50 parametric, reflected linear units [4]. Two state LSTMs were used as hidden layers of 500 units. A final layer was retained and an activation function was applied."}, {"heading": "4.3 Results", "text": "In Table 3, we specify the range below the curve (AUC) of the classifier, using both the compressed and uncompressed datasets. AUC is calculated per user and per app category and then averaged. Overall, we achieved an AUC of 0.70 in the test set and 0.69 in the unknown test set. Similar accuracies in both sets suggest that the model is resilient to users outside of the training set, which would be a very desirable feature. By applying time-based compression, we achieved a 95% reduction in the size of the data set and a relative improvement of 3.5% in predicting presence at notification. In addition, the training time of the model significantly improved from 1.3 hours to 2.8 minutes per epoch. We note that without the normalization and the lid part described in Section 2, the model had some convergence problems. In Fig. 3, we report on the performance of the classifier in a recisting part of the ROC (inaccuracy of the binder part of the ROC is known)."}, {"heading": "5. CONCLUSIONS AND FUTURE WORK", "text": "We present a practical approach to preparing mobile sensor data in time series for deep learning applications and demonstrate its effectiveness in a case study involving 279 participants. An RNN created using our approach achieved a 40% increase in performance with respect to a probable random baseline in predicting whether a notification would be visited within 10 minutes. We note that the model generalizes to unknown users without significant performance loss. The proposed data processing approach enables continuous prediction of mobile sensor data streams. The proposed time-based compression also allows for practical implementations where the phone collects and compresses the data and then sends it to the server to perform predictions. Future work includes comparing performance with canonical approaches, improving the compression strategy and the potential application of more sophisticated deep learning techniques such as transfer learning or unattended learning with generalized networks."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "The authors thank the participants of the study and Alexandros Karatzoglou for the useful discussions."}, {"heading": "7. REFERENCES", "text": "[1] F. Chollet. Keras. https: / / github.com / fchollet / keras, on net networks, 2015. [2] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT Press, Massachusetts, USA, 2016. [3] A. Graves. Generating sequences with recurrent neural networks. ArXiv: 1308.0850, 2013. [4] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers pages on ImageNet classification. In Proc. of the IEEE Int. Conf. on Computer Vision (ICCV), S. Ren, and J. Sun. J. Schmidhuber. Long short-term memory. Neural Comput., 9 (8): 1735-1780, November 1997. [6] N. S. Keskar, D. Mudigere, J. Nocedal."}], "references": [{"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press, Massachusetts, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "ArXiv: 1308.0850", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Delving deep into rectifiers: surpassing human-level performance on ImageNet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. of the IEEE Int. Conf. on Computer Vision (ICCV), pages 1026\u20131034", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "On large-batch training for deep learning: generalization gap and sharp minima", "author": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"], "venue": "Proc. of the Int. Conf. on Learning Representations (ICLR)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "Proc. of the Int. Conf. on Learning Representations (ICLR)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems (NIPS), volume 25, pages 1097\u20131105. Curran Associates Inc.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "silence your phones\u201d: Smartphone notifications increase inattention and hyperactivity symptoms", "author": ["K. Kushlev", "J. Proulx", "E.W. Dunn"], "venue": "Proc CHI \u201916, pages 1011\u20131020. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Can deep learning revolutionize mobile sensing? In Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications", "author": ["N.D. Lane", "P. Georgiev"], "venue": "HotMobile \u201915, pages 117\u2013122. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of mobile phone sensing", "author": ["N.D. Lane", "E. Miluzzo", "H. Lu", "D. Peebles", "T. Choudhury", "A.T. Campbell"], "venue": "Comm. Mag.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge University Press, Cambridge, UK", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Phased lstm: Accelerating recurrent network training for long or event-based sequences", "author": ["D. Neil", "M. Pfeiffer", "S.-C. Liu"], "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems (NIPS), volume 29, pages 3882\u20133890. Curran Associates, Inc.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proc. of the Int. Conf. on Machine Learning (ICML), pages 1310\u20131318", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "When attention is not scarce - detecting boredom from mobile phone usage", "author": ["M. Pielot", "T. Dingler", "J.S. Pedro", "N. Oliver"], "venue": "Proc. UbiComp \u201915, UbiComp \u201915, pages 825\u2013836. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Productive", "author": ["M. Pielot", "L. Rello"], "venue": "anxious, lonely - 24 hours without push notifications. In MobileHCI \u201917", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Mobile sensing at the service of mental well-being: A large-scale longitudinal study", "author": ["S. Servia-Rod\u0155\u0131guez", "K.K. Rachuri", "C. Mascolo", "P.J. Rentfrow", "N. Lathia", "G.M. Sandstrom"], "venue": "Proc. WWW \u201917, pages 103\u2013112", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "The attentional cost of receiving a cell phone notification", "author": ["C. Stothart", "A. Mitchum", "C. Yehnert"], "venue": "Journal of experimental psychology: human perception and performance, 41(4):893", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Interruptibility prediction for ubiquitous systems: Conventions and new directions from a growing field", "author": ["L.D. Turner", "S.M. Allen", "R.M. Whitaker"], "venue": "Proc UbiComp \u201915. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepsense: A unified deep learning framework for time-series mobile sensing data processing", "author": ["S. Yao", "S. Hu", "Y. Zhao", "A. Zhang", "T. Abdelzaher"], "venue": "arXiv preprint arXiv:1611.01942", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": ", the environment, health & well-being, and emotional states of the phone user [11, 15, 17].", "startOffset": 79, "endOffset": 91}, {"referenceID": 14, "context": ", the environment, health & well-being, and emotional states of the phone user [11, 15, 17].", "startOffset": 79, "endOffset": 91}, {"referenceID": 16, "context": ", the environment, health & well-being, and emotional states of the phone user [11, 15, 17].", "startOffset": 79, "endOffset": 91}, {"referenceID": 1, "context": "Deep learning proposes to solve this problem by learning the feature sets and the classifier at the same time, in a supervised way, and for a specific domain [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 3, "context": "Deep learning is significantly outperforming state-of-theart methods in several domains, such as image classification [4, 8].", "startOffset": 118, "endOffset": 124}, {"referenceID": 7, "context": "Deep learning is significantly outperforming state-of-theart methods in several domains, such as image classification [4, 8].", "startOffset": 118, "endOffset": 124}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) [2] are more suited for variable-length sequential data, such as the one produced by mobile sensors.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Nonetheless, they are typically designed for constant rate, synchronous sequences [3].", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "[10]: \u201cIf deep learning could lead to significantly more robust and efficient mobile sensor inference, it would revolutionize the field by rapidly expanding the number of sensor apps ready for mainstream usage\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] propose a phasedtriggered RNN that uses a time gate to down-sample and discretize continuous sensor input, but is not capable of \u2018desparsifying\u2019 sparse sensor data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "DeepSense [21] is, to our knowledge, the only work that inputs time series mobile sensor data into an RNN.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "A deep neural network [2] is a series of fully connected layers of units (nodes) capable of mapping an input vector (raw data) into an output vector (e.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "An RNN is a specific type of deep network that takes sequential data as an input [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 2, "context": "An RNN is a specific type of deep network that takes sequential data as an input [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 4, "context": "The most-used RNN architecture is the so-called long short-term memory (LSTM) network [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "They are also used to backpropagate the error when training the RNN [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "The batch size has implications for the robustness of the error that is propagated in the learning phase [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "This is important since the attention to past time spans of current RNN architectures is limited, a phenomenon known as the vanishing gradients problem [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 11, "context": "We consider 4 different strategies: i) no weights, therefore we resort to a simple binary indicator of whether to use the sample or not, ii) inverse frequency weighting [12], iii) inverse log-frequency weighting [12], and iv) inverse square root frequency weighting.", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "We consider 4 different strategies: i) no weights, therefore we resort to a simple binary indicator of whether to use the sample or not, ii) inverse frequency weighting [12], iii) inverse log-frequency weighting [12], and iv) inverse square root frequency weighting.", "startOffset": 212, "endOffset": 216}, {"referenceID": 15, "context": "While notifications help to avoid missing important content [16], they can have substantial negative effects.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "They disrupt and impair work performance, even when they are are not attended [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "Constant exposure to notifications can negatively affect well-being [9], as they induce symptoms of hyperactivity and inattention.", "startOffset": 68, "endOffset": 71}, {"referenceID": 15, "context": "At the same time, notifications are essential for people to keep up with expectations towards responsiveness [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "One approach that the community follows is to predict how reactive a user would be to a notification [20] to enable intelligent ways of handling them.", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "3 [1] with Theano v0.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "As an input layer we used a fully-connected time-distributed linear layer with 50 parametric rectified linear units [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "We trained our model using standard cross-entropy loss [2] and the Adam optimizer [7] with default parameters.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "We trained our model using standard cross-entropy loss [2] and the Adam optimizer [7] with default parameters.", "startOffset": 82, "endOffset": 85}], "year": 2017, "abstractText": "We present a practical approach for processing mobile sensor time series data for continual deep learning predictions. The approach comprises data cleaning, normalization, capping, time-based compression, and finally classification with a recurrent neural network. We demonstrate the effectiveness of the approach in a case study with 279 participants. On the basis of sparse sensor events, the network continually predicts whether the participants would attend to a notification within 10 minutes. Compared to a random baseline, the classifier achieves a 40% performance increase (AUC of 0.702) on a withheld test set. This approach allows to forgo resource-intensive, domain-specific, error-prone feature engineering, which may drastically increase the applicability of machine learning to mobile phone sensor data.", "creator": "LaTeX with hyperref package"}}}