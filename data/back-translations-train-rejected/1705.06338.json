{"id": "1705.06338", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Distributed Vector Representation Of Shopping Items, The Customer And Shopping Cart To Build A Three Fold Recommendation System", "abstract": "The main idea of this paper is to represent shopping items through vectors because these vectors act as the base for building em- beddings for customers and shopping carts. Also, these vectors are input to the mathematical models that act as either a recommendation engine or help in targeting potential customers. We have used exponential family embeddings as the tool to construct two basic vectors - product embeddings and context vectors. Using the basic vectors, we build combined embeddings, trip embeddings and customer embeddings. Combined embeddings mix linguistic properties of product names with their shopping patterns. The customer embeddings establish an understand- ing of the buying pattern of customers in a group and help in building customer profile. For example a customer profile can represent customers frequently buying pet-food. Identifying such profiles can help us bring out offers and discounts. Similarly, trip embeddings are used to build trip profiles. People happen to buy similar set of products in a trip and hence their trip embeddings can be used to predict the next product they would like to buy. This is a novel technique and the first of its kind to make recommendation using product, trip and customer embeddings.", "histories": [["v1", "Wed, 17 May 2017 20:28:14 GMT  (2259kb,D)", "http://arxiv.org/abs/1705.06338v1", "Cicling 2017"]], "COMMENTS": "Cicling 2017", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["bibek behera", "manoj joshi", "abhilash kk", "mohammad ansari ismail"], "accepted": false, "id": "1705.06338"}, "pdf": {"name": "1705.06338.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bibek Behera", "Manoj Joshi", "Abhilash KK", "Mohammad Ansari Ismail"], "emails": ["bibek.behera@searshc.com", "manoj.joshi@searshc.com", "Abhilash.KK@searshc.com", "Ansari.MohammedIsmail@searshc.com"], "sections": [{"heading": null, "text": "Keywords: product embedding, shopping cart, vector representation"}, {"heading": "1 Introduction", "text": "The word on the vector model proposed by Mikolov et al. [9] took the field of AI and NLP by storm. Later, the word2vec model was extended to sentences and documents, such as sent2vec and doc2vec models [4], and found its way into a variety of applications. Recently, there has been research that brings similar concepts to other areas. Rudoplh et al. [10] has shown how to apply word2vec models in various areas such as films, food and medical products using the concept of family embedding. In the case of food, we get embedding for each product by conducting transactions that are available in huge quantities of transactions. Product embedding of retail companies we have the diverse space of product tsar Xiv: 170 5.06 338v 1 [cs.I R], where their abstract representation makes sense."}, {"heading": "2 Exponential family embeddings", "text": "Exponential Family Embedding (Ef-emb) [2] are statistical tools that generalize the technique to gather contextual information for different datasets and their different distributions, used by Rudolph et al. [10] to extend the word2vec model to different areas such as grocery stores, movies, etc. This model requires two inputs - the data point and the context. Thus, Ef-emb gives the user the flexibility to choose its context function, while the context behind the use of an Ef-emb is all other items purchased in that basket or trip. In the case of movies, the movie to watch is the data point, while the context vector, the distribution of the data points and their embedding are all evaluated by the same person."}, {"heading": "2.1 Similar and co-occuring products", "text": "According to the concept described in Rudolph et. al 2016 [10], product embedding can help in the determination of similar products by using the cosine distance in the vector space to generate the closest neighbors, as shown in Equation 1, where \u03c1 stands for product embedding. In other words, if the product embedding of a product pair has a high cosine resemblance, they can replace each other. Similarity value = cosine distance (\u2212 \u2192 \u03c1x, \u2212 \u03c1y) (1) equations result from the calculation of the inner product between all pairs of product embedding and context vectors. It was found that the inner product of the product embedding of x (\u03c1x) and the context vector of y (\u03b1y), as shown in Equation 2, is higher. If they do not occur simultaneously, their inner products tend to be negative."}, {"heading": "3 Recommendation engine", "text": "First of all, we found all the products purchased in a single transaction, which is henceforth called journey. We look at these journeys with 5 or more products, so that we have a context for each product. Each product is a data point and the products purchased in the transaction become a context word. Now, this data is fed into the Ef-emb model, which assumes a Bernoulli distribution, as we only take into account the presence or absence of each product. We use the algorithm implemented by Rudolph et. al 2016. [10] The algorithm runs over 1000 iterations and the data has about 20 million journeys and 500,000 products. Once completed, it generates two arrays - for product embedding and \u03b1 for context vectors for each product. Each vector has a length of 100. These vectors are transmitted to Annoy, which is a fast c + + library with python bindings. Annoy generates an approximately nearest neighbour model, as suggested by Arya."}, {"heading": "4 Visualisation of product embedding and context vectors", "text": "We used TSNE to represent the 100-dimensional vector. TSNE is available as a Python or R package and was developed by Maaten et. al. 2008 [7]. It converts the n-dimensional vector space into a m-dimensional vector space and minimizes the KL divergence between the two datasets. The TSNE algorithm works well for data that is difficult to classify in higher dimensions. The beauty of TSNE is that it maintains clusters in the higher dimensions with a high degree of precision even after conversion to lower dimensions. Thus, we can interpret patterns in data by actually visualizing them in 2D or 3D. In reality, we see a multiple number of interpretable clusters and find a relationship easier to interpret. For example, cat food and cat accessories are placed nearby. Likewise, dog food and cat food are visualized near vectors."}, {"heading": "5 Sent2Vec model and combined model", "text": "The motivation is to find linguistic patterns in products and then combine them with product embedding to form a vector representation that represents a fusion of product language and purchasing patterns. We create the labels of about five hundred thousand products and give them as input to the sent2vec model. The sent2vec model then generates vectors based on the context of each word contained in these product names. The characteristic of this model is that it finds clusters of products with similar names, while the characteristic of product embedding is that they are able to find product clusters that are conceptually similar. In order to bridge this gap between the two models, we build a combined model that incorporates the vector representation of product embedding and sentence embedding and concatenates them. We use TSNE to project them onto a 2D map. In the projection of projection of projector wear, however, they appear far away from each other than swimwear and combined models."}, {"heading": "6 Application of product embedding", "text": "Once we have received the product embeddings of all products, we use them to find out samples in travel and customers. By samples, we mean all the information that could help us profile the travels and customers. By profiling, we mean a summary of the products purchased by a group of travelers or a group of customers. This information is especially useful to target customers for offers and discounts. The only information we have before obtaining product embeddings is the product departments, and we will use these departments to profile a cluster of travelers and customers. Every time a new product appears in a department, we could find a potential customer and then appeal to him by offering offers and discounts in that department. Some of the department categories were listed in Table 2."}, {"heading": "7 Trip Embedding", "text": "The idea is to transform each trip into a vector by taking into account the sum of product embedding \u03c1 of all n products purchased in a single trip, as in Equation 5.trip embedding [i] = 1n n n \u2211 i = 1 \u03c1 [i] (4). In total, there were 22 million trips and we converted them to vectors. We took a subsample of 10k vectors after filtering all trips with 5 or more products, this was because we had to extract purchasing behavior in travel. If the number of products in a trip is less than a threshold (here 5), then we may not get a significant pattern. After using TSNE, we project them onto a 2D map where we could actually see clusters as shown in Figure 2."}, {"heading": "7.1 Trip analysis", "text": "After we got the projection onto the 2D map, we took the KNN across the trip embedding space and extracted 5 clusters. It is noteworthy that despite the lack of common elements, some trip embedding was paired with each other. We call such trip fake pairs. To estimate the number of fake pairs, we find the closest neighbors with Annoy and if they do not have similar elements, we call them fake pairs. For each cluster, we performed the trip analysis as shown in Table 3."}, {"heading": "7.2 Trip Profiling", "text": "The second analysis consisted of profiling each cluster based on the frequency of department labels associated with the product purchased on the trip. Table 4 illustrated the cluster and its top tags."}, {"heading": "8 Customer Embedding", "text": "The idea is to represent each customer by a vector, showing the product embedding of all n products that a customer i purchased on all journeys over a six-month period, as in Equation 5.customer embedding [i] = 1n n n \u0445 j = 1 \u03c1 [i] (5) In total, there were 16 million journeys and we converted them into vectors. We took a subsample of 10k vectors after filtering all journeys with 5 or more products, because we had to extract customer clusters. On smaller journeys, the appearance of a pattern is not significant. After using TSNE, we projected it onto a 2D map where we could actually see clusters as shown in Figure 3."}, {"heading": "8.1 Customer analysis", "text": "After we received the projection on the 2D card, we led the KNN over the embedding area of the customer and extracted 5 clusters. The customer analysis is similar to the trip analysis as shown in Table 5."}, {"heading": "8.2 Customer Profiling", "text": "As shown in the customer profiles, we can find that cluster 1 reflects sportswear, cluster 2 reflects tops, cluster 3 reflects chemicals, cluster 4 reflects more hygiene related items and cluster 5 reflects drinks. In this way, we can find clusters and give them profiles."}, {"heading": "9 Schematic representation", "text": "Figure 4 shows the semantic data model, which contains the abstracted information of the embedding module. It mainly consists of two layers, which include data and model, the data layer contains the instances and attributes for further processing, and on top of the model layer, the data passes through the pre-processing phase. The model layer converts the characteristics into vector representations, using exponential family embedding as a tool. These vector representations are, as already discussed, embedding for products, travel and customers. The basic two vectors are product embedding and context vectors. After the filter embedding, the entire embedding takes place in N dimensions, which are converted into M dimensions (2D) before visualization. The visualization can be considered the last level of the module, which is responsible for graphically displaying all embedding using required plugins."}, {"heading": "10 Conclusion", "text": "We have proposed a recommendation system with a novel approach that uses product embedding, travel embedding and customer embedding to recommend products. We start with Ef-emb to generate product embedding and recommend products that are similar or occur together. We have also created combined embedding by combining product embedding with sentence embedding to bring linguistic patterns into purchase patterns. We then use product embedding to generate travel and customer embedding. In essence, we have tried to generalize product-level embedding patterns at the travel (shopping basket) and customer levels for greater understanding. We also visually demonstrate the presence of meaningful clusters in all embedding and suggest ways to recommend products through cluster analyses.The recommendation of a combined embedding model must be mathematically validated to determine whether they are qualitatively better than recommendations through product embedding and manual embedding we are currently evaluating."}], "references": [{"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["Sunil Arya", "David M Mount", "Nathan S Netanyahu", "Ruth Silverman", "Angela Y Wu"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Fundamentals of statistical exponential families with applications in statistical decision theory", "author": ["Lawrence D Brown"], "venue": "Lecture Notes-monograph series,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Itemrank: A random-walk based scoring algorithm for recommender engines", "author": ["Marco Gori", "Augusto Pucci", "V Roma", "I Siena"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Amazon. com recommendations: Item-to-item collaborative filtering", "author": ["Greg Linden", "Brent Smith", "Jeremy York"], "venue": "IEEE Internet computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Least squares quantization in pcm", "author": ["Stuart Lloyd"], "venue": "IEEE transactions on information theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Generalized linear models", "author": ["Peter McCullagh"], "venue": "European Journal of Operational Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1984}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Exponential family embeddings", "author": ["Maja Rudolph", "Francisco Ruiz", "Stephan Mandt", "David Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Recommender systems in ecommerce", "author": ["J Ben Schafer", "Joseph Konstan", "John Riedl"], "venue": "In Proceedings of the 1st ACM conference on Electronic commerce,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}], "referenceMentions": [{"referenceID": 8, "context": "[9] took the field of AI and NLP by storm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Later, word2vec model was extended to sentences and documents as sent2vec and doc2vec models [4] and found their ways into myriad of applications.", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "[10] has shown how to apply word2vec model in various domains like movies, grocery and medical science using the concept of family embeddings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "These kind of semantic structure hidden in the product embeddings speak of the quality of the representations and lead us to build the trip and customer embeddings on the foundation of product embeddings In this paper we begin with the mathematical model called exponential family embedding (Ef-emb) [10] and the characteristics of product embeddings and context vectors.", "startOffset": 300, "endOffset": 304}, {"referenceID": 5, "context": "Similarly we find customer embeddings and then find meaningful customer clusters using K-means algorithm based on Lloyd\u2019s algorithm [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "Exponential family embeddings (Ef-emb) [2] are statistical tools that generalise the technique to capture contextual information for various data sets and their varying distributions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "[10] to extend the word2vec model to different domains like grocery, movies, etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "For example in grocery data we can have product quantities which are numerical in nature hence we can use Bernoulli or Poisson distribution [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "In case of shopping data, the motivation is to get similar and co-occurring products [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "al 2016 [10], product embeddings can help in determining similar products by using cosine distance in the vector space to generate nearest neighbors as shown in equation 1 where \u03c1 stands for product embeddings.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "al 2016 [10].", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "al 1998 [1] for a million products .", "startOffset": 8, "endOffset": 11}, {"referenceID": 10, "context": "This model can be employed in a realtime recommendation system [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "2008 [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "The problem with recommendation systems is that they have online verification algorithms [3] i.", "startOffset": 89, "endOffset": 92}], "year": 2017, "abstractText": "The main idea of this paper is to represent shopping items through vectors because these vectors act as the base for building embeddings for customers and shopping carts. Also, these vectors are input to the mathematical models that act as either a recommendation engine or help in targeting potential customers. We have used exponential family embeddings as the tool to construct two basic vectors product embeddings and context vectors. Using the basic vectors, we build combined embeddings, trip embeddings and customer embeddings. Combined embeddings mix linguistic properties of product names with their shopping patterns. The customer embeddings establish an understanding of the buying pattern of customers in a group and help in building customer profile. For example a customer profile can represent customers frequently buying pet-food. Identifying such profiles can help us bring out offers and discounts. Similarly, trip embeddings are used to build trip profiles. People happen to buy similar set of products in a trip and hence their trip embeddings can be used to predict the next product they would like to buy. This is a novel technique and the first of its kind to make recommendation using product, trip and customer embeddings.", "creator": "LaTeX with hyperref package"}}}