{"id": "1705.06342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering", "abstract": "In this work, we present a methodology that enables an agent to make efficient use of its exploratory actions by autonomously identifying possible objectives in its environment and learning them in parallel. The identification of objectives is achieved using an online and unsupervised adaptive clustering algorithm. The identified objectives are learned (at least partially) in parallel using Q-learning. Using a simulated agent and environment, it is shown that the converged or partially converged value function weights resulting from off-policy learning can be used to accumulate knowledge about multiple objectives without any additional exploration. We claim that the proposed approach could be useful in scenarios where the objectives are initially unknown or in real world scenarios where exploration is typically a time and energy intensive process. The implications and possible extensions of this work are also briefly discussed.", "histories": [["v1", "Wed, 17 May 2017 20:55:15 GMT  (601kb,D)", "http://arxiv.org/abs/1705.06342v1", "Accepted in Neurocomputing: Special Issue on Multiobjective Reinforcement Learning: Theory and Applications, 24 pages, 6 figures"]], "COMMENTS": "Accepted in Neurocomputing: Special Issue on Multiobjective Reinforcement Learning: Theory and Applications, 24 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["thommen george karimpanal", "erik wilhelm"], "accepted": false, "id": "1705.06342"}, "pdf": {"name": "1705.06342.pdf", "metadata": {"source": "CRF", "title": "Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering", "authors": ["Thommen George Karimpanal", "Erik Wilhelm"], "emails": ["thommen_george@mymail.sutd.edu.sg,", "erikwilhelm@sutd.edu.sg"], "sections": [{"heading": null, "text": "In this paper, we present a methodology that enables an actor to make efficient use of his or her exploratory activities by independently identifying and learning potential goals in his or her environment in parallel, using an online and unsupervised adaptive cluster algorithm, and learning (at least in part) in parallel using Q-Learning. Using a simulated actor and the environment, it is shown that the converged or partially converged function weights resulting from non-political learning can be used to accumulate knowledge about multiple goals without additional exploration.We argue that the proposed approach could be useful in scenarios where the goals are initially unknown, or in real-world scenarios where exploration is typically a time and energy intensive process. The implications and possible extensions of this work are also briefly discussed."}, {"heading": "1. Introduction", "text": "Intelligent agents are characterized by their ability to learn from and adapt to their environments, with the aim of fulfilling specific tasks. Very often, in reinforcement of learning [1], and in machine learning in general, algorithms are structured so that they are able to fulfill a particular primary goal, which is normally specified in relation to a particular region in the trait space associated to Neurocomputing: Special Issue on MORL: Theory and applications @ mymail.sutd.edu.May 2017ar Xiv: 170 5.06 342v 1 [cs.A] 1 7M aywith a high reward. Generally, environments will likely contain multiple traits, and different regions in the trait space that could be assigned to the agent to learn."}, {"heading": "2. Background", "text": "Most research in the field of enhanced learning is based on the theformalism of Markov Decision Processes (MDPs) [9]. Within this framework, an actor in State s multiple Q multiple Q multiple Q multiple choices (MDPs) [9]. Within this framework, an actor in State s multiple Q multiple selection criteria (Q multiple selection criteria) can take action to maximize the expected reward over time, as the actor interacts with the environment. The result of these methods is a mapping of states to actions designated as policies. If the learning agent learns the value function for the implemented policies, it is referred to as on-policy learning, and if he learns the value function for objective learning, if he learns the value function for an objective learning, and if he learns the value function for an objective policy, if he learns the value function independently of the policy."}, {"heading": "3. Description", "text": "In order to demonstrate the proposed approach to identifying and learning multiple objectives, we consider an agent in a 30x30 continuous space containing obstacles, a region illuminated by a light source, and a bumpy / rough area. We assume that characteristics corresponding to these regions can be detected by the agent using its on-board sensors: a series of range sensors, a light sensor, and an inertial motion unit (IMU) to detect changes in the roughness of the surface. Reach sensors on the robot are separated by 72 degrees radially, as shown in Figure 1, and are able to detect the presence of obstacles within a unit distance. A sample of the environment is shown in Figure 2. First, the agent has no foreknowledge of the environment and can move forward and backward, sideways and diagonally up or down to both sides. In addition, he can capture his current characteristics."}, {"heading": "3.1. Agent Features", "text": "The agent is able to detect different characteristics in the environment with the help of its sensors. Sensors are simulated in such a way that they have 5% Gaussian noise. We refer to the resulting feature vector as an environmental feature vector (~ Fe). Therefore, for learning strategies that use linear function approximation, additional characteristics are required for localizing the agent. We refer to the vector of these characteristics as an agent feature vector (~ Fa). Therefore, the complete feature vector for the agent consists of these two feature vector components (~ F = ~ Fe-Fa). All the characteristics used in this work are binary for simplicity (1 or 0). The feature vector ~ Fe consists of the following features: 1. Feature indicating either the presence or absence of obstacles, as it corresponds to one of the three relevant Uranges, or consists of 2."}, {"heading": "4. Methodology", "text": "Section 3 describes the simulated environment, the agent and the characteristics he can perceive. In this section, we describe the methodology used to identify regions of interest in the characteristic space, and how these regions, which are treated as secondary objectives, can be learned with non-political methods."}, {"heading": "4.1. Adaptive Clustering", "text": "As already described, the feature vector perceived by the agent consists of features that relate to the environment as well as features for localizing the agent. First, the agent is assigned an arbitrary primary objective that is specified with respect to the primary structure, which is a certain configuration of ~ Fe. When specifying a feature value, apart from the binary values that each feature can take, a \"don't care\" case is also recorded. When learning the primary objective, the agent learns a policy that leads him from any arbitrary state in the environment to a state in which ~ Fe corresponds to the feature vector described by the environment. As the agent moves in search of the feature vector, he is continuously presented with new ~ Fe vectors."}, {"heading": "4.2. Off-Policy Learning", "text": "The clustering algorithms described in Section 4.1 are vectors ~ Fe in different clusters in an adaptive and unattended manner. As and when each new cluster is sown, an associated set of weight vectors (to learn the corresponding Q function) is also successfully created. Thus, the middle vectors of each of these clusters are treated as a secondary objective vector and the associated set of weight vectors is simultaneously updated using the Q \u2212 \u03bb algorithm, based on measures arising from behavioral policy1. Thus, during each episode of learning the primary objective, the secondary objectives identified by the clustering algorithm are learned simultaneously with off-policy learning. At the same time, new secondary objectives are identified by the clustering algorithm 1 Adaptive clustering algorithm1: Inputs: Feature Vector ~ Fe, Threshold of variance n, Number of existing clusters K (initially based on 1), Number of existing clusters (initially based on their means): K (initially based on the number of clusters)."}, {"heading": "5. Results", "text": "In this section, we summarize the results obtained by applying the methodology described in Section 4 to the agent and the environment used in Section 3. The sample environment used for the simulations is in Figures 2 and 5. In these figures, larger markers corresponding to the agent's path are identified as points closer to the agent's starting point. Configuration of the obstacles in the environment is designed to resemble the \"puddle world\" problem [24] in the sense that it may be necessary for the agent to temporarily move away from his destination location. The agent executes a greedy policy while learning a primary goal in which he perceives the characteristics of the environment and continuously sorts them into new or existing clusters dictated by Algorithm 1. Figure 3 shows the algorithms identified by the algorithm. It is applied to learn the primary destination of the destination navigation at the destination."}, {"heading": "6. Discussion", "text": "As shown in Section 5, the weights of value functions, even if partially converged, can be a good starting point for performing subsequent Q \u2212 Learning episodes when an improvement in value function estimates is required. Although we have used the Q \u2212 \u03bb algorithm in this work, other non-political methods could also be used, perhaps in conjunction with appropriate abstraction techniques such as tile encoding [24, 26]. Applying the approach described here, it should be noted that the secondary objectives identified during clustering may or may not be relevant to the actor in the future. Assessing the relevance or relative importance of these objectives could be an area for further research. Furthermore, the construction of the reward structure could be investigated in a more informed manner. Nevertheless, we believe that our approach could be useful in several areas, with direct applications in the transfer of learning outcomes [27], where it could provide incremental improvements [28] if the weights are partially transferred between actors."}, {"heading": "7. Conclusion", "text": "The methodology developed and presented here shows how the discovery and learning of potential targets in an agent's environment is possible; potential targets are identified by a simulated agent and the environment; the performance of the cluster algorithm in terms of its input parameters is presented in a table and the results are explained; the cluster algorithm is able to identify most different regions in the environment during the early episodes of Q-Learning; simulations conducted to confirm the usefulness of this approach show that the agent is able to learn multiple targets at least partially in parallel without additional exploration, especially if the behavioural policy itself is exploratory; the future scope, possible extensions of this work and its application to areas such as transfer learning and multi-agent systems are also briefly discussed, although the efficiency of our approach depends to some extent on how the behaviour of the actors in the environment improves, and we believe that the exploration environment dramatically improves the potential of exploration."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this work, we present a methodology that enables an agent to make efficient use of its exploratory actions by autonomously identifying possible objectives in its environment and learning them in parallel. The identification of objectives is achieved using an online and unsupervised adaptive clustering algorithm. The identified objectives are learned (at least partially) in parallel using Q\u2212learning. Using a simulated agent and environment, it is shown that the converged or partially converged value function weights resulting from off-policy learning can be used to accumulate knowledge about multiple objectives without any additional exploration. We claim that the proposed approach could be useful in scenarios where the objectives are initially unknown or in real world scenarios where exploration is typically a time and energy intensive process. The implications and possible extensions of this work are also briefly discussed.", "creator": "LaTeX with hyperref package"}}}