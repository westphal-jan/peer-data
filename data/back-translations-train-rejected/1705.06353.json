{"id": "1705.06353", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Political Footprints: Political Discourse Analysis using Pre-Trained Word Vectors", "abstract": "In this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora. This paper presents a simple implementation of political footprints, some heuristics on how to use them, and their application to four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S. presidential elections. The reader will be offered a number of reasons to believe that political footprints produce meaningful results, along with some suggestions on how to improve their implementation.", "histories": [["v1", "Wed, 17 May 2017 21:29:08 GMT  (993kb,D)", "http://arxiv.org/abs/1705.06353v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christophe bruchansky"], "accepted": false, "id": "1705.06353"}, "pdf": {"name": "1705.06353.pdf", "metadata": {"source": "CRF", "title": "Political Footprints: Political Discourse Analysis using Pre-Trained Word Vectors", "authors": ["Christophe Bruchansky"], "emails": ["christophe@plural.world"], "sections": [{"heading": null, "text": "Political Footprints: Political Discourse Analysis using Pre-Trained Word VectorsChristophe Bruchansky christophe @ plural.worldIn this paper we discuss how machine learning could be used to generate a more systematic and objective political discourse analysis. Political Footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word and arises from the training of the English lexicon on large corpora text. This paper presents a simple implementation of political footprints, some heuristics as they can be used, and their application to four cases: the UN Kyoto Protocol and the Paris Agreement, as well as two US presidential elections. Readers are offered a number of reasons to believe that political footprints produce meaningful outcomes, along with some suggestions for improving their implementation.Keywords: machine learning, natural language processing, vector space model, political discourse, semantics, semantics."}, {"heading": "Context and Methodology", "text": "Vector space models (VSMs) represent words in a continuous vector space, in which semantically similar words are mapped to nearby points (TensorFlow word2vec, 2017). VSMs are produced by algorithms that analyze large corpora of text and determine how likely two words are to appear in the same passage (word \"co-occurrence\"): the more often two words appear together, the closer these algorithms will place their vectors. The resulting vector-space models are not only statistically significant, but also have a semantic value. This is due to the distribution hypothesis that words that appear in the same context have semantic meanings (D. Turney & Pantel, 2010). Vector space models allow documents to be classified by meaning, understand natural language, and, in our case, create semantic word clouds. \""}, {"heading": "Raw data", "text": "Four case studies were selected to prove our concept: the Kyoto Protocol, the Paris Agreement, and the 2008 and 2016 US presidential elections; the Kyoto Protocol and the Paris Agreement were adopted from the United Nations website (United Nations Framework Convention on Climate Change, 2017); and the US elections were televised by the American Presidency Project (Peters & T. Woolley, 2017), a \"non-profit and impartial source of presidential documents\" hosted at the University of California. They were selected, firstly, because most readers are familiar with the topics to compare the results with their own intuition; and, secondly, because they are accessible to anyone who wants to consult the data.In the case of political elections, televised debates on rallies and other public statements were selected because they are considered representative of the US political landscape: processes that define which candidates were invited to televised debates, how long they could speak, and what is objectively considered to be our purpose."}, {"heading": "Key terms identification", "text": "Our decision to use IBM Watson is convenient: it allows us to perform our analysis on any personal computer, with quick results. It comes at a cost: there is not much control or explanation of how the terms are selected and weighted. Thus, IBM Watson Natural Language Understanding generates several files per text, including one for its entities and one for its keywords. It is not clear how the two lists are created, and it is assumed that entities are more relevant than keywords because they are more structured objects (entities have types and subtypes). If there is a term in both files, our script retains only the entity version. As the reader will later discover, IBM Watson's emotion recognition is not always absolutely reliable. A better solution might be to use an emotion recognition mechanism that can adapt to the political language (Rheault, Beelen, Cochrane, & Hirst, Nistuds, 2016), which is an important word to remember."}, {"heading": "Political footprints", "text": "We have decided to set out in search of new ways to lead us into the future. (...) We feel that we are able to survive ourselves. (...) We feel that we are able to survive ourselves. (...) We have not managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have succeeded. (...) We have succeeded. \"() We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded.\""}, {"heading": "Conclusions", "text": "In this paper, we have presented a very simple implementation of political footprints: one that can be calculated on any PC and yet uses some of the semantic value contained in vector space models. Determining the \"true\" meaning of a political discourse is in itself an impossible undertaking. A better way to test its validity would be to compare the resulting word clouds with those that the public, commentators and authors of a discourse would draw themselves. We could ask an audience to identify a discourse based on its political footprint and measure its success rate. Or we could compare the political footprints of the same politician in different contexts (debates, rallies, public statements, twittering) and test how consistent they are. We have proposed various improvements, such as the use of a domain-specific footprint and an open source solution that does not involve the use of IBM techniques, but a method that researchers use to extract from the pre-IBM system."}, {"heading": "Acknowledgement", "text": "We would like to thank Niel Chah (Chah, n.d.) for his support in the process of developing the political footprint. (http: / / www.facebookresearch / fastText.) Bruchansky, C. (2017). Political footprints. (http: / / www.facebookresearch / fastText.)"}], "references": [{"title": "Centro\u00efd, center of gravity, center of mass, barycenter", "author": ["H. Abdi"], "venue": "https://www.utdallas.edu/~herve/ Abdi-Centroid2007-pretty.pdf.", "citeRegEx": "Abdi,? 2017", "shortCiteRegEx": "Abdi", "year": 2017}, {"title": "Enriching word vectors with subword information. https:// github.com/facebookresearch/fastText", "author": ["P. Bojanowski", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Political footprints: Political discourse analysis using pre-trained word vectors", "author": ["C. Bruchansky"], "venue": "https://github .com/Plural-thinktank/pfootprint. Plural think tank.", "citeRegEx": "Bruchansky,? 2017", "shortCiteRegEx": "Bruchansky", "year": 2017}, {"title": "word2vec4everything", "author": ["N. Chah"], "venue": "https://github.com/ nchah/word2vec4everything.", "citeRegEx": "Chah,? 2017", "shortCiteRegEx": "Chah", "year": 2017}, {"title": "From frequency to meaning: Vector space models of semantics (Vol", "author": ["P.D. Turney", "P. Pantel"], "venue": null, "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Text comparison using word vector representations and dimensionality reduction", "author": ["H. Heuer"], "venue": "https://arxiv.org/ abs/1607.00534.", "citeRegEx": "Heuer,? 2015", "shortCiteRegEx": "Heuer", "year": 2015}, {"title": "Automatic exploration of argument and ideology in political texts. http://ftp.cs.toronto .edu/pub/gh/Hirst+Feng-ECA-2016.pdf", "author": ["G. Hirst", "V. Wei Feng"], "venue": null, "citeRegEx": "Hirst and Feng,? \\Q2015\\E", "shortCiteRegEx": "Hirst and Feng", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. https:// code.google.com/archive/p/word2vec", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation. https:// nlp.stanford.edu/pubs/glove.pdf", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The american presidency project. http://www.presidency.ucsb.edu", "author": ["G. Peters", "J.T. Woolley"], "venue": null, "citeRegEx": "Peters and Woolley,? \\Q2017\\E", "shortCiteRegEx": "Peters and Woolley", "year": 2017}, {"title": "K means", "author": ["C. Piech"], "venue": "http://stanford.edu/~cpiech/ cs221/handouts/kmeans.html.", "citeRegEx": "Piech,? 2013", "shortCiteRegEx": "Piech", "year": 2013}, {"title": "Measuring emotion in parliamentary debates with automated textual analysis", "author": ["L. Rheault", "K. Beelen", "C. Cochrane", "G. Hirst"], "venue": null, "citeRegEx": "Rheault et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rheault et al\\.", "year": 2016}, {"title": "Existentialism is a humanism", "author": ["Sartre", "J.-P"], "venue": null, "citeRegEx": "Sartre and J..P.,? \\Q1946\\E", "shortCiteRegEx": "Sartre and J..P.", "year": 1946}], "referenceMentions": [{"referenceID": 5, "context": "This enables users to explore a text source like a geographical map\u201d (Heuer, 2015).", "startOffset": 69, "endOffset": 82}, {"referenceID": 3, "context": "A large proportion focus on social media, allowing, for instance, the categorization of election-related tweets; others focus on the political discourse itself, such as argument-based analysis (Hirst & Wei Feng, 2015) and semantic word clouds (Chah, 2017).", "startOffset": 243, "endOffset": 255}, {"referenceID": 8, "context": "\u2022 GloVe from Stanford University (Pennington et al., 2014): package featuring word vectors trained using Wikipedia (2014) and Gigaword 5 (2011);", "startOffset": 33, "endOffset": 58}, {"referenceID": 8, "context": "\u2022 GloVe from Stanford University (Pennington et al., 2014): package featuring word vectors trained using Wikipedia (2014) and Gigaword 5 (2011);", "startOffset": 34, "endOffset": 122}, {"referenceID": 8, "context": "\u2022 GloVe from Stanford University (Pennington et al., 2014): package featuring word vectors trained using Wikipedia (2014) and Gigaword 5 (2011);", "startOffset": 34, "endOffset": 144}, {"referenceID": 2, "context": "All scripts and data are available on Github (Bruchansky, 2017).", "startOffset": 45, "endOffset": 63}, {"referenceID": 10, "context": "This heuristic was performed manually but could have been automated using unsupervised machine learning techniques such as k-means (Piech, 2013).", "startOffset": 131, "endOffset": 144}, {"referenceID": 0, "context": "We calculated their center of gravity, or centroids (Abdi, 2017), and compared their distance with one another.", "startOffset": 52, "endOffset": 64}], "year": 2017, "abstractText": "In this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora. This paper presents a simple implementation of political footprints, some heuristics on how to use them, and their application to four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S. presidential elections. The reader will be offered a number of reasons to believe that political footprints produce meaningful results, along with some suggestions on how to improve their implementation.", "creator": "TeX"}}}