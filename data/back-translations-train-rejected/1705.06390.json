{"id": "1705.06390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "Scalable Exact Parent Sets Identification in Bayesian Networks Learning with Apache Spark", "abstract": "In Machine Learning, the parent set identification problem is to find a set of random variables that best explain selected variable given the data and some predefined scoring function. This problem is a critical component to structure learning of Bayesian networks and Markov blankets discovery, and thus has many practical applications ranging from fraud detection to clinical decision support. In this paper, we introduce a new distributed memory approach to the exact parent sets assignment problem. To achieve scalability, we derive theoretical bounds to constraint the search space when MDL scoring function is used, and we reorganize the underlying dynamic programming such that the computational density is increased and fine-grain synchronization is eliminated. We then design efficient realization of our approach in the Apache Spark platform. Through experimental results, we demonstrate that the method maintains strong scalability on a 500-core standalone Spark cluster, and it can be used to efficiently process data sets with 70 variables, far beyond the reach of the currently available solutions.", "histories": [["v1", "Thu, 18 May 2017 01:50:04 GMT  (119kb,D)", "http://arxiv.org/abs/1705.06390v1", null], ["v2", "Tue, 24 Oct 2017 20:24:01 GMT  (116kb,D)", "http://arxiv.org/abs/1705.06390v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["subhadeep karan", "jaroslaw zola"], "accepted": false, "id": "1705.06390"}, "pdf": {"name": "1705.06390.pdf", "metadata": {"source": "CRF", "title": "Scalable Exact Parent Sets Identification in Bayesian Networks Learning with Apache Spark", "authors": ["Subhadeep Karan", "Jaroslaw Zola"], "emails": ["skaran@buffalo.edu", "jzola@buffalo.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to move around without being able to move."}, {"heading": "2. Preliminaries", "text": "Consider a set of n random variables X = {X1,., Xn}, and assume that we have a complete input data table D = {D1,., Dn} where Xi Xi Xi is a vector of Xi Xi's parent observations of Xi. Let s (Xi, Pa (Xi) be a maximum input data table that quantifies how well Xi is explained by a set of variables Pa (Xi). We will call Pa (Xi) a parent group, or simply parent characteristics of Xi. We assume that s could be one of the various available functions, such as the information theory MDL [14] and the BD family that implements Bayesian scoring criteria [16], [17] In this work, we will largely focus on the MDL scoring function, but our results can be generalized to other scoring criteria."}, {"heading": "3. Proposed Approach", "text": "Given a set of variables X, database of observations D, and a scoring function s, our goal is to enumerate all maximum parent sets for all XIX. If we look at a single variable Xi, then we can directly consider recursion in Eq. (1) and starting from the empty set, we can consider parents with increasing size. However, this process can be considered as the maximum exceeding of the dynamic programming grid with n levels formed by the suborder \"set inclusion\" to the power set of X \u2212 {Xi} (see Figure 1a). At the level l = 0 of the grid, we have an empty set. Two nodes in the grid, U \u2032 and U, are connected only when U \u2032 and | U \u2032 | + 1. Here, we use U to denote both a subset of X \u2212 {Xi} and the corresponding node in the grid."}, {"heading": "3.1. Constraining the Search Space", "text": "To achieve a scalable strategy, we assume that we restrict the search space, which is necessary because the exponential cost of looking at all the optimal parent sets is prohibitive for realistic problems, regardless of how efficient our parallel exploration algorithms are. For each Xi variable, it is reasonable to expect that its optimal parent set will not include all the other variables. In other words, there is a limit to the depth to which we should explore Xi Xi Xi Xi's dynamic programming grid. To ensure accuracy, we must ensure that the limit of the depth of exploration is not less than the unknown size of the optimal parent network. Here, we provide such a limit for the information theory MDL scoring function. We note that similar limits can be derived for other functions, and indeed significant work has been done in this direction, for example, in [20].The MDL score is defined as: Xi (U, U), Xi (Xi \u00b7 H = Xi), Xi (Xi), Xi (H = Xi)."}, {"heading": "3.2. Parallel Exploration", "text": "(.). (.). (.). (.). (.). (.). In fact, it is so that most of them are able to keep to the rules that they have imposed on themselves. (.). (.). In fact, it is so that they are able to outdo themselves. (.). (.). \"It is so that they are able to outdo themselves. (.).\" It is so that they are able to outdo themselves. (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (. \"(.).\" (.). (.). (.). \"(.).). (.). (.). (. (.). (.). (.). (.). (. (.). (.).). (. (.). (.). (.). (.). (.). (.).). (. (.). (.).). (.). (.).). (.).). (. (.). (.).).). (. (.). (.).).). (.). (.).). (.).). (.). (.). (.). (.).).). (.). (.). (.).).). (.). (.).).)."}, {"heading": "3.3. Apache Spark Implementation", "text": "In fact, it is in such a way that we are able to move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into another world, in which we move into which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we live in which we, in which we live in which we live in which we live in which we live in which we, in which we live in which we live in which we live in which we, in which we live in which we live in which we live in which we live in which we, in which we live in which we live in which we live in which we, in which we live in which we live in which we live in which we, in which we live in which we live in which we live in which we, in which we live in which we live in which we in which we live in which we live in which we live in which we, in which we in which we live in which we live in which we live in which we in which we live in which we live in which we live in which we live in which we, in which we in which we live in which we live in which we in which we live in which we in which we live in which we, in which we in which we live in which we live in which we live in which we in which we live in which we in which we, in which we in which we in which we in which we live in which we live in which we in which we live in which we, in which we in which we in which we live in which we in which we in which we, in which we in which we, in which we in which"}, {"heading": "4. Experimental Results", "text": "To understand the performance characteristics of our approach, we conducted a series of experiments on a standalone Apache Spark cluster with 25 nodes and GbE connection. Each node in the cluster is equipped with a 20-core dual socket Intel Xeon E5v3 2.3 GHz processor, 64 GB of RAM and a standard SATA hard disk. The shared file system is operated under GPFS, which is of minor importance, however, considering that the input data is very small even for the biggest problems considered and is accessed only once at the beginning of the calculations. We used several popular benchmark data sets from the UCI Machine Learning Repository [24], including Alarm (AL), Hail Finder (HF), the US Census Data (UCSD) and HEPAR II. These are generally considered too difficult to be solved accurately with sequential techniques, and are among the most demanding tests for assigning the total set of the superordinate data sets (including the number of all data sets in Xi)."}, {"heading": "4.1. Scalability Tests", "text": "In the first series of experiments, we analyzed the scalability of the platform based on the number of input variables n and the number of observations m. We executed our Spark software on the varying number of nodes and we recorded the wall time, as well as: lmax - the deepest processed layer in the dynamic programming level, lz - the last layer in which we found new maximum parent sets, and the amount of additional work we had to perform due to remote synchronization (Section 3.2.2). In any case, we ran one Spark execution level per node, and Spark driver was classified as negligible along with one of the performers since its resource usage. The results of this experiment are summarized in Tables 2-3 and Figure 2. Here, we report only on relative velocity we have achieved in relation to two nodes because, except AL-4K, we have not been able to process the test sets with sequential soft.Let's start the analysis by looking at the second time of executing the first one."}, {"heading": "4.2. HEPAR II Test", "text": "In our second experiment, we focused on the HEPAR II test data. This benchmark comes from one of the early clinical decision support systems for diagnosing multiple liver disorders, which comprised a complex Bayesian network. [13] As we mentioned earlier, the problem of parent assignment plays a crucial role in accurate Bayesian network learning, and is therefore directly reflected in our ability to build high-quality models for critical applications. It is also interesting from a practical point of view, as it contains 70 variables and all variables take only a few states, making it difficult to identify variables that should be circumcised. To process HEPAR II, we used all 25 nodes of our cluster. The experiment took 20 hours and 17 minutes to complete, with lmax = 8 and lz = 4. To the best of knowledge, this is the first time that exact results are reported for HEPAR II mode. Peak memory consumption was limited to 19.3GB in total DF474 and 774TB was again available in our most available mode."}, {"heading": "5. Related Work", "text": "Due to its importance, the problem of parent assignment has been considered a separate issue [1], [3] and in terms of Bayesian network structural learning [2], [4]. In [1], Koivisto provides several hardship results indicating that parent assignment to a single variable is most likely to have no polynomial time solution, which motivates our parallel approach, as there is a practical need to increase the magnitude of problems that can be solved precisely within realistic time limits. In [2], [4] several authors discuss the application of maximum parent assignments in exact Bayesian structural learning, but in each case maximum parent assignments are assumed and no details on how to achieve them. In this paper, we provide the actual scalable algorithm for maximum parent assignments, which can indeed be combined with any Bayesian network structural learning strategy."}, {"heading": "6. Conclusion", "text": "Accurately identifying the parent set is a difficult problem for important applications in the exact structural learning of Bayesian networks. In this work, we proposed a new scalable approach to the problem and used it to efficiently process HEPAR II data. This experiment clearly demonstrated that our method can handle even the most difficult data sets and uses limited hardware resources, which in turn opens up new opportunities for accurate learning of large Bayesian networks, as our method can be combined with existing solutions with a little effort, for example [4]. Our approach is scalable and we believe it can be generalized to other popular scoring functions such as AIC and BDeu. Since the efficiency of restricting the search space for these functions is currently unclear, the ability of our solution to adapt to high workloads offers a significant advantage."}, {"heading": "Acknowledgments", "text": "The authors want to recognize hardware and technical support from the Center for Computational Research at the University at Buffalo."}], "references": [{"title": "Parent assignment is hard for the MDL, AIC, and NML costs", "author": ["M. Koivisto"], "venue": "International Conference on Computational Learning Theory, 2006, pp. 289\u2013303.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning optimal Bayesian networks using A* search", "author": ["C. Yuan", "B. Malone", "X. Wu"], "venue": "International Joint Conference on Artificial Intelligence, 2011, pp. 2186\u20132191.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Bayesian networks with thousands of variables", "author": ["M. Scanagatta", "C. de Campos", "G. Corani", "M. Zaffalon"], "venue": "Neural Information Processing Systems, 2015, pp. 1864\u20131872.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Exact structure learning of Bayesian networks by optimal path extension", "author": ["S. Karan", "J. Zola"], "venue": "IEEE International Conference on Big Data, 2016, pp. 48\u201355.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Algorithms for large scale Markov blanket discovery.", "author": ["I. Tsamardinos", "C. Aliferis", "A. Statnikov", "E. Statnikov"], "venue": "in FLAIRS Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Clinical Decision Support Systems: A review on knowledge representation and inference under uncertainties", "author": ["G. Kong", "D.-L. Xu", "J.-B. Yang"], "venue": "International Journal of Computational Intelligence Systems, vol. 1, no. 2, pp. 159\u2013167, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Comparing risks of alternative medical diagnosis using Bayesian arguments", "author": ["N. Fenton", "M. Neil"], "venue": "Journal of Biomedical Informatics, vol. 43, no. 4, pp. 485\u2013495, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Using Bayesian belief networks for credit card fraud detection", "author": ["L. Mukhanov"], "venue": "International Conference on Artificial Intelligence and Applications, 2008, pp. 221\u2013225.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Using Bayesian networks to analyze expression data", "author": ["N. Friedman", "M. Linial", "I. Nachman", "D. Pe\u2019er"], "venue": "Journal of Computational Biology, vol. 7, no. 3-4, pp. 601\u2013620, 2000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "An improved lower bound for Bayesian Network structure learning", "author": ["X. Fan", "Y. Changhe"], "venue": "Association for the Advancement of Artificial Intelligence, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Apache Spark: a unified engine for big data processing", "author": ["M. Zaharia", "M. Franklin", "A. Ghodsi", "J. Gonzalez", "S. Shenker", "I. Stoica", "R. Xin", "P. Wendell", "T. Das", "M. Armbrust", "A. Dave", "X. Meng", "J. Rosen", "S. Venkataraman"], "venue": "Communication of the ACM, vol. 59, no. 11, pp. 56\u201365, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics, vol. 6, pp. 461\u2013464, 1978.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "Second International Symposium on Information Theory, 1973, pp. 267\u2013281.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1973}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G. Cooper", "E. Herskovits"], "venue": "Machine Learning, vol. 9, pp. 309\u2013347, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "A tutorial on learning with Bayesian networks", "author": ["D. Heckerman"], "venue": "1995. [Online]. Available: http://tinyurl.com/j939ua3", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "A work-efficient parallel breadthfirst search algorithm (or how to cope with the nondeterminism of reducers)", "author": ["C. Leiserson", "T. Schardl"], "venue": "ACM Symposium on Parallelism in Algorithms and Architectures, 2010, pp. 303\u2013314.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallel globally optimal structure learning of Bayesian networks", "author": ["O. Nikolova", "J. Zola", "S. Aluru"], "venue": "Journal of Parallel and Distributed Computing, vol. 73, pp. 1039\u20131048, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient structure learning of Bayesian networks using constraints", "author": ["C. de Campos", "Q. Ji"], "venue": "Journal of Machine Learning Research, pp. 663\u2013689, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A branch-and-bound algorithm for MDL learning Bayesian networks", "author": ["J. Tian"], "venue": "Conference on Uncertainty in Artificial Intelligence, 2000, pp. 580\u2013588.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Finding optimal models for small gene networks", "author": ["S. Ott", "S. Imoto", "S. Miyano"], "venue": "Pacific Symposium on Biocomputing, 2004, pp. 557\u2013567.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Exact Bayesian structure discovery in Bayesian networks", "author": ["M. Koivisto", "K. Sood"], "venue": "Journal of Machine Learning Research, vol. 5, pp. 549\u2013573, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Finding optimal Bayesian networks by dynamic programming", "author": ["A. Singh", "A. Moore"], "venue": "Carnegie Mellon University, Tech. Rep., 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Parallel algorithm for learning optimal Bayesian network structure", "author": ["Y. Tamada", "S. Imoto", "S. Miyano"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2437\u20132459, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "In Machine Learning, the parent set assignment problem is to find a set of random variables that best explain a selected variable given input data and some predefined scoring criterion [1].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "It is a precursor to Bayesian networks structure learning, where it is solved for each variable to produce a list of potential predecessors of that variable in a final network thus drastically limiting the number of structures that have to be considered [2], [3], [4].", "startOffset": 254, "endOffset": 257}, {"referenceID": 2, "context": "It is a precursor to Bayesian networks structure learning, where it is solved for each variable to produce a list of potential predecessors of that variable in a final network thus drastically limiting the number of structures that have to be considered [2], [3], [4].", "startOffset": 259, "endOffset": 262}, {"referenceID": 3, "context": "It is a precursor to Bayesian networks structure learning, where it is solved for each variable to produce a list of potential predecessors of that variable in a final network thus drastically limiting the number of structures that have to be considered [2], [3], [4].", "startOffset": 264, "endOffset": 267}, {"referenceID": 4, "context": "It is also closely related to the feature selection problem, since it directly translates into Markov blankets discovery [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "Because of these connections, the problem has many practical applications spanning clinical decision support systems, risk assessment, strategic planning, fraud detection and many others [6], [7], [8], [9].", "startOffset": 187, "endOffset": 190}, {"referenceID": 6, "context": "Because of these connections, the problem has many practical applications spanning clinical decision support systems, risk assessment, strategic planning, fraud detection and many others [6], [7], [8], [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 7, "context": "Because of these connections, the problem has many practical applications spanning clinical decision support systems, risk assessment, strategic planning, fraud detection and many others [6], [7], [8], [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "Because of these connections, the problem has many practical applications spanning clinical decision support systems, risk assessment, strategic planning, fraud detection and many others [6], [7], [8], [9].", "startOffset": 202, "endOffset": 205}, {"referenceID": 0, "context": "For instance, it is NP-complete for the Normalized Maximum Likelihood (NML) criterion [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "In fact, the largest problems solved by exact algorithms do not contain more than 40 variables [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "[3], do not provide any guarantees on the quality of the solutions they find.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "This significantly impacts their usefulness, since the inherent uncertainty of the model due to the input data and the scoring function, cannot be separated from the deficiencies of the learning algorithm [11].", "startOffset": 205, "endOffset": 209}, {"referenceID": 11, "context": "To validate our approach, we provide an efficient implementation on the Apache Spark platform [12], and demonstrate its strong scalability across different ML test sets.", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "For example, it could be one of the several available functions, such as information theoretic MDL [14] and AIC [15] or the BD family implementing Bayesian scoring criteria [16], [17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "For example, it could be one of the several available functions, such as information theoretic MDL [14] and AIC [15] or the BD family implementing Bayesian scoring criteria [16], [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "For example, it could be one of the several available functions, such as information theoretic MDL [14] and AIC [15] or the BD family implementing Bayesian scoring criteria [16], [17].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "For example, it could be one of the several available functions, such as information theoretic MDL [14] and AIC [15] or the BD family implementing Bayesian scoring criteria [16], [17].", "startOffset": 179, "endOffset": 183}, {"referenceID": 1, "context": "with the efficient algorithms such as [2], [4] this requires large and hard to predict number of queries for optimal parent sets, owing to the component d(Xi, U\u2212{Xi}) in the recursion.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "with the efficient algorithms such as [2], [4] this requires large and hard to predict number of queries for optimal parent sets, owing to the component d(Xi, U\u2212{Xi}) in the recursion.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "In contrast, the set of all maximal parent sets is usually many orders of magnitude smaller, and hence using it instead, in the way we explained before, is the desired and viable alternative [2], [4], [3].", "startOffset": 191, "endOffset": 194}, {"referenceID": 3, "context": "In contrast, the set of all maximal parent sets is usually many orders of magnitude smaller, and hence using it instead, in the way we explained before, is the desired and viable alternative [2], [4], [3].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "In contrast, the set of all maximal parent sets is usually many orders of magnitude smaller, and hence using it instead, in the way we explained before, is the desired and viable alternative [2], [4], [3].", "startOffset": 201, "endOffset": 204}, {"referenceID": 16, "context": "complexity, irrespective of which parallel BFS realization we assume [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "With DFS we can benefit from techniques like hypercube pipelining, similar to [19], but this strategy requires that we store partial results and update them each time a node is discovered before its all predecessors are processed.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "We note that similar bounds can be derived for other functions, and in fact a significant work in this direction has been done, for example in [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "We can further extend our pruning strategy by using the following observation [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Because of the memory and computational complexity, which remains challenging even when our pruning conditions are applied, we focus our parallel strategy on the distributed memory systems, with the Apache Spark platform [12] serving as an execution vehicle.", "startOffset": 221, "endOffset": 225}, {"referenceID": 3, "context": "The high level exploration components of our method are implemented in Python, and the computationally intensive parts, specifically evaluations of function s, are offloaded to the efficient, SIMD-parallel C++ kernel derived from our SABNA package [4], [22].", "startOffset": 248, "endOffset": 251}, {"referenceID": 0, "context": "Because of its importance, the parent assignment problem has been considered as a standalone question [1], [3] and in the relation to the structure learning of Bayesian networks [2], [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Because of its importance, the parent assignment problem has been considered as a standalone question [1], [3] and in the relation to the structure learning of Bayesian networks [2], [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "Because of its importance, the parent assignment problem has been considered as a standalone question [1], [3] and in the relation to the structure learning of Bayesian networks [2], [4].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "Because of its importance, the parent assignment problem has been considered as a standalone question [1], [3] and in the relation to the structure learning of Bayesian networks [2], [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 0, "context": "In [1], Koivisto provides several hardness results that suggest that the parent assignment for a single variable most likely has no polynomial-time solution.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2], [4], multiple authors discuss the application of maximal parent sets in exact Bayesian networks structure learning.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [2], [4], multiple authors discuss the application of maximal parent sets in exact Bayesian networks structure learning.", "startOffset": 8, "endOffset": 11}, {"referenceID": 20, "context": "There is a significant body of work on solving maximal parent sets enumeration while discovering Bayesian network structure [25], [26], [27], including parallel algorithms [19], [28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "There is a significant body of work on solving maximal parent sets enumeration while discovering Bayesian network structure [25], [26], [27], including parallel algorithms [19], [28].", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "There is a significant body of work on solving maximal parent sets enumeration while discovering Bayesian network structure [25], [26], [27], including parallel algorithms [19], [28].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "There is a significant body of work on solving maximal parent sets enumeration while discovering Bayesian network structure [25], [26], [27], including parallel algorithms [19], [28].", "startOffset": 172, "endOffset": 176}, {"referenceID": 23, "context": "There is a significant body of work on solving maximal parent sets enumeration while discovering Bayesian network structure [25], [26], [27], including parallel algorithms [19], [28].", "startOffset": 178, "endOffset": 182}, {"referenceID": 17, "context": "As a result, these combined approaches do not scale and are limited to the instances with 30 to 40 variables, even when using thousands of cores and provably optimal MPIbased realizations [19], [28].", "startOffset": 188, "endOffset": 192}, {"referenceID": 23, "context": "As a result, these combined approaches do not scale and are limited to the instances with 30 to 40 variables, even when using thousands of cores and provably optimal MPIbased realizations [19], [28].", "startOffset": 194, "endOffset": 198}, {"referenceID": 2, "context": "[3] proposed a greedy heuristic that depends on a fast approximation of the actual scoring function to constraint the number of explored parent sets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "In Machine Learning, the parent set identification problem is to find a set of random variables that best explain selected variable given the data and some predefined scoring function. This problem is a critical component to structure learning of Bayesian networks and Markov blankets discovery, and thus has many practical applications ranging from fraud detection to clinical decision support. In this paper, we introduce a new distributed memory approach to the exact parent sets assignment problem. To achieve scalability, we derive theoretical bounds to constraint the search space when MDL scoring function is used, and we reorganize the underlying dynamic programming such that the computational density is increased and fine-grain synchronization is eliminated. We then design efficient realization of our approach in the Apache Spark platform. Through experimental results, we demonstrate that the method maintains strong scalability on a 500-core standalone Spark cluster, and it can be used to efficiently process data sets with 70 variables, far beyond the reach of the currently available solutions.", "creator": "LaTeX with hyperref package"}}}