{"id": "1705.06460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "Evolving Ensemble Fuzzy Classifier", "abstract": "The concept of ensemble learning offers a promising avenue in learning from data streams under complex environments because it addresses the bias and variance dilemma better than its single model counterpart and features a reconfigurable structure, which is well suited to the given context. While various extensions of ensemble learning for mining non-stationary data streams can be found in the literature, most of them are crafted under a static base classifier and revisits preceding samples in the sliding window for a retraining step. This feature causes computationally prohibitive complexity and is not flexible enough to cope with rapidly changing environments. Their complexities are often demanding because it involves a large collection of offline classifiers due to the absence of structural complexities reduction mechanisms and lack of an online feature selection mechanism. A novel evolving ensemble classifier, namely Parsimonious Ensemble pENsemble, is proposed in this paper. pENsemble differs from existing architectures in the fact that it is built upon an evolving classifier from data streams, termed Parsimonious Classifier pClass. pENsemble is equipped by an ensemble pruning mechanism, which estimates a localized generalization error of a base classifier. A dynamic online feature selection scenario is integrated into the pENsemble. This method allows for dynamic selection and deselection of input features on the fly. pENsemble adopts a dynamic ensemble structure to output a final classification decision where it features a novel drift detection scenario to grow the ensemble structure. The efficacy of the pENsemble has been numerically demonstrated through rigorous numerical studies with dynamic and evolving data streams where it delivers the most encouraging performance in attaining a tradeoff between accuracy and complexity.", "histories": [["v1", "Thu, 18 May 2017 08:19:41 GMT  (1053kb)", "http://arxiv.org/abs/1705.06460v1", "this paper is currently submitted for possible publication in IEEE"]], "COMMENTS": "this paper is currently submitted for possible publication in IEEE", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mahardhika pratama", "witold pedrycz", "edwin lughofer"], "accepted": false, "id": "1705.06460"}, "pdf": {"name": "1705.06460.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "III. LEARNING POLICY OF PENSEMBLE", "text": "This year it is more than ever before."}, {"heading": "Else", "text": ") 1), 2 (min (pii)"}, {"heading": "End", "text": "iy End1,..., max () O C / / Generates the global prediction1i i Mii / / normalizes the weightIF i Prune i-th local expert / / Cuts the local expert with low weight"}, {"heading": "End", "text": "For 1,... I calculate the localized generalization error (5) to estimate the generalization power of a local expert. A local expert with low generalization capability is removed - Section 3.B.2 IF (7)"}, {"heading": "End End", "text": "Adopts the drift detection method to determine appropriate learning actions, whether a new classifier should be introduced, a learning process is conducted to update the winning classifier, or no learning process is conducted - Section 3.B.1"}, {"heading": "End", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B. Learning Algorithm of Parsimonious Ensemble", "text": "(...). (...). (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\")."}, {"heading": "Else", "text": "This year is the highest in the history of the country."}, {"heading": "A. Prediction of Coronary Heart Disease", "text": "pENsemble was tested on a real-life problem, namely predicting coronary heart disease (CTE) (courtesy of Dr. Agus Salim, La Trobe University) Our study was conducted using a real-life data set derived from a Nested Case Control (NCC) experiment within the Singapore Chinese Health Study (SCHS) with 63,257 participants. Subjects were only those who had donated their blood and never suffered from CTE or stroke verified from the self-reported diagnosis or data from the hospital discharge database [63]. The aim of this study is to determine the disease outcome of participants who had CTE by December 31, 2010. Both myocardial infarction (AMI) or coronary heart disease-related deaths were grouped as cases, while others were classified as a control group."}, {"heading": "B. Online Tool Condition Monitoring of Metal Cutting", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves."}], "references": [{"title": "Knowledge Discovery from Data Streams", "author": ["J. Gama"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Autonomous Learning Systems: From Data Streams to Knowledge in Real-time", "author": ["P. Angelov"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning in Non-Stationary Environments: Methods and Applications", "author": ["M. Sayed-Mouchaweh", "E. Lughofer"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Incremental Learning of Concept Drift Using Evolving Type-2 Recurrent Fuzzy Neural Network", "author": ["M. Pratama", "J. Lu", "E. Lughofer", "G. Zhang", "M.J. Er"], "venue": "IEEE Transactions on Fuzzy Systems, on-line and in press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "et al", "author": ["G. Ditzler"], "venue": "\u201c Learning in Nonstationary Environments: A Survey\u201d, IEEE Computational Intelligence Magazine, Vol.10(4), pp. 12-25, ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Catastrophic forgetting in connectionist networks", "author": ["R.M. French"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "An approach to online identification of Takagi- Sugeno fuzzy models,", "author": ["P.Angelov", "D. Filev"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "DENFIS: dynamic evolving neural-fuzzy inference system and its application for time series prediction", "author": ["N. Kasabov", "Q. Song"], "venue": "IEEE Transactions on Fuzzy Systems .vol10 (2).pp. 144\u2013154. ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "J", "author": ["M. Pratama"], "venue": "Lu, G.Zhang, \u201c Evolving Type-2 Fuzzy Classifier\u201d, online and in press, IEEE Transactions on Fuzzy Systems, on line and in press, ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["A. Lemos"], "venue": "Adaptive fault detection and diagnosis using an evolving fuzzy classifier, Information Sciences, vol. 220, pp. 64-85, ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Ensemble Method Based on Individual Evolving Classifiers", "author": ["J.A Iglesias", "A. Ledezma", "A. Sanchiz"], "venue": "Evolving and Adaptive Intelligent Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Analyzing the structure of ensembles based-on evolving classifiers", "author": ["J.A Iglesias", "A. Ledezma", "A. Sanchiz"], "venue": "FINO/CAEPIA,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "DELA: A Dynamic Online Ensemble Learning Algortihm", "author": ["A. Bouchachia"], "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Classifiers Ensemble for Changing Environments", "author": ["L. Kuncheva"], "venue": "Lecture Notes on Computer Sciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R. Elwell", "R. Polikar"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Ensemble of subset online sequential extreme learning machine for class imbalance and concept", "author": ["B. Mirza", "Z. Lin", "N. Liu"], "venue": "drift,\u201d Neurocomputing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Handling Drifts and Shifts in On-Line Data Streams with Evolving Fuzzy Systems", "author": ["E. Lughofer", "P. Angelov"], "venue": "Applied Soft Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Hellinger Distance based Drift Detection for Nonstationary Environments", "author": ["G. Dirzler", "R. Polikar"], "venue": "IEEE Symposium on Computational Intelligence in Computational Intelligence in Dynamic and Uncertain Environments,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Caballero-Mota, \u201cOnline and Non-Parametric Drift Detection Methods Based on Hoeffding\u2019s Bounds", "author": ["I. Frias-Blanco", "J.D. Campo-Avilla", "G. Ramos-Jimenes", "R. Morales- Bueno", "Y.A. Ortiz-Diaz"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Sensitivity Growing and Pruning Method for RBF Networks in Online Learning Environments", "author": ["P.P.K. Chan"], "venue": "in International Conference on Machine Learning and Cybernetics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Dynamic weighted majority: An ensemble method for drifting concepts", "author": ["J. Kolter", "M. Maloof"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "An on-line self-constructing neural fuzzy inference network and its applications", "author": ["C. Juang", "C. Lin"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1998}, {"title": "Recurrent Classifier Based on an Incremental Meta-cognitive-based Scaffolding Algorithm", "author": ["M.Pratama", "S.Anavatti", "J.Lu"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Extended sequential adaptive fuzzy inference system for classification problems,", "author": ["H.-J. Rong", "N. Sundarajan", "G.-B. Huang", "G.-S. Zhao"], "venue": "Evolving Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Evolving Fuzzy Systems --- Methodologies, Advanced Concepts and Applications", "author": ["E. Lughofer"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "An Incremental Type-2 Metacognitive Extreme Learning Machine", "author": ["M. Pratama", "G. Zhang", "M-J. Er", "S. Anavatti"], "venue": "IEEE Transactions on Cybernetics, online and in press,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Extensions of Vector Quantization for Incremental Clustering", "author": ["E. Lughofer"], "venue": "Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "On-line Quality Control with Flexible Evolving Fuzzy Systems, in: Learning in Non-Stationary Environments", "author": ["E. Lughofer"], "venue": "Methods and Applications,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Generalized Smart Evolving Fuzzy Systems", "author": ["E. Lughofer"], "venue": "Evolving Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "A generalized growing and pruning RBF (GGAP-RBF) neural network for function  approximation,", "author": ["G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2005}, {"title": "Sequential adaptive fuzzy inference system (SAFIS) for nonlinear system identification and time series prediction,", "author": ["H.J. Rong", "N. Sundararajan", "G.B. Huang", "P. Saratchandran"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "An ensemble method based on evolving classifiers: eStacking", "author": ["J.A. Iglesias", "A. Ledezma", "A. Sanchis"], "venue": "IEEE Symposium on Evolving and Autonomous Learning System,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Reliable All-Pairs Evolving Fuzzy Classifiers", "author": ["E. Lughofer"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Polikar,\u201cIncremental learning of concept drift from streaming imbalanced data,", "author": ["R.G. Ditzler"], "venue": "IEEE Transactions on Knowledge & Data Engineering,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Learning with drift detection,", "author": ["J. Gama", "P. Medas", "G. Castillo", "P. Rodrigues"], "venue": "Proceeding of Brazilian Symposium on Artificial Intelligence.,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Improved GART neural network model for pattern classification and rule extraction with application to power system,", "author": ["K.S. Yap"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "The Bayesian ARTMAP,", "author": ["B. Vigdor", "B. Lerner"], "venue": "IEEE Transactions Neural Networks,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2007}, {"title": "On-line identification of computationally undemanding evolving fuzzy models,", "author": ["J.-C. de Barros", "A.L. Dexter"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}, {"title": "Generalized recursive least square to the training of neural network,", "author": ["Y. Xu", "K.W. Wong", "C.S. Leung"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2006}, {"title": "Learn++: An incremental learning algorithm for supervised neural networks,", "author": ["R. Polikar", "L. Udpa", "S. Udpa", "V. Honavar"], "venue": "IEEE Transactions on System, Man and Cybernetics (C), Special Issue on Knowledge Management,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2001}, {"title": "DDD: A new ensemble approach for dealing with drifts,", "author": ["L.L. Minku", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2012}, {"title": "The impact of diversity on online ensemble learning in the presence concept of drift,", "author": ["L.L. Minku", "A.P. White", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification,", "author": ["W.N. Street", "Y. Kim"], "venue": "in International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2001}, {"title": "A metacognitive neurofuzzy inference system (McFIS) for sequential classification problems", "author": ["K. Subramanian", "S. Suresh", "N. Sundararajan"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "C-reactive protein and serum creatinine, but not haemoglobin A1c, are independent predictors of coronary heart disease risk in non-diabetic Chinese", "author": ["Agus Salim"], "venue": "European journal of preventive cardiology,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2016}, {"title": "Evolving fuzzy classifiers using different model architectures", "author": ["P. Angelov"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2008}, {"title": "Evolving fuzzy rule-based classifiers from data streams,", "author": ["P.Angelov"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2008}, {"title": "J", "author": ["R.D. Baruah", "P. Angelov"], "venue": "Andreu, \u201cSimpl_eClass: Simplified potentialfree evolving fuzzy rule-based classifiers,\u201d in Proceeding of IEEE International Conference on Systems, Man and Cybernetics, Anchorage, AK, USA, Oct. 7\u2013 9", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Evolving Intelligent Systems, eIS", "author": ["P. Angelov", "N. Kasabov"], "venue": "IEEE SMC eNewsletter,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION The data-intensive era where data are collected continuously in a fast rate under dynamic and evolving environments opens a new research direction to process data streams efficiently [1], [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 1, "context": "INTRODUCTION The data-intensive era where data are collected continuously in a fast rate under dynamic and evolving environments opens a new research direction to process data streams efficiently [1], [2].", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "Another challenging trait of data streams lies in the non-stationary characteristics [3] where the data does not follow static and predictable distributions and contains a variety of concept drifts [4], [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Another challenging trait of data streams lies in the non-stationary characteristics [3] where the data does not follow static and predictable distributions and contains a variety of concept drifts [4], [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 4, "context": "Another challenging trait of data streams lies in the non-stationary characteristics [3] where the data does not follow static and predictable distributions and contains a variety of concept drifts [4], [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 5, "context": "These facts make a retraining phase when incorporating a new sample to an old dataset impossible to be performed because it leads to the socalled catastrophic forgetting [6] of previously valid knowledge and is not scalable when dealing with massive data streams.", "startOffset": 170, "endOffset": 173}, {"referenceID": 6, "context": "Evolving Intelligent System (EIS) provides a unique solution for data stream mining because a strictly one-pass learning procedure involved here has delivered great success to cope with time-critical applications where data streams are generated at a very fast sampling rate [7].", "startOffset": 275, "endOffset": 278}, {"referenceID": 7, "context": "Furthermore, EIS adopts an open structure where its components can be automatically generated, pruned, merged and recalled on the fly [8], [9] and can be well-suited to a given problem.", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "This trait reflects the true data distributions and tracks changing data distributions [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 49, "context": "EIS has transformed to be one of the most active research area in the computational intelligence research as evidenced by the number of published works in this area [71].", "startOffset": 165, "endOffset": 169}, {"referenceID": 9, "context": "Nonetheless, EIS is typically built upon a single classifier architecture which often does not produce adequate accuracy for complex problems [11], [35].", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "Nonetheless, EIS is typically built upon a single classifier architecture which often does not produce adequate accuracy for complex problems [11], [35].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "In fact, from classical batch learning perspective, it is well-known that ensemble classifiers outperform single base classifiers in case of high noise levels and a low number of available training samples [12] because they can better resolve the bias-variance dilemma due to proper subspace and data exploration using weak classifiers [13].", "startOffset": 336, "endOffset": 340}, {"referenceID": 11, "context": "While few works about a synergy between EIS and an ensemble structure can be found in the literature [14], [15], most of them utilise a static ensemble architecture, which should be predetermined in advance.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "While few works about a synergy between EIS and an ensemble structure can be found in the literature [14], [15], most of them utilise a static ensemble architecture, which should be predetermined in advance.", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "The ensemble learning concept uses combination of individual base classifiers with a modularity principle, where it enables a dynamic evolution of the ensemble structure [12][19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "Adaptability of the ensemble classifier plays a vital role to the success of ensemble learning because it formulates mechanisms how an ensemble classifier adapts itself when changing data distributions are presented [18].", "startOffset": 216, "endOffset": 220}, {"referenceID": 16, "context": "The ensemble classifier can also be distinguished into two groups: active and passive approach: the passive approach relies on continuous updates of its components and assumes that the concept drifts occur in the ongoing fashion; the active approach is equipped by a dedicated drift detection mechanism in which it is restructured and parameters are fine-tuned when a drift is captured [19].", "startOffset": 386, "endOffset": 390}, {"referenceID": 13, "context": "Although ensemble algorithms like DELA [16] is excluded from the local concept drift bottleneck due to its three levels of adaptivity, namely structural adaptivity, combination adaptivity, model adaptivity, it suffers from the absence of a dedicated drift detection method [16].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Although ensemble algorithms like DELA [16] is excluded from the local concept drift bottleneck due to its three levels of adaptivity, namely structural adaptivity, combination adaptivity, model adaptivity, it suffers from the absence of a dedicated drift detection method [16].", "startOffset": 273, "endOffset": 277}, {"referenceID": 21, "context": "The dynamic ensemble concept is inspired by the evolving trait of DWM [34] but different criteria are applied to perform the structural learning scenarios of pENsemble.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "\u2022 Online Drift Detection Scenario: pENsemble adopts a dynamic ensemble structure where a new local expert can be added when a concept change presents in the data streams [26].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "This procedure is governed by a non-parametric drift detection method derived from the concept of Hoeffding\u2019s bounds [27].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "This method estimates generalization performance of a local expert [29] and determines local experts to be pruned [30].", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "This paper conveys four major contributions as follows: 1) a novel ensemble learning algorithm inspired by a seminal work, namely DWM [34], is proposed.", "startOffset": 134, "endOffset": 138}, {"referenceID": 7, "context": "DENFIS in [9] is another early example of EIS which combines the working principle of TSK fuzzy system and the Evolving Clustering Method (ECM).", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Angelov and Filev proposed the so-called eTS [7] which benefits from the data potential theory forming an evolving version of the mountain clustering.", "startOffset": 45, "endOffset": 48}, {"referenceID": 46, "context": "This work is modified for a classification problem [65], [66] and has formed the first evolving classifier, termed eClass.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "This work is modified for a classification problem [65], [66] and has formed the first evolving classifier, termed eClass.", "startOffset": 57, "endOffset": 61}, {"referenceID": 49, "context": "The term EIS has not been however formalised until the clarification in [71] since the term \u201cevolving\u201d is sometime confused with the concept of evolutionary computation.", "startOffset": 72, "endOffset": 76}, {"referenceID": 25, "context": "Several extensions and variations of EIS have been put forward in the literature [39], [40], [67]-[70].", "startOffset": 81, "endOffset": 85}, {"referenceID": 26, "context": "Several extensions and variations of EIS have been put forward in the literature [39], [40], [67]-[70].", "startOffset": 87, "endOffset": 91}, {"referenceID": 48, "context": "Several extensions and variations of EIS have been put forward in the literature [39], [40], [67]-[70].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "An evolving version of Vector quantization was designed in [41] and is algorithmic backbone of FLEXFIS [42], which was later extended to a more robust version including rule merging in [43], generalized rules and an incremental feature weighting mechanism in [44].", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "An evolving version of Vector quantization was designed in [41] and is algorithmic backbone of FLEXFIS [42], which was later extended to a more robust version including rule merging in [43], generalized rules and an incremental feature weighting mechanism in [44].", "startOffset": 185, "endOffset": 189}, {"referenceID": 29, "context": "An evolving version of Vector quantization was designed in [41] and is algorithmic backbone of FLEXFIS [42], which was later extended to a more robust version including rule merging in [43], generalized rules and an incremental feature weighting mechanism in [44].", "startOffset": 259, "endOffset": 263}, {"referenceID": 29, "context": "A generalized TSK fuzzy rule was put forward in [45]-[47] and generates a non-axis parallel ellipsoidal cluster, which happens to have better coverage and flexibility than conventional fuzzy rules [44].", "startOffset": 197, "endOffset": 201}, {"referenceID": 30, "context": "statistical contribution borrowing the concept of hidden neuron statistical contribution in [48], [49].", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "statistical contribution borrowing the concept of hidden neuron statistical contribution in [48], [49].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Evolving Ensemble (eEnsemble) was proposed in [14] where it makes use of eTS [7] as a base-classifier and is realised under different configurations of the ensemble classifier.", "startOffset": 46, "endOffset": 50}, {"referenceID": 6, "context": "Evolving Ensemble (eEnsemble) was proposed in [14] where it makes use of eTS [7] as a base-classifier and is realised under different configurations of the ensemble classifier.", "startOffset": 77, "endOffset": 80}, {"referenceID": 32, "context": "This work was extended in [50] where eStacking is put forward using the concept of stacking ensemble.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "The all-pair classifier in [50] can be also grouped as an ensemble approach.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "The ensemble learning concept is well-known for its powerful generalization power because it address the bias-and-variance better and produces a model with high diversity covering a rich data region; 2) The use of evolving base classifier in the ensemble structure has been initiated in [14], [15], [50], [69], [70] but it relies on a static ensemble structure which is predetermined during the training process; 3) Existing EISs are categorized as a passive approach in handling concept drift because changing data distributions are overcome by continuously adapting a classifier.", "startOffset": 287, "endOffset": 291}, {"referenceID": 12, "context": "The ensemble learning concept is well-known for its powerful generalization power because it address the bias-and-variance better and produces a model with high diversity covering a rich data region; 2) The use of evolving base classifier in the ensemble structure has been initiated in [14], [15], [50], [69], [70] but it relies on a static ensemble structure which is predetermined during the training process; 3) Existing EISs are categorized as a passive approach in handling concept drift because changing data distributions are overcome by continuously adapting a classifier.", "startOffset": 293, "endOffset": 297}, {"referenceID": 32, "context": "The ensemble learning concept is well-known for its powerful generalization power because it address the bias-and-variance better and produces a model with high diversity covering a rich data region; 2) The use of evolving base classifier in the ensemble structure has been initiated in [14], [15], [50], [69], [70] but it relies on a static ensemble structure which is predetermined during the training process; 3) Existing EISs are categorized as a passive approach in handling concept drift because changing data distributions are overcome by continuously adapting a classifier.", "startOffset": 299, "endOffset": 303}, {"referenceID": 21, "context": "pENsemble stores a collection of local experts, which can be automatically generated when a drift is detected and pruned when it is no longer relevant to capture current data trends [34].", "startOffset": 182, "endOffset": 186}, {"referenceID": 19, "context": "The drift detection strategy relies on the concept of Hoeffding\u2019s bounds to determine the drift\u2019s level [27].", "startOffset": 104, "endOffset": 108}, {"referenceID": 35, "context": "The statistical process control approach is integrated to monitor dynamic of data streams [53] and classifies system behaviours into three stages, namely normal, warning and drift.", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "It allows an ensemble structure to expand its size when an uncharted training region comes into picture [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "An online non-parametric drift detection method is integrated using the Hoeffding\u2019s inequalities to determine acceptable level of concept changes in data streams [27].", "startOffset": 162, "endOffset": 166}, {"referenceID": 19, "context": "This method is capable of capturing significant distributional changes in data streams in the one-pass mode and is confirmed by solid theoretical guarantees in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": "It is worth mentioning that the drift handling strategy in [23] does not specifically detect the exact time period where a drift presents since it is derived from the forgetting concept \u2013 categorized as a passive approach.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "Referring to original work [27], two performance metrics, namely moving average and weighted moving average, are put forward.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "This approach is similar to the idea of statistical process control [53] except the basis of normality is relaxed here.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "We apply the same settings in [27] where , W D \uf061 \uf061", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Such trait is capable of lowering the fuzzy rule demand and retains inter-relations among input variables [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "The statistical contribution, however, ignores summarization power of a rule because it does not consider how strategic a current position of rule in the feature space is [24], [38].", "startOffset": 177, "endOffset": 181}, {"referenceID": 1, "context": "This concept follows the concept of recursive density estimation (RDE) [2], [7] where a density of a local region is computed recursively.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "This concept follows the concept of recursive density estimation (RDE) [2], [7] where a density of a local region is computed recursively.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "The DQ method differs from the RDE method [7] in two facets: 1) it involves a weighting strategy reducing the influence of outliers which causes a drop of density for next samples; 2) it uses the inverse multi-quadratic function in lieu of the Cauchy function; 3) it is tailored for the multivariate Gaussian function.", "startOffset": 42, "endOffset": 45}, {"referenceID": 36, "context": "The third rule growing strategy aims to overcome this issue borrowing the concept of GART+ [54].", "startOffset": 91, "endOffset": 95}, {"referenceID": 37, "context": "It limits the growth of the winning rule where a new rule is introduced when the size of winning rule exceeds a prespecified level [55].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "This scenario is realised by extending the concept of data potential [7], [56] for the rule pruning scenario.", "startOffset": 69, "endOffset": 72}, {"referenceID": 38, "context": "This scenario is realised by extending the concept of data potential [7], [56] for the rule pruning scenario.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "Furthermore, pClass utilises a direct update scheme of the inverse covariance matrix according to the formulas derived in [39] which shelves the reinversion of the covariance matrix.", "startOffset": 122, "endOffset": 126}, {"referenceID": 37, "context": "This winning rule selection is preferred over the compatibility measure [55] since it takes into account the rule\u2019s population.", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "The FWGRLS is a derivation of the FWRLS method originally proposed by Angelov in [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 39, "context": "It borrows the concept of weight decay function of the GRLS method in [57].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "\u2022 Learn++NSE is seen as one of pioneer works in dynamic ensemble classifier for non-stationary environments [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 40, "context": "It presents an extension of Learn++ [58] to tackle concept drifts in data streams.", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "\u2022 Learn++CDE is a generalized version of Learn++NSE integrating a specific mechanism to handle the class imbalanced problem in data streams [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "\u2022 eT2Class is another case of evolving classifiers unifying the dynamic network structure and the online learning capability [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 44, "context": "\u2022 McFIS characterises the so-called metacognitive learning machine assumed as an extension of evolving classifiers [62].", "startOffset": 115, "endOffset": 119}, {"referenceID": 41, "context": "Popular DDD problems characterizing the abrupt and gradual drifts, namely sin, sinh, line and 10dplane, were explored to investigate the performance of consolidated algorithms [59], [60].", "startOffset": 176, "endOffset": 180}, {"referenceID": 42, "context": "Popular DDD problems characterizing the abrupt and gradual drifts, namely sin, sinh, line and 10dplane, were explored to investigate the performance of consolidated algorithms [59], [60].", "startOffset": 182, "endOffset": 186}, {"referenceID": 43, "context": "The SEA problem introduced in [61] was used to bear out the efficacy of benchmarked algorithms.", "startOffset": 30, "endOffset": 34}, {"referenceID": 34, "context": "Moreover, an extension of the SEA problem contributed by Ditzler and Polikar [52] was put forward instead of its original version since it offers the class imbalance property and the cyclical drift which often occurs in the real-world data streams.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Another popular problem in the data stream mining area, namely the Gaussian problem, was exploited [18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "This scenario leads to a more resilient approach to deal with the plasticity-stability dilemma than static ensemble or greedy ensemble [27].", "startOffset": 135, "endOffset": 139}, {"referenceID": 45, "context": "The subjects of the experiment were only those donated their blood and never suffered from CHD or stroke verified from self-reported diagnosis or data from the hospital discharge database [63].", "startOffset": 188, "endOffset": 192}], "year": 2017, "abstractText": "The concept of ensemble learning offers a promising avenue in learning from data streams under complex environments because it addresses the bias and variance dilemma better than its single-model counterpart and features a reconfigurable structure, which is well-suited to the given context. While various extensions of ensemble learning for mining nonstationary data streams can be found in the literature, most of them are crafted under a static base-classifier and revisits preceding samples in the sliding window for a retraining step. This feature causes computationally prohibitive complexity and is not flexible enough to cope with rapidly changing environments. Their complexities are often demanding because it involves a large collection of offline classifiers due to the absence of structural complexities reduction mechanisms and lack of an online feature selection mechanism. A novel evolving ensemble classifier, namely Parsimonious Ensemble (pENsemble), is proposed in this paper. pENsemble differs from existing architectures in the fact that it is built upon an evolving classifier from data streams, termed Parsimonious Classifier (pClass). pENsemble is equipped by an ensemble pruning mechanism, which estimates a localized generalization error of a base-classifier. A dynamic online feature selection scenario is integrated into the pENsemble. This method allows for dynamic selection and deselection of input features on the fly. pENsemble adopts a dynamic ensemble structure to output a final classification decision where it features a novel drift detection scenario to grow the ensemble\u2019s structure. The efficacy of the pENsemble has been numerically demonstrated through rigorous numerical studies with dynamic and evolving data streams where it delivers the most encouraging performance in attaining a tradeoff between accuracy and complexity.", "creator": "Microsoft\u00ae Word 2016"}}}