{"id": "1705.06640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "DeepXplore: Automated Whitebox Testing of Deep Learning Systems", "abstract": "Deep learning (DL) systems are increasingly deployed in security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner-case inputs are of great importance. However, systematic testing of large-scale DL systems with thousands of neurons and millions of parameters for all possible corner-cases is a hard problem. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose different erroneous behaviors for rare inputs.", "histories": [["v1", "Thu, 18 May 2017 15:09:39 GMT  (7849kb,D)", "http://arxiv.org/abs/1705.06640v1", null], ["v2", "Sun, 4 Jun 2017 23:40:43 GMT  (4771kb,D)", "http://arxiv.org/abs/1705.06640v2", null], ["v3", "Mon, 10 Jul 2017 17:05:40 GMT  (4771kb,D)", "http://arxiv.org/abs/1705.06640v3", null], ["v4", "Sun, 24 Sep 2017 15:55:11 GMT  (5526kb,D)", "http://arxiv.org/abs/1705.06640v4", "To be published in SOSP'17"]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.SE", "authors": ["kexin pei", "yinzhi cao", "junfeng yang", "suman jana"], "accepted": false, "id": "1705.06640"}, "pdf": {"name": "1705.06640.pdf", "metadata": {"source": "CRF", "title": "DeepXplore: Automated Whitebox Testing of Deep Learning Systems", "authors": ["Kexin Pei", "Yinzhi Cao", "Junfeng Yang", "Suman Jana"], "emails": ["suman}@cs.columbia.edu", "yinzhi.cao@lehigh.edu"], "sections": [{"heading": null, "text": "We design, implement and evaluate DeepXplore, the first white box framework for systematic testing of DL systems in the real world. We solve two main problems: (1) the generation of inputs that trigger different parts of the logic of a DL system, and (2) the identification of wrong behaviors of DL systems without manual effort. First, we introduce neuron coverage to systematically estimate the parts of the DL system that are exercised by a series of test inputs. Next, we use multiple DL systems with similar functionality as cross-referencing oracles, thus avoiding manual checking for erroneous behavior. We show how finding input triggers different behaviors, while simultaneously presenting high neuron coverage for DL algorithms as a common optimization problem and efficiently solving it using gradient-based optimization techniques. DeepXplore efficiently finds thousands of unique malware cornered cars (for example, the self-crafted CA case data) based on optimization techniques."}, {"heading": "1. Introduction", "text": "In fact, it is so that most of them will be able to survive themselves, without it coming to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it"}, {"heading": "2. DL Background", "text": "The main component of any DL system is an underlying deep neural network (DNN). A DNN consists of several layers, each of which contains several neurons, as shown in Figure 2. A neuron is an individual processing unit within a DNN that applies an activation function to its inputs and passes the result to other connected neurons (see Figure 2.) Common activation functions include sigmoid, hyperbolic tangent, or ReLU (Rectified Linear Unit) [32]. A DNN usually has at least three (often more) layers: an input, an output, and one or more hidden layers. Each neuron in a layer has directed connections to the neurons in the next layer. The number of neurons in each layer and the connections between them vary significantly via DNNs. Overall, a DNN can be mathematically defined as multiple input, with the parametric F function consisting of many different subparametric functions."}, {"heading": "3. Limitations of existing DNN testing", "text": "In fact, it is a very complex story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, in which we have to tell a story, a story, a story, a story, a story, a story, a story, a story, a story, a story, a story, a story, a story, a story, a story."}, {"heading": "4. Overview of DeepXplore", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5. Methodology", "text": "In this section, we provide a detailed technical description of our algorithm. First, we define and explain the concepts of neuron coverage and gradient for DNNs. Next, we describe how the test problem can be formulated as a common optimization problem. Finally, we provide the algorithm for common optimization using gradients."}, {"heading": "5.1 Definitions", "text": "We define the neuron coverage of a set of test inputs as the ratio of the number of unique activated neurons for all test inputs in the set and the total number of neurons in the DNN. We consider a neuron to be activated if its output is higher than a threshold value (e.g. 0). Formally, we assume that all neurons in a DNN are represented by the set N = {n1, n2,...}, all test inputs are represented by the set T = {x1, x2,...}, and off (n, x) is a function that returns the initial value of neurons n in the DNN for a given test input x. Let us represent the threshold for viewing a neuron to be activated. Neuron coverage can essentially be defined as: NCov (T, x) = {n | x x x x, out (n, x > t)."}, {"heading": "5.2 DeepXplore algorithm", "text": "The main advantage of the test input generation process for a DNN over conventional software is that once defined as an optimization problem, the test generation process can be efficiently solved using gradient ascendant. In this section, we describe the details of formulating and solving the optimization process. Note that an efficient solution of the optimization problem for DNNs is possible as gradients of the objective functions of DNNs, while maintaining domain-specific constraints provided by users. Algorithm 1 shows the algorithm for generating test inputs by solving this common optimization problem. Below, we formally define the objectives of our common optimization problems and explain the details of the algorithm to solve this problem."}, {"heading": "6. Implementation", "text": "We implement DeepXplore with Tensorflow 1.0.1 [1] and Keras 2.0.3 [10] DL frameworks. Our implementation consists of approximately 7,086 lines of code. We use Tensorflow's efficient implementation of gradient calculations in our joint optimization process. Tensorflow also supports the creation of SubDNNs by marking arbitrary neurons as the output of the SubDNN while maintaining the input of the original DNN. We use this function to intercept and record the output of neurons in the intermediate layers of a DNN and calculate the appropriate gradations in relation to the input of the DNN. All of our experiments were performed on a Linux laptop running Ubuntu 16.04 (an Intel i7-6700HQ 2.60GHz processor with 4 cores, 16GB of memory and an NVIDIA GTX 1070 GPU)."}, {"heading": "7. Experimental Setup", "text": "In this section, we will first describe the records and DNNs we used to evaluate DeepXplore, and then present an overview of DeepXplore's performance and various types of erroneous behavior it has found in the tested DNNs."}, {"heading": "7.1 Test datasets and DNNs", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "7.2 Domain-specific constraints", "text": "This year, it has come to the point where there will only be one person who is able to look after another person who is able to look after another person, who is able to look after another person."}, {"heading": "8. Results", "text": "DeepXplore was able to find thousands of erroneous behaviors in all tested DNNs. For space reasons, we present only a few representative inputs generated by DeepXplore that resulted in erroneous behavior of the DNNs. Figure 7 shows some of the differential-inducing inputs generated by DeepXplore (with different domain-specific limitations) for MNIST, ImageNet, and Driving datasets, along with the erroneous behavior of the corresponding DNNs. Table 2 (Contagio) and Table 3 (Drebin) show two sample inputs generated by DeepXplore that caused erroneous behavior in at least one of the tested DNNs."}, {"heading": "8.1 Effectiveness of neuron coverage as a testing metric", "text": "It has been shown that every human being is able to move into a different world, in which he is able to explore another world."}, {"heading": "8.2 DeepXplore performance", "text": "To compare the neuron coverage achieved by the test cases generated by DeepXplore, we compare the neuron coverage for the same number of tests (1% of the original test set) (1) generated by DeepXplore, (2) generated by adversarial tests [18], and (3) randomly selected from the original test set. Results are shown in Table 4. We found that the neuron coverage threshold t (defined in Section 5), which determines when a neuron is activated, strongly influences the value of coverage achieved. Therefore, Figure 8 compares the changes in average neuron coverage achieved by these three methods with the threshold that changes from 0 to 0.75."}, {"heading": "8.3 Other usages of inputs generated by DeepXplore", "text": "In addition to highlighting incorrect behavior in DNNs, the test inputs generated by DeepXplore can also be used for other purposes such as increasing training data to improve the accuracy of a DNN and detecting contamination of training data. We summarize our experimental results for such use of DeepXplore generated inputs below. Augmenting training data to improve DNNs can be incorporated back into the training program by DeepXplore and improve the accuracy of DNNs. Such a strategy has also been adopted by opposing testing work [18], although it requires manual labeling of test inputs. In contrast, we adopt majority decisions to automatically label generated test inputs.We compare the improvement in DNN accuracy by augmenting the training set with the inputs of LeepXplore inputs."}, {"heading": "9. Discussion", "text": "DeepXplore adopts the differential testing technique from software analysis, and therefore DeepXplore has the limitations of differential testing. Here are two main limitations: First, differential testing requires at least two different DNNs with the same functionality to identify flawed key cases without manual effort. If several similar DNNs are not available, differential testing tools like DeepXplore cannot identify the flawed behavior of the DNNs. If two DNNs differ only slightly (i.e. by one or two neurons), DeepXplore will take longer to find differential behavior than if the DNNs differ significantly. However, our assessment shows that in most cases it is relatively easy to find several different DNNs for a particular problem. This is understandable because developers prefer to define and train their own DNNs for improved flexibility. Furthermore, differential testing on each DNNs is best suited to the parameters that are configured for the different ones."}, {"heading": "10. Related Work", "text": "In fact, most people who are able to move are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "11. Conclusion", "text": "We developed and implemented DeepXplore, a differential whitebox test system that systematically tests DL systems and automatically identifies potentially erroneous behaviors. In this post, we introduced the concept of neuron coverage to estimate how much logic a DNN exerts through a series of inputs. DeepXplore performs gradient ascent to solve a common optimization problem that maximizes both neuron coverage and the number of potentially erroneous behaviors. We evaluated DeepXplore using 15 state-of-the-art DNNs trained on 5 real-world datasets. DeepXplore found thousands of erroneous behaviors in all DNNs tested."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["M. ABADI", "P. BARHAM", "J. CHEN", "Z. CHEN", "A. DAVIS", "J. DEAN", "M. DEVIN", "S. GHEMAWAT", "G. IRVING", "M ISARD"], "venue": "In Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Sfadiff: Automated evasion attacks and fingerprinting using black-box differential automata learning", "author": ["G. ARGYROS", "I. STAIS", "S. JANA", "A.D. KEROMYTIS", "A. KIAYIAS"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (2016),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Effective and explainable detection of android malware in your pocket", "author": ["D. ARP", "M. SPREITZENBARTH", "M. HUBNER", "H. GASCON", "K. RIECK", "SIEMENS", "C. Drebin"], "venue": "In 21th Annual Network and Distributed System Security Symposium (NDSS)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "End to end learning for self-driving cars", "author": ["M. BOJARSKI", "D. DEL TESTA", "D. DWORAKOWSKI", "B. FIRNER", "B. FLEPP", "P. GOYAL", "L.D. JACKEL", "M. MON- FORT", "U. MULLER", "J ZHANG"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Using frankencerts for automated adversarial testing of certificate validation in SSL/TLS implementations", "author": ["C. BRUBAKER", "S. JANA", "B. RAY", "S. KHURSHID", "V. SHMATIKOV"], "venue": "In Proceedings of the 2014 IEEE Symposium on Security and Privacy (S&P)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Towards making systems forget with machine unlearning", "author": ["Y. CAO", "J. YANG"], "venue": "In Proceedings of the 2015 IEEE Symposium on Security and Privacy (Washington, DC,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Coveragedirected differential testing of JVM implementations", "author": ["CHEN Y", "SU T", "SUN C", "SU Z", "ZHAO J"], "venue": "In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI) (2016),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Guided differential testing of certificate validation in ssl/tls implementations", "author": ["CHEN Y", "SU"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering (2015),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. DENG", "W. DONG", "R. SOCHER", "LI", "LI L.-J", "L. FEI-FEI"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["M. FREDRIKSON", "S. JHA", "T. RISTENPART"], "venue": "In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (2015),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Privacy in pharmacogenetics: An endto-end case study of personalized warfarin dosing", "author": ["M. FREDRIKSON", "E. LANTZ", "S. JHA", "S. LIN", "D. PAGE", "T. RISTENPART"], "venue": "In USENIX Security", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. FREUND", "R.E. SCHAPIRE"], "venue": "In European conference on computational learning theory", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "A neural algorithm of artistic style", "author": ["L.A. GATYS", "A.S. ECKER", "M. BETHGE"], "venue": "arXiv preprint arXiv:1508.06576", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. GOODFELLOW", "J. SHLENS", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1412.6572", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Randomized differential testing as a prelude to formal verification", "author": ["A. GROCE", "G. HOLZMANN", "R. JOSHI"], "venue": "In Proceedings of the 29th international conference on Software Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. GROSSE", "N. PAPERNOT", "P. MANOHARAN", "M. BACKES", "P. MCDANIEL"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. IOFFE", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Policy compression for aircraft collision avoidance systems", "author": ["K.D. JULIAN", "J. LOPEZ", "J.S. BRUSH", "M.P. OWEN", "M.J. KOCHENDERFER"], "venue": "In Digital Avionics Systems Conference (DASC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Privacy oracle: a system for finding application leaks with black box differential testing", "author": ["J. JUNG", "A. SHETH", "B. GREENSTEIN", "D. WETHERALL", "G. MAGANIS", "T. KOHNO"], "venue": "In Proceedings of the 15th ACM conference on Computer and communications security (2008),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LECUN", "L. BOTTOU", "Y. BENGIO", "P. HAFFNER"], "venue": "Proceedings of the IEEE 86,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Mnist handwritten digit database", "author": ["Y. LECUN", "C. CORTES", "C.J. BURGES"], "venue": "AT&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Combining markov random fields and convolutional neural networks for image synthesis", "author": ["C. LI", "M. WAND"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Understanding deep image representations by inverting them", "author": ["A. MAHENDRAN", "A. VEDALDI"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Differential testing for software", "author": ["W.M. MCKEEMAN"], "venue": "Digital Technical Journal 10,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. NAIR", "G.E. HINTON"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. NGUYEN", "J. YOSINSKI", "J. CLUNE"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. PAPERNOT", "P. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI"], "venue": "IEEE European Symposium on Security and Privacy (EuroS P) (March", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Misleading worm signature generators using deliberate noise injection", "author": ["R. PERDISCI", "D. DAGON", "W. LEE", "P. FOGLA", "M. SHARIF"], "venue": "IEEE Symposium on Security and Privacy (S", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "NEZHA: Efficient Domain-independent Differential Testing", "author": ["T. PETSIOS", "A. TANG", "S.J. STOLFO", "A.D. KEROMYTIS", "S. JANA"], "venue": "In Proceedings of the 38th IEEE Symposium on Security & Privacy (San Jose,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Learning to generate reviews and discovering sentiment", "author": ["A. RADFORD", "R. JOZEFOWICZ", "I. SUTSKEVER"], "venue": "arXiv preprint arXiv:1704.01444", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Artistic style transfer for videos", "author": ["M. RUDER", "A. DOSOVITSKIY", "T. BROX"], "venue": "In German Conference on Pattern Recognition", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. RUMELHART", "G.E. HINTON", "R.J. WILLIAMS"], "venue": "Cognitive modeling 5,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1988}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. SHARIF", "S. BHAGAVATULA", "L. BAUER", "M.K. REITER"], "venue": "In Proceedings of the 23rd ACM SIGSAC Conference on Computer and Communications Security (Oct", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. SILVER", "A. HUANG", "C.J. MADDISON", "A. GUEZ", "L. SIFRE", "G. VAN DEN DRIESSCHE", "J. SCHRITTWIESER", "I. ANTONOGLOU", "V. PANNEERSHELVAM", "M. LANCTOT", "S. DIELEMAN", "D. GREWE", "J. NHAM", "N. KALCHBREN- NER", "I. SUTSKEVER", "T. LILLICRAP", "M. LEACH", "K. KAVUKCUOGLU", "T. GRAEPEL", "D. HASSABIS"], "venue": "Nature", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. SIMONYAN", "A. VEDALDI", "A. ZISSERMAN"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. SIMONYAN", "A. ZISSERMAN"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "HVLearn: Automated Black-box Analysis of Hostname Verification in SSL/TLS Implementations", "author": ["S. SIVAKORN", "G. ARGYROS", "K. PEI", "A.D. KEROMYTIS", "S. JANA"], "venue": "In Proceedings of the 38th IEEE Symposium on Security & Privacy (San Jose,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2017}, {"title": "Malicious pdf detection using metadata and structural features", "author": ["C. SMUTZ", "A. STAVROU"], "venue": "In Proceedings of the 28th Annual Computer Security Applications Conference (2012),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Mobile-sandbox: having a deeper look into android applications", "author": ["M. SPREITZENBARTH", "F. FREILING", "F. ECHTLER", "T. SCHRECK", "J. HOFFMANN"], "venue": "In Proceedings of the 28th Annual ACM Symposium on Applied Computing (2013),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. SRIVASTAVA", "G.E. HINTON", "A. KRIZHEVSKY", "I. SUTSKEVER", "R. SALAKHUTDINOV"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ERHAN", "I. GOODFELLOW", "R. FERGUS"], "venue": "In arXiv preprint arXiv:1312.6199", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Stealing machine learning models via prediction apis", "author": ["F. TRAM\u00c8R", "F. ZHANG", "A. JUELS", "M.K. REITER", "T. RISTENPART"], "venue": "In 25th USENIX Security Symposium (USENIX Security 16) (Austin,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Practical evasion of a learningbased classifier: a case study", "author": ["N. \u0160RNDIC", "P. LASKOV"], "venue": "In Proceedings of the 2014 IEEE 14  Symposium on Security and Privacy (Washington, DC,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. WANG", "A.C. BOVIK", "H.R. SHEIKH", "E.P. SIMON- CELLI"], "venue": "IEEE transactions on image processing 13,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2004}, {"title": "A methodology for formalizing model-inversion attacks", "author": ["X. WU", "M. FREDRIKSON", "S. JHA", "J.F. NAUGHTON"], "venue": "In Computer Security Foundations Symposium (CSF),", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2016}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W. XIONG", "J. DROPPO", "X. HUANG", "F. SEIDE", "M. SELTZER", "A. STOLCKE", "D. YU", "G. ZWEIG"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2016}, {"title": "Automatically evading classifiers", "author": ["XU W", "QI Y", "EVANS D"], "venue": "In Proceedings of the 2016 Network and Distributed Systems Symposium", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2016}, {"title": "Finding and understanding bugs in c compilers", "author": ["X. YANG", "Y. CHEN", "E. EIDE", "J. REGEHR"], "venue": "In ACM SIGPLAN Notices", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2011}, {"title": "Understanding neural networks through deep visualization", "author": ["J. YOSINSKI", "J. CLUNE", "T. FUCHS", "H. LIPSON"], "venue": "ICML Workshop on Deep Learning", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Droid-sec: deep learning in android malware detection", "author": ["YUAN Z", "LU Y", "WANG Z", "XUE"], "venue": "In ACM SIG- COMM Computer Communication Review (2014),", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 181, "endOffset": 189}, {"referenceID": 36, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 181, "endOffset": 189}, {"referenceID": 46, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 210, "endOffset": 214}, {"referenceID": 34, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 242, "endOffset": 246}, {"referenceID": 3, "context": "This has also led to widespread adoption and deployment of DL in security- and safety-critical systems like self-driving cars [5], malware detection [63], and aircraft collision avoidance systems [24].", "startOffset": 126, "endOffset": 129}, {"referenceID": 50, "context": "This has also led to widespread adoption and deployment of DL in security- and safety-critical systems like self-driving cars [5], malware detection [63], and aircraft collision avoidance systems [24].", "startOffset": 149, "endOffset": 153}, {"referenceID": 18, "context": "This has also led to widespread adoption and deployment of DL in security- and safety-critical systems like self-driving cars [5], malware detection [63], and aircraft collision avoidance systems [24].", "startOffset": 196, "endOffset": 200}, {"referenceID": 13, "context": "Recent work on adversarial deep learning [18, 33, 50] have demonstrated how to create synthetic images that can fool state-of-the-art DL systems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 26, "context": "Recent work on adversarial deep learning [18, 33, 50] have demonstrated how to create synthetic images that can fool state-of-the-art DL systems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 41, "context": "Recent work on adversarial deep learning [18, 33, 50] have demonstrated how to create synthetic images that can fool state-of-the-art DL systems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 1, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 4, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 6, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 7, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 24, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 48, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 25, "context": "The common activation functions include sigmoid, hyperbolic tangent, or ReLU (Rectified Linear Unit) [32].", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "DNNs can be trained using different training algorithms, but gradient descent using backpropagation is by far the most popular training algorithm for DNNs [40].", "startOffset": 155, "endOffset": 159}, {"referenceID": 49, "context": "Next, the first few hidden layers transform the raw pixel values into lowlevel texture features like edges or colors and feed them to the deeper layers [62].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 15, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 27, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 33, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 43, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 47, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 13, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 15, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 27, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 41, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 23, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 123, "endOffset": 135}, {"referenceID": 35, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 123, "endOffset": 135}, {"referenceID": 49, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 123, "endOffset": 135}, {"referenceID": 49, "context": "We provide a brief definition here for the sake of completeness and refer interested readers to [62] for more details.", "startOffset": 96, "endOffset": 100}, {"referenceID": 33, "context": "One important aspect of the optimization process is that the generated test inputs must satisfy several domain-specific constraints to be considered physically realistic inputs [42].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "1 [1] and Keras 2.", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": "We construct three different neural networks based on the LeNet family [26], namely the LeNet-1, LeNet-4, and LeNet-5.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "The detailed architecture description of these DNNs can be found in [26].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "ImageNet is a large image dataset with over 10, 000, 000 hand-annotated images that were crowdsourced and labeled manually [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "We test three well-known pre-trained DNNs: VGG-16 [45], VGG-19 [45], and ResNet50 [22].", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "We test three well-known pre-trained DNNs: VGG-16 [45], VGG-19 [45], and ResNet50 [22].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "We test three well-known pre-trained DNNs: VGG-16 [45], VGG-19 [45], and ResNet50 [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "We used three DNNs [4, 11, 55] based on the DAVE-2 selfdriving car architecture from Nvidia [5] with slightly different configurations, which are called DAVE-orig, DAVE-norminit, and DAVE-dropout respectively.", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "Specifically, DAVE-orig [4] fully replicates the original architecture from the Nvidia\u2019s paper [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 17, "context": "DAVE-norminit [55] removes the first batch normalization layer [23] and normalizes the randomly initialized network weights.", "startOffset": 63, "endOffset": 67}, {"referenceID": 40, "context": "DAVE-dropout [11] simplifies DAVE-orig with less number of convolutional layers and fully connected layers and adds two dropout layer [49] between the final 3 fully-connected layers.", "startOffset": 134, "endOffset": 138}, {"referenceID": 38, "context": "PDFrate [35, 47], an online service for PDF malware detection, defines 135 static features to transform a PDF document into a feature vector.", "startOffset": 8, "endOffset": 16}, {"referenceID": 43, "context": "Since, to the best of our knowledge, there is no DNN-based, publicly available PDF malware detection system, we defined and trained three different DNNs and ensured that they achieve similar performance as reported by [56] using SVMs.", "startOffset": 218, "endOffset": 222}, {"referenceID": 43, "context": "We use 5, 000 benign and 12, 205 malicious PDF documents from Contagio database, which are preprocessed into 135 feature vectors as described in [56].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "Drebin is a dataset [3, 48] that includes 129, 013 Android applications among which 123, 453 are benign and 5, 560 are malicious.", "startOffset": 20, "endOffset": 27}, {"referenceID": 39, "context": "Drebin is a dataset [3, 48] that includes 129, 013 Android applications among which 123, 453 are benign and 5, 560 are malicious.", "startOffset": 20, "endOffset": 27}, {"referenceID": 15, "context": "[21] evaluated 36 DNNs with different numbers of layers and number of neurons per layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[26, 28] 99.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[26, 28] 99.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[45] 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[45] 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[22] 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5] N/A 99.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[21] 98.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[21] 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[21] 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[56]", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "We use the same training/testing split (66%/33%) as reported in [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 43, "context": "For Contagio dataset, we follow the feature definitions described in [56], which clearly defines the restrictions on each feature, i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": "It has recently been shown that each neuron in a DNN tends to be responsible for extracting a specific feature of the input instead of multiple neurons collaborating to extract a feature [38, 62].", "startOffset": 187, "endOffset": 195}, {"referenceID": 49, "context": "It has recently been shown that each neuron in a DNN tends to be responsible for extracting a specific feature of the input instead of multiple neurons collaborating to extract a feature [38, 62].", "startOffset": 187, "endOffset": 195}, {"referenceID": 13, "context": "In order to compare the neuron coverage achieved by the test cases generated by DeepXplore, we compare the neuron coverage for the same number of tests (1% of the original testing set) (1) generated by DeepXplore, (2) generated by adversarial testing [18], and (3) randomly selected from the original test set.", "startOffset": 251, "endOffset": 255}, {"referenceID": 0, "context": "For the cases where the activation functions of intermediate DNN layers produce values in [0,+\u221e] and those in the final layers produce a value in [0, 1], we use a strategy similar to the batch normalization [23] for making the neuron outputs across all layers comparable.", "startOffset": 146, "endOffset": 152}, {"referenceID": 17, "context": "For the cases where the activation functions of intermediate DNN layers produce values in [0,+\u221e] and those in the final layers produce a value in [0, 1], we use a strategy similar to the batch normalization [23] for making the neuron outputs across all layers comparable.", "startOffset": 207, "endOffset": 211}, {"referenceID": 13, "context": "Figure 8: The neuron coverage achieved by the same number of inputs (1% of the original test set) produced by DeepXplore, adversarial testing [18], and random selection from the original test set.", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "generated by random selection (\u201crandom\u201d), adversarial testing (\u201cadversarial\u201d) [18], and DeepXplore.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "Such strategy has also been adopted by the adversarial testing works [18] although, unlike us, they require manual labeling of test inputs.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "By contrast, we adopt majority voting [16] to automatically generate labels for generated test inputs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 44, "context": "We then searched for samples in the training set that are closest to the test inputs generated by DeepXplore in terms of structural similarity [57] and identified them as polluted data.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 121, "endOffset": 133}, {"referenceID": 26, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 121, "endOffset": 133}, {"referenceID": 41, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 121, "endOffset": 133}, {"referenceID": 5, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 9, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 10, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 28, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 33, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 42, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 45, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 36, "context": "[45] and Mahendran et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[30] used gradients to visualize activation of different intermediate layers of a convolutional DNN for tasks like object segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Several prior works have used gradient ascent on a DNN\u2019s intermediate layers\u2019 outputs to maximize those layers\u2019 activation for transferring the artistic styles of one image to the other [17, 29, 39].", "startOffset": 186, "endOffset": 198}, {"referenceID": 22, "context": "Several prior works have used gradient ascent on a DNN\u2019s intermediate layers\u2019 outputs to maximize those layers\u2019 activation for transferring the artistic styles of one image to the other [17, 29, 39].", "startOffset": 186, "endOffset": 198}, {"referenceID": 31, "context": "Several prior works have used gradient ascent on a DNN\u2019s intermediate layers\u2019 outputs to maximize those layers\u2019 activation for transferring the artistic styles of one image to the other [17, 29, 39].", "startOffset": 186, "endOffset": 198}, {"referenceID": 6, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 131, "endOffset": 134}, {"referenceID": 24, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 148, "endOffset": 156}, {"referenceID": 48, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 148, "endOffset": 156}, {"referenceID": 4, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 7, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 29, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 37, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 29, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 225, "endOffset": 229}, {"referenceID": 14, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 253, "endOffset": 257}, {"referenceID": 19, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 279, "endOffset": 283}, {"referenceID": 1, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 315, "endOffset": 318}], "year": 2017, "abstractText": "Deep learning (DL) systems are increasingly deployed in safetyand security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system\u2019s behavior for corner-case inputs are of great importance. However, systematic testing of large-scale DL systems with thousands of neurons and millions of parameters for all possible corner-cases is a hard problem. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose different erroneous behaviors for rare inputs. We design, implement, and evaluate DeepXplore, the first white-box framework for systematically testing real-world DL systems. We address two main problems: (1) generating inputs that trigger different parts of a DL system\u2019s logic and (2) identifying incorrect behaviors of DL systems without manual effort. First, we introduce neuron coverage for systematically estimating the parts of DL system exercised by a set of test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles and thus avoid manual checking for erroneous behaviors. We demonstrate how finding inputs triggering differential behaviors while achieving high neuron coverage for DL algorithms can be represented as a joint optimization problem and solved efficiently using gradient-based optimization techniques. DeepXplore efficiently finds thousands of unique incorrect corner-case behaviors (e.g., self-driving cars crashing into guard rails, malware masquerading as benign software) in state-of-the-art DL models trained on five popular datasets including driving data collected by Udacity from cars around Mountain View (CA) and ImageNet data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running on a commodity laptop. The inputs generated by DeepXplore achieved 33.2% higher neuron coverage on average than existing testing methods. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve classification accuracy or identify polluted training data.", "creator": "LaTeX with hyperref package"}}}