{"id": "1705.06769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning", "abstract": "The problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning. Hierarchical reinforcement learning (HRL) tackles this problem by using a set of temporally-extended actions, or options, each of which has its own subgoal. These subgoals are normally handcrafted for specific tasks. Here, though, we introduce a generic class of subgoals with broad applicability in the visual domain. Underlying our approach (in common with work using \"auxiliary tasks\") is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have. We incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the Atari suite. We highlight the advantage of our approach in one of the hardest games -- Montezuma's revenge -- for which the ability to handle sparse rewards is key. Our agent learns several times faster than the current state-of-the-art HRL agent in this game, reaching a similar level of performance.", "histories": [["v1", "Thu, 18 May 2017 19:00:43 GMT  (963kb,D)", "http://arxiv.org/abs/1705.06769v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["nat dilokthanakul", "christos kaplanis", "nick pawlowski", "murray shanahan"], "accepted": false, "id": "1705.06769"}, "pdf": {"name": "1705.06769.pdf", "metadata": {"source": "CRF", "title": "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning", "authors": ["Nat Dilokthanakul", "Christos Kaplanis"], "emails": ["n.dilokthanakul14@imperial.ac.uk", "christos.kaplanis14@imperial.ac.uk", "n.pawlowski16@imperial.ac.uk", "m.shanahan@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is an approach aimed at tackling the problem of gender inequality."}, {"heading": "1.1 Related work", "text": "The idea of embodying an agent with a form of intrinsic motivation, which in our case is the desire to be able to control aspects of his environment, is one that has been explored in several other works. Klyubin et al. introduced empowerment as an information-theoretical measure of degrees of freedom that an agent has in an environment [15]. The concept of empowerment has recently gained interest in connection with intrinsically motivated solidification learning [22, 10]. In the other lines of the work, intrinsic motivation is defined in the form of curiosity, which can be measured with model learning progress [25, 12] or information acquisition [4]. Jaderberg et al. introduced the idea of off-policy training with auxiliary control tasks, such as pixel control or function control, which can significantly accelerate learning."}, {"heading": "2 The model", "text": "We look at the default amplification learning situation, where an agent interacts with an environment by observing the state of the environment st and performing an action at each individual time step t. The environment provides the agent rextt with an extrinsic reward and then transitions to the next state st + 1. The objective of the agent is to maximize the cumulative sum of extrinsic rewards over the finite horizon of an episode. Specifically, we look at a hierarchical agent with two components: a meta controller and a subcontroller. The subcontroller is responsible for selecting the actions of the agent and interacts directly with the environment. The meta controller operates on a longer time scale of c-time steps and influences the behavior of the subcontroller through a subgoal argument, gt. This influence is imposed by assigning an input function to the subcontroller next to st."}, {"heading": "2.1 Measure of control for intrinsic reward", "text": "It is indeed the case that most people are able to keep to the rules that they have imposed on themselves. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is the case that they keep to the rules. (...) It is the case that they do not. (...) It is the case that they keep to the rules. (...) It is the case that they keep to the rules. (...) It is the case that they keep to the rules. (...) It is the case that they keep to the rules. (...) It is the case. (...) It is. (...) It is. (...) It is. (...) It is. (It is. (...) It is. (It is. (...) It is. (It is. (It is.) It is. (It is.) It is. (It is. (It is.) It is. (It is. (It is.) It is. (It is. (It is.) It is. (It is. (It is.) It is. (It is. (it.) It is. It is. (It is.). (it is. It is."}, {"heading": "2.2 Architectural and optimisation details", "text": "Our baseline model is a variant of the asynchronous advantage-actor-critic algorithm (A3C) = additional function. [21] We adapted OpenAI's A3C implementation2 3 to follow the architecture specified by Wang et al. [31] The model consists of two parts: an encryption module and a long-short-term memory layer (LSTM) [11]. The encryption module consists of two encryption layers and a fully connected layer. The first link has 16 8x8 filters with a step length of 4 and the second layer has 32 4x4 filters with a step length of 2. The second link follows a fully connected layer with 256 units. The output of the fully connected layer is then linked to the previous action and reward, and then fed into an LSTM layer. The LSTM argument has 256 cells whose output projects linear politics and value networks."}, {"heading": "3 Experimental results and discussion", "text": "(https: / / github.com / openai / universe-starter-agent), which was written with Tensorflow [1]. 3The source code of our implementation will be made publicly available. 1. To verify whether the influence of the meta-controller is beneficial in sparse reward environments. 2. To evaluate the performance of our agent in multiple environments compared to current state-of-the-art HRL systems. 3. To investigate the behavior of agents under the influence of pixel control and intrinsic motivation."}, {"heading": "3.1 The environments", "text": "We tested the model for Atari games in the OpenAI gym environment [7], a toolkit for comparing reinforcement learning algorithms that envelops the Arcade Learning Environment (ALE) [5] with a series of modifications. In our experiments, we make comparisons with the Feudal Network (FuN) [30] and Option-Critic [2] architectures, both evaluated at ALE. 4 The environment returns the state as 210x160x3 RGB pixels. We pre-process the state by transforming it into an 84x84x3 matrix, maintaining the RGB channels. We also cut the extrinsic reward into the range of [-1, 1]. We used v0 setting for all games, e.g. MontezumaRevenge-v0 for Montezuma's Revenge."}, {"heading": "3.2 Experiment 1: Influence of the meta-controller on performance", "text": "In order to evaluate the effectiveness of the meta-controller, we ran our agent with different relative weights \u03b2 {0.00, 0.25, 0.50, \u03b2, 1.00} between extrinsic and intrinsic reward and compared the performance with the baseline agent. We used BPTT = 20 for the sub-controller. Firstly, we found that the performance of the agent with \u03b2 = 1.00 (no intrinsic reward) was similar to that of the baseline. This result shows that any significant gain or decrease in performance with other values \u03b2 can be attributed to the intrinsic reward. As shown in Figure 2, the feature-control agent with \u03b2 = 0.75 outperforms other agents in Montezuma's Revenge and Frostbite and competes with other agents in Q * bert and Private Eye. This result suggests that the introduction of a certain proportion of intrinsic reward in the sub-controller is not directly relevant, however, it is important to note that they are not directly comparable."}, {"heading": "3.3 Experiment 2: Performance instability and the length of the BPTT roll-out", "text": "In our first experiments, we observed instability in the training curve of the BPTT control agent, which manifests itself in catastrophic performance drops. To alleviate this problem, we tried to increase the BPTT roll-out from 20 steps to 100 steps for the sub-controller. We argued that a longer sequence of BPTT could contribute to training stability in the following ways: (i) the updates are less frequent and give the agent more stable properties, which are a critical component in calculating intrinsic reward, and (ii) it allows the gradient to be propagated further back into the past, potentially reducing the distortion during updating. Figure 3 shows that the agent with BPTT = 100 achieves a much higher value than with BPTT = 20. However, with Frostbite, we observe the opposite effect. This could be because the course is already stable at BPTT = 20 and thus the BT formation has no positive effect (see figure 3)."}, {"heading": "3.4 Experiment 3: Comparison to state-of-the-art hierarchical RL systems", "text": "In this experiment, we examined our agent for controlling features using Pac-Man, Asterix, Zaxxon and Montezuma's Revenge. The aim was to show that the method can be applied to a wide range of games, and to compare our system with two state-of-the-art end-to-end HRL systems, Option-Critic [2] and FuN [30] architectures. Our results are shown in Figure 4 and we note that: (i) For Ms. Pac-Man, Asterix and Zaxxon, we achieve better maximum values than the Option-Critic network, but worse maximum values than the FuN network; (ii) for Montezuma's Revenge, our agent achieves roughly the same maximum score as the FuN network, but learns much faster and achieves this level of performance after less than a fifth of the number of observations. We expect to be able to improve the performance of our agent with a broader parameter search."}, {"heading": "3.5 Experiment 4: Visualisation of the agent\u2019s behaviour under intrinsic motivation", "text": "The influence of the intrinsic motivation exerted by the meta-controller on the agent's behavior is easiest to visualize with the Pixel Controller Agent. Figure 5a shows a sequence of screenshots from Montezuma's Revenge, where we see the subcontroller move the character to the patch selected by the meta-controller and cause him to jump around in the patch to generate an intrinsic reward. Figure 5b shows another sequence in which the meta-controller selects a patch over the ladder he needs to climb to collect the key. The character moves to the patch, but when the meta-controller then changes the location of the patch, the subcontroller ignores it and instead proceeds to collect the key, resulting in an extrinsic reward. This example highlights the importance of the subcontroller's motivation with extrinsic and intrinsic rewards, where the intrinsic reward musts on the intrinsic reward."}, {"heading": "4 Conclusion", "text": "In this paper, we have presented an approach to combating the problem of reward in the form of a two-modulated, hierarchical agent. In Montezuma's Revenge, an Atari game with particularly meagre rewards, our agent learns several times faster than current HRL agents and achieves a similar level of performance. We also show that our objectives are generally outdated by evaluating the agent on several levels. almost5This approach to visualizing attention traits is developed by Xu et al. (2015). It performs better than the basic agent, suggesting that the ability to control aspects of the environment plays a general overriding role."}, {"heading": "Acknowledgments", "text": "We would like to thank Marc Deisenroth for providing Azure credits from the Microsoft Azure scholarship program for teaching and research as well as Kyriacos Nikiforou, Hugh Salimbeni and Kai Arulkumaran for fruitful discussions. N.D. is supported by the DPST scholarship of the Thai government."}], "references": [{"title": "and X", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu"], "venue": "Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "The option-critic architecture", "author": ["P.-L. Bacon", "J. Harb", "D. Precup"], "venue": "AAAI Conference on Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems, 13(4):341\u2013379", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["M. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "Advances in Neural Information Processing Systems, pages 1471\u20131479", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Independently Controllable Features", "author": ["E. Bengio", "V. Thomas", "J. Pineau", "D. Precup", "Y. Bengio"], "venue": "arXiv preprint arXiv:1703.07718", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Openai gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": "arXiv preprint arXiv:1606.01540", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Feudal reinforcement learning", "author": ["P. Dayan", "G.E. Hinton"], "venue": "Proceedings of the 5th International Conference on Neural Information Processing Systems, pages 271\u2013278. Morgan Kaufmann Publishers Inc.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["T.G. Dietterich"], "venue": "Journal of Artificial Intelligence Research, 13:227\u2013303", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Variational Intrinsic Control", "author": ["K. Gregor", "D. Jimenez Rezende", "D. Wierstra"], "venue": "International Conference on Learning Representations Workshop", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8): 1735\u20131780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Vime: Variational information maximizing exploration", "author": ["R. Houthooft", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, pages 1109\u20131117", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Learning Representations", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Empowerment: A universal agent-centric measure of control", "author": ["A.S. Klyubin", "D. Polani", "C.L. Nehaniv"], "venue": "Evolutionary Computation, 2005. The 2005 IEEE Congress on, volume 1, pages 128\u2013135. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["G. Konidaris", "A.G. Barto"], "venue": "Advances in Neural Information Processing Systems, pages 1015\u20131023", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, pages 3675\u20133683", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["A. McGovern", "A.G. Barto"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, pages 361\u2013368. Morgan Kaufmann Publishers Inc.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Q-cut\u2014dynamic discovery of sub-goals in reinforcement learning", "author": ["I. Menache", "S. Mannor", "N. Shimkin"], "venue": "European Conference on Machine Learning, pages 295\u2013306. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning, pages 1928\u20131937", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["S. Mohamed", "D.J. Rezende"], "venue": "Advances in Neural Information Processing Systems, pages 2125\u2013 2133", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["G. Ostrovski", "M.G. Bellemare"], "venue": "van den Oord, and R. Munos. Count-based exploration with neural density models. In International Conference on Machine Learning", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["R. Parr", "S.J. Russell"], "venue": "Advances in Neural Information Processing Systems, pages 1043\u20131049", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 1991. 1991 IEEE International Joint Conference on, pages 1458\u20131463. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1991}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "International Conference on Learning Representations", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Using relative novelty to identify useful temporal abstractions in reinforcement learning", "author": ["\u00d6. \u015eim\u015fek", "A.G. Barto"], "venue": "Proceedings of the twenty-first international conference on Machine learning, page 95. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA, USA, 1st edition", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence, 112(1-2):181\u2013211", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Feudal networks for hierarchical reinforcement learning", "author": ["A.S. Vezhnevets", "S. Osindero", "T. Schaul", "N. Heess", "M. Jaderberg", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning to reinforcement learn", "author": ["J.X. Wang", "Z. Kurth-Nelson", "D. Tirumala", "H. Soyer", "J.Z. Leibo", "R. Munos", "C. Blundell", "D. Kumaran", "M. Botvinick"], "venue": "arXiv preprint arXiv:1611.05763", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048\u20132057", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "Reinforcement learning methods [28, 20] often struggle in environments where the rewards are sparsely encountered, and when their acquisition requires the coordination of temporally extended sequences of actions.", "startOffset": 31, "endOffset": 39}, {"referenceID": 19, "context": "Reinforcement learning methods [28, 20] often struggle in environments where the rewards are sparsely encountered, and when their acquisition requires the coordination of temporally extended sequences of actions.", "startOffset": 31, "endOffset": 39}, {"referenceID": 2, "context": "Hierarchical reinforcement learning (HRL) [3] is an approach that aims to deal with the reward sparsity problem by equipping the agent with temporally extended macro-actions, also known as options [29] or skills [16], which abstract over sequences of primitive actions.", "startOffset": 42, "endOffset": 45}, {"referenceID": 28, "context": "Hierarchical reinforcement learning (HRL) [3] is an approach that aims to deal with the reward sparsity problem by equipping the agent with temporally extended macro-actions, also known as options [29] or skills [16], which abstract over sequences of primitive actions.", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "Hierarchical reinforcement learning (HRL) [3] is an approach that aims to deal with the reward sparsity problem by equipping the agent with temporally extended macro-actions, also known as options [29] or skills [16], which abstract over sequences of primitive actions.", "startOffset": 212, "endOffset": 216}, {"referenceID": 2, "context": "However, learning useful options is a difficult task in itself; one possibility is to incorporate prior knowledge about the task into their construction [3, 24, 9] but this can limit the generalisability of the algorithm to other tasks.", "startOffset": 153, "endOffset": 163}, {"referenceID": 23, "context": "However, learning useful options is a difficult task in itself; one possibility is to incorporate prior knowledge about the task into their construction [3, 24, 9] but this can limit the generalisability of the algorithm to other tasks.", "startOffset": 153, "endOffset": 163}, {"referenceID": 8, "context": "However, learning useful options is a difficult task in itself; one possibility is to incorporate prior knowledge about the task into their construction [3, 24, 9] but this can limit the generalisability of the algorithm to other tasks.", "startOffset": 153, "endOffset": 163}, {"referenceID": 7, "context": "The architecture of our agent is inspired by feudal reinforcement learning [8, 30] and the hierarchical deep reinforcement learning framework [17], whereby a meta-controller provides embedded subgoals to a sub-controller that interacts directly with the environment (see Figure 1a).", "startOffset": 75, "endOffset": 82}, {"referenceID": 29, "context": "The architecture of our agent is inspired by feudal reinforcement learning [8, 30] and the hierarchical deep reinforcement learning framework [17], whereby a meta-controller provides embedded subgoals to a sub-controller that interacts directly with the environment (see Figure 1a).", "startOffset": 75, "endOffset": 82}, {"referenceID": 16, "context": "The architecture of our agent is inspired by feudal reinforcement learning [8, 30] and the hierarchical deep reinforcement learning framework [17], whereby a meta-controller provides embedded subgoals to a sub-controller that interacts directly with the environment (see Figure 1a).", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "By taking an existing idea of feature control [13, 6] and incorporating it into the subgoal design, we introduce a hierarchical agent with generically useful learnable options which we empirically evaluate in the Atari domain.", "startOffset": 46, "endOffset": 53}, {"referenceID": 5, "context": "By taking an existing idea of feature control [13, 6] and incorporating it into the subgoal design, we introduce a hierarchical agent with generically useful learnable options which we empirically evaluate in the Atari domain.", "startOffset": 46, "endOffset": 53}, {"referenceID": 14, "context": "introduced empowerment as an information theoretic measure of degrees of freedom that an agent has over an environment [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "The concept of empowerment has recently gained interest in the context of intrinsically motivated reinforcement learning [22, 10].", "startOffset": 121, "endOffset": 129}, {"referenceID": 9, "context": "The concept of empowerment has recently gained interest in the context of intrinsically motivated reinforcement learning [22, 10].", "startOffset": 121, "endOffset": 129}, {"referenceID": 24, "context": "In the other lines of work, intrinsic motivation is defined in the form of curiosity, which can be measured with model-learning progress [25, 12] or information gain [4].", "startOffset": 137, "endOffset": 145}, {"referenceID": 11, "context": "In the other lines of work, intrinsic motivation is defined in the form of curiosity, which can be measured with model-learning progress [25, 12] or information gain [4].", "startOffset": 137, "endOffset": 145}, {"referenceID": 3, "context": "In the other lines of work, intrinsic motivation is defined in the form of curiosity, which can be measured with model-learning progress [25, 12] or information gain [4].", "startOffset": 166, "endOffset": 169}, {"referenceID": 12, "context": "introduced the idea of off-policy training with auxiliary control tasks, such as pixel control or feature control, which can significantly speed up learning of the main task [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "[17] and Vezhnevets et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "There are a large number of works on subgoal discovery [27, 19, 18], most of which are based on finding bottleneck states.", "startOffset": 55, "endOffset": 67}, {"referenceID": 18, "context": "There are a large number of works on subgoal discovery [27, 19, 18], most of which are based on finding bottleneck states.", "startOffset": 55, "endOffset": 67}, {"referenceID": 17, "context": "There are a large number of works on subgoal discovery [27, 19, 18], most of which are based on finding bottleneck states.", "startOffset": 55, "endOffset": 67}, {"referenceID": 1, "context": "Option-Critic [2], which has moved towards end-to-end training where options and subgoals can automatically emerge from the optimisation of the system, with carefully designed architectures and objective functions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": ", we study the most basic form of controlling ability in the visual domain, which is the ability to control a given subset of pixels in the visual input [13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 5, "context": "introduced the notion of feature selectivity that measures how much a feature can be controlled, independently from other features [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 20, "context": "Our baseline model is a variant of the asynchronous advantage actor-critic algorithm (A3C) [21].", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The model consists of two parts: an encoding module and a Long-Short Term Memory layer (LSTM) [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "The value and policy networks are optimised using the A3C loss function with 8 asynchronous agents, where the advantage is estimated with the generalized advantage estimator [26], using \u03b3 = 0.", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": "We use the ADAM optimizer [14] with a learning rate of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "com/openai/universe-starter-agent) which is written with Tensorflow [1] The source code of our implementation will be made publicly available.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "We evaluated the model on Atari games in the OpenAI gym environment [7], a toolkit for comparing reinforcement learning algorithms that wraps the Arcade Learning Environment (ALE) [5] with a number of modifications.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "We evaluated the model on Atari games in the OpenAI gym environment [7], a toolkit for comparing reinforcement learning algorithms that wraps the Arcade Learning Environment (ALE) [5] with a number of modifications.", "startOffset": 180, "endOffset": 183}, {"referenceID": 29, "context": "In our experiments, we make comparisons with the Feudal Network (FuN) [30] and Option-Critic [2] architectures, both evaluated on the ALE.", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "In our experiments, we make comparisons with the Feudal Network (FuN) [30] and Option-Critic [2] architectures, both evaluated on the ALE.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "We also clip the extrinsic reward to the range of [-1, 1].", "startOffset": 50, "endOffset": 57}, {"referenceID": 1, "context": "The aim was to show that the method is applicable to a broad range of games, and to compare our system to two state-of-the-art end-to-end HRL systems, namely the Option-Critic [2] and FuN [30] architectures.", "startOffset": 176, "endOffset": 179}, {"referenceID": 29, "context": "The aim was to show that the method is applicable to a broad range of games, and to compare our system to two state-of-the-art end-to-end HRL systems, namely the Option-Critic [2] and FuN [30] architectures.", "startOffset": 188, "endOffset": 192}, {"referenceID": 29, "context": "For example, the discount parameter, \u03b3, has been shown to have a significant impact on the performances of both A3C and FuN on different Atari games [30].", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "We did not compare with other state-of-the-art results in Montezuma\u2019s Revenge, such as those obtained with the UNREAL agent [13], DQN-CTS [4] and DQN-PixelCNN [23], since these are not competing HRL methods and their advantageous features could easily be integrated into our agent.", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "We did not compare with other state-of-the-art results in Montezuma\u2019s Revenge, such as those obtained with the UNREAL agent [13], DQN-CTS [4] and DQN-PixelCNN [23], since these are not competing HRL methods and their advantageous features could easily be integrated into our agent.", "startOffset": 138, "endOffset": 141}, {"referenceID": 22, "context": "We did not compare with other state-of-the-art results in Montezuma\u2019s Revenge, such as those obtained with the UNREAL agent [13], DQN-CTS [4] and DQN-PixelCNN [23], since these are not competing HRL methods and their advantageous features could easily be integrated into our agent.", "startOffset": 159, "endOffset": 163}, {"referenceID": 31, "context": "(2015) [32].", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "By leaking some extrinsic reward to the sub-controller, it frees us from the restriction that subgoals need to be complete or carefully designed, which can lead to brittle or sub-optimal solutions [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 14, "context": "Additionally it would be interesting to quantify the extent to which our agents have learned to control their environment, perhaps by using the measure of empowerment [15].", "startOffset": 167, "endOffset": 171}], "year": 2017, "abstractText": "The problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning. Hierarchical reinforcement learning (HRL) tackles this problem by using a set of temporally-extended actions, or options, each of which has its own subgoal. These subgoals are normally handcrafted for specific tasks. Here, though, we introduce a generic class of subgoals with broad applicability in the visual domain. Underlying our approach (in common with work using \u201cauxiliary tasks\u201d) is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have. We incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the Atari suite. We highlight the advantage of our approach in one of the hardest games \u2013 Montezuma\u2019s revenge \u2013 for which the ability to handle sparse rewards is key. Our agent learns several times faster than the current state-of-the-art HRL agent in this game, reaching a similar level of performance.", "creator": "LaTeX with hyperref package"}}}