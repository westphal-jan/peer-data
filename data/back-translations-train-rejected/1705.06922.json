{"id": "1705.06922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Spectral-graph Based Classifications: Linear Regression for Classification and Normalized Radial Basis Function Network", "abstract": "Spectral graph theory has been widely applied in unsupervised and semi-supervised learning. In this paper, we find for the first time, to our knowledge, that it also plays a concrete role in supervised classification. It turns out that two classifiers are inherently related to the theory: linear regression for classification (LRC) and normalized radial basis function network (nRBFN), corresponding to linear and nonlinear kernel respectively. The spectral graph theory provides us with a new insight into a fundamental aspect of classification: the tradeoff between fitting error and overfitting risk. With the theory, ideal working conditions for LRC and nRBFN are presented, which ensure not only zero fitting error but also low overfitting risk. For quantitative analysis, two concepts, the fitting error and the spectral risk (indicating overfitting), have been defined. Their bounds for nRBFN and LRC are derived. A special result shows that the spectral risk of nRBFN is lower bounded by the number of classes and upper bounded by the size of radial basis. When the conditions are not met exactly, the classifiers will pursue the minimum fitting error, running into the risk of overfitting. It turns out that $\\ell_2$-norm regularization can be applied to control overfitting. Its effect is explored under the spectral context. It is found that the two terms in the $\\ell_2$-regularized objective are one-one correspondent to the fitting error and the spectral risk, revealing a tradeoff between the two quantities. Concerning practical performance, we devise a basis selection strategy to address the main problem hindering the applications of (n)RBFN. With the strategy, nRBFN is easy to implement yet flexible. Experiments on 14 benchmark data sets show the performance of nRBFN is comparable to that of SVM, whereas the parameter tuning of nRBFN is much easier, leading to reduction of model selection time.", "histories": [["v1", "Fri, 19 May 2017 10:35:37 GMT  (154kb)", "https://arxiv.org/abs/1705.06922v1", null], ["v2", "Tue, 13 Jun 2017 15:18:08 GMT  (154kb)", "http://arxiv.org/abs/1705.06922v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhenfang hu", "gang pan", "zhaohui wu"], "accepted": false, "id": "1705.06922"}, "pdf": {"name": "1705.06922.pdf", "metadata": {"source": "CRF", "title": "Spectral-graph Based Classifications: Linear Regression for Classification and Normalized Radial Basis Function Network", "authors": ["Zhenfang Hu", "Gang Pan", "Zhaohui Wu"], "emails": ["fancij@zju.edu.cn,", "gpan@zju.edu.cn,", "wzh@zju.edu.cn"], "sections": [{"heading": null, "text": "In this paper, we find for the first time that it also plays a concrete role in the supervised classification. It turns out that two classifiers are inherently related to the theory: linear regression for classification (LRC) and the normalized radial base function network (nRBFN), which corresponds to linear and nonlinear kernels. Spectral graph theory offers us a new insight into a fundamental aspect of classification: the compromise between adjusting errors and covering risks. Theory presents ideal working conditions for the LRC and nRBFN, which ensure that not only the error but also the low coverage of risks exists."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, while others are able to survive on their own."}, {"heading": "1.1 Our Work", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2 Spectral-graph Based Clustering: a Review", "text": "Faced with an undirected graph of n corners (data points), with the adjacency matrix defined as similarity matrix W, Rn, n, where S is a diagonal degree matrix, where the diagonal is the sum of the weight differences between data points, Wij = Wji \u2265 0, the laplactic eigenmatrix is defined as L. = S \u2212 W, where S is a diagonal degree matrix, where the diagonal is the sum of the weight differences between each vertex, i.e. S = diag (1TW). The laplactic eigenmatrix has the following properties [65].21 It is positively semidefinite.2. Vector 1 is always a eigenvalue vector. 3. Suppose there are K connected components in the graph, then the indicator vectors of these components (series vectors of F) can include the eigenvalue ero space."}, {"heading": "3 Spectral-graph Based Classifications", "text": "In spectral clustering, we extract the cluster information directly from the eigenvectors of the laplac matrix, i.e. we recover the indicator vectors from the eigenvectors. However, in classification, the indicator vectors are given. It turns out that we have to find the next components in the eigenspace in order to approximate the indicator vectors. If an ideal condition is met, the indicator vectors appear in the leading eigenspace, thereby achieving zero adjustment errors and a low overfitting risk."}, {"heading": "3.1 Linear Version: Linear Regression for Classification (LRC)", "text": "We will show that the singular vectors of the data matrix are the eigenvectors of a laplac matrix, thus establishing the link to the theory of spectral graphs. The laplac matrix is constructed by the inner product between the data, i.e. by a linear core. In the following, we will first present the basic formulation of the LRC, then analyze it from the line space view, which leads to the relationship to the theory of spectral graphs. Based on the theory, an ideal working condition for the LRC is presented. Finally, we will analyze the LRC from the column space view, which paves the way to nRBFN."}, {"heading": "3.1.1 Basic Formulation", "text": "Considering the data matrix A (mean removed, A1 = 0) and the corresponding class names, we convert the labels into an indicator matrix F and define an extended data matrix A = [\u221a \u03b21TA] [25], with \u03b2 being a constant scalar to be introduced later. The aim of the LRC is to find a weight matrix D so that the linear combinations of columns of D and samples come closer to the indicator vectors: 3min Dn \u0445 i = 1 \u0432Fi \u2212 DA \u0441i \u0432 22 = 1 \u0432Fi \u2212 210 F. (1) 2These properties are not divided by the similarity matrix. 3It corresponds to the classic LRC, in which \u03b2 = 1 [6], since \u03b2 can be included in the first column of D. In the implementation, we index \u03b2 = 1. Provided rank (A) = p + 1, there is a unique closed solution: D = FA = always B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B A B B B B B B B B B A B B B B B A B B A B B B A B B A B B A B B A B B B B B B A matrix matrix matrix A matrix A matrix A matrix A matrix A matrix A matrix A matrix A (mean matrix A)."}, {"heading": "3.1.2 Row-space View", "text": "It is shown that LRC must find the nearest components in the data row (ATA) in order to approximate the indicator vectors (A = S = S), and this refers to the spectral graph (A = S = S = S). (A = S = S = S = S). (A = S = S = S). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). A A A A A A A A A A A). (A A A A A). (A A). A). (A). (A) (A). (A). (A). (A) A). (A) (A). A) (A). (A). A). A) A (A) A). A (A) A). A (A). A) A (A). A) A) A (A) A A) A A A A A A A A A (A A A) A A A A A A A A A A A A A A A A A A A A A A A A (A A A A A A A A A A A A A A A A A A A (A A A) A A A A A A A A A A A A A A A A A (A A A A A (A A A A A A A A A A A A A A A A A) A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A"}, {"heading": "3.1.3 Column-space View", "text": "The facts are routine. With (4) we have the vector D-B = FV-T-V-B = 1 Fi (V-T i-V-B), where V-B = V-V-T-T-B is the normalized complete PCs of b-i and V-i are those of A-i. The mechanism of class prediction becomes clear: The class of a sample is determined by the voices of the training set, where the indicator vectors Fi's play the role of the voices, and the similarities between the training set and the sample serve as the weights assigned to the voices. Here, the similarity is measured by the inner product of the PCs. For general data, this is not a good choice, and this is one of the limitations of the LRC. We now introduce the stronger core version, which measures the similarity on the Euclidean distance."}, {"heading": "3.2 Kernel Version: Normalized RBF Network (nRBFN)", "text": "We will apply the kernel trick to LRC in two ways: one traditional way leads to RBFN, 7 the other way leads to nRBFN. The function matrix used by RBFN is the similarity matrix, while the one used by nRBFN is the normalized laplac matrix. Since the similarity matrix does not share the characteristics of the laplac matrix, we cannot analyze RBFN directly using the spectral graph theory, whereas linking nRBFN to the theory is simple. In the following subsection, we will introduce the routine base reduction to reduce the size of the networks. Afterwards, we will interpret nRBFN from the row and column space view and analyze it using the spectral graph theory. An ideal working condition related to base reduction will be introduced, and some properties of nRBFN will be shown."}, {"heading": "3.2.1 RBFN", "text": "We derive RBFN by applying the kernel trick to LRC. The solution of (1) can be rewritten asD * = (FA * T (A * A * T) \u2212 2A * T, which is a linear combination of training data. If we assume that (1) is converted to another objective matrix X * F \u2212 XA * T A * 2F. (6) If we apply the kernel trick to A * T, we get min X * F \u2212 XW * 2F. (7) W is a kernel matrix defined by a kernel function Wij = \u03c6 (Ai, Aj). In the case of LRC it is a linear function trick. (Ai, Aj) = A i + \u03b2. Among the non-linear properties, the Gaussian kernel is most commonly used when the matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix matrix matrix matrix matrix is used."}, {"heading": "3.2.2 nRBFN", "text": "We start the derivative with the kernel trick from another path that will lead to nRBFN. By absorbing \u03b2n intoX, (6) is equivalent to tomin X-F-XA-T-A (\u03b2n) -1-2F = min X-F-XA-T A-A-diag (1T A-T A-A-A) -1-2F. (9) 7There is another standard path based on the reproducing kernel Hilbert trick that leads to a kernel classifier, see e.g. [53], but it cannot lead to nRBFN. 8Compared with some traditional RBFNs, the RBFN here, derived from the kernel trick, is no constant vector 1T to W and a bias vector to X. Note that A-T A-diag (1T A-T-A-S-A) -1 is the normalized solution of WS-W \u2212 x."}, {"heading": "3.2.3 Basis Reduction", "text": "The basics of RBFN and nRBFN consist of the entire training set, which will result in expensive calculations. Traditionally, a base reduction is applied [52]. A smaller base is chosen through a strategy (discussed in Section 6). At the moment, we are starting from the basis G = [G1,.., Gr], r < n. Now, W is from the size r \u00b7 n, andWij =? (Gi, Aj). DenotingW. = WS \u2212 1, Rr \u00b7 n, the formulation of base-reduced nRBFN results from the formulation of base-reduced nRBFN."}, {"heading": "3.2.4 Row-space View and Column-space View to nRBFN", "text": "In fact, it is not that it is a mere diversionary manoeuvre, but rather a diversionary manoeuvre, which is a matter of pushing oneself to the fore. (...) It is not as if it is a diversionary manoeuvre. (...) It is a diversionary manoeuvre, which is able to put oneself to the fore. (...) It is a diversionary manoeuvre, which is able to put oneself to the centre. (...) It is a diversionary manoeuvre, which is able to put oneself to the centre. (...) It is a diversionary manoeuvre, which is able to put oneself to the centre. (...) (...) () (...) (() () (...) () (...) () () (() () () (()) (()) ((()) (()) (()) ((())) ((())) (((())) ((())) ((()) (()) (())) (() ()) (()) () () () () () ()) () () () () () () () () ()) () () () () () ()) () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () (() () () () () () (() () () () (() () () (() () () (() (() ((() () () () () () () () ((() () () (() () (() () () (() (() () ((()) (((() ())) () ((("}, {"heading": "4 Regularization for LRC and nRBFN", "text": "In practice, the ideal graph conditions are not easy to accurately fulfill. In this case, it may be that no adjustment error is achieved and some of the closest components may be in the small subspace, leading to an increase in overfitting risk. In this section, we present the traditional regulation of the 2 standards and show their qualitative effects on overfitting risk control from a spectral viewpoint. Regulated versions of LRC and nRBFN are what we will really apply in the real world."}, {"heading": "4.1 Regularized LRC", "text": "For LRC, the ideal graph condition is difficult to meet, except perhaps for high-dimensional data: By constructing W, the condition essentially requires that, after translation along a new dimension, different classes become orthogonal. Thus, in general, the leading line subspace of A \u00b2 may differ greatly from F. LRC then searches the entire line space to find a subspace closest to approximateF. The subspace found corresponds to small singular values that may represent discriminating characteristics or, more often, disturbances (e.g., due to insufficient sample size). In other words, LRC may consider the noise components of the data as discriminating characteristics for classification. Poor generalization can be expected. In this regard, we would encourage LRC to search within the main line subspace. This can be achieved by arranging T."}, {"heading": "4.2 Regularized nRBFN", "text": "The ideal graph condition for nRBFN does not require orthogonality between classes. However, if the condition is not exactly met to prevent nRBFN from searching for the next components in the small subspace, we introduce a regularization (the analysis follows LRC, and we omit it): min X-F \u2212 XW-X-2F + \u03bb-X-2F, (15) where \u03bb-W-2F and \u03bb > 0 is a scalar weight. The solution is X-Z = FW-T (W-W-T + \u03bb-I) \u2212 1, regardless of the rank of W-1. The name of a sample b is specified as (13)."}, {"heading": "5 Error and Risk Analysis", "text": "The trade-off between adjustment error and overpass risk is an important problem of classification. In this section, we analyze this problem quantitatively for LRC and nRBFN from a spectral point of view, and further demonstrate the impact of regulation through adjustment regulation. First, we define a quantitative criterion, spectral risk, to measure overpass risk, and the adjustment error is also formally defined. Next, we analyze unregulated regulation by nRBFN and LRC in predominantly ideal cases, and derive the limits of the two quantities. Finally, we examine regulation through adjustment (regardless of the ideal graph condition). We will show that the two concepts of adjustment target and spectral risk correspond one to the adjustment error and spectral risk."}, {"heading": "5.1 Definitions of Spectral Risk and Fitting Error", "text": "The definitions apply to linear regression, including LRC and nRBFN as special cases. We will define an absolute measure and relative measure of spectral risk and adjustment errors, the reasons for which will be clear later. Relative measures are used as standard definitions. Let's formulate the linear regression problem as asmin D, F, DA, 2F, (16), where F, RK, n is any target matrix that is not limited to indicators, A, Rr, n (r, \u2264 n) is any data matrix of complete rank. The solution is D * = FAT (AAT) \u2212 1. To exclude meaningless cases, we assume D * 6 = 0, which means that the data is not orthogonal to the target."}, {"heading": "5.1.1 Spectral Risk", "text": "Definition 4. (Absolute Spectral Risk) The absolute spectral risk is defined as \u03b1. (17) However, the justification can be understood by the following spectral expression. Suppose the SVD of A is A = UV, then for problem (16), D \u0445 = FV T\u03a3 \u2212 1UT, and we have Proposition 6. For linear regression problem (16), \u03b1 = r \u2211 i = 1a2i \u03c32i, (18) where a2i is the projection of the target on Vi: a 2 i. = Kk = 1 (FkV T i) 2, and Fk is the lowest row of F. It implies that if the projections focus on the leading singular vectors, the closest components are located in the main subreed. Conversely, if they are concentrated in the main subreed, we will measure the size."}, {"heading": "5.1.2 Fitting Error", "text": "First of all, it should be made clear that the adjustment error is different from the error rate of the classification. (22) For problem (16), we have f = VP F-2F-D-A-2F = n-2 i. For the reason that will be clear later, we define the relative measure as: Definition 7. (adjustment error) The relative adjustment error, simply called adjustment error, is defined as follows: = VP F-D-A-2F-D-A-2F + 1. (23) For problem (16), the definition is equivalent to: Definition 7. (adjustment error) The relative adjustment error, simply called adjustment error, is defined as follows."}, {"heading": "5.2 Error and Risk Bounds of nRBFN", "text": "The ideal graph condition ensures zero fit errors and a low overfit risk. We calculate the specific limits for nRBFN (unregulated) and then extend to the case of failure where the condition is not exactly met. We assume FW-T 6 = 0, i.e. X-6 = 0."}, {"heading": "5.2.1 Ideal Case", "text": "We have the following result: Theorem 10. For nRBFN problem (12), if the ideal graph condition is met, the fit error reaches the minimum value, and the spectral risk has the limits srnK = 1nk rk. (24) The maximum risk is reached if there is only one unequal entry in each column of W. The minimum risk is addressed if the entries in each column array are uniformly distributed within the corresponding class, it is reached if and only if the maximum risk is achieved for all k.Proof. The maximum risk is reached if the entries in each column array are uniformly distributed within the corresponding class, it is achieved if and only if rk = 1 for all k.Proof. By theorem 3, if the condition is maintained, perfect reconstruction is achieved, i.e., X, W = F, so that X, W = 2F. \""}, {"heading": "5.2.2 Perturbation Case", "text": "(D) D (W) D (W) D (W) D (W) D (W) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (W) D (W) D (W) D) D (W) D (W) D (W) D (W) D (W) D (W) D (W) D (W) D \"D\" D \"D\" D D D \"D D D\" D \"D D\" D D D \"D D D D\" D D (W) D) D (W) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D) D (W) D (W) D (W) D) D (W) D) D (W) D (W) D) (W) D) D (W) D) (W) D (W) D) D (W) D) (W) D) (W) D) D (W) D) (W) D (W) D) D (W) D) D (W) (W) D) (W) D) D (W) D) (W) D) (W) (W) D) D (W) D) (W) D) (W) (W) D) D) (W) D (W) (W) (W) D) D) (W) (W) D) (W) D) (W) D) (W) (W) (W) D) (W) D) D) (W) ("}, {"heading": "5.3 Error and Risk Bounds of LRC", "text": "8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "5.4 Effect of \u21132-norm Regularization", "text": "First, we expand the previous definitions of fit error and spectral risk on regularized linear regression, then we show the one-one correspondence between the two quantities and the regularized objective, and finally we examine the effects of regularized regression. The previous four definitions are trivially extended by replacing the new D-One problem (42), the spectral expressions are changed to: Proposition 16. For the regularized linear regression problem (42), where it cannot be executed. (44) The previous four definitions are trivially expanded by replacing the new D-One of the problem (42), and the spectral expressions are changed to: Proposition 16. For the regularized linear regression problem (42)."}, {"heading": "6 Basis Selection Strategy", "text": "This year it is more than ever before."}, {"heading": "7 Experiments", "text": "The experiments consist of two parts: the demonstration of the performance of nRBFN and the evaluation of the error and risk of nRBFN and LRC. Experiments are performed using a series of benchmark datasets presented in Table 1.10. The datasets include classic small datasets from UCIMachine Learning Repository [35]: iris, wdbc, glass, sonar, wine; high-dimensional and smallest gene data: colon, leukemia; human facial images: ORL, AR, YaleB; high-dimensional and large-scale text data: TDT2, 20news; and large-scale handwritten digital images: USPS, MNIST. If the original dataset does not have a training and test gap, we use the first half of each class as a training set / large-scale text data: TDT2, 20news; and large-scale handwriting digital images: NSPUT, LRC."}, {"heading": "7.1 Performance of nRBFN", "text": "We first evaluate the parameters of nRBFN and then compare the performance of nRBFN with some other algorithms."}, {"heading": "7.1.1 Evaluation of the Parameters", "text": "We evaluate the influence of the three parameters of nRBFN, \u03bb, t and k on the classification performance. The results of five representative datasets are presented in Figure 1. Three default values \u03bb = 10 \u2212 13, t = 0.9 and k = 20. If one parameter varies, the others are fixed with the default values. We find the following: (1) The test error generally decreases with increasing regulation and reaches a plateau after 10 \u2212 8. The exception is the iris, where due to insufficient sampling, a larger \u03bb is required to avoid noisy subspaces. However, the difference is not large. If methods using regulatory measures are generally plagued with the problem of adjusting the regulatory weight, nRBFN shows a desirable feature: as a rule of thumb, an essential determination of the series, \u03bb < 10 \u2212 8 usually provides approximately optimal results of nRFN. Our experience has shown that other data also applies to us."}, {"heading": "7.1.2 nRBFN v.s. Other Classification Algorithms", "text": "In this sub-section, we compare the classification performance of the nRBFN with some other algorithms, the results of which are shown in Table 1. the algorithms involved include: (1) KNN (k = 20), (2) LRC (2) (a classic incremental learning method for basic selection over 10 {-13, \u2212 12, \u2212 2}), (3) ROLS (regulates orthogonally least square algorithms for RBFN) [12] (a classic incremental learning method for basic selection is selected using the same scheme as nRBFN, and the basic quantities are provided by nRBFN), (4) RBFNFNl (RBFN by Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering-Gaussian mixing model, without regulation, is the breadth of the Gaussian mixtures."}, {"heading": "7.1.3 nRBFN v.s. SVM with the Same Basis Size", "text": "Next, to allow a fair comparison between nRBFN and SVM, we let the base size of nRBFN correspond to the size of support vectors by selecting the specific number of samples with the lowest confidence as a basis. Other parameters are set as before. Results are in Table 3. In this test, nRBFN performs even better, and the time cost is comparable to SVM. Note that the time cost does not include the part of cross-validation. SVM usually needs to be run hundreds of times during cross-validation, while nRBFN is run only a dozen times."}, {"heading": "7.1.4 nRBFN with fixed parameters v.s. SVM", "text": "Finally, we test the performance of nRBFN with a fixed set of parameters (\u03bb = 10 \u2212 13, t = 0.9, k = 20). In this case, nRBFN does not include a model selection, but the results are still not far from those of the SVM, as shown in Table 4."}, {"heading": "7.2 Empirical Evaluation of Error and Risk", "text": "In this section we evaluate empirically the adjustment error and spectral risk of nRBFN and LRC and examine their influence on performance."}, {"heading": "7.2.1 The Effect of Regularization: nRNwr v.s. nRBFN", "text": "The results are presented in Table 5. Due to a rank (W) < r, the results of nRNwr on glass, wine, TDT2 and 20news are incorrect and are not presented. According to the table, regulation on the bold labeled sets significantly reduces the spectral risk, usually by orders of magnitude, while the adjustment error is slightly increased but not yet sacrificed much. In all of these sets, the test error has been reduced, demonstrating the effectiveness of regulation. In the other sets, regulation does not affect much. As for optimal performance, this implies that the row subspaces found on these sets by nRNwr are likely to contain discriminatory features and not noise. Note that most of these data sets have large samples, suggesting that these results are sufficient, although they are not sufficient to undermine the effectiveness of regulation."}, {"heading": "7.2.2 LRC v.s. nRBFN", "text": "The results are given in Table 6.Theoretically, this can be controlled by t. In the extreme case of the full scale, the zero error can be reached (although the spectral risk is not guaranteed).The other factor is the ideal graph condition that the rank is lower than the condition that the condition can be low as long as the condition is almost met, both the condition of the condition and the spectral risk is low. We analyze Table 6 by two parts. For the data sets from the colon to AR, LRRC leads equally or better than nRBFN."}, {"heading": "7.2.3 How Error and Risk of nRBFN Change as \u03bb and t Vary", "text": "The setting is the same as Section 7.1.1, now we focus on the adjustment error and the spectral risk. The results are shown in Figure 2 and Figure 3.We note from Figure 2 that: (a) As \u03bb decreases, the adjustment error decreases steadily. This is as expected. Convergences confirm the law of \u03bb: as a rule of thumb, the determination of \u03bb = 10 \u2212 8 usually yields an almost optimal result of nRBFN. (b) For most data sets, according to \u03bb < 10 \u2212 12, there is a relatively stable convergence range of spectral risk: 108 \u0445 10. This also applies to most other data sets that are not shown, especially large data sets. The cases of the two gene data are abnormal, they exhibit a low spectral risk."}, {"heading": "8 Related Work", "text": "We discuss the related work of nRBFN, regularization and generalization error. 1. nRBFN. nRBFN is first mentioned by [39] and later derived by [59, 60, 67] from the estimation of probability density and kernel regression. It is also closely related to the Gaussian mixing model [63]. Comparison with RBFN, nRBFN has a characteristic feature: in regions far away from the samples, the output of RBFN disappears due to the localized property of the basic radial function, while that of nRBFN does not due to normalization [9]. Consequently, nRBFN provides a better smoothness. Meanwhile, the universal approximation capacity remains [67, 4]. However, the connection of nRBFN to spectral graph theory is of no help. 2. Regularization. The standpoint for the regularization is the probability."}, {"heading": "9 Future Work", "text": "With respect to the performance improvements of nRBFN: (1) The relationship between the ideal graph condition and base selection has not yet been investigated. Our motivation to develop the base selection strategy and parameter replacement scheme is to demonstrate the practical performance of nRBFN and to provide a user-friendly base algorithm. (3) Finding the optimal basis that is consistent with the theory is an important direction for future work. (4) Online base learning can be considered [49], where the basis can be increased, updated or pruned to correct the bottleneck in speed improvement. (n) The basis can be further reduced, as there may be many boundaries that overlap greatly. (4) Online base learning can be considered [49], where the basis can be increased, updated or pruned. It will allow RBBFN to handle the data on a large scale, if a better comparability of the FBFN can be found."}, {"heading": "A The Gap between F and the Leading Row-subspace of W\u0303", "text": "It is not as if these are the largest single vectors of each block. It is a uniform row vector of W, f is a uniform row vector of W, f is a uniform row vector of 1. The deviation can be measured with the idea of the 2 operator norm [23]: 0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-"}], "references": [{"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["Michal Aharon", "Michael Elad", "Alfred Bruckstein"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "On functional approximation with normalized gaussian units", "author": ["M Benaim"], "venue": "Neural Computation, 6(2):319\u2013 333", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Improving the generalization properties of radial basis function neural networks", "author": ["Christopher M Bishop"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Theoretical interpretations and applications of radial basis function networks", "author": ["Enrico Blanzieri"], "venue": "Technical Report No. DIT-03-023,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Multivariable functional interpolation and adaptive networks", "author": ["D. S Broomhead", "David Lowe"], "venue": "Complex Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Normalized gaussian radial basis function", "author": ["Guido Bugmann"], "venue": "networks. Neurocomputing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["Pak K Chan", "Martine DF Schlag", "Jason Y Zien"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih Chung Chang", "Chih Jen Lin"], "venue": "Acm Transactions on Intelligent Systems and Technology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Regularized orthogonal least squares algorithm for constructing radial basis function networks", "author": ["S. Chen", "E.S. Chng", "K. Alkadhimi"], "venue": "International Journal of Control, 64(5):829\u2013837", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Orthogonal least squares learning algorithm for radial basis function networks", "author": ["S Chen", "C.F.N Cowan", "P.M Grant"], "venue": "IEEE Transactions on Neural Networks, 2(2):302 \u2013 309", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Spectral Graph Theory", "author": ["Fan RK Chung"], "venue": "American Mathematical Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Multidimensional scaling", "author": ["Trevor F Cox", "Michael AA Cox"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Neural Networks and Statistical Learning", "author": ["Kelin Du", "M.N.S. Swamy"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing", "author": ["Michael Elad"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["Ehsan Elhamifar", "Rene Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S.O. Aase", "J. Hakon Husoy"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 5, pages 2443\u20132446", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Do we need hundreds of classifiers to solve real world classification problems", "author": ["Manuel Fern\u00e1ndez-Delgado", "Eva Cernadas", "Sen\u00e1n Barro", "Dinani Amorim"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Locality preserving projections", "author": ["Xiaofei He", "ParthaNiyogi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Spectral sparse representation for clustering: Evolved from pca, k-means, laplacian eigenmap, and ratio cut", "author": ["Zhenfang Hu", "Gang Pan", "Yueming Wang", "Zhaohui Wu"], "venue": "Eprint Arxiv,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "An efficient sequential learning algorithm for growing and pruning rbf (gap-rbf) networks", "author": ["Guang-Bin Huang", "P Saratchandran", "Narasimhan Sundararajan"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Principal Component Analysis", "author": ["IT Jolliffe"], "venue": "Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Reformulated radial basis neural networks trained by gradient descent", "author": ["N.B. Karayiannis"], "venue": "IEEE Transactions on Neural Networks, 10(3):657\u2013671", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "AKrzy\u017cak", "author": ["B Kegl"], "venue": "and HNiemann. Radial basis function networks and complexity regularization in function learning and classification. In International Conference on Pattern Recognition, pages 81\u201386", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Radial basis function networks and complexity regularization in function learning", "author": ["A Krzy\u017cak", "T Linder"], "venue": "IEEE Transactions on Neural Networks, 9(2):247\u2013256", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonparametric estimation and classification using radial basis function nets and empirical risk minimization", "author": ["A Krzy\u017cak", "T Linder", "C Lugosi"], "venue": "IEEE Transactions on Neural Networks, 7(2):475\u201387", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}, {"title": "Semi-supervised graph clustering: a kernel approach", "author": ["Brian Kulis", "Sugato Basu", "Inderjit Dhillon", "Raymond Mooney"], "venue": "In International conference on Machine learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["Wei Liu", "Junfeng He", "Shih Fu Chang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["James MacQueen"], "venue": "In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1967}, {"title": "Fast learning in networks of locally-tuned processing units", "author": ["John Moody", "Christian J. Darken"], "venue": "Neural Computation,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1989}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["Marius Muja", "David G. Lowe"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "On the training of radial basis function classifiers", "author": ["M.T. Musavi", "W. Ahmed", "K.H. Chan", "K.B. Faris", "D.M. Hummels"], "venue": "Neural Networks, 5(05):595\u2013603", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1992}, {"title": "Netlab: Algorithms for Pattern Recognition", "author": ["Ian T. Nabney"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2003}, {"title": "On spectral clustering: analysis and an algorithm", "author": ["AndrewYNg", "Michael I Jordan", "Yair Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "On the relationship between generalization error", "author": ["P Niyogi", "F Girosi"], "venue": "hypothesis complexity, and sample complexity for radial basis functions. Neural Computation, 8(4):819\u2013842", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1996}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Regularization in the selection of radial basis function centers", "author": ["Mark J.L. Orr"], "venue": "Neural Computation,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1995}, {"title": "Data classification with radial basis function networks based on a novel kernel density estimation algorithm", "author": ["Yen Jen Oyang", "Shien Ching Hwang", "Yu Yen Ou", "Chien Yu Chen", "Zhi Wei Chen"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Universal approximation using radial-basis-function networks", "author": ["J Park", "I Sandberg"], "venue": "Neural Computation, 3(2):246\u2013257", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1991}, {"title": "A resource-allocating network for function interpolation", "author": ["John Platt"], "venue": "Neural Computation,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1991}, {"title": "Networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Proceedings of the IEEE, 78(9):1481\u2013 1497", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1990}, {"title": "Radial basis functions for multivariable interpolation: a review", "author": ["M.J.D. Powell"], "venue": "Algorithms for Approximation, pages 143\u2013167", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1987}, {"title": "Back to the future: radial basis function networks revisited", "author": ["Qichao Que", "Mikhail Belkin"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2016}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2000}, {"title": "Comparing support vector machines with gaussian kernels to radial basis function classifiers", "author": ["B. Sch\u00f6lkopf", "K. Sung", "C. Burges", "F. Girosi", "P. Niyogi", "T. Poggio", "V. Vapnik"], "venue": "IEEE Transactions on Signal Processing, 45(11):2758\u20132765", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["Bernhard Sch\u00f6lkopf", "Alexander Smola", "Klaus-Robert M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1998}, {"title": "Three learning phases for radial-basis-function networks", "author": ["Friedhelm Schwenker", "Hans A. Kestler", "G\u00fcnther Palm"], "venue": "Neural Networks,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2001}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2000}, {"title": "Probabilistic neural networks", "author": ["Donald F. Specht"], "venue": "Neural Networks,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1990}, {"title": "A general regression neural network", "author": ["Donald F. Specht"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1991}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "Ji Guang Sun"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1990}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2000}, {"title": "Network structuring and training using rule-based knowledge", "author": ["Volker Tresp", "J\u00fcrgen Hollatz", "Subutai Ahmad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1993}, {"title": "The Nature of Statistical Learning", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2000}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2007}, {"title": "Fast and efficient second-order method for training radial basis function networks", "author": ["Tiantian Xie", "Yu Hao", "J. Hewlett", "P. Rozycki", "B. Wilamowski"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "On radial basis function nets and kernel regression: Statistical consistency, convergence rates, and receptive field size", "author": ["Lei Xu", "Adam Krzy\u017cak", "Alan Yuille"], "venue": "Neural Networks,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1994}, {"title": "Least squares linear discriminant analysis", "author": ["Jieping Ye"], "venue": "International Conference on Machine Learning,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2007}, {"title": "An incremental design of radial basis function networks", "author": ["H. Yu", "P.D. Reiner", "T. Xie", "T Bartczak", "B.M. Wilamowski"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 25(10):1793\u20131803", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with local and global consistency", "author": ["Dengyong Zhou", "Olivier Bousquet", "Thomas Navin Lal", "Jason Weston", "Bernhard Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2004}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": "Computer Science,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2008}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John D. Lafferty"], "venue": "International Conference on Machine Learning,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2003}], "referenceMentions": [{"referenceID": 13, "context": "Spectral graph theory is a theory that centers around the graph Laplacian matrix [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 59, "context": "The theory has found wide applications in unsupervised learning, including clustering [65] (generally named spectral clustering, including, e.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": ", ratio cut (Rcut) [10] and normalized cut (Ncut) [58, 43]), and dimensionality reduction (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 52, "context": ", ratio cut (Rcut) [10] and normalized cut (Ncut) [58, 43]), and dimensionality reduction (e.", "startOffset": 50, "endOffset": 58}, {"referenceID": 37, "context": ", ratio cut (Rcut) [10] and normalized cut (Ncut) [58, 43]), and dimensionality reduction (e.", "startOffset": 50, "endOffset": 58}, {"referenceID": 1, "context": ", Laplacian eigenmap (LE) [2] and locality preserving projections (LPP) [24]).", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": ", Laplacian eigenmap (LE) [2] and locality preserving projections (LPP) [24]).", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 107, "endOffset": 115}, {"referenceID": 66, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 64, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 2, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 65, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 22, "context": "Recently it has been discovered that, in the scope of unsupervised learning, spectral graph theory unifies a series of elementary methods of machine learning into a complete framework [25].", "startOffset": 184, "endOffset": 188}, {"referenceID": 24, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "also incorporates extended relations to conventional over-complete sparse representations [19], e.", "startOffset": 90, "endOffset": 94}, {"referenceID": 39, "context": ", [45], method of optimal directions (MOD) [21], KSVD [1]; manifold learning, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [45], method of optimal directions (MOD) [21], KSVD [1]; manifold learning, e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": ", [45], method of optimal directions (MOD) [21], KSVD [1]; manifold learning, e.", "startOffset": 54, "endOffset": 57}, {"referenceID": 50, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 50, "endOffset": 54}, {"referenceID": 56, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": ", sparse subspace clustering (SSC) [20], low-rank representation (LRR) [36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": ", sparse subspace clustering (SSC) [20], low-rank representation (LRR) [36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": ", [6]), and when used for classification (LRC), its link to linear discriminant analysis was already discovered [68].", "startOffset": 2, "endOffset": 5}, {"referenceID": 62, "context": ", [6]), and when used for classification (LRC), its link to linear discriminant analysis was already discovered [68].", "startOffset": 112, "endOffset": 116}, {"referenceID": 45, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 5, "endOffset": 16}, {"referenceID": 7, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 5, "endOffset": 16}, {"referenceID": 33, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 5, "endOffset": 16}, {"referenceID": 33, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 52, "endOffset": 60}, {"referenceID": 54, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 52, "endOffset": 60}, {"referenceID": 42, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 178, "endOffset": 189}, {"referenceID": 61, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 178, "endOffset": 189}, {"referenceID": 3, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 178, "endOffset": 189}, {"referenceID": 6, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 41, "endOffset": 44}, {"referenceID": 44, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 105, "endOffset": 109}, {"referenceID": 54, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 165, "endOffset": 173}, {"referenceID": 61, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 165, "endOffset": 173}, {"referenceID": 47, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 41, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 19, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 46, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 19, "context": "Especially, in a comprehensive evaluation involving 179 classifiers and 121 data sets by [22], RBFN ranks third, immediately following SVM.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 40, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 49, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 41, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 46, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 59, "context": "The Laplacian matrix has the following properties [65].", "startOffset": 50, "endOffset": 54}, {"referenceID": 59, "context": "These properties are exploited for clustering purpose [65].", "startOffset": 54, "endOffset": 58}, {"referenceID": 22, "context": "Assume we are to findK clusters, if the ideal graph condition for clustering (Definition 1) [25] holds (the condition implies the between-cluster weights are all zero: Wij = 0, if the ith and jth points are of different clusters), then we can compute the K eigenvectors of L with the smallest eigenvalues (zero), and then postprocess these eigenvectors to finish clustering.", "startOffset": 92, "endOffset": 96}, {"referenceID": 59, "context": "Finally, the eigenvectors of Lwith eigenvalue zero (smallest) are the eigenvectors of S\u22121W , called normalized Laplacian matrix, with eigenvalue one (largest) [65].", "startOffset": 159, "endOffset": 163}, {"referenceID": 22, "context": "[25], where \u03b2 is a constant scalar that will be introduced later.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "It is equivalent to the classical LRC where \u03b2 = 1 [6], since \u221a \u03b2 can be absorbed into the first column of D.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "Provided rank(\u00c3) = p+1, there is a unique closed-form solution: D\u2217 = F\u00c3 (\u00c3\u00c3 )\u22121 [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 22, "context": "Since the data is mean-removed, we have minij (A A)ij < 0, 5 \u03b2 thus defined makes W become nonnegative [25].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Assume the thin SVD [23] of A to be U\u03a3V , where the singular values are arranged in descending order, then [25] \u00c3 = \u0168 \u03a3\u0303\u1e7c = [", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "Assume the thin SVD [23] of A to be U\u03a3V , where the singular values are arranged in descending order, then [25] \u00c3 = \u0168 \u03a3\u0303\u1e7c = [", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "Further, by L = n\u03b2I \u2212 \u00c3 \u00c3, we obtain the spectral decomposition [23] of L:", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "Note that the condition ensures not only perfect reconstruction but also that the target lies in the principal row-subspace of data, or principal components (PCs) in the language of PCA [28].", "startOffset": 186, "endOffset": 190}, {"referenceID": 45, "context": "(7) is an RBFN [51, 8, 39], where the kernel function is viewed as radial basis function \u03c6(\u2016Gi \u2212 x\u2016):8 each column of W corresponds to a sample x, while each row a basis vector Gi.", "startOffset": 15, "endOffset": 26}, {"referenceID": 7, "context": "(7) is an RBFN [51, 8, 39], where the kernel function is viewed as radial basis function \u03c6(\u2016Gi \u2212 x\u2016):8 each column of W corresponds to a sample x, while each row a basis vector Gi.", "startOffset": 15, "endOffset": 26}, {"referenceID": 33, "context": "(7) is an RBFN [51, 8, 39], where the kernel function is viewed as radial basis function \u03c6(\u2016Gi \u2212 x\u2016):8 each column of W corresponds to a sample x, while each row a basis vector Gi.", "startOffset": 15, "endOffset": 26}, {"referenceID": 47, "context": ", [53], but it cannot lead to nRBFN.", "startOffset": 2, "endOffset": 6}, {"referenceID": 33, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 33, "endOffset": 37}, {"referenceID": 53, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 117, "endOffset": 129}, {"referenceID": 54, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 117, "endOffset": 129}, {"referenceID": 61, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 117, "endOffset": 129}, {"referenceID": 57, "context": "It is also closely related to Gaussian mixture model [63].", "startOffset": 53, "endOffset": 57}, {"referenceID": 46, "context": "Traditionally, basis reduction is applied [52].", "startOffset": 42, "endOffset": 46}, {"referenceID": 58, "context": "In addition, the size of basis also makes us recall the VC-dimension [64]\u2013a classical measure of model complexity.", "startOffset": 69, "endOffset": 73}, {"referenceID": 55, "context": "\u03be < 1 is a condition frequently appeared in the perturbation analysis of linear system [61].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 8, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 23, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 63, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 44, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 83, "endOffset": 95}, {"referenceID": 25, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 83, "endOffset": 95}, {"referenceID": 60, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 83, "endOffset": 95}, {"referenceID": 33, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 4, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 35, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 46, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 33, "context": "The Gaussian width can be set via some heuristics [39, 57, 18], e.", "startOffset": 50, "endOffset": 62}, {"referenceID": 51, "context": "The Gaussian width can be set via some heuristics [39, 57, 18], e.", "startOffset": 50, "endOffset": 62}, {"referenceID": 15, "context": "The Gaussian width can be set via some heuristics [39, 57, 18], e.", "startOffset": 50, "endOffset": 62}, {"referenceID": 53, "context": "It was reported that the performance is not so sensitive to this parameter [59], especially for nRBFN [9].", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "It was reported that the performance is not so sensitive to this parameter [59], especially for nRBFN [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 49, "context": "It is this view that leads to the clustering heuristics [55].", "startOffset": 56, "endOffset": 60}, {"referenceID": 49, "context": "This has been investigated by [55, 9, 47].", "startOffset": 30, "endOffset": 41}, {"referenceID": 8, "context": "This has been investigated by [55, 9, 47].", "startOffset": 30, "endOffset": 41}, {"referenceID": 41, "context": "This has been investigated by [55, 9, 47].", "startOffset": 30, "endOffset": 41}, {"referenceID": 11, "context": ",\u22122}), (3) ROLS (regularized orthogonal least squares algorithm for RBFN) [12] (a classical incremental learning method for basis selection, \u03bb is selected using the same scheme as nRBFN, \u03c3 and basis size are provided by nRBFN), (4) RBFNnl (RBFN from Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering\u2013gaussian mixture model, without regularization, the width of Gaussian is set as the maximum distance between the basis vectors, bias parameters are included, basis size follows nRBFN), (5) nRNwr (nRBFN without regularization, using the same basis as nRBFN), (6) nRNrb (nRBFN with basis chosen randomly from the training set, basis size follows nRBFN), (7) SVM [11] (Gaussian kernel, the weight C is selected over 2{\u22121,0,.", "startOffset": 74, "endOffset": 78}, {"referenceID": 36, "context": ",\u22122}), (3) ROLS (regularized orthogonal least squares algorithm for RBFN) [12] (a classical incremental learning method for basis selection, \u03bb is selected using the same scheme as nRBFN, \u03c3 and basis size are provided by nRBFN), (4) RBFNnl (RBFN from Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering\u2013gaussian mixture model, without regularization, the width of Gaussian is set as the maximum distance between the basis vectors, bias parameters are included, basis size follows nRBFN), (5) nRNwr (nRBFN without regularization, using the same basis as nRBFN), (6) nRNrb (nRBFN with basis chosen randomly from the training set, basis size follows nRBFN), (7) SVM [11] (Gaussian kernel, the weight C is selected over 2{\u22121,0,.", "startOffset": 266, "endOffset": 270}, {"referenceID": 10, "context": ",\u22122}), (3) ROLS (regularized orthogonal least squares algorithm for RBFN) [12] (a classical incremental learning method for basis selection, \u03bb is selected using the same scheme as nRBFN, \u03c3 and basis size are provided by nRBFN), (4) RBFNnl (RBFN from Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering\u2013gaussian mixture model, without regularization, the width of Gaussian is set as the maximum distance between the basis vectors, bias parameters are included, basis size follows nRBFN), (5) nRNwr (nRBFN without regularization, using the same basis as nRBFN), (6) nRNrb (nRBFN with basis chosen randomly from the training set, basis size follows nRBFN), (7) SVM [11] (Gaussian kernel, the weight C is selected over 2{\u22121,0,.", "startOffset": 687, "endOffset": 691}, {"referenceID": 33, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 32, "endOffset": 36}, {"referenceID": 53, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 58, "endOffset": 70}, {"referenceID": 54, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 58, "endOffset": 70}, {"referenceID": 61, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 58, "endOffset": 70}, {"referenceID": 57, "context": "It is also closely related to Gaussian mixture model [63].", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "Comparing with RBFN, nRBFN has a distinctive feature: in regions far from the samples, the output of RBFN will vanish due to the localized property of radial basis function, while that of nRBFN will not due to the normalization [9].", "startOffset": 228, "endOffset": 231}, {"referenceID": 61, "context": "Meanwhile, the universal approximation capacity is preserved [67, 4].", "startOffset": 61, "endOffset": 68}, {"referenceID": 3, "context": "Meanwhile, the universal approximation capacity is preserved [67, 4].", "startOffset": 61, "endOffset": 68}, {"referenceID": 5, "context": ", [6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 44, "context": ", [50].", "startOffset": 2, "endOffset": 6}, {"referenceID": 38, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 28, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 27, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 46, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 61, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 86, "endOffset": 94}, {"referenceID": 26, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 86, "endOffset": 94}, {"referenceID": 38, "context": "A typical result states that with probability greater than 1 \u2212 \u03b4, the generalization error of RBFN is upper bounded by O(1/r) + O( \u221a (pr log(nr)\u2212 log \u03b4)/n) [44].", "startOffset": 156, "endOffset": 160}, {"referenceID": 34, "context": ", [40], can be applied to address the bottleneck of speed improvement.", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "(4) Online basis learning can be considered [49], where the basis can be increased, updated, or pruned.", "startOffset": 44, "endOffset": 48}, {"referenceID": 46, "context": ", RBFN [52], ELM [27].", "startOffset": 7, "endOffset": 11}, {"referenceID": 58, "context": "(3) It will be interesting to compare the spectral risk with the VC dimension [64], the error-and-risk tradeoff with the structural risk minimization [64].", "startOffset": 78, "endOffset": 82}, {"referenceID": 58, "context": "(3) It will be interesting to compare the spectral risk with the VC dimension [64], the error-and-risk tradeoff with the structural risk minimization [64].", "startOffset": 150, "endOffset": 154}, {"referenceID": 20, "context": "The deviation can be measured using the idea of the l2 operator norm [23]:", "startOffset": 69, "endOffset": 73}, {"referenceID": 38, "context": ", [44], the \u201crisk\u201d of expected risk actually means error.", "startOffset": 2, "endOffset": 6}, {"referenceID": 58, "context": "A similar concept is empirical risk [64], actually it is training/fitting error.", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "= I \u2212 \u0174 \u0174 , that is also for dealing with the scalable problem of large graph construction [37].", "startOffset": 91, "endOffset": 95}], "year": 2017, "abstractText": "Spectral graph theory has been widely applied in unsupervised and semi-supervised learning. It is still unknown how it can be exploited in supervised learning. In this paper, we find for the first time, to our knowledge, that it also plays a concrete role in supervised classification. It turns out that two classifiers are inherently related to the theory: linear regression for classification (LRC) and normalized radial basis function network (nRBFN), corresponding to linear and nonlinear kernel respectively. The spectral graph theory provides us with a new insight into a fundamental aspect of classification: the tradeoff between fitting error and overfitting risk. With the theory, ideal working conditions for LRC and nRBFN are presented, which ensure not only zero fitting error but also low overfitting risk. For quantitative analysis, two concepts, the fitting error and the spectral risk (indicating overfitting), have been defined. Their bounds for nRBFN and LRC are derived. A special result shows that the spectral risk of nRBFN is lower bounded by the number of classes and upper bounded by the size of radial basis. When the conditions are not met exactly, the classifiers will pursue the minimum fitting error, running into the risk of overfitting. It turns out that l2-norm regularization can be applied to control overfitting. Its effect is explored under the spectral context. It is found that the two terms in the l2-regularized objective are one-one correspondent to the fitting error and the spectral risk, revealing a tradeoff between the two quantities. Concerning practical performance, we devise a basis selection strategy to address the main problem hindering the applications of (n)RBFN. With the strategy, nRBFN is easy to implement yet flexible. Experiments on 14 benchmark data sets show the performance of nRBFN is comparable to that of SVM, whereas the parameter tuning of nRBFN is much easier, leading to reduction of model selection time.", "creator": "LaTeX with hyperref package"}}}