{"id": "1705.06936", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Atari games and Intel processors", "abstract": "The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions. In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0 machine learning framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.", "histories": [["v1", "Fri, 19 May 2017 11:19:45 GMT  (1399kb,D)", "http://arxiv.org/abs/1705.06936v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["robert adamski", "tomasz grel", "maciej klimek", "henryk michalewski"], "accepted": false, "id": "1705.06936"}, "pdf": {"name": "1705.06936.pdf", "metadata": {"source": "CRF", "title": "Atari games and Intel processors", "authors": ["Robert Adamski", "Tomasz Grel", "Maciej Klimek", "Henryk Michalewski"], "emails": ["Robert.Adamski@intel.com,", "T.Grel@deepsense.io,", "M.Klimek@deepsense.io,", "H.Michalewski@mimuw.edu.pl"], "sections": [{"heading": null, "text": "Keywords: reinforcement learning, deep learning, Atari games, asynchronous calculations"}, {"heading": "1 Introduction", "text": "In this paper, we approach the problem of learning strategies in Atari games from the perspective of the hardware architecture. We use a variation of the statistical model developed in [13,14]. With the help of the provided code1, our experiments are easy to understand and we encourage the reader to draw his own conclusions from how CPUs behave in the context of Atari games. In the following, we will treat Atari games as a central benchmark problem for modern learning. We will use a statistical model consisting of about one million floating-point numbers that are iteratively updated with a gradient lineage algorithm described in [12]. At first glance, such a model appears to be a relatively simple task: a screen from the simulator that decides which button to press; over one episode of a game, we estimate how the agent performs and calculates the loss so that the loss is reduced."}, {"heading": "1.1 Related tools", "text": "Our work is based on OpenAI Gym [7], an open source machine learning platform that provides very easy access to a rich library of games, including Atari games - Google's TensorFlow 0.11rc0, an open source machine learning framework [4] that enables the rational integration of various neural networks (layers) implemented elsewhere - Tensorpack, an open source library [23] that implements a very efficient learning algorithm to amplify - Intel's Math Kernel Library 2017 (MKL) [19], a freely available library that implements primitive (layers) neural networks and accelerates overall matrix and, in particular, deep learning calculations on Intel's processors."}, {"heading": "1.2 Related work", "text": "In any case, the impact of the statistical model is one of 18 possible steps taken by the taxpayer: in each episode, the agent generates a certain amount of reward, which consists in the fact that decisions taken during the episode are ignored by a factor. (0 < < < < < < < < < < < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > < < < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > < < < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > < < < < < < < < < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >"}, {"heading": "2 The Batch Asynchronous Advantage Actor Critic Algorithm (BA3C)", "text": "The Advantage Actor Critic Algorithm (A2C) is a reinforcement learning algorithm that combines positive aspects of both political and value-based approaches to reinforcement learning, and the results recently published by Mnih et al. in [13] provide a compelling case for using its asynchronous version (A3C). After testing several implementations of this algorithm, we found that a high-quality open source implementation of this algorithm is available within the TensorPack (TP) framework [23]. However, the differences between this variant, which resembles an algorithm introduced in [5], and the algorithm originally described in [13] are significant enough to warrant a new name. Therefore, we will refer to this implementation as Batch Asynchronous Advantage Actor Critic (BA3C)."}, {"heading": "2.1 Asynchronous reinforcement learning algorithms", "text": "Asynchronous reinforcement learning methods are designed to use multiple simultaneous environments to speed up the training process, leaving the question of how the model or models are stored and synchronized between the environments. In 2.2, we discuss a few possible options, including describing our own decisions. Apart from the obvious accelerations that result from simultaneous use, this approach also has some statistical consequences. Normally, in an environment, subsequent states correlate strongly, which can have some adverse effects on the training process."}, {"heading": "2.2 BA3C \u2013 details of the implementation", "text": "The consequence is that the number of persons who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2.3 Effects of asynchronism on convergence", "text": "The reason for this is that most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "3 Specification of involved hardware", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Intel Xeon\u00ae (Broadwell)", "text": "We used Intel Xeon \u00ae E5 2600 v4 processors to perform benchmark winding tests. Xeon Broadwell is based on a processor microarchitecture known as a \"tick\" [15] - a shrinkage of an existing architecture rather than a new architecture. In this sense, Broadwell is essentially a haswell based on Intel's 14nm second tri-gate transistor process with few improvements to the microarchitecture. Important changes are: up to 22 cores per CPU; support for DDR4 memory up to 2400 MHz; faster floating point instruction; improved performance on large datasets. The results reported here are obtained on a system running two Intel Xeon \u00ae processors E5 2689 (3.10 GHz, 10 core) with 128GB DDR4 2400MHz RAM, Intel Compute Modules S2600TP and Intel Server Chassis H2312XXLR2. Ubuntu \u00ae LTS 16.04 operating system was linked to the 20100CC library (the GCC)."}, {"heading": "3.2 Intel Xeon\u00ae (Haswell)", "text": "Haswell, together with a new microarchitecture, brings important features such as AVX2. We used the Prometheus cluster with a peak performance of 2.4 Pflops, located in the Academic Computer Center Cyfronet AGH, as our test platform. Prometheus consists of more than 2,200 servers, accompanied by a total of 279 TB of RAM, and two storage file systems with a total capacity of 10 PB and an access speed of 180 GB / s. The experiments were conducted in single node mode, with each node consisting of two Intel Xeon E5-2680v3 processors with 24 cores at 2.5 GHz and 128 GB of RAM, with a peak performance of 1.07 TFlops.Xeon Haswell CPU enables effective calculations of CNN algorithms and volumes in particular by supporting SIMD (Single Instruction, Multiple Data Performance of 1.07 TFlops.Xeon Haswell CPU)."}, {"heading": "4 The MKL library", "text": "The Intel Math Kernel Library (Intel MKL) 2017 introduces a series of Deep Neural Networks (DNN) [19] primitives for DNN applications optimized for the Intel architecture, which implement forward and reverse gears for the following operations: (1) convolution: direct bundling, (2) internal product, (3) pooling: maximum, minimum, and average, (4) normalization: local channel response normalization and stack normalization, (5) activation: reflected linear neuron activation (ReLU), (6) data manipulation: multidimensional transposition (conversion), split, concatenation, sum, and scale. Intel MKL DNN primitives implement a simple C programming interface (API) that can be used in existing C / C + + + DNN frameworks as well as in user-defined DNN applications."}, {"heading": "5 Changes in TensorFlow 0.11rc0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Motivation", "text": "Preliminary benchmarks showed that the vast majority of the computing time spent during training is spent performing convolutions. On the CPU, the most expensive operation was reverse gear in terms of the cores of the convolution, especially in the first layers working on the largest inputs. Therefore, significant performance gains had to be achieved by optimizing convolutions operation. We considered the following approaches to this problem: Tuning the current implementation of convolutions - TensorFlow (TF) uses the Own [10] library as the backend for performing matrix operations on the CPU. Therefore, this approach would require changes in the code of this library. Matrix multiplication procedures used within Own have several hyperparameters that determine the way work is divided between the threads. Also, some fairly strong assumptions about the configuration of the machine (e.g. its cache size) require changes. This leaves room for security with specific improvements, especially for optimization of the very specific application."}, {"heading": "5.2 Implementation", "text": "TensorFlow provides a well-documented mechanism for adding custom operations in C + + that allows additional operations to be loaded as common objects. However, creating a build for a separate binary would make it difficult to use some internal TF tools and share code with the original folding operation. Therefore, we decided to split the entire framework and provide the additional operations.The functionality of another TF called \"label\" made it very easy to deploy several different implementations of the same operation in C + + and select between them from the python layer by defining a \"label map.\" This proved particularly helpful in testing and benchmarking our implementation, as we were able to quickly compare it to the original implementation.The implementation consisted of linking against the MKL library and providing the three additional operations propagated: (1) MKL folding forward, (2) ML (KL) of the MKL."}, {"heading": "5.3 Benchmark results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "128,84,84,16 16,32,5,5 10.03 23.61 90.11 99.74", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "128,40,40,32 32,32,5,5 4.58 8.76 43.83 33.61", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "128,18,18,32 32,64,5,5 1.61 2.71 17.20 10.22", "text": "In order to evaluate the performance of our implementation, several benchmarks were performed focusing on a specific 4-layer ConvNet architecture for processing the Atari input pictures.13 Tables 2, 3 and 4 show the benchmark results for the TensorFlow, which was converted to MKL and standard TensorFlow. Measurements consist of the times when windings with specific parameters (input and filter sizes) were performed for Xeon and Xeon Phi CPUs. The same folding parameters were used in the conventional network used in the Atari games. Results show that the MKL windings can be significantly faster than those implemented in TensorFlow. In some operations, an acceleration of more than 10 times was achieved, the results are consistent with those reported in [8]. It is also noteworthy that most of the time is spent in the first shift, which is responsible for processing the largest images."}, {"heading": "128,40,40,32 32,32,5,5 11.17 16.99 468.82 112.77", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "128,84,84,16 16,32,5,5 8.97 29.63 1,236.98 368.18", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "128,40,40,32 32,32,5,5 6.33 19.55 343.73 114.72", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.4 Possible improvements", "text": "Data layout can have a huge impact on the performance of low-level array operations. Conversely, the efficiency of these operations is critical to the performance of higher-level machine learning algorithms. 14TensorFlow and MKL pursue radically different philosophies of storing visual data. TensorFlow mainly uses its standard \"NHWC\" format, in which pixels with the same spatial location but different channel indexes are placed in memory. Some operations also offer the widely used \"NCHW\" format used by other deep learning frameworks such as Caffe [11]. On the other hand, MKL does not have a predefined standard format, but is designed to easily connect MKL layers to each other. Specifically, the same operation may require different data layouts depending on the size of their input channels (e.g. the number of input channels), ensuring that the number of intermediate \"conversions\" for each of pipelines is preferred \"during its pipeline or simultaneous use.\""}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Game scores and overall training time", "text": "Using the custom folding primitives from the MKL library, the training speed could be increased by a factor of 3.8 (from 151.04 examples / s to 517.12 examples / s), making it possible to train powerful agents in less than 24 hours. As a result, new concepts and improvements in the algorithm can now be tested more quickly, potentially leading to further advances in reinforcement learning, which has been achieved without compromising the results achieved by the trained agents. Exemplary training curves for 3 different games are shown in Figure 7."}, {"heading": "6.2 Batch size and learning rate tuning", "text": "Using the previously described pipeline, which was optimized for better CPU performance, we conducted a series of experiments aimed at determining the optimal lot size and learning rate hyperparameters, which were conducted using the random search method [6]. For each hyperparameter, its value was derived from a logically uniform distribution defined in a range [10 \u2212 4, 10 \u2212 2] for the learning rate and [21, 210] for the lot size. In total, over 200 experiments were conducted in this way for 5 different games, the results are presented in the numbers 8.9 below. It appears that for the 5 games tested, a combination of learning rate and lot size could be selected that would work relatively well for all. However, the optimal settings for certain games seem to diverge. As might have been expected, better results with higher learning rate were obtained when using large batches, most likely caused by the stabilizing effects of larger batches and lot sizes."}, {"heading": "7 Conclusions and further work", "text": "Preliminary results included in this paper can be considered as the next step in narrowing the gap between CPU and GPU performance in deep learning applications. As shown in this paper, CPU-only algorithms are already achieving very competitive performance in the area of amplification learning and in the context of asynchronous algorithms. We see the expansion of the results of [18] and tuning the performance of asynchronous amplification learning algorithms to large computer clusters with the idea of reducing training time from hours to minutes as the most interesting future research direction. Also, creating a compelling experiment for the Xeon Phi \u00ae platform seems to be an interesting challenge. Our current approach would require significant modification due to the much slower single-core performance of Xeon Phi \u00ae. However, preliminary results of the pong game are promising with a state of the art achieved in 12 hours on a single Xeon Phi \u00ae server."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems (2015), http://tensorflow.org/, software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "GA3C: gpubased A3C for deep reinforcement learning", "author": ["M. Babaeizadeh", "I. Frosio", "S. Tyree", "J. Clemons", "J. Kautz"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "J. Mach. Learn. Res", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Myth busted: General purpose CPUs can\u2019t tackle deep neural network training (Jun 2016)", "author": ["P. Dubey"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Eigen v3", "author": ["G. Guennebaud", "B Jacob"], "venue": "http://eigen.tuxfamily.org", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R.B. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR abs/1412.6980", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M.A. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Intel xeon processor e5-2600 v4 product family technical overview (Jan 2017)", "author": ["D. Mulnix"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A.D. Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "ArXiv e-prints", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Learning Episode 4: Supercomputer vs Pong II (Oct 2016)", "author": ["Mark O\u2019Connor"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Introducing DNN primitives in Intel Math Kernel Library (Mar 2017)", "author": ["V. Pirogov"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Evolution strategies as a scalable alternative to reinforcement learning (Mar 2017)", "author": ["T. Salimans", "J. Ho", "X. Chen", "I. Sutskever"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Reinforcement learning - an introduction. Adaptive computation and machine learning, MIT", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Optimizing Tensorflow on Intel architecture for AI applications (Mar 2017)", "author": ["E. Ould-ahmed vall"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Tensorpack", "author": ["Y. Wu"], "venue": "https://github.com/ppwwyyxx/tensorpack", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "We use a variation of the statistical model developed in [13,14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 9, "context": "We use a variation of the statistical model developed in [13,14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 8, "context": "Following [7,13,14] we treat Atari games as a key benchmark problem for modern reinforcement learning.", "startOffset": 10, "endOffset": 19}, {"referenceID": 9, "context": "Following [7,13,14] we treat Atari games as a key benchmark problem for modern reinforcement learning.", "startOffset": 10, "endOffset": 19}, {"referenceID": 7, "context": "We use a statistical model consisting of approximately one million floating point numbers which are iteratively updated using a gradient descent algorithm described in [12].", "startOffset": 168, "endOffset": 172}, {"referenceID": 8, "context": "In this work we accept a number of technical solutions presented in [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "follows closely research done in [5], where a batch version of [13] is analyzed.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "follows closely research done in [5], where a batch version of [13] is analyzed.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "11rc0, an open source machine learning framework [4] allowing for streamlined integration of various neural networks primitives (layers) implemented elsewhere, \u2013 Tensorpack, an open source library [23] implementing a very efficient reinforcement learning algorithm, \u2013 Intel\u2019s Math Kernel Library 2017 (MKL) [19], a freely available library which implemented neural networks primitives (layers) and overall speeds up matrix and in particular deep learning computations on Intel\u2019s processors.", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "11rc0, an open source machine learning framework [4] allowing for streamlined integration of various neural networks primitives (layers) implemented elsewhere, \u2013 Tensorpack, an open source library [23] implementing a very efficient reinforcement learning algorithm, \u2013 Intel\u2019s Math Kernel Library 2017 (MKL) [19], a freely available library which implemented neural networks primitives (layers) and overall speeds up matrix and in particular deep learning computations on Intel\u2019s processors.", "startOffset": 197, "endOffset": 201}, {"referenceID": 14, "context": "11rc0, an open source machine learning framework [4] allowing for streamlined integration of various neural networks primitives (layers) implemented elsewhere, \u2013 Tensorpack, an open source library [23] implementing a very efficient reinforcement learning algorithm, \u2013 Intel\u2019s Math Kernel Library 2017 (MKL) [19], a freely available library which implemented neural networks primitives (layers) and overall speeds up matrix and in particular deep learning computations on Intel\u2019s processors.", "startOffset": 307, "endOffset": 311}, {"referenceID": 8, "context": "Relation to [13].", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "Decisions in which we follow [13].", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Authors of [13] reported good CPU performance and this encouraged the experiment described in this paper.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "Decisions left to readers of [13].", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "Relation to [5] and [18].", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "Relation to [5] and [18].", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "Since the publication of [14] a significant number of new results was obtained in the domain of Atari games, however to the best of our knowledge only the works [5] and [18] were focused on the hardware performance.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "Since the publication of [14] a significant number of new results was obtained in the domain of Atari games, however to the best of our knowledge only the works [5] and [18] were focused on the hardware performance.", "startOffset": 161, "endOffset": 164}, {"referenceID": 13, "context": "Since the publication of [14] a significant number of new results was obtained in the domain of Atari games, however to the best of our knowledge only the works [5] and [18] were focused on the hardware performance.", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "In [5] authors modify the approach from [13] so it fits better into the GPU multicore infrastructure.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [5] authors modify the approach from [13] so it fits better into the GPU multicore infrastructure.", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "This work can be considered a CPU variant of [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 13, "context": "In [18] a significant speedup of the A3C algorithm was obtained using large CPU clusters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Also the announcement [18] does not contain neither technical details or implementation.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "Relation to [22].", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "The fork of TensorFlow announced in [22] will offer a much deeper integration of TensorFlow and Intel\u2019s Math Kernel Library (MKL).", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "However, at the moment of writing of this paper we had to do the integration of these tools on our own, because the fork mentioned in [22] was not ready for our experiments.", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "The work [14] approaches the problem of learning a strategy in Atari games through approximation of the Q-function, that is implicitly it learns a synthesized values of every move of a player in a given situation on the screen.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "We did not consider this method, because of overall weaker results and much longer training times comparing to the asynchronous methods in [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "The DeepBench [8], the FALCON Library [3] and the study [1] compare a performance of CPU and GPU on neural network primitives (single convolutional and dense layers) as well as on a supervised classification problem.", "startOffset": 14, "endOffset": 17}, {"referenceID": 15, "context": "A recently published work [20] shows a very promising CPU-only results for agent training tasks.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "The learning algorithm proposed in [20] is a novel approach with yet untested stability properties.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "For a broad introduction to reinforcement learning we refer the reader to [21].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "For a historical background on Atari games we refer to [14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "in [13] provide strong arguments for using its asynchronous version (A3C).", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "After testing several implementations of this algorithm we found that a high quality open source implementation of this algorithm is provided in the TensorPack (TP) framework [23].", "startOffset": 175, "endOffset": 179}, {"referenceID": 1, "context": "However, the differences between this variant, which resembles an algorithm introduced in [5], and the one described originally in [13] are significant enough to justify a new name.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "However, the differences between this variant, which resembles an algorithm introduced in [5], and the one described originally in [13] are significant enough to justify a new name.", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "2 In [5] is proposed a different name GA3C derived from \u201chybrid CPU/GPU implementation of the A3C algorithm\u201d.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "This approach is used and extensively described in [13,16,17] and we refer to it as A3C.", "startOffset": 51, "endOffset": 61}, {"referenceID": 11, "context": "This approach is used and extensively described in [13,16,17] and we refer to it as A3C.", "startOffset": 51, "endOffset": 61}, {"referenceID": 12, "context": "This approach is used and extensively described in [13,16,17] and we refer to it as A3C.", "startOffset": 51, "endOffset": 61}, {"referenceID": 1, "context": "This is much more suitable for use on massively parallel hardware [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "0 5 10 15 20 25 30 35 40 45 training step [10] 0 20 40 60 80 100 120 140", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "Xeon Broadwell is based on processor microarchitecture known as a \u201ctick\u201d [15] \u2013 a die shrink of an existing architecture, rather than a new architecture.", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "advantages have been implemented in the Math Kernel Library (MKL) and used in deep learning framework Caffe [9], [2] resulting in improved convolutions performance.", "startOffset": 108, "endOffset": 111}, {"referenceID": 14, "context": "The Intel Math Kernel Library (Intel MKL) 2017 introduces a set of Deep Neural Networks (DNN) [19] primitives for DNN applications optimized for the Intel architecture.", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "We considered the following approaches to this problem: Tuning the current implementation of convolutions \u2013 TensorFlow (TF) uses the Eigen [10] library as a backend for performing matrix operations on CPU.", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "Some tests of convolutions on a comparable hardware had already been performed by Baidu [8] and showed promising results.", "startOffset": 88, "endOffset": 91}, {"referenceID": 17, "context": "A similar decision was taken also in the development of the Intelfocused fork of TensorFlow [22].", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "The results agree with the ones reported in [8].", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "Some operations also provide the \u201cNCHW\u201d format widely used by other deep learning frameworks such as Caffe [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "The experiments were performed using the random search method [6].", "startOffset": 62, "endOffset": 65}, {"referenceID": 13, "context": "As the most interesting future research direction we perceive extending results of [18] and tuning of performance of asynchronous reinforcement learning algorithms on large computer clusters with the idea of bringing the training time down from hours to minutes.", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage ActorCritic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions. In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0 machine learning framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.", "creator": "LaTeX with hyperref package"}}}