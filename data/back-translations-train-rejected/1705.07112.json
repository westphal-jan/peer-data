{"id": "1705.07112", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Fast Singular Value Shrinkage with Chebyshev Polynomial Approximation Based on Signal Sparsity", "abstract": "We propose an approximation method for thresholding of singular values using Chebyshev polynomial approximation (CPA). Many signal processing problems require iterative application of singular value decomposition (SVD) for minimizing the rank of a given data matrix with other cost functions and/or constraints, which is called matrix rank minimization. In matrix rank minimization, singular values of a matrix are shrunk by hard-thresholding, soft-thresholding, or weighted soft-thresholding. However, the computational cost of SVD is generally too expensive to handle high dimensional signals such as images; hence, in this case, matrix rank minimization requires enormous computation time. In this paper, we leverage CPA to (approximately) manipulate singular values without computing singular values and vectors. The thresholding of singular values is expressed by a multiplication of certain matrices, which is derived from a characteristic of CPA. The multiplication is also efficiently computed using the sparsity of signals. As a result, the computational cost is significantly reduced. Experimental results suggest the effectiveness of our method through several image processing applications based on matrix rank minimization with nuclear norm relaxation in terms of computation time and approximation precision.", "histories": [["v1", "Fri, 19 May 2017 17:55:58 GMT  (3735kb,D)", "http://arxiv.org/abs/1705.07112v1", "This is a journal paper"]], "COMMENTS": "This is a journal paper", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["masaki onuki", "shunsuke ono", "keiichiro shirai", "yuichi tanaka"], "accepted": false, "id": "1705.07112"}, "pdf": {"name": "1705.07112.pdf", "metadata": {"source": "CRF", "title": "Fast Singular Value Shrinkage with Chebyshev Polynomial Approximation Based on Signal Sparsity", "authors": ["Masaki Onuki", "Shunsuke Ono", "Keiichiro Shirai", "Yuichi Tanaka"], "emails": ["masaki.o@msp-lab.org;", "ytnk@cc.tuat.ac.jp).", "ono@isl.titech.ac.jp).", "(keiichi@shinshu-u.ac.jp)."], "sections": [{"heading": null, "text": "In fact, most people are able to survive themselves, and they are able to survive themselves, \"he said.\" I don't think they are able to survive me. \"He added,\" I don't think they are able to survive me. \"He added,\" I don't think they are able to survive me. \"He added,\" I don't think they are able to survive me. \"He added,\" I don't think they are able to survive me. \"He added,\" I don't think I believe I am, I believe I don't. \""}, {"heading": "A. Notations", "text": "Upper and lower case letters indicate a matrix or vector. Upper case \u00b7 > is the transposition of a matrix and a vector, upper case \u00b7 \u2212 1 is the opposite of a non-singular matrix. The matrices Id and O are the identity matrix or null matrix, respectively. The vector 1n: = [1,..., 1 n] >. The \"p norm for p \u2265 1 is defined as follows:"}, {"heading": "B. Chebyshev Polynomial Approximation", "text": "Let h (x) and h (x) be a real function defined at the interval x (\u2212 1, 1] and its approximate function using CPA. Chebyshev's polynomial approximation [29] - [31] yields an approximate solution of h (x) using the truncated Chebyshev series: h (x): = 12 c0 + \u03b1 \u2212 1 ck-k = 1 ck-k (x), (1) where ck and \u03b1 represent a Chebyshev coefficient (later described) and an approximation order. (2) It can also be calculated using the stable reciprocity relationship: k (x) = 2xx-ks \u2212 kcos of the first kind, defined as k (x): = cos (x): 0 x-cosmic (x)."}, {"heading": "III. SINGULAR VALUE SHRINKAGE USING CHEBYSHEV POLYNOMIAL APPROXIMATION BY EXPLOITING", "text": "SPARSITYWe discuss singular value shrinkage using CPA. First, the CPA of a matrix form that can approximately shrink the eigenvalues of a matrix (eigenvalue shrinkage) is indexed and then extended to the singular way.May 22, 2017 DRAFT5"}, {"heading": "A. Chebyshev Polynomial Approximation for Matrix", "text": "In this subsection we consider the approximate solution of (5) Amazonas. The CPA of Matrix [30, 34, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44"}, {"heading": "B. CPA-based Singular Value Shrinkage", "text": "Let B (Rm) \u00b7 n (m > n) be a rectangular matrix (B = > > TB > its single value decomposition, whereas U (Rm) \u00b7 m and V (Rn) n are orthogonal matrices. However, the individual values of B are the single value matrix represented as a single value. (12) Without the loss of generality, we can assume that g (13) is an arbitrary function. The eigenvalues of B have shrunk with the singular value function G () asG () asG (B): = U g (1) O... (g) The eigenvalue shrinkage in (5) can be extended to G (B) as [35] G (B) = BH. (B) = BH."}, {"heading": "C. Computational Complexity of CPA-based Singular Value Shrinkage", "text": "Let us now discuss the computational complexity of our method. Suppose that the matrices B, Rn and k, B and Rn, M and Mk have unequal elements. In the case of a sparse matrix, the maximum number of multiplications of elements not equal to zero required for the computation of B, B and K is represented as MMk. The computational complexity of line 9 in algorithm 1 can be represented as O due to the multiplication B, K and MMK. In line 10 of algorithm 1, the computational complexity is derived from the multiplication ck (B, K and MMK), that is, the total computational complexity is represented as O (M + 1) maxk {Mk}, with maxk and Mk representing the maximum value under Mk."}, {"heading": "IV. SHRINKAGE FUNCTIONS AND APPROXIMATION ORDER", "text": "In this section, we discuss appropriate approximation orders for shrinkage functions approximated by CPA and with small section errors. Furthermore, we argue that CPA is a reasonable choice for our method among a variety of polynomial approximation methods.As an introduction, we consider the shrinkage function shown in Figure 2 (a). Let us let Hard (x) be the hard shrinkage ageresponse defined ashhard (x): = 1 if x > \u03c4hard, otherwise 0, (25), where \u03c4hard is an arbitrary real value, and x \u0432 [0, 1]. Chebyshev polynomial approximation gives an approximate answer to Hard (x) in (1). As in Figure 2 (b), the approximated response h-hard (x) is an important topic, which is also widely known in digital filter design [32], [46] - [49]. Therefore, the study of the shrinkage method is an important one, even for our approximations."}, {"heading": "A. Approximation Order", "text": "Possible shrinkage reactions handled by our method can be expressed as the following generic form: h (x; w (x) \u03c1, \u03c4): = \u221a x \u2212 w (x) \u03c1 \u221a x, 0 otherwise: (26) DRAFT10where w (x) is a weight function, and holds are arbitrary shrolding values. The decisions of w (x) and \u03c4 determine the properties of (26) as follows: \u2022 h (x; 0, \u03c4hard): Hard shrinkage. \u2022 h (x; w (x) \u03c1, w (x) \u03c1: Soft shrinkage. \u2022 h (x; 1\u03c1, 1): Soft shrinkage."}, {"heading": "B. Suitability of CPA", "text": "There are many polynomial approximations. Even among them, minimax polynomial approximation = > PA = > errors in verses 11, [50] - [52] and approximation of the smallest squares [53] is known as the best approximation in terms of minimizing the infinity norm and the smallest squares. To derive polynomial coefficients, their optimization requires a minimization of the \"p norm\" of May 22, 2017. DRAFT12 represents asmin h \"(x) the difference between an exact and approximate response. (x) \u2212 h\" \u2212 p, \"(27) where h\" \u2212 \u2212 \"shrinkage response with the above two approximate approximations. Clearly, (27) requires the exact response h\" (x) for x \"R. If h\" (x) is accurately represented, h \"x\" (x) performs well."}, {"heading": "V. APPLICATIONS", "text": "We compared our CPA-based singular value shrinkage method with the exact and approximate singular value shrinkage method. Specifically, we applied our method to two applications using nuclear standard relaxation, i.e. coloring texture images and background subtraction of videos. In addition, we compared our CPA-based method with existing methods, i.e. the exact method of partial singular value substitution (PSVD) and fast singular value shrinkage methods [25] - [27], in Section V-F. Calculation time and approximation accuracy were given for the comparisons."}, {"heading": "A. Experimental Conditions", "text": "The applications were implemented with MATLAB R2015b and run on a 3.2 GHz Intel Xeon E5-2667 processor with 512 GB of RAM. We compared our method with the SVD-based naive method (referred to as the SVD-based method) in (13) and EVD-based methods in (15) in terms of approximation accuracy and computation time. Both SVDMay 22, 2017 DRAFT13 and EVD-based methods are exact singular value soft shrinkage methods. EVD-based method 2 is generally faster than the SVD-based method and is used in many applications. The computation time of only the EVD-based method is given for the results of the exact methods. The SVD-based method influences the SVD-based selection of an arbitrary matrix X waveform."}, {"heading": "B. Optimization Tools", "text": "1) The proximity operator [54] of a function f-0 (RN) of index \u03b3 > 0 is defined as the proximity operator (R N) as discussed in this section. If the function f is defined as the nuclear standard, i.e., the proximity operator (R) as the proximity operator f (y) + 12\u03b3 x \u2212 y. (29) The proximity operator plays a central role in the optimization of applications as discussed in this section. If the function f is defined as the nuclear standard, i.e., the proximity operator can be calculated by singular value shrinkage with the threshold shrinkage. [2] Therefore, our CPA-based method is applied to the operator in the case of the nuclear standard. 2) Alternating DirectMethod of Multipliers: The ADMM [41] is an algorithm for solving a convex problem."}, {"heading": "C. Texture Image Inpainting [12], [13]", "text": "The objective with this application is to restore a missing region (as shown in later Figure 6 (b)).The objective with this application is to restore a missing region (as shown in the presupposed Figure 6 (b)).The objective with this application is to restore a missing region (as shown in Figure 5).The objective with this application is to restore a missing region (as shown in Figure 5).The objective with this application is to restore a missing region (as shown in Figure 5)."}, {"heading": "D. Background Modeling of Video [14]\u2013[18]", "text": "The object of this application is to divide a video sequence into background and object sequences (as in Figure 7 (a) and (b)).Let I (i).Rm \u00b7 n Be the i-th frame of a video sequence. (33) The sequence is converted into a matrix, the sequence and sequence of moving objects of a video sequence. (33) The sequence of L and S is the background sequence and sequence of moving objects of a video sequence. (5) The pixel values corresponding to the S are zero and vice versa. (5) The background and moving objects can be assumed as low order. (6) Background modeling is solved, like the following convex optimization: min L, S, S, S, S, S, S., S."}, {"heading": "E. Effects of Selections: Transform Matrix and Thresholding Value", "text": "The selections of a transformation matrix T in (22) and a threshold \u03b5 in (20) influence the calculation time and size of the approximation errors. In this subsection, we show these effects experimentally by using the threshold of 0.5 percentage points for bricks. In all experiments, the 15th order approximation method was used for our methodology. 1) Effects of the selected transformation matrices T in (22): We compared the DWT with the DCT and the block diagonal shapes of the DCT (Block DCT) [32], whose block size was 8 \u00b7 8, to show the differences between the selected transformation matrices T in (22). The thresholds in (20) were fixed at \u03b5 = 250 for the threshold of DCTs. The other experimental conditions were the same as those in section V-C. The results of the proposed method in Table III show the performance comparison method with the DWT components."}, {"heading": "F. Comparison with Existing Methods", "text": "To illustrate the advantage of our method, we compared it with the single value contraction by using the exact single value contraction without the exact single value contraction (FSVS). Experiments were carried out using the image information method for bricks and synthetic data. Synthetic data is constructed as block diagonal values whose number of diagonal blocks is equal to their rank. Let's define the matrix form of the synthetic data with rank n, and this is defined as D: = J \u2212 blkdiag (Ds, n), where J: = 110001 > 1000, Ds: = 1000 and Ds: = 1000 and Ds (Ds)."}, {"heading": "VI. CONCLUSION", "text": "The key tool of the proposed method is CPA. Based on the characteristics of the CPA, the contraction of the singular values could be calculated by multiplying matrices. Furthermore, the proposed method was further accelerated by using the sparseness of a signal, using the frequency transformation to achieve sparse coefficients. Furthermore, we investigated the approximation sequence to reduce the approximation errors. Experimental results showed that our method was much faster than the exact methods with high approximation accuracy in the case of a large data size. Furthermore, our method can result in the optimization method being stable compared to the existing fast singular value shrinkage methods due to its approximation accuracy."}, {"heading": "APPENDIX A", "text": "ADMM APPLICABLE FORMS"}, {"heading": "A. Texture Image Inpainting", "text": "Allow i: = vec (I), l: = vec (L), m: = vec (M), and m: vec (M). The 2-D DCT matrix is represented as area value, and the matrix form of P and P is defined as area value and area value. (In addition, the indicator functions of the sentences I: = {x \u00b2 Rmn | x = land use plan (x) = land use plan (m) and D as land use plan (m). (M), where N is the size of the land use plan (M). (32) By using the above definitions, the land use plan is redefined. (l) + land use plan I (l) + land use plan I (l) + land use plan I (l) + land use plan I (l) + land use plan I (l) + land use plan I (l) + land use plan I (l) (l) (38)"}, {"heading": "B. Background Modeling", "text": "Leave i: = vec (I), l: = vec (L), and s: = vec (S). The indicator function of the set I: = {x-RmnK | x = i} is defined by the above definitions. (34) If an auxiliary vector z (2) z (2) z (3) = Id O IdId ls = Kl \u2032. (41) Problem (40) is further rewritten if an auxiliary vector z (1) is represented as z: = z (1) z (2) z (3) s.t. z = Kl \u2032. (42) = Id O IdId ls = Kl \u2032. (41) Problem (40) is further rewritten asmin z, l \u00b2 z (1).z (2)."}], "references": [{"title": "Exact matrix completion via convex optimization,", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Found. Comput. Math. (FoCM),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "A singular value thresholding algorithm for matrix completion,", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. Optim., vol. 20,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Tensor completion for estimating missing values in visual data,", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelli. (TPAMI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization,", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Problems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Cartoon-texture image decomposition using blockwise low-rank texture characterization,", "author": ["S. Ono", "T. Miyata", "I. Yamada"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A low patch-rank interpretation of texture,", "author": ["H. Schaeffer", "S. Osher"], "venue": "SIAM J. Imag. Sci.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Robust photometric stereo via low-rank matrix completion and recovery,", "author": ["L. Wu", "A. Ganesh", "B. Shi", "Y. Matsushita", "Y. Wang", "Y. Ma"], "venue": "in Proc. Asian Conf. Comput. Vis. (ACCV),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Papadhimitri, \u201cA closed-form solution to uncalibrated photometric stereo via diffuse maxima,", "author": ["T.P. Favaro"], "venue": "in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Online robust image alignment via iterative convex optimization,", "author": ["Y. Wu", "B. Shen", "H. Ling"], "venue": "in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Colorization by matrix completion,", "author": ["S. Wang", "Z. Zhang"], "venue": "in Proc. AAAI Conf. Artificial Intelli.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Repairing sparse low-rank texture,", "author": ["X. Liang", "X. Ren", "Z. Zhang", "Y. Ma"], "venue": "in Proc. Euro. Conf. Comput. Vis. (ECCV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Convexity in source separation : models, geometry, and algorithms,", "author": ["M.B. McCoy", "V. Cevher", "Q.T. Dinh", "A. Asaei", "L. Baldassarre"], "venue": "IEEE Signal Process. Magazine, vol. 31,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Low-rank modeling and its applications in image analysis,", "author": ["X. Zhou", "C. Yang", "H. Zhao", "W. Yu"], "venue": "ACM Comput. Surv. (CSUR), vol. 47,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Robust principal component analysis: exact recovery of corrupted low-rank matrices via convex optimization,", "author": ["J. Wright", "A. Ganesh", "S. Rao", "Y. Peng", "Y. Ma"], "venue": "in Adv. Neural Info. Process. Sys. (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A multi-transformational model for background subtraction with moving cameras,", "author": ["D. Zamalieva", "A. Yilmaz", "J.W. Davis"], "venue": "Proc. Euro. Conf. Comput. Vis. (ECCV),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Moving object detection by detecting contiguous outliers in the low-rank representation,", "author": ["X. Zhou", "C. Yang", "W. Yu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelli. (TPAMI),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Color-line regularization for color artifact removal,", "author": ["S. Ono", "I. Yamada"], "venue": "IEEE Trans. Comput. Image,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Collaborative spectrum sensing from sparse observations using matrix completion for cognitive radio networks,", "author": ["J. Meng", "W. Yin", "H. Li", "E. Houssain", "Z. Han"], "venue": "in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Singing-voice separation from monaural recordings using robust principal component analysis,", "author": ["P.S. Huang", "S.D. Chen", "P. Smaragdis", "M. Hasegawa-Johnson"], "venue": "in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "The power of convex relaxation: near-optimal matrix completion,", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Info. Theory, vol. 56,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Reduced rank regression via adaptive nuclear norm penalization,", "author": ["K. Chen", "H. Dong", "K.-S. Chan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Weighted nuclear norm minimization with application to image denoising,", "author": ["S. Gu", "L. Zhang", "W. Zuo", "X. Feng"], "venue": "in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Fast randomized singular value thresholding for nuclear norm minimization,", "author": ["T.-H. Oh", "Y. Matsushita", "Y.-W. Tai", "I.S. Kweon"], "venue": "in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Spectral grouping using the Nystr\u00f6m method,", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelli. (TPAMI),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Fast singular value thresholding without singular value decomposition,", "author": ["J.-F. Cai", "S. Osher"], "venue": "Methods Appl. Anal., vol. 20,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Interpolation and approximation by polynomials", "author": ["G.M. Phillips"], "venue": "CMS Books Mathematics. Springer-Verlag,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "Wavelets on graphs via spectral graph theory,", "author": ["D.K. Hammond", "P. Vandergheynst", "R. Gribonval"], "venue": "Appl. Comput. Harmonic Anal., vol. 30,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Multirate Systems and Filter Banks, Prentice-Hall, Inc", "author": ["P. P Vaidyanathan"], "venue": "Upper Saddle River, NJ,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Fast and stable least-squares approach for the design of linear phase FIR filters,", "author": ["M. Okuda", "M. Ikehara", "S. Takahashi"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}, {"title": "Polynomial filtering in latent semantic indexing for information retrieval,", "author": ["E. Kokipoulou", "Y. Saad"], "venue": "in Proc. ACM SIGIR Conf. Res. develop. info. Retrieval,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "A Chebyshev-Davidson algorithm for large symmetric eigenproblems,", "author": ["Y. Zhou", "Y. Saad"], "venue": "SIAM J. Matrix Anal. Appl., vol. 29,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Approximating spectral densities of large matrices,", "author": ["L. Lin", "Y. Saad", "C. Yang"], "venue": "SIAM Rev.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Non-local/local image filters using fast eigenvalue filtering,", "author": ["M. Onuki", "S. Ono", "K. Shirai", "Y. Tanaka"], "venue": "in Proc. IEEE Int. Conf. Image Process. (ICIP),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Globalized BM3D using fast eigenvalue filtering,", "author": ["K. Suwabe", "M. Onuki", "Y. Iizuka", "Y. Tanaka"], "venue": "in Proc. IEEE Global Conf. Signal Info. Process. (GlobalSIP),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "An algorithm for improving non-local means operators via low-rank approximation,", "author": ["V. May", "Y. Keller", "N. Sharon", "Y. Shkolnisky"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "On the Douglas-Rachford splitting method and proximal point algorithm for maximal monotone operators,", "author": ["J. Eckstein", "D.P. Bertsekas"], "venue": "Math. Program.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1992}, {"title": "Image colorization based on ADMM with fast singular value thresholding by Chebyshev polynomial approximation,", "author": ["M. Onuki", "S. Ono", "K. Shirai", "Y. Tanaka"], "venue": "in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Digital signal processing: principles, algorithms, and applications, Pentice", "author": ["J.G. Proakis", "D.K. Manolakis"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1996}, {"title": "Discrete Cosine Transform: Algorithms, Advantages", "author": ["K.R. Rao", "P. Yip"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1990}, {"title": "A wavelet tour of signal processing", "author": ["S. Mallat"], "venue": "Academic press,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1999}, {"title": "Discrete-Time Signal Processing, Prentice-Hall, Inc", "author": ["A.V. Oppenheim", "R.W. Schafer", "J.R. Buck"], "venue": "Upper Saddle River, NJ,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1989}, {"title": "A unified approach to the design of optimum FIR linear-phase digital filters,", "author": ["J.H. McClellan", "T.W. Parks"], "venue": "IEEE Trans. Circuit Theory, vol. 20,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1973}, {"title": "Equiripple FIR filter design by the FFT algorithm,", "author": ["A.E. Cetin", "O.N. Gerek", "Y. Yardimci"], "venue": "IEEE Signal Process. Magazine, vol. 14,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1997}, {"title": "Remez, \u201cSur la d\u00e9rmination des polyn\u00f4mes d\u2019approximation de degr\u00e9 donn\u00e9e,\u201d Comm", "author": ["E. Ya"], "venue": "Soc. Math. Kharkov, vol. 10, pp. 41\u201363,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1934}, {"title": "A survey of methods of computing minimax and near-minimax polynomial approximations for functions of a single independent variable,", "author": ["W. Fraser"], "venue": "J. ACM,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1965}, {"title": "Introduction to Linear Algebra, Fifth ed", "author": ["G. Strang"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Fonctions convexes duales er points proximaux dans un espace hilbertien,", "author": ["J.J. Moreau"], "venue": "C. R. Acad. Sci. Prais Se\u0301r. A Math,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1962}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION The low-rank structure inherent in various signals has been widely exploited in many signal processing applications, such as matrix and tensor completion [1]\u2013[4], image decomposition [5], [6], photometric stereo [7], [8], image", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "INTRODUCTION The low-rank structure inherent in various signals has been widely exploited in many signal processing applications, such as matrix and tensor completion [1]\u2013[4], image decomposition [5], [6], photometric stereo [7], [8], image", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "INTRODUCTION The low-rank structure inherent in various signals has been widely exploited in many signal processing applications, such as matrix and tensor completion [1]\u2013[4], image decomposition [5], [6], photometric stereo [7], [8], image", "startOffset": 196, "endOffset": 199}, {"referenceID": 5, "context": "INTRODUCTION The low-rank structure inherent in various signals has been widely exploited in many signal processing applications, such as matrix and tensor completion [1]\u2013[4], image decomposition [5], [6], photometric stereo [7], [8], image", "startOffset": 201, "endOffset": 204}, {"referenceID": 6, "context": "INTRODUCTION The low-rank structure inherent in various signals has been widely exploited in many signal processing applications, such as matrix and tensor completion [1]\u2013[4], image decomposition [5], [6], photometric stereo [7], [8], image", "startOffset": 225, "endOffset": 228}, {"referenceID": 7, "context": "INTRODUCTION The low-rank structure inherent in various signals has been widely exploited in many signal processing applications, such as matrix and tensor completion [1]\u2013[4], image decomposition [5], [6], photometric stereo [7], [8], image", "startOffset": 230, "endOffset": 233}, {"referenceID": 8, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": "alignment [9], [10], colorization [11], inpainting [12], [13], background modeling [14]\u2013[18], color artifact removal [19], cognitive radio [20], and voice separation [21].", "startOffset": 166, "endOffset": 170}, {"referenceID": 19, "context": "The other is the nuclear norm relaxation [22].", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "Weighted nuclear norm relaxation [23], [24] has recently been proposed as a non-convex but continuous approximation of the rank function.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "Weighted nuclear norm relaxation [23], [24] has recently been proposed as a non-convex but continuous approximation of the rank function.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "Several methods have been proposed to tackle this issue [25]\u2013[27].", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "Several methods have been proposed to tackle this issue [25]\u2013[27].", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "The basic concept of [25], [26] is to approximately compute partial singular values and/or vectors.", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "The basic concept of [25], [26] is to approximately compute partial singular values and/or vectors.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "With the other method [27], singular value shrinkage is carried out by computing neither singular values nor vectors, but the reduction in the computation time is still", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "We consider a method similar to that by Cai and Osher [27]: We only need a \u201cprocessed\u201d matrix with thresholded singular values.", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "Note that the proposed method computes neither singular values nor vectors during the process of singular value shrinkage, similar to the method by Cai and Osher [27].", "startOffset": 162, "endOffset": 166}, {"referenceID": 25, "context": "\u2022 Chebyshev polynomial approximation (CPA) [29]\u2013[31]: This tool is often used for designing filters in signal processing [32], [33] and is a key tool for reducing computational cost.", "startOffset": 43, "endOffset": 47}, {"referenceID": 27, "context": "\u2022 Chebyshev polynomial approximation (CPA) [29]\u2013[31]: This tool is often used for designing filters in signal processing [32], [33] and is a key tool for reducing computational cost.", "startOffset": 121, "endOffset": 125}, {"referenceID": 28, "context": "\u2022 Chebyshev polynomial approximation (CPA) [29]\u2013[31]: This tool is often used for designing filters in signal processing [32], [33] and is a key tool for reducing computational cost.", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "[34]\u2013[37].", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "The concept of the applications has recently been used for improving the performance of image filtering methods such as bilateral filter, non-local means, and BM3D [38]\u2013[40].", "startOffset": 164, "endOffset": 168}, {"referenceID": 34, "context": "The concept of the applications has recently been used for improving the performance of image filtering methods such as bilateral filter, non-local means, and BM3D [38]\u2013[40].", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "For this study, we validated the proposed method by using two image processing applications: image inpainting [12] and background modeling [14]\u2013[18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "For this study, we validated the proposed method by using two image processing applications: image inpainting [12] and background modeling [14]\u2013[18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "For this study, we validated the proposed method by using two image processing applications: image inpainting [12] and background modeling [14]\u2013[18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 35, "context": "In these applications, target problems are formulated as convex optimization problems involving the nuclear norm so that they can be efficiently solved using the alternating direction method of multiplier (ADMM) [41] with our method.", "startOffset": 212, "endOffset": 216}, {"referenceID": 22, "context": "Although the ADMM is widely known as a robust method for computation errors in each iteration, optimization methods (including the ADMM) with the other fast singular value shrinkage methods [25]\u2013[27] do not converge well due to their large approximation errors.", "startOffset": 190, "endOffset": 194}, {"referenceID": 24, "context": "Although the ADMM is widely known as a robust method for computation errors in each iteration, optimization methods (including the ADMM) with the other fast singular value shrinkage methods [25]\u2013[27] do not converge well due to their large approximation errors.", "startOffset": 195, "endOffset": 199}, {"referenceID": 36, "context": "The preliminary version of this study, without using signal sparsity, analysis of our method, and new applications, has previously been published [42].", "startOffset": 146, "endOffset": 150}, {"referenceID": 25, "context": "Chebyshev polynomial approximation [29]\u2013[31] gives an approximate solution of h(x) by using the truncated Chebyshev series:", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "The CPA of the matrix form [30], [34], [36], [37] gives an approximated solution of the eigenvalue shrinkage function H(\u00b7) by using truncated Chebyshev series as \u0124(A) := 1 2 \u01090 Id + \u03b1\u22121 \u2211", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "The CPA of the matrix form [30], [34], [36], [37] gives an approximated solution of the eigenvalue shrinkage function H(\u00b7) by using truncated Chebyshev series as \u0124(A) := 1 2 \u01090 Id + \u03b1\u22121 \u2211", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "The CPA of the matrix form [30], [34], [36], [37] gives an approximated solution of the eigenvalue shrinkage function H(\u00b7) by using truncated Chebyshev series as \u0124(A) := 1 2 \u01090 Id + \u03b1\u22121 \u2211", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "The eigenvalue shrinkage in (5) can be extended to G(B) in (13) as [35] G(B) = BH(B>B), (14)", "startOffset": 67, "endOffset": 71}, {"referenceID": 29, "context": "Note that [35] aims to calculate a vector represented as x\u0302 = BH(B>B)x, (17)", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "The CPA is applied to H(B>B)x to quickly derive x\u0302 in [35].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": ", T is considered as the discrete Fourier transform [43], discrete cosine transform (DCT) [44], and discrete wavelet transform (DWT) [45].", "startOffset": 52, "endOffset": 56}, {"referenceID": 38, "context": ", T is considered as the discrete Fourier transform [43], discrete cosine transform (DCT) [44], and discrete wavelet transform (DWT) [45].", "startOffset": 90, "endOffset": 94}, {"referenceID": 39, "context": ", T is considered as the discrete Fourier transform [43], discrete cosine transform (DCT) [44], and discrete wavelet transform (DWT) [45].", "startOffset": 133, "endOffset": 137}, {"referenceID": 0, "context": "where \u03c4hard is an arbitrary real value and x \u2208 [0, 1].", "startOffset": 47, "endOffset": 53}, {"referenceID": 27, "context": "2(b), the approximated response \u0125hard(x) has ripples, which is widely known in digital filter design [32], [46]\u2013[49].", "startOffset": 101, "endOffset": 105}, {"referenceID": 40, "context": "2(b), the approximated response \u0125hard(x) has ripples, which is widely known in digital filter design [32], [46]\u2013[49].", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "Even among them, minimax polynomial approximation [46], [47], [50]\u2013[52] and least squares approximation [53] are well known as the best approximation in the sense of the minimization of the infinity norm and the least squares error w.", "startOffset": 50, "endOffset": 54}, {"referenceID": 41, "context": "Even among them, minimax polynomial approximation [46], [47], [50]\u2013[52] and least squares approximation [53] are well known as the best approximation in the sense of the minimization of the infinity norm and the least squares error w.", "startOffset": 56, "endOffset": 60}, {"referenceID": 44, "context": "Even among them, minimax polynomial approximation [46], [47], [50]\u2013[52] and least squares approximation [53] are well known as the best approximation in the sense of the minimization of the infinity norm and the least squares error w.", "startOffset": 67, "endOffset": 71}, {"referenceID": 45, "context": "Even among them, minimax polynomial approximation [46], [47], [50]\u2013[52] and least squares approximation [53] are well known as the best approximation in the sense of the minimization of the infinity norm and the least squares error w.", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": ", the exact partial singular value decomposition (PSVD) based method and fast singular value shrinkage methods [25]\u2013[27], in Section V-F.", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": ", the exact partial singular value decomposition (PSVD) based method and fast singular value shrinkage methods [25]\u2013[27], in Section V-F.", "startOffset": 116, "endOffset": 120}, {"referenceID": 39, "context": "The DWT [45] was used in (22) to sparsify the signals.", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "The proximity operator [54] of a function f \u2208\u03930(R ) of index \u03b3>0 is defined as prox\u03b3f : R \u2192 R : x 7\u2192 arg min y\u2208RN f(y) + 1 2\u03b3 \u2016x\u2212 y\u2016.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": ", prox\u03b3\u2016\u00b7\u2016\u2217 , the proximity operator can be calculated by singular value shrinkage with the thresholding parameter \u03b3 [2].", "startOffset": 117, "endOffset": 120}, {"referenceID": 35, "context": "2) Alternating Direction Method of Multipliers: The ADMM [41] is an algorithm for solving a convex optimization problem represented as min x\u2208Rn1 ,z\u2208Rn2 f(x) + g(z) s.", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "We recall a convergence analysis of the ADMM by Eskstein and Bertsekas [41].", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "Fact 1 (Convergence of the ADMM [41]): Consider Prob.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "Texture Image Inpainting [12], [13] The objective with this application is to recover a missing region (as shown in the later Fig.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "Texture Image Inpainting [12], [13] The objective with this application is to recover a missing region (as shown in the later Fig.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Additionally, the set of a normalized dynamic range constraint is defined as D :={x :=[xi] i=1| xi\u2208 [0, 1]}.", "startOffset": 100, "endOffset": 106}, {"referenceID": 12, "context": "Background Modeling of Video [14]\u2013[18] The objective with this application is to divide a video sequence into background and object sequences (as shown in Fig.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Background Modeling of Video [14]\u2013[18] The objective with this application is to divide a video sequence into background and object sequences (as shown in Fig.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "Existing method (see Section V-F for the explanation) Used algorithms Exact PSVD FRSVS [25] NSVS [26] FSVS [27] Total computation time (s) 1935.", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "Existing method (see Section V-F for the explanation) Used algorithms Exact PSVD FRSVS [25] NSVS [26] FSVS [27] Total computation time (s) 1935.", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "Existing method (see Section V-F for the explanation) Used algorithms Exact PSVD FRSVS [25] NSVS [26] FSVS [27] Total computation time (s) 1935.", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "1) Effects of Selected Transform Matrix: We compared the DWT with the DCT and the block diagonal forms of the DCT (block DCT) [32] whose block size was 8\u00d7 8 for indicating the differences among chosen transform matrices T in (22).", "startOffset": 126, "endOffset": 130}, {"referenceID": 22, "context": "Comparison with Existing Methods As previously mentioned, there are several fast singular value shrinkage methods [25]\u2013[27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "Comparison with Existing Methods As previously mentioned, there are several fast singular value shrinkage methods [25]\u2013[27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "To illustrate the advantage of our method, we compared it with the singular value shrinkage by using the exact PSVD, fast randomized singular value shrinkage (FRSVS) [25], singular value shrinkage by using the Nystr\u00f6m method (NSVS) [26], and the fast singular value shrinkage without the exact SVD (FSVS) [27].", "startOffset": 166, "endOffset": 170}, {"referenceID": 23, "context": "To illustrate the advantage of our method, we compared it with the singular value shrinkage by using the exact PSVD, fast randomized singular value shrinkage (FRSVS) [25], singular value shrinkage by using the Nystr\u00f6m method (NSVS) [26], and the fast singular value shrinkage without the exact SVD (FSVS) [27].", "startOffset": 232, "endOffset": 236}, {"referenceID": 24, "context": "To illustrate the advantage of our method, we compared it with the singular value shrinkage by using the exact PSVD, fast randomized singular value shrinkage (FRSVS) [25], singular value shrinkage by using the Nystr\u00f6m method (NSVS) [26], and the fast singular value shrinkage without the exact SVD (FSVS) [27].", "startOffset": 305, "endOffset": 309}, {"referenceID": 22, "context": "All preferences of the FRSVS [25] was determined in the original code12 provided by the authors.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "preferences used in the FSVS were directly used as suggested in [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Practically, \u03a0I(\u00b7) is given by maintaining the assigned pixels, and \u03a0D(\u00b7) is calculated by pushing the entries outside [0, 1] into 0 or 1 (the nearest is chosen).", "startOffset": 119, "endOffset": 125}], "year": 2017, "abstractText": "We propose an approximation method for thresholding of singular values using Chebyshev polynomial approximation (CPA). Many signal processing problems require iterative application of singular value decomposition (SVD) for minimizing the rank of a given data matrix with other cost functions and/or constraints, which is called matrix rank minimization. In matrix rank minimization, singular values of a matrix are shrunk by hard-thresholding, softthresholding, or weighted soft-thresholding. However, the computational cost of SVD is generally too expensive to handle high dimensional signals such as images; hence, in this case, matrix rank minimization requires enormous computation time. In this paper, we leverage CPA to (approximately) manipulate singular values without computing singular values and vectors. The thresholding of singular values is expressed by a multiplication of certain matrices, which is derived from a characteristic of CPA. The multiplication is also efficiently computed using the sparsity of signals. As a result, the computational cost is significantly reduced. Experimental results suggest the effectiveness of our method through several image processing applications based on matrix rank minimization with nuclear norm relaxation in terms of computation time and approximation precision.", "creator": "LaTeX with hyperref package"}}}