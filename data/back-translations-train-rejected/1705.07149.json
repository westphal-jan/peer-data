{"id": "1705.07149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Local Information with Feedback Perturbation Suffices for Dictionary Learning in Neural Circuits", "abstract": "While the sparse coding principle can successfully model information processing in sensory neural systems, it remains unclear how learning can be accomplished under neural architectural constraints. Feasible learning rules must rely solely on synaptically local information in order to be implemented on spatially distributed neurons. We describe a neural network with spiking neurons that can address the aforementioned fundamental challenge and solve the L1-minimizing dictionary learning problem, representing the first model able to do so. Our major innovation is to introduce feedback synapses to create a pathway to turn the seemingly non-local information into local ones. The resulting network encodes the error signal needed for learning as the change of network steady states caused by feedback, and operates akin to the classical stochastic gradient descent method.", "histories": [["v1", "Fri, 19 May 2017 19:06:27 GMT  (725kb,D)", "http://arxiv.org/abs/1705.07149v1", "10 pages, 4 figures"]], "COMMENTS": "10 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["tsung-han lin"], "accepted": false, "id": "1705.07149"}, "pdf": {"name": "1705.07149.pdf", "metadata": {"source": "CRF", "title": "Local Information with Feedback Perturbation Suffices for Dictionary Learning in Neural Circuits", "authors": ["Tsung-Han Lin"], "emails": ["tsung-han.lin@intel.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it will be able to find a solution that adapts to the needs of the people."}, {"heading": "2 Integrate-and-Fire Neuron Model", "text": "First, we consider a network of N simple integrate-and-fire neurons. Each neuron-i, for i = 1, 2,., N, has two internal state variables, the soma current \u00b5i (t) and the membrane potential vi (t), which regulates its dynamics. Soma current is determined by two inputs: the first is a constant current bi; the second is the peak traits of all adjacent neuron-js to which neuron-i is connected. Each peak tract is of the form \u03c3j (t) = [t \u2212 tj, k), where tj, k is the time of the k-th peaks of neuron-j and g (t) is the Dirac delta function. The soma current \u00b5i (t) is the sum of bi and the filtered peak traits neighbors."}, {"heading": "3 Nonnegative Sparse Coding", "text": "Solving the sparse coding problem under a given dictionary is an important part of our learning scheme. (In this section, we will look at previous results for solving this problem in an SNN [17, 18], and we will focus on the use of non-negative dictionaries. Let's look at the network topology in Figure 1 (a). Each neuron-i receives an input current bi and has incoming synapses weighted wij from the other (N \u2212 1) neurons. Suppose that the synapses are all inhibitory, that is, wij \u2264 0, and therefore none of the neurons can arbitrarily spike fast. (2,5) The equilibrium spike rates a-i = limt \u2192 ai (t) must be {bi + j 6 = convergence-j 6 = i-wija = i-wija = i-wija-k."}, {"heading": "4 Online Dictionary Learning", "text": "We are interested in learning a non-negative dictionary from non-negative P-training data examples x1, x2, xP. The dictionary-learning problem is generally formulated as follows: min D \u2265 0, \u0441dj 22 = 1, \u2265 01P \u2211 P i = 1 L (xi, D, ai), L (x, D, a) = 1 x x - Da 22 + \u03bb n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-g-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n"}, {"heading": "4.1 A Two-Layer Network for Dictionary Learning", "text": "Consider the network topology in a constant rate of 1 (b), which consists of two levels of neurons, an input layer of M neurons at the bottom and a sparsely coded layer of N neurons at the top. There are four groups of synaptic weights: the excitatory challenges and the feedback synapses, F-RN-RN-RN-RN-1, and W-RA, L-RA, B-RA, B-RA, E-RA, E-RA, E-RA, E-RA, E-RA, E-RA, E-RA, E-RA, E-RA, E-RA, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, RA, A, A, A, A, RA, A, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, A, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA, RA,"}, {"heading": "4.2 Synaptically Local Learning", "text": "In the second stage of learning, the so-called feedback stage, we set a non-zero value to activate the feedback synapses and move the network toward a new steady state. Interestingly, there is a condition that when we include the feedback synapses, only the equilibrium peaks at the input level are disturbed (leaving the sparse layer of code untouched). \u2212 w1N \u2212 w2N \u2212 w2N \u2212 w2N.... \u2212 wN1 \u2212 wN2 \u03b8N (4.4) Note that H is composed of the lateral weights weights and shot thresholds. To see this, we leave y RM, z, z, RN, which are the equilibrium peaks at the input and output levels, respectively. The equilibrium peaks at the input level can be easily derived."}, {"heading": "5 Numerical Simulations", "text": "We examined the proposed learning algorithm simply as the total number of spikes within the 20 time frame. We deliberately chose a short time span (which has only one time frame). We examined the proposed learning algorithm using three standard datasets in image processing, machine learning, and computer-aided neuroscience. Dataset B. 28 x 28 MNIST images [10] to learn 512 atoms. Dataset C. Randomical Sampled 16 x 16 patches from white natural scenes [15] to learn 1024 atoms, the patches are further subtracted, standardized, and divided into positive and negative channels to generate non-negative inputs [8]. The spinning networks are executed at a time step of 1 / 32. For each input, the feedback stage runs from 0 to t = 20 and the feedback stage runs from t = 20 to t = 40, and the spike rates are simply measured as the total number of spikes within the 20 time frame."}, {"heading": "6 Discussion", "text": "In this context, it should be noted that the solution to the problems is a purely theoretical solution, capable of resolving the problems that have arisen in recent years."}, {"heading": "Acknowledgments", "text": "The author thanks Peter Tang, Javier Turek, Narayan Srinivasa and Stephen Tarsa for the insightful discussion and feedback on the manuscript and Hong Wang for the encouragement and support."}], "references": [{"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on signal processing, 54(11):4311\u20134322", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast convolutional sparse coding", "author": ["H. Bristow", "A. Eriksson", "S. Lucey"], "venue": "CVPR, pages 391\u2013398", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear hebbian learning as a unifying principle in receptive field formation", "author": ["C.S.N. Brito", "W. Gerstner"], "venue": "PLoS Comput Biol, 12(9):1\u201324", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Mirrored stdp implements autoencoder learning in a network of spiking neurons", "author": ["K.S. Burbank"], "venue": "PLoS Comput Biol, 11(12):e1004566", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A common network architecture efficiently implements a variety of sparsity-based inference problems", "author": ["A.S. Charles", "P. Garrigues", "C.J. Rozell"], "venue": "Neural computation, 24(12):3317\u20133339", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Group sparse coding with a laplacian scale mixture prior", "author": ["P. Garrigues", "B.A. Olshausen"], "venue": "Advances in neural information processing systems, pages 676\u2013684", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning representations by recirculation", "author": ["G.E. Hinton", "J.L. McClelland"], "venue": "Neural information processing systems, pages 358\u2013366", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Non-negative matrix factorization with sparseness constraints", "author": ["P.O. Hoyer"], "venue": "Journal of machine learning research, 5(Nov):1457\u20131469", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization", "author": ["T. Hu", "C. Pehlevan", "D.B. Chklovskii"], "venue": "2014 48th Asilomar Conference on Signals, Systems and Computers, pages 613\u2013619. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Sparse modeling for image and vision processing", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "Foundations and Trends R  \u00a9 in Computer Graphics and Vision, 8(2-3):85\u2013283", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 689\u2013696. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura"], "venue": "A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197):668\u2013673", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Contrastive hebbian learning in the continuous hopfield model", "author": ["J.R. Movellan"], "venue": "Connectionist models: Proceedings of the 1990 summer school, pages 10\u201317", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Nature, 381:13", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Sparse coding via thresholding and local competition in neural circuits", "author": ["C.J. Rozell", "D.H. Johnson", "R.G. Baraniuk", "B.A. Olshausen"], "venue": "Neural computation, 20(10):2526\u20132563", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimal sparse approximation with integrate and fire neurons", "author": ["S. Shapero", "M. Zhu", "J. Hasler", "C. Rozell"], "venue": "International journal of neural systems, 24(05):1440001", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse coding by spiking neural networks: Convergence theory and computational results", "author": ["P.T.P. Tang", "T.-H. Lin", "M. Davies"], "venue": "ArXiv e-prints", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised learning of an efficient short-term memory network", "author": ["P. Vertechi", "W. Brendel", "C.K. Machens"], "venue": "Advances in Neural Information Processing Systems, pages 3653\u20133661", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Spike-based learning rules and stabilization of persistent neural activity", "author": ["X. Xie", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems, pages 199\u2013208", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "CVPR, pages 2528\u20132535. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "J. Royal Statist. Soc B., 67:301\u2013320", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of v1 simple cell receptive fields", "author": ["J. Zylberberg", "J.T. Murphy", "M.R. DeWeese"], "venue": "PLoS Comput Biol, 7(10):e1002250", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 16, "context": "Using the above principle, one can show that a SNN can be configured to efficiently solve the well-known `1-minimizing sparse approximation problem [17, 18], which is to determine a sparse subset of features from a feature dictionary to represent a given input, and the features can be viewed as competing neurons that seek to form the best fit of the input data [16].", "startOffset": 148, "endOffset": 156}, {"referenceID": 17, "context": "Using the above principle, one can show that a SNN can be configured to efficiently solve the well-known `1-minimizing sparse approximation problem [17, 18], which is to determine a sparse subset of features from a feature dictionary to represent a given input, and the features can be viewed as competing neurons that seek to form the best fit of the input data [16].", "startOffset": 148, "endOffset": 156}, {"referenceID": 15, "context": "Using the above principle, one can show that a SNN can be configured to efficiently solve the well-known `1-minimizing sparse approximation problem [17, 18], which is to determine a sparse subset of features from a feature dictionary to represent a given input, and the features can be viewed as competing neurons that seek to form the best fit of the input data [16].", "startOffset": 363, "endOffset": 367}, {"referenceID": 14, "context": "Dictionary learning was first proposed to model mammalian visual cortex [15], and later found numerous applications in image processing and machine learning [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Dictionary learning was first proposed to model mammalian visual cortex [15], and later found numerous applications in image processing and machine learning [11].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": ", minimizing over long-term average neuron activities [23], or maximizing input-output similarity [9]), or are forced to take approximate gradient directions at the cost of suboptimal results (e.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": ", minimizing over long-term average neuron activities [23], or maximizing input-output similarity [9]), or are forced to take approximate gradient directions at the cost of suboptimal results (e.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": ", simplifying the learning rules to be only Hebbian [3, 19]).", "startOffset": 52, "endOffset": 59}, {"referenceID": 18, "context": ", simplifying the learning rules to be only Hebbian [3, 19]).", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "If the average current converges to a fixed point, one can show that as t \u2192 \u221e, the imbalance converges towards satisfying the following equilibrium condition [18], { limt\u2192\u221e\u2206i(t) = 0, if limt\u2192\u221e ai(t) > 0 limt\u2192\u221e\u2206i(t) \u2264 0, if limt\u2192\u221e ai(t) = 0 (2.", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": ", see [17, 18]), although it is not the main focus of this work.", "startOffset": 6, "endOffset": 14}, {"referenceID": 17, "context": ", see [17, 18]), although it is not the main focus of this work.", "startOffset": 6, "endOffset": 14}, {"referenceID": 16, "context": "In this section, we revisit prior results on solving this problem in a SNN [17, 18], and we will focus on using nonnegative dictionaries.", "startOffset": 75, "endOffset": 83}, {"referenceID": 17, "context": "In this section, we revisit prior results on solving this problem in a SNN [17, 18], and we will focus on using nonnegative dictionaries.", "startOffset": 75, "endOffset": 83}, {"referenceID": 9, "context": "28 \u00d7 28 MNIST images [10] to learn 512 atoms.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Randomly sampled 16\u00d716 patches from whitened natural scenes [15] to learn 1024 atoms.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "For Dataset A and C, the patches are further subtracted by the means, normalized, and split into positive and negative channels to create nonnegative inputs [8].", "startOffset": 157, "endOffset": 160}, {"referenceID": 19, "context": "11) share the same form with differential Hebbian and anti-Hebbian plasticity, whose link to STDP has been shown [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "In autoencoder learning, [7, 4] similarly explored using feedback synapses for gradient computations.", "startOffset": 25, "endOffset": 31}, {"referenceID": 3, "context": "In autoencoder learning, [7, 4] similarly explored using feedback synapses for gradient computations.", "startOffset": 25, "endOffset": 31}, {"referenceID": 13, "context": "This strategy is actually a form of contrastive Hebbian learning [14] in that the feedback synapses serve to bring the network from its \u201cfree state\u201d to a \u201cclamped state\u201d.", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": ", [13], where each neuron can be implemented as a processing element.", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": ", [1, 12], can be accelerated by exploiting data parallelism, while it is less clear how to parallelize them within a single training sample to further reduce computation latency.", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": ", [1, 12], can be accelerated by exploiting data parallelism, while it is less clear how to parallelize them within a single training sample to further reduce computation latency.", "startOffset": 2, "endOffset": 9}, {"referenceID": 5, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 102, "endOffset": 105}, {"referenceID": 21, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 132, "endOffset": 139}, {"referenceID": 17, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 132, "endOffset": 139}, {"referenceID": 20, "context": "It can also be extended to be a parallel solver for convolutional sparse coding [21, 2].", "startOffset": 80, "endOffset": 87}, {"referenceID": 1, "context": "It can also be extended to be a parallel solver for convolutional sparse coding [21, 2].", "startOffset": 80, "endOffset": 87}], "year": 2017, "abstractText": "While the sparse coding principle can successfully model information processing in sensory neural systems, it remains unclear how learning can be accomplished under neural architectural constraints. Feasible learning rules must rely solely on synaptically local information in order to be implemented on spatially distributed neurons. We describe a neural network with spiking neurons that can address the aforementioned fundamental challenge and solve the `1-minimizing dictionary learning problem, representing the first model able to do so. Our major innovation is to introduce feedback synapses to create a pathway to turn the seemingly non-local information into local ones. The resulting network encodes the error signal needed for learning as the change of network steady states caused by feedback, and operates akin to the classical stochastic gradient descent method.", "creator": "LaTeX with hyperref package"}}}