{"id": "1705.07175", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Espresso: Efficient Forward Propagation for BCNNs", "abstract": "There are many applications scenarios for which the computational performance and memory footprint of the prediction phase of Deep Neural Networks (DNNs) needs to be optimized. Binary Neural Networks (BDNNs) have been shown to be an effective way of achieving this objective. In this paper, we show how Convolutional Neural Networks (CNNs) can be implemented using binary representations. Espresso is a compact, yet powerful library written in C/CUDA that features all the functionalities required for the forward propagation of CNNs, in a binary file less than 400KB, without any external dependencies. Although it is mainly designed to take advantage of massive GPU parallelism, Espresso also provides an equivalent CPU implementation for CNNs. Espresso provides special convolutional and dense layers for BCNNs, leveraging bit-packing and bit-wise computations for efficient execution. These techniques provide a speed-up of matrix-multiplication routines, and at the same time, reduce memory usage when storing parameters and activations. We experimentally show that Espresso is significantly faster than existing implementations of optimized binary neural networks ($\\approx$ 2 orders of magnitude). Espresso is released under the Apache 2.0 license and is available at", "histories": [["v1", "Fri, 19 May 2017 20:29:42 GMT  (1015kb,D)", "http://arxiv.org/abs/1705.07175v1", "10 pages, 4 figures"]], "COMMENTS": "10 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.DC cs.CV cs.LG cs.NE", "authors": ["fabrizio pedersoli", "george tzanetakis", "andrea tagliasacchi"], "accepted": false, "id": "1705.07175"}, "pdf": {"name": "1705.07175.pdf", "metadata": {"source": "CRF", "title": "Espresso: Efficient Forward Propagation for BCNN", "authors": ["Fabrizio Pedersoli", "George Tzanetakis"], "emails": ["fpeder@uvic.ca", "gtzan@uvic.ca", "ataiya@uvic.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to go in search of a solution that fulfills its purpose, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of solutions, most of them are able to find a solution for themselves."}, {"heading": "2 Related Works", "text": "At the hardware level, chipsets dedicated to DNN execution can outperform universal CPUs / GPUs [16]. At the software level, the network can be simplified to increase performance, with a common approach being to penalize the total number of non-zero weights (i.e. connections) via a modified loss function [5]. Another recently proposed approach is the quantification of linear algebra operations, which can be performed more efficiently. In quantified networks, the goal is to train DNNs whose (quantified) weights do not significantly affect the precision of the network (i.e., classification accuracy). For example, Courbariaux operations et. [6] show that 10 bits are sufficient for DNN networks."}, {"heading": "3 The Espresso Framework", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "4 Binary Deep Neural Networks (BDNN) \u2013 Hubara et al. [14]", "text": "In this section, we give an overview of the basic features of BDNNs that form the basis of espresso design. In Binary DNNs, compute-intensive FMA operations are replaced by XNOR (for multiplications) and Bitcount (for additions), allowing significant computational acceleration. Specifically, XNOR is a simpler machine instruction than floating-point multiplication, and therefore achieves much higher throughput on many architectures. More importantly, a single XNOR step can execute several 64-bit-wide blocks of dot products, further increasing overall computing efficiency. Below, we describe how to binarize a network, detail a compressed memory layout that allows for efficient execution of dot products, show how to reinterpret input data to allow execution of dot products with fixed precision (e.g. images), and give some notes on the training sequence."}, {"heading": "4.1 Network binarization", "text": "A BDNN consists of a sequence of k = 1,.., L layers whose weights W bk and activations a b k are binarized to the values {\u2212 1, + 1}. The high sentence b in the notation indicates binary quantities. Weights and activations are {\u2212 1, + 1}, but on the hardware level they must be encoded as {0, 1}. Our convention is the encoding \u2212 1 \u2192 0 and + 1 \u2192 1. Among many possible options, e.g. stochastic binarization [7], due to their efficient implementation, we use the following activation function: xb = character (x) = {+ 1 x \u2265 0 \u2212 1 otherwise (1)"}, {"heading": "4.2 Bit-packing", "text": "The weights of a BDNN can be stored in the bits of a 64-bit word. An immediate advantage of bit packing is the drastic reduction of memory consumption by a factor of 32. An even more significant advantage is the ability to process multiple values simultaneously via registers. This is particularly useful for point products: With bit packing, we can calculate a point product of 64 element vectors by using only one XNOR and one bit counting. Furthermore, modern computer architectures provide hardware instructions for counting the number of bits set to 1 in a word. If one assumes binary vectors a, b-B1 \u00b7 N, where N is a multiple of 64, the point product then corresponds to: a \u00b7 b-B-N-N / 64 \u2211 i = 1 bit number (XNOR (ai, bi) 1, a b (2), where the bit shift operator is represented by: a simple way, this is calculated according to DNx and NDN is optimized."}, {"heading": "4.3 Input data binarization", "text": "BDNNs require binary input data that is not typically available on the first level of the network, but the input data is usually in a fixed precision format (e.g. 8-bit / channel in RGB images), so the optimized calculation of dot products can still be applied if we split the input data by bit level and then add each post according to its weight. For example, if we specify the nth bit of a fixed precision vector with < a > n and the corresponding bit level with i, we get: a \u00b7 b \u0445 n \u2212 1 \u0445 i = 0 2i < a b > i (3)"}, {"heading": "4.4 Training", "text": "When forming a BDNN, it is important to note that the gradient is calculated with binary weights but accumulated with floating point accuracy. [14] This is because the optimizer needs sufficient precision to perform a reliable update. Furthermore, the derivative of the drawing function, which is almost always zero, cannot be used for backpropagation. To solve these problems, the simple estimator [3] is used, where 1 is backpropagated if the floating point argument | x | \u2264 1 and otherwise 0. Finally, weights are shortened to [\u2212 1, 1] during training to avoid large growth of floating point weights, which would have no effect on binary weights."}, {"heading": "5 Espresso architecture", "text": "The main components of our framework are tensors, layers and the network. These components are hierarchically organized. Tensors are n-dimensional matrices that are used to store inputs, weights and activations (outputs). One layer processes an input tensor and generates an output tensor, while a network consists of a concatenation of layers."}, {"heading": "5.1 Tensors", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "6 Evaluation", "text": "Execution times, averaging over 100 experiments, are determined on a machine equipped with an NVIDIA GeForce GTX 960 with 2GB RAM and an Intel R \u00a9 dual-Xeon R \u00a9 X5660 @ 2.80 GHz. In CPU mode, we configure the OpenBLAS matrix multiplication library to use all 24 available cores. Experimental design. We perform three quantitative evaluations: (Section 6.1) Matrix multiplications of two dense square matrices of size 8192 \u00d7 8192; (Section 6.2) Forward propagation of a MultiLayer Perceptron (MLP) trained on the MNIST dataset [19]; (Section 6.3) Forward implementations of a Convolutional Neural Network (CNN) trained on the CIFAR-10 dataset. [17] We compare the net data [19] (so)."}, {"heading": "6.1 Dense matrix multiplication \u2013 Figure 1a", "text": "Espresso outperforms BinaryNet by an \u2248 8 factor when multiplying dense matrices. Much of the increase can be attributed to our optimized cores and the use of register blocking: by retrieving larger amounts of data from main memory and shared memory, our kernel increases bandwidth usage by reducing the number of memory commands; using 64-bit packets instead of the 32-bit packets (such as BinaryNet's) results in additional performance improvements; the 64-bit kernel achieves 40 GB / s of memory DRAM throughput when reading and 5 GB / s when writing, while the 32-bit kernel receives 29.6 GB / s when reading and 3.6 GB / s when writing."}, {"heading": "6.2 Multi-layer perceptron on MNIST \u2013 Figure 1b and Figure 1d", "text": "We evaluate the average execution time of the classification via the MNIST database, in which we trained the MLP architecture of [8, Sec 2.1] with the sources provided by the author, and then converted it into the Espresso format. In Figure 1b, Espresso achieves a consistent acceleration of 68 \u00d7 compared to BinaryNet. Since the Nervana / Neon implementation of the binary network is a BinaryNet derivative, it is affected by the same disadvantages of BinaryNet and therefore achieves comparable performance. Both alternatives have the additional cost of running CUDA by Python / Theano, which can introduce further latency into the process. In Figure 1b, the evaluation of the three variants of Espresso shows the expected result, with the GPU implementation leading the ranking. Note that we are able to achieve an acceleration of the XVIA GT960 GT960."}, {"heading": "6.3 Convolutional Neural Network on CIFAR-10 \u2013 Figure 1c", "text": "To the best of our knowledge, no BDNN implementation of binary optimized CNN layers is publicly available. Our self-evaluation implements the VGGNet-like CNN architecture by Hubara et al. [14, paragraph 2.3] and evaluates it using our three modalities: As expected, the GPU implementation achieves significantly better performance. Unrolling and pooling. Note how the GPU implementation provides a slightly better improvement over the CPU in terms of the MLP test, with \u2248 16 acceleration. In this experiment, the inherent deployment and pooling of the GPU, as well as the higher memory throughput of the GPU, leads to this behavior. The benefits are marginal, as the FMA still represents the computorial bottleneck. Bit packing. The GPU implementation results in an approximately 5-fold increase in performance over the GPU. These increases, to optimization milebinations, are significantly larger than the 6.P output, therefore, which is lower than the 6.P output."}, {"heading": "7 Conclusions", "text": "In this article, we introduced Espresso, a highly optimized forward propagation framework for both traditional DNNs and BCNNs that supports heterogeneous deployment on both CPU and GPU. While BinaryNet and Nervana / Neon BDNN implementations are limited to MLP networks, our framework also supports the popular CNN, while outperforming state-of-the-art implementations of MLP networks. Espresso is highly efficient, lightweight and self-contained. On the GPU side, calculation is done using specially designed CUDA kernels, which, combined with a more careful approach to memory allocation and bit packing, allow us to significantly improve performance."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "There are many applications scenarios for which the computational performance<lb>and memory footprint of the prediction phase of Deep Neural Networks (DNNs)<lb>needs to be optimized. Binary Neural Networks (BDNNs) have been shown to be<lb>an effective way of achieving this objective. In this paper, we show how Convolu-<lb>tional Neural Networks (CNNs) can be implemented using binary representations.<lb>Espresso is a compact, yet powerful library written in C/CUDA that features all the<lb>functionalities required for the forward propagation of CNNs, in a binary file less<lb>than 400KB, without any external dependencies. Although it is mainly designed<lb>to take advantage of massive GPU parallelism, Espresso also provides an equiv-<lb>alent CPU implementation for CNNs. Espresso provides special convolutional<lb>and dense layers for BCNNs, leveraging bit-packing and bit-wise computations for<lb>efficient execution. These techniques provide a speed-up of matrix-multiplication<lb>routines, and at the same time, reduce memory usage when storing parameters<lb>and activations. We experimentally show that Espresso is significantly faster than<lb>existing implementations of optimized binary neural networks (\u2248 2 orders of<lb>magnitude). Espresso is released under the Apache 2.0 license and is available at<lb>http://github.com/organization/project.", "creator": "LaTeX with hyperref package"}}}