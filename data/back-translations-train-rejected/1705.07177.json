{"id": "1705.07177", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Model-Based Planning in Discrete Action Spaces", "abstract": "Planning actions using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete, which may have limited its adoption. In this work, we introduce two discrete planning tasks inspired by existing question-answering datasets and show that it is in fact possible to effectively perform planning via backprop in discrete action spaces using two simple yet principled modifications. Our experiments show that this approach can significantly outperform model-free RL based methods and supervised imitation learners.", "histories": [["v1", "Fri, 19 May 2017 20:38:49 GMT  (126kb,D)", "http://arxiv.org/abs/1705.07177v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mikael henaff", "william f whitney", "yann lecun"], "accepted": false, "id": "1705.07177"}, "pdf": {"name": "1705.07177.pdf", "metadata": {"source": "CRF", "title": "Model-Based Planning in Discrete Action Spaces", "authors": ["Mikael Henaff", "William F. Whitney"], "emails": ["mbh305@nyu.edu", "ww1114@nyu.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Planning with Forward Models", "text": "The planning with a forward model consists of two phases. This is done by the following optimization: A forward model of the world is constructed by observing how the environment reacts to actions. Specifically, the agent is confronted with triplets (s0, a, s) from the environment, each consisting of an initial state s0, an action a, and a final state s (s0, s), which can represent individual instances or sequences of actions or states. The agent learns a forward model f (s0, a) with comprehensible parameters to predict a state that minimizes a loss function. L (s, s) the measurement of the discrepancy between the predicted state and the observed state: poorly minuted state E (s0, a, s). L (s0, a, s). After the training, the agent can use the forward model to achieve a desired final state."}, {"heading": "3 Tasks and Architectures", "text": "In this section, we present two separate planning tasks inspired by question and answer data sets, and the different methods we use to solve them. Question-answer text tasks are a common benchmark in machine learning [34, 15, 14]. In these tasks, the model is given triplets that consist of a story (or context), a question, and an answer, and it is trained to predict the correct answer in terms of the question and context. [34] Several of the bAbI tasks introduced in [34] consist of stories that describe the actions of one or more actors acting in the world, and the question requires the model to answer about an aspect of the world that depends on the actions of the agent. The question, along with its answer, can be regarded as an aspect of the final state of history that we call \"s,\" and the story can consist of a sequence of actions. Tasks of this kind can be directly adapted to become planning tasks."}, {"heading": "3.1 Navigation Task", "text": "The first task we introduce is a navigation task and is inspired by the task of the world model in [13]. In this scenario, an agent moves on a 2D grid, and the model must learn to plan action sequences to move the agent from one point to another. The agent can perform one of 10 actions: face in a certain direction (north, south, east, or west), move between one and five steps, or do nothing. We created action sequences as follows: The agent was first placed at a random location in a 15 x 15 grid, and then a number of actions was uniformly selected between 1 and a maximum length Tmax. For each action, the agent chooses either to change direction or to move with equal probability. If he changes direction, he randomly aligns himself with north, south, east, or west, again with equal probability. If he moves, he makes a number of steps between 1 and a maximum length Tmax."}, {"heading": "3.2 Transport Task", "text": "The second task we introduce requires that an agent travels to different locations to pick up objects. There are 4 objects and 8 locations. In the initial state, each object is placed in a specific place. At each subsequent time frame, the agent can either travel to a place or try to pick up the object at its current location. If the agent tries to pick up an object, if there is none, this action has no effect. At the end of the sequence, the model must indicate the current location of each object. Therefore, the model must learn to keep a list of objects that are in memory, track the location of the agent and update the list as a function of the actions of the agent. This task is related to bAbI tasks 2 and 8 (\"Lists and Sentences,\" \"\" Two supporting facts \") and requires up to four supporting facts: the original location of an object, whether the agent has moved to that place, and whether the agent has picked up the object while the agent is at that place, and the current location of the agent."}, {"heading": "3.3 Architectures", "text": "We will now describe the various methods we have tested to address these planning tasks; more details are given in the appendix. As a future model, we used an RNN with 100 hidden units and parametric ReLU nonlinearity combined with a spherical normalization layer to prevent divergence of activations; the forward model was trained to minimize cross-entropy between its output at the end time and the correct end state; we then used this forward model to perform planning through backprop, calling it the forward strategy; our first starting line was a discrete Q-learning agent trained on a method similar to [26] target networks and replay memory; to encode stories in state vectors, we used the same recurring architecture as the forward model, in which we returned the hidden state at the last step of each sequence."}, {"heading": "4 Related Work", "text": "The idea of planning actions by reverse propagation along a political trajectory has existed since the 1960s [19, 7]. These methods were applied to environments where the dynamics of state transition of the environment were analytically known and reverse derivatives could be calculated precisely, such as the planning of flight paths. Later work [30, 27, 17] explored the idea of reverse propagation by scholarly, approximate forward models of the environment. Several recent work has focused on predictive modelling in more general environments such as complex physical systems [9, 5, 6, 22] and video [24, 18]. This work has largely focused on challenges associated with the formation of a precise forward model by designing model architectures with common parameters across different object instances or opposite loss functions. Other work used forward models for planning in continuous action spaces, such as billiards [9], vehicle navigation [12] or robotics [32] [1, 32]."}, {"heading": "5 Experiments", "text": "In this section, we describe the results of applying the Advance Planner (FP), the Q-Learner and the Imitation Learner to the two planning tasks. In the navigation task, performance is measured on the basis of accuracy, i.e. the proportion of time the model generates an action sequence that takes it to the specified location. In the transport task, the distribution is distorted towards sequences in which few objects have moved. Therefore, we calculate sensitivity (the proportion of objects that have been moved and that are correctly classified) and specificity (the proportion of objects that have not been moved and that are correctly classified).Since we found that all models had a high specificity (> 0.95), we give sensitivity as a measure of performance in the transport task, unless stated otherwise."}, {"heading": "5.1 Effect of Regularizers", "text": "Our first experiment compared the three different methods with distortions in relation to the most uniform vectors: input noise, gradient noise and entropy regulation. Results for several different hyperparameter values are shown in Table 2. In the navigation tasks, all three methods showed an improvement over the unregulated case (\u03b3 = 0). The input noise produced the greatest improvement, with the other two being approximately equivalent. In the transport task, entropy regulation did not help at all. Input noise and gradient noise showed similar best values, but the gradient noise was morerrobust compared to the hyperparameter \u03b3. Subsequently, we recorded the evolution of network loss and entropy during minimization for each of the methods, with the values of entropy providing the best performance (Figure 1). If no regulation is applied, entropy initially decreases, but then remains constant; all other control methods cause further reduction of the entropy throughout the process."}, {"heading": "5.2 Comparison to Baselines", "text": "In this context, it should be noted that the measures that have been taken are not measures that have been taken in the past, but measures that have been taken in the past."}, {"heading": "6 Conclusion", "text": "In this thesis, we introduced a simple but effective method of planning in discrete action spaces, using differentiated forward models of the environment. We addressed the problem of backpropagation into undefined regions by applying a Softmax paramaterization of action vectors combined with additional input noise, which we have theoretically and empirically demonstrated to produce solutions with low entropy, i.e. solutions that are close to valid actions. Interestingly, this method yields better results than the explicit introduction of an entropy penalty term. This could be due to the additional term in punishment caused by the additional noise that penalizes the curvature of the loss area, or because additional noise can help to escape local minima. A more thorough examination of this approach would be an interesting direction for future work. We also introduced two discrete planning tasks that build on previous frameworks to answer questions. Although none of the design tasks could be easily described by us, therefore none of them could be fully evaluated by us."}, {"heading": "6.1 Experimental Details", "text": "All methods were trained with ADAM [20] using size 32 minibatches for a maximum of two million updates; the forward models had an initial learning rate of 0.01, which proved to be good, and both Q-Learner and Imitation Learners had chosen their initial learning rate by grid search via the set {0.01, 0.0001, 0.0001}; for both the forward models and the Imitation Learner, we divided the learning rate by 2 during training, when the training loss did not decrease by a factor of 10 for more than three epochs, with one epoch to be updated to 2000. Due to the noise of the reward signal, we did not use this glow strategy for the Q-Learner, instead reducing the learning rate linearly by a factor of 10 over the course of one million updates; the Q-Learner's target network was updated after each 1000 lots of experience; the measures were chosen at training times using a greedy policy, starting at 0 = 10 and early interpolation at 0.5% / 0."}, {"heading": "6.2 Dataset Statistics", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["Pieter Abbeel", "Adam Coates", "Morgan Quigley", "Andrew Y Ng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["Guozhong An"], "venue": "Neural Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Efficient Reductions for Imitation Learning", "author": ["J Andrew Bagnell", "St\u00e9phane Ross"], "venue": "In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["Peter W. Battaglia", "Razvan Pascanu", "Matthew Lai", "Danilo Jimenez Rezende", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "A compositional object-based approach to learning physical dynamics", "author": ["Michael B. Chang", "Tomer Ullman", "Antonio Torralba", "Joshua B. Tenenbaum"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The numerical solution of variational problems", "author": ["Stuart Dreyfus"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1962}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Shixiang Gu Eric Jang", "Ben Poole"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["Katerina Fragkiadaki", "Pulkit Agrawal", "Sergey Levine", "Jitendra Malik"], "venue": "CoRR, abs/1511.07404,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "States versus rewards: Dissociable neural prediction error signals underlying model-based and model-free reinforcement learning", "author": ["Jan Glascher", "Nathaniel Daw", "Peter Dayan", "John P. O\u2019Doherty"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Deep reinforcement learning for robotic manipulation", "author": ["Shixiang Gu", "Ethan Holly", "Timothy P. Lillicrap", "Sergey Levine"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Metacontrol for adaptive imagination-based optimization", "author": ["Jessica Hamrick", "Andrew Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter Battaglia"], "venue": "ICLR 2017,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Tracking the world state with recurrent entity", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "venue": "networks. ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Don\u2019t until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["Alvin C. Grissom Ii", "He He", "John Morgan", "Hal Daume III"], "venue": "Proceedings of EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Forward models: Supervised learning with a distal teacher", "author": ["Michael I. Jordan", "David E. Rumelhart"], "venue": "Cognitive Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Video pixel networks", "author": ["Nal Kalchbrenner", "Aaron van den Oord", "Karen Simonyan", "Ivo Danihelka", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.00527,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Henry Kelley"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1960}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Optimal control with learned local models: Application to dexterous manipulation", "author": ["Vikash Kumar", "Emanuel Todorov", "Sergey Levine"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning physical intuition of block towers by example", "author": ["Adam Lerer", "Sam Gross", "Rob Fergus"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Micha\u00ebl Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "CoRR, abs/1511.05440,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "IEEE Spoken Language Technology Workshop (SLT), Miami, FL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Neural networks for control. chapter The Truck Backerupper: An Example of Self-learning in Neural Networks, pages 287\u2013299", "author": ["Derrick Nguyen", "Bernard Widrow"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1990}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Dean A. Pomerleau"], "venue": "Neural Computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1991}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Jurgen Schmidhuber"], "venue": "In Proceedings of the International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1990}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz"], "venue": "ICML, volume 37 of JMLR Workshop and Conference Proceedings,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems", "author": ["Emanuel Todorov", "Weiwei Li"], "venue": "In American Control Conference,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Towards ai-complete question answering: A set of prerequisite toy", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "tasks. CoRR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 27, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 187, "endOffset": 191}, {"referenceID": 15, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 217, "endOffset": 221}, {"referenceID": 30, "context": "However, it also suffers from several limitations, including a difficult temporal credit assignment problem, high sample complexity [31], and non-stationarity of the input distribution.", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "An alternative approach is imitation learning [29] where the agent is trained to predict actions of an expert given a state; this can be cast as a supervised learning problem.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "However, small inaccuracies can accumulate over time, leading to situations where the agent finds itself in situations not encountered during training [4].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "Furthermore, since the learner does not directly optimize a reward function and instead relies on the expert\u2019s policy to be close to optimal, in the absence of additional queries to the environment its performance will be bounded by that of the expert [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 9, "context": "Evidence from neuroscience also suggests that humans use forward models for certain decision-making tasks [10].", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "The work of [3] shows that adding independent noise to the weights of a neural network during training induces an additional term on the loss function.", "startOffset": 12, "endOffset": 15}, {"referenceID": 19, "context": "In the case of adaptive learning rates such as that used in ADAM [20], this corresponds to scaling the noise by the running average of the gradient magnitude, which may be desirable.", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "Textual question-answering tasks are a common benchmark in machine learning [34, 15, 14].", "startOffset": 76, "endOffset": 88}, {"referenceID": 14, "context": "Textual question-answering tasks are a common benchmark in machine learning [34, 15, 14].", "startOffset": 76, "endOffset": 88}, {"referenceID": 13, "context": "Textual question-answering tasks are a common benchmark in machine learning [34, 15, 14].", "startOffset": 76, "endOffset": 88}, {"referenceID": 33, "context": "Several of the bAbI tasks introduced in [34] consist of stories describing the actions of one or more agents acting in the world, and the question requires the model to produce an answer about some aspect of the world which depends on the agent\u2019s actions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "The first task we introduce is a navigation task, and is inspired by the World Model task in [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "Our first baseline was a Q-learning agent in the discrete space, trained via a method similar to [26] including target networks and replay memory.", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "As a second baseline, we trained a purely supervised Imitation Learner to predict each action conditioned on previous actions and the desired final state, using the Contextual RNN architecture introduced in [25].", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "The idea of planning actions by backpropagating along a policy trajectory has existed since the 1960\u2019s [19, 7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 6, "context": "The idea of planning actions by backpropagating along a policy trajectory has existed since the 1960\u2019s [19, 7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 29, "context": "Later works [30, 27, 17] explored the idea of backpropagating through learned, approximate forward models of the environment.", "startOffset": 12, "endOffset": 24}, {"referenceID": 26, "context": "Later works [30, 27, 17] explored the idea of backpropagating through learned, approximate forward models of the environment.", "startOffset": 12, "endOffset": 24}, {"referenceID": 16, "context": "Later works [30, 27, 17] explored the idea of backpropagating through learned, approximate forward models of the environment.", "startOffset": 12, "endOffset": 24}, {"referenceID": 8, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 4, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 5, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 21, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 23, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 138, "endOffset": 146}, {"referenceID": 17, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 138, "endOffset": 146}, {"referenceID": 8, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 96, "endOffset": 99}, {"referenceID": 11, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 32, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 0, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 31, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 20, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 22, "context": "There has been recent work in continuous relaxations of discrete random variables [23, 8], that also uses a softmax to form a continuous approximation to a discrete set.", "startOffset": 82, "endOffset": 89}, {"referenceID": 7, "context": "There has been recent work in continuous relaxations of discrete random variables [23, 8], that also uses a softmax to form a continuous approximation to a discrete set.", "startOffset": 82, "endOffset": 89}], "year": 2017, "abstractText": "Planning actions using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete, which may have limited its adoption. In this work, we introduce two discrete planning tasks inspired by existing question-answering datasets and show that it is in fact possible to effectively perform planning via backprop in discrete action spaces using two simple yet principled modifications. Our experiments show that this approach can significantly outperform model-free RL based methods and supervised imitation learners.", "creator": "LaTeX with hyperref package"}}}