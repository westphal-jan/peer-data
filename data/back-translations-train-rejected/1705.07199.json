{"id": "1705.07199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "histories": [["v1", "Fri, 19 May 2017 21:33:00 GMT  (264kb,D)", "http://arxiv.org/abs/1705.07199v1", "12 pages, 4 Figures"]], "COMMENTS": "12 pages, 4 Figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexander g", "erson", "cory p berg"], "accepted": false, "id": "1705.07199"}, "pdf": {"name": "1705.07199.pdf", "metadata": {"source": "CRF", "title": "The High-Dimensional Geometry of Binary Neural Networks", "authors": ["Alexander G. Anderson"], "emails": ["aga@berkeley.edu", "cberg500@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "The rapid reduction in the cost of networking has driven many achievements in the field of deep learning in recent years (2016). Given these successes, researchers are now looking at applications of deep learning in resource-limited hardware such as neuromorphic chips, embedded devices and smartphones. (2016), Neftci et al. (2016), Andri et al. (2017). A recent success for both theoretical researchers and industry practitioners is that traditional neural networks can be compressed because they are highly over-parameterized. While there has been a large amount of experimental work devoted to the compression of neural networks (Sec. 2), we are focusing on the particular approach that replaces costly 32-bit floating point multiplications with cheap binary operations. Our analysis shows a simple geometric picture based on the geometry of high-dimensional binary vectors that allows us to understand the successes of recent efforts."}, {"heading": "2 Related Work", "text": "For example, AlexNet 2016 has 61 million parameters and performs 1.5 billion operations to classify a 224 by 224 image (30,000 operations / pixel) (Rastegari et al. (2016). There has been a considerable amount of work to reduce these computing costs for embedded applications. Firstly, there are a variety of approaches aimed at compressing lecture networks. Kim et al. (2015) uses a decomposition of the core and fine-tuning the network thereafter. (2015b) There are a variety of approaches aimed at compressing lecture networks. (2015a) expands their previous work by sharing the weight of the tensor and fine-tuning the network thereafter. Han et al."}, {"heading": "3 Theory and Experiments", "text": "In this section we outline two theoretical predictions and then verify them experimentally. We train a binary neural network on CIFAR-10 (the same learning algorithm and architecture as in Courbariaux et al. (2016)).This Convolutionary Neural Network has six layers of coils, all of which have a 3 by 3 spatial nucleus. The number of character cards in each layer is 128, 128, 256, 256, 512, 512. After the second, fourth and sixth coils we perform a 2 by 2 max pooling operation. Then we have two fully connected layers, each with 1024 units. Each layer has a stacking standard layer between them. We find that the dimension of the weight vector under consideration (i.e. the convolution multiplied into a matrix) is the patch size (= 3-3 = 9) of the number of channels."}, {"heading": "3.1 Preservation of Direction During Binarization", "text": "In the hyperdimensional computer theory of Kanerva (2009), one of the key ideas is that two random, high-dimensional vectors of dimension d, whose entries are uniformly selected from the set, are approximately orthogonal (according to the central boundary theorem, the cosinal angle between two such random vectors is normally distributed with \u00b5 = 0 and \u03c3 \u0445 1 / \u221a d (cos \u03b8 \u2248 0 \u2192 \u03b8 \u2248 \u03c02). We apply this approach to the analysis of geometry (i.e. angle distributions) of high-dimensional vectors to binary vectors. As a zero distribution, we use the standard normal distribution, which is rotationally invariant, to generate our vectors. In moderately high dimensions, the binarization of a random vector changes its direction by a small amount relative to the angle between two random vectors. This contradicts our low-dimensional intuition, which is guided by the fact that the angle between two random 2D vectors is a more uniform distributions theory (GR)."}, {"heading": "3.2 Dot Product Preservation as a Sufficient Condition for Sensible Learning Dynamics", "text": "A reasonable question is: Are these so-called \"continuous weights\" just a learning artefact with no clear correspondence to the binary weights? While we know that wb = \u03b8 (wc), there are many continuous weights that are mapped to a particular binary weight vector. Which one do we find when we discuss the method of Courbariaux et al. (2016) As we discuss below, we obtain the continuous weight that preserves the Dot products with the activations.The key to our analysis is to focus on the transformers in our network whose forward and backward propagation functions are not related in such a way that they are normally used in typical decentered gradients. We show that the modified gradient that we would use as an estimator of the true gradient to train the continuous weights in traditional baking propagation."}, {"heading": "3.3 Permutation of Activations Reveals Fundamental Difference Between First Layer and Subsequent Layers", "text": "The question is whether this is a way in which the various factors that lead to a correlation between the two dot products are related to each other. For example, we might apply a random permutation to the two dot products to generate a distribution with the same marginal statistics as the original data, but the independent common statistics. Sucha transformation gives us a distribution with a correlation that is equal to the standardized dot product of the weight vectors (SI Sec. 3) As we essentially decrease the correlations for the higher layers in Fig. 4, but the correlation increases in the first layer."}, {"heading": "4 Conclusion", "text": "Neural networks with binary weights and activations have similar performance to their continuous counterparts with significantly reduced execution time and energy consumption. We provide an experimentally verified theory to understand how to get away with such a massive decrease in precision based on the geometry of HD vectors. First, we show that the binarization of high-dimensional vectors maintains its direction in the sense that the angle between a random vector and its binarized version is much smaller than the angle between two random vectors (Angle Preservation Property). Second, we take the perspective of the network and show that binarization preserves roughly weight-activating Dot products (Dot Product Preservation Property). Generally, applying a network compression technique we recommend considering the weight-activation Dot product histograms as heuristics to pinpoint the layers that are most responsible for the deterioration in performance (Product Property)."}, {"heading": "Acknowledgments", "text": "The authors thank Bruno Olshausen, Urs K\u00f6ster, Spencer Kent, Eric Dodds, Dylan Paiton and members of the Redwood Center for their useful feedback on this work. This material is based on work supported by the National Science Foundation under grant number DGE 1106400 (AGA). Any opinions, findings, conclusions or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of the National Science Foundation. This work was partially supported by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers sponsored by MARCO and DARPA (AGA)."}, {"heading": "5 Supplementary Information", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Expected Angles", "text": "We draw random n dimensional vectors from a rotationally invariant distribution and compare the angles between two random vectors and the binarized version of that vector. We find that a rotationally invariant distribution in a pdf can yield a distribution on angles for the magnitude of the vector times. In the expectations we calculate, the magnitude cancels out and there is only one rotationally invariant distribution on angles. So it is enough to use a Gaussian. Lemmas: 1. Let us consider a vector, v, chosen from a standard normal distribution of the dimension n. Let us leave the magnitude v1 v21 + v 2 n. Then the magnitude is distributed accordingly: g (1) = 1. We (n \u2212 2) have the magnitude (n \u2212 1)."}, {"heading": "5.2 An Explicit Example of Learning Dynamics", "text": "In this subsection, we look at the learning dynamics for the BNN training algorithm (compared to other components) and gain an insight into the learning algorithm. Consider the case of regression in which we try to predict y \u2212 \u2212 \u2212 x with a binary linear predictor. However, if we use a squared error loss, we have L = (y \u2212 y \u00b2) 2 = (y \u2212 wbx) 2 = (y \u2212 \u03b8 (wc) x) 2. (In this notation, x is a column vector.) If we take the derivative of this loss with respect to the continuous weights and use the rule for the backward propagation of the binary function, we get a quantitative difference between the dimensions \u2212 dL / dwc = \u2212 dwb \u00b7 dwc / dwc = (y \u2212 wbx) xT. Finally, by averaging the training data, we get an average equation we get."}, {"heading": "5.3 Dot Product Correlations After Activation Permutation", "text": "Suppose we look at A = w \u00b7 a and B = v \u00b7 a, where a are now the randomly permutated activations. What is the distribution of A, B? To answer this, we look at the correlation between A and B and show that it is the correlation between w and v. First, we assume that p (a) = E (ai) = E (ai) = 0, E (a2i) = \u03c3 2. Then we calculate E (A) = E (B) = 0. Now we calculate: E (AB) = \u2211 i, j wivjE (aiaj) = \u03c32 (w \u00b7 v) Likewise E (A2) = \u03c32 (w \u00b7 w) and E (B2) = \u03c32 (v \u00b7 v)."}], "references": [{"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["Pulkit Agrawal", "Ross Girshick", "Jitendra Malik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Agrawal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2014}, {"title": "Ternary neural networks for resource-efficient ai applications", "author": ["Hande Alemdar", "Nicholas Caldwell", "Vincent Leroy", "Adrien Prost-Boucle", "Fr\u00e9d\u00e9ric P\u00e9trot"], "venue": "arXiv preprint arXiv:1609.00222,", "citeRegEx": "Alemdar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alemdar et al\\.", "year": 2016}, {"title": "Yodann: An architecture for ultra-low power binary-weight cnn acceleration", "author": ["Renzo Andri", "Lukas Cavigelli", "Davide Rossi", "Luca Benini"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "Andri et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Andri et al\\.", "year": 2017}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarized neural networks: Training neural networks with weights and activations constrained to +1 and -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Inverting visual representations with convolutional networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dosovitskiy and Brox.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2016}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["Steven K Esser", "Paul A Merolla", "John V Arthur", "Andrew S Cassidy", "Rathinakumar Appuswamy", "Alexander Andreopoulos", "David J Berg", "Jeffrey L McKinstry", "Timothy Melano", "Davis R Barch"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Esser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Esser et al\\.", "year": 2016}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML, pp", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Hardware-oriented approximation of convolutional neural networks", "author": ["Philipp Gysel", "Mohammad Motamedi", "Soheil Ghiasi"], "venue": "arXiv preprint arXiv:1604.03168,", "citeRegEx": "Gysel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gysel et al\\.", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "Hubara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hubara et al\\.", "year": 2016}, {"title": "Reduced-precision strategies for bounded memory in deep neural nets", "author": ["Patrick Judd", "Jorge Albericio", "Tayler Hetherington", "Tor Aamodt", "Natalie Enright Jerger", "Raquel Urtasun", "Andreas Moshovos"], "venue": "arXiv preprint arXiv:1511.05236,", "citeRegEx": "Judd et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2015}, {"title": "Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors", "author": ["Pentti Kanerva"], "venue": "Cognitive Computation,", "citeRegEx": "Kanerva.,? \\Q2009\\E", "shortCiteRegEx": "Kanerva.", "year": 2009}, {"title": "Bitwise neural networks", "author": ["Minje Kim", "Paris Smaragdis"], "venue": "arXiv preprint arXiv:1601.06071,", "citeRegEx": "Kim and Smaragdis.,? \\Q2016\\E", "shortCiteRegEx": "Kim and Smaragdis.", "year": 2016}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Yong-Deok Kim", "Eunhyeok Park", "Sungjoo Yoo", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "venue": "arXiv preprint arXiv:1511.06530,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Deep convolutional neural network inference with floating-point weights and fixed-point activations", "author": ["Liangzhen Lai", "Naveen Suda", "Vikas Chandra"], "venue": "arXiv preprint arXiv:1703.03073,", "citeRegEx": "Lai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2017}, {"title": "Ternary weight networks", "author": ["Fengfu Li", "Bo Zhang", "Bin Liu"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Darryl Lin", "Sachin Talathi", "Sreekanth Annapureddy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "author": ["Paul Merolla", "Rathinakumar Appuswamy", "John Arthur", "Steve K Esser", "Dharmendra Modha"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 1981}, {"title": "Neuromorphic deep learning machines", "author": ["Emre Neftci", "Charles Augustine", "Somnath Paul", "Georgios Detorakis"], "venue": "arXiv preprint arXiv:1612.05596,", "citeRegEx": "Neftci et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neftci et al\\.", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "venue": "arXiv preprint arXiv:1702.03044,", "citeRegEx": "Zhou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Trained ternary quantization", "author": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1612.01064,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the highdimensional geometry of binary vectors.", "startOffset": 121, "endOffset": 142}, {"referenceID": 4, "context": "Given these successes, researchers are now considering applications of deep learning in resource limited hardware such as neuromorphic chips, embedded devices and smart phones (Esser et al. (2016), Neftci et al.", "startOffset": 177, "endOffset": 197}, {"referenceID": 4, "context": "Given these successes, researchers are now considering applications of deep learning in resource limited hardware such as neuromorphic chips, embedded devices and smart phones (Esser et al. (2016), Neftci et al. (2016), Andri et al.", "startOffset": 177, "endOffset": 219}, {"referenceID": 2, "context": "(2016), Andri et al. (2017)).", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "(2016), Andri et al. (2017)). A recent success for both theoretical researchers and industry practitioners is that traditional neural networks can be compressed because they are highly over-parameterized. While there has been a large amount of experimental work dedicated to compressing neural networks (Sec. 2), we focus on the particular approach that replaces costly 32-bit floating point multiplications with cheap binary operations. Our analysis reveals a simple geometric picture based on the geometry of high dimensional binary vectors that allows us to understand the successes of the recent efforts to compress neural networks. Recent work by Courbariaux et al. (2016) and Hubara et al.", "startOffset": 8, "endOffset": 678}, {"referenceID": 2, "context": "(2016), Andri et al. (2017)). A recent success for both theoretical researchers and industry practitioners is that traditional neural networks can be compressed because they are highly over-parameterized. While there has been a large amount of experimental work dedicated to compressing neural networks (Sec. 2), we focus on the particular approach that replaces costly 32-bit floating point multiplications with cheap binary operations. Our analysis reveals a simple geometric picture based on the geometry of high dimensional binary vectors that allows us to understand the successes of the recent efforts to compress neural networks. Recent work by Courbariaux et al. (2016) and Hubara et al. (2016) has shown that one can efficiently train neural networks with binary weights and activations that have similar performance to their continuous counterparts.", "startOffset": 8, "endOffset": 703}, {"referenceID": 3, "context": "Figure 1: Review of the Courbariaux et al. (2016) BNN Training Algorithm: a.", "startOffset": 24, "endOffset": 50}, {"referenceID": 3, "context": "Since the binarize function is non-differentiable, they use a smoothed version of the forward function for the backward function (in particular, the straight-through estimator of Bengio et al. (2013)).", "startOffset": 179, "endOffset": 200}, {"referenceID": 4, "context": "Furthermore, we show that this property is present in the weight vectors of a network trained using the method of Courbariaux et al. (2016). 2.", "startOffset": 114, "endOffset": 140}, {"referenceID": 4, "context": "Furthermore, we show that this property is present in the weight vectors of a network trained using the method of Courbariaux et al. (2016). 2. Dot Product Preservation Property: We show that the batch normalized weight-activation dot products, an important intermediate quantity in these BNNs, are approximately preserved under the binarization of the weight vectors. Relatedly, we argue that the continuous weights in the Courbariaux et al. (2016) method aren\u2019t just a learning artifact - they correspond to continuous weights trained using an estimator of the true gradient.", "startOffset": 114, "endOffset": 450}, {"referenceID": 6, "context": "5 billion operations to classify one 224 by 224 image (30 thousand operations/pixel) (Rastegari et al. (2016)).", "startOffset": 86, "endOffset": 110}, {"referenceID": 5, "context": "Kim et al. (2015) uses a Tucker decomposition of the kernel tensor and fine tunes the network afterwards.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "Han et al. (2015b) train a network, prune low magnitude connections, and retrain.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Han et al. (2015b) train a network, prune low magnitude connections, and retrain. Han et al. (2015a) extend their previous work to additionally include a weight sharing quantization step and Huffman coding of the weights.", "startOffset": 0, "endOffset": 101}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al.", "startOffset": 165, "endOffset": 191}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al.", "startOffset": 165, "endOffset": 212}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al.", "startOffset": 165, "endOffset": 232}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al. (2016), Lin et al.", "startOffset": 165, "endOffset": 253}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al. (2016), Lin et al. (2016), Lai et al.", "startOffset": 165, "endOffset": 272}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al. (2016), Lin et al. (2016), Lai et al. (2017)).", "startOffset": 165, "endOffset": 291}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit.", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al.", "startOffset": 13, "endOffset": 145}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al. (2016)).", "startOffset": 13, "endOffset": 260}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al. (2016)). Rastegari et al. (2016) replace a continuous weight matrix with a scalar times a binary matrix (and have a similar approximation for weight activation dot products).", "startOffset": 13, "endOffset": 286}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al. (2016)). Rastegari et al. (2016) replace a continuous weight matrix with a scalar times a binary matrix (and have a similar approximation for weight activation dot products). Kim & Smaragdis (2016) train a network with weights restricted in the range \u22121 to 1 and then use a noisy backpropagation scheme train a network with binary weights and activations.", "startOffset": 13, "endOffset": 451}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights.", "startOffset": 0, "endOffset": 62}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al.", "startOffset": 0, "endOffset": 231}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)).", "startOffset": 0, "endOffset": 253}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)). Zhou et al. (2017) use weights and activations that are zero or powers of two.", "startOffset": 0, "endOffset": 274}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)). Zhou et al. (2017) use weights and activations that are zero or powers of two. Lin et al. (2015) and Zhou et al.", "startOffset": 0, "endOffset": 352}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)). Zhou et al. (2017) use weights and activations that are zero or powers of two. Lin et al. (2015) and Zhou et al. (2016) quantize backpropagation in addition to the forward propagation.", "startOffset": 0, "endOffset": 375}, {"referenceID": 0, "context": "Agrawal et al. (2014) found that feature magnitudes in higher layers do not matter (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2014) found that feature magnitudes in higher layers do not matter (e.g. binarizing features barely changes classification performance). Merolla et al. (2016) analyze the robustness of neural network representations to a collection of different distortions.", "startOffset": 0, "endOffset": 175}, {"referenceID": 0, "context": "Agrawal et al. (2014) found that feature magnitudes in higher layers do not matter (e.g. binarizing features barely changes classification performance). Merolla et al. (2016) analyze the robustness of neural network representations to a collection of different distortions. Dosovitskiy & Brox (2016) observe that binarizing features in intermediate layers of a CNN and then using backpropagation to find an image with those features leads to relatively little distortion of the image compared to dropping out features.", "startOffset": 0, "endOffset": 300}, {"referenceID": 4, "context": "We train a binary neural network on CIFAR-10 (same learning algorithm and architecture as in Courbariaux et al. (2016)).", "startOffset": 93, "endOffset": 119}, {"referenceID": 15, "context": "In the hyperdimensional computing theory of Kanerva (2009), one of the key ideas is that two random, high-dimensional vectors of dimension d whose entries are chosen uniformly from the set {\u22121, 1} are approximately orthogonal (by the central limit theorem, the cosine angle between two such random vectors is normally distributed with \u03bc = 0 and \u03c3 \u223c 1/ \u221a d (cos \u03b8 \u2248 0 \u2192 \u03b8 \u2248 \u03c02 )).", "startOffset": 44, "endOffset": 59}, {"referenceID": 4, "context": "In order to test the applicability of our theory of Gaussian random vectors to real neural networks, we train a multilayer binary CNN on CIFAR10 (using the Courbariaux et al. (2016) method) and look at the weight vectors 1 in that trained network.", "startOffset": 156, "endOffset": 182}, {"referenceID": 4, "context": "Which one do we find when we apply the method of Courbariaux et al. (2016)? As we discuss below, we get the continuous weight that preserves the dot products with the activations.", "startOffset": 49, "endOffset": 75}, {"referenceID": 11, "context": "Han et al. (2015b) find that compressing the first set of convolutional weights of a particular layer by the same fraction has the highest impact on performance if done on the first layer.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "Han et al. (2015b) find that compressing the first set of convolutional weights of a particular layer by the same fraction has the highest impact on performance if done on the first layer. Zhou et al. (2016) find that accuracy degrades by about 0.", "startOffset": 0, "endOffset": 208}], "year": 2017, "abstractText": "Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the highdimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are wellapproximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}