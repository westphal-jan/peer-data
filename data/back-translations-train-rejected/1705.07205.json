{"id": "1705.07205", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Machine learning modeling for time series problem: Predicting flight ticket prices", "abstract": "Machine learning has been used in all kinds of fields. In this article, we introduce how machine learning can be applied into time series problem. Especially, we use the airline ticket prediction problem as our specific problem. Airline companies use many different variables to determine the flight ticket prices: indicator whether the travel is during the holidays, the number of free seats in the plane etc. Some of the variables are observed, but some of them are hidden. Based on the data over a 103 day period, we trained our models, getting the best model - which is AdaBoost-Decision Tree Classification. This algorithm has best performance over the observed 8 routes which has 61.35$\\%$ better performance than the random purchase strategy, and relatively small variance over these routes. And we also considered the situation that we cannot get too much historical datas for some routes (for example the route is new and does not have historical data) or we do not want to train historical data to predict to buy or wait quickly, in which problem, we used HMM Sequence Classification based AdaBoost-Decision Tree Classification to perform our prediction on 12 new routes. Finally, we got 31.71$\\%$ better performance than the random purchase strategy.", "histories": [["v1", "Fri, 19 May 2017 21:59:02 GMT  (374kb,D)", "http://arxiv.org/abs/1705.07205v1", "17 pages, 19 figures"]], "COMMENTS": "17 pages, 19 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun lu"], "accepted": false, "id": "1705.07205"}, "pdf": {"name": "1705.07205.pdf", "metadata": {"source": "CRF", "title": "Machine learning modeling for time series problem: Predicting flight ticket prices", "authors": ["Jun Lu"], "emails": ["jun.lu.locky@gmail.com"], "sections": [{"heading": null, "text": "This year, the majority of them will be able to address the countries of origin."}, {"heading": "A. Pricing Behavior in the Collected Data", "text": "We have found that the price of tickets for flights can vary considerably over time. Table I shows the minimum price, maximum price and maximum price difference that can occur for flights on the 8 specific routes. Although this table is the maximum price and minimum price for all departure dates for each route, we can have an overall overview of how we can achieve a reduction in the ticket purchase.Fig.2, 3 and 4 show how pricing strategies differ from flights. Page 2 of 17"}, {"heading": "V. Machine Learning Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Feature Extraction", "text": "Characteristics extracted for training and testing are aggregated variables calculated from the list of offers observed on each day of the query. On each day of the query, there may be eight airlines specifying flights for a particular combination of departure and departure dates. 5 characteristics are calculated for each query date: the flight number (encoded by dummy variables), the previous minimum price, the previous maximum price, the query and departure time (number of days between the first query date (in our case 9 / 11 / 2015) and the departure date), the days to departure (number of days between the query and departure date), and the current price. Specifically, the inputs have the following format, which is shown in Figure 5 (note that the flight number should be encoded with dummy variables). For the issue of the regression problem, we set it to the minimum price for each departure date and flight; and for the issue for the classification date, which we specify by the minimum date of departure (1)."}, {"heading": "B. Data Description and Interpretation", "text": "Our data set consists of a set of input feature vectors, each of which is divided into training data set and test data set. In our case, we share the data that matches flights with departure date between November 9, 2015 and January 15, 2016 as a training data set, and the data that matches flights with departure date between January 16, 2016 and February 20, 2016 as a test data set; in terms of the generalized problem, we received the same period as the test data set, which ranges from January 16, 2016 to February 20, 2016. Finally, the training data set consists of NT = 16, 208 data samples of an output variable y and an input variable X. The problematic test set consists of NT = 14, as detailed above. The test data set consists of NT = 20, 367, for which the output is unknown, and where we have falsified our predictions."}, {"heading": "C. Model Construction", "text": "Our regression method consists in predicting the expected minimum price for a certain date of departure and the given flight with the input functions. Thus, if the current price is lower than the expected minimum price, we forecast to buy; otherwise, we forecast to wait. After studying the data in detail, we were able to determine that the last date always has a very high price. Thus, we make the last purchase date 7 days before the date of departure. The following purchase rule (algorithm 1) explicitly explains how to predict in the model of regression whether to buy or wait: Algorithm 1 Purchase rule for the classification Uccess: For days until the date of departure = N, N-1,... 7 Tun 2: IF ExpectedMinimum price for the classification Uccess: For days until the end of the classification Uccess: For days until the end of the classification Uccess: For days until the end of the classification Uccess: For days until the end of the classification Uccess: for days until the date of the classification Uccess: for days until the date of the classification Uccess: for days until the date of the classification Uccess: for days until the date of the classification of the Uccess: for days until the date of the classification of the classification Uccess: for days until the date of the classification of the Uccess:"}, {"heading": "D. Methodology review", "text": "In this area, we are able to achieve an approximate solution. \"We have a closed form of expression for the minimum,\" he says. \"We have a closed form of expression for the minimum.\" \"We have a closed form of expression for the minimum.\" \"We have a closed form of expression for the minimum.\" \"We have a closed form of expression for the minimum.\" \"If it is not deepened, we can have a closed form of expression for the minimum.\" \"We have a closed form of expression for the minimum.\" \"If it is not deepened, we can use pseudo-inverse solutions.\" \"We have a closed form of expression for the minimum.\" \"We have a closed form of expression for the minimum.\""}, {"heading": "E. Performance Benchmarks", "text": "The naive buying algorithm, called random purchase, consists of randomly purchasing a ticket before the departure date. To be more specific, we randomly select multiple tickets at that interval for each departure date, for example departure date B, then from the first historical dates for that departure date (namely date A), to simulate the customer buying the ticket, and the average price would be calculated as a random purchase price. Another random purchase strategy measure introduced in Groves & Gini (2013) is called early purchase, where the time interval is divided into many daily stages and one ticket is purchased in each day period. These two measures of performance are similar. However, we thought that the random purchase strategy is closer to reality because in reality some customers can choose the same period to purchase. Imagine that there are 100 customers who want to buy the tickets from date A to departure date B. The random purchase strategy is implemented in that period randomly."}, {"heading": "F. Performance Metric", "text": "As long as we receive the Random Purchase Price, the Optimal Price and the Predicted Price, we can use the following performance metric to evaluate our results: Performance = Random Purchase Price \u2212 Predicted PriceRandom Purchase Price% (10) Optimal Performance = Random Purchase Price \u2212 Optimal PriceRandom Purchase Price% (11) Normalized Performance = Performance Optimal Performance% (12) Taking these metrics into account, we have used Normalized Performance to evaluate our results because it normalizes each way and ranges from 0% to 100%. In this case, the higher the better, so that we have more intuition about how good or bad the performance is. VI. Regression Results"}, {"heading": "A. Hyperparameter Tuning", "text": "1) Least Squares: In our problem, although the least squares did not work very well, it is useful to describe the uniform blending model described in the following section: 2) Neural networks: page 7 of 17a) Hidden layers: With regard to the neural network architecture, we have examined several configurations in terms of the number of nodes and the number of hidden layers. We have come to the conclusion that the neural network architecture, which consists of the best of a single hidden layer network of 6 neurons, even if the increasing number of hidden layers makes it possible to express the output as any function of input, also carries the risk of overmatching the model. Therefore, a hidden model can explain the input-output relationship. In terms of the number of neurons in that layer, it is known that a low number of neurons in the hidden layers can prevent the loss of important information."}, {"heading": "B. Regression Performance Results", "text": "Table II shows the results of the regression methods. Random Forest Regression achieves the best performance in the regression method. However, its variance is not small enough, which means that it is sensitive to different routes. In this case, while it performs well on some routes, it performs poorly on other routes. From a customer perspective, it is not fair for some customers to buy tickets for which the system may make poor predictions. The preferred method in regression is the AdaBoost-Decision Tree regression method, which has the least variance and relatively high performance."}, {"heading": "A. Solving Imbalanced Data Set", "text": "In fact, it is the case that the number of people mentioned, who are able to sit at the top of the group in which they are, is very high. (...) In fact, the number of people mentioned is very high. (...) In fact, the number of people mentioned is very high. (...) The number of people mentioned is so high that the number of people mentioned is very high. (...) The number of people mentioned is so high that the number of people mentioned is very high. (...) The number of people mentioned is so high that the number of people mentioned is very high. (...) The number of people mentioned is so high that the number of people mentioned is very high. (...) The number of people mentioned is so high that the number of people mentioned is very high. (...)"}, {"heading": "C. Hyperparameter Tuning", "text": "This year it is more than ever before."}, {"heading": "D. Classification Performance Results", "text": "Table V shows the results of the classification methods. As we can see, AdaBoost-DecisionTree, KNN and Uniform Blending achieve positive results on all 8 routes and have smaller deviations on these routes compared to other classification algorithms. AdaBoost-DecisionTree achieves the best performance and a relatively small deviation on 8 routes. And, as expected, the uniform mixing method has the smallest deviation, just like the theory of uniform mixing descriptions. VIII. Q Learning"}, {"heading": "A. Parameter Assignment", "text": "In our case, the reward associated with b (buy) is the negative of the ticket price in this state, and the state resulting from b is an end state, so there is no future reward. Instant reward associated with w (wait) is zero. And we bet \u03b3 = 1, so we have not discounted any future reward. In short, we define the Q function as follows: Q (b, s) = \u2212 Price (e) Q (w, s) = max (Q (b, s \u2032), Q (w, s \u2032)) (14)."}, {"heading": "B. Averaging step by Equivalence class", "text": "We defined an equivalence class by state. Our equivalence class is the group of states with the same flight number and the same days before departure, but different departure dates. We designate that s and s * are in the same equivalence class. Therefore, our revised Q-Learning formula is as follows: Q (a, \"s\") = Avgs \u0445 \"s (R (s,\" a \") + \u03b3maxa\" (Q (a, \"s\")) (15) The result of Q-Learning is the learned policy that determines whether to buy or wait in invisible states by assigning them to the corresponding equivalence class and selecting the action with the lowest learning costs."}, {"heading": "C. Q Learning Performance Result", "text": "Table VI shows the result of Q-Learning. As we can see, the Q-Learning method described in [Etzioni et al., 2003] shows an acceptable performance, and the variance is not great either. Performance is very close to the AdaBoost-DecisionTree Classification and Uniform Blending Classification Algorithms.Page 13 of 17Performance (%) Model Routes\\ Method Optimal Random Purch. Logistic Reg NN Decison Tree KNN AdaBoost Random Forest Uniform BlendingBCN \u2192 BUD 100.0 0.00 36.13 36.13 36.13 54.67 17.58 75.27 29.95 48.49 BUD \u2192 BCN 100.0 0.0 -39.97 39.98 11.02 64.62 60.34 86.60 30.32 CRL \u2192 OTP 100.0 0.0 40.60 18.60 70.83 70.51 70.51 83.48 MLH \u2192 31.-30.00 \u2192 30.00 -30.00 -31.00 -30.00 -30.00 -00 Nor62 100.00 864 100.632 331.00 431.00 431.00 SK00 40.00 430.00 430.00 SK00 430.00 430.00"}, {"heading": "A. Uniform Blending", "text": "From our specific problem, we have 8 patterns (i.e. 8 flight numbers). As long as we trained the specific problem, we could get 8 models (or learners) for 8 flight numbers separately. Afterwards, we can use the 8 models to make predictions for our new route, and then we have these 8 models tuned to buy or maintain each ticket."}, {"heading": "B. HMM Sequence Classification", "text": "The second idea came to us to assign a \"flight number\" from the 8 routes to each data entry, instead of describing the average 8 Q models. We then used sequence classification to assign the \"flight number\" to each data entry. In this section, we mainly review the HMM sequence classification and describe how it can be applied to the generalized problem we mentioned earlier. - The hidden Markov model (HMM) represents stochastic sequences as Markov chains in which the states are not directly observed but are associated with a probability density function (p.d.f). The generation of a random sequence is then the result of a random walk in the chain (i.e. searching a random sequence of states Q = qK) and a draw (also referred to as emission)."}, {"heading": "C. Generalized Problem Performance Result", "text": "Table VIII shows the result of a generalized problem. As we see, the uniform mix does not result in an improvement. But the HMM sequence classification algorithm results in 9 routes receiving an improvement, 3 routes having a negative performance. Although the average performance is 31.71%, which is lower than that of the specific problem, it makes sense that we did not use historical data of these routes to predict (in fact, two routes already appear in the specific problem, namely CRL \u2192 OTP and OTP \u2192 CRL). In the specific problem when using the AdaBoost DecisionTree Classification, the performance for these two routes (i.e. CRL \u2192 OTP and OTP \u2192 CRL) is 71.69% and 85.33% respectively. In the generalized problem, however, when using the same classification method, the performance is 63.11% and 0.54%, which is worse than the specific problem. This is tolerable because we only used the formula for a specific page of 17."}, {"heading": "X. Future Work", "text": "In this project, we have used only a few features to increase the price as much as possible."}, {"heading": "Acknowledgment", "text": "I would like to thank Professor Boi and Igor for their help and suggestions during this project. All rights to this report and the published codes are reserved."}], "references": [{"title": "k-means\u2013: A unified approach to clustering and outlier detection", "author": ["Sanjay Chawla", "Aristides Gionis"], "venue": "In Proceedings of the 2013 SIAM International Conference on Data Mining,", "citeRegEx": "Chawla and Gionis.,? \\Q2013\\E", "shortCiteRegEx": "Chawla and Gionis.", "year": 2013}, {"title": "Maximum likelihood modeling with gaussian distributions for classification", "author": ["Ramesh A Gopinath"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "Gopinath.,? \\Q1998\\E", "shortCiteRegEx": "Gopinath.", "year": 1998}, {"title": "Optimal airline ticket purchasing using automated user-guided feature selection", "author": ["William Groves", "Maria L Gini"], "venue": "In IJCAI,", "citeRegEx": "Groves and Gini.,? \\Q2013\\E", "shortCiteRegEx": "Groves and Gini.", "year": 2013}, {"title": "Continuous speech recognition using multilayer perceptrons with hidden markov models", "author": ["Nelson Morgan", "Herve Bourlard"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Morgan and Bourlard.,? \\Q1990\\E", "shortCiteRegEx": "Morgan and Bourlard.", "year": 1990}, {"title": "Maximum likelihood and minimum classification error factor analysis for automatic speech recognition", "author": ["Lawrence K Saul", "Mazin G Rahim"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "Saul and Rahim.,? \\Q2000\\E", "shortCiteRegEx": "Saul and Rahim.", "year": 2000}, {"title": "On the perturbation of pseudo-inverses, projections and linear least squares problems", "author": ["GW Stewart"], "venue": "SIAM review,", "citeRegEx": "Stewart.,? \\Q1977\\E", "shortCiteRegEx": "Stewart.", "year": 1977}], "referenceMentions": [{"referenceID": 5, "context": "Roughly speaking, the pseudo-inverse of X\u0303 is (X\u0303 X\u0303 + \u03b5I)\u22121X\u0303T , where the limits are taken with \u03b5 > 0 (Stewart, 1977).", "startOffset": 104, "endOffset": 119}, {"referenceID": 1, "context": "The sequence of states, which is the quantity of interest in speech recognition and in most of the other pattern recognition problems (Gopinath, 1998) (Saul & Rahim, 2000) (Morgan & Bourlard, 1990), can be observed only through the stochastic processes defined into each state (i.", "startOffset": 134, "endOffset": 150}], "year": 2017, "abstractText": "Machine learning has been used in all kinds of fields. In this article, we introduce how machine learning can be applied into time series problem. Especially, we use the airline ticket prediction problem as our specific problem. Airline companies use many different variables to determine the flight ticket prices: indicator whether the travel is during the holidays, the number of free seats in the plane etc. Some of the variables are observed, but some of them are hidden. Based on the data over a 103 day period, we trained our models, getting the best model which is AdaBoost-Decision Tree Classification. This algorithm has best performance over the observed 8 routes which has 61.35% better performance than the random purchase strategy, and relatively small variance over these routes. And we also considered the situation that we cannot get too much historical datas for some routes (for example the route is new and does not have historical data) or we do not want to train historical data to predict to buy or wait quickly, in which problem, we used HMM Sequence Classification based AdaBoost-Decision Tree Classification to perform our prediction on 12 new routes. Finally, we got 31.71% better performance than the random purchase strategy. A python implementation of this project is available online.", "creator": "LaTeX with hyperref package"}}}