{"id": "1705.07208", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "PixColor: Pixel Recursive Colorization", "abstract": "We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a \"Visual Turing Test\".", "histories": [["v1", "Fri, 19 May 2017 22:10:51 GMT  (5605kb,D)", "https://arxiv.org/abs/1705.07208v1", null], ["v2", "Mon, 5 Jun 2017 18:38:01 GMT  (5600kb,D)", "http://arxiv.org/abs/1705.07208v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["sergio guadarrama", "ryan dahl", "david bieber", "mohammad norouzi", "jonathon shlens", "kevin murphy"], "accepted": false, "id": "1705.07208"}, "pdf": {"name": "1705.07208.pdf", "metadata": {"source": "CRF", "title": "PIXCOLOR: PIXEL RECURSIVE COLORIZATION", "authors": ["Sergio Guadarrama", "Ryan Dahl", "David Bieber", "Mohammad Norouzi", "Jonathon Shlens", "Kevin Murphy"], "emails": ["sguada@google.com", "rld@google.com", "dbieber@google.com", "mnorouzi@google.com", "shlens@google.com", "kpmurphy@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Building a computer system that can automatically convert a black-and-white image into a plausible color image is useful for restoring old photos, videos [34] or even assisting cartoon artists [26, 32]. From the perspective of computer vision, this may seem like a simple picture-to-picture imaging problem accessible to a Convolutionary Neural Network (CNN). We call this y = f (x), where x is the input of grayscale images, y is the predicted color image, and f is a CNN. This approach has been pursued in several recent papers [5, 15, 20, 41, 10, 7, 17] which take advantage of the fact that unlimited image pairs can be obtained by converting color images into grayscale images. Removing the chromaticity from an image is a surjective operation thus, restoration to an image is a surjective operation, thus restoration to an image is a one-to-operation (Figure 1)."}, {"heading": "2 RELATED WORK", "text": "Early approaches to coloring relied on a certain amount of human effort, either to identify a relevant source color image from which the colors could be transferred particularly strongly [38, 6, 13, 16, 24, 33, 28, 25, 3], or to avoid a coarse coloring by a human commentator to serve as a series of \"cues\" [21, 14, 22, 26, 40, 42, 11]. Recently, there has been a wave of interest in developing fully automated solutions that do not require human interaction (see Table 1). However, most newer methods form a series of \"cues\" that produce a colored image into a single color image [5, 15, 20, 41, 7]. When such models are trained with L2 or L1 loss, the colorations often look somewhat \"washed out,\" as the model is encouraged to predict the average color (e.g. [20, 41]."}, {"heading": "3 PIXEL RECURSIVE COLORIZATION (PIXCOLOR)", "text": "The main intuition behind our approach is that it is sufficient to predict a plausible low-resolution color image because the color is much lower than the intensity. To illustrate this point, we take the basic chrominance of an image, take it down to 28 \u00d7 28, take it back to its original size, and then combine it with the original brightness. Figure 3 shows some examples of this process. It is clear that the resulting color images look very close to the original color images. In the following sections, we describe how we train a model to predict several plausible low-resolution color images, and how we then train a second model to combine these predictions with the original grayscale input to produce a high-resolution color output. See Figure 4 for an overview of the approach."}, {"heading": "3.1 PIXELCNN FOR LOW-RESOLUTION COLORIZATION", "text": "Inspired by the success of autoregressive models for unconditional image generation [35, 36] and superresolution [8], we use a conditional PixelCNN [36] to produce multiple low-resolution color images. That is, we turn coloring into a sequential decision task where pixels are sequentially colored, and the color of each pixel is conditioned to the input image and previously colored pixels. Although scanning from a PixelCNN is generally quite slow (since it is sequential by nature), all we need to do is create a low-resolution image (28x28) that is relatively fast. In addition, there are several additional acceleration tricks that we can apply (see, for example, [27, 19]) when necessary. Our architecture is based on [8] that PixelCNNs have used to perform superresolutions (another one-dimensional problem)."}, {"heading": "3.2 FEEDFORWARD CNN FOR HIGH-RESOLUTION REFINEMENT", "text": "One simple way to take advantage of the low resolution of the network is to upload it (e.g. bilinear or nearest interpolation) and then link the result to the original grayscale image. This can work quite well, as we have shown in Figure 3. However, it is possible to do better by combining the predicted color image with the original high resolution grayscale image using a picture-to-picture network that we call a refinement network. It is architecturally comparable to the network used, but with more layers in the decryption. In addition, we use bilinear interpolation for upsampling."}, {"heading": "4 EVALUATION METHODOLOGY", "text": "Since the gray-to-color mapping is one too many, we cannot evaluate performance by comparing the predicted color image with the \"ground truth\" color image with respect to mean square errors or even other perceptual similarity metrics such as SSIM [37]. Instead, we follow the approach of [41] and perform a \"Visual Turing Test\" (VTT) using human craters. In this test, we present two different color versions of an image, one the basic truth and one that corresponds to the predicted colors generated by a method. Then, we ask the crater to select the image that shows the \"true colors.\" A method that always produces the basic truth coloring would achieve 50% with this metric. To be comparable to [41], we show the two images one at a time for 1 second. (We randomize which image is shown first.) According to standard practice, we train on the 1.2 M training images from the SVC-CILC dataset [RLS-30], we extract the SVC-10C dataset from the 500 M training images."}, {"heading": "5 RESULTS", "text": "Table 3 shows a qualitative comparison of various current methods applied to a few randomly selected test images. Based on these examples, it seems that our method (PixColor) and several newer CNN-based methods, namely LTBC [15], LRAC [20] and CIC [41], are among the best. Therefore, we are performing a more expensive \"Visual Turing Test\" (VTT) on these four systems, as explained in Section 4.Figure 6. We see that our method significantly exceeds the previous state of the art with an average VTT value of 33.9%. One reason we believe that our results are better is that the colors they produce are more \"natural\" and are placed in the \"right\" places. To evaluate the first output, Figure 7 draws the marginal statistics of the color of one and the channels (the images generated from the other model), which we see weighted more closely than the previous images."}, {"heading": "5.1 SAMPLE DIVERSITY", "text": "To quantify how different these samples are from each other, we calculate the multi-scale SSIM [37] measure between the sample pairs. The results are shown in Figure 8. We see that most pairs have an SSIM value in the range of 0.95-0.99, which means that they are very similar, but in some places differ, which corresponds to subtle details, such as the color of a person's shirt. The pairs that have the lowest SSIM value are those where large objects get different colors (see the bird pair on the left). In an ideal world, we could automatically select the best sample and show it to the user. To get a feel for this well-picked sample, we could decide how best to use the sample to select three samples."}, {"heading": "6 CONCLUSION", "text": "We showed that PixColor produces multiple colors and found that the results of our model perform better on average than other published methods in a mass-based human evaluation. We avoided the problem of slow inference in PixelCNN by scanning only low-resolution color channels and used a standard picture-to-picture CNN to refine the result. We justified the need for the grafting network with ablation studies and showed that the PixColor output is closer to the marginal color distributions compared to other methods. The model has a variety of error modes, as shown in Figure 10, which we will address in our future work."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Stephen Mussmann and Laurent Dinh for their work and discussion on earlier versions of this project; Julia Winn, Jingyu Cui and Dhyanesh Narayanan for helping with an earlier prototype; Aa \ufffd ron van den Oord for advising and guiding on using PixelCNN architectures; and the TensorFlow team for technical and infrastructure support."}], "references": [{"title": "Multiple hypothesis colorization and its application to image compression", "author": ["Mohammad Haris Baig", "Lorenzo Torresani"], "venue": "Comput. Vis. Image Underst.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Unsupervised diverse colorization via generative adversarial networks", "author": ["Yun Cao", "Zhiming Zhou", "Weinan Zhang", "Yong Yu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Automatic image colorization via multimodal predictions", "author": ["G. Charpiat", "M. Hofmann", "B. Sch\u00f6lkopf"], "venue": "ECCV", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Semantic colorization with internet images", "author": ["Alex Yong-Sang Chia", "Shaojie Zhuo", "Raj Kumar Gupta", "Yu-Wing Tai", "Siu-Yeung Cho", "Ping Tan", "Stephen Lin"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Automatic colorization. http://tinyclouds.org/colorize/, 2016", "author": ["Ryan Dahl"], "venue": "Pixel Recursive Colorization,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Pixel recursive super resolution", "author": ["Ryan Dahl", "Mohammad Norouzi", "Jonathon Shlens"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Learning diverse image colorization", "author": ["Aditya Deshpande", "Jiajun Lu", "Mao-Chuang Yeh", "David A. Forsyth"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1958}, {"title": "Learning large-scale automatic image colorization", "author": ["Aditya Deshpande", "Jason Rock", "David Forsyth"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Outline colorization through tandem adversarial networks", "author": ["Kevin Frans"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Image colorization using similar images", "author": ["Raj Kumar Gupta", "Alex Yong-Sang Chia", "Deepu Rajan", "Ee Sin Ng", "Huang Zhiyong"], "venue": "ACM Multimedia,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "An adaptive edge detection based colorization algorithm and its applications", "author": ["Yi-Chin Huang", "Yi-Shin Tung", "Jun-Cheng Chen", "Sung-Wen Wang", "Ja-Ling Wu"], "venue": "ACM Multimedia,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Let there be Color!: Joint Endto-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification", "author": ["Satoshi Iizuka", "Edgar Simo-Serra", "Hiroshi Ishikawa"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Colorization by example", "author": ["Revital Irony", "Daniel Cohen-Or", "Dani Lischinski"], "venue": "Eurographics Symposium on Rendering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "A Efros. Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Auto-Encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Latent variable PixelCNNs for natural image modeling", "author": ["Alexander Kolesnikov", "Christoph H Lampert"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Learning representations for automatic colorization", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Colorization using optimization", "author": ["Anat Levin", "Dani Lischinski", "Yair Weiss"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Natural image colorization", "author": ["Qing Luan", "Fang Wen", "Daniel Cohen-Or", "Lin Liang", "Ying-Qing Xu", "Heung-Yeung Shum"], "venue": "Eurographics Symposium on Rendering,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Automatic colorization of grayscale images using multiple images on the web", "author": ["Yuji Morimoto", "Yuichi Taguchi", "Takeshi Naemura"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Automated colour grading using colour distribution transfer", "author": ["Fran\u00e7ois Piti\u00e9", "Anil C. Kokaram", "Rozenn Dahyot"], "venue": "Comput. Vis. Image Underst.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Manga colorization", "author": ["Y Qu", "T Wong", "P Heng"], "venue": "ACM Transactions on Graphics, 25(3)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast generation for convolutional autoregressive models", "author": ["Prajit Ramachandran", "Tom Le Paine", "Pooya Khorrami", "Mohammad Babaeizadeh", "Shiyu Chang", "Yang Zhang", "Mark A Hasegawa-Johnson", "Roy H Campbell", "Thomas S Huang"], "venue": "ICLR workshop,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Color transfer between images", "author": ["Erik Reinhard", "Michael Ashikhmin", "Bruce Gooch", "Peter Shirley"], "venue": "IEEE Comput. Graph. Appl.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Probabilistic image colorization", "author": ["Amelie Royer", "Alexander Kolesnikov", "Christoph H Lampert"], "venue": "Pixel Recursive Colorization,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications", "author": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P Kingma"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "Unsupervised colorization of black-and-white cartoons", "author": ["D Sykora", "J Burianek", "J Zara"], "venue": "International symposium on Non-photorealistic animation and rendering", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Local color transfer via probabilistic segmentation by expectation-maximization", "author": ["Yu-Wing Tai", "Jiaya Jia", "Chi-Keung Tang"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "A novel visualization tool for art history and conservation: Automated colorization of black and white archival photographs of works of art", "author": ["S Tsaftaris", "F Casadio", "J Andral", "K Katsaggelos"], "venue": "Studies in Conservation, 59(3)", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional Image Generation with PixelCNN Decoders", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["Z Wang", "E P Simoncelli", "A C Bovik"], "venue": "Asilomar Conference on Signals, Systems, Computers", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Transferring color to greyscale images", "author": ["Tomihisa Welsh", "Michael Ashikhmin", "Klaus Mueller"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R J Williams", "D Zipser"], "venue": "Neural Computation, 1(2)", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1989}, {"title": "Fast image and video colorization using chrominance blending", "author": ["L. Yatziv", "G. Sapiro"], "venue": "IEEE Trans. Img. Proc., 15(5)", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Colorful image colorization", "author": ["Richard Zhang", "Phillip Isola", "Alexei A Efros"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}], "referenceMentions": [{"referenceID": 30, "context": "Building a computer system that can automatically convert a black and white image to a plausible color image is useful for restoring old photographs, videos [34], or even assisting cartoon artists [26, 32].", "startOffset": 157, "endOffset": 161}, {"referenceID": 23, "context": "Building a computer system that can automatically convert a black and white image to a plausible color image is useful for restoring old photographs, videos [34], or even assisting cartoon artists [26, 32].", "startOffset": 197, "endOffset": 205}, {"referenceID": 28, "context": "Building a computer system that can automatically convert a black and white image to a plausible color image is useful for restoring old photographs, videos [34], or even assisting cartoon artists [26, 32].", "startOffset": 197, "endOffset": 205}, {"referenceID": 13, "context": "This approach has been pursued in several recent papers [5, 15, 20, 41, 10, 7, 17] which leverages the fact that one may obtain unlimited labeled training pairs by converting color images to grayscale.", "startOffset": 56, "endOffset": 82}, {"referenceID": 18, "context": "This approach has been pursued in several recent papers [5, 15, 20, 41, 10, 7, 17] which leverages the fact that one may obtain unlimited labeled training pairs by converting color images to grayscale.", "startOffset": 56, "endOffset": 82}, {"referenceID": 36, "context": "This approach has been pursued in several recent papers [5, 15, 20, 41, 10, 7, 17] which leverages the fact that one may obtain unlimited labeled training pairs by converting color images to grayscale.", "startOffset": 56, "endOffset": 82}, {"referenceID": 8, "context": "This approach has been pursued in several recent papers [5, 15, 20, 41, 10, 7, 17] which leverages the fact that one may obtain unlimited labeled training pairs by converting color images to grayscale.", "startOffset": 56, "endOffset": 82}, {"referenceID": 5, "context": "This approach has been pursued in several recent papers [5, 15, 20, 41, 10, 7, 17] which leverages the fact that one may obtain unlimited labeled training pairs by converting color images to grayscale.", "startOffset": 56, "endOffset": 82}, {"referenceID": 15, "context": "This approach has been pursued in several recent papers [5, 15, 20, 41, 10, 7, 17] which leverages the fact that one may obtain unlimited labeled training pairs by converting color images to grayscale.", "startOffset": 56, "endOffset": 82}, {"referenceID": 31, "context": "In this paper, we propose a new method, that employs a PixelCNN [36] probabilistic model to produce a coherent joint distribution over color images given a grayscale input.", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 4, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 11, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 14, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 21, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 29, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 25, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 22, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 2, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 164, "endOffset": 198}, {"referenceID": 19, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 279, "endOffset": 307}, {"referenceID": 12, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 279, "endOffset": 307}, {"referenceID": 20, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 279, "endOffset": 307}, {"referenceID": 23, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 279, "endOffset": 307}, {"referenceID": 35, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 279, "endOffset": 307}, {"referenceID": 9, "context": "Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred [38, 6, 13, 16, 24, 33, 28, 25, 3], or to get a rough coloring from a human annotator to serve as a set of \u201dhints\u201d [21, 14, 22, 26, 40, 42, 11].", "startOffset": 279, "endOffset": 307}, {"referenceID": 13, "context": "Most recent methods train a CNN to map a gray input image to a single color image [5, 15, 20, 41, 10, 7].", "startOffset": 82, "endOffset": 104}, {"referenceID": 18, "context": "Most recent methods train a CNN to map a gray input image to a single color image [5, 15, 20, 41, 10, 7].", "startOffset": 82, "endOffset": 104}, {"referenceID": 36, "context": "Most recent methods train a CNN to map a gray input image to a single color image [5, 15, 20, 41, 10, 7].", "startOffset": 82, "endOffset": 104}, {"referenceID": 8, "context": "Most recent methods train a CNN to map a gray input image to a single color image [5, 15, 20, 41, 10, 7].", "startOffset": 82, "endOffset": 104}, {"referenceID": 5, "context": "Most recent methods train a CNN to map a gray input image to a single color image [5, 15, 20, 41, 10, 7].", "startOffset": 82, "endOffset": 104}, {"referenceID": 18, "context": ", [20, 41]) discretize the color space, and use a per-pixel cross-entropy loss on the softmax outputs of a CNN, resulting in more colorful pictures, especially if rare colors are upweighted during training (e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 36, "context": ", [20, 41]) discretize the color space, and use a per-pixel cross-entropy loss on the softmax outputs of a CNN, resulting in more colorful pictures, especially if rare colors are upweighted during training (e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 36, "context": ", [41]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "One approach is to use a conditional random field (CRF) [3], although inference in such models can be slow.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "Model Color Loss Multi Dataset AICMP [3] CRF Lab N/A N N/A LTBC [15] CNN Lab L2 + class CE N MIT places LRAC [20] CNN Lab CE N ImageNet CIC [41] CNN Lab CE N ImageNet MOE [1] MOE YCbCr L2 Y ImageNet VAE [9] MDN + VAE Lab Mahal.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "Model Color Loss Multi Dataset AICMP [3] CRF Lab N/A N N/A LTBC [15] CNN Lab L2 + class CE N MIT places LRAC [20] CNN Lab CE N ImageNet CIC [41] CNN Lab CE N ImageNet MOE [1] MOE YCbCr L2 Y ImageNet VAE [9] MDN + VAE Lab Mahal.", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Model Color Loss Multi Dataset AICMP [3] CRF Lab N/A N N/A LTBC [15] CNN Lab L2 + class CE N MIT places LRAC [20] CNN Lab CE N ImageNet CIC [41] CNN Lab CE N ImageNet MOE [1] MOE YCbCr L2 Y ImageNet VAE [9] MDN + VAE Lab Mahal.", "startOffset": 109, "endOffset": 113}, {"referenceID": 36, "context": "Model Color Loss Multi Dataset AICMP [3] CRF Lab N/A N N/A LTBC [15] CNN Lab L2 + class CE N MIT places LRAC [20] CNN Lab CE N ImageNet CIC [41] CNN Lab CE N ImageNet MOE [1] MOE YCbCr L2 Y ImageNet VAE [9] MDN + VAE Lab Mahal.", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "Model Color Loss Multi Dataset AICMP [3] CRF Lab N/A N N/A LTBC [15] CNN Lab L2 + class CE N MIT places LRAC [20] CNN Lab CE N ImageNet CIC [41] CNN Lab CE N ImageNet MOE [1] MOE YCbCr L2 Y ImageNet VAE [9] MDN + VAE Lab Mahal.", "startOffset": 171, "endOffset": 174}, {"referenceID": 7, "context": "Model Color Loss Multi Dataset AICMP [3] CRF Lab N/A N N/A LTBC [15] CNN Lab L2 + class CE N MIT places LRAC [20] CNN Lab CE N ImageNet CIC [41] CNN Lab CE N ImageNet MOE [1] MOE YCbCr L2 Y ImageNet VAE [9] MDN + VAE Lab Mahal.", "startOffset": 203, "endOffset": 206}, {"referenceID": 15, "context": "Y ImageNet Pix2Pix [17] GAN Lab Adv.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "N ImageNet GAN [2] GAN YUV Adv.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "The CRF method of [3] requires that the user specify one or more training images that are similar to the input gray image.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "Although the CRF is is capable of generating multiple solutions, [3] uses graph-cuts to produce a single MAP estimate.", "startOffset": 65, "endOffset": 68}, {"referenceID": 15, "context": "Similarly, although the GAN method of [17] is capable of producing multiple solutions, they report that their GAN ignores the noise, and always predicts the same answer for each input.", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "This problem is fixed in [2] by introducing noise at multiple levels of the generator.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "This mixture of experts (MOE) approach was used in [1] mainly for image compression, rather than colorization per se.", "startOffset": 51, "endOffset": 54}, {"referenceID": 16, "context": "A third approach is to use a (conditional) variational autoencoder (VAE) [18] to capture dependencies amongst outputs via a low dimensional latent space.", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "To capture the dependence on the input image, [9] proposes to use a mixture density network (MDN) to learn a mapping from a gray input image to a distribution over the latent codes, which is then converted to a color image using the VAE\u2019s decoder.", "startOffset": 46, "endOffset": 49}, {"referenceID": 10, "context": "A fourth approach is to use a (conditional) generative adversarial network (GAN) [12] to train a generative model jointly with a discriminative model.", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "This approach results sharp images, but [17] reports that a GAN-based colorization results underperform previous CNN approaches [41].", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "This approach results sharp images, but [17] reports that a GAN-based colorization results underperform previous CNN approaches [41].", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "More recently, [2] have applied a slightly different GAN to colorization.", "startOffset": 15, "endOffset": 18}, {"referenceID": 18, "context": "Most papers (including ours) employ the \u201cctest10k\u201d split of the ImageNet validation dataset from [20] (see Section 4 for more details).", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "We propose a novel approach that uses a PixelCNN [36] to produce multiple low resolution color images, which are then deterministically converted to high resolution color images using a CNN refinement network.", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "Very recently, in a concurrent submission, [29] proposed an approach which is similar to ours.", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "We first pre-train the conditioning network on COCO image segmentation following [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 31, "context": "Inspired by the success of autoregressive models for unconditional image generation [35, 36] and super resolution [8], we use a conditional PixelCNN [36] to produce multiple low resolution color images.", "startOffset": 84, "endOffset": 92}, {"referenceID": 6, "context": "Inspired by the success of autoregressive models for unconditional image generation [35, 36] and super resolution [8], we use a conditional PixelCNN [36] to produce multiple low resolution color images.", "startOffset": 114, "endOffset": 117}, {"referenceID": 31, "context": "Inspired by the success of autoregressive models for unconditional image generation [35, 36] and super resolution [8], we use a conditional PixelCNN [36] to produce multiple low resolution color images.", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": ", [27, 19]) if necessary.", "startOffset": 2, "endOffset": 10}, {"referenceID": 17, "context": ", [27, 19]) if necessary.", "startOffset": 2, "endOffset": 10}, {"referenceID": 6, "context": "Our architecture is based on [8] who used PixelCNNs to perform super resolution (another one-tomany problem).", "startOffset": 29, "endOffset": 32}, {"referenceID": 27, "context": "We performed some preliminary experiments using Logistic mixture models to represent the output values as suggested by the PixelCNN++ of [31], as opposed to using multinomials over discrete values [36].", "startOffset": 137, "endOffset": 141}, {"referenceID": 31, "context": "We performed some preliminary experiments using Logistic mixture models to represent the output values as suggested by the PixelCNN++ of [31], as opposed to using multinomials over discrete values [36].", "startOffset": 197, "endOffset": 201}, {"referenceID": 34, "context": "During training, we \u201dclamp\u201d all the previous pixels to the ground truth values (an approach known as \u201dteacher forcing\u201d [39]), and just train the network to predict a single pixel at a time.", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "It is similar in architecture to the network used in [15] but with more layers in the decoding part.", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "Since the mapping from gray to color is one-to-many, we cannot evaluate performance by comparing the predicted color image to the \u201dground truth\u201d color image in terms of mean squared error or even other perceptual similarity metrics such as SSIM [37].", "startOffset": 245, "endOffset": 249}, {"referenceID": 36, "context": "Instead, we follow the approach of [41] and conduct a \u201dVisual Turing Test\u201d (VTT) using a crowd sourced human raters.", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "To be comparable with [41], we show the two images sequentially for 1 second each.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "2M training images from the ILSVRC-CLS dataset [30], and use 500 images from the \u201dctest10k\u201d split of the 50k ILSVRCCLS validation dataset proposed in [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "Based on these examples, it seems that the best methods include our method (PixColor), and several recent CNN-based methods, namely LTBC [15], LRAC [20], and CIC [41].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Based on these examples, it seems that the best methods include our method (PixColor), and several recent CNN-based methods, namely LTBC [15], LRAC [20], and CIC [41].", "startOffset": 148, "endOffset": 152}, {"referenceID": 36, "context": "Based on these examples, it seems that the best methods include our method (PixColor), and several recent CNN-based methods, namely LTBC [15], LRAC [20], and CIC [41].", "startOffset": 162, "endOffset": 166}, {"referenceID": 36, "context": "other methods, without needing to do any explicit reweighting of color bins, as was done in previous work [41].", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "To quantitatively assess how different these samples are from each other, we compute the multiscale SSIM [37] measure between pairs of samples.", "startOffset": 105, "endOffset": 109}], "year": 2017, "abstractText": "We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a \u201dVisual Turing Test\u201d.", "creator": "LaTeX with hyperref package"}}}