{"id": "1705.07213", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Securing Deep Neural Nets against Adversarial Attacks with Moving Target Defense", "abstract": "Deep Neural Networks (DNNs) are presently the state-of-the-art for image classification tasks. However, recent works have shown that these systems can be easily fooled to misidentify images by modifying the image in a particular way. Moreover, defense mechanisms proposed in the literature so far are mostly attack-specific and prove to be ineffective against new attacks. Indeed, recent work on universal perturbations can generate a single modification for all test images that is able to make existing networks misclassify 90% of the time. Presently, to our knowledge, no defense mechanisms are effective in preventing this. As such, the design of a general defense strategy against a wide range of attacks for Neural Networks becomes a challenging problem. In this paper, we derive inspiration from recent advances in the field of cybersecurity and multi-agent systems and propose to use the concept of Moving Target Defense (MTD) for increasing the robustness of well-known deep networks trained on the ImageNet dataset towards such adversarial attacks. In using this technique, we formalize and exploit the notion of differential immunity of different networks to specific attacks. To classify a single test image, we pick one of the trained networks each time and then use its classification output. To ensure maximum robustness, we generate an effective strategy by formulating this interaction as a Repeated Bayesian Stackelberg Game with a Defender and the Users. As a network switching strategy, we compute a Strong Stackelberg Equilibrium that optimizes the accuracy of prediction while at the same time reduces the misclassification rate on adversarial modification of test images. We show that while our approach produces an accuracy of 92.79% for the legitimate users, attackers can only misclassify images 58% (instead of 93.7%) of the time even when they select the best attack available to them.", "histories": [["v1", "Fri, 19 May 2017 22:36:55 GMT  (394kb,D)", "http://arxiv.org/abs/1705.07213v1", "10 page, 4 figures"], ["v2", "Fri, 22 Sep 2017 20:32:46 GMT  (145kb,D)", "http://arxiv.org/abs/1705.07213v2", "9 page, 5 figures"]], "COMMENTS": "10 page, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.GT", "authors": ["sailik sengupta", "tathagata chakraborti", "subbarao kambhampati"], "accepted": false, "id": "1705.07213"}, "pdf": {"name": "1705.07213.pdf", "metadata": {"source": "CRF", "title": "Securing Deep Neural Nets against Adversarial Attacks with Moving Target Defense", "authors": ["Sailik Sengupta", "Tathagata Chakraborti", "Subbarao Kambhampati"], "emails": ["sailiks@asu.edu", "tchakra2@asu.edu", "rao@asu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Attacks on Deep Neural Networks and Existing Defense Strategies", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2.2 Moving Target Defense (MTD) and its Adoption for Security of DNNs", "text": "Moving Target Defense (MTD) is a paradigm in software security that attempts to prevent an attacker from carrying out an attack by constantly switching between multiple software systems [14]. The practical use of MTD in web application systems has proven to be a difficult problem. [16] To our knowledge, in this paper we are designing the first universal security framework for neural networks using MTD (MTD-NN). Developing strategies for MTD systems has proven to be a difficult problem. [16] In order to provide guarantees for the security of such systems, it is necessary to think about these attacks in a multi-agent game theoretical manner, leading to defense strategies that outperform randomized strategies previously considered to be the best strategies for building robust systems. In this paper, therefore, we compile user interaction in an image classification application that is similar to NATS, although NATS is expected to provide a series of NATS-based security."}, {"heading": "3 Moving Target Defense for Deep Neural Networks (MTD-NN)", "text": "As we have already explained, the Defender has several system configurations in a Moving Target Defense system. The attacker has a number of attacks that he can execute concerning some of the configurations in the Defender's system. Faced with an input into the system, the Defender selects one of the configurations to execute the input and returns the outputs generated by that system. As the attacker does not know which system was specifically selected, it is no longer as effective as before (Figure 3). Thus, although randomization is required when selecting a classification configuration, it must be done in a non-trivial way to ensure maximum security. A potential disadvantage of such a framework is that it could reduce the accuracy of the overall system in classifying normal images. Therefore, we want to maintain good classification performance while guaranteeing a high level of security.In this section, we will first describe the agents within our framework and the actions that they can perform, including the description of the Defender's TDND and their frame configuration, and the way we will calculate them."}, {"heading": "3.1 Defender Configurations", "text": "The Defender configuration space in the MTD Framework for DNNs consists of a number of different DNNs trained for the same task but not affected by the same attack. Constitutional neural networks (CNNs) are known to achieve the best results when classifying images. So, although the different DNN configurations used by the Defender may vary in terms of the number of layers, parameters, hyperparameters, or activation functions, they must still use CNN units to achieve comparable results. Formally, N should denote the configuration of the Defender. In our case, N is a group of six neural networks that we will use for our MTD NN system (see Table 1)."}, {"heading": "3.2 User types and Universal Perturbation", "text": "Our second player, namely the users, are of two types of legitimate user (L) and opponent (A). L is trying to use the MTD-NN system to classify images for a specific task without any hostile intent; these are the target users of all current DNN designers who are just trying to improve accuracy; the second type, i.e. opponent A, is essentially trying to manipulate input images so that the DNNs misclassify the label for these inputs. L has a single action that gives inputs for classification where the attacker uses several attack actions that we are now defining. Universal interference are frames generated by an opponent, specific to a DNN, so that XOR-ed using the test images to generate modified test images will cause the system to incorrectly classify the actual labels of the test image [8]. Essentially, once they become a universal DNN for this system, they will manipulate a universal DNN for this to be effective."}, {"heading": "3.3 Differential Immunity", "text": "If this property applies to all attacks against an MTD system, we can be sure that there is at least something to gain by switching between multiple configurations. We call this differential immunity. To define differential immunity, we formally consider a function f: N: U \u2192 R + 0 that returns a non-negative impact value in the face of a particular configuration and attack, which is proportional to the damage done to the defender or the reward to an opponent. For example, let's look at a neural network n: N: N \u2192 R + 0 used to classify a modified image."}, {"heading": "3.4 Moving Target Defense Architecture as a Repeated Bayesian Game", "text": "Although we have designed MTD-NN, it will only be safe if it uses a random mixed strategy. For deterministic (or pure) strategies, an attacker knows what configuration the system is using to classify a test image, so it can select the most effective attack relevant to the DNN configuration, defeating the purpose of an MTD system. Selecting a unified random strategy (URS), i.e. retrieving any defender in the MTD-NN system with equal probability (in an unbiased manner), is often not the best approach in multi-agent systems."}, {"heading": "3.5 Defender\u2019s Strategy for Switching", "text": "It can be stated that the fulfillment of the multi-objective criterion for our system is essentially the solution of the Stackelberg balance, which provides the Defender with the optimal switching strategy. Ideally, for each i (N) there is an attack j (N), which represents the universal disturbance that occurs for the DNN i. Let's designate the strategy vector for the Defender as ~ x and its rewards as RDi, j if the Defender chooses to use the i-th network and the User chooses the j-th action. Let's also designate the strategy vectors for the Users as ~ qA and ~ qL, and the rewards as RAi, j and R L i, j for the Adversary and the Legitimate User. We will now use the DOBSS Solver [21] to optimize the Defender's reward."}, {"heading": "4 Experimental Results", "text": "For our experiments, we use the six different architectures that stand out in ILSVRC 2012 [22] multiple DNRC networks when the validation sets (50,000 frames) are useful. These architectures (shown in Table 1 and 2) are now the configurations of the defender under which it must switch to confuse the attacker who tries to misclassify the system, a particular test image. We use the universal perturbations (UP) provided in [8] as the attack actions of adversary A. These UPs are generated by ensuring that the l \u2212 \u221e standard of perturbations is less than a bound number. The rewards for this game are shown in Table 2 for A and Table 1 for L. Using the formula for differentiated immunity, we find that the differentiated immunity of our system is 0.34, which, although we produce the highest so-far.In Figure 3, we see that the net value (TN) given by the MD equation for the ND speaks."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we formulated a Moving Target Defense System to secure Deep Neural Networks MTD-NN by exploiting the differential immunity of each network. We show that by interpreting user interaction as a Repeated Bayesian Game, we are able to find effective strategies that minimize the deception rate for test images modified by an adversary, as well as maximize the accuracy of classification for legitimate test images. With this approach, we were able to show a > 20% increase in classification accuracy at full attack in experiments conducted on six popular DNNs. The method provided is generic and can be used for any machine learning model and data set, and provides a degree of protection against attacks. As future work, we plan to cast other machine learning models in this framework and use existing attacks to test the resilience of our system."}], "references": [{"title": "Automatic processing of handwritten bank cheque images: a survey", "author": ["R Jayadevan", "Satish R Kolhe", "Pradeep M Patil", "Umapada Pal"], "venue": "International Journal on Document Analysis and Recognition (IJDAR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Tracking and object classification for automated surveillance", "author": ["Omar Javed", "Mubarak Shah"], "venue": "Vision\u2014ECCV", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Road traffic sign detection and classification", "author": ["Arturo De La Escalera", "Luis E Moreno", "Miguel Angel Salichs", "Jos\u00e9 Mar\u00eda Armingol"], "venue": "IEEE transactions on industrial electronics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Security and Privacy (EuroS&P),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K Reiter"], "venue": "In Proceedings of the SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Practical black-box attacks against machine learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the Conference on Computer and Communications Security,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Universal adversarial perturbations", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Adversarial pattern classification using multiple classifiers and randomisation", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Robustness to adversarial examples through an ensemble of specialists", "author": ["Mahdieh Abbasi", "Christian Gagn\u00e9"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Towards a theory of moving target defense", "author": ["Rui Zhuang", "Scott A DeLoach", "Xinming Ou"], "venue": "In Proceedings of the First ACM Workshop on Moving Target Defense,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Toward a Moving Target Defense for Web Applications", "author": ["Marthony Taguinod", "Adam Doup\u00e9", "Ziming Zhao", "Gail-Joon Ahn"], "venue": "In Proceedings of the IEEE International Conference on Information Reuse and Integration (IRI),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A game theoretic approach to strategy generation for moving target defense in web applications", "author": ["Sailik Sengupta", "Satya Gautam Vadlamudi", "Subbarao Kambhampati", "Adam Doup\u00e9", "Ziming Zhao", "Marthony Taguinod", "Gail-Joon Ahn"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["Ken Chatfield", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games", "author": ["Praveen Paruchuri", "Jonathan P Pearce", "Janusz Marecki", "Milind Tambe", "Fernando Ordonez", "Sarit Kraus"], "venue": "In Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep Neural Networks are presently state-of-the-art systems in image classification and are used in many important tasks such as recognizing handwritten digits on cheques [1], object classification for automated surveillance [2] and autonomous vehicles [3].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Deep Neural Networks are presently state-of-the-art systems in image classification and are used in many important tasks such as recognizing handwritten digits on cheques [1], object classification for automated surveillance [2] and autonomous vehicles [3].", "startOffset": 225, "endOffset": 228}, {"referenceID": 2, "context": "Deep Neural Networks are presently state-of-the-art systems in image classification and are used in many important tasks such as recognizing handwritten digits on cheques [1], object classification for automated surveillance [2] and autonomous vehicles [3].", "startOffset": 253, "endOffset": 256}, {"referenceID": 3, "context": "In fact, in [4] and [5] authors show how models for ar X iv :1 70 5.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "In fact, in [4] and [5] authors show how models for ar X iv :1 70 5.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "Figure 1: Adversial attacks on DDNs - Gradient based perturbations (left) on handwritten digit recognition [4], black box attack (middle) [7] on stop signs using images which are practically indistinguishable, and universal perturbations (right) [8] when added to the original images on the left is able to make the DNN misclassify it on the right.", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "Figure 1: Adversial attacks on DDNs - Gradient based perturbations (left) on handwritten digit recognition [4], black box attack (middle) [7] on stop signs using images which are practically indistinguishable, and universal perturbations (right) [8] when added to the original images on the left is able to make the DNN misclassify it on the right.", "startOffset": 138, "endOffset": 141}, {"referenceID": 7, "context": "Figure 1: Adversial attacks on DDNs - Gradient based perturbations (left) on handwritten digit recognition [4], black box attack (middle) [7] on stop signs using images which are practically indistinguishable, and universal perturbations (right) [8] when added to the original images on the left is able to make the DNN misclassify it on the right.", "startOffset": 246, "endOffset": 249}, {"referenceID": 3, "context": "In [4], road signs saying \u2018stop\u2019 is misclassified, which can make an autonomous vehicle behave dangerously.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "as shown in [6].", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "Recent work on generating universal perturbation [8] is one case where all existing defense mechanisms fall short.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "This generated perturbation, when added to the original test images, can adversely bring down the classification accuracy of even the state-of-the-art classifiers (ResNet-152 [9]) from 95.", "startOffset": 175, "endOffset": 178}, {"referenceID": 9, "context": "\u2022 Gradient-based perturbations: Recent literature [10, 4, 5] has shown multiple ways of generating adversarial samples for a test image input to a DNN.", "startOffset": 50, "endOffset": 60}, {"referenceID": 3, "context": "\u2022 Gradient-based perturbations: Recent literature [10, 4, 5] has shown multiple ways of generating adversarial samples for a test image input to a DNN.", "startOffset": 50, "endOffset": 60}, {"referenceID": 4, "context": "\u2022 Gradient-based perturbations: Recent literature [10, 4, 5] has shown multiple ways of generating adversarial samples for a test image input to a DNN.", "startOffset": 50, "endOffset": 60}, {"referenceID": 6, "context": "\u2022 Black-box attacks: Latest work on black-box attacks against DNNs use distillation techniques and assume the presence of an oracle that provides test labels for a list of images the adversary provides [7], similar to chosen plaintext attacks.", "startOffset": 202, "endOffset": 205}, {"referenceID": 7, "context": "\u2022 Universal perturbations: This is capable of creating a single image for the set of all test samples and is able to make a DNN misclassify a vast majority of test samples [8].", "startOffset": 172, "endOffset": 175}, {"referenceID": 10, "context": "There has been some effort in trying to protect machine learning systems attacks like the ones above by using randomization techniques such as in [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "On the contrary, work on using ensemble models for DNNs simply try to increase classification accuracy for legitimate users but have no protection against adversarial modifications of test images [12].", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "There has also been work on using ensemble models to detect adversarial samples for the MINST dataset [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Moving Target Defense (MTD) is a paradigm used in software security that tries to prevent an attacker for executing an attack by constantly switching between multiple software systems [14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "Practical use of MTDs in Web Application Systems have been shown to enhance system security [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "[16] shows that in order to provide guarantees on the security of such systems, it is necessary to reason about these", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In this paper, we thus compile the user interaction in a image classification application driven by a set of DNNs into a Repeated Bayesian Game, similar to [16], providing provable guarantees on the expected performance and security of the DNNs.", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "Universal Perturbation are single images generated by an adversary, specific to a DNN, that when XOR-ed with the test images so as to generate modified test images, result in the system misclassifying actual labels of the test image [8].", "startOffset": 233, "endOffset": 236}, {"referenceID": 4, "context": "As shown by existing work [5], simple ideas like partitioning the training data and training multiple (C)NNs on the disjoint sets of this data does not make the networks differentially immune.", "startOffset": 26, "endOffset": 29}, {"referenceID": 15, "context": "picking up any DNN in the MTD-NN system with equal probability (in an unbiased manner) in often not be the best approach in multi-agent systems [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "MTD-NN System VGG-F [17] (92.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "9) CaffeNet [18] (83.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "6) GoogLeNet [19] (93.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "3) VGG-16 [20] (92.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "5) VGG-19 [20] (92.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "5) ResNet-152 [9] (95.", "startOffset": 14, "endOffset": 17}, {"referenceID": 20, "context": "We will now use the DOBSS solver [21] to optimize the defender\u2019s reward give the attacker chooses to maximize her reward.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "For our experiments, we use the six different architectures that have excelled in ILSVRC 2012 [22] validation set (50, 000 images).", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "We use the Universal Perturbations (UP) provided in [8] as attack actions of the adversary A.", "startOffset": 52, "endOffset": 55}], "year": 2017, "abstractText": "Deep Neural Networks (DNNs) are presently the state-of-the-art for image classification tasks. However, recent works have shown that these systems can be easily fooled to misidentify images by modifying the image in particular ways, often rendering them practically useless. Moreover, defense mechanisms proposed in the literature so far are mostly attack-specific and prove to be ineffective against new attacks. Indeed, recent work on universal perturbations can generate a single modification for all test images that is able to make existing networks misclassify 90% of the time. Presently, to our knowledge, no defense mechanisms are effective in preventing this. As such, the design of a general defense strategy against a wide range of attacks for Neural Networks becomes a challenging problem. In this paper, we derive inspiration from recent advances in the field of cybersecurity and multi-agent systems, and propose to use the concept of Moving Target Defense (MTD) for increasing the robustness of well-known deep networks trained on the ImageNet dataset towards such adversarial attacks. In using this technique, we formalize and exploit the notion of differential immunity of different networks to specific attacks. To classify a single test image, we pick one of the trained networks each time and then use its classification output. To ensure maximum robustness, we generate an effective strategy by formulating this interaction as a Repeated Bayesian Stackelberg Game (BSG) with a Defender (who hosts the classification networks) and Users (both Legitimate users and Attackers). As a network switching strategy, we compute a Strong Stackelberg Equilibrium that optimizes the accuracy of prediction while at the same time reduces the misclassification rate on adversarial modification of test images. We show that while our approach produces an accuracy of 92.79% for the legitimate users, attackers can only misclassify images 58% (instead of 93.7%) of the time even when they select the best attack available to them. This is at least twice as good, to sometimes even an order of magnitude better, compared the accuracy rates of the worst affected networks.", "creator": "LaTeX with hyperref package"}}}