{"id": "1705.07215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "How to Train Your DRAGAN", "abstract": "Generative Adversarial Networks have emerged as an effective technique for estimating data distributions. The basic setup consists of two deep networks playing against each other in a zero-sum game setting. However, it is not understood if the networks reach an equilibrium eventually and what dynamics makes this possible. The current GAN training procedure, which involves simultaneous gradient descent, lacks a clear game-theoretic justification in the literature. In this paper, we introduce regret minimization as a technique to reach equilibrium in games and use this to motivate the use of simultaneous GD in GANs. In addition, we present a hypothesis that mode collapse, which is a common occurrence in GAN training, happens due to the existence of spurious local equilibria in non-convex games. Motivated by these insights, we develop an algorithm called DRAGAN that is fast, simple to implement and achieves competitive performance in a stable fashion across different architectures, datasets (MNIST, CIFAR-10, and CelebA), and divergence measures with almost no hyperparameter tuning.", "histories": [["v1", "Fri, 19 May 2017 22:41:56 GMT  (2994kb,D)", "http://arxiv.org/abs/1705.07215v1", null], ["v2", "Wed, 24 May 2017 15:13:01 GMT  (2463kb,D)", "http://arxiv.org/abs/1705.07215v2", "Fixed multiple typos and added small clarifications. No new results"], ["v3", "Thu, 25 May 2017 00:51:40 GMT  (2463kb,D)", "http://arxiv.org/abs/1705.07215v3", "Updated Abstract. No new results"], ["v4", "Fri, 27 Oct 2017 21:47:51 GMT  (4869kb,D)", "http://arxiv.org/abs/1705.07215v4", "Some new results, fixes"]], "reviews": [], "SUBJECTS": "cs.AI cs.CV cs.GT cs.LG cs.NE", "authors": ["naveen kodali", "jacob abernethy", "james hays", "zsolt kira"], "accepted": false, "id": "1705.07215"}, "pdf": {"name": "1705.07215.pdf", "metadata": {"source": "CRF", "title": "How to Train Your DRAGAN", "authors": ["Naveen Kodali", "Jacob Abernethy"], "emails": ["nkodali3@gatech.edu", "jabernet@umich.edu", "hays@gatech.edu", "zkira@gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "Generative Adversarial Networks (GANs) [9] are a class of implicit generative models that have achieved significant success in generating realistic samples of high-dimensional data. GANs find applications in a variety of areas, including settings that require multimodal outputs, the use of model-based attachment algorithms, and the inclusion of blank data in semi-monitored settings [13]. At its core, the GAN training is framed as a zero-sum game between two players - a generator G and a discriminator D. The goal is to achieve a balance in this game in which the divergences between Pmodel and Preal are minimized."}, {"heading": "2 Related Work", "text": "There was a lot of work aimed at finding a stable way to train GANs. Radford et al. [11] proposed a stable family of architectures called deep revolutionary generative adversarial networks (DCGANs). We show that such constraints on architectures can be eased while still being able to achieve stability in the training process. In an alternative direction, a number of papers focused on the development of specific objective functions that improve the stability and performance of GANs. [16] introduced a variety of techniques to improve the quality of samples. Che et.al [12] proposed a family of regulators to address the problem of missing modes in GANs. Zhao et.al [17] introduced an energy-based GAN framework that is more stable to train. [14] developed unwanted GANs that take inspiration from game theory literature."}, {"heading": "3 Formulation and Proposed Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Game Formulation", "text": "The GAN framework consists of two actors - the generator, whose goal is to create realistic samples, and the discriminator, whose goal is to classify an instance as originating from training data or the generator. Generator model G is parameterized by \u03c6, takes a latent variable z as input and outputs sample G\u03c6 (z). discriminator model D is parameterized by \u03b8, takes a sample x as input and gives the probability that it is real. Models G, D are usually represented by deep networks, and their cost functions are represented as -J (D) (\u03c6, \u03b8) = \u2212 1 2 Ex x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (((x x x x x x x x x x (((((x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.2 Proposed Algorithm", "text": "Given this background, we first describe our algorithm and then provide our theoretical motivation and justification for it in Section 4. Algorithm 1 shows pseudo-code for our proposed approach. We deviate from the vanilla algorithm as we update the discriminator. Our proposed method includes constraint points in local regions around real examples to have Norm 1 gradients. To achieve this, we create three different mini-lots in each iteration: real data examples from the training set xi, samples from the normal distribution zi, and real data examples using small noise xi (the magnitude of noise determines the size of local regions). Ideally, we would like to be able to impose the constraint anywhere in these local regions, but we only apply it to randomly selected points (as shown in Step 6). The objective function of D is terminated with a regulatory term that penalizes violations of the above restrictions."}, {"heading": "4 Theoretical Foundations", "text": "In this section, we describe how the training procedure for GANs in general can be viewed through the lens of equilibrium calculation in convex / concave zero-sum games. Simultaneous gradient descend is consistent with the minimization techniques well studied in online learning and game theory communities. We observe, of course, that the lack of convexity leads to multiple saddle points (Nash balances), but we argue that the existence of such non-optimal solutions can be mitigated by appropriate regulation smoothing of the equilibrium function, an idea that Nash himself used in his early work [2]. We argue empirically for this hypothesis in section 5."}, {"heading": "4.1 Equilibrium Computation and Generalization via Regret Minimization", "text": "Von Neumann's Minimax theorem for zero-sum games in [1] or equivalent has the existence of mixed Nash balances (Nash and von Neumann originally envisaged two players selecting probability distributions via a finite strategy and receiving random rewards according to the payout function and these distributions. The Minimax theorem has been significantly generalized since this initial formulation, and the work of Sion [3] offers a broad class of scenarios in which inf sup {s takes into account the payout function and these distributions, for example, when we get convex and compact subsets or a function of Rn and a function of Rm."}, {"heading": "4.2 Local Nash equilibria and Smoothing in games", "text": "The discussion about this and the very general results that can be demonstrated with no-contrition techniques are still based on the crucial assumption that the payout function g (\u00b7, \u00b7) is convex-concave. In fact, convexity is used in a number of places in the evidence: Vanishing regret is based on a sequence of convex loss functions, and online-to-batch conversion is based on Jensen's inequality to show 1 T-T = 1 g. (1 T-T = 1-T, \u03c6) However, due to the non-convex nature of the GAN conversions, there is no guarantee that a single Nash equilibrium exists - there can be many non-optimal saddle points that are essentially local minimums of the game. (1 T-T = 1-Z) on the other hand, when the no-contrition dynamic in the convergence of convergence is converted from."}, {"heading": "4.3 Regularization Scheme", "text": "Our proposed approach falls roughly within this class of models, and specifically, improved WGAN is the closest related approach to ours. Although it was developed for completely different motives (Waterstone distance and game theory), both impose a similar form of constraint on the gradients of D (x). However, there are differences with important implications for generative modeling performance. The most important difference is that we impose these constraints only in local regions around real samples, while improved WGAN imposes them everywhere. We claim that the resulting class of functions that improve the WGAN model is severely limited compared to our method. This can be demonstrated by a simple experiment. In Figure 1, we track the performance of Vanilla GAN, improved Wasserstein GAN and our algorithm on the swissroll Dataset2 during the training period. Let's observe that the vanilla algorithm captures data density quickly, our algorithm captures it, but does not improve WAN accurately."}, {"heading": "5 Experimental Results", "text": "We present a series of experiments using the MNIST, CIFAR-10 and CelebA datasets, which show competitive incentive score results [16] and sample quality compared to baseline algorithms. Importantly, we also show a significant improvement in the stability of our algorithm over a large randomized set of network architectures and over a range of divergence measures. For the set of randomized architectures, we perform a qualitative and quantitative evaluation ranking of our algorithm over the vanilla GAN algorithm to assess improvements in stability, i.e. features related to the failure of the mode or failure to learn."}, {"heading": "5.1 Inception Scores on CIFAR-10 using DCGAN architecture", "text": "The DCGAN is a family of architectures designed to work well with the CIFAR-10 dataset [11]. They are ubiquitous in the GAN literature due to the instability of the vanilla algorithm in the general settings. We use this architecture to model the CIFAR-10 dataset and compare it with the vanilla GAN, WGAN, and the enhanced WGAN. We have not added any normalization layers in the discriminator for our algorithm and improved WGAN. As shown in Figure 2, our algorithm is competitive with the vanilla method in terms of speed, stability, and best receiver value. In some cases, our final sample quality is superior. In contrast, the improved WGAN achieves a significant 2We use experimental setups from http ps: / github.com / igul222 / improved _ willa _ gan _ training (a) Incore Score Vanilla-GAN (Vanilla) Imager 3.GAN (Incore)."}, {"heading": "5.2 BogoNet score to measure stability across architectures", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "5.3 Robustness across Divergence Measures", "text": "Nowozin et al. [15] show that any f-divergence can be used to train GANs using a suitable generator function. To demonstrate this in a challenging environment, we use the case \"4-layer 512-dim ReLU MLP Generator\" from the previous subsection. Our algorithm is stable in all cases except for total variation, while the vanilla algorithm failed in all cases (see Figure 3 for two examples and supplementary 2.5 for all five). Therefore, practitioners can now use a greater number of objective functions suitable for their application (as opposed to the LSGAN or WGAN algorithm family). (a) Reverse KL (b) Pearson \u043f2 Figure 3: Inception Score Curves for two divergence measures demonstrating superior stability of our algorithm."}, {"heading": "6 Conclusions", "text": "In this paper, we draw on the literature on game theory to justify the current GAN training and propose an improved algorithm. We combine the idea of minimizing remorse with GANs and present a novel way to think about the dynamics of this game. Against this background, we analyze the breakdown of the mode from a game theory perspective and hypothesize that false local balances are responsible for this problem. On this basis, we propose a novel regulation scheme as part of our DRAGAN algorithm. Unlike previous enhancements, our algorithm is easy to implement, fast, and improves stability in a variety of settings."}, {"heading": "1 Samples and Latent Space Walks", "text": "In this section, we provide additional examples across algorithms and datasets. Furthermore, Radford et al. [1] suggest that walking can uncover signs of memorization in a variety of ways that the generator learns. We use DCGAN to model MNIST and CelebA datasets using our algorithm, and the results shown below show that our generator learns smooth transitions between different images. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."}, {"heading": "2 Additional Experiments", "text": "To calculate this value, we need a classification model, and the original paper suggests using the perception model [2] for this purpose. However, this model is too powerful for smaller datasets such as MNIST, which makes it difficult to analyze differences in the modeling capabilities of different algorithms. Therefore, we use a custom classifier developed for the MNIST dataset to calculate the capture values in the next two subsections (we use a dagger to make this distinction)."}, {"heading": "2.1 One layer networks with MNIST dataset", "text": "We are designing a simple experiment in which G and D are fully interconnected networks with only one hidden layer. Vanilla GAN performs poorly even in this simple environment and we observe severe breakdown in mode. By contrast, our algorithm is consistently stable and receives samples of decent quality despite the limited setup."}, {"heading": "2.2 Smoothing penalty tracking", "text": "Stability in our algorithm is based on regulating D with the penalty (2 \u2212 1), as formulated in step 9 of algorithm 1. We will now take a closer look at mode collapse events occurring in the vanilla GAN and follow the penalty in parallel (see Figure 7). Note how the penalty increases when we enter the mode and falls when we recover, indicating that the nature of the gradient of the discriminator has a strong correlation with stability in GANs."}, {"heading": "2.3 8-Gaussians Experiment", "text": "We track the performance of the improved WGAN and our algorithm on the 8-gauss dataset over time. As shown in Figure 8, both approximate real density, but note that in the case of an improved WGAN, discriminator function is more limited throughout the training period."}, {"heading": "2.4 Robustness across DCGAN Architecture Variations", "text": "DCGANs have to be designed according to certain guidelines to make them stable [1]. We repeat the proposed rules here.1. Use all Convolutionary Networks that learn their own spatial downsampling (discriminator) or upsampling (generator) 2. Remove fully connected hidden layers for deeper architecture3. Use batch normalization in both the generator and the discriminator4. Use ReLU activation in the generator for all layers except the base layer that uses tanh5. Use LeakyReLU activation in the discriminator for all layers We show that such constraints on our algorithm can relax, and therefore practitioners can choose from a more diverse set of architectures. Below we present a series of experiments in which we remove various stabilizing components from the DCGAN architecture and analyze the performance of our algorithm. In any case, our algorithm is stable while the vanilla process fails."}, {"heading": "2.5 Robustness across Divergence Measures (Complete)", "text": "For reasons of space we showed only two examples for the variation of divergence measures. In the following we show results for all five measures."}, {"heading": "3 BogoNet Details", "text": "First, we choose from three families of architectures with probabilities - DCGAN (0.6), ResNet (0.2), MLP (0.2). Next, we further parameterise each family to create additional variations. Thus, the DCGAN family can lead to networks with or without batch normalization, LeakyReLU or Tanh nonlinearities. Number and width of filters, latent spatial dimensionality are other variations. Likewise, the number of layers and hidden units in each layer is chosen randomly for MLP. ForResNets we select their depth at random, thus creating a series of hard games that test the stability of a given training algorithm."}], "references": [{"title": "Zur Theorie der Gesellschaftsspiele", "author": ["J. von Neumann"], "venue": "Mathematische Annalen", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1928}, {"title": "Two-person cooperative games", "author": ["John Nash"], "venue": "Journal of the Econometric Society", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1953}, {"title": "On general minimax theorems", "author": ["Maurice Sion"], "venue": "Pacific J. Math 8.1", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1958}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Algorithmic game theory", "author": ["Noam Nisan"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Characterization and computation of local nash equilibria in continuous games", "author": ["Lillian J Ratliff", "Samuel A Burden", "S Shankar Sastry"], "venue": "Communication, Control, and Computing (Allerton),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Generative Adversarial Nets", "author": ["Ian Goodfellow"], "venue": "Advances in Neural Information Processing Systems 27. Ed. by Z. Ghahramani et al. Curran Associates, Inc.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "(Nov", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Mode Regularized Generative Adversarial Networks", "author": ["Tong Che"], "venue": "arXiv preprint arXiv:1612.02136", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "NIPS 2016 Tutorial: Generative Adversarial Networks", "author": ["Ian Goodfellow"], "venue": "arXiv preprint arXiv:1701.00160", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Unrolled Generative Adversarial Networks", "author": ["Luke Metz"], "venue": "CoRR abs/1611.02163", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "f-GAN: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Improved Techniques for Training GANs", "author": ["Tim Salimans"], "venue": "CoRR abs/1606.03498", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Wasserstein gan", "author": ["Martin Arjovsky", "Soumith Chintala", "L\u00e9on Bottou"], "venue": "arXiv preprint arXiv:1701.07875", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Improved Training of Wasserstein GANs", "author": ["Ishaan Gulrajani"], "venue": "arXiv preprint arXiv:1704.00028", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}], "referenceMentions": [{"referenceID": 8, "context": "Generative Adversarial Networks (GANs) [9] are a class of implicit generative models that have achieved significant success in generating realistic samples for high-dimensional data.", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "GANs find applications in a variety of domains, including settings demanding multi-modal outputs, use within model-based reinforcement learning algorithms, and incorporation of unlabeled data in semi-supervised settings [13].", "startOffset": 220, "endOffset": 224}, {"referenceID": 6, "context": "In this paper, we introduce regret-minimization (RM) [7, 6] as a technique to reach Nash Equilibrium in games.", "startOffset": 53, "endOffset": 59}, {"referenceID": 5, "context": "In this paper, we introduce regret-minimization (RM) [7, 6] as a technique to reach Nash Equilibrium in games.", "startOffset": 53, "endOffset": 59}, {"referenceID": 3, "context": "Leveraging the analysis of no-regret algorithms (specifically, online gradient descent [4]) \u2217Submitted to NIPS\u201917.", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "Further, we hypothesize that the difficulty in training GANs, especially due to mode collapse, results from the existence of spurious local Nash Equilibria in non-convex games [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 10, "context": "[11] proposed a stable family of architectures called deep convolutional generative adversarial networks (DCGANs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] introduced a variety of techniques to improve the quality of samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "al [12] proposed a family of regularizers to address the missing modes problem in GANs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "al [17] introduced energy based GAN framework which is more stable to train.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[14] developed unrolled GANs taking inspiration from game theory literature.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] proposed Wasserstein GAN which uses Earth-Mover distance as the objective to address problems with the vanilla objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposed an extension to address various shortcomings of the original WGAN and they impose the following condition on D ||\u2207x\u0302D\u03b8(x\u0302)||2 \u2248 1 where x\u0302 = (\u01eb)x + (1 \u2212 \u01eb)G\u03c6(z) is some point between randomly chosen real and fake sample pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "6: Define x\u0302 = \u03b1 \u00b7 x + (1\u2212 \u03b1) \u00b7 (x + \u03b4) for random \u03b1 \u2208 U [0, 1] 7: Update the discriminator by descending its gradient (using Adam):", "startOffset": 57, "endOffset": 63}, {"referenceID": 9, "context": "We now provide some implementation details for our algorithm: Hyperparameters We use Adam [10] with default parameters everywhere.", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "[19] use a similar penalty term albeit for different reasons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "std() \u00b7 U [0, 1]", "startOffset": 10, "endOffset": 16}, {"referenceID": 1, "context": "This idea was used by Nash himself in his early work [2].", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Von Neumann established the minimax theorem for zero-sum games in [1] or equivalently the existence of mixed Nash equilibria.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "The minimax theorem has been significantly generalized since this original formulation, and the work of Sion [3] provides a broad class of scenarios in which inf sup{} = sup inf{}.", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": ", Chapter 4 of [7]).", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "Let us consider a specific family of no-regret algorithms, and in particular we look at the well-studied familiy [6] known as Follow The Regularized Leader (FTRL).", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "then FTRL becomes the well-known Online Gradient Descent (OGD) [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "The celebrated \u201conline-to-batch conversion\u201d [5], now a standard result in online learning theory, implies that Ex,z[gx,z(\u03b8\u0304T , \u03c6)], for any \u03c6, is no more than the optimal value Ex,z[gx,z(\u03b8\u2217, \u03c6)] plus an \u201cestimation error\u201d bounded by E [ R1(T )+R2(T ) T ] , where the expectation is taken with respect to the sequence of samples observed along the way, and any randomness in the algorithm.", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "We present a series of experiments using the MNIST, CIFAR-10, and CelebA datasets demonstrating competitive inception score [16] results and sample quality compared to the baseline algorithms.", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "The DCGAN is a family of architectures designed to perform well with the vanilla training procedure [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "Similar to [18, 19] we have removed stabilizing components of DCGAN and demonstrated improved reliability compared to the vanilla GAN algorithm (see Section 2.", "startOffset": 11, "endOffset": 19}, {"referenceID": 18, "context": "Similar to [18, 19] we have removed stabilizing components of DCGAN and demonstrated improved reliability compared to the vanilla GAN algorithm (see Section 2.", "startOffset": 11, "endOffset": 19}, {"referenceID": 14, "context": "[15] show that any f -divergence can be used for training GANs with the help of an appropriate generatorfunction.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Generative Adversarial Networks have emerged as an effective technique for estimating data distributions. The basic setup consists of two deep networks playing against each other in a zero-sum game setting. However, it is not understood if the networks reach an equilibrium eventually and what dynamics makes this possible. The current GAN training procedure, which involves simultaneous gradient descent, lacks a clear game-theoretic justification in the literature. In this paper, we introduce regret minimization as a technique to reach equilibrium in games and use this to motivate the use of simultaneous GD in GANs. In addition, we present a hypothesis that mode collapse, which is a common occurrence in GAN training, happens due to the existence of spurious local equilibria in non-convex games. Motivated by these insights, we develop an algorithm called DRAGAN that is fast, simple to implement and achieves competitive performance in a stable fashion across different architectures, datasets (MNIST, CIFAR-10, and CelebA), and divergence measures with almost no hyperparameter tuning.", "creator": "LaTeX with hyperref package"}}}