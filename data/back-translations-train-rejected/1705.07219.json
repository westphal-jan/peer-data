{"id": "1705.07219", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "GAR: An efficient and scalable Graph-based Activity Regularization for semi-supervised learning", "abstract": "In this paper, we propose a novel graph-based approach for semi-supervised learning problems, which considers an adaptive adjacency of the examples throughout the unsupervised portion of the training. Adjacency of the examples is inferred using the predictions of a neural network model which is first initialized by a supervised pretraining. These predictions are then updated according to a novel unsupervised objective which regularizes another adjacency, now linking the output nodes. Regularizing the adjacency of the output nodes, inferred from the predictions of the network, creates an easier optimization problem and ultimately provides that the predictions of the network turn into the optimal embedding. Ultimately, the proposed framework provides an effective and scalable graph-based solution which is natural to the operational mechanism of deep neural networks. Our results show state-of-the-art performance within semi-supervised learning with the highest accuracies reported to date in the literature for SVHN and NORB datasets.", "histories": [["v1", "Fri, 19 May 2017 23:02:58 GMT  (649kb,D)", "http://arxiv.org/abs/1705.07219v1", "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017)"]], "COMMENTS": "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ozsel kilinc", "ismail uysal"], "accepted": false, "id": "1705.07219"}, "pdf": {"name": "1705.07219.pdf", "metadata": {"source": "CRF", "title": "GAR: An efficient and scalable Graph-based Activity Regularization for semi-supervised learning", "authors": ["Ozsel Kilinc"], "emails": ["ozsel@mail.usf.edu", "iuysal@usf.edu"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related work", "text": "Consider a semi-supervised learning problem in which basic truth markers are determined from m observations between the individual reference variables, while a subset of mL examples and the designations of the complementary subset of mU examples are unknown, where typically mL mU is used. Let x1: m and y1: m denote the input vectors and the output predictions respectively, and t1: mL denote the available output markers. The main goal is to train a classifier f: x \u2192 y using all m observations, which is more accurate than another classifier using only the designated examples. Graph-based semi-supervised methods consider a contiguous graph G = (V, E), from which the settings V correspond to all m examples and edges specified by an m \u00b7 m adjactivity matrix A, the entries of which indicate the similarity between the approximations of the adju indicates many different adaptations of A. Consider a semi-supervised learning problem."}, {"heading": "3 Proposed framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Bipartite graph approach", "text": "Instead of estimating the adjacence matrix A by means of an auxiliary algorithm such as the nearest neighboring or auxiliary knowledge, we propose to use the actual predictions of a neural network model initialized by a supervised pre-schooling, with the probability of the subdivision of the mL being characterized by examples from the jth class. Suppose that these predictions are in fact obtained as a bipartite diagram G = (V, E) as m matrix, where n is the number of output nodes and Bij is the probability of the example belonging to jth class. We observe that these predictions actually define a bipartite diagram G = (V, E) whose links V \u0445 are m examples together with n output nodes, and Bij is the probability of the example belonging to jth class."}, {"heading": "3.2 Adaptive adjacency", "text": "In fact, the idea of upgrading B-matrix matrix matrix matrix matrix matrix matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matri"}, {"heading": "3.3 Activity regularization", "text": "s leave Y (l) the output of the nodes at layer l. Y (0) = X is the input and f (X) = f (L) = Y (L) = Y is the output of the entire mesh. W (l) and b (l) are the weights and distortions of layer l (8). However, then the forward trajectory of the neural networks can be written in such a way that Y (l) = f (l) (X) = h (l) (Y) (l \u2212 1) W (l) + b (l)) (8), where the activation function is applied to layer l. In the proposed framework, instead of using the output probabilities of the softmax nodes, we use the activations at their inputs to calculate the regulation."}, {"heading": "3.4 Training", "text": "The training of the proposed framework consists of two successive steps: supervised pretraining and subsequent unattended regularization. We use stochastic gradient descent in mini-batch mode [Bottou, 2010] to optimize both steps. In fact, a mini-batch mode is required for the unattended task, since the proposed regularizers are implicitly dependent on comparing the examples with each other. Algorithm 1 below describes the entire training process. Pretraining is a typical supervised training task in which mL examples XL = [x1,..., xmL] T are updated into the network with the appropriate soil truth labels tL = [t1,..., tmL] T and the network parameters to minimize protocol loss L (.). Once the pretraining is completed, this monitored target is never visited again and the labels tL are never reintroduced into the network."}, {"heading": "4 Experimental results", "text": "The models were implemented in Python using Keras [Chollet, 2015] and Theano [Theano Development Team, 2016]."}, {"heading": "Supervised pretraining:", "text": "Input: XL = [x1,..., xmL] T, tL = [t1,,..., tmL] T, Repeat batch size b {(X '1, t '1),..., (X'mL / b, t'mL / b)} \u2190 \u2212 (XL, tL) / / Shuffle and create batch paarsfor i \u2190 1 to mL / b do Take ith pair (X'i, t'i) Take a gradient step for L (f (X'i), t'i) until the stop criteria are met"}, {"heading": "Unsupervised training:", "text": "Input: Model, XL = [x1,..., xmL] T, XU = [xmL + 1,,..., xm] T, bL, bUrepeat {X '1,..., X'mU / bU,} \u2190 \u2212 XU / / Shuffle and create input batchesfor i \u2190 1 to mU / bU do Take ith input batch X \u0301 i X \ufffd \u2190 \u2212 random (XL, bL) / / Randomly sample bL examples from XL Take a gradient step for U (g ([X'T i X \ufffd T] T) T), until stop criteria is methttp: / / github.com / ozcell / phdwork that can be reproduced the experimental results received on the three image datasets, MNIST [LeCun et al., 1998], SVHN [Netzer et al., 2011] and NORB [LeCun et al., 2004]."}, {"heading": "4.1 MNIST", "text": "In MNIST, experiments were performed with 4 different settings for the number of described examples, i.e. mL = {100, 600, 1000, 3000} following the literature used for comparison results. Unattended regularization parts were formed by visualizing bL = 16 and bU = 112.Figure 1 the realization of the graph-based approach in this paper with real predictions for the MNIST dataset. After supervised pretraining, most of the examples are associated with multiple output nodes at the same time. Indeed, the graph between the GM examples (derived from M = BBT) looks like a sea of edges. However, thanks to pretraining, some of these edges are actually quite close to the numerical probability value of 1."}, {"heading": "4.2 SVHN and NORB", "text": "Both data sets represent a significant jump in the difficulty of classification compared to the MNIST dataset. Table 2 summarizes the semi-monitored test accuracies observed on SVHN and NORB. For SVHN experiments, 1000 marked examples were selected from 73257 training examples. In NORB experiments, however, 2 different settings for the number of marked examples (531131 more samples) were omitted from unattended training or not. In both experiments, the same batch ratio was used as bL = 96, bU = 32. In NORB experiments, 2 different settings for the number of marked examples were used, i.e. mL = 300, 1000} with the same batch ratio as bL = 32, bU = 96. Both results are used for comparative purposes."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a novel graph-based framework that takes into account the adaptive adaptability of examples M derived from the predictions of a neural network model. If the adaptive adaptation approach is well constrained, it contributes to improved accuracy results and automatically results in the network predictions becoming the optimal embedding without requiring additional steps such as applying laplac eigenmaps. We met these limitations by defining a regulation of the neighborhood of the starting nodes N, which is also derived from the network predictions. Such regulation helped us to develop an efficient and scalable framework that is natural for the operation of neural networks. Through this framework, we established the state of the art within semi-supervised learning on SVHN and NORB datasets. Preschooling (baseline) 19.11% (\u00b1 1.09) 17.93% (1.07 \u00b1 10.00% (1.67%) (1.65%) reported as pre-schooling (0.82%)."}, {"heading": "A Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Datasets used in the experiments", "text": "\u2022 MNIST: Images were normalized by 255. \u2022 SVHN: We applied random centering and normalization, i.e. we set each sample to 0 and divided each input by its standard deviation. \u2022 NORM: According to [Maal\u00f8e et al., 2016], images were sampled down to 32 \u00d7 32. We added uniform noise between 0 and 1 to each pixel value. First, we normalized the NORB dataset by 256, then we applied both random centering and normalization, as well as functional centering and normalization."}, {"heading": "A.2 Models used in the experiments", "text": "Table 4 summarizes all the models used in the experiments. MNIST results were reported with the model called 6-layer CNN, while SVHN results were obtained with the 9-layer CNN-2 model and NORB results with the 9-layer CNN model. Results obtained with different models are also presented in the following sections to show the effects of the selected model on test accuracy. In all models, the ReLU activation function was used. Both for supervised pretraining and for unattended regularization, models with stochastic gradient descent were trained with the following settings: lr = 0.01, decay = 1e \u2212 6, momentum = 0.95 with Nesterov updates."}, {"heading": "A.3 Effects of hyperparameters", "text": "The proposed unattended regularization improves test accuracy in all models. However, the best case depends on the model specifications chosen. A.3.2 Effects of the bL / bU ratio and the use of dropout during unattended regularization The labeled / unlabeled data ratios of unattended regularization, however, are the most critical hyperparameter of the proposed regularization. Figure 4 illustrates the effect of this ratio for MNIST and SVHN datasets. These two datasets have different characteristics. MNIST datasets exhibit a smaller variance between their samples in relation to SVHN. As a result, even if the designated examples introduced during supervised training are not mixed with the unattended training parts, i.e. bL = 0, this has no dramatic effect on performance."}, {"heading": "A.4 More on activity regularization", "text": "Remember that v = [N11 N22. nnn] andV: = vTv = 1111N11N11N2222."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Belkin", "Niyogi", "M. 2003] Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Belkin et al", "M. 2006] Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Generative adversarial nets", "author": ["Goodfellow et al", "I.J. 2014] Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A.C. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Hadsell et al", "R. 2006] Hadsell", "S. Chopra", "Y. LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton et al", "G.E. 2006] Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural computation,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma et al", "D.P. 2014] Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Welling", "D.P. 2013] Kingma", "M. Welling"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "The mnist database of handwritten digits", "author": ["LeCun et al", "Y. 1998] LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun et al", "Y. 2004] LeCun", "F.J. Huang", "L. Bottou"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Auxiliary deep generative models", "author": ["Maal\u00f8e et al", "L. 2016] Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Maaten", "Hinton", "2008] Maaten", "L. v. d", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Distributional smoothing by virtual adversarial examples. CoRR, abs/1507.00677", "author": ["Miyato et al", "T. 2015] Miyato", "S. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer et al", "Y. 2011] Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Feature selection, l 1 vs. l 2 regularization, and rotational invariance", "author": ["Y. A"], "venue": "[Ng,", "citeRegEx": "A.,? \\Q2004\\E", "shortCiteRegEx": "A.", "year": 2004}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["Ranzato", "Szummer", "M. 2008] Ranzato", "M. Szummer"], "venue": "In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML", "citeRegEx": "Ranzato et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "Semi-supervised learning with ladder networks", "author": ["Rasmus et al", "A. 2015] Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende et al", "D.J. 2014] Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Rule-injection hints as a means of improving network performance and learning time", "author": ["Suddarth", "Kergosien", "S.C. 1990] Suddarth", "Y.L. Kergosien"], "venue": "In Neural Networks, EURASIP Workshop", "citeRegEx": "Suddarth et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Suddarth et al\\.", "year": 1990}, {"title": "Deep learning via semi-supervised embedding", "author": ["Weston et al", "J. 2012] Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "In Neural Networks: Tricks of the Trade - Second Edition,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Revisiting semisupervised learning with graph embeddings", "author": ["Yang et al", "Z. 2016] Yang", "W.W. Cohen", "R. Salakhutdinov"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Zhu et al", "X. 2003] Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "In Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we propose a novel graph-based approach for semi-supervised learning problems, which considers an adaptive adjacency of the examples throughout the unsupervised portion of the training. Adjacency of the examples is inferred using the predictions of a neural network model which is first initialized by a supervised pretraining. These predictions are then updated according to a novel unsupervised objective which regularizes another adjacency, now linking the output nodes. Regularizing the adjacency of the output nodes, inferred from the predictions of the network, creates an easier optimization problem and ultimately provides that the predictions of the network turn into the optimal embedding. Ultimately, the proposed framework provides an effective and scalable graph-based solution which is natural to the operational mechanism of deep neural networks. Our results show state-of-the-art performance within semi-supervised learning with the highest accuracies reported to date in the literature for SVHN and NORB datasets.", "creator": "LaTeX with hyperref package"}}}