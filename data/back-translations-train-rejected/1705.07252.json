{"id": "1705.07252", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "SVM via Saddle Point Optimization: New Bounds and Distributed Algorithms", "abstract": "Support Vector Machine is one of the most classical approaches for classification and regression. Despite being studied for decades, obtaining practical algorithms for SVM is still an active research problem in machine learning. In this paper, we propose a new perspective for SVM via saddle point optimization. We provide an algorithm which achieves $(1-\\epsilon)$-approximations with running time $\\tilde{O}(nd+n\\sqrt{d / \\epsilon})$ for both separable (hard margin SVM) and non-separable cases ($\\nu$-SVM ), where $n$ is the number of points and $d$ is the dimensionality. To the best of our knowledge, the current best algorithm for hard margin SVM achieved by Gilbert algorithm~\\cite{gartner2009coresets} requires $O(nd / \\epsilon )$ time. Our algorithm improves the running time by a factor of $\\sqrt{d}/\\sqrt{\\epsilon}$. For $\\nu$-SVM, besides the well known quadratic programming approach which requires $\\Omega(n^2 d)$ time~\\cite{joachims1998making,platt199912}, no better algorithm is known. In the paper, we provide the first nearly linear time algorithm for $\\nu$-SVM. We also consider the distributed settings and provide distributed algorithms with low communication cost via saddle point optimization. Our algorithms require $\\tilde{O}(k(d +\\sqrt{d/\\epsilon}))$ communication cost where $k$ is the number of clients, almost matching the theoretical lower bound.", "histories": [["v1", "Sat, 20 May 2017 03:06:13 GMT  (1558kb,D)", "https://arxiv.org/abs/1705.07252v1", "20 pages"], ["v2", "Tue, 23 May 2017 07:09:30 GMT  (0kb,I)", "http://arxiv.org/abs/1705.07252v2", "Error for template"], ["v3", "Wed, 24 May 2017 05:40:16 GMT  (1555kb,D)", "http://arxiv.org/abs/1705.07252v3", null]], "COMMENTS": "20 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yifei jin", "lingxiao huang", "jian li"], "accepted": false, "id": "1705.07252"}, "pdf": {"name": "1705.07252.pdf", "metadata": {"source": "CRF", "title": "SVM via Saddle Point Optimization: New Bounds and Distributed Algorithms", "authors": ["Yifei Jin", "Lingxiao Huang", "Jian Li"], "emails": [], "sections": [{"heading": null, "text": "), where n is the number of points and d is the dimensionality. To our knowledge, the currently best algorithm for hard-margin SVM achieved by the Gilbert algorithm [16] requires O (nd /) time. Our algorithm improves runtime by a factor \u221a d / \u221a. For \u03bd-SVM, we do not provide a better algorithm in addition to the well-known square programming approach, where the required \"n2d\" time [21, 31] is not available. In the paper, we provide the first almost linear time algorithm for \u03bd-SVM. We also take into account distributed settings and provide distributed algorithms with low communication costs through saddle point optimization. Our algorithms require communication costs O (k (d + \u221a d /)) -where k is the number of clients, which is almost the theoretical lower limit."}, {"heading": "1 Introduction", "text": "It is a basic model for classification and regression [7, 11], widely used in many areas such as natural language, computer vision and bioinformatics. Consider a set of instance-label pairs (xi, yi) for i [n], xi-Rd, yi [S]. In the hard-margin scenario, where all marked points are linearly separable, the goal of the SVM is to find a separating hyperplane so that dots with different labels are separated by the hyperplane, and the margin of separation is maximized. Several SVM variants have been developed to handle linear non-separable cases (see the common strategy for these variants is to add a penalty term for the misclassified dots. The most common penalty terms are l1 and l2 losses. The corresponding SVM variant for l2-SVM or the standard version SVM in the literature."}, {"heading": "2 Saddle Point Optimization for SVM", "text": "In this section, we first formulate both hard-margin SVM and \u03bd-SVM and show that they can be reduced to saddle point optimizations. Afterwards, we provide an SVMSPSolver algorithm to solve saddle point optimizations. For convenience, the standard vectors in the work are all column vectors."}, {"heading": "2.1 Formulate SVM as Saddle Point Optimization", "text": "It is well known that the hard margin SVM can be formalized as the following square programming [11].min Suppose we have n dots xi-Rd = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 2 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 2 2 \u00b7 2 2 2 2 \u00b7 2 2 2 2 2 \u00b7 2 2 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 2 2 2 \u00b7 2 2 2 2 \u00b7 2 2 2 2 2 2 \u00b7 2 2 2 2 2 2 \u00b7 2 = 2 = 2 = 2 = 2 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="}, {"heading": "2.2 Saddle Point Optimization Algorithms for SVM", "text": "In this section, we propose efficient algorithms to solve HM saddle (4), and we will then most likely select them. The framework is inspired by the previous work of Allen-Zhu et al. [3] They provide an L1L2SPSolver algorithm for saddle point optimization. However, we mentioned in \"related work\" that their algorithm does not imply an effective SVM algorithm. [4] Instead, we show that under the same pre-processing step, through some modified update rules, we can apply their framework to solve HM saddle and saddle efficiently."}, {"heading": "3 Distributed SVM", "text": "We consider a popular distributed environment: the servers and clients stored in the server to represent a variable number stored in the server. (First, we initialize some parameters in each client as a pre-processing step in algorithm 1 (see algorithm 3 for the pseudo-code).Each client maintains the same random diagonal matrix Dd \u00b7 d and the total number of points in each client as a pre-processing step in algorithm 1 (see algorithm 3 for the pseudo-code).Each client maintains the same diagonal matrix Dd and the total number of points in each client. (i.e, P | n1 and n2).5 In addition, each client C applies a Hadamard transformation to its own data and initializes the partial probability vectors C.2 for its own points."}, {"heading": "4 Experiments", "text": "In this section we first compare our SVMSPSolver with the NuSVC library in scikitlearn [30]. We show that under the same parameters for \u03bd-SVM6, our algorithm achieves better accuracy with significantly less time. Second, in the distributed environment, we compare DisSVMSPSolver with two distributed algorithms for SVM, HOGWILD! [32] and distributed Gilbert algorithm [25]. Our simulation shows that our DisSVMSPSolver has lower communication costs in practice. Our platform's CPU is Intel (R) Xeon (R) CPU E5-2690 v3 @ 2.60GHz and the system is CentOS Linux. We use both synthetic and real data sets. The real data comes from [8]. See Appendix E for the way of generating synthetic data. We can generate synthetic data."}, {"heading": "A Missing Proofs in Section 2.1", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & & # 10; & # 10; & & # 10; & & # 10; & & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & & # 10; & &"}, {"heading": "B The Equivalence of the Explicit and Implicit Update Rules of \u03b7 and \u03be", "text": "Lemma 9 (Update of the rules of HM-Saddle) + 1. The following two updated rules are equivalent: < p (t + 1) = p (t + 1) = p (t) = p (t) + p (n) = p (t) + p (t) \u2212 p (t) + p (n) \u2212 p (t) \u2212 p (n) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t), w (t) \u2212 p (t), w (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t) \u2212 p (t), f)."}, {"heading": "C Proof of Theorem 6", "text": "It is not the first time that we are looking for a solution. (...) It is the second time that we are finding a solution. (...) It is the first time that we are finding a solution. (...) It is the second time that we are finding a solution. (...) It is the second time that we are finding a solution. (...) It is the third time that we are finding a solution. (...) It is the third time that we are finding a solution. (...) It is the third time that we are finding a solution. (...) It is the first time that we are finding a solution. (...) It is the second time that we are finding a solution. (...) It is the third time that we are finding a solution. (...) It is the third time that we are finding a solution. (...) It is the first time that we are finding a solution. (...) It is the second time that we are finding a solution. (...) It is the third time that we are finding a solution."}, {"heading": "D Missing Details in Section 3", "text": "First, we give the pseudo code of DisSVMSPSolver. See algorithm 3 for the preprocessing step for each client. Remember that we assume that there is a vector with all components, which is 1, 2,. \u2212 \u2212 \u2212 The initialization is as follows: \u2212 C.X + = HD \u00b7 [x + 1, x + 2,., x + m1], C.\u03b7 [\u2212 1] = C.\u03b7 [0] = C.\u03b7 [n \u2212 1 C.X \u2212 HD \u00b7 [x \u2212 1, x \u2212 2,."}, {"heading": "E Supplementary of Experiments", "text": "The real data comes from [8]. The synthetic data is generated as follows: For the divisible data, we randomly select a hyperplane that overlaps with the standard ball in the Rd space. Then, we randomly balance n points in a subset of the standard sphere so that the ratio of the maximum distance between the points to H is less than \u03b22 = 0.1. Let the labels of the points be about H + 1 and others \u2212 1. For the non-divisible data, the difference is that for these points with the distance to H less than \u03b22 = 0.1, we randomly select their labels to be with equal probability. In addition, we also use the real world including the divisible data \"mushrooms\" and non-divisible data \"w8a,\" gisette, \"\" a1a, \"\" a5a. \""}], "references": [{"title": "Faster dimension reduction", "author": ["Nir Ailon", "Bernard Chazelle"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Katyusha: The first direct acceleration of stochastic gradient methods", "author": ["Zeyuan Allen-Zhu"], "venue": "Proceedings of the 49th annual ACM symposium on theory of computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Optimization algorithms for faster computational geometry", "author": ["Zeyuan Allen-Zhu", "Zhenyu Liao", "Yang Yuan"], "venue": "In LIPIcs-Leibniz International Proceedings in Informatics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Duality and geometry in svm classifiers", "author": ["Kristin P Bennett", "Erin J Bredensteiner"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Convex optimization algorithms", "author": ["Dimitri P Bertsekas", "Athena Scientific"], "venue": "Athena Scientific Belmont,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard E Boser", "Isabelle M Guyon", "Vladimir N Vapnik"], "venue": "In Proceedings of the fifth annual workshop on Computational learning theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Psvm: Parallelizing support vector machines on distributed computers", "author": ["Edward Y Chang"], "venue": "In Foundations of Large-Scale Multimedia Information Management and Retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Coresets, sparse greedy approximation, and the frank-wolfe algorithm", "author": ["Kenneth L Clarkson"], "venue": "ACM Transactions on Algorithms (TALG),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "A geometry interpretation of \u03bc-svm classifiers", "author": ["DJ Crisp", "CJC Burges"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Consensus-based distributed support vector machines", "author": ["Pedro A Forero", "Alfonso Cano", "Georgios B Giannakis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Optimized cutting plane algorithm for support vector machines", "author": ["Vojt\u011bch Franc", "Soeren Sonnenburg"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Coresets for polytope distance", "author": ["Bernd G\u00e4rtner", "Martin Jaggi"], "venue": "In Proceedings of the twenty-fifth annual symposium on Computational geometry,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "An iterative procedure for computing the minimum of a quadratic form on a convex set", "author": ["Elmer G Gilbert"], "venue": "SIAM Journal on Control,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1966}, {"title": "Parallel support vector machines: The cascade svm", "author": ["Hans Peter Graf", "Eric Cosatto", "Leon Bottou", "Igor Durdanovic", "Vladimir Vapnik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Maximum margin coresets for active and noise tolerant learning", "author": ["Sariel Har-Peled", "Dan Roth", "Dav Zimak"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin", "S Sathiya Keerthi", "Sellamanickam Sundararajan"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Making large-scale svm learning practical", "author": ["Thorsten Joachims"], "venue": "Technical report,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Online learning with kernels", "author": ["Jyrki Kivinen", "Alexander J Smola", "Robert C Williamson"], "venue": "IEEE transactions on signal processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Communication complexity", "author": ["Eyal Kushilevitz"], "venue": "Advances in Computers,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Distributed and robust support vector machine", "author": ["Yangwei Liu", "Hu Ding", "Ziyun Huang", "Jinhui Xu"], "venue": "In LIPIcs-Leibniz International Proceedings in Informatics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Distributed parallel support vector machines in strongly connected networks", "author": ["Yumao Lu", "Vwani Roychowdhury", "Lieven Vandenberghe"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Distributed support vector machines", "author": ["A Navia-Vazquez", "D Gutierrez-Gonzalez", "Emilio Parrado-Hern\u00e1ndez", "JJ Navarro-Abellan"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Average and randomized communication complexity", "author": ["Alon Orlitsky", "Abbas El Gamal"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1990}, {"title": "Solving large scale linear svm with distributed block minimization", "author": ["Dmitry Pechyony", "Libin Shen", "Rosie Jones"], "venue": "In NIPS workshop on Big Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "12 fast training of support vector machines using sequential minimal optimization", "author": ["John C Platt"], "venue": "Advances in kernel methods,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "New approximation algorithms for minimum enclosing convex shapes", "author": ["Ankan Saha", "SVN Vishwanathan", "Xinhua Zhang"], "venue": "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "New support vector algorithms", "author": ["Bernhard Sch\u00f6lkopf", "Alex J Smola", "Robert C Williamson", "Peter L Bartlett"], "venue": "Neural computation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Bundle methods for machine learning", "author": ["Alexander J Smola", "SVN Vishwanathan", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Simpler core vector machines with enclosing balls", "author": ["Ivor W Tsang", "Andras Kocsor", "James T Kwok"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Core vector machines: Fast svm training on very large data sets", "author": ["Ivor W Tsang", "James T Kwok", "Pak-Ming Cheung"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Some complexity questions related to distributive computing (preliminary report)", "author": ["Andrew Chi-Chih Yao"], "venue": "In Proceedings of the eleventh annual ACM symposium on Theory of computing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1979}, {"title": "Efficient distributed linear classification algorithms via the alternating direction method of multipliers", "author": ["Caoxie Zhang", "Honglak Lee", "Kang G Shin"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization", "author": ["Yuchen Zhang", "Xiao Lin"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "To the best of our knowledge, the current best algorithm for hard margin SVM achieved by Gilbert algorithm [16] requires O(nd/ ) time.", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "For \u03bd-SVM, besides the well known quadratic programming approach which requires \u03a9(nd) time [21, 31], no better algorithm is known.", "startOffset": 91, "endOffset": 99}, {"referenceID": 29, "context": "For \u03bd-SVM, besides the well known quadratic programming approach which requires \u03a9(nd) time [21, 31], no better algorithm is known.", "startOffset": 91, "endOffset": 99}, {"referenceID": 6, "context": "Support Vector Machine (SVM) is a fundamental model for classification and regression [7, 11], widely used in many areas such as nature language process, computer vision, and bioinformatics.", "startOffset": 86, "endOffset": 93}, {"referenceID": 14, "context": "Several SVM variants have been designed to handle linearly non-separable cases (see [16]).", "startOffset": 84, "endOffset": 88}, {"referenceID": 32, "context": "\u03bd-SVM is first proposed by Sch\u00f6lkopf and Smola [34].", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "The advantage of \u03bd-SVM is that the parameter \u03bd has a clearer meaning, which is always between [0, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 32, "context": "[34] showed that \u03bd is an upper bound on the fraction of margin errors and a lower bound on the fraction of support vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "It takesO(nd) time by solving quadratic programs directly [21, 31].", "startOffset": 58, "endOffset": 66}, {"referenceID": 29, "context": "It takesO(nd) time by solving quadratic programs directly [21, 31].", "startOffset": 58, "endOffset": 66}, {"referenceID": 7, "context": "QP-based algorithms are widely used in open source projects such as libsvm [8], scikit-learn [30] and so on.", "startOffset": 75, "endOffset": 78}, {"referenceID": 28, "context": "QP-based algorithms are widely used in open source projects such as libsvm [8], scikit-learn [30] and so on.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "For hard-margin SVM, based on the geometric linear separable property, Gartner and Jaggi [16] showed that Gilbert algorithm [17] achieves a (1\u2212 )-approximation with O(nd/ \u03b2) running time where \u03b2 is the distance between the two polytopes of the two types of points after we scale all points in a unit ball.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "For hard-margin SVM, based on the geometric linear separable property, Gartner and Jaggi [16] showed that Gilbert algorithm [17] achieves a (1\u2212 )-approximation with O(nd/ \u03b2) running time where \u03b2 is the distance between the two polytopes of the two types of points after we scale all points in a unit ball.", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "For the l2-SVM and C-SVM, since we can transform the quadratic programs to a single-objective unconstrained optimization problem, there also exist efficient algorithms for the two variants [23, 35, 13, 13, 15, 2].", "startOffset": 189, "endOffset": 212}, {"referenceID": 33, "context": "For the l2-SVM and C-SVM, since we can transform the quadratic programs to a single-objective unconstrained optimization problem, there also exist efficient algorithms for the two variants [23, 35, 13, 13, 15, 2].", "startOffset": 189, "endOffset": 212}, {"referenceID": 11, "context": "For the l2-SVM and C-SVM, since we can transform the quadratic programs to a single-objective unconstrained optimization problem, there also exist efficient algorithms for the two variants [23, 35, 13, 13, 15, 2].", "startOffset": 189, "endOffset": 212}, {"referenceID": 11, "context": "For the l2-SVM and C-SVM, since we can transform the quadratic programs to a single-objective unconstrained optimization problem, there also exist efficient algorithms for the two variants [23, 35, 13, 13, 15, 2].", "startOffset": 189, "endOffset": 212}, {"referenceID": 13, "context": "For the l2-SVM and C-SVM, since we can transform the quadratic programs to a single-objective unconstrained optimization problem, there also exist efficient algorithms for the two variants [23, 35, 13, 13, 15, 2].", "startOffset": 189, "endOffset": 212}, {"referenceID": 1, "context": "For the l2-SVM and C-SVM, since we can transform the quadratic programs to a single-objective unconstrained optimization problem, there also exist efficient algorithms for the two variants [23, 35, 13, 13, 15, 2].", "startOffset": 189, "endOffset": 212}, {"referenceID": 29, "context": "Except the traditional quadratic programming approach such as Sequential Minimal Optimization(SMO) [31, 34], there is no better algorithm with the theoretical guarantee for \u03bd-SVM.", "startOffset": 99, "endOffset": 107}, {"referenceID": 32, "context": "Except the traditional quadratic programming approach such as Sequential Minimal Optimization(SMO) [31, 34], there is no better algorithm with the theoretical guarantee for \u03bd-SVM.", "startOffset": 99, "endOffset": 107}, {"referenceID": 12, "context": "A number of distributed algorithms for SVM in this setting have been obtained in the past [14, 29, 27, 26, 9, 18, 40].", "startOffset": 90, "endOffset": 117}, {"referenceID": 27, "context": "A number of distributed algorithms for SVM in this setting have been obtained in the past [14, 29, 27, 26, 9, 18, 40].", "startOffset": 90, "endOffset": 117}, {"referenceID": 25, "context": "A number of distributed algorithms for SVM in this setting have been obtained in the past [14, 29, 27, 26, 9, 18, 40].", "startOffset": 90, "endOffset": 117}, {"referenceID": 24, "context": "A number of distributed algorithms for SVM in this setting have been obtained in the past [14, 29, 27, 26, 9, 18, 40].", "startOffset": 90, "endOffset": 117}, {"referenceID": 8, "context": "A number of distributed algorithms for SVM in this setting have been obtained in the past [14, 29, 27, 26, 9, 18, 40].", "startOffset": 90, "endOffset": 117}, {"referenceID": 16, "context": "A number of distributed algorithms for SVM in this setting have been obtained in the past [14, 29, 27, 26, 9, 18, 40].", "startOffset": 90, "endOffset": 117}, {"referenceID": 38, "context": "A number of distributed algorithms for SVM in this setting have been obtained in the past [14, 29, 27, 26, 9, 18, 40].", "startOffset": 90, "endOffset": 117}, {"referenceID": 37, "context": "Typically, the communication complexity is one of the most important performance measurements for distributed algorithms, and has been studied extensively (see [39, 28] and the book [24] for more details).", "startOffset": 160, "endOffset": 168}, {"referenceID": 26, "context": "Typically, the communication complexity is one of the most important performance measurements for distributed algorithms, and has been studied extensively (see [39, 28] and the book [24] for more details).", "startOffset": 160, "endOffset": 168}, {"referenceID": 22, "context": "Typically, the communication complexity is one of the most important performance measurements for distributed algorithms, and has been studied extensively (see [39, 28] and the book [24] for more details).", "startOffset": 182, "endOffset": 186}, {"referenceID": 23, "context": "[25] proposed a distributed algorithm with O(kd/ ) communication cost, where k is the number of the clients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "Hard-Margin SVM: Inspired by the recent work of Zhang and Lin [41] and Allen-Zhu et al.", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "[3], we propose a new perspective for solving hard-margin SVM via saddle point optimization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Compared to Gilbert algorithm [16], our algorithm improves the running time by a factor of \u221a d/ \u221a .", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "It is known that \u03bd-SVM is equivalent to computing the distance between two reduced polytopes [5, 12].", "startOffset": 93, "endOffset": 100}, {"referenceID": 10, "context": "It is known that \u03bd-SVM is equivalent to computing the distance between two reduced polytopes [5, 12].", "startOffset": 93, "endOffset": 100}, {"referenceID": 19, "context": "Compared with the QP-based algorithms in previous work [21, 31], our algorithm significantly improves the running time, by a factor of n.", "startOffset": 55, "endOffset": 63}, {"referenceID": 29, "context": "Compared with the QP-based algorithms in previous work [21, 31], our algorithm significantly improves the running time, by a factor of n.", "startOffset": 55, "endOffset": 63}, {"referenceID": 28, "context": "The experimental result shows that our algorithm is much faster than the previous QP based algorithm NuSVC, implemented in scikit-learn [30].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "[25] proposed a distributed algorithm withO(kd/ ) communication cost where k is the number of the clients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Moreover, this communication cost is almost optimal according to the lower bound provided in [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 30, "context": "In Section 4, we provide the experimental results, including our algorithm of \u03bd-SVM versus the QP based algorithm NuSVC and our distributed algorithms for SVM versus traditional distributed algorithm HOGWILD! [32] and Gilbert Algorithm [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "In Section 4, we provide the experimental results, including our algorithm of \u03bd-SVM versus the QP based algorithm NuSVC and our distributed algorithms for SVM versus traditional distributed algorithm HOGWILD! [32] and Gilbert Algorithm [16].", "startOffset": 236, "endOffset": 240}, {"referenceID": 21, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 78, "endOffset": 97}, {"referenceID": 33, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 78, "endOffset": 97}, {"referenceID": 11, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 78, "endOffset": 97}, {"referenceID": 13, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 78, "endOffset": 97}, {"referenceID": 1, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 78, "endOffset": 97}, {"referenceID": 20, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 34, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 18, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 36, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 173, "endOffset": 181}, {"referenceID": 35, "context": "Basically, there are three main strategies: the primal gradient-based methods [23, 35, 13, 15, 2], dual quadratic programming methods [22, 36, 20] and dual geometry methods [38, 37].", "startOffset": 173, "endOffset": 181}, {"referenceID": 1, "context": "Recently, Allen-Zhu [2] provided the current best algorithms which achieve O(nd/ \u221a ) time for l2-SVM and O(nd/ ) time for C-SVM.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "[3] used the saddle point optimization and obtained an \u00d5(nd + n \u221a d/ \u221a ) algorithm for the minimum enclosing ball problem (MinEB) in Euclidean space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "This result also implies algorithms for l2-SVM directly by the connection of MinEB and l2-SVM (see [38, 19, 16, 10, 33, 37]).", "startOffset": 99, "endOffset": 123}, {"referenceID": 17, "context": "This result also implies algorithms for l2-SVM directly by the connection of MinEB and l2-SVM (see [38, 19, 16, 10, 33, 37]).", "startOffset": 99, "endOffset": 123}, {"referenceID": 14, "context": "This result also implies algorithms for l2-SVM directly by the connection of MinEB and l2-SVM (see [38, 19, 16, 10, 33, 37]).", "startOffset": 99, "endOffset": 123}, {"referenceID": 9, "context": "This result also implies algorithms for l2-SVM directly by the connection of MinEB and l2-SVM (see [38, 19, 16, 10, 33, 37]).", "startOffset": 99, "endOffset": 123}, {"referenceID": 31, "context": "This result also implies algorithms for l2-SVM directly by the connection of MinEB and l2-SVM (see [38, 19, 16, 10, 33, 37]).", "startOffset": 99, "endOffset": 123}, {"referenceID": 35, "context": "This result also implies algorithms for l2-SVM directly by the connection of MinEB and l2-SVM (see [38, 19, 16, 10, 33, 37]).", "startOffset": 99, "endOffset": 123}, {"referenceID": 36, "context": "[38, 37], the dual of l2-SVM is equivalent to a MinEB by a specific feature mapping.", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "[38, 37], the dual of l2-SVM is equivalent to a MinEB by a specific feature mapping.", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "The dual problem of (1) is equivalent to the problem of finding the two closest points between the convex hulls of two types of points [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "This is a commonly used approach in optimization (see [3] for example).", "startOffset": 54, "endOffset": 57}, {"referenceID": 10, "context": "Next, we discuss \u03bd-SVM (see [12, 34]) and again provide an equivalent saddle point optimization formulation.", "startOffset": 28, "endOffset": 36}, {"referenceID": 32, "context": "Next, we discuss \u03bd-SVM (see [12, 34]) and again provide an equivalent saddle point optimization formulation.", "startOffset": 28, "endOffset": 36}, {"referenceID": 10, "context": "Crisp and Burges [12] present a geometry interpretation for \u03bd-SVM.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "We first apply a randomized Hadamard space rotation as in [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "It is well known [1] that with high probability, for any point xi we have \u2200j \u2208 [d], |(HDxi)j | \u2264 O( \u221a log n/d).", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "The update rules use some useful technique for speeding up the convergence, such as the proximal gradient method and momentum (see the book [6]).", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "The above update rules of \u03b7 and \u03be can be also considered as the multiplicative weight update method (see [4]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 28, "context": "In this section, we first compare our SVMSPSolver with library NuSVC in scikitlearn [30].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "Second, in the distributed setting, we compare DisSVMSPSolver with two distributed algorithms for SVM, HOGWILD! [32] and distributed Gilbert algorithm [25].", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "Second, in the distributed setting, we compare DisSVMSPSolver with two distributed algorithms for SVM, HOGWILD! [32] and distributed Gilbert algorithm [25].", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "The real data is from [8].", "startOffset": 22, "endOffset": 25}, {"referenceID": 28, "context": "The experimental results for \u03bd-Saddle: We compare our SVMSPSolver with NuSVC in scikit-learn [30] and summarize the results in Table 1.", "startOffset": 93, "endOffset": 97}, {"referenceID": 30, "context": "Distributed experiments: In the distributed setting, we compare our algorithms with HOGWILD! [32] and distributed Gilbert algorithm [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "Distributed experiments: In the distributed setting, we compare our algorithms with HOGWILD! [32] and distributed Gilbert algorithm [25].", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "We also select the real world datasets \u201cphishing\u201d and \u201ca9a\u201d from [8].", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "Support Vector Machine is one of the most classical approaches for classification and regression. Despite being studied for decades, obtaining practical algorithms for SVM is still an active research problem in machine learning. In this paper, we propose a new perspective for SVM via saddle point optimization. We provide an algorithm which achieves (1 \u2212 )-approximations with running time \u00d5(nd + n \u221a d/ ) for both separable (hard margin SVM) and non-separable cases (\u03bd-SVM ), where n is the number of points and d is the dimensionality. To the best of our knowledge, the current best algorithm for hard margin SVM achieved by Gilbert algorithm [16] requires O(nd/ ) time. Our algorithm improves the running time by a factor of \u221a d/ \u221a . For \u03bd-SVM, besides the well known quadratic programming approach which requires \u03a9(nd) time [21, 31], no better algorithm is known. In the paper, we provide the first nearly linear time algorithm for \u03bd-SVM. We also consider the distributed settings and provide distributed algorithms with low communication cost via saddle point optimization. Our algorithms require \u00d5(k(d+ \u221a d/ )) communication cost where k is the number of clients, almost matching the theoretical lower bound.", "creator": "LaTeX with hyperref package"}}}