{"id": "1705.07262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Batch Reinforcement Learning on the Industrial Benchmark: First Experiences", "abstract": "The Particle Swarm Optimization Policy (PSO-P) has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off-policy, batch-based setting. To further investigate the properties and feasibility on real-world applications, this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a novel reinforcement learning (RL) benchmark that aims at being realistic by including a variety of aspects found in industrial applications, like continuous state and action spaces, a high dimensional, partially observable state space, delayed effects, and complex stochasticity. The experimental results of PSO-P on IB are compared to results of closed-form control policies derived from the model-based Recurrent Control Neural Network (RCNN) and the model-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not only of interest for academic benchmarks, but also for real-world industrial applications, since it also yielded the best performing policy in our IB setting. Compared to other well established RL techniques, PSO-P produced outstanding results in performance and robustness, requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions.", "histories": [["v1", "Sat, 20 May 2017 05:31:52 GMT  (341kb)", "https://arxiv.org/abs/1705.07262v1", null], ["v2", "Thu, 27 Jul 2017 15:34:21 GMT  (341kb)", "http://arxiv.org/abs/1705.07262v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE cs.SY", "authors": ["daniel hein", "steffen udluft", "michel tokic", "alexander hentschel", "thomas a runkler", "volkmar sterzing"], "accepted": false, "id": "1705.07262"}, "pdf": {"name": "1705.07262.pdf", "metadata": {"source": "CRF", "title": "Batch Reinforcement Learning on the Industrial Benchmark: First Experiences", "authors": ["Daniel Hein", "Steffen Udluft", "Michel Tokic", "Alexander Hentschel", "Thomas A. Runkler", "Volkmar Sterzing"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "III. PSO-P", "text": "Within the framework of the Particle Swarm Optimization Policy (PSO-P) [17], the solution of a RL problem (st + 1, rt) is reformulated as an optimization problem. RL is an area of machine learning in which the Markov decision problem must be solved by learning from observed state transitions (st + 1, rt), where st + 1 represents the Markov states in the applied action and rt the real value of reward is measured in discrete increments of time and t + 1. The goal is to find a policy that maximizes the expected cumulative reward. (st + 1, q), i.e. the information contained in st is calculated by the so-called discounting factor [1]. Since the true underlying Markov state is not observable in the IB, it is approximated by a sufficient amount of historical observations."}, {"heading": "IV. EXPERIMENTS", "text": "We compare the performance of PSO-P on IB with the well established RCNN and NFQ methods. For all RL techniques used, we needed an adequate system model that simulates the IB trajectory rollouts: \u2022 RCNN: The RCNN is trained on m to calculate the weight shifts for policy. \u2022 NFQ: Although the NFQ is considered a model-free RL, it is still very useful to evaluate the performance of policy after each Q iteration step to m, as performance drops during training are very likely to occur when the NFQ is applied to the off-policy batch data."}, {"heading": "A. RCNN", "text": "The Recurrent Control Neural Network (RCNN) [15] consists of two parts. One is a system model m, which is trained to predict the return by rolling out the length T. The second part is a policy network, which calculates one action for each step of the rollout to be fed into the system model. This policy network takes as input the internal state of the system model m, which has the Markov property, approximately [21].The policy network was trained with the same data set D as the system model m. It uses the 30 + 30 neurons of the internal states of consumption and fatigue networks, mc and mf, as input, followed by two hidden layers of 12 and 6 neurons and three output neurons, respectively, to encode the changes in the three control variables, velocity, gain and displacement. The hidden layers use the hyperbolic tangent as an activation function, the output layer uses the sinus function, and all of these dynamics were not sufficiently trained, although the system was not sufficiently configured."}, {"heading": "B. NFQ", "text": "The guidelines of Neural Fitted Q-iteration (NFQ) [7] were trained with a [9-20-1] -layered feed-forward MLP, with 9 neurons on the input layer for observing ot and action at, and 20 neurons on the hidden layer. The initial layer consists of one neuron for the associated Q value Q (ot, at). All neurons of the neural network use a logistical activation function. Since NFQ is an algorithm for discrete actions, we discredited the three delta controls in the direction of setting either \u2212 1, 0, or 1 to each control, which yields a total of | = 27 different types of action. The weights of the networks were trained with a non-batch stochastic gradient descent with a manually tuned constant learning rate of \u03b7 = 0.1. This setting yielded better results than the use of RPROP since weights were trained with RPROP."}, {"heading": "C. PSO-P", "text": "In the PSO-P setup, a PSO search was performed on m with 100 particles, which searched for 100 iterations until the best trajectory found to date was returned, and the planning horizon was set to T = 50, resulting in \u03b3 = 0.251 / 49 = 0.9721 as a discount factor. Particles were arranged in the so-called star topology, i.e. each particle associated with every other particle in the swarm [27]. The fit of the particles could be calculated in parallel on 96 CPUs 2, resulting in a total calculation time of less than 8 seconds for the calculation at = GDP (ot). While the calculation time today could still be too long for several industrial applications in the real world, the increase in CPU velocity and / or parallelization, as well as the calculation on GPU clusters PSO-P, could be traceable for more and more applications."}, {"heading": "V. DISCUSSION", "text": "All three applied RL techniques were able to produce decent results at the IB. Average rewards per step are given in tables in Appendix A.Fig.3, condensing the results of 30 RL strategies and emphasizing the superior performance of PSO-P. This RL technique leads to significantly more robust results than NFQ and RCNN. This can be explained in part by the fact that all techniques on the same system model m.2Intel Xeon CPU E7-4850 v2 @ 2.30GHzThe NFQ results are of the lowest performance in our experiments. This can be explained in part by the fact that NFQ has only applied discreet measures, which represents a certain limitation, as the IB is designed to work on continuous action spaces. Nevertheless, a second NFQ inherent problem that has been uncovered is its highly unstable training behavior in off-policy, batch-based training settings that are not supposed to be answered when the training process itself is supposed to end."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we compared the new RL approach PSOP with two standard RL techniques, RCNN and NFQ, on a recently introduced industrial benchmark, which is designed to mimic realistic behavior and aspects that can be found in real industrial applications. Experiments show important steps of the non-political, batch-based method stack necessary for the application of RL in industrial plants. Based on limited exploration data, an RNN system model is trained and tested. Such a model is crucial as the application of random strategies to the real system is usually prohibited in real-world applications. Although NFQ is classified as a model-free RL technique, our experiments show that it still requires a precise system model for policy selection, the same model has been used for the formation of a closed neural network policy (RCNN), policy selection (NFQ) and the use of the model for determining optimal measures (PO-SP)."}, {"heading": "ACKNOWLEDGMENT", "text": "The project on which this report is based was funded with funds from the Federal Ministry of Education and Research under project number 01IB15001, the content of which is the sole responsibility of the authors."}, {"heading": "APPENDIX A RESULT TABLES", "text": "Tables I, II and III contain the average rewards per step for each of the experiments. The maximum achievable average reward is given in brackets in the first column. These values were calculated by applying PSO-P to real IB system dynamics while retaining the original seed of the pseudo random number generator, i.e. the optimizer searched for the best actions in a fixed and infinitely replicable future. As a result, a very accurate estimate of the maximum achievable average reward for each initial Markov state was evaluated."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The Particle Swarm Optimization Policy (PSO-P)<lb>has been recently introduced and proven to produce remarkable<lb>results on interacting with academic reinforcement learning<lb>benchmarks in an off-policy, batch-based setting. To further<lb>investigate the properties and feasibility on real-world applica-<lb>tions, this paper investigates PSO-P on the so-called Industrial<lb>Benchmark (IB), a novel reinforcement learning (RL) benchmark<lb>that aims at being realistic by including a variety of aspects found<lb>in industrial applications, like continuous state and action spaces,<lb>a high dimensional, partially observable state space, delayed<lb>effects, and complex stochasticity.<lb>The experimental results of PSO-P on IB are compared to<lb>results of closed-form control policies derived from the model-<lb>based Recurrent Control Neural Network (RCNN) and the<lb>model-free Neural Fitted Q-Iteration (NFQ).<lb>Experiments show that PSO-P is not only of interest for<lb>academic benchmarks, but also for real-world industrial appli-<lb>cations, since it also yielded the best performing policy in our IB<lb>setting. Compared to other well established RL techniques, PSO-<lb>P produced outstanding results in performance and robustness,<lb>requiring only a relatively low amount of effort in finding<lb>adequate parameters or making complex design decisions.", "creator": "LaTeX with hyperref package"}}}