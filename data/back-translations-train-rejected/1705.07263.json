{"id": "1705.07263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods", "abstract": "Neural networks are known to be vulnerable to adversarial examples: inputs that are close to valid inputs but classified incorrectly. We investigate the security of ten recent proposals that are designed to detect adversarial examples. We show that all can be defeated, even when the adversary does not know the exact parameters of the detector. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and we propose several guidelines for evaluating future proposed defenses.", "histories": [["v1", "Sat, 20 May 2017 05:59:23 GMT  (483kb,D)", "http://arxiv.org/abs/1705.07263v1", null], ["v2", "Wed, 1 Nov 2017 04:07:05 GMT  (1478kb,D)", "http://arxiv.org/abs/1705.07263v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.CV", "authors": ["nicholas carlini", "david wagner"], "accepted": false, "id": "1705.07263"}, "pdf": {"name": "1705.07263.pdf", "metadata": {"source": "META", "title": "Adversarial Examples Are Not Easily Detected:  Bypassing Ten Detection Methods", "authors": ["Nicholas Carlini", "David Wagner"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, we will be able to look for a solution that is capable of finding a solution, that is able to find a solution that enables us to find a solution, that is able to find a solution that enables us to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution that enables us to find a solution that is able to find a solution, that is able to find a solution that is able to find a solution. \""}, {"heading": "2 BACKGROUND", "text": "The rest of this section provides a brief overview of the area of neural networks and hostile machine learning. We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26] and [6]."}, {"heading": "2.1 Notation", "text": "F (\u00b7) denotes a neural network used for classification, and the last layer in this network is softmax activation, so that the output represents a probability distribution, where F (x) i represents the probability that object x is labeled with class i. All neural networks we examine are feed-forward networks consisting of several layers F i, using the result of the previous Xiv as input: 170 5.07 263v 1 [cs.L G] 20 May 201 7"}, {"heading": "10 classes in the test set. The second row shows untargeted adversaries examples generated by Carlini and Wagner\u2019s attack algorithm for the L2 distance metric. Adversarial examples on MNIST have low distortion; distortion on CIFAR is 10\u00d7 smaller.", "text": "The results of the last layer are called logits; we represent them by Z (\u00b7). Some layers contain the non-linear activation of ReLU [25]. Thus, the ith layer F calculates i (x) = ReLU (Ai \u00b7 F i \u2212 1 (x) + bi), where Ai is a matrix and bi is a vector. Z (x) denotes the output of the last layer (before the softmax), i.e. Z (x) = Fn (x). Then the final output of the network isF (x) = Softmax (Z (x)). When we write C (x), we mean the classification of F (\u00b7) to x: C (x) = arg maxi (F (x) i).Together with the neural network, we obtain a series of training instances with the corresponding labels (x, y) and X."}, {"heading": "2.2 Adversarial Examples", "text": "We call an input to the classifier F (\u00b7) valid if it is an instance that has been created benevolently, and all instances in the training and test set are valid instances. In view of a network F (\u00b7) and a valid input x, so that C (x) = l we say that x \"is a (non-target) counterexample when x is\" close to x and C (x \"), l. A more restrictive case is when the opponent selects a target t, l and tries to find x\" close to x examples that C (x \") = t; in this case we call x\" a targeted counterexample. When we say that a neural Aric network is robust, we mean that it is difficult to find adverse examples on it. To define proximity, most attacks use a Lp distance that is defined as \"d,\" [d \"] adverse, adverse, adverse, adversally, adversally adversally, adversally, adversally, adversally, adversally siversally, adversally siversally, siversally siversally."}, {"heading": "2.3 Threat Model", "text": "We look at three different threat models in this paper: (1) An Oblivious Adversary generates adverse examples on the unsecured model F and is unaware that the detectorD exists; the detector succeeds if it can detect these adversary examples simultaneously; (2) A white box adversary is aware that the neural network is secured with a given detection scheme D. He knows the model parameters used by D and can use them to try to evaluate both the original network F and the detector simultaneously; (3) A black box adversary is aware that the neural network is secured with a given detection scheme, knows how it has been trained, but does not have access to the trained detector D (or the precise training data); we evaluate each defense under these three threat models. We discuss our assessment techniques in Section 3.False Positives vs. False Negatives Tradeoff to be useful in practice, a high probability."}, {"heading": "2.4 Datasets", "text": "We look at two datasets in this paper.TheMNIST dataset [22] consists of 70 000 28 x 28 grayscale images with handwritten digits from 0 to 9. Our standard Convolutionary Network achieves an accuracy of 99.4% on this dataset. The CIFAR-10 dataset [20] consists of 60 000 32 x 32 color images of ten different objects (e.g. trucks, airplanes, etc.) This dataset is much more difficult: the state of the art reaches an accuracy of 95%. For comparison with previous work we use the CIFAR-10 architecture by Metzen et al. [15] The model is a 32-layer ResNet [14] with 470k parameters, which we train with SGD with a dynamics of 0.9. The learning rate is initially 0.1 and is reduced to 0.1."}, {"heading": "2.5 Defenses", "text": "(1) Grosse et al. [12] propose two schemes. (2) Gong et al. [9] use a high-dimensional statistical test (Maximum Mean Discrepancy) to detect opposing examples. (3) Metzen et al. [15] take a similar approach but train the detector on the inner layers of the classifier. (4) Li et al. [23] propose two schemes, the first of which performs PCA on the internal revolutionary layers of the primary network and trains classifiers to distinguish between valid and adversarial data. The second scheme applies a mean blur to images before they are fed to the network. (5) Hendrycks & Gimpel [16) and larger examples of PCA are adversarial images that show adversarial and adversarial data."}, {"heading": "2.6 Generating Adversarial Examples", "text": "We use the L2 attack algorithm of Carlini and Wagner [6] to produce target opposing examples because it is superior to other published attacks. Faced with a neural network F with logits Z, the attack uses gradient descent to solve the target class 1 2 (tanh (w) + 1) -x 22 + c \u00b7 (1 2 (tanh (w) + 1), with the loss function as (x) = max (max {Z (x) i: i, t} \u2212 Z (x) t, \u2212. We now give some intuition behind this loss function. The difference max {Z (x) i: i, t} \u2212 Z (x) t is used to compare the target class t with the next most likely class. However, this is minimized if the target class is significantly more probable than the second most probable class, which is not a property we want."}, {"heading": "3 ATTACK APPROACH", "text": "In fact, most of them are able to play by the rules they have established over the last five years."}, {"heading": "4 SECONDARY CLASSIFICATION BASED DETECTION", "text": "Three of the approaches go in this direction. For the rest of this section, define F (\u00b7) as a classification network and D (\u00b7) as a recognition network. F (\u00b7) is defined as in Section 2.1, with a probability distribution across the 10 classes, and D: Rw \u00b7 h \u00b7 c \u2192 (\u2212 \u221e, \u221e) represent the logits of the probability at which the instance is adversarial, that is, sigmoid (D (x)): Rw \u00b7 h \u00b7 c \u2192 [0, 1] represents the probability at which the instance is adversarial."}, {"heading": "4.1 Adversarial Retraining", "text": "In fact, it is the case that one is able to find a solution that enables one to find a solution that adapts to the needs of the people who are able to find a solution, and that is able to find a solution that is able to find a solution, that is able to find a solution that is able to find a solution, that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution that is able to find a solution, that is able to find a solution."}, {"heading": "4.2 Examining Convolutional Layers", "text": "This year it will be able to introduce the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated csrteeaeFl."}, {"heading": "5 PRINCIPAL COMPONENT ANALYSIS DETECTION", "text": "The Principal Component Analysis (PCA) transforms a set of points in n-dimensional space by a linear transformation into a new set of points in k-dimensional space (k \u2264 n). We assume that the reader is familiar with PCA for the rest of this section."}, {"heading": "5.1 Input Image PCA", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a region and in which it is a country."}, {"heading": "5.2 Dimensionality Reduction", "text": "In fact, most of them will be able to move to a different world in which they are able to escape than to another world in which they are able to escape."}, {"heading": "5.3 Hidden Layer PCA", "text": "Li et al. [23] apply PCAs to the values according to inner revolutionary layers of the neural network and use a cascade classifier to detect adversarial examples. Specifically, they propose the construction of a cascade classifier that accepts the input as valid only if all classifiers accept Ci the input but reject it if necessary. Each classifier Ci is implemented as a linear SVM acting on the revolutionary layer of the network. They evaluate their schema on ImageNet. For the rest of this section, we show that their defense on MNIST and CIFAR is not effective; based on experience with other defense mechanisms, we expect that an attack on an ImageNet classifier would be even easier. Oblivious Attack Evaluation. Li et al. generated adversarial examples with Szegedy et al."}, {"heading": "6 DISTRIBUTIONAL DETECTION", "text": "Next, we examine two defense mechanisms that recognize counter-examples by comparing the distribution of valid images with the distribution of adversarial examples: They use classical statistical methods to distinguish valid images from counter-images. We show that none of these defense mechanisms is able to recognize a white box or black box opponent."}, {"heading": "6.1 MaximumMean Discrepancy", "text": "This year it is more than ever before in the history of the city."}, {"heading": "6.2 Kernel Density Estimation", "text": "(...). \"(...). (...).\" (...). \"(...).\" (...). \"(...).\" It is not the case that you keep to the rules. (...). It is not the case that you keep to the rules. (...). \"It is the case that you keep to the rules.\" (...). \"It is not the case that you keep to the rules.\" (...). \"It is the case that you keep to the rules.\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...). (...).\" (...). \"(...).\" (...). \"(.\" (...). \"(.\" (...). \"(.\" (...). \"(.\" (...). \"(.\" (...). \"(...).\" (. \"(...).\" (. \"(...).\" (...). \"(.\" (.). \"(...).\" (...). \"(.\" (.). \"(...).\" (. \"(.).\" (.). \"(.).\" (. \"(.).\" (...). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).).\" (.). \"(.\" (. \"(.).).\" (. \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. (.). \"(.).\" (.). \"(.).\" (.).). \"(.\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (.). \"(.). (.).\" (.).)."}, {"heading": "7 NORMALIZATION DETECTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Dropout Randomization", "text": "Feinman et al. propose a second detection method called Bayesian neural network security, which measures the uncertainty of the neural network on the given input. Instead of relying on the reported trust of the network (which can easily be controlled by an opponent), they add randomization to the network. It is hoped that a valid image will have the same (correct) label, regardless of the random values chosen, while adversarial examples are not always predicted with the same probability. They use an approach used during training to reduce network overmatch [34], for randomization. Dropout is applied to a level of a network, and randomly sets an output of neurons to a certain probability p. Conventionally, it is used only during training and switched off during testing, but Feinman et al."}, {"heading": "7.2 Mean Blur", "text": "The second detection method proposed by Li et al. uses an average filter of 3 x 3 to blur the image before applying the classifier. The authors acknowledge that this defense is \"overly simplistic,\" but nonetheless argue that it is effective in alleviating opposing examples created with fast gradient marks, as noted in their papers.Oblivious Attack Evaluation. If we use C & W's attack, we find that this defense effectively eliminates contradictory examples with low confidence: 80% of contradictory examples (with an average L2 distortion of 2.05) are no longer misclassified. This attack can partially alleviate even contradictory examples with high confidence. To ensure that they remain contradictory after blurring, we must increase the distortion by a factor of 3. White-box Attack Evaluation."}, {"heading": "8 LESSONS", "text": "Having studied these ten defense mechanisms, we believe we have learned some general lessons from what worked, what didn't, and advice on how to evaluate future defense mechanisms."}, {"heading": "8.1 What Worked?", "text": "Applying random to the network (by dropout) has been the most effective defense against our attacks on MNIST: it makes it as difficult to create enemy examples on the network as it is to generate transferable enemy examples. If it were possible to find a way to eliminate transferability, a randomization-based defense might be able to detect enemy examples. MNIST's core density estimate, the other defense that significantly increased the required distortion, was only effective for MNIST."}, {"heading": "8.2 What Didn\u2019t Work?", "text": "Given that opposing examples can deceive a single classifier, it makes sense that opposing examples can deceive a classifier and detector. None of these approaches entails more than a 30% increase in robustness in MNIST (and much less in CIFAR) if the opponent was aware of the model, and black box attacks are also possible and almost as effective as white box attacks. Defense measures based directly on pixel values were too simple to succeed. In MNIST, these defense mechanisms offered adequate robustness against weak attacks, but when assessing stronger attacks, these defense mechanisms all failed. This should come as no surprise: The reason that neural networks are used is because they are able to extract deep and significant properties from the input data. A simple linear detector is not effective in classification when they are operated on raw values."}, {"heading": "8.3 Recommendations for Defenses", "text": "This year, it has come to the point that never before has it been possible to reactivate the erroneous orders."}, {"heading": "9 CONCLUSION", "text": "Unlike standard machine learning tasks, where achieving higher accuracy on a single scale is a useful and interesting outcome in itself, this is not enough for safe machine learning. We need to consider how an attacker might respond to a proposed attack, and assess whether the defense against an attacker who knows how the defense works remains safe. In this paper, we evaluate ten proposed defense strategies and show that none of them are able to withstand a white box attack, all of which fail even in a black box environment where the opponent knows only the technology the defender plans to use, but does not know the specific model parameters used. From examining these ten defense strategies, we have learned two lessons: resistance examples are much more difficult to recognize than previously recognized, and existing defense strategies do not have a thorough safety assessment. We hope that our work will help raise the bar for evaluating proposed defense strategies and perhaps help others build effective defense strategies."}, {"heading": "10 ACKNOWLEDGEMENTS", "text": "We would like to thank Kathrin Grosse, Fuxin Li, Reuben Feinman, Metzen Jan Hendrik for discussing their defence with us."}, {"heading": "A ADDITIONAL FIGURES", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Measuring neural net robustness with constraints", "author": ["Osbert Bastani", "Yani Ioannou", "Leonidas Lampropoulos", "Dimitrios Vytiniotis", "Aditya Nori", "Antonio Criminisi"], "venue": "In Advances In Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Dimensionality Reduction as a Defense against Evasion Attacks on Machine Learning Classifiers", "author": ["Arjun Nitin Bhagoji", "Daniel Cullina", "Prateek Mittal"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Integrating structured biological data by kernel maximum mean discrepancy", "author": ["Karsten M Borgwardt", "Arthur Gretton", "Malte J Rasch", "Hans-Peter Kriegel", "Bernhard Sch\u00f6lkopf", "Alex J Smola"], "venue": "Bioinformatics 22,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Bringing Big Neural Networks to Self-Driving Cars, Smartphones, and Drones. http://spectrum.ieee.org/computing/embedded-systems/ bringing-big-neural-networks-to-selfdriving-cars-smartphones-and-drones", "author": ["Katherine Bourzac"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "IEEE Symposium on Security and Privacy", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Detecting Adversarial Samples from Artifacts", "author": ["Reuben Feinman", "Ryan R Curtin", "Saurabh Shintre", "Andrew B Gardner"], "venue": "arXiv preprint arXiv:1703.00410", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Adversarial and Clean Data Are Not Twins", "author": ["Zhitao Gong", "Wenlu Wang", "Wei-Shinn Ku"], "venue": "arXiv preprint arXiv:1704.04960", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "Journal of Machine Learning Research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "On the (Statistical) Detection of Adversarial Examples", "author": ["Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint arXiv:1412.5068", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "On Detecting Adversarial Perturbations", "author": ["Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff"], "venue": "arXiv preprint arXiv:1702.04267", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "EarlyMethods for Detecting Adversarial Images", "author": ["DanHendrycks", "Kevin Gimpel"], "venue": "In International Conference on Learning Representations (Workshop Track)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Learning with a strong adversary", "author": ["RuitongHuang", "Bing Xu", "Dale Schuurmans", "Csaba Szepesv\u00e1ri"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Robust Convolutional Neural Networks under Adversarial Noise", "author": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "venue": "arXiv preprint arXiv:1511.06306", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Adversarial examples in the physical world", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "In International Conference on Learning Representations (Workshop Track)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "The MNIST database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics", "author": ["Xin Li", "Fuxin Li"], "venue": "arXiv preprint arXiv:1612.07767", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "PatrickMcDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Security and Privacy (EuroS&P),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "IEEE Symposium on Security and Privacy", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Announcing syntaxnet: The world\u2019s most accurate parser goes open source", "author": ["Slav Petrov"], "venue": "Google Research Blog, May", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Adversarial diversity and hard positive generation", "author": ["Andras Rozsa", "Ethan M Rudd", "Terrance E Boult"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization", "author": ["Uri Shaham", "Yutaro Yamada", "Sahand Negahban"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "In International Conference on Learning Representations (Workshop Track)", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Improving the robustness of deep neural networks via stability training", "author": ["Stephan Zheng", "Yang Song", "Thomas Leung", "Ian Goodfellow"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 32, "context": "Their successes have been demonstrated in awide set of domains, from classifying images [35], to beating the best humans at Go [32], to translation and NLP [29], and to self driving cars [3, 5].", "startOffset": 88, "endOffset": 92}, {"referenceID": 27, "context": "Their successes have been demonstrated in awide set of domains, from classifying images [35], to beating the best humans at Go [32], to translation and NLP [29], and to self driving cars [3, 5].", "startOffset": 156, "endOffset": 160}, {"referenceID": 3, "context": "Their successes have been demonstrated in awide set of domains, from classifying images [35], to beating the best humans at Go [32], to translation and NLP [29], and to self driving cars [3, 5].", "startOffset": 187, "endOffset": 193}, {"referenceID": 33, "context": "Such an instance x \u2032 is known as an adversarial example [36], and they have been shown to exist in nearly all domains that neural networks are used.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 11, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 15, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 17, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 26, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 28, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 29, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 34, "context": "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 145, "endOffset": 176}, {"referenceID": 1, "context": "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.", "startOffset": 75, "endOffset": 100}, {"referenceID": 6, "context": "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.", "startOffset": 75, "endOffset": 100}, {"referenceID": 7, "context": "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.", "startOffset": 75, "endOffset": 100}, {"referenceID": 10, "context": "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.", "startOffset": 75, "endOffset": 100}, {"referenceID": 13, "context": "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.", "startOffset": 75, "endOffset": 100}, {"referenceID": 14, "context": "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.", "startOffset": 75, "endOffset": 100}, {"referenceID": 21, "context": "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.", "startOffset": 75, "endOffset": 100}, {"referenceID": 33, "context": "These attacks work by exploiting transferability [36], and our techniques for generating adversarial examples that transfer well may be of independent interest.", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 23, "context": "Some layers involve the non-linear ReLU [25] activation.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L\u221e, a measure of the maximum absolute change to any pixel [10].", "startOffset": 75, "endOffset": 79}, {"referenceID": 4, "context": "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L\u221e, a measure of the maximum absolute change to any pixel [10].", "startOffset": 113, "endOffset": 124}, {"referenceID": 22, "context": "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L\u221e, a measure of the maximum absolute change to any pixel [10].", "startOffset": 113, "endOffset": 124}, {"referenceID": 33, "context": "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L\u221e, a measure of the maximum absolute change to any pixel [10].", "startOffset": 113, "endOffset": 124}, {"referenceID": 8, "context": "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L\u221e, a measure of the maximum absolute change to any pixel [10].", "startOffset": 187, "endOffset": 191}, {"referenceID": 8, "context": "One further property of adversarial examples we will make use of is the transferability property [10, 36].", "startOffset": 97, "endOffset": 105}, {"referenceID": 33, "context": "One further property of adversarial examples we will make use of is the transferability property [10, 36].", "startOffset": 97, "endOffset": 105}, {"referenceID": 4, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 94, "endOffset": 109}, {"referenceID": 8, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 94, "endOffset": 109}, {"referenceID": 22, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 94, "endOffset": 109}, {"referenceID": 25, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 94, "endOffset": 109}, {"referenceID": 0, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 11, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 15, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 17, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 26, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 28, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 29, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 34, "context": "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].", "startOffset": 167, "endOffset": 198}, {"referenceID": 1, "context": "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].", "startOffset": 38, "endOffset": 63}, {"referenceID": 6, "context": "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].", "startOffset": 38, "endOffset": 63}, {"referenceID": 7, "context": "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].", "startOffset": 38, "endOffset": 63}, {"referenceID": 10, "context": "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].", "startOffset": 38, "endOffset": 63}, {"referenceID": 13, "context": "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].", "startOffset": 38, "endOffset": 63}, {"referenceID": 14, "context": "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].", "startOffset": 38, "endOffset": 63}, {"referenceID": 21, "context": "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].", "startOffset": 38, "endOffset": 63}, {"referenceID": 13, "context": "[15] NA/34% (\u00a74.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] 11%/24% (\u00a74.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[12] 10%/8% (\u00a74.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] 85%/0% (\u00a76.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[23] 0%/0% (\u00a75.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] 0%/0% (\u00a75.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[16] 0%/0% (\u00a75.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "TheMNIST dataset [22] consists of 70, 000 28\u00d7 28 greyscale images of handwritten digits from 0 to 9.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "The CIFAR-10 dataset [20] consists of 60, 000 32 \u00d7 32 color images of ten different objects (e.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "This dataset is substantially more difficult: the state of the art approaches achieve 95% accuracy [33].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The model is a 32layer ResNet [14] with 470k parameters that we train with SGD with momentum set to 0.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "We apply L2 regularization and use BatchNorm [18] after every convolution.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "Some of the defenses we evaluate also argue robustness against ImageNet [7], a database of a million 224\u00d7224 images.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "Prior work [6, 24] has clearly demonstrated that constructing adversarial examples on ImageNet is a strictly easier task than MNIST or CIFAR, and constructing defenses is strictly harder.", "startOffset": 11, "endOffset": 18}, {"referenceID": 22, "context": "Prior work [6, 24] has clearly demonstrated that constructing adversarial examples on ImageNet is a strictly easier task than MNIST or CIFAR, and constructing defenses is strictly harder.", "startOffset": 11, "endOffset": 18}, {"referenceID": 10, "context": "[12] propose two schemes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] detect adversarial examples by building a second neural network that detects adversarial examples from valid images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "(3) Metzen et al [15] follow a similar approach, but train the detector on the inner layers of the classifier.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "[23] propose two schemes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "(5) Hendrycks & Gimpel [16] perform PCA on the pixels of an image and argue adversarial examples place higher emphasis on larger components.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "[8] detect adversarial examples by keeping dropout [34] on during evaluation; additionally, they construct a kernel density measure and show that adversarial examples are drawn from a different distribution than valid images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[8] detect adversarial examples by keeping dropout [34] on during evaluation; additionally, they construct a kernel density measure and show that adversarial examples are drawn from a different distribution than valid images.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": "[2] show that adversarial images require use of more PCA dimensions than valid images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We use the L2 attack algorithm of Carlini and Wagner [6] to generate targeted adversarial examples, as it is superior to other published attacks.", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "\u2022 The Fast Gradient Sign attack [10] takes a single step, for all pixels, in the direction of the gradient.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "\u2022 JSMA [27] is an attack that greedily modifies one pixel at a time until the image is classified incorrectly.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "As with any experiment, it is better to change only one variable at a time: if we were to compare white-box-no-detector distortion to black-box-with-detector distortion, we would not know if the detector has added any robustness, or if it just that adversarial examples require larger distortion to transfer (which they do [6, 21]).", "startOffset": 323, "endOffset": 330}, {"referenceID": 19, "context": "As with any experiment, it is better to change only one variable at a time: if we were to compare white-box-no-detector distortion to black-box-with-detector distortion, we would not know if the detector has added any robustness, or if it just that adversarial examples require larger distortion to transfer (which they do [6, 21]).", "startOffset": 323, "endOffset": 330}, {"referenceID": 0, "context": "That is, sigmoid(D(x)) : Rw \u00b7h \u00b7c \u2192 [0, 1] represents the probability the instance is adversarial.", "startOffset": 36, "endOffset": 42}, {"referenceID": 10, "context": "[12] propose a variant on adversarial re-training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] construct a similar detector; however, instead of re-training the full network to detect the original examples along with adversarial examples, they train a second binary classifier to predict whether the instance is adversarial or valid.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[15] detect adversarial examples by looking at the inner convolutional layers of the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We refer interested readers to the original paper for complete details on the detector setup [15].", "startOffset": 93, "endOffset": 97}, {"referenceID": 22, "context": "Their paper demonstrates that it is also able to detect adversarial examples generated by two other attacks, DeepFool [24] and the Basic Iterative Method [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 19, "context": "Their paper demonstrates that it is also able to detect adversarial examples generated by two other attacks, DeepFool [24] and the Basic Iterative Method [21].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "We then train the detector by attaching it to the output of the first residual block as done in [15].", "startOffset": 96, "endOffset": 100}, {"referenceID": 14, "context": "Hendrycks & Gimpel [16] use PCA to detect valid images from adversarial examples, finding that adversarial examples place a higher weight on the larger principal components than valid images (and lower weight on the earlier principal components).", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "al [16]: there is no difference on the first principal components, but there is a substantial difference between valid and adversarial instances on the later components.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "[2] propose a defense based on dimensionality reduction: instead of training a classifier on the original training data, they reduce theW \u00b7H \u00b7C = N -dimensional input (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[23] apply PCA to the values after inner convolutional layers of the neural network, and use a cascade classifier to detect adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "al\u2019s L-BFGS algorithm [36] and found that the first linear SVM achieved 80% true positive rate at 0% false positive rate \u2013 an ideal use-case for a cascade classifier.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "[12] consider a significantly more powerful (perhaps so powerful as to be impractical) threat model: assume we are given two sets of images S1 and S2, such that we know S1 contains only valid images, and we know that S2 contains either all adversarial examples, or all valid images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "They ask the question: can we determine which of these two situations is the case? To achieve this, they use theMaximumMeanDiscrepancy (MMD) test [4, 11], a statistical hypothesis test that answers the question \u201care these two sets drawn from the same underlying distribution?\u201d Let F represents a (possibly unbounded) set of functions.", "startOffset": 146, "endOffset": 153}, {"referenceID": 9, "context": "They ask the question: can we determine which of these two situations is the case? To achieve this, they use theMaximumMeanDiscrepancy (MMD) test [4, 11], a statistical hypothesis test that answers the question \u201care these two sets drawn from the same underlying distribution?\u201d Let F represents a (possibly unbounded) set of functions.", "startOffset": 146, "endOffset": 153}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] present a defense they call kernel density estimation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "They use dropout, a common approach used during training to reduce network overfitting [34], for randomization.", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "While we have found CIFAR to be a reasonable task for evaluating security, in the future as defenses improve it may become necessary to evaluate on harder datasets (such as ImageNet [7]).", "startOffset": 182, "endOffset": 185}], "year": 2017, "abstractText": "Neural networks are known to be vulnerable to adversarial examples: inputs that are close to valid inputs but classified incorrectly. We investigate the security of ten recent proposals that are designed to detect adversarial examples. We show that all can be defeated, even when the adversary does not know the exact parameters of the detector. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and we propose several guidelines for evaluating future proposed defenses.", "creator": "LaTeX with hyperref package"}}}