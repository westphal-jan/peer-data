{"id": "1705.07267", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Search Engine Guided Non-Parametric Neural Machine Translation", "abstract": "In this paper, we extend an attention-based neural machine translation (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage--retrieval stage--, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage--translation stage--, a novel translation model, called translation memory enhanced NMT (TM-NMT), seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (En-Fr, En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.", "histories": [["v1", "Sat, 20 May 2017 06:53:09 GMT  (2338kb,D)", "http://arxiv.org/abs/1705.07267v1", "8 pages, 4 figures, 2 tables"]], "COMMENTS": "8 pages, 4 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["jiatao gu", "yong wang", "kyunghyun cho", "victor o k li"], "accepted": false, "id": "1705.07267"}, "pdf": {"name": "1705.07267.pdf", "metadata": {"source": "CRF", "title": "Search Engine Guided Non-Parametric Neural Machine Translation", "authors": ["Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to integrate themselves, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they"}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural Machine Translation", "text": "In this essay we assume a recently proposed and widely used attention-based neural translation model (Bahdanau et al., 2014). The attention-based neural translation model is a conditional recursive language model of a conditional distribution p (Y | X) across all possible translations Y = {y1,., yT} in the face of a source set X = {x1,., xTx}. This conditional recursive language model is an autoregressive model that calculates the conditional probability as p (Y | X) = \"T t = 1 p\" (yt | y < t, \"X).\" Each term on the right side is approxxed by a recursive network source (yt | y < t, \"extor,\" extor, \"extor,\" extor, \"(\" exg, \"),\" extor, \"(\" exg, \"),\" extor, \"(\" exg, \"),\" extor, \"(\", \"),\" extor, \"(\")."}, {"heading": "2.2 Translation Memory", "text": "Translation Memory is a computer-aided translation tool widely used by professional human translators. It is a database of pairs of sentences of the source phrase and its translation. This database is built step by step, just like a human translator translates sentences. If a new source sentence is present, sentences from the original sentence are queried (overlapping) against the translation memory, and the corresponding entries are displayed to the human translator to speed up the translation process. Due to the problem of sparseness (paragraph 5.2 of Cho, 2015), exact matches are rarely found, and approximate string matching is often used. In this essay, we consider a more general concept of translation memory, in which not only phrase pairs of the translation are stored, but any type of translation pairs. In this more general definition, a parallel training corpus is also considered as a translation memory, in which not only phrase pairs of the translation are stored, but any kind of translation pairs."}, {"heading": "3 Search Engine Guided Non-Parametric Neural Machine Translation", "text": "We propose a non-parametric neural machine translation model guided by a standard, efficient search engine. Unlike conventional neural machine translation systems, the proposed model does not discard a training corpus, but receives and actively uses it during the test period. Thus, the proposed neural translation model effectively becomes a completely non-parametric model. The proposed non-parametric neural translation model consists of two phases. The first phase is an on-call phase, in which the proposed model queries a training corpus or equivalent translation memory to obtain a set of source translation pairs with an up-to-date source set. To maximize computing efficiency, we first use a standard, highly optimized search engine to quickly restore a large set of similar source sentences and their translations, after which the top-K pairs are translated by means of an approximate string matching based on editing of a second-stage translation."}, {"heading": "3.1 Retrieval Stage", "text": "We refer to the first stage as a retrieval stage. At this stage, we go over the entire training series M = (X1, Y 1) to find pairs whose source page is similar to a current source X. That is, we define a similarity function s (X, X), and find (Xn, Y) where s (X, Xn) is large. Algorithm 1 Greedy selection method to maximize coverage of source symbols. Requirement: Input X, Translation memoryM1: Reach the subset M that an off-the-shelf search engine uses. Algorithm 1 Greedy selection method to maximize coverage of source symbols. Requirement: Input X, Translation memoryM1: Reach the subset M that deals with an off-the-shelf search engine."}, {"heading": "3.2 Translation Stage", "text": "In the second stage, we build a novel enhancement of attention-based neural machine translation, TM-NMT, which seamlessly fuses both a current source sentence and a specified number of retrieved translation pairs. At a high level, the proposed TM-NMT first saves each target symbol of each retrieved translation sentence into a key-value memory (Gulcehre et al., 2016b; Miller et al., 2016). In each step of the decoder, TM-NMT first draws attention to the current source sentence to calculate the time-dependent context vector on which the key-value memory is queried. TM-NMT uses information from both contexts vector of the current source set and the retrieved value from the key-value memory to generate a next target symbol."}, {"heading": "3.3 Learning and Inference", "text": "The proposed model, which includes both the first and second levels, can be trained end-to-end to maximize log probability with a parallel corpus. For hands-on training, we prepare a training-parallel corpus by adding a set of translation pairs to each pair of sentences, which are retrieved by a search engine, while ensuring that the exact copy is not included in the retrieved sentence. See Figure 2 for a detailed description. During the test, we search the entire sentence to retrieve relevant translation pairs. Similar to a conventional neural translation model, we use bar search to decipher the best translation for a source sentence."}, {"heading": "4 Related Work", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "7 Conclusion", "text": "We proposed a practical, non-parametric extension of attention-based neural machine translation by using a standard black box search engine to quickly select a small subset of translation pairs for training, and the proposed model, called TM-NMT, learns to incorporate both source and target information from these retrieved pairs to improve translation quality. We demonstrated empirically the effectiveness of the proposed approach to the JRC-Acquis corpus using six language pairs. Although we presented the proposed approach in the context of machine translation, it is generally applicable to a variety of problems. By embedding any modality in a fixed vector space and using a fast approximate search (Douze et al., 2017), for example, this approach can be used to generate multimedia descriptions (Cho et al, 2015)."}, {"heading": "Acknowledgments", "text": "KC thanks the support of Tencent, eBay, Facebook, Google and NVIDIA. This work was partially supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI)."}], "references": [{"title": "Towards string-to-tree neural machine translation", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1704.04743 .", "citeRegEx": "Aharoni and Goldberg.,? 2017", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data. AcM, pages 1247\u20131250.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u00eda-Mart\u00ednez", "Fethi Bougares", "Lo\u00efc Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1511.07916 .", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio."], "venue": "Multimedia, IEEE Transactions on 17(11):1875\u20131886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Systran\u2019s pure neural machine translation systems", "author": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aurelien Coquard", "Yongchao Deng"], "venue": null, "citeRegEx": "Crego et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Crego et al\\.", "year": 2016}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["Jacob Devlin", "Saurabh Gupta", "Ross Girshick", "Margaret Mitchell", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1505.04467 .", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "FAISS: A library for efficient similarity search", "author": ["Matthijs Douze", "Jeff Johnson", "Herv\u00e9 Jegou."], "venue": "https://code.facebook.com/posts/1373769912645926/ faiss-a-library-for-efficient-similarity-search/. Accessed: 2017-05-14.", "citeRegEx": "Douze et al\\.,? 2017", "shortCiteRegEx": "Douze et al\\.", "year": 2017}, {"title": "Learning to parse and translate improves neural machine translation", "author": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1702.03525 .", "citeRegEx": "Eriguchi et al\\.,? 2017", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2017}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "NAACL.", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T Yarman Vural", "Kyunghyun Cho."], "venue": "EMNLP.", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Ensemble learning for multi-source neural machine translation", "author": ["Ekaterina Garmash", "Christof Monz."], "venue": "COLING.", "citeRegEx": "Garmash and Monz.,? 2016", "shortCiteRegEx": "Garmash and Monz.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."], "venue": "arXiv preprint arXiv:1603.06393 .", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.08148 .", "citeRegEx": "Gulcehre et al\\.,? 2016a", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Dynamic neural turing machine with continuous and discrete addressing schemes", "author": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1607.00036 .", "citeRegEx": "Gulcehre et al\\.,? 2016b", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1503.03535 .", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Does neural machine translation benefit from larger context", "author": ["Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2017}, {"title": "Learning to remember rare events", "author": ["\u0141ukasz Kaiser", "Ofir Nachum", "Aurko Roy", "Samy Bengio."], "venue": "arXiv preprint arXiv:1703.03129 .", "citeRegEx": "Kaiser et al\\.,? 2017", "shortCiteRegEx": "Kaiser et al\\.", "year": 2017}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP. pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1. Association for Computational Linguistics, pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Phrase-level combination of smt and tm using constrained word lattice", "author": ["Liangyou Li", "Andy Way", "Qun Liu."], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. page 275.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1606.03126 .", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Syntax-aware neural machine translation using ccg", "author": ["Maria Nadejde", "Siva Reddy", "Rico Sennrich", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Philipp Koehn", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1702.01147 .", "citeRegEx": "Nadejde et al\\.,? 2017", "shortCiteRegEx": "Nadejde et al\\.", "year": 2017}, {"title": "Task-oriented query reformulation with reinforcement learning", "author": ["Rodrigo Nogueira", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1704.04572 .", "citeRegEx": "Nogueira and Cho.,? 2017", "shortCiteRegEx": "Nogueira and Cho.", "year": 2017}, {"title": "Neural episodic control", "author": ["Alexander Pritzel", "Benigno Uria", "Sriram Srinivasan", "Adri\u00e0 Puigdom\u00e8nech", "Oriol Vinyals", "Demis Hassabis", "Daan Wierstra", "Charles Blundell."], "venue": "arXiv preprint arXiv:1703.01988 .", "citeRegEx": "Pritzel et al\\.,? 2017", "shortCiteRegEx": "Pritzel et al\\.", "year": 2017}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages", "author": ["Ralf Steinberger", "Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "D\u00e1niel Varga."], "venue": "arXiv preprint cs/0609058 .", "citeRegEx": "Steinberger et al\\.,? 2006", "shortCiteRegEx": "Steinberger et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS .", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "arXiv preprint arXiv:1601.04811 .", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems. pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Exploiting cross-sentence context for neural machine translation", "author": ["Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu."], "venue": "arXiv preprint arXiv:1704.04347 .", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "arXiv preprint arXiv:1601.00710 .", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 7, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 33, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 22, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 37, "context": "The success of neural machine translation, which has already been adopted by major industry players in machine translation (Wu et al., 2016; Crego et al., 2016), is often attributed to the advances in building and training recurrent networks as well as the availability of large-scale parallel corpora for machine translation.", "startOffset": 123, "endOffset": 160}, {"referenceID": 8, "context": "The success of neural machine translation, which has already been adopted by major industry players in machine translation (Wu et al., 2016; Crego et al., 2016), is often attributed to the advances in building and training recurrent networks as well as the availability of large-scale parallel corpora for machine translation.", "startOffset": 123, "endOffset": 160}, {"referenceID": 24, "context": "Neural machine translation is most characteristically distinguished from the existing approaches to machine translation, such as phrase-based statistical machine translation (Koehn et al., 2003), in that it projects a sequence of discrete source symbols into a continuous space and decodes back the corresponding translation.", "startOffset": 174, "endOffset": 194}, {"referenceID": 13, "context": "This property has been recently noticed and used for building more advanced translation systems such as multilingual neural machine translation (Firat et al., 2016a; Luong et al., 2015; Dong et al., 2015), multi-source translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 144, "endOffset": 204}, {"referenceID": 26, "context": "This property has been recently noticed and used for building more advanced translation systems such as multilingual neural machine translation (Firat et al., 2016a; Luong et al., 2015; Dong et al., 2015), multi-source translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 144, "endOffset": 204}, {"referenceID": 10, "context": "This property has been recently noticed and used for building more advanced translation systems such as multilingual neural machine translation (Firat et al., 2016a; Luong et al., 2015; Dong et al., 2015), multi-source translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 144, "endOffset": 204}, {"referenceID": 38, "context": ", 2015), multi-source translation (Zoph and Knight, 2016; Firat et al., 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 14, "context": ", 2015), multi-source translation (Zoph and Knight, 2016; Firat et al., 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 15, "context": ", 2015), multi-source translation (Zoph and Knight, 2016; Firat et al., 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 4, "context": ", 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al., 2016) and linguistic knowledge guided translation (Nadejde et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 28, "context": ", 2016) and linguistic knowledge guided translation (Nadejde et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017).", "startOffset": 52, "endOffset": 125}, {"referenceID": 0, "context": ", 2016) and linguistic knowledge guided translation (Nadejde et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017).", "startOffset": 52, "endOffset": 125}, {"referenceID": 12, "context": ", 2016) and linguistic knowledge guided translation (Nadejde et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017).", "startOffset": 52, "endOffset": 125}, {"referenceID": 32, "context": "We evaluate the proposed search engine guided non-parametric neural machine translation on three language pairs (En-Fr, En-De, and En-Es, in both directions) from JRC-Acquis Corpus (Steinberger et al., 2006) which consists of documents from a legal domain.", "startOffset": 181, "endOffset": 207}, {"referenceID": 1, "context": "Our experiments reveal that the proposed approach exploits the availability of the retrieved training sentence pairs very well, achieving significant improvement over the strong baseline of attention-based neural machine translation (Bahdanau et al., 2014).", "startOffset": 233, "endOffset": 256}, {"referenceID": 1, "context": "In this paper, we start from a recently proposed, and widely used, attention-based neural machine translation model (Bahdanau et al., 2014).", "startOffset": 116, "endOffset": 139}, {"referenceID": 24, "context": "This saves us from building a phrase table (Koehn et al., 2003), which is yet another active research topic, but requires us to be efficient and flexible in retrieving relevant translation pairs given a source sentence, as the issue of data sparsity amplifies.", "startOffset": 43, "endOffset": 63}, {"referenceID": 5, "context": "This allows us to focus entirely on the effectiveness of the proposed algorithm while being agnostic to the choice of similarity metric. Under this constraint, we follow an earlier work by Li et al. (2016) and use a fuzzy matching score which is defined as sfuzzy(X,X \u2032) = 1\u2212 Dedit(X,X \u2032) max (|X|, |X \u2032|) , (3) where Dedit is an edit distance.", "startOffset": 108, "endOffset": 206}, {"referenceID": 18, "context": "In a high level, the proposed TM-NMT first stores each target symbol of each retrieved translation pair into a key-value memory (Gulcehre et al., 2016b; Miller et al., 2016).", "startOffset": 128, "endOffset": 173}, {"referenceID": 27, "context": "In a high level, the proposed TM-NMT first stores each target symbol of each retrieved translation pair into a key-value memory (Gulcehre et al., 2016b; Miller et al., 2016).", "startOffset": 128, "endOffset": 173}, {"referenceID": 35, "context": "That is, pcopy(y\u2032 \u03c4 ) = qt,\u03c4 , similarly to the pointer network (Vinyals et al., 2015).", "startOffset": 64, "endOffset": 86}, {"referenceID": 17, "context": "5 23: Update \u03c6\u2190 \u03c6+ \u03b3 \u2202 \u2202\u03c6 \u2211T t=1 log p(yt|\u00b7) Incorporation We consider two separate approaches to incorporating the retrieved values from the key-value memory, motivated by Gulcehre et al. (2015). The first approach, called deep fusion, weightedaverage the retrieved hidden state z\u0303t and the decoder\u2019s hidden state zt: zfusion =\u03b6t \u00b7 z\u0303t + (1\u2212 \u03b6t) \u00b7 zt (5)", "startOffset": 173, "endOffset": 196}, {"referenceID": 17, "context": "This is equivalent to copying over a retrieved target symbol y\u2032 \u03c4 with the probability of \u03b6tpcopy(y \u2032 \u03c4 ) as the next target symbol (Gulcehre et al., 2016a; Gu et al., 2016).", "startOffset": 132, "endOffset": 173}, {"referenceID": 16, "context": "This is equivalent to copying over a retrieved target symbol y\u2032 \u03c4 with the probability of \u03b6tpcopy(y \u2032 \u03c4 ) as the next target symbol (Gulcehre et al., 2016a; Gu et al., 2016).", "startOffset": 132, "endOffset": 173}, {"referenceID": 34, "context": "Motivated by the coverage penalty from (Tu et al., 2016), we propose to augment the bilinear matching function (in Eq.", "startOffset": 39, "endOffset": 56}, {"referenceID": 38, "context": "The proposed TM-NMT has been largely motivated by recently proposed multilingual attention-based neural machine translation models (see, e.g., Firat et al., 2016a; Zoph and Knight, 2016).", "startOffset": 131, "endOffset": 186}, {"referenceID": 20, "context": "More recently, this kind of larger context translation has been applied to cross-sentential modeling, where the translation of a current sentence is done with respect to previous sentences (Jean et al., 2017; Wang et al., 2017).", "startOffset": 189, "endOffset": 227}, {"referenceID": 36, "context": "More recently, this kind of larger context translation has been applied to cross-sentential modeling, where the translation of a current sentence is done with respect to previous sentences (Jean et al., 2017; Wang et al., 2017).", "startOffset": 189, "endOffset": 227}, {"referenceID": 2, "context": "(2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al., 2008).", "startOffset": 100, "endOffset": 124}, {"referenceID": 6, "context": "Devlin et al. (2015) proposed an automatic image caption generation model based on nearest neighbours.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bordes et al. (2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "(2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al., 2008). The output module of the memory network used simple n-gram matching to create a small set of candidate facts from the Freebase. Each of these candidates was scored by the memory network to create a representation used by the response module. This is similar to our approach in that it exploits an intermediate black-box search module (n-gram matching) for generating a small candidate set. The proposed method makes the attention-based neural machine translation model non-parametric by incorporating a key-value memory that stores a training set. A similar approach was very recently proposed for deep reinforcement learning by Pritzel et al. (2017), where they store pairs of observed state and the corresponding (estimated) value in a key-value memory to build a non-parametric deep Q network.", "startOffset": 101, "endOffset": 777}, {"referenceID": 2, "context": "(2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al., 2008). The output module of the memory network used simple n-gram matching to create a small set of candidate facts from the Freebase. Each of these candidates was scored by the memory network to create a representation used by the response module. This is similar to our approach in that it exploits an intermediate black-box search module (n-gram matching) for generating a small candidate set. The proposed method makes the attention-based neural machine translation model non-parametric by incorporating a key-value memory that stores a training set. A similar approach was very recently proposed for deep reinforcement learning by Pritzel et al. (2017), where they store pairs of observed state and the corresponding (estimated) value in a key-value memory to build a non-parametric deep Q network. We consider it as a confirmation of the general applicability of the proposed approach to a wider array of problems in machine learning. In the context of neural machine translation, Kaiser et al. (2017) also proposed to use an external key-value memory to remember training examples in the test time.", "startOffset": 101, "endOffset": 1127}, {"referenceID": 5, "context": "Recently, Nogueira and Cho (2017) proposed task-oriented query reformulation in which a neural network is trained to use a black-box search engine to maximize the recall of relevant documents, which can be integrated into the proposed TM-NMT.", "startOffset": 23, "endOffset": 34}, {"referenceID": 32, "context": "Data We use the JRC-Acquis corpus (Steinberger et al., 2006) for evaluating the proposed TM-NMT model.", "startOffset": 34, "endOffset": 60}, {"referenceID": 25, "context": "This corpus was recently used by Li et al. (2016) in investigating the combination of translation memory and phrase-based statistical machine translation.", "startOffset": 33, "endOffset": 50}, {"referenceID": 1, "context": "Translation Stage We use a standard attention-based neural machine translation model (Bahdanau et al., 2014) with 1,024 gated recurrent units (GRU, Cho et al.", "startOffset": 85, "endOffset": 108}, {"referenceID": 23, "context": "We train both the vanilla model as well as the proposed TM-NMT based on this configuration from scratch using Adam optimizer (Kingma and Ba, 2014) with the initial learning rate set to 0.", "startOffset": 125, "endOffset": 146}, {"referenceID": 11, "context": "By embedding an input of any modality into a fixed vector space and using fast approximate search (Douze et al., 2017), this approach can, for instance, be used for multimedia description generation (Cho et al.", "startOffset": 98, "endOffset": 118}, {"referenceID": 6, "context": ", 2017), this approach can, for instance, be used for multimedia description generation (Cho et al., 2015).", "startOffset": 88, "endOffset": 106}], "year": 2017, "abstractText": "In this paper, we extend an attention-based neural machine translation (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage\u2013retrieval stage\u2013, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage\u2013translation stage\u2013, a novel translation model, called translation memory enhanced NMT (TM-NMT), seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (En-Fr, En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.", "creator": "LaTeX with hyperref package"}}}