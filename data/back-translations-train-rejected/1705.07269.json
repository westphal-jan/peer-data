{"id": "1705.07269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement learning", "abstract": "Deep Reinforcement Learning (DRL) methods have performed well in an increasing numbering of high-dimensional visual decision making domains. Among all such visual decision making problems, those with discrete action spaces often tend to have underlying compositional structure in the said action space. Such action spaces often contain actions such as go left, go up as well as go diagonally up and left (which is a composition of the former two actions). The representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces. We propose a new learning paradigm, Factored Action space Representations (FAR) wherein we decompose a control policy learned using a Deep Reinforcement Learning Algorithm into independent components, analogous to decomposing a vector in terms of some orthogonal basis vectors. This architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously, while executing only one of them. We demonstrate that FAR yields considerable improvements on top of two DRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage Actor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous n-step Q-Learning) in 9 out of 13 tasks.", "histories": [["v1", "Sat, 20 May 2017 07:18:40 GMT  (1662kb,D)", "http://arxiv.org/abs/1705.07269v1", "11 pages + 7 pages appendix"]], "COMMENTS": "11 pages + 7 pages appendix", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sahil sharma", "aravind suresh", "rahul ramesh", "balaraman ravindran"], "accepted": false, "id": "1705.07269"}, "pdf": {"name": "1705.07269.pdf", "metadata": {"source": "CRF", "title": "Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement learning", "authors": ["Sahil Sharma", "Balaraman Ravindran"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Q-learning", "text": "One such non-political TD learning algorithm [Sutton, 1988] is Q-Learning [Watkins & Dayan, 1992]. Q-Learning leads to the convergence of an estimated Q function with the optimal Q function. Q-Learning updates are derived from the equation: Q (st, at) \u2190 Q (st, at) + \u03b1 (rt + 1 + \u03b3maxa \u2032 Q (st + 1, a \u2032) \u2212 Q (st, at)). From the Q function estimate, a policy can be derived by selecting each action to produce an \"argmaxa \u2032 Q (s, a \u2032)."}, {"heading": "2.2 Advantage Actor-Critic", "text": "Actor-critical algorithms [Konda & Tsitsiklis, 2000] are methods for the political gradient [Sutton & Barto, 1998]. Most actor-critical algorithms use parametric representations of the actor (\u03c0 (s; \u03b8) and the critic V (s; w). A distorted sampling for the political gradient results from Q (st, at, at). To reduce the deviation in the updates, a basic term is introduced, and the sampling of the political gradient becomes (p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p:"}, {"heading": "2.3 Asynchronous Advantage Actor-Critic (A3C)", "text": "Extending the Advantage Actor-Critic algorithms in the DRL setup naively does not work, because the stochastic Gradient Descent algorithms commonly used with such DRL setups assume that the input samples are independent and identically distributed. An actor-critical algorithm parameterized by neural networks has highly correlated inputs and therefore performs poorly when using gradient-based methods. Asynchronous Advantage Actor-Critic (A3C) methods [Mnih et al., 2016b] solve this problem by using asynchronous parallel actor-learners who simultaneously explore different parts of government space. Each learner maintains its own set of parameters that are routinely synchronized with the other learners."}, {"heading": "2.4 Asynchronous N-step Q-learning (AQL)", "text": "A modified version of Q-Learning uses an n-step return-based TD target [Peng & Williams, 1996; Watkins & Dayan, 1992] to achieve a faster spread of reward and a balance between bias and variance in the estimation of the action value function; the modified update equation is given by: Q (st, at) \u2190 Q (st, at) + \u03b1 (n \u00b2 i = 1 Rt + i + \u03b3max a \u00b2 Q (st + n, a \u00b2) \u2212 Q (st, at)). Similar to the Advantage Actor-Critic, n-Step Q-Learning was extended to work in the DRL setup by introducing asynchronous n-step Q-learning [Mnih et al., 2016b]."}, {"heading": "3 Factored Action Representations for Deep Reinforcement Learning", "text": "In fact, it is a reactionary act, which is an act that is in the way that it has actually taken place in the past, in the way that it has taken place in the past, in the way that it has taken place in the past, in the way that it has taken place in the past, in the way that it has taken place in the past, in the way that it has been carried out in the past, and in the way that it has taken place in the past, in the way that it has acted in the present and in the present."}, {"heading": "3.1 Visualization of Action Factoring for Atari", "text": "The first factor encodes horizontal movement (ah = left / right / no horizontal movement), the second factor encodes vertical movement (av = up / down / no vertical movement), the third factor encodes whether or not to shoot (af = fire / not fire). A visualization of this action space decomposition for the Atari domain is in Figure 2. This composite action space can be represented as a three-dimensional cuboid of the dimensions 3 x 3 x 2, where each 1 x 1 x 1 x 1 cell represents a composite action. The axes of the cuboid represent the action factors."}, {"heading": "3.2 FARA3C", "text": "In this section, we describe the instantiation of FAR for the A3C algorithm. X is the policy of the actor part of A3C. Let ph, pv, and pf specify the action factors corresponding to the horizontal, vertical, and launch dimensions. The action factors are specified by a combination function M = Softmax. The action policy is specified by: \u03c0 (a | s) = Softmax (m (ph (ah | s), pv (av | s), pf (af | s))))) In the equation, a = [ah, av, af] BA, ah-Ah, av-Av, and af-Af. Figure 3 contains a visual representation of the architecture of FARA3C."}, {"heading": "3.3 The Additive Combination Function", "text": "The additive function is a non-parametric choice for m in the definition of M. For FARA3C, the additive combination function has the form: \u03c0 (a | s) = softmax (ph (ah | s) + pv (av | s) + pf (af | s)). This policy can now be rewritten as the product of three probability distributions: \u03c0 (a | s) = softmax (ph (ah | s))) \u00d7 softmax (pv (av | s)) \u00b7 softmax (pf (af | s))). Consider an action a = [ah, av, af] BA, so that the sampling of a composite action a from \u03c0 (a | s) is equivalent to the independent sampling of ai = softmax (pi (ai | s)), whereby the choice of additive terms for the interpretation of another combination policy (a | s) functions as constitutive."}, {"heading": "3.4 FARAQL", "text": "In this section, we describe the instantiation of our framework for the AQL algorithm. X is the Q function modelled by AQL. Let us call Qh, Qv, and Qf the set of action factors corresponding to the horizontal, vertical, and launch dimensions. The action factors are combined by a combination function M. In this case: Q (s, a) = M (Qh (ah | s), Qv (av | s), Qf (af | s)), where a = [ah, av, aw] \u0394A, ah-Ah, av-Av, and af-Af."}, {"heading": "4 Experiments and results", "text": "Our experiments are designed to answer the following questions: 1. Which combination functions (M) work well for action space factoring? 2. Does action factoring improve the performance of DRL algorithms (A3C & AQL)? 3. Does FAR learn fundamentally robust political representations compared to baselines?"}, {"heading": "4.1 Choice of the Combination Function (M)", "text": "In this subsection, we answer the first question. To select a good M, many competing FARA3C networks were trained, each implementing a different non-parametric selection of M at the alien task in the Atari domain. Networks were trained for 50 million steps based on the same random initialization. The combination functions M, with which we experimented, had the form Softmax (m (f1 (a1 | s), f2 (a2 | s),..., fn (an | s))), with m, summation, multiplication, arithmetic mean, harmonic mean, geometric mean, minimum). Note that the arithmetic mean and the summation functions are identical, excluding a scaling factor of n (number of action factors). However, this subtle difference could lead to learning different strategies af. We observed that the choice of summing function = summation resulted in a result similar to FAR3Q (we specified an AAR3Q for the highest function)."}, {"heading": "4.2 Gameplay Experiments and Results: FARA3C", "text": "We trained FARA3C networks on 14 Atari tasks as shown in Figure 3. For each of the tasks, we trained a network of three different random seeds and obtained the results to estimate the performance of the algorithm. An A3C base agent was also trained with the same random seeds. A comparison of the performance of FARA3C and A3C agents is shown in Figure 5. All results of our experiments are shown in Table 1 in Appendix C. The development of the game performance for each of the tasks was plotted in relation to the training time. Training curves were averaged across the three random seeds. Figure 6 contains the training curves for six of the games. Training curves for all games are shown in Appendix D. Detailed explanations of the training sequence, the evaluation procedure and the hyperparameters can be found in Appendix A."}, {"heading": "4.3 Gameplay Experiments and Results: FARAQL", "text": "Similar to Figure 3, a modified network with Equation 1 was built for FARAQL and trained for thirteen tasks in the Atari domain. Performance was estimated by averaging performances over three random seeds. An AQL base agent was trained with the same random seeds. A visual comparison between AQL and the FARAQL agent is shown in Figure 8. Training curves for six of the games were shown in Figure 7. Training curves for all games are shown in Appendix D."}, {"heading": "5 Analysis: Test for robustness of policies learned by FARA3C & A3C", "text": "We claim that the strategies learned from FARA3C are more robust than those learned from A3C. This is because the FARA3C agents are able to learn about multiple actions by using factorized political representation while executing one. Consequently, one should expect that FARA3C has a better order of actions in politics than A3C. To empirically confirm this hypothesis, we conducted experiments comparing the robustness of the A3C and FARA3C strategies. 5.1 Equal coincidence from Best-K analyses The experiment is that a trained agent is taken. In probability 1, the agent stings actions from the learned policies. In all likelihood, he stings actions randomly from the best strategies. The use of the learned strategies leads to noise in the learned policies, and the agents with the more robust strategies would exhibit a lower rate of increases."}, {"heading": "5.2 Noise Injection analysis", "text": "s guidelines are written as \u03c0 (a | s) = softmax (G (s)), the corrupt policy \u03c0cor (a | s) of the agent can be represented as \u03c0cor (a | s) = softmax (G (s) / Z). Increasing Z leads to an injection of more noise into the guidelines. Therefore, a more robust policy is expected to have a lower performance drop when increasing Z. Hyperparameter Z has been varied from 1.0 to 3.0 in steps of 0.25 and the performance has been calculated against the maximum score of the agent after normalization. Figure 9 shows that the FARA3C agents are more robust than noise injection for most tasks."}, {"heading": "6 Conclusion and Future Work", "text": "We propose a novel framework (FAR) for the degradation of policies and action value functions via a separate action space. FAR takes a policy / value function into independent components and models those that use independent output levels of a neural network. FAR framework allows DRL agents to exploit the underlying compositional structure in discrete action spaces in common action areas to learn better strategies and action value functions. We demonstrate empirically the superiority of FAR over the base methods considered. A possible extension of this framework would be the combination of action space factoring with the concept of action repetition, as discussed by Sharma et al. [2017]. Action repetition could act as another factor of action space and this expansion could allow to capture a large number of macroeconomic actions."}, {"heading": "Appendix A: Experimental Details", "text": "In this appendix we document the experimental details of all our experiments. Note that all reported game performances and diagrams are averages above 3 random seeds to ensure that the comparisons are robust against random starting points for the parameter vectors.FARA3C and A3CThe human starts evaluation paradigm proposed by Mnih et al. [2016b] is difficult to replicate without the same human tester trajectories. Therefore, we use the same training and evaluation method as Sharma et al. [2017]."}, {"heading": "On hyper-parameters", "text": "This year, it has come to the point where it can only take a year for an outcome to be reached."}, {"heading": "Appendix B: The FARA3C Algorithm", "text": "In this attachment, we present pseudo-code versions of the training algorithm for FARA3C = > Q \u00b2 Reward (31) \u00b7 Q \u00b2 Reward (32) \u00b7 FARA3CAlgorithm (27) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "Appendix C: Raw Performance tables", "text": "In this appendix we document the raw performance of all our agents and the corresponding output performance. All performance was determined by averaging three random seeds to ensure a robust comparison. FARA3C and A3C Table 1 show that FARA3C outperforms A3C in 9 of the 14 tasks we experimented with. FARAQL and AQL Table 2 show that FARAQL outperforms AQL in 9 of the 13 tasks we experimented with."}, {"heading": "Appendix D: Training curves", "text": "This appendix contains all the training curves for all the algorithms presented in this paper. All the training curves were averaged over three random seeds to ensure a robust comparison."}], "references": [{"title": "Orthogonal representation of sound dimensions in the primate midbrain", "author": ["Baumann", "Simon", "Griffiths", "Timothy D", "Sun", "Li", "Petkov", "Christopher I", "Thiele", "Alexander", "Rees", "Adrian"], "venue": "Nature neuroscience,", "citeRegEx": "Baumann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baumann et al\\.", "year": 2011}, {"title": "The \u201cindependent components\u201d of natural scenes are edge filters", "author": ["Bell", "Anthony J", "Sejnowski", "Terrence J"], "venue": "Vision research,", "citeRegEx": "Bell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bell et al\\.", "year": 1997}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Actor-critic algorithms. In Advances in neural information processing", "author": ["Konda", "Vijay R", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Konda et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2000}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Mnih", "Volodymyr", "Agapiou", "John", "Osindero", "Simon", "Graves", "Alex", "Vinyals", "Oriol", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1606.04695,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Incremental multi-step q-learning", "author": ["Peng", "Jing", "Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Peng et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Peng et al\\.", "year": 1996}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "4th International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning", "author": ["Sharma", "Sahil", "Lakshminarayanan", "Aravind S", "Ravindran", "Balaraman"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Sharma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2017}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Intra-option learning about temporally abstract actions", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder P"], "venue": "In ICML,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Technical note: Q-learning", "author": ["Watkins", "Christopher J.C. H", "Dayan", "Peter"], "venue": "Mach. Learn.,", "citeRegEx": "Watkins et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watkins et al\\.", "year": 1992}, {"title": "Torcs, the open racing car simulator. Software available at http://torcs", "author": ["Wymann", "Bernhard", "Espi\u00e9", "Eric", "Guionneau", "Christophe", "Dimitrakakis", "Christos", "Coulom", "R\u00e9mi", "Sumner", "Andrew"], "venue": "sourceforge. net,", "citeRegEx": "Wymann et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wymann et al\\.", "year": 2000}, {"title": "We used a low level architecture similar to Mnih et al", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2017\\E", "shortCiteRegEx": "Sharma", "year": 2017}, {"title": "Figure 3 contains a visual depiction", "author": ["Mnih"], "venue": null, "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "2016b] the Actor and Critic networks share all but the final output", "author": ["Mnih"], "venue": null, "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 8, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 13, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 6, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 12, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 4, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 2, "context": ", 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al., 2013], MuJoCo [Todorov et al.", "startOffset": 120, "endOffset": 144}, {"referenceID": 18, "context": ", 2013], MuJoCo [Todorov et al., 2012], TORCS Wymann et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 2, "context": ", 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al., 2013], MuJoCo [Todorov et al., 2012], TORCS Wymann et al. [2000] and the board game of Go [Silver et al.", "startOffset": 121, "endOffset": 204}, {"referenceID": 0, "context": "The key motivations for our work are principles underlying biological systems, that have shown to learn representations in an independent and orthogonal manner [Baumann et al., 2011; Bell & Sejnowski, 1997].", "startOffset": 160, "endOffset": 206}, {"referenceID": 16, "context": "At an abstract level, our work is also similar to intra-option learning [Sutton et al., 1998] frameworks, which revolves around the idea of learning about an option while executing another.", "startOffset": 72, "endOffset": 93}, {"referenceID": 9, "context": "Sharma et al. [2017] have demonstrated remarkable improvements in a variety of domains using a framework that factors the policy into one for choosing actions and another for choosing repetitions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "This includes algorithms which model Q-functions (like DQN [Mnih et al., 2015] or asynchronous n-step Q-learning [Mnih et al.", "startOffset": 59, "endOffset": 78}, {"referenceID": 14, "context": "This sub-section demonstrates that the choice of additive combination function for A3C gives the action policy a nice alternate interpretation in terms a product of constituent factors\u2019 policies (such representations have been explored in other works such as Sharma et al. [2017]).", "startOffset": 259, "endOffset": 280}, {"referenceID": 14, "context": "A possible extension of this framework would be to combine action-space factoring with the concept of action repetition as discussed by Sharma et al. [2017]. Action repetition could act as yet another factor of the action space and this extension could allow one to capture a large set of macro actions.", "startOffset": 136, "endOffset": 157}], "year": 2017, "abstractText": "Deep Reinforcement Learning (DRL) methods have performed well in an increasing numbering of high-dimensional visual decision making domains. Among all such visual decision making problems, those with discrete action spaces often tend to have underlying compositional structure in the said action space. Such action spaces often contain actions such as go left, go up as well as go diagonally up and left (which is a composition of the former two actions). The representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces. We propose a new learning paradigm, Factored Action space Representations (FAR) wherein we decompose a control policy learned using a Deep Reinforcement Learning Algorithm into independent components, analogous to decomposing a vector in terms of some orthogonal basis vectors. This architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously, while executing only one of them. We demonstrate that FAR yields considerable improvements on top of two DRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage Actor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous n-step Q-Learning) in 9 out of 13 tasks.", "creator": "LaTeX with hyperref package"}}}