{"id": "1705.07290", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Deep Sparse Coding Using Optimized Linear Expansion of Thresholds", "abstract": "We address the problem of reconstructing sparse signals from noisy and compressive measurements using a feed-forward deep neural network (DNN) with an architecture motivated by the iterative shrinkage-thresholding algorithm (ISTA). We maintain the weights and biases of the network links as prescribed by ISTA and model the nonlinear activation function using a linear expansion of thresholds (LET), which has been very successful in image denoising and deconvolution. The optimal set of coefficients of the parametrized activation is learned over a training dataset containing measurement-sparse signal pairs, corresponding to a fixed sensing matrix. For training, we develop an efficient second-order algorithm, which requires only matrix-vector product computations in every training epoch (Hessian-free optimization) and offers superior convergence performance than gradient-descent optimization. Subsequently, we derive an improved network architecture inspired by FISTA, a faster version of ISTA, to achieve similar signal estimation performance with about 50% of the number of layers. The resulting architecture turns out to be a deep residual network, which has recently been shown to exhibit superior performance in several visual recognition tasks. Numerical experiments demonstrate that the proposed DNN architectures lead to 3 to 4 dB improvement in the reconstruction signal-to-noise ratio (SNR), compared with the state-of-the-art sparse coding algorithms.", "histories": [["v1", "Sat, 20 May 2017 11:14:39 GMT  (3497kb,D)", "http://arxiv.org/abs/1705.07290v1", "Submission date: November 11, 2016. 19 pages; 9 figures"]], "COMMENTS": "Submission date: November 11, 2016. 19 pages; 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["debabrata mahapatra", "subhadip mukherjee", "chandra sekhar seelamantula"], "accepted": false, "id": "1705.07290"}, "pdf": {"name": "1705.07290.pdf", "metadata": {"source": "CRF", "title": "Deep Sparse Coding Using Optimized Linear Expansion of Thresholds", "authors": ["Debabrata Mahapatra", "Subhadip Mukherjee", "Chandra Sekhar Seelamantula"], "emails": [], "sections": [{"heading": null, "text": "Index terms - compressive sensing, sparse recovery, iterative shrinkage, ISTA, FISTA, Hesse-free optimization, deep learning, reverse propagation, linear expansion of thresholds (LET).F"}, {"heading": "1 INTRODUCTION", "text": "[1], [2], [3], [4] has become enormously important in signal processing in the last decade. [Most signals occurring in real-world application techniques allow for sparse representation on an appropriate basis. The problem is also comparable to that of best base selection or sparse encoding, with the intention of finding the sparse representation of a signal in a complete set of base vectors. [4] Compressive sensor techniques have been successfully used in a variety of signal processing techniques such as deconvolution [5], denoising [6], superresolution [7], etc. The applications of CS have transcended the domain of classical signal processing and applications far beyond medical applications."}, {"heading": "1.1 Prior Art", "text": "The use of a trained NN for sparse coding is based primarily on the observation that the updates in an iterative algorithm can be interpreted as layers of a deep neural network. Your model, called a learned ISTA, implements a truncated version of ISTA with trained weights and biases. The parameters of the network are formed via a dataset of measurement signal pairs, reducing the number of parameters fed into the system."}, {"heading": "1.2 This Paper", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2 ITERATIVE SHRINKAGE ALGORITHMS: A NEURAL NETWORK PERSPECTIVE", "text": "Compressive sampling deals with the restoration of a sparse signal x-Rn from a measurement of the format = Ax + EX, (1) where y-Rm, m < n, A-Rm \u00b7 n is the sampling matrix and denotes the measurement noise. Signal x itself could be sparse or it could allow a sparse representation in an appropriate base B-Rn, which means that x = B\u03b1-Rn is sparse. For example, natural images can be sparse when B is taken as an orthogonal discrete cosine transformation or wave base. Without loss of generality, we consider the spareness of x in the canonical base (columns of an identity matrix). If x allows a sparse representation in a base that differs from identity B, A and x in (1) the signal should be replaced by A-Ratix."}, {"heading": "2.1 Proximal Gradient Methods", "text": "To make the exposure self-contained, let us briefly recall the connection between the proximal gradient methods and the supplying neural networks (NN) originally established by Gregor and LeCun [26]. Let us consider an iterative algorithm to the solution (3) that generates an estimate xt in the tth iteration. (4) The GD algorithm to minimize f (x), with a fixed increment of f \u2212 g, takes the form text + 1 = xt \u2212 f (xt) = f (xt) > f (xt) > f (xt) > f (xt) > f (xt) > f (xt). (4) The GD algorithm to minimize f (x), with a fixed step size of f \u2212 g, assumes the form xt + 1 = xt \u2212 f (xt). The GD update corresponds to the minimization of a quadratically regulated affinapproximation: x x x x."}, {"heading": "3 PARAMETRIC ACTIVATION: LINEAR EXPANSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "OF THRESHOLDS (LET)", "text": "For convenience, we refer to the NN analogy of the generic proximal operator P g\u03bd = q = = activation function as we know it. Our approach refers to the construction of the x-system by selecting a linear combination of K-elementary thresholds (u) = K-elementary threshold (u), u-elementary functions (u), u-elementary functions (u), u-elementary functions (u), u-elementary functions (u), u-elementary functions (u), u-elementary functions (u), u-elementary functions (u), u-elementary functions (u), x-elementary functions (u), x-elementary functions (u) based on the derivatives of a Gaussian (DoG)."}, {"heading": "4 ARCHITECTURE OF THE PROPOSED NETWORK", "text": "Subsequently, we refer to NN-based activation as LETnet. Contrary to classical predictions, we are able to orient ourselves on two levels. (7) We can move on two levels. (7) We are proficient on two levels. (7) We are proficient on two levels. (7) We are proficient on three levels. (7) We have built a bridge between iterative algorithms for sparse coding and fodder forward. (8) The flexibility of LET-based control and efficient control via the induced regulators, as elucidated in Section 3. We deal with them as non-linear functions. Subsequently, we refer to NN-based activation as LETnet. Unlike the classical regulators, as elucidated in Section 3, we refer to them as non-linear functions."}, {"heading": "5.2 Exact Computation of the Hessian-vector Product", "text": "The key to the exact calculation of the Hessian vector product lies in the definition of the directional derivative operator (J = > J) (50).For a vector-weighted function h (c) of a vector c, the derivative product along u is defined as a standard derivative operator and is therefore linear.The dimensions of Ru (h) (c) are the same as those of h. The properties of Ru are specified in [50].For completeness, we point out that Ru behaves like a standard derivative operator and is therefore linear."}, {"heading": "5.2.1 Forward Pass", "text": "Note that Ru (xt) = Ru (K) = 1 ctk (x-t)), (a) = K-t = 1 (Ru (ctk) \u03c6k (x-t) + ctkRu (\u03c6k (x-t))), (b) = K-k = 1 [utk\u03c6k (x-t) + ctkRu (\u03c6k (x-t))], (30) where (a) is achieved by applying properties 1, 4 and 5; and (b) by applying properties 2. Using properties 6, Ru (x-t) yields values for Ru (x-t)) = MkRu (x-t)))) = MkRu (x-t), (31) where the Jacobic Mk (i, j) = x-t-t (x-t), as follows: 0 \u2264 i, j-t \u2212 unambiguous, Mk (ij) disappears for i = j-t (i), (i-t)."}, {"heading": "5.2.2 Backward Pass", "text": "The application of Ru on both sides (20) and the use of property 5 of the operator Ru = J (32), we getRu (3) (4), we getRu (4), we getRu (4), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (6), we getRu (6), we getRu (6), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (5), we getRu (7), we (7), we getRu (7), we getRu (7), we getRu (7), we getRu (7), we getRu (6, we getRu (6), we getRu (6), we getRu (5, we getRu (5), we getRu (5), we getRu (5, we getRu (5), we getRu (5), we getRu (5, we getRu (5), we getRu (5, we getRu (5), we getRu (5), we getRu (5, we getRu (5), we getRu (5, we getRu (5), we getRu (5), we getRu (5, we getRu (5), we getRu (5, we getRu (5), we getRu (5, we getRu (5), we getRu (5, we getRu), we (5, we getRu (5, we getRu), we getRu (5, we getRu (5, we getRu (5),"}, {"heading": "6.1 Experimental Setting", "text": "The reconstruction of a sparse signal x = 256 of m = d0,7ne-dimensional reconstruction is a reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reconstructivist reactionary reactionary reactionary reactionary reactivist reactivist reactivist reactivist reactiviiiiiiiiiiiiiiiiiiiiiiiiiiireactireactiritic reactireactionary reactiriviriviiiioiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "6.2.1 Initialization and termination criteria for CG", "text": "The CG algorithm required to calculate the optimal direction \u03b4 1 + \u03b22 \u2212 \u03b23 \u2212 \u03b23 \u2212 \u03b23 \u2212 JER + \u03b22 \u2212 YES functions is initialized with the maximum value achieved in the previous epoch of the HFO. Theoretically, KL iterations for the CG algorithm are required to calculate an exact solution for (17) q, since variable c is based on the dimension KL. Martens et al. [38] reported that it is sufficient to execute the CG algorithm only a few times in each HFO iteration according to a given criterion, for example, a criterion based on the relative improvement based on the definition of Q (i2) \u2212 Q (i2) \u2212 Q (2 \u2212 i1) c) Q (approximate (1) c), x W (1)."}, {"heading": "6.3 Explanation and Interpretation of the Result", "text": "The average and standard deviation of the reconstruction SNR, calculated over 10 independent studies, as a function of \u03c1 and input SNR, for different methods is shown in Figure 3. We observe that LETnetVar exceeds the ISTA and FISTA by a margin of about 4 dB, while there is a relatively small deviation and a margin of 3 dB over IRLS. The CoSaMP algorithm, which is by far the best greedy algorithm for an economical recovery, is reconstructed SNR in 1 to 2 dB higher than LETnetVar, especially for smaller values of the input SNR. However, unlike ISTA, FISTA, LETnet and IRLS, one needs to know the frugality of the ground reality in order to successfully execute CoSaMP. The improvement achieved with LETnetVar via MSE-ISTA algorithms is largely due to accurate and parsimonious modeling of the activation using LET."}, {"heading": "8 CONCLUSIONS AND OUTLOOK", "text": "In fact, most of them are able to decide for themselves what they want."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Ulugbek Kamilov from the Mitsubishi Electric Research Laboratory for clarifying several aspects related to the implementation of the MMSEISTA algorithm and Thierry Blu from the Chinese University of Hong Kong for his insights on LET representation and feedback on the manuscript."}, {"heading": "11 HESSIAN-VECTOR PRODUCT COMPUTATION", "text": "FOR fLETnet Analogous to LETnetVar, the Hessian vector product Hu, where H = 2J (c) and u is any vector with the same dimension as c, can be calculated exactly as Hu = Ru (\u0435cJ), with the operator Ru defined in (M-27). To evaluate Ru (\u0418cJ), one must execute a forward pass and a backward pass through fLETnet."}, {"heading": "11.1 Forward pass", "text": "If we apply Ru (zt) = (1 + \u03b2t) xt \u2212 1 \u2212 \u03b2txt \u2212 2 and (59) Ru (x-t = zt + b, we get Ru (zt) = (1 + \u03b2t) Ru (xt \u2212 1) \u2212 \u03b2tRu (xt \u2212 2) and (59) Ru (x-t) = WRu (zt) = (1 + \u03b2t) WRu (xt \u2212 1) \u2212 \u03b2tWRu (xt \u2212 2), (60); if we use the properties 1, 3 and 4 of Ru listed in Appendix B. The evaluation of (60) requires the calculation of Ru (xt), for t = 1: L. Similar to (M-30) we apply Ru on both sides of xt = (t) (x-t) (x-t) = K k = 1 c-t) and the calculation of (x-t)."}, {"heading": "11.2 Backward pass", "text": "Applied Ru (53), we getRu (5), we getRu (5), we getJ (5), we getJ (5), we getRu (5), we getJ (5), we getJ (5), we getRu (5), we tJ (5), we tJ (5), we tJ (5), we tJ (5), we tJ (5), we tJ (5), we tJ (5), we tJ (5), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ (6), we tJ, we tJ (6), we tJ, we tJ, we (6), we tJ (8 tJ, we, we, we (6), we tJ, we (6), we tJ (6), we tJ (6, we tJ (6), we tJ (6), we tJ (6), we tJ (6, we tJ (6), we tJ (6), we tJ (6, we tJ (6), we tJ (6), we tJ (6, we tJ (6), we tJ (6), we tJ (6, we tJ (6), we tJ (6, we tJ (6), we tJ (6), we tJ (6), we tJ (6, we tJ (6), we tJ (6), we tJ (6, we tJ (6), we tJ (6, we tJ (6), we tJ (6), we tJ (6, we tJ (6, we tJ (6), we tJ ("}], "references": [{"title": "An introduction to compressive sampling", "author": ["E.J. Cand\u00e9s", "M. Wakin"], "venue": "IEEE Signal Process. Mag., vol. 25, pp. 21 \u201330, Mar. 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressive sensing", "author": ["R. Baraniuk"], "venue": "IEEE Signal Process. Mag., vol. 24, no. 4, pp. 118\u2013121, Jul. 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e9s", "J. Romberg", "T. Tao"], "venue": "IEEE Trans. Info. Theory, vol. 52, no. 2, pp. 489\u2013509, Feb. 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Trans. Info. Theory, vol. 52, no. 4, pp. 1289\u20131306, Apr. 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Process., vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Trans. Image Process., vol. 19, no. 11, pp. 2861\u20132873, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse MRI: The application of compressed sensing for rapid MR imaging", "author": ["M. Lustig", "D. Donoho", "J.M. Pauly"], "venue": "Magnetic Resonance in Medicine, vol. 58, no. 6, pp. 1182\u20131195, Dec. 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "DNA array decoding from nonlinear measurements by belief propagation", "author": ["M. Sheikh", "S. Sarvotham", "O. Milenkovic", "R. Baraniuk"], "venue": "Proc. IEEE Workshop on Statistical Signal Process. (SSP), Aug. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Compressive radar imaging", "author": ["R. Baraniuk", "P. Steeghs"], "venue": "Proc. IEEE Radar Conf., Apr. 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Patt. Anal. and Mach. Intell., vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Image feature extraction by sparse coding and independent component analysis", "author": ["A. Hyvarinen", "E. Oja", "P. Hoyer", "J. Hurri"], "venue": "Proc. 14th IEEE Intl. Conf. on Patt. Recog., vol. 2, pp. 1268\u20131273, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E.J. Cand\u00e9s", "J. Romberg", "T. Tao"], "venue": "Comm. on Pure and Appl. Math. vol. 59 no. 8, pp. 1207\u20131223, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. Advances in Neural Info. Process. Systems, pp. 1097\u20131105, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1512.03385v1, Dec. 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks.", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "in Proc. Intl. Conf. on Learning Representations,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv:1412.7062, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural Networks: Tricks of the Trade, Second Edition, Springer, Berlin", "author": ["G. Hinton"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["S. Rifai", "Y. Bengio", "A. Courville", "P. Vincent", "M. Mirza"], "venue": "Proc. European Conference on Computer Vision, pp. 808\u2013822, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Training recurrent neural networks", "author": ["I. Sutskever"], "venue": "Ph.D. dissertation, University of Toronto, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends", "author": ["Z.H. Ling", "S.Y. Kang", "H. Zen", "A. Senior", "M. Schuster", "X.J. Qian", "H.M. Meng", "L. Deng"], "venue": "IEEE Signal Process. Mag., vol. 32, no. 3, pp. 35\u201352, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Process., pp. 6645\u20136649, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "arXiv:1206.6434, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, Dec. 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Large-scale feature learning with spike-and-slab sparse coding", "author": ["I. Goodfellow", "A. Courville", "Y. Bengio"], "venue": "Proc. Intl. Conf. on Machine Learning, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proc. Intl. Conf. on Machine Learning, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["J.R. Hershey", "J.L. Roux", "F. Weninger"], "venue": "arXiv:1409.2575v4, Nov. 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning optimal nonlinearities for iterative thresholding algorithms", "author": ["U.S. Kamilov", "H. Mansour"], "venue": "IEEE Signal Process. Letters, vol. 23, no. 5, pp. 747\u2013751, May 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C.D. Mol"], "venue": "Comm. Pure Appl. Math., vol. 57, pp. 1413\u20131457, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Parameter learning with truncated message-passing", "author": ["J. Domke"], "venue": "Proc. Computer Vision and Patt. Recog., pp. 2937\u20132943, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["J. Domke"], "venue": "IEEE Trans. Patt. Anal. and Mach. Intell., vol. 35, no. 10, pp. 24\u201354, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning efficient sparse and low rank models", "author": ["P. Sprechmann", "A.M. Bronstein", "G. Sapiro"], "venue": "IEEE Trans. Patt. Anal. and Mach. Intell., vol. 37, no. 5, pp. 1821\u20131833, Sep. 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1821}, {"title": "Learning deep `0 encoders", "author": ["Z. Wang", "Q. Ling", "T.S. Huang"], "venue": "arXiv:1509.00153v2, Nov. 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximal sparsity with deep networks", "author": ["B. Xin", "Y. Wang", "W. Gao", "D. Wipf"], "venue": "arXiv:1605.01636v2, May 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "The SURE-LET approach to image denoising", "author": ["T. Blu", "F. Luisier"], "venue": "IEEE Trans. Image Process., vol. 16, no. 11, pp. 2778\u20132786, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "A new SURE approach to image denoising: Interscale orthonormal wavelet thresholding", "author": ["F. Luisier", "T. Blu", "M. Unser"], "venue": "IEEE Trans. Image Process. vol. 16, no. 3, pp. 593\u2013606, Mar. 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "An iterative linear expansion of thresholds for `1-based image restoration", "author": ["H. Pan", "T. Blu"], "venue": "IEEE Trans. Image Process., vol. 22, no. 9, pp. 3715\u20133728, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proc. 27th Intl. Conf. Machine Learning, 2010. (ICML), pp. 735\u2013742, 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "IEEE Trans. Info. Theory, vol. 53, no. 12, pp. 4655\u20134666, Dec. 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "CoSamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Appl. and Computational Harmonic Anal., vol. 26, issue 3, pp. 301\u2013321, May 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Info. Theory, vol. 55, no. 5, pp. 2230\u20132249, May 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Royal Stat. Society, Series B, vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1996}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Ann. Statist., vol. 32, no. 2, pp. 407\u2013499, 2004.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-Wiener SURE-LET deconvolution", "author": ["F. Xue", "F. Luisier", "T. Blu"], "venue": "IEEE Trans. Image Process., vol. 22, no. 5, pp. 1954-1968, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1954}, {"title": "SURE-LET multichannel image denoising: interscale orthonormal wavelet thresholding", "author": ["F. Luisier", "T. Blu"], "venue": "IEEE Trans. Image Process., vol. 17, no. 4, pp. 482\u2013492, 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proc. Intl. Conf. on Machine Learning, 2013.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Iteratively re-weighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R. DeVore", "M. Fornasier", "C.S. G\u00fcnt\u00fcrk"], "venue": "Communications on Pure and Applied Math., vol. LXIII, pp. 1\u2013 38, 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "A method for solving the convex programming problem with convergence rate O ( 1 k2  )", "author": ["Y.E. Nesterov"], "venue": "Dokl. Akad. Nauk SSSR, vol. 269, pp. 543\u2013547, 1983.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1983}, {"title": "Sparse signal estimation by maximally sparse convex optimization", "author": ["I.W. Selesnick", "I. Bayram"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 5, pp. 1078\u20131092, Mar. 2014.  xv", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "Compressive sensing techniques have been successfully employed in a variety of signal processing applications such as deconvolution [5], denoising [6], superresolution [7], inpainting, etc.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "Compressive sensing techniques have been successfully employed in a variety of signal processing applications such as deconvolution [5], denoising [6], superresolution [7], inpainting, etc.", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "Compressive sensing techniques have been successfully employed in a variety of signal processing applications such as deconvolution [5], denoising [6], superresolution [7], inpainting, etc.", "startOffset": 168, "endOffset": 171}, {"referenceID": 7, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "In a seminal work [13], Cand\u00e8s et al.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "The convex programming formulation in [13] is also known as basis pursuit denoising.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 194, "endOffset": 198}, {"referenceID": 17, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 182, "endOffset": 186}, {"referenceID": 21, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 203, "endOffset": 207}, {"referenceID": 22, "context": "Several DNNbased methods have also been proposed for unsupervised feature extraction [23], [24].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "Several DNNbased methods have also been proposed for unsupervised feature extraction [23], [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "Specifically, the spike-and-slab model [25] has been shown to work efficiently for sparse feature extraction from large-scale data.", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "Gregor and Le Cun showed, for the first time, that a custom-designed discriminatively trained DNN architecture can solve the sparse coding problem efficiently [26].", "startOffset": 159, "endOffset": 163}, {"referenceID": 26, "context": "Further efforts have been made to make a one-to-one correspondence between iterative inferencing algorithms and DNNs [27], and to efficiently represent the nonlinearity that is at the heart of sparse coding [28].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "Further efforts have been made to make a one-to-one correspondence between iterative inferencing algorithms and DNNs [27], and to efficiently represent the nonlinearity that is at the heart of sparse coding [28].", "startOffset": 207, "endOffset": 211}, {"referenceID": 28, "context": "This unfolding process has been applied in the context of ISTA [29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 25, "context": "The possibility of using a trained feed-forward neural network for efficient sparse coding was first demonstrated by Gregor and LeCun [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": "[27] proposed the idea of deep unfolding, wherein an iterative inference strategy, such as ISTA, inspires the architecture of a DNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The idea of unfolding an iterative inference algorithm with shared parameters over the layers was developed by Domke [30], [31], in the context of tree-reweighted belief propagation and mean-field inference.", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "The idea of unfolding an iterative inference algorithm with shared parameters over the layers was developed by Domke [30], [31], in the context of tree-reweighted belief propagation and mean-field inference.", "startOffset": 123, "endOffset": 127}, {"referenceID": 31, "context": "[32] proposed a learnable architecture of fixed complexity, by unfolding the iterative proximal descent algorithms for structured sparse and low-rank models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] to design a deep `0 encoder, with applications in image classification and clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] critically analyzed the merits of designing trainable models over conventional optimizationbased sparse coding algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Kamilov and Mansour [28] proposed a deep architecture for sparse coding motivated by ISTA, wherein the nonlinear activation function is modeled using cubic B-splines.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "However, we parameterize the nonlinear activation function using a linear expansion of thresholds (LET) [37] instead of cubic B-splines (Section 3).", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Unlike [28], the coefficients of the LET-based activations are untied across the layers of the network to enhance the expressive power of the model, without significantly affecting the training overhead.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "Also, a small number of coefficients, typically five per layer, suffice, which results in considerably less number of parameters to learn overall in comparison with [28].", "startOffset": 165, "endOffset": 169}, {"referenceID": 37, "context": "We also derive an efficient Hessian-free optimization (HFO) technique [38] to train the network (Section 5).", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "The motivation is derived from the fast iterative shrinkage threshold algorithm (FISTA) [5], which has a faster convergence rate than ISTA without increasing the computational load per iteration, thereby requiring considerably less number of iterations overall.", "startOffset": 88, "endOffset": 91}, {"referenceID": 14, "context": "We show that the resulting fast LETnet architecture, dubbed as fLETnet (Section 7), bears close resemblance to the recently proposed deep residual learning architecture [15], wherein each layer draws direct connections from two preceding layers instead of one, so as to improve the convergence performance.", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "It has been shown in [15] that the scheme enjoys the merits of a deep architecture, while successfully alleviating the problems of vanishing/exploding gradients.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "We carry out experimental validation of LETnet and fLETnet for sparse signal recovery and demonstrate their superiority over the the learning-based approach proposed in [28] as well as the conventional sparse coding algorithms that are not set up within a learning paradigm.", "startOffset": 169, "endOffset": 173}, {"referenceID": 38, "context": "Algorithms such as orthogonal matching pursuit (OMP) [39], compressive sampling matching pursuit (CoSaMP) [40], subspace pursuit [41], etc.", "startOffset": 53, "endOffset": 57}, {"referenceID": 39, "context": "Algorithms such as orthogonal matching pursuit (OMP) [39], compressive sampling matching pursuit (CoSaMP) [40], subspace pursuit [41], etc.", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "Algorithms such as orthogonal matching pursuit (OMP) [39], compressive sampling matching pursuit (CoSaMP) [40], subspace pursuit [41], etc.", "startOffset": 129, "endOffset": 133}, {"referenceID": 41, "context": "In the special case where G (x) = \u2016x\u20161, the resulting optimization is referred to as LASSO regression [42], [43].", "startOffset": 102, "endOffset": 106}, {"referenceID": 42, "context": "In the special case where G (x) = \u2016x\u20161, the resulting optimization is referred to as LASSO regression [42], [43].", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "To make the exposition self-contained, we briefly recall the connection between the proximal gradient methods and the feed-forward neural networks (NN), originally established by Gregor and LeCun [26].", "startOffset": 196, "endOffset": 200}, {"referenceID": 25, "context": "Gregor and LeCun [26] interpreted (8) as the feed-forward computation through a NN with weight matrix W and bias b shared across every layer.", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "We advocate the use of elementary functions based on the derivatives of a Gaussian (DoG) [35], [37], specified as", "startOffset": 89, "endOffset": 93}, {"referenceID": 36, "context": "We advocate the use of elementary functions based on the derivatives of a Gaussian (DoG) [35], [37], specified as", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 109, "endOffset": 113}, {"referenceID": 43, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 115, "endOffset": 119}, {"referenceID": 34, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 143, "endOffset": 147}, {"referenceID": 35, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 149, "endOffset": 153}, {"referenceID": 44, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "ISTA for the sparse estimation problem has a linear rate of convergence [5] and performs reasonably well for sufficiently large number of iterations.", "startOffset": 72, "endOffset": 75}, {"referenceID": 27, "context": "[28] proposed to employ a parametric linear combination of cubic B-splines instead of the soft-threshold, to parametrize the activation function, which is optimized during training and kept fixed across all layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "The reason for divergence is the exploding gradient [46], primarily caused by the unboundedness of \u03c61 in the LET expansion.", "startOffset": 52, "endOffset": 56}, {"referenceID": 37, "context": "We overcome this problem by the Hessian-Free optimization (HFO) technique [38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "In the ith epoch of HFO [38] (summarized in Algorithm 3), the search direction \u03b4c is obtained by minimizing a locally quadratic Taylor-series approximation J (q) i (c) to the actual cost J(c) at the ith iterate ci:", "startOffset": 24, "endOffset": 28}, {"referenceID": 37, "context": "In this case, explicit knowledge of Hi is not required, but it is required to compute the Hessian-vector product Hiu, for any vector u [38].", "startOffset": 135, "endOffset": 139}, {"referenceID": 37, "context": "However, in practice, only a small number of iterations of CG suffice to obtain an accurate estimate of the optimal search direction \u03b4c [38].", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "The entries of xsupp are either 0 or 1, drawn independently from a Bernoulli distribution with parameter \u03c1 \u2208 [0, 1].", "startOffset": 109, "endOffset": 115}, {"referenceID": 46, "context": "For performance comparison, we consider five state-ofthe-art sparse recovery algorithms: (i) ISTA, which employs the ST proximal operator; (ii) FISTA, which is an accelerated version of ISTA; (ii) CoSaMP, which is a greedy algorithm; (iv) the iteratively reweighted least-squares (IRLS) algorithm [47], which is based on majorization-minimization; and (v) MMSE-ISTA, which is a training-based algorithm developed by Kamilov and Mansour [28].", "startOffset": 297, "endOffset": 301}, {"referenceID": 27, "context": "For performance comparison, we consider five state-ofthe-art sparse recovery algorithms: (i) ISTA, which employs the ST proximal operator; (ii) FISTA, which is an accelerated version of ISTA; (ii) CoSaMP, which is a greedy algorithm; (iv) the iteratively reweighted least-squares (IRLS) algorithm [47], which is based on majorization-minimization; and (v) MMSE-ISTA, which is a training-based algorithm developed by Kamilov and Mansour [28].", "startOffset": 436, "endOffset": 440}, {"referenceID": 27, "context": "1 and the same value is used in MMSE-ISTA, as suggested in [28].", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "The MMSE-ISTA network is trained using GD with a step-size of \u03b7 = 10\u22124, as suggested in [28], and the algorithm is terminated when the number of training epochs exceeds 1000.", "startOffset": 88, "endOffset": 92}, {"referenceID": 37, "context": "[38] reported that it suffices to run the CG algorithm in each HFO iteration only a few times according to a preset criterion, for example, a criterion based on the relative improvement defined as", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "1i2) following [38].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "However, Beck and Teboulle [5] have devised a strategy to improve the convergence rate from linear to quadratic, without significantly increasing the computations in each iteration.", "startOffset": 27, "endOffset": 30}, {"referenceID": 47, "context": "Their algorithm, referred to as fast ISTA or FISTA, relies on an original idea by Nesterov [48], who showed how to accelerate the projected gradientdescent by incorporating a momentum factor.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "[15], who demonstrated significant performance improvements in various computer vision tasks using DRNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "However, it is known in the literature that convexity of the regularizer is not necessary [49] for promoting sparsity and may even be a restriction.", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "We address the problem of reconstructing sparse signals from noisy and compressive measurements using a feed-forward deep neural network (DNN) with an architecture motivated by the iterative shrinkage-thresholding algorithm (ISTA). We maintain the weights and biases of the network links as prescribed by ISTA and model the nonlinear activation function using a linear expansion of thresholds (LET), which has been very successful in image denoising and deconvolution. The optimal set of coefficients of the parametrized activation is learnt over a training dataset containing measurement-sparse signal pairs, corresponding to a fixed sensing matrix. For training, we develop an efficient second-order algorithm, which requires only matrix-vector product computations in every training epoch (Hessian-free optimization) and offers superior convergence performance than gradient-descent optimization. Subsequently, we derive an improved network architecture inspired by FISTA, a faster version of ISTA, to achieve similar signal estimation performance with about 50% of the number of layers. The resulting architecture turns out to be a deep residual network, which has recently been shown to exhibit superior performance in several visual recognition tasks. Numerical experiments demonstrate that the proposed DNN architectures lead to 3-4 dB improvement in the reconstruction signal-to-noise ratio (SNR), compared with the state-of-the-art sparse coding algorithms.", "creator": "LaTeX with hyperref package"}}}