{"id": "1705.07312", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Lower Bound On the Computational Complexity of Discounted Markov Decision Problems", "abstract": "We study the computational complexity of the infinite-horizon discounted-reward Markov Decision Problem (MDP) with a finite state space $|\\mathcal{S}|$ and a finite action space $|\\mathcal{A}|$. We show that any randomized algorithm needs a running time at least $\\Omega(|\\mathcal{S}|^2|\\mathcal{A}|)$ to compute an $\\epsilon$-optimal policy with high probability. We consider two variants of the MDP where the input is given in specific data structures, including arrays of cumulative probabilities and binary trees of transition probabilities. For these cases, we show that the complexity lower bound reduces to $\\Omega\\left( \\frac{|\\mathcal{S}| |\\mathcal{A}|}{\\epsilon} \\right)$. These results reveal a surprising observation that the computational complexity of the MDP depends on the data structure of input.", "histories": [["v1", "Sat, 20 May 2017 14:21:30 GMT  (374kb,D)", "http://arxiv.org/abs/1705.07312v1", null]], "reviews": [], "SUBJECTS": "cs.CC cs.LG", "authors": ["yichen chen", "mengdi wang"], "accepted": false, "id": "1705.07312"}, "pdf": {"name": "1705.07312.pdf", "metadata": {"source": "CRF", "title": "Lower Bound On the Computational Complexity of Discounted Markov Decision Problems", "authors": ["Yichen Chen", "Mengdi Wang"], "emails": [], "sections": [{"heading": null, "text": "These results show a surprising observation that the computational complexity of the MDP depends on the data structure of the input."}, {"heading": "1 Introduction", "text": "The Markov Decision Problem (MDP tuple M) arises from stochastic control processes in which a planner attempts to make a sequence of decisions about how the state of the process develops. It provides a basic mathematical framework for dynamic programming and reinforcement learning. It has wide applications in technical systems, surgical research, artificial intelligence, and computer games. Consider the finite action space in which the MDP reward is ineffective with a finite number of states and an infinite number of actions. An example of such MDP can be described by a tuple M = (S, A, P, R, AP), where S is a finite state space, is a finite action space in which it is (0, 1) the discount factor, r: S \u00d7 7 \u2192 a reward function, and P = (Pa) a collection of matrices of transition probabilities. The size of an MDP model is O (| 2).The MDP model specifies a random state walk."}, {"heading": "2 Related Works", "text": "The MDP has been known to be solvable in polynomial time by dynamic programming [PT87], but the dependence of complexity on the size of the state space and the space for action is not clear. There are three important approaches to solving the MDP: the value orientation method, the policy iteration method, the policy iteration method, and the linear programming method. Most of the existing work focuses on determining the upper limit of computational complexity for these methods; see, for example, [How60, Tse90, LDK95, MS99, Ye05, Ye11, Sch13, FH14]. The best known complexity results for these methods are superlinear with respect to the input size O (S | 2). We have recently developed a randomized primary dual method that achieves a sublinear runtime."}, {"heading": "3 Main Theorems", "text": "In this section, we will establish the lower limit of computational complexity for all algorithms that use the specification of an instance of MDP as the input and output of a policy. We will show that the format and data structure of the input is crucial. We will consider three different cases: (1) the default case in which the transition probabilities P are given as arrays; (2) the case in which the cumulative probabilities are given instead of the transition probabilities in the binary tree format. We will show that the first case is more difficult than the latter two cases. Proofs will be moved to Sections 4, 5 and application probabilities."}, {"heading": "4 Family of Hard Instances of MDP", "text": "In this section, we take an essential step toward establishing the lower limit of computational complexity for the MDP system. (We) Allow a computational error to occur. (We) Allow a computational error to occur that leads to a single state. (We) Allow a single state to occur in which the action space A = AU-1 and aN is a single action. (We) construct two sets of MDP instances that are difficult to distinguish. (...) We construct two sets of MDP instances. (...) We construct two sets of MDP instances that are difficult to distinguish. (...) We construct two sets of MDP instances. (...) We construct two sets of MDP instances. (...) We construct two sets of MDP instances. (...) We construct two sets of MDP instances. (...) We construct two sets of MDP instances that are difficult to distinguish. (...) We construct two sets of MDP instances."}, {"heading": "5 Proof of Main Theorems", "text": "In this section, we first introduce a partial problem about the differentiation of matrices and the determination of their complexity in the lower boundary range. Then, we develop the proofs for the main theorems using the Minimax principle of Yao."}, {"heading": "5.1 A Sub-Problem", "text": "To facilitate our proof, let A and B be two m x n matrices, the values of 0 or 1. For each row of A there is an entry of 1 and the other entries are 0. B is similar to A except that there is a row with all zeros. We say that (A, B) is a \"pair of matrices\" if A and B differ by exactly one entry. Note that there are a total of mnm pairs of matrices. We focus on deterministic algorithms that take the m x x as input. For each pair (A, B) we say that a deterministic algorithm is able to distinguish A from B when it queries the entry (i, j), with Aij 6 = Bij within a certain number of steps, if given as either A or B."}, {"heading": "5.2 Proof of Theorem 1", "text": "Let us summarize the main arguments of the evidence: Suppose that any randomized algorithm can find an optimal policy for the MDP with high probability within a time limit. (It follows that an algorithm for matrix differentiation can be constructed and applied if there is a deterministic algorithm capable of successfully distinguishing a large number of hard instances in M1 from these inM2. (It follows that one can construct and apply an algorithm for matrix differentiation.) The proof has a spirit similar to the method of the relational adversary [Amb00, Aar06]. Proof Theorem 1. Given the state space S and the discount factor M0, denote M0, where the entries of the MDP instances are set, where the entries of P and r multiples are 0.01. Note: M1 and M2 are two sets of the M0."}, {"heading": "5.3 Proofs of Theorem 2 and 3", "text": "The proofs of theorems 2 and 3 are much simpler than those of Theorem 1. This is because M3 and M4 differ in a single entry. The following proofs are not based on Lemma 1. Proofs of Theorem 2. Using similar notations as in section 5.2, we place Theorem 2 as: Ifmax \u00b5 | A | /) min M0, M0, M0, M0, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4, M4 M4, M4, M4, M4, M4, M4, M4, M4 M4, M4, M4, M4, M4 M4, M4, M4, M4 M4, M4, M4, M4 M4, M4, M4, M4 M4, M4, M4 M4, M4, M4 M4, M4, M4, M4, M4 M4, M4 M4, M4, M4 M4, M4, M4, M4, M4 M4, M4, M4, M4, M4, M4 M4, M4, M4, M4, M4, M4, M4, M4, M4 M4, M4, M4, M4, M4 M4, M4, M4, M4, M4, M4, M4, M4, M4 M4, M4, M4, M4, M4, M4, M4, M4, M4,"}, {"heading": "A Proof of Lemma 1", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf"}], "references": [{"title": "Lower bounds for local search by quantum arguments", "author": ["Scott Aaronson"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Aaronson.,? \\Q2006\\E", "shortCiteRegEx": "Aaronson.", "year": 2006}, {"title": "Quantum lower bounds by quantum arguments", "author": ["Andris Ambainis"], "venue": "In Proceedings of the thirty-second annual ACM symposium on Theory of computing,", "citeRegEx": "Ambainis.,? \\Q2000\\E", "shortCiteRegEx": "Ambainis.", "year": 2000}, {"title": "On the sample complexity of reinforcement learning with a generative model", "author": ["Mohammad Gheshlaghi Azar", "R\u00e9mi Munos", "Bert Kappen"], "venue": "arXiv preprint arXiv:1206.6461,", "citeRegEx": "Azar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2012}, {"title": "Dynamic Programming", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas.", "year": 1995}, {"title": "Abstract dynamic programming", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2013\\E", "shortCiteRegEx": "Bertsekas.", "year": 2013}, {"title": "A time-space tradeoff for sorting on non-oblivious machines", "author": ["Allan Borodin", "Michael J Fischer", "David G Kirkpatrick", "Nancy A Lynch", "Martin Tompa"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Borodin et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Borodin et al\\.", "year": 1981}, {"title": "A time-space tradeoff for element distinctness", "author": ["Allan Borodin", "Faith Fich", "Friedhelm Meyer auf der Heide", "Eli Upfal", "Avi Wigderson"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Borodin et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Borodin et al\\.", "year": 1987}, {"title": "Neuro-dynamic programming: an overview", "author": ["Dimitri P Bertsekas", "John N Tsitsiklis"], "venue": "In Proceedings of the 34th IEEE Conference on Decision and Control,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1995}, {"title": "A survey of computational complexity results in systems and control", "author": ["Vincent D Blondel", "John N Tsitsiklis"], "venue": null, "citeRegEx": "Blondel and Tsitsiklis.,? \\Q2000\\E", "shortCiteRegEx": "Blondel and Tsitsiklis.", "year": 2000}, {"title": "Sublinear optimization for machine learning", "author": ["Kenneth L Clarkson", "Elad Hazan", "David P Woodruff"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Clarkson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2012}, {"title": "The complexity of dynamic programming", "author": ["Chef-Seng Chow", "John N Tsitsiklis"], "venue": "Journal of complexity,", "citeRegEx": "Chow and Tsitsiklis.,? \\Q1989\\E", "shortCiteRegEx": "Chow and Tsitsiklis.", "year": 1989}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Christoph Dann", "Emma Brunskill"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dann and Brunskill.,? \\Q2015\\E", "shortCiteRegEx": "Dann and Brunskill.", "year": 2015}, {"title": "Pac bounds for multi-armed bandit and Markov decision processes", "author": ["Eyal Even-Dar", "Shie Mannor", "Yishay Mansour"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "The value iteration algorithm is not strongly polynomial for discounted dynamic programming", "author": ["Eugene A Feinberg", "Jefferson Huang"], "venue": "Operations Research Letters,", "citeRegEx": "Feinberg and Huang.,? \\Q2014\\E", "shortCiteRegEx": "Feinberg and Huang.", "year": 2014}, {"title": "Subexponential lower bounds for randomized pivoting rules for the simplex algorithm", "author": ["Oliver Friedmann", "Thomas Dueholm Hansen", "Uri Zwick"], "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Friedmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Friedmann et al\\.", "year": 2011}, {"title": "Dynamic programming and Markov processes", "author": ["Ronald A. Howard"], "venue": "The MIT press,", "citeRegEx": "Howard.,? \\Q1960\\E", "shortCiteRegEx": "Howard.", "year": 1960}, {"title": "Lower bounds for Howards algorithm for finding minimum mean-cost cycles", "author": ["Thomas Hansen", "Uri Zwick"], "venue": "Algorithms and Computation,", "citeRegEx": "Hansen and Zwick.,? \\Q2010\\E", "shortCiteRegEx": "Hansen and Zwick.", "year": 2010}, {"title": "On the complexity of solving Markov decision problems", "author": ["Michael L Littman", "Thomas L Dean", "Leslie Pack Kaelbling"], "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "PAC bounds for discounted MDPs", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Lattimore and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2012}, {"title": "On the complexity of policy iteration", "author": ["Yishay Mansour", "Satinder Singh"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Mansour and Singh.,? \\Q1999\\E", "shortCiteRegEx": "Mansour and Singh.", "year": 1999}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["Shie Mannor", "John N Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "The complexity of Markov decision processes", "author": ["Christos H Papadimitriou", "John N Tsitsiklis"], "venue": "Mathematics of operations research,", "citeRegEx": "Papadimitriou and Tsitsiklis.,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis.", "year": 1987}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q2014\\E", "shortCiteRegEx": "Puterman.", "year": 2014}, {"title": "Fast learning requires good memory: A time-space lower bound for parity learning", "author": ["Ran Raz"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Raz.,? \\Q2016\\E", "shortCiteRegEx": "Raz.", "year": 2016}, {"title": "Improved and generalized upper bounds on the complexity of policy iteration", "author": ["Bruno Scherrer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Scherrer.,? \\Q2013\\E", "shortCiteRegEx": "Scherrer.", "year": 2013}, {"title": "Reinforcement learning in finite mdps: Pac analysis", "author": ["Alexander L Strehl", "Lihong Li", "Michael L Littman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Pac model-free reinforcement learning", "author": ["Alexander L Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L Littman"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Solving h-horizon, stationary markov decision problems in time proportional to log (h)", "author": ["Paul Tseng"], "venue": "Operations Research Letters,", "citeRegEx": "Tseng.,? \\Q1990\\E", "shortCiteRegEx": "Tseng.", "year": 1990}, {"title": "Randomized linear programming solves the discounted Markov decision problem in nearly-linear running time", "author": ["Mengdi Wang"], "venue": "arXiv preprint arXiv:1704.01869,", "citeRegEx": "Wang.,? \\Q2017\\E", "shortCiteRegEx": "Wang.", "year": 2017}, {"title": "An efficient method for weighted sampling without replacement", "author": ["Chak-Kuen Wong", "Malcolm C. Easton"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Wong and Easton.,? \\Q1980\\E", "shortCiteRegEx": "Wong and Easton.", "year": 1980}, {"title": "Probabilistic computations: Toward a unified measure of complexity", "author": ["Andrew Chi-Chin Yao"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Yao.,? \\Q1977\\E", "shortCiteRegEx": "Yao.", "year": 1977}, {"title": "Near-optimal time-space tradeoff for element distinctness", "author": ["Andrew Chi-Chih Yao"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Yao.,? \\Q1994\\E", "shortCiteRegEx": "Yao.", "year": 1994}, {"title": "A new complexity result on solving the Markov decision problem", "author": ["Yinyu Ye"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Ye.,? \\Q2005\\E", "shortCiteRegEx": "Ye.", "year": 2005}, {"title": "The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate", "author": ["Yinyu Ye"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Ye.,? \\Q2011\\E", "shortCiteRegEx": "Ye.", "year": 2011}], "referenceMentions": [], "year": 2017, "abstractText": "We study the computational complexity of the infinite-horizon discounted-reward Markov Decision Problem (MDP) with a finite state space S and a finite action space A. We show that any randomized algorithm needs a running time at least \u03a9(|S||A|) to compute an -optimal policy with high probability. We consider two variants of the MDP where the input is given in specific data structures, including arrays of cumulative probabilities and binary trees of transition probabilities. For these cases, we show that the complexity lower bound reduces to \u03a9 ( |S||A| ) . These results reveal a surprising observation that the computational complexity of the MDP depends on the data structure of input.", "creator": "LaTeX with hyperref package"}}}