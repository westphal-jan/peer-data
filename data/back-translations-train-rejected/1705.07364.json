{"id": "1705.07364", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Stabilizing Adversarial Nets With Prediction Methods", "abstract": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse\", and enables faster training with larger learning rates.", "histories": [["v1", "Sat, 20 May 2017 22:27:19 GMT  (17818kb,D)", "https://arxiv.org/abs/1705.07364v1", null], ["v2", "Fri, 9 Jun 2017 04:22:35 GMT  (19105kb,D)", "http://arxiv.org/abs/1705.07364v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NA", "authors": ["abhay yadav", "sohil shah", "zheng xu", "david jacobs", "tom goldstein"], "accepted": false, "id": "1705.07364"}, "pdf": {"name": "1705.07364.pdf", "metadata": {"source": "CRF", "title": "Stabilizing Adversarial Nets With Prediction Methods", "authors": ["Abhay Yadav", "Sohil Shah", "Zheng Xu", "David Jacobs"], "emails": ["tomg}@cs.umd.edu,", "sohilas@umd.edu,", "djacobs@umiacs.umd.edu"], "sections": [{"heading": null, "text": "Enemy neural networks solve many important data science problems, but are notoriously difficult to train. These difficulties stem from the fact that optimal weights for enemy networks correspond to saddle points rather than minimizers of the loss function. Alternate stochastic gradient methods typical of such problems do not converge reliably with saddle points, and when convergence occurs, they are often highly sensitive to learning rates. We propose a simple modification of stochastic gradient declines that stabilizes opposing networks. In theory as well as in practice, we show that the proposed method reliably converges with saddle points, making hostile networks less \"collapse\" and enabling faster training with higher learning rates."}, {"heading": "1 Introduction", "text": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transmission [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc. A particularly motivating use of adversarial networks is their ability to form generative models, as opposed to the classical discriminatory models [14, 32, 8, 28].While adversarial networks have the power to attack a wide range of previously unsolved problems, they suffer from a major flaw: they are difficult to train. This is because adversarial networks attempt to achieve two goals at the same time; weights are adjusted to the performance of one task while performance is minimized at another. Mathematically, this corresponds to determining one saddle of a loss function - a point that is minimal."}, {"heading": "2 Proposed Method", "text": "Most authors use the alternating stochastic gradient method to solve saddle point problems associated with neural networks. This method alternates between updating u with a stochastic gradient downward step and updating v with a stochastic gradient upward step. If simple / classic SGD updates are used, the steps of this method can be written from (uk + 1, vk). (2) Here, the gradient downward step is written into u, starting with (uk, vk) vk + 1 = vk \u00b2 v (uk + 1, vk) | Gradient upward step in v, starting with (uk + 1, vk) | Gradient downward step in u (uk, vk). (2) Here, {\u03b1k} and {\u03b2k} learn rate plans for minimizing and maximizing steps in uk (uk + 1, vk) and if necessary."}, {"heading": "3 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Adversarial Networks as a Saddle-Point Problem", "text": "Generative Adversarial Networks (GANs) adapt a generative model to a data set by using a game in which a generative model competes with a discriminator [14]. The generator, G (z; \u03b8g), takes random noise vectors z as inputs and maps them to points in the target data distribution. The discriminator, D (x; \u03b8d), accepts a candidate dot x and tries to determine whether it is really drawn from the empirical distribution (in which case it prints 1) or whether it is generated by the generator (output 0). During a training iteration, noise vectors from a Gaussian distribution G are pushed through the generator network G to form a batch of generated data patterns designated by Dfake. A batch of empirical patterns, Dreal, is also prepared."}, {"heading": "3.2 Stabilizing saddle point solvers", "text": "Changing stochastic gradient methods are known to be unstable when using simple logarithmic losses, which led researchers to identify several directions to stabilize GANs, either by adding regulatory terms [2, 23, 5, 45], replacing a variety of training problems [34, 15], renewing network architectures [45], and devising different approaches to solving them. Specifically, the Waterstone approach modified the original goal by replacing f (x) with f (x), resulting in a training scheme in which the weights of discrimination were \"truncated.\" However, as discussed in the discussion, WGAN training is unstable at high learning rates, or when used with popular solvers."}, {"heading": "4 Interpretations of the prediction step", "text": "We present three ways to explain the impact of predictions: an intuitive, non-mathematical perspective, a more analytical standpoint involving dynamic systems, and finally a rigorous evidence-based approach."}, {"heading": "4.1 An intuitive viewpoint", "text": "In this algorithm, there is a risk that the minimization step may override the maximization step, and in this case, the iteration will \"slip\" from the saddle edge, resulting in instability (Figure 1b). Conversely, an overwhelming maximization step will dominate the minimization step and also drive the iteration rate to extremes. The effect of the prediction is visualized in Figure 2. Suppose that a maximization step takes place from the red dot. Without prediction, the maximization step will be the same, regardless of whether the previous minimization update was weak (Figure 2a) or strong (Figure 2b). Prediction allows the maximization step to take advantage of information about the minimization step. If the previous minimization step was weak (Figure 2a), the prediction step (dotted black arrow) remains close to the surface when we reach a weak, white step to maximize."}, {"heading": "4.2 A more mathematical perspective", "text": "To get a stronger intuition about prediction methods, let us look at the behavior of the algorithm (3) on a simple bilinear saddle of FormL (u, v) = vTKu (6), where K is a matrix. If exact (non-stochastic) gradient updates are used, the iterates follow the path of a simple dynamic system with closed solutions. Here, we give a sketch of this argument: a detailed derivative is provided in the supplementary material. If the (non-stochastic) gradient method (2) is applied to the linear problem (6), the resulting iterations can be written + 1 \u2212 uk \u03b1 = \u2212 KT vk, v + 1 \u2212 vk \u03b1 = (\u03b2 / \u03b1) Kuk + 1. If the step size \u03b1 becomes small, this behaves like a discreditization of the system of differential equations (6), where KT = \u2212 ampel = \u2212 step = \u2212 vk, \u2212 v = \u2212 \u03b1 and when the system is equal to K\u03b1 (\u2212 \u03b1)."}, {"heading": "4.3 A rigorous perspective", "text": "While the above arguments are intuitive, they are also informal and do not deal with problems such as stochastic gradients, non-constant sequences of increments and more complex loss functions. We now provide a strict convergence analysis that addresses these problems. We assume that the function L (u, v) is convex in u and concave in v. We can then measure convergence using the \"primary-dual\" gap, P (u, v) = L (u, v) \u2212 L (u?, v), where (u?, v) is a saddle. On the basis of these definitions, we formulate the following convergence result. The proof lies in the complementary material. Theorem 1. Let us replace the function L (u, v) = convex in u, v \u2212 l if (u, v) is a saddle. On the basis of these definitions, we formulate the following convergence result."}, {"heading": "5 Experiments", "text": "We will present a wide range of experiments to demonstrate the benefits of the proposed predictive step for enemy nets. We will look at a saddle point problem on a toy dataset created using MNIST images, and then proceed to look at state-of-the-art models for three tasks: GANs, domain matching, and learning fair classifiers."}, {"heading": "5.1 MNIST Toy problem", "text": "To make the problem interesting, we note that the network encodes and uses information about noise; when a noise-and-no-noise classifier is trained to the low properties produced by LeNet, it gets 100% accuracy. The goal of this task is to force LeNet to ignore noise in decision making. We create a contradictory model of form (5) in which Ly is a soft maximum loss for even the odd classifier. We make Ld a soft maximum loss for the task of distinguishing whether the input sample is loud or not. The classifier and discriminator are both pre-trained by using the default LeNet implementation in Caffe [18]."}, {"heading": "5.2 Generative Adversarial Networks", "text": "Next, we will test the effectiveness and stability of our proposed predictive step for generative adversarial networks (GAN), which are formulated as saddle point problems (4) and are popularly solved using a heuristic approach [14]. We will consider an image modeling task using CIFAR-10 [20] on the recently popular revolutionary GAN architecture, DCGAN [32]. We will compare our predictive method with that of DCGAN and the unrolled GAN [27] using the training protocol described in [32]. Note that we will proceed against the unrolled GAN with stop switches and K = 5 unrolling steps. All approaches have been trained for random seeds and 100 epochs. We will start by comparing all three methods that use the standard solver for DCGAN optimizers (the Adam Optimizer) with the learning rate."}, {"heading": "5.3 Domain Adaptation", "text": "We look at the domain fit task [33, 12, 38], in which the representation learned from the source domain samples is altered so that it can be generalized to target distribution samples. We use the problem definition and hyperparameters described in [12] using the OFFICE dataset [33] (experimental details are shared in the supplementary material).Table 1 compares target domain accuracy for six pairs of source and target domain tasks. We observe that the prediction step has slight advantages in the \"simple\" adaptation tasks with very similar sources2We found the unrolled GAN with no stop gradient switch and smaller values of K when used on the DCGAN architecture and target domain samples. However, in the transfer learning tasks of AMAZON-to-WEBCAM, WEBCAM-to-AMAZON and DSLR-AAM1.3%, the improvement is too pronounced."}, {"heading": "5.4 Fair Classifier", "text": "Finally, we consider a task of learning fair attribute representations [26, 10, 24] in such a way that the final learned classifier does not discriminate in relation to a sensitive variable. As proposed in [10], one method of measuring fairness is the use of discrimination. (8) Here, si is a binary sensitive variable for the ith data sample and Nk is the total number of samples belonging to the kth-sensitive class. Similar to the domain fitting task, learning each classifier can be formulated as a minimax problem in (5) [10, 26]. Unlike the previous example, this task has a model selection component. From a pool of hundreds of randomly generated adversarially deep networks, for each value of t, one selects the model that maximizes the different UCC models."}, {"heading": "6 Conclusion", "text": "We present a simple modification of the alternative SGD method, called the prediction step, which improves the stability of opposing networks. We present theoretical results, which show that the prediction step for solving saddle point problems is asymptotically stable. We show using a variety of test problems, that prediction steps prevent the collapse of the network and allow a training with higher learning rates than pure SGD methods."}, {"heading": "A Detailed derivation of the harmonic oscillator equation", "text": "Here we provide a detailed derivation of the harmonic oscillator behavior from algorithm (3) to the simple bi-linear saddle of formula L (x, y) = yTKx, where K is a matrix. Note that within a small neighborhood of a saddle all smooth, weakly convex functions behave like (6). To see why, we consider a smooth objective function L with a saddle point at x-0, y-1 = 0. Within a small neighborhood of the saddle we can use the function L with high accuracy using its Taylor approximationL (x, y). L (x, y)."}, {"heading": "A Proof of Theorem 1", "text": "Assuming that the optimal solution (u? v? v? v) exists, then L? k (u? v) = L? v (u, v? v) = L? v? v v? v? v v? v? v? v? v v? v v? v v? v v v v? v v v v v? v v v v v? v v v v v? v v v v v? v v v v v? v v v v? v v v? v v v v? v v v v v v? v v v v v? v v v v v? v v v v v? v v v v? v v v v? v v v v? v v v v? v v v v v? v? v? v v v? v? v v v? v v? v v? v v? v v? v v? v v? v? v v? v? v v? v? v? v v? v? v? v? v v? v? v v? v? v v? v? v? v v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v? v?? v? v? v?? v? v? v? v? v??? v? v? v? v? v???? v? v? v? v? v?"}, {"heading": "B MNIST Toy example", "text": "Experimental Details: We are looking at a classic MNIST dataset [22] consisting of 60,000 training images and 10,000 test images of size 28 x 28. For the sake of simplicity, we are looking at a task (T1) of classification into odd and even numbered images. Suppose 50% of the data instances were corrupted with salt and pepper noise of probability 0.2 and this distortion process was distorted. Specifically, only 30% of even images were distorted against the 70% of odd images. We have observed that any functional representation network trained with the binary classification function for the loss of task T1 is also afflicted with noise distortion. This has been confirmed by the formation of an independent noise classifier on the learned features. This led us to design a simple adversarial network to learn the noise distortions from the function. We formulate this with the Minimax target in (5).In our Ld model, the loss is a soft task."}, {"heading": "C Domain Adaptation", "text": "Experimental Details: To evaluate the task of domain matching, consider the OFFICE dataset [33]. OFFICE is a small dataset consisting of images from three different domains: AMAZON, DSLR, and WEBCAM. For such a small dataset, it is not trivial to learn features from images from a single domain. For example, consider the largest subset of AMAZON, which contains only 2,817 labeled images distributed across 31 different categories. However, the power of domain matching can be used to improve cross-domain accuracy. We follow the protocol listed in [12] and use the same network architecture. Caffe [18] is used for implementation. The training procedure in [12] is kept intact, except for the additional prediction step. Table 1 reports comparisons in terms of target domain accuracy for three pairs of source and target domain assignments at the end of 50,000."}, {"heading": "D Fair Classifier", "text": "Experimental Details: Using the \"adult\" data set from the UCI Machine Learning Repository, which consists of census data from 450,000 people, the task is to classify whether a person earns \u2265 $50,000 per year. As a sensitive variable, the gender of the person is selected. We binarize all category attributes to obtain a total of 102 input traits per sample. We randomly divide the data into 35,000 samples for training, 5000 for validation, and 5000 for testing. The results shown here are an average of five such random splits."}, {"heading": "E Generative Adversarial Networks", "text": "Architecture Details: The DCGAN architecture used in our experiments with Cifar-10 is listed in Table 2, 3.Toy Dataset: To illustrate the advantage of the prediction method, we are experimenting with a simple organ architecture with fully connected layers using the toy dataset. The constructed toy example and its architecture are inspired by the ones shown in [27]. The two-dimensional data is sampled from a mixture of eight Gaussians with their means, which are evenly centered around the unit circle (0, 0). The standard deviation of each Gaussian is 0.01. The two-dimensional latent vector z is sampled from the multivariate Gaussian. The generator and discriminator network consists of two completely hidden layers, each with 128 hidden units and tanh activations. The last layer of the generator has a linear activation, while that of the discriminator has a sigmoid activation of the object network."}], "references": [{"title": "Learning to protect communications with adversarial neural cryptography", "author": ["Mart\u00edn Abadi", "David G Andersen"], "venue": "arXiv preprint arXiv:1610.06918,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural photo editing with introspective adversarial networks", "author": ["Andrew Brock", "Theodore Lim", "JM Ritchie", "Nick Weston"], "venue": "arXiv preprint arXiv:1609.07093,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["Antonin Chambolle", "Thomas Pock"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Mode regularized generative adversarial networks", "author": ["Tong Che", "Yanran Li", "Athul Paul Jacob", "Yoshua Bengio", "Wenjie Li"], "venue": "arXiv preprint arXiv:1612.02136,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Optimal primal-dual methods for a class of saddle point problems", "author": ["Yunmei Chen", "Guanghui Lan", "Yuyuan Ouyang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Randomized first-order methods for saddle point optimization", "author": ["Cong Dang", "Guanghui Lan"], "venue": "arXiv preprint arXiv:1409.8625,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Stochastic variance reduction methods for policy evaluation", "author": ["Simon S Du", "Jianshu Chen", "Lihong Li", "Lin Xiao", "Dengyong Zhou"], "venue": "arXiv preprint arXiv:1702.07944,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Censoring representations with an adversary", "author": ["Harrison Edwards", "Amos Storkey"], "venue": "arXiv preprint arXiv:1511.05897,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A general framework for a class of first order primal-dual algorithms for tv minimization", "author": ["Ernie Esser", "Xiaoqun Zhang", "Tony Chan"], "venue": "UCLA CAM Report,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Adaptive primal-dual splitting methods for statistical learning and image processing", "author": ["Tom Goldstein", "Min Li", "Xiaoming Yuan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Improved training of wasserstein gans", "author": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville"], "venue": "arXiv preprint arXiv:1704.00028,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Model-free imitation learning with policy optimization", "author": ["Jonathan Ho", "Jayesh Gupta", "Stefano Ermon"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1611.07004,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "An optimal randomized incremental gradient method", "author": ["Guanghui Lan", "Yi Zhou"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard S Zemel"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The variational fair autoencoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel"], "venue": "arXiv preprint arXiv:1511.00830,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Disentangling factors of variation in deep representation using adversarial training", "author": ["Michael F Mathieu", "Junbo Jake Zhao", "Junbo Zhao", "Aditya Ramesh", "Pablo Sprechmann", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Unrolled generative adversarial networks", "author": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1611.02163,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on optimization,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Stochastic variance reduction methods for saddle-point problems", "author": ["Balamurugan Palaniappan", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "On stochastic primal-dual hybrid gradient approach for compositely regularized minimization", "author": ["Linbo Qiao", "Tianyi Lin", "Yu-Gang Jiang", "Fan Yang", "Wei Liu", "Xicheng Lu"], "venue": "In ECAI 2016: 22nd European Conference on Artificial Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Adapting visual category models to new domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Stochastic primal dual coordinate method with non-uniform sampling based on optimality violations", "author": ["Atsushi Shibagaki", "Ichiro Takeuchi"], "venue": "arXiv preprint arXiv:1703.07056,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Learning from simulated and unsupervised images through adversarial training", "author": ["Ashish Shrivastava", "Tomas Pfister", "Oncel Tuzel", "Josh Susskind", "Wenda Wang", "Russ Webb"], "venue": "arXiv preprint arXiv:1612.07828,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Unsupervised cross-domain image generation", "author": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "venue": "arXiv preprint arXiv:1611.02200,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Adversarial discriminative domain adaptation", "author": ["Eric Tzeng", "Judy Hoffman", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1702.05464,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Exploiting strong convexity from data with primal-dual first-order algorithms", "author": ["Jialei Wang", "Lin Xiao"], "venue": "arXiv preprint arXiv:1703.02624,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "An online primal-dual method for discounted markov decision processes", "author": ["Mengdi Wang", "Yichen Chen"], "venue": "In Decision and Control (CDC),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Doubly stochastic primal-dual coordinate method for empirical risk minimization and bilinear saddle-point problem", "author": ["Adams Wei Yu", "Qihang Lin", "Tianbao Yang"], "venue": "arXiv preprint arXiv:1508.03390,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "author": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaolei Huang", "Xiaogang Wang", "Dimitris Metaxas"], "venue": "arXiv preprint arXiv:1612.03242,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization", "author": ["Yuchen Zhang", "Xiao Lin"], "venue": "In ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "An efficient primal-dual hybrid gradient algorithm for total variation image restoration", "author": ["Mingqiang Zhu", "Tony Chan"], "venue": "UCLA CAM Report,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Adaptive stochastic primal-dual coordinate descent for separable saddle point problems", "author": ["Zhanxing Zhu", "Amos J Storkey"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Stochastic parallel block coordinate descent for large-scale saddle point problems", "author": ["Zhanxing Zhu", "Amos J Storkey"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}], "referenceMentions": [{"referenceID": 40, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 101, "endOffset": 109}, {"referenceID": 38, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 101, "endOffset": 109}, {"referenceID": 1, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 34, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 38, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 15, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 34, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 35, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 10, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 33, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 14, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 198, "endOffset": 202}, {"referenceID": 8, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 212, "endOffset": 219}, {"referenceID": 0, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 212, "endOffset": 219}, {"referenceID": 23, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 241, "endOffset": 249}, {"referenceID": 8, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 241, "endOffset": 249}, {"referenceID": 12, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 29, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 6, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 25, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 12, "context": "As a result, many authors suggest using early stopping, gradients/weight clipping [2], or specialized objective functions [14, 45, 2] to maintain stability.", "startOffset": 122, "endOffset": 133}, {"referenceID": 42, "context": "As a result, many authors suggest using early stopping, gradients/weight clipping [2], or specialized objective functions [14, 45, 2] to maintain stability.", "startOffset": 122, "endOffset": 133}, {"referenceID": 17, "context": "In practice, the gradient updates are often performed by an automated solver, such as the Adam optimizer [19], and include momentum updates.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Generative Adversarial Networks (GANs) fit a generative model to a dataset using a game in which a generative model competes against a discriminator [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "Initially, [14] proposed using f(x) = log(x).", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "Domain Adversarial Networks (DANs) [25, 12, 10] take data collected from a \u201csource\u201d domain, and extract a feature representation that can be used to train models that generalize to another \u201ctarget\u201d domain.", "startOffset": 35, "endOffset": 47}, {"referenceID": 8, "context": "Domain Adversarial Networks (DANs) [25, 12, 10] take data collected from a \u201csource\u201d domain, and extract a feature representation that can be used to train models that generalize to another \u201ctarget\u201d domain.", "startOffset": 35, "endOffset": 47}, {"referenceID": 10, "context": "For example, in the domain adversarial neural network (DANN [12]), a set of feature layers maps data points into an embedded feature space, and a classifier is trained on these embedded features.", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 112, "endOffset": 126}, {"referenceID": 3, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 112, "endOffset": 126}, {"referenceID": 42, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 112, "endOffset": 126}, {"referenceID": 31, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 157, "endOffset": 165}, {"referenceID": 13, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 157, "endOffset": 165}, {"referenceID": 42, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 24, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 242, "endOffset": 246}, {"referenceID": 24, "context": "The unrolled GAN [27] is a new solver that can stabilize training at the cost of more expensive gradient computations.", "startOffset": 17, "endOffset": 21}, {"referenceID": 43, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 68, "endOffset": 76}, {"referenceID": 9, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 68, "endOffset": 76}, {"referenceID": 2, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 233, "endOffset": 237}, {"referenceID": 5, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 19, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 41, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 44, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 45, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 36, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 32, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 4, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 186, "endOffset": 193}, {"referenceID": 28, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 186, "endOffset": 193}, {"referenceID": 37, "context": "Similar optimization algorithms have been studied for reinforcement learning [40, 9].", "startOffset": 77, "endOffset": 84}, {"referenceID": 7, "context": "Similar optimization algorithms have been studied for reinforcement learning [40, 9].", "startOffset": 77, "endOffset": 84}, {"referenceID": 39, "context": "Recently, a \u201cdoubly\u201d stochastic method that randomizes both primal and dual updates was proposed for strongly convex bilinear saddle point problems [42].", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "For general saddle point problems, \u201cdoubly\u201d stochastic gradient descent methods are discussed in [29, 30], in which primal and dual variables are updated simultaneously based on the previous iterates and the current gradients.", "startOffset": 97, "endOffset": 105}, {"referenceID": 27, "context": "For general saddle point problems, \u201cdoubly\u201d stochastic gradient descent methods are discussed in [29, 30], in which primal and dual variables are updated simultaneously based on the previous iterates and the current gradients.", "startOffset": 97, "endOffset": 105}, {"referenceID": 20, "context": "When we train a LeNet network [22] on this problem, we find that the network encodes and uses information about the noise; when a noise vs no-noise classifier is trained on the deep features generated by LeNet, it gets 100% accuracy.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "The classifier and discriminator were both pre-trained using the default LeNet implementation in Caffe [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "2 Generative Adversarial Networks Next, we test the efficacy and stability of our proposed predictive step on generative adversarial networks (GAN), which are formulated as saddle point problems (4) and are popularly solved using a heuristic approach [14].", "startOffset": 251, "endOffset": 255}, {"referenceID": 18, "context": "We consider an image modeling task using CIFAR-10 [20] on the recently popular convolutional GAN architecture, DCGAN [32].", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "We consider an image modeling task using CIFAR-10 [20] on the recently popular convolutional GAN architecture, DCGAN [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "We compare our predictive method with that of DCGAN and the unrolled GAN [27] using the training protocol described in [32].", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "We compare our predictive method with that of DCGAN and the unrolled GAN [27] using the training protocol described in [32].", "startOffset": 119, "endOffset": 123}, {"referenceID": 29, "context": "As observed in [32], the standard and unrolled solvers are very unstable and collapse at this higher rate.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "3 Domain Adaptation We consider the domain adaptation task [33, 12, 38] wherein the representation learned using the source domain samples is altered so that it can also generalize to samples from the target distribution.", "startOffset": 59, "endOffset": 71}, {"referenceID": 10, "context": "3 Domain Adaptation We consider the domain adaptation task [33, 12, 38] wherein the representation learned using the source domain samples is altered so that it can also generalize to samples from the target distribution.", "startOffset": 59, "endOffset": 71}, {"referenceID": 35, "context": "3 Domain Adaptation We consider the domain adaptation task [33, 12, 38] wherein the representation learned using the source domain samples is altered so that it can also generalize to samples from the target distribution.", "startOffset": 59, "endOffset": 71}, {"referenceID": 10, "context": "We use the problem setup and hyper-parameters as described in [12] using the OFFICE dataset [33] (experimental details are shared in the Supplementary Material).", "startOffset": 62, "endOffset": 66}, {"referenceID": 30, "context": "We use the problem setup and hyper-parameters as described in [12] using the OFFICE dataset [33] (experimental details are shared in the Supplementary Material).", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Method Source AMAZON WEBCAM DSLR WEBCAM AMAZON DSLR Target WEBCAM AMAZON WEBCAM DSLR DSLR AMAZON DANN [12] 73.", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "4 Fair Classifier Finally, we consider a task of learning fair feature representations [26, 10, 24] such that the final learned classifier does not discriminate with respect to a sensitive variable.", "startOffset": 87, "endOffset": 99}, {"referenceID": 8, "context": "4 Fair Classifier Finally, we consider a task of learning fair feature representations [26, 10, 24] such that the final learned classifier does not discriminate with respect to a sensitive variable.", "startOffset": 87, "endOffset": 99}, {"referenceID": 22, "context": "4 Fair Classifier Finally, we consider a task of learning fair feature representations [26, 10, 24] such that the final learned classifier does not discriminate with respect to a sensitive variable.", "startOffset": 87, "endOffset": 99}, {"referenceID": 8, "context": "As proposed in [10] one way to measure fairness is using discrimination,", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "Similar to the domain adaptation task, the learning of each classifier can be formulated as a minimax problem in (5) [10, 26].", "startOffset": 117, "endOffset": 125}, {"referenceID": 23, "context": "Similar to the domain adaptation task, the learning of each classifier can be formulated as a minimax problem in (5) [10, 26].", "startOffset": 117, "endOffset": 125}, {"referenceID": 8, "context": "To demonstrate the advantage of using prediction for model selection, we follow the protocol developed in [10].", "startOffset": 106, "endOffset": 110}], "year": 2017, "abstractText": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \u201ccollapse,\u201d and enables faster training with larger learning rates.", "creator": "TeX"}}}