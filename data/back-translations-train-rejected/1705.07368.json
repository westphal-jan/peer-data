{"id": "1705.07368", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Mixed Membership Word Embeddings for Computational Social Science", "abstract": "Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. These models have recently risen in popularity due to the performance of scalable algorithms trained in the big data setting. Despite their success, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage the notion of mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. Leveraging connections to topic models, I show how to train these models in high dimensions using a combination of state-of-the-art techniques for word embeddings and topic modeling. Experimental results show an improvement in predictive performance of up to 63% in MRR over the skip-gram on small datasets. The models are interpretable, as embeddings of topics are used to encode embeddings for words (and hence, documents) in a model-based way. I illustrate this with two computational social science case studies, on NIPS articles and State of the Union addresses.", "histories": [["v1", "Sat, 20 May 2017 23:45:54 GMT  (3297kb,D)", "http://arxiv.org/abs/1705.07368v1", null], ["v2", "Thu, 25 May 2017 03:12:35 GMT  (3300kb,D)", "http://arxiv.org/abs/1705.07368v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["james foulds"], "accepted": false, "id": "1705.07368"}, "pdf": {"name": "1705.07368.pdf", "metadata": {"source": "CRF", "title": "Mixed Membership Word Embeddings for Computational Social Science", "authors": ["James Foulds"], "emails": ["jfoulds@ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "It has been shown to be worthwhile for a variety of NLP tasks such as statistical machine translation (Vaswani et al., 2013), part of the language typically used for the success of NLP applications, as they allow for a more differentiated representation of words as a simple indicator vector in a dictionary. These models have become popular due to the success of models specifically designed for the big data setting. In particular, Mikolos et al. (2013a, b) showed that very simple word embedding models with high-dimensional representations become a dictionary, allowing them to process more complex neural network language models that have a fixed computational budget. In this work, I offer a somewhat contrasting perspective on the current trend of big data optimism, such as it is."}, {"heading": "2 Background", "text": "In this section, I provide the necessary background information on word embedding, as well as on topic models and mixed membership models. A more detailed discussion of related work is given in the appendix. (2003) Traditional language models aim to predict words based on the contexts in which they are found, thereby creating a common probabilistic model for word sequences in a language. (2003) The authors have found that these word embedding models are useful for semantic representations of words, regardless of whether a complete common probabilistic language model is learned, and that alternative training programs can be useful for learning the embedding. (2013a) The authors have proposed the model of language embedding, which promotes language prediction and aims to predict the context in which an alternative training program may be beneficial. (In particular Mikolov et al al al al al al al al al al al al al al al al al (2013a)"}, {"heading": "3 The Mixed Membership Skip-Gram", "text": "To design this model, we need to identify links between word embedding and theme models (2015), and we need to adapt advances from the topic modeling literature. Although the embedding model is discriminatory in the sense that it does not jointly model the input words, we can interpret it as a \"conditionally generative\" process for the context in which the words are used to develop likely models that expand the issue of distribution. Following the distribution hypothesis (Harris, 1954), we can interpret the word of embedding as a discrete probability distribution across words p (wc) that tend to occur together, and to be semantically coherent. A property model of the Gaussian LDA model by Das et al. (2015) By identifying these discrete distributions with topics (wi), we see that the model of embedding can be reinterpreted as the parameterization of a particular theme."}, {"heading": "4 Learning Mixed Membership Word Embeddings", "text": "To train the mixed membership skip program, an online EM algorithm with stochastic gradient M-step updates can be easily derived, similar to the multi-prototype embedding model developed by Tian et al. (2014), which has multiple vectors per word. However, this is impractical due to the aO (KD) complexity of the E-step and an O (D) complexity of the M-step, where K and D are the number of topics and dictionaries respectively. Instead, I propose a principled approximate algorithm that is sublinear in both K and D. The overall strategy is to implement the z-assignments first by simulated annealing using an efficient implementation of a collapsed Gibbs sampler, reducing the learning problem to standard word embedding. Considering the z's implied, we then learn the AP's maximum topic and the estimation by means of Word-V, which is approximate since 1986."}, {"heading": "4.1 Imputing the z\u2019s: Topic Model Pre-Clustering via Annealed Metropolis-Hastings-Walker", "text": "To develop such an algorithm, the key finding is that our mixed membership model (Table 2, bottom left) corresponds to the model version theme (Table 2, bottom right), right up to parameterization and the previous (if any) one. However, with sufficiently high dimensional embedding, the logbilinear model can capture any distribution (wc | zi), and thus the maximum probability of embedding would encode the exact word distribution during the e-step. I therefore propose to repair the mixed theme for the theme model (zi), but the theme model allows a collapsed Gibbs sampler (CGS) to efficiently resolve the cluster aspects underlying the bottleneck of the mixed membership skip as an associated theme for the purpose of implementing the z, and performs simulated annealing based on the collapsed Gibbs model."}, {"heading": "4.2 Learning the Embeddings: Noise-Contrastive Estimation", "text": "Finally, with the subject assignments implied and \u03b8 estimated over the subject model, we need to learn the embeddings, which are still an expensive O (D) per context over SGD for maximum probability estimation. This same complexity is also a problem for the standard skip-gram, the Mnih and Teh (2012); Mnih and Kavukcuoglu (2013) have come up with the noise-contrasting estimation (NCE) algorithm of Gutmann and Hyv\u00e4rinen (2010, 2012). NCE avoids the expensive normalization step, making the algorithm scale sublinear in word size D. The algorithm solves uncontrolled learning tasks by placing them in the monitored learning task of distinguishing the data from randomly sampled noise. Since the number of noise samples tends to infinity, the method is increasingly well approaching the maximum probability estimation, while explicitly calculating the normalization approximation of the probability we require or the likelihood of its approximation as a L."}, {"heading": "5 Experiments", "text": "The objectives of my research were to validate the proposed methods in order to investigate their applicability to sociological research, and in order to substantiate my claims, we must be able to understand the manner and manner in which they have been conducted in recent years. (...) We have been able to put ourselves in a position to put ourselves in a position to put ourselves in a position to put ourselves in a position to put ourselves in a position to put ourselves in a position to put ourselves in a position to put ourselves in a position to put ourselves in a position, to put us in a position to put ourselves in a position to put ourselves in a position to put S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S.: S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S.:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::"}, {"heading": "6 Conclusion", "text": "My approach uses mixed member representation, the Metropolis-Hastings-Walker algorithm, and noise-contrasting estimates. Real-world predictive and case studies of NIPS and the State of the Union suggest that the algorithm can achieve high-quality embedding and topics. I plan to use this approach for substantial social science applications and address algorithmic bias / fairness issues. I also plan to expand the methods for using large datasets along with a small target dataset."}, {"heading": "Acknowledgments", "text": "I thank Eric Nalisnick and Padhraic Smyth for many helpful discussions."}, {"heading": "A Related Work", "text": "In this appendix, I will discuss the related work in the literature and its relationship to the proposed methods to improve the three authors. In this appendix, I will discuss how the work in the literature and its relationship to the proposed methods based on the condition, not on the context, but on the context in which they move, but on the context in which they move. In this case, LDA modifies the generative process of LDA so that each topic is adopted to generate the vectors via its own Gaussian distribution. Similarly to our MMSG model, each topic is encoded with a vector, in this case, the meaning of the Gaussian model is modified so that each topic is used to generate the vectors via its own Gaussian distribution, rather than learning the embedding of data within the same model, and does not aim to perform word embedding."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "On the statistical analysis of dirty pictures", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 259\u2013302.", "citeRegEx": "Besag,? 1986", "shortCiteRegEx": "Besag", "year": 1986}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings", "author": ["T. Bolukbasi", "Chang", "K.-W.", "J.Y. Zou", "V. Saligrama", "A.T. Kalai"], "venue": "Advances in Neural Information Processing Systems 29, pages 4349\u20134357.", "citeRegEx": "Bolukbasi et al\\.,? 2016", "shortCiteRegEx": "Bolukbasi et al\\.", "year": 2016}, {"title": "An efficient, probabilistically sound algorithm for segmentation and word discovery", "author": ["M.R. Brent"], "venue": "Machine Learning, 34(1):71\u2013105.", "citeRegEx": "Brent,? 1999", "shortCiteRegEx": "Brent", "year": 1999}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Gaussian LDA for topic models with word embeddings", "author": ["R. Das", "M. Zaheer", "C. Dyer"], "venue": "ACL, pages 795\u2013804.", "citeRegEx": "Das et al\\.,? 2015", "shortCiteRegEx": "Das et al\\.", "year": 2015}, {"title": "Topics in semantic representation", "author": ["T.L. Griffiths", "M. Steyvers", "J.B. Tenenbaum"], "venue": "Psychological review, 114(2):211.", "citeRegEx": "Griffiths et al\\.,? 2007", "shortCiteRegEx": "Griffiths et al\\.", "year": 2007}, {"title": "A Bayesian hierarchical topic model for political texts: Measuring expressed agendas in senate press releases", "author": ["J. Grimmer"], "venue": "Political Analysis, pages 1\u201335.", "citeRegEx": "Grimmer,? 2010", "shortCiteRegEx": "Grimmer", "year": 2010}, {"title": "The Bayesian echo chamber: Modeling social influence via linguistic accommodation", "author": ["F. Guo", "C. Blundell", "H. Wallach", "K. Heller"], "venue": "Artificial Intelligence and Statistics, pages 315\u2013323.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "AISTATS.", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2010}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, 13(Feb):307\u2013361.", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? 2012", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2012}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation, 14(8):1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Distributed representations", "author": ["G.E. Hinton", "J.L. Mcclelland", "D.E. Rumelhart"], "venue": "Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, chapter 3, pages 77\u2013109. MIT Press, Cambridge, MA.", "citeRegEx": "Hinton et al\\.,? 1986", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873\u2013882. Association for Computational Linguistics.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Advances in neural information processing systems, pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Reducing the sampling complexity of topic models", "author": ["A.Q. Li", "A. Ahmed", "S. Ravi", "A.J. Smola"], "venue": "KDD, pages 891\u2013900. ACM.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Topical word embeddings", "author": ["Y. Liu", "Z. Liu", "Chua", "T.-S.", "M. Sun"], "venue": "AAAI, pages 2418\u20132424.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Visualizing data using t-SNE", "author": ["Maaten", "L. v. d.", "G. Hinton"], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten et al\\.,? 2008", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Computational historiography: Data mining in a century of classics journals", "author": ["D. Mimno"], "venue": "Journal on Computing and Cultural Heritage (JOCCH), 5(1):3.", "citeRegEx": "Mimno,? 2012", "shortCiteRegEx": "Mimno", "year": 2012}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems, pages 2265\u20132273.", "citeRegEx": "Mnih and Kavukcuoglu,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "Proceedings of the 29th International Conference on Machine Learning.", "citeRegEx": "Mnih and Teh,? 2012", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Modeling topic control to detect influence in conversations using nonparametric topic models", "author": ["Nguyen", "V.-A.", "J. Boyd-Graber", "P. Resnik", "D.A. Cai", "J.E. Midberry", "Y. Wang"], "venue": "Machine Learning, 95(3):381\u2013421.", "citeRegEx": "Nguyen et al\\.,? 2014", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["J. Reisinger", "R.J. Mooney"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109\u2013117. Association for Computational Linguistics.", "citeRegEx": "Reisinger and Mooney,? 2010", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["F. Tian", "H. Dai", "J. Bian", "B. Gao", "R. Zhang", "E. Chen", "Liu", "T.-Y."], "venue": "COLING, pages 151\u2013160.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP, pages 1387\u20131392.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 19\u201327.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "apply a similar approach, but they use initial single-prototype word embeddings to provide the features used for clustering. These clustering methods have some resemblance to our topic model pre-clustering step, although their clustering is applied within instances of a given word", "author": ["Huang"], "venue": null, "citeRegEx": "Huang,? \\Q2012\\E", "shortCiteRegEx": "Huang", "year": 2012}, {"title": "who treat the prototype assignment of a word as a latent variable, assumed drawn from a mixture over prototypes for each word. The embeddings are then trained using EM. Our MMSG model can be understood as the mixed membership version of this model, in which the prototypes (vectors) are shared across all word types, and each word type", "author": ["Tian"], "venue": null, "citeRegEx": "Tian,? \\Q2014\\E", "shortCiteRegEx": "Tian", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Word embedding models, which learn to encode dictionary words with vector space representations, have been shown to be valuable for a variety of NLP tasks such as statistical machine translation (Vaswani et al., 2013), part-of-speech tagging, chunking, and named entity recogition (Collobert et al.", "startOffset": 195, "endOffset": 217}, {"referenceID": 5, "context": ", 2013), part-of-speech tagging, chunking, and named entity recogition (Collobert et al., 2011), as they provide a more nuanced representation of words than a simple indicator vector into a dictionary.", "startOffset": 71, "endOffset": 95}, {"referenceID": 8, "context": "It should be noted that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al.", "startOffset": 282, "endOffset": 297}, {"referenceID": 22, "context": "It should be noted that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al.", "startOffset": 317, "endOffset": 330}, {"referenceID": 30, "context": "It should be noted that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al., 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al.", "startOffset": 338, "endOffset": 356}, {"referenceID": 4, "context": ", 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015).", "startOffset": 44, "endOffset": 96}, {"referenceID": 25, "context": ", 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015).", "startOffset": 44, "endOffset": 96}, {"referenceID": 9, "context": ", 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015).", "startOffset": 44, "endOffset": 96}, {"referenceID": 4, "context": ", 2013), part-of-speech tagging, chunking, and named entity recogition (Collobert et al., 2011), as they provide a more nuanced representation of words than a simple indicator vector into a dictionary. These models have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, Mikolov et al. (2013a,b) showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents given a fixed computational budget. In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of Collobert et al. (2011), Mikolov et al.", "startOffset": 72, "endOffset": 810}, {"referenceID": 5, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 20, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 26, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 16, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 3, "context": "Even more concerningly, Bolukbasi et al. (2016) have recently shown that word embeddings from standard corpora can encode sexist assumptions implicit in a dataset, such as the analogy \u201cman is to computer programmer as woman is to homemaker.", "startOffset": 24, "endOffset": 48}, {"referenceID": 14, "context": "(2003) developed improved language models by using distributed representations (Hinton et al., 1986), in which words are represented by neural network synapse weights, or equivalently, vector space embeddings.", "startOffset": 79, "endOffset": 100}, {"referenceID": 0, "context": "Bengio et al. (2003) developed improved language models by using distributed representations (Hinton et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Topic models such as latent Dirichlet allocation (LDA) (Blei et al., 2003) are another class of probabilistic language models that have been used for semantic representation (Griffiths et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 7, "context": ", 2003) are another class of probabilistic language models that have been used for semantic representation (Griffiths et al., 2007).", "startOffset": 107, "endOffset": 131}, {"referenceID": 6, "context": "Some recent papers have aimed to combine topic models and word embeddings (Das et al., 2015; Liu et al., 2015), but they do not aim to address the small data problem for computational social science, which I focus on here.", "startOffset": 74, "endOffset": 110}, {"referenceID": 18, "context": "Some recent papers have aimed to combine topic models and word embeddings (Das et al., 2015; Liu et al., 2015), but they do not aim to address the small data problem for computational social science, which I focus on here.", "startOffset": 74, "endOffset": 110}, {"referenceID": 12, "context": "Following the distributional hypothesis (Harris, 1954), the skip-gram\u2019s word embeddings parameterize discrete probability distributions over words p(wc|wi) which tend to co-occur, and tend to be semantically coherent \u2013 a property leveraged by the Gaussian LDA model of Das et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 6, "context": "Following the distributional hypothesis (Harris, 1954), the skip-gram\u2019s word embeddings parameterize discrete probability distributions over words p(wc|wi) which tend to co-occur, and tend to be semantically coherent \u2013 a property leveraged by the Gaussian LDA model of Das et al. (2015). By identifying these discrete distributions with topics \u03c6i, we see that the skip-gram can be reinterpreted as a parameterization of a certain supervised naive Bayes topic model (Table 2, top-right).", "startOffset": 269, "endOffset": 287}, {"referenceID": 1, "context": "The overall algorithm can be understood as approximately finding a posterior mode, arg maxPr(V,V\u2032, \u03b8, z|w,wc), in the spirit of iterated conditional models (Besag, 1986), but with approximate updates and with iteration avoided, since z can be estimated without the vectors.", "startOffset": 156, "endOffset": 169}, {"referenceID": 27, "context": "To train the mixed membership skip-gram, an online EM algorithm with stochastic gradient M-step updates can readily be derived, similar to that of Tian et al. (2014)\u2019s multi-prototype embedding model, which has multiple vectors per word.", "startOffset": 147, "endOffset": 166}, {"referenceID": 25, "context": "This topic model pre-clustering step is reminiscent of the work of Reisinger and Mooney (2010); Huang et al.", "startOffset": 67, "endOffset": 95}, {"referenceID": 15, "context": "This topic model pre-clustering step is reminiscent of the work of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 15, "context": "This topic model pre-clustering step is reminiscent of the work of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al. (2015), who apply an off-the-shelf clustering algorithm (or LDA) to initially identify different clusters of contexts, and then apply word embedding algorithms on the cluster assignments.", "startOffset": 96, "endOffset": 135}, {"referenceID": 17, "context": "We scale this algorithm up to thousands of topics using an adapted version of the recently proposed Metropolis-Hastings-Walker algorithm for high-dimensional topic models, which scales sublinearly in K (Li et al., 2014).", "startOffset": 202, "endOffset": 219}, {"referenceID": 13, "context": "We can interpret the product over the context, which dominates the collapsed Gibbs update, as a product of experts (Hinton, 2002), where each word in the context is an \u201cexpert\u201d which weighs in multiplicatively on the update.", "startOffset": 115, "endOffset": 129}, {"referenceID": 17, "context": "The proposal is implemented efficiently by sampling from the experts via the alias table data structure, in amortized O(1) time, rather than in time linear in the sparsity pattern, as in (Li et al., 2014), since the proposal does not involve the sparse term (which is less important in our case).", "startOffset": 187, "endOffset": 204}, {"referenceID": 21, "context": "This same complexity is also an issue for the standard skip-gram, which Mnih and Teh (2012); Mnih and Kavukcuoglu (2013) have addressed using the noise-contrastive estimation (NCE) algorithm of Gutmann and Hyv\u00e4rinen (2010, 2012).", "startOffset": 72, "endOffset": 92}, {"referenceID": 21, "context": "This same complexity is also an issue for the standard skip-gram, which Mnih and Teh (2012); Mnih and Kavukcuoglu (2013) have addressed using the noise-contrastive estimation (NCE) algorithm of Gutmann and Hyv\u00e4rinen (2010, 2012).", "startOffset": 93, "endOffset": 121}, {"referenceID": 24, "context": "Following Mnih and Teh (2012), we fix a = 0, under the supposition that the NCE procedure will compensate for this by encouraging the distributions to \u201cself-normalize\u201d in order to optimize the NCE objective.", "startOffset": 10, "endOffset": 30}, {"referenceID": 13, "context": "I obtained document embeddings, in the same latent space as the topic embeddings, by summing the posterior mean vectors v\u0302wi for each token (and similarly for author embeddings), and visualized them in two dimensions using t-SNE (Maaten and Hinton, 2008) (all vectors were normalized to unit length). The state of the Union addresses (Figure 1) are embedded almost linearly by year, with a major jump around the New Deal (1930s), and are well separated by party at any given time period.", "startOffset": 241, "endOffset": 429}], "year": 2017, "abstractText": "Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. These models have recently risen in popularity due to the performance of scalable algorithms trained in the big data setting. Despite their success, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage the notion of mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. Leveraging connections to topic models, I show how to train these models in high dimensions using a combination of state-of-the-art techniques for word embeddings and topic modeling. Experimental results show an improvement in predictive performance of up to 63% in MRR over the skip-gram on small datasets. The models are interpretable, as embeddings of topics are used to encode embeddings for words (and hence, documents) in a model-based way. I illustrate this with two computational social science case studies, on NIPS articles and State of the Union addresses.", "creator": "LaTeX with hyperref package"}}}