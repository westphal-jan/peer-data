{"id": "1705.07371", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Spelling Correction as a Foreign Language", "abstract": "In this paper, we reformulated the spell correction problem as a machine translation task under the encoder-decoder framework. This reformulation enabled us to use a single model for solving the problem that is traditionally formulated as learning a language model and an error model. This model employs multi-layer recurrent neural networks as an encoder and a decoder. We demonstrate the effectiveness of this model using an internal dataset, where the training data is automatically obtained from user logs. The model offers competitive performance as compared to the state of the art methods but does not require any feature engineering nor hand tuning between models.", "histories": [["v1", "Sun, 21 May 2017 00:14:07 GMT  (24kb,D)", "http://arxiv.org/abs/1705.07371v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yingbo zhou", "utkarsh porwal", "roberto konow"], "accepted": false, "id": "1705.07371"}, "pdf": {"name": "1705.07371.pdf", "metadata": {"source": "CRF", "title": "Spelling Correction as a Foreign Language", "authors": ["Yingbo Zhou", "Utkarsh Porwal", "Roberto Konow"], "emails": ["yingbzhou@ebay.com", "uporwal@ebay.com", "rkonow@ebay.com"], "sections": [{"heading": "1 Introduction", "text": "Correct spell-checking not only reduces the mental burden on the user for the task, but also improves the quality of the service by trying to predict the intention of the user. Traditionally, the spelling problem was mostly addressed by using the loud channel model Kernigan et al. (1990). The model consists of two parts: 1) a language model (or source model, i.e. P (x))), which represents the previous probability of the intended correct input text; and 2) an error model (or channel model, i.e. P (x), which represents the process in which the correct input text becomes an incorrect text, and 2) an error model (or channel model, i.e. P (x), which represents the process in which the correct text becomes an incorrect problem."}, {"heading": "2 Background and Preliminaries", "text": "The recurrent neural network (RNN) is a natural extension of the feed-forward neural network = from this neural network the ability to model sequential data. Formally, let (x1, x2,..., xT), xt - Rd will be the input, an RNN will update its internal recursive hidden states by performing the following calculation: ht (ht \u2212 1, xt) (1) ar Xiv: 170 5.07 371v 1 [cs.C L] 21 May 201 7, where it is a non-linear function. Traditionally, in a standard RNN the term is implemented as affine transformation, followed by a pointed nonlinearity, such as asht = expanded (ht \u2212 1, xt) = tanh (Wxt + Uht \u2212 1 + bh). In addition, the RNN may also have outputs (y1, y2, y2,., yT), yt and yt - function that can not be calculated by another non-linear use."}, {"heading": "3 Spelling Correction as a Foreign Language", "text": "It is easy to see that spell-checking problem can be formulated as a sequence to a sequence learning problem, as in Section 2. In this sense, it is very similar to a machine translation problem where the input includes the misspelled text and the output of the correct letter combinations. A challenge to this formulation is that unlike the machine translation problem, the vocabulary is large but still limited. However, the input vocabulary is potentially limitless, which precludes the possibility of applying word-based coding to this problem. Furthermore, the large output vocabulary is a general challenge in neural translation models due to the large Softmax output matrix.The input vocabulary problem can be solved by using a character-based coding scheme. Although it seems appropriate for encoding the input, this scheme represents an unnecessary burden on the decoder as it needs to be a way of correcting the word."}, {"heading": "4 Experiments", "text": "Unlike machine translation problems, there are no public spelling records for e-commerce, so we collect both training and rating data internally. For training data, we use the event logs that track user behavior on an e-commerce site. Our heuristics for spotting potential spelling errors are based on consecutive user actions in a search session. The hypothesis is that users will try to change the search query until the search result is desirable with the search intention, and from this sequence of actions we can potentially extract the spelling errors and the correct query pair. Obviously, this involves a greater variety of query activities in addition to spelling errors, and therefore additional filtering is needed to obtain representative spelling data. We use the same techniques as Hasan et al.Hasan et al. (2015). The filtering of multiple months of spelling data from our spelling stock results in approximately 70 months of spelling data pairs."}, {"heading": "5 Conclusion", "text": "In this thesis, we have reformulated the problem of spell-checking as a machine translation task within the encoder decoder framework. Reformulation enabled us to use a single model to solve the problem and can be trained from end to end. We demonstrate the effectiveness of this model using an internal dataset in which the training data is automatically extracted from user logs. Despite the simplicity of the model, it has performed competitively compared to the state of the art, which requires a lot of feature engineering and human intervention."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Spelling Correction of User Search Queries through Statistical Machine Translation", "author": ["Hasan", "Sasa", "Heger", "Carmen", "Mansour", "Saab"], "venue": null, "citeRegEx": "Hasan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasan et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "A spelling correction program based on a noisy channel model. Pages 205\u2013210", "author": ["Kernighan", "Mark D", "Church", "Kenneth W", "Gale", "William A"], "venue": "of: Proceedings of the 13th conference on Computational linguistics-Volume", "citeRegEx": "Kernighan et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kernighan et al\\.", "year": 1990}, {"title": "Neural machine translation of rare words with subword units", "author": ["Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra"], "venue": "arXiv preprint arXiv:1508.07909", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. Pages 3104\u20133112 of: Advances in neural information processing systems", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Traditionally, spelling correction problem has been mostly approached by using the noisy channel model Kernighan et al. (1990). The model consists of two parts: 1) a language model (or source model, i.", "startOffset": 103, "endOffset": 127}, {"referenceID": 5, "context": "Traditionally, spelling correction problem has been mostly approached by using the noisy channel model Kernighan et al. (1990). The model consists of two parts: 1) a language model (or source model, i.e. P (x)) that represent the prior probability of the intended correct input text; and 2) an error model (or channel model, i.e. P (x\u0303|x)) that represent the process, in which the correct input text got corrupted to an incorrect misspelled text. The final correction is therefore obtained by using the Bayes rule, i.e. x\u2217 = argmaxx P (x)P (x\u0303|x). There are several problem with this approach: 1) we need two separate models and the error in estimating one model would affect the performance of the final output. 2) It is not easy to model the channel since there is a lot of sources for spelling mistakes, e.g. typing too fast, unintentional key stroke, phonetic ambiguity, etc. 3) In certain context (e.g. in a search engine) it is not easy to obtain clean training data for language model as the input does not follow what is typical in natural language. Since the goal is to get text that maximize P (x|x\u0303), can we directly model this conditional distribution instead? In this work, we explore this route, which by passes the need to have multiple models and avoid getting errors from multiple sources. We achieve this by applying the sequence to sequence learning framework using recurrent neural networks Sutskever et al. (2014) and reformulate the spelling correction problem as a neural machine translation problem, where the misspelled input is treated as a foreign language.", "startOffset": 103, "endOffset": 1435}, {"referenceID": 1, "context": "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al.", "startOffset": 95, "endOffset": 116}, {"referenceID": 1, "context": "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al. (2001), and therefore in practice more sophisticated function \u03c8 are often used to alleviate this problem.", "startOffset": 95, "endOffset": 142}, {"referenceID": 1, "context": "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al. (2001), and therefore in practice more sophisticated function \u03c8 are often used to alleviate this problem. For example the long short term memory (LSTM) Hochreiter & Schmidhuber (1997) is one widely used recursive unit that is designed to learn long term dependencies.", "startOffset": 95, "endOffset": 319}, {"referenceID": 1, "context": "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al. (2001), and therefore in practice more sophisticated function \u03c8 are often used to alleviate this problem. For example the long short term memory (LSTM) Hochreiter & Schmidhuber (1997) is one widely used recursive unit that is designed to learn long term dependencies. A layer LSTM consists of three gates and one memory cell, the computation of LSTM is as following1: it = \u03c3(Wixt + Uiht\u22121 + bi) (2) ot = \u03c3(Woxt + Uoht\u22121 + bo) (3) ft = \u03c3(Wfxt + Ufht\u22121 + bf ) (4) ct = ft ct\u22121 + (1\u2212 ft) tanh(Wcxt + Ucht\u22121 + bc) (5) ht = ot tanh(ct) (6) where W , U , and b represents the corresponding input-to-hidden, hidden-to-hidden weights and biases respectively. \u03c3(\u00b7) denotes the sigmoid function, and is the elementwise product. Another problem when using RNN to solve sequence to sequence learning problem is that it is not clear what strategy to apply when the input and output sequence does not share the same length (i.e. for outputs we have T \u2032 time steps, which may not equal to T ), which is the typical setting for this type of tasks. Sutskever et al. Sutskever et al. (2014) propose to use an auto-encoder type of strategy, where the input sequence is encoded to a fixed length vector by using the last hidden state of the recurrent neural network, and then decode the output sequence from the vector.", "startOffset": 95, "endOffset": 1208}, {"referenceID": 6, "context": "We choose the byte pair encoding (BPE) scheme Sennrich et al. (2015) that strikes the balance between too large output vocabulary and too much learning burden for decoders.", "startOffset": 46, "endOffset": 69}, {"referenceID": 0, "context": "As shown in papers Bahdanau et al. (2014), encoding the whole input string to a single fixed length vector is not optimal, since it may not reserve all the information that is required for a successful decoding.", "startOffset": 19, "endOffset": 42}, {"referenceID": 0, "context": "As shown in papers Bahdanau et al. (2014), encoding the whole input string to a single fixed length vector is not optimal, since it may not reserve all the information that is required for a successful decoding. Therefore, we introduce the attention mechanism from Bahdanau et al.Bahdanau et al. (2014) into this model.", "startOffset": 19, "endOffset": 303}, {"referenceID": 2, "context": "We use the same techniques as Hasan et al.Hasan et al. (2015). Filtering multiple months of data from our data warehouse, we got about 70 million misspelling and spell correction pairs as our training data.", "startOffset": 30, "endOffset": 62}, {"referenceID": 2, "context": "We use the same techniques as Hasan et al.Hasan et al. (2015). Filtering multiple months of data from our data warehouse, we got about 70 million misspelling and spell correction pairs as our training data. For testing, we use the same dataset as in paper Hasan et al. (2015), where it contains 4602 queries and the samples are labeled by human.", "startOffset": 30, "endOffset": 276}, {"referenceID": 2, "context": "Method Accuracy Hasan et al.Hasan et al. (2015) 62.", "startOffset": 16, "endOffset": 48}], "year": 2017, "abstractText": "In this paper, we reformulated the spell correction problem as a machine translation task under the encoder-decoder framework. This reformulation enabled us to use a single model for solving the problem that is traditionally formulated as learning a language model and an error model. This model employs multi-layer recurrent neural networks as an encoder and a decoder. We demonstrate the effectiveness of this model using an internal dataset, where the training data is automatically obtained from user logs. The model offers competitive performance as compared to the state of the art methods but does not require any feature engineering nor hand tuning between models.", "creator": "LaTeX with hyperref package"}}}