{"id": "1705.07393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Recurrent Additive Networks", "abstract": "We introduce recurrent additive networks (RANs), a new gated RNN which is distinguished by the use of purely additive latent state updates. At every time step, the new state is computed as a gated component-wise sum of the input and the previous state, without any of the non-linearities commonly used in RNN transition dynamics. We formally show that RAN states are weighted sums of the input vectors, and that the gates only contribute to computing the weights of these sums. Despite this relatively simple functional form, experiments demonstrate that RANs outperform both LSTMs and GRUs on benchmark language modeling problems. This result shows that many of the non-linear computations in LSTMs and related networks are not essential, at least for the problems we consider, and suggests that the gates are doing more of the computational work than previously understood.", "histories": [["v1", "Sun, 21 May 2017 04:42:11 GMT  (87kb,D)", "http://arxiv.org/abs/1705.07393v1", null], ["v2", "Thu, 29 Jun 2017 14:37:32 GMT  (103kb)", "http://arxiv.org/abs/1705.07393v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton lee", "omer levy", "luke zettlemoyer"], "accepted": false, "id": "1705.07393"}, "pdf": {"name": "1705.07393.pdf", "metadata": {"source": "CRF", "title": "Recurrent Additive Networks", "authors": ["Kenton Lee", "Omer Levy", "Luke Zettlemoyer"], "emails": ["kentonl@cs.washington.edu", "omerlevy@cs.washington.edu", "lsz@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Recurrent Additive Networks", "text": "In this section, we first formally define the RAN model and then show that it is a relatively simple class of additive functions via the input vectors."}, {"heading": "2.1 Model Definition", "text": "We start from a sequence of input vectors {x1,., xn} and define a network that produces a sequence of output vectors {h1,.,., hn}. All repetitions over time are calculated by a sequence of state vectors {c1,.., cn}, as follows for each time step t: 2c \u00b2 t = Wcxxt it is simply a weighted sum in which two gates, it (input) and ft (forgot), the mixture of the content layer c \u00b2 t and the previous state ct \u2212 1. The content layer c \u00b2 t is a linear transformation Wx."}, {"heading": "2.2 Analysis", "text": "Another advantage of the relative simplicity of the RANs is that we can formally characterize the space of the functions used to calculate the hidden states ct. Specifically, each state is a component-wise 2In this essay, we omit the terms bias for brevity. However, biased terms are always present. Operations such as Wxxt should always be interpreted as Wxxt + bx.weighted sum of input with the form: ct = it; xt + ft; ct \u2212 1 = t; j = 1 (ij; t; k = j + 1 fk); xt = t; j = 1 wtj; xt (3), considering the simpler RAN described in Equation Theorem (2). 3 Each weight wtj is a product of the input gate ij (as whose respective input xj was read) and each subsequent gate fk is forgotten. An interesting property of these weights is that, like the gates, they can also be returned to the component-wise state of any highly interpretable component."}, {"heading": "3 Language Modeling Experiments", "text": "The fact is that we are going to be able to be in the position we are in, \"he said in an interview with the Welt am Sonntag newspaper."}, {"heading": "4 The Role of Recurrent Additive Networks in Long Short-Term Memory", "text": "The need for LSTMs is typically motivated by the fact that they can alleviate the problem of disappearing gradients in simple RNNs (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997). By introducing cell states controlled by a series of gates, LSTMs provide abbreviations through which gradients can flow easily when learning with back propagation, a mechanism that allows dependencies to be learned over long distances while maintaining the meaningfulness of recurring nonlinearity provided by S-RNNs. We argue that it is possible to reinterpret LSTMs as a hybrid of two other recursive architectures: (1) S-RNNs and (2) RANs. More specifically, LSTMs can be seen as simplification by calculating a layer that is not weighted."}, {"heading": "4.1 LSTM as a Hybrid of S-RNN and RAN", "text": "We first demonstrate the two LSTM subcomponents of the LSTM by dissecting its definition: c-t = tanh (Wchht \u2212 1 + Wcxxt) it = \u03c3 (Wihht \u2212 1 + Wixxt) ft = \u03c3 (Wfhht \u2212 1 + Wfxxt) ot = \u03c3 (Whht \u2212 1 + Woxxt) ct = it-t + ft \u0445 ct \u2212 1 ht = ot-tanh (ct) (4) We refer to c-t as the content layer that is a non-linear recurring layer like S-RNNs. The cell state ct behaves like a RAN, using input and forwarding layers to calculate a weighted sum of the current content layer c-t and the previous cell state ct-1. Actually, just like in RANs, we can express each cell state as component-weighted sum of all previous contents layers ct ct ct ct ct-jit = jt-v-v v v-v v v v-v v v-v v-v v-v v-v-v v-t = t \u2212 v v v v-t v-v-v-v-v-v-v-v-v-v-v v-t-v-v-v-v-v-t-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-t-v-v-v-v-v-v-v-t-v-v-v-v-v-v-v-v-v-v-t-v-v-v-v-v-v-v-v-v-t-v-v-v-v-v-v-v-v-v-v-t-v-v-v-v-v-v-v-v-v-t-v-v-v-v-v-v-v-v-v-v-v-v-t-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-"}, {"heading": "4.2 Simplifying the Content Layer", "text": "We first examine the need to use an embedded S-RNN to calculate the content layer. In this step of the derivative, we remove the non-linearity of the content layer c-t and its dependence on the previous output vector ht-1 and replace it with a simple linear projection of the input vector c-t = Wxxt: c-t = Wcxxt it = \u03c3 (Wihht-1 + Wixxt) ft = \u03c3 (Wfhht-1 + Wfxxt) ot = \u03c3 (Wohht-1 + Woxxt) ct = it \u0445 c-t + ft-ct - 1 ht = ot-tanh (ct) (5) This modification results in a content layer c-t that is identical to the content layer in a RAN. We can also consider this step as the removal of the S-RNN from the LSTM. In our experiments, we find that this simplification (simplified content layer in Table 2) represents a large NS-embedded result in the language as well."}, {"heading": "4.3 Simplifying the Output Layer", "text": "In the second step, which brings us directly to the RANs, we simplify two aspects of the output layer. Firstly, we remove the output gate. Secondly, we extract the remaining tanh nonlinearity from the recurring process, making it an effective external component (like g in RANs). We do this by rewiring the recurring connection of the gates: Instead of relying on the previous output gate \u2212 1, the gates take the previous cell state ct \u2212 1 as the recurring argument that does not go through the tanh nonlinearity. These two modifications result in the RAN, which we present here again for completeness: c-t = Wcxxt it = \u03c3 (Wicct \u2212 1 + Wixxt) ft = inequalities (Wfcct \u2212 1 + Wfxxt) ct = it is c-t + ft-ct \u2212 1 ht = tanh (ct) This simplification of the output layer leads to further language-based improvements during the RAM definitive structuring (almost identical)."}, {"heading": "4.4 Deriving RAN from GRU", "text": "A similar analysis can be applied to other gated RNNs such as GRUs, which are typically defined as follows: zt = \u03c3 (Wzhht \u2212 1 + Wzxxt) rt = \u03c3 (Wrhht \u2212 1 + Wrxxt) h = tanh (Whh (r-ht \u2212 1) + Whxxt) ht = zt h + (1 \u2212 zt)."}, {"heading": "5 Weight Visualization", "text": "Given the relative interpretability of the RAN predictions, we can explicitly understand the meaning of each component in each of the preceding elements, as shown in Equation (3). In language modeling, we can also calculate which previous word had the greatest overall impact on predicting the next word in time step t: vt = t \u2212 1argmax j = 1 (dh max m = 1 (wtj (m)))) (8). In Figure 2, we visualize this calculation by drawing arrows from each word t to its most influential predecessor (i.e. the previous word with the highest weighted component). In these four example sentences, we see that RANs can intuitively restore dependencies due to the syntactical and semantic structure of the text. For example, verbs are related to their arguments, even if they are coordinated, and nouns in relative clauses depend on the noun they modify. This illustration may only provide an insight into the more detailed understanding of the component that can be included in practice."}, {"heading": "6 Related Work", "text": "Many variants of LSTMs (Hochreiter and Schmidhuber, 1997) and alternative designs for more general GRNs have been proposed, such a variant adding peephole connections between the cell state and the gates (Gers and Schmidhuber, 2000). GRUs (Cho et al., 2014) reduce the number of parameters by coupling the entry and forgetfulness gates and rewiring the gate calculations (see Section 4.4 for an alternative formulation). Greff et al. (2016) conducted an LSTM deposition study, while J\u00f3zefowicz et al. (2015) took an automatic approach to the task of architectural design and found additional variants of GRNNNs. While we show that RANs can be regarded as ablation of LSTMs or GRUs, they are much simpler than the variants studied in the above mentioned materials. Several approaches represent sequences as weighted summaries of their elements."}, {"heading": "7 Conclusion", "text": "We have introduced recurring additive networks (RANs), a type of gated RNNs that merely perform additive updates to their latent state. While RANs do not contain the nonlinear recurring connections that are typically critical for RNNNs, they perform remarkably well in multi-language modeling compared to other popular recurring architectures such as LSTMs and GRUs. RANs are also significantly more transparent than other RNNNs; their limited use of nonlinearity allows the state vector to be expressed as a component-weighted sum of earlier input vectors, which also allows the individual effects of sequence inputs on the state to be explicitly recovered and directly indicate which factors most strongly influence the individual parts of the current state. This work also sheds light on the inner workings of existing, more opaque models, primarily LSTMs and GRUs. We show that RAN components exist within similar STMs."}, {"heading": "Acknowledgements", "text": "The research was supported in part by DARPA through the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS-1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award. We also thank Yoav Goldberg, Luheng He, Aaron Jaech, Ariel Holtzman, and the UW NLP Group for helpful conversations and comments on the work."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "venue": null, "citeRegEx": "Adi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Adi et al\\.", "year": 2017}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "venue": "In ICLR,", "citeRegEx": "Arora et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Y. Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": "In INTERSPEECH,", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Recurrent nets that time and count", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber"], "venue": "In IJCNN,", "citeRegEx": "Gers and Schmidhuber.,? \\Q2000\\E", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Deep semantic role labeling: What works and what\u2019s next", "author": ["Luheng He", "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "He et al\\.,? \\Q2017\\E", "shortCiteRegEx": "He et al\\.", "year": 2017}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In ICML,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Assessing the ability of lstms to learn syntaxsensitive", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg"], "venue": "dependencies. TACL,", "citeRegEx": "Linzen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Parikh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Gated recurrent neural networks (GRNNs), such as long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs) (Cho et al.", "startOffset": 82, "endOffset": 116}, {"referenceID": 6, "context": "Gated recurrent neural networks (GRNNs), such as long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs) (Cho et al., 2014), have become ubiquitous in natural language processing (NLP).", "startOffset": 150, "endOffset": 168}, {"referenceID": 0, "context": "GRNN\u2019s widespread popularity is at least in part due to their ability to model crucial language phenomena such as word order (Adi et al., 2017), syntactic structure (Linzen et al.", "startOffset": 125, "endOffset": 143}, {"referenceID": 15, "context": ", 2017), syntactic structure (Linzen et al., 2016), and even long-range semantic dependencies (He et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 11, "context": ", 2016), and even long-range semantic dependencies (He et al., 2017).", "startOffset": 51, "endOffset": 68}, {"referenceID": 7, "context": "Like simple recurrent neural networks (S-RNNs) (Elman, 1990), they are able to learn non-linear functions of arbitrary-length input sequences, while at the same time alleviating the problem of vanishing gradients (Bengio et al.", "startOffset": 47, "endOffset": 60}, {"referenceID": 3, "context": "Like simple recurrent neural networks (S-RNNs) (Elman, 1990), they are able to learn non-linear functions of arbitrary-length input sequences, while at the same time alleviating the problem of vanishing gradients (Bengio et al., 1994) by including gated additive state updates.", "startOffset": 213, "endOffset": 234}, {"referenceID": 16, "context": "Benchmarks We use three language modeling benchmarks: Penn Treebank (PTB) (Marcus et al., 1993), Google\u2019s billion-word benchmark (BWB) (Chelba et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 4, "context": ", 1993), Google\u2019s billion-word benchmark (BWB) (Chelba et al., 2014), and the Text8 character-based language modeling benchmark.", "startOffset": 47, "endOffset": 68}, {"referenceID": 18, "context": "Training We applied 50% dropout (Srivastava et al., 2014) immediately before, after, and inside each of the RNNs (using variational dropout (Gal and Ghahramani, 2016)).", "startOffset": 32, "endOffset": 57}, {"referenceID": 8, "context": ", 2014) immediately before, after, and inside each of the RNNs (using variational dropout (Gal and Ghahramani, 2016)).", "startOffset": 90, "endOffset": 116}, {"referenceID": 14, "context": "We optimized the cross-entropy loss using Adam (Kingma and Ba, 2014), using a batch size of 512.", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "The need for LSTMs is typically motivated by the fact that they can ease the vanishing gradient problem found in simple RNNs (S-RNNs) (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997).", "startOffset": 134, "endOffset": 189}, {"referenceID": 12, "context": "The need for LSTMs is typically motivated by the fact that they can ease the vanishing gradient problem found in simple RNNs (S-RNNs) (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997).", "startOffset": 134, "endOffset": 189}, {"referenceID": 2, "context": "However, it is closely related to another architecture that computes weighted averages: attention (Bahdanau et al., 2015).", "startOffset": 98, "endOffset": 121}, {"referenceID": 12, "context": "Many variants of LSTMs (Hochreiter and Schmidhuber, 1997) and alternative designs for more general GRNNs have been proposed.", "startOffset": 23, "endOffset": 57}, {"referenceID": 9, "context": "One such variant adds peephole connections between the cell state and the gates (Gers and Schmidhuber, 2000).", "startOffset": 80, "endOffset": 108}, {"referenceID": 6, "context": "GRUs (Cho et al., 2014) decrease the number of parameters by coupling the input and forget gates and rewiring the gate computations (see Section 4.", "startOffset": 5, "endOffset": 23}, {"referenceID": 2, "context": "Perhaps the most popular mechanism in NLP is attention (Bahdanau et al., 2015), which assigns a normalized scalar weight to each element (typically a word vector) as a function of its compatibility with an external element.", "startOffset": 55, "endOffset": 78}, {"referenceID": 5, "context": "Selfattention (Cheng et al., 2016; Parikh et al., 2016) extends this notion by computing intra-sequence attention.", "startOffset": 14, "endOffset": 55}, {"referenceID": 17, "context": "Selfattention (Cheng et al., 2016; Parikh et al., 2016) extends this notion by computing intra-sequence attention.", "startOffset": 14, "endOffset": 55}, {"referenceID": 3, "context": "GRUs (Cho et al., 2014) decrease the number of parameters by coupling the input and forget gates and rewiring the gate computations (see Section 4.4 for an alternative formulation). Greff et al. (2016) conducted an LSTM ablation study that probed the importance of each component independently, while J\u00f3zefowicz et al.", "startOffset": 6, "endOffset": 202}, {"referenceID": 3, "context": "GRUs (Cho et al., 2014) decrease the number of parameters by coupling the input and forget gates and rewiring the gate computations (see Section 4.4 for an alternative formulation). Greff et al. (2016) conducted an LSTM ablation study that probed the importance of each component independently, while J\u00f3zefowicz et al. (2015) took an automatic approach to the task of architecture design, and found additional variants of GRNNs.", "startOffset": 6, "endOffset": 326}, {"referenceID": 1, "context": "Recently, Arora et al. (2017) proposed a theory-driven approach to assign scalar weights to elements in a bag of words.", "startOffset": 10, "endOffset": 30}], "year": 2017, "abstractText": "We introduce recurrent additive networks (RANs), a new gated RNN which is distinguished by the use of purely additive latent state updates. At every time step, the new state is computed as a gated component-wise sum of the input and the previous state, without any of the non-linearities commonly used in RNN transition dynamics. We formally show that RAN states are weighted sums of the input vectors, and that the gates only contribute to computing the weights of these sums. Despite this relatively simple functional form, experiments demonstrate that RANs outperform both LSTMs and GRUs on benchmark language modeling problems. This result shows that many of the non-linear computations in LSTMs and related networks are not essential, at least for the problems we consider, and suggests that the gates are doing more of the computational work than previously understood.", "creator": "LaTeX with hyperref package"}}}