{"id": "1705.07404", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "CrossNets : A New Approach to Complex Learning", "abstract": "We propose a novel neural network structure called CrossNets, which considers architectures on directed acyclic graphs. This structure builds on previous generalizations of feed forward models, such as ResNets, by allowing for all forward cross connections between layers (both adjacent and non-adjacent). The addition of cross connections among the network increases information flow across the whole network, leading to better training and testing performances. The superior performance of the network is tested against four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. We conclude with a proof of convergence for Crossnets to a local minimum for error, where weights for connections are chosen through backpropagation with momentum.", "histories": [["v1", "Sun, 21 May 2017 06:50:49 GMT  (1726kb,D)", "http://arxiv.org/abs/1705.07404v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["chirag agarwal", "mehdi sharifzhadeh", "joe klobusicky", "dan schonfeld"], "accepted": false, "id": "1705.07404"}, "pdf": {"name": "1705.07404.pdf", "metadata": {"source": "CRF", "title": "CrossNets : A New Approach to Complex Learning", "authors": ["Chirag Agarwal", "Mehdi Sharifzhadeh", "Dan Schonfeld"], "emails": ["cagarw2@uic.edu,", "mshari5@uic.edu,", "dans@uic.edu", "klobuj@rpi.edu"], "sections": [{"heading": null, "text": "Index Terms - Interconnections, Directional Acyclic Graphs (DAG), Image Classification, ConvergenceF"}, {"heading": "1 INTRODUCTION", "text": "In fact, the fact is that most of them will be able to feel as if they are able to move, and that they will be able to move, and that they will be able to move, and that they will be able to move."}, {"heading": "1.1 Prior and Related Work", "text": "For example, Kothari and Agyepong [5] introduced simple lateral connections in the form of a chain, where each unit in a hidden layer is connected to the next. In one example, by adding these lateral connections, a small network with a hidden layer of 12 units came closer to a complex function. The problem of disappearing gradients was solved in [14], [15] by connecting some hidden layers directly to the classification layer. ResNets integrated additional \"short connecting paths,\" thus increasing connections within a given network. ResNets are built up of dense blocks and pooling operations, and they combine features by summing them up before passing them on to subsequent layers. Similar to our work, DenseNets [1] * recently should link all layers directly to each other (with matching feature card)."}, {"heading": "1.2 Contribution", "text": "In this paper, we propose an architecture that is a combination of the traditional feed-forward network, along with connections between neurons of different layers of higher order, which enables the global traits learned in the early layers to be preserved in the later layers. We link characteristic maps from early layers by bundling layers between them. [10] One of the problems mentioned by [10] was that Deep Neural Networks (DNN) tend to learn low and mid-level traits rather than just show the global structure of objects, which easily deceived the network, in a more general case, that is, linking networked neural networks together. This leads to a higher robustness of our architecture. [11] Another goal of this paper is to generalize the results in a more general case, that is, linking networked neural networks."}, {"heading": "2 ARCHITECTURE: FEED-FORWARD NETWORK WITH CROSS-LAYER CONNECTIVITY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Convergence under a single lateral weight", "text": "For a simplified example, we show the convergence to a local minimum indicated by the type J = J = 1 shown in Figure 1. The proof is similar to [18]. Nodes consist of n input nodes, two hidden nodes and a single output node. Weight values consist of a matrix V = (vi, j) n \u00b7 2 of the weights from input nodes to hidden nodes, a vector (w1, w2) of the weights from the hidden level to the output and a lateral weight z from the first hidden level to the second. Adding the weight z indeed separates the second hidden node as an additional hidden layer for the network.Let g: R \u2192 R denote an activation function. For an initial input x = (x1,., xn), the input characteristics of ares1 = 2, j."}, {"heading": "2.2 DAG architecture and statement of theorem", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "2.3 CrossNets Implementation Details:", "text": "From Equation (18), each hidden layer is recursively calculated using the enabled output of all previous layers of neurons plus the weighted sum of input from the input layer. Outputs from all previous layers are stacked together and then entered as input into the following layer. Traditional Convolutionary Neural Networks combine the output characteristic map of the jth revolutionary layer with input from the (j + 1) th revolutionary layer. For a revolutionary network version of CrossNets, we combine the output characteristic map of the jth revolutionary layer with all input from the ith revolutionary layer, for all i > j. These cross connections between non-adjacent layers lead to Cross Network or CrossNet architecture. Instead of using simple revolutionary layers, we used a combination of three consecutive operations: Batch Normalization, rectified linear unit (ReLU), and finally a 3 x revolutionary layer."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate the effectiveness of our proposed architecture by implementing cross-layer connections on different feed-forward and Convolutional Neural Networks. The main purpose of the following experiments was not to design the architecture that provides the state-of-the-art for the various datasets, but to introduce the idea of using cross-links for both multilayered perception and revolutionary networks. We compare the results of networked architectures with their feed-forward counterparts and also with other previously proposed architectures."}, {"heading": "3.1 Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 MNIST", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "3.1.2 CIFAR-10 and CIFAR-100", "text": "The CIFAR datasets [26] consist of 32 x 32 RGB color images from different classes. CIFAR-10 and CIFAR-100 contain images from 10 and 100 classes, respectively. Both datasets consist of 60,000 images, divided into 50,000 training images and 10,000 test images. Of the 50,000 training images, 5,000 were retained for cross-validation to optimize hyperparameters. As we see, each class can find 6000 and 600 images in CIFAR-10 and CIFAR-100 datasets, respectively. Input images have been normalized and experiments performed with and without augmentation. A more detailed description of the experimental setup and architectures can be found in Appendix B.2.Table 3, which shows the classification error of various methods on the CIFAR-10 and CIFAR-100 datasets."}, {"heading": "3.1.3 Street View House Numbers", "text": "The Street View House Numbers (SVHN) dataset [32] consists of 3232 color RGB images. The dataset contains 73,257 samples for training, 26,032 samples for testing and an additional 531,131 samples for training. It consists of 10-digit classes from 0-9. We take 6000 samples (600 from each class - 400 from the training set and 200 from the additional training set) and use it as a validation dataset. Pre-processing was done on the samples with a view to global contrast normalization. As in the CIFAR dataset, we selected the best hyperparameters by monitoring validation performance. The performance of DropConnect was a result of their matching scheme. ResNets and FractalNets did not perform well compared to SVHN datasets and achieved an error rate of 2.01% and 1.87%, respectively. We trained the same three different versions of CrossNets as they were used for CIAR net rates, 1.92 M, 1.71% and 1.4% respectively."}, {"heading": "4 DISCUSSION", "text": "In fact, we will be able to move to another world, in which we will be able to change the world we are in."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have presented a feed-forward network architecture with cross-layer connections, the network has been trained using the traditional back-propagation algorithm, but the back-layer algorithm is slower than advanced second-order algorithms [33], [34]. Training our proposed network architecture with these second-order algorithms would make it faster. Cross-layer connections allow neurons to learn more efficiently, as confirmed in the performance results for MNIST, CIFAR-10, CIFAR-100 and SVHN datasets. CrossNets can be considered more of a complete networked network, and the question remains whether we should connect all layers together or just connect certain layers together. We plan to explore this question in our future work."}, {"heading": "APPENDIX A CONVERGENCE PROOF", "text": "For sufficiently small learning rates, we show that the difference iteration in the error results in a sum of error gradient terms of the first order, which can be controlled by a sufficiently small learning rate, and also quadratic terms. Specifically, a constant C > 0 results in such a way that for all k = 1, 2,.., Ek + 1 \u2264 (both) (both + quadratic terms) (both + quadratic terms) a constant C > 0 exists so that for all k = 1, 2,., Ek + 1 \u2264 (both + quadratic terms) (both + quadratic terms) (two + quadratic terms + quadratic terms). (26) + C (two + quadratic terms) (two + quadratic terms)."}, {"heading": "APPENDIX B DETAILED DESCRIPTION OF EXPERIMENTS", "text": "In fact, most of them will be able to move to another world, in which they can move to another world, but in which they are not able to integrate."}], "references": [{"title": "Densely connected convolutional networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1608.06993, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Principles of neurodynamics. perceptrons and the theory of brain mechanisms", "author": ["F. Rosenblatt"], "venue": "DTIC Document, Tech. Rep., 1961.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1961}, {"title": "Solving the n-bit parity problem using neural networks", "author": ["M.E. Hohil", "D. Liu", "S.H. Smith"], "venue": "Neural Networks, vol. 12, no. 9, pp. 1321\u20131323, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Solving parityn problems with feedforward neural networks", "author": ["B.M. Wilamowski", "D. Hunter", "A. Malinowski"], "venue": "Neural Networks, 2003. Proceedings of the International Joint Conference on, vol. 4. IEEE, 2003, pp. 2546\u20132551.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "On lateral connections in feedforward neural networks", "author": ["R. Kothari", "K. Agyepong"], "venue": "Neural Networks, 1996., IEEE International Conference on, vol. 1. IEEE, 1996, pp. 13\u201318.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Layered neural networks with horizontal connections can reduce the number of units", "author": ["J. Smid"], "venue": "Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1346\u20131350.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "The cascade-correlation learning architecture", "author": ["S.E. Fahlman", "C. Lebiere"], "venue": "1990.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE transactions on neural networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["X. Glorot", "Y. Bengio"], "venue": "in Aistats, vol", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 427\u2013436.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2015, pp. 2377\u20132385.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deeplysupervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "Artificial Intelligence and Statistics, 2015, pp. 562\u2013570.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1302.4389, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Convergence of gradient method with momentum for back-propagation neural networks", "author": ["W. Wu", "N. Zhang", "Z. Li", "L. Li", "Y. Liu"], "venue": "Journal of Computational Mathematics, pp. 613\u2013623, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Convergence of gradient method with momentum for two-layer feedforward neural networks", "author": ["N. Zhang", "W. Wu", "G. Zheng"], "venue": "IEEE Transactions on Neural Networks, vol. 17, no. 2, pp. 522\u2013525, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimal convergence of on-line backpropagation", "author": ["M. Gori", "M. Maggini"], "venue": "IEEE transactions on neural networks, vol. 7, no. 1, pp. 251\u2013254, 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "European Conference on Computer Vision. Springer, 2016, pp. 630\u2013645.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": "1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Best practices for convolutional neural networks applied to visual document analysis.", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "in ICDAR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "What is the best multistage architecture for object recognition?", "author": ["K. Jarrett", "K. Kavukcuoglu", "Y. LeCun"], "venue": "in Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "European Conference on Computer Vision. Springer, 2016, pp. 646\u2013661.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 1058\u20131066.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree", "author": ["C.-Y. Lee", "P.W. Gallagher", "Z. Tu"], "venue": "International conference on artificial intelligence and statistics, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Resnet in resnet: generalizing residual architectures", "author": ["S. Targ", "D. Almeida", "K. Lyman"], "venue": "arXiv preprint arXiv:1603.08029, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv preprint arXiv:1605.07146, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, vol. 2011, no. 2, 2011, p. 5.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Two highly efficient second-order algorithms for training feedforward networks", "author": ["N. Ampazis", "S.J. Perantonis"], "venue": "IEEE Transactions on Neural Networks, vol. 13, no. 5, pp. 1064\u20131074, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural network architectures and learning algorithms", "author": ["B.M. Wilamowski"], "venue": "IEEE Industrial Electronics Magazine, vol. 3, no. 4, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimization theory and methods: nonlinear programming", "author": ["W. Sun", "Y.-X. Yuan"], "venue": "Springer Science & Business Media,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "From multilayer perceptron (MLP) networks to the more prominent recurrent neural networks (RNN) and convolutional neural networks (CNN), neural networks have become a dominant force in the fields of computer vision, speech recognition, and machine translation [2].", "startOffset": 260, "endOffset": 263}, {"referenceID": 2, "context": "However, Arbitrary Networks (AC) networks have been shown to train and perform better than the commonly used feed-forward architectures [3], [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "However, Arbitrary Networks (AC) networks have been shown to train and perform better than the commonly used feed-forward architectures [3], [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Neural networks with lateral connections (another special case of AC networks) between hidden layer neurons are also found to be more efficient than MLP networks [5], [6].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "Neural networks with lateral connections (another special case of AC networks) between hidden layer neurons are also found to be more efficient than MLP networks [5], [6].", "startOffset": 167, "endOffset": 170}, {"referenceID": 6, "context": "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and \u201dfooling images\u201d [10] have emerged in regards to deep networks.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and \u201dfooling images\u201d [10] have emerged in regards to deep networks.", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and \u201dfooling images\u201d [10] have emerged in regards to deep networks.", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and \u201dfooling images\u201d [10] have emerged in regards to deep networks.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use \u201dshort connections\u201d to connect nonconsecutive layers.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use \u201dshort connections\u201d to connect nonconsecutive layers.", "startOffset": 176, "endOffset": 180}, {"referenceID": 12, "context": "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use \u201dshort connections\u201d to connect nonconsecutive layers.", "startOffset": 194, "endOffset": 198}, {"referenceID": 0, "context": "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use \u201dshort connections\u201d to connect nonconsecutive layers.", "startOffset": 214, "endOffset": 217}, {"referenceID": 0, "context": "[1] performed experiments independently on this subject similar to ours.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Kothari and Agyepong [5], for instance, introduced simple lateral connections in the form of a chain, where each unit in a hidden layer is connected to the next.", "startOffset": 21, "endOffset": 24}, {"referenceID": 13, "context": "The problem of vanishing gradients were addressed in [14], [15] by connecting some hidden layers directly to the classifier layer.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "The problem of vanishing gradients were addressed in [14], [15] by connecting some hidden layers directly to the classifier layer.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Similar to our work, DenseNets [1]* recently connected all layers (with matching feature-map sizes) directly with each other resulting in feature reuse, all layers can easily access their preceding layers making it easy to reuse the information from previously computed feature maps.", "startOffset": 31, "endOffset": 34}, {"referenceID": 15, "context": "Many different architectures like Maxout Networks [16], Network In Network (NIN) [17], Deeply Supervised Networks (DSN) [14], and FractalNets are being proposed which give competitive results on the above mentioned datasets.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "Many different architectures like Maxout Networks [16], Network In Network (NIN) [17], Deeply Supervised Networks (DSN) [14], and FractalNets are being proposed which give competitive results on the above mentioned datasets.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "Many different architectures like Maxout Networks [16], Network In Network (NIN) [17], Deeply Supervised Networks (DSN) [14], and FractalNets are being proposed which give competitive results on the above mentioned datasets.", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "One of the problems mentioned by [10] was that Deep Neural Networks (DNN) tend to learn low and middle-level features rather than the global structure of objects due to which the network was easily fooled on showing only subcomponents of an object.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "Another aim of this paper is to generalize the results in [11] to a more general case, that is, cross-connected neural networks.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "The proof is similar to [18].", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "This choice of momentum was also used in [19].", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "This proof will require some regularity and boundedness assumptions [18] Assumptions 1.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "This set of assumptions is equivalent to that used in [18], and may also be found in other nonlinear optimization problems such as [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "This set of assumptions is equivalent to that used in [18], and may also be found in other nonlinear optimization problems such as [20].", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": "We followed the same configuration in our experiments as [21] and define each stack of these three layers as a composite unit.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "1 MNIST We performed experiments on hand-written digit classification problem [22] where input patterns are digits.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "60% [23].", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "25% error for a traditional feed forward network with same architecture using dropouts [24].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "53% was achieved in [25] using a 2layer CNN+2-layer NN configuration (0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Network in Network [17] surpassed this result by achieving an error of 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "By using maxout activation function [16] reduced the error to 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "39% achieved by [14].", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "2 CIFAR-10 and CIFAR-100 The CIFAR datasets [26] consist of 32x32 RGB color images taken from various classes.", "startOffset": 44, "endOffset": 48}, {"referenceID": 26, "context": "66%) [27] without any data augmentation by a large margin for CIFAR-10 dataset.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "DropConnect[28] 18.", "startOffset": 11, "endOffset": 15}, {"referenceID": 26, "context": "32% ResNet[27] 13.", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "22% Stochastic Depth[27] 11.", "startOffset": 20, "endOffset": 24}, {"referenceID": 28, "context": "39% Generalized Pooling[29] 7.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "37% ResNet in ResNet [30] - 5.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "90% Wide ResNet [31] - 4.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "3 Street View House Numbers The Street View House Numbers (SVHN) dataset [32] consists of 3232 color RGB images.", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "In [3], [4] it was proposed that Arbitrary Connected (AC) networks are much more powerful than commonly used feed-forward networks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [3], [4] it was proposed that Arbitrary Connected (AC) networks are much more powerful than commonly used feed-forward networks.", "startOffset": 8, "endOffset": 11}, {"referenceID": 32, "context": "However, the back-propagation algorithm is slower than more advanced second-order algorithms [33], [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "However, the back-propagation algorithm is slower than more advanced second-order algorithms [33], [34].", "startOffset": 99, "endOffset": 103}, {"referenceID": 34, "context": "2 Proof of convergence We now use the a Lemma found in [35]: Lemma 4.", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "The 2-CNN-2-fully connected architecture was designed as per [25] The first convolutional produces 32 feature maps, and the second convolutional layer produces 64 feature maps, each of using 5 \u00d7 5 filters followed by 2 \u00d7 2 pooling.", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "We propose a novel neural network structure called CrossNets, which considers architectures on directed acyclic graphs. This structure builds on previous generalizations of feed forward models, such as ResNets, by allowing for all forward cross connections between layers (both adjacent and non-adjacent). The addition of cross connections among the network increases information flow across the whole network, leading to better training and testing performances. The superior performance of the network is tested against four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. We conclude with a proof of convergence for Crossnets to a local minimum for error, where weights for connections are chosen through backpropagation with momentum.", "creator": "LaTeX with hyperref package"}}}