{"id": "1705.07445", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep Reinforcement Learning", "abstract": "Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state's value function. $\\lambda$-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While lambda-returns have been extensively studied in RL, they haven't been explored a lot in Deep RL. This paper's first contribution is an exhaustive benchmarking of lambda-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, lambda-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.", "histories": [["v1", "Sun, 21 May 2017 12:47:37 GMT  (5800kb,D)", "http://arxiv.org/abs/1705.07445v1", "10 pages + 11 page appendix"]], "COMMENTS": "10 pages + 11 page appendix", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sahil sharma", "srivatsan ramesh", "girish raguvir j", "balaraman ravindran"], "accepted": false, "id": "1705.07445"}, "pdf": {"name": "1705.07445.pdf", "metadata": {"source": "CRF", "title": "Learning to Mix n-Step Returns: Generalizing \u03bb-Returns for Deep Reinforcement Learning", "authors": ["Sahil Sharma", "Srivatsan Ramesh", "Balaraman Ravindran"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It was necessary to learn what abstract control functions are required to solve targeted sequential decision-making tasks, whereas conventional methods of machine learning such as supervised learning are not suitable. An RL agent is not explicitly informed about the optimal actions, but must instead discover them based on assessment feedback given in relation to rewards sampled from the environment. Targeted sequential decision-making tasks are modelled in the way Markov Decision Processs (MDP) (Puterman, 2014). Traditionally, tabular methods have been widely used to solve MDPs, retaining values or political estimates for each state. Such methods become unfeasible when the underlying state space of the problem is exponentially large or continuous. Traditional RL methods have also used linear function approximators in conjunction with handcrafted government spaces for learning strategies and value functions."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preliminaries", "text": "An MDP (Puterman, 2014) is defined as the tuple < S, A, r, P, \u03b3 >, where S is the set of states in the MDP, A is the set of actions, r: S \u00b7 A 7 \u2192 R is the reward function, P: S \u00b7 A \u00b7 S 7 \u2192 [0, 1] is the transition probability function, in which the sequential decision task is modeled as an MDP and the actor interacts with an environment E over a number of discrete time steps. At some point, the actor receives a state stamp and selects an action from the set of available measures. Considering a state, the actor may decide to choose his action pochastically. His policy is a mapping defined by: S \u00d7 A 7 \u2192 [0], which selects an action from the set of available measures."}, {"heading": "2.2 Actor Critic Algorithms", "text": "Actor-critical algorithms (Konda & Tsitsiklis, 2000) are a class of approaches that directly parameterise politics (using an actor) \u03c0\u03b8a (a | s) and the value function (using a critic), updating policy parameters using the Policy Gradient Theorem (Sutton et al., 1999; Silver et al., 2014), which is based on objective functions."}, {"heading": "2.3 Asynchronous Advantage Actor Critic", "text": "Asynchronous Advantage Actor Critic (A3C) (Mnih et al. (2016)) introduced the first class of actor-critic algorithms that worked on high-dimensional, complex visual entrance spaces. A key finding in this work is that the RL agent can simultaneously explore different parts of the state by executing multiple actors on different strings in a CPU. This ensures that the updates of the agent's parameters are not correlated. In practice, Gt is often replaced by a distorted lower estimate of variance based on multi-stage returns. In the A3C algorithm n-step returns, n-tri and hyper-parameter are used."}, {"heading": "2.4 Weighted Returns", "text": "The weighted average of the n-step return estimates for different n's can be used to achieve TD targets as long as the sum of the weights associated with the different n-step returns is 1 (Sutton & Barto (1998) Other words with a weight vector w = (w (1), w (2), \u00b7 \u00b7, w (m), so that i = m \u00b2 i = 1 w (i) = 1 and n-step returns for n \u00b2 {1, 2, \u00b7, m}: G (1) t, G (2) t, \u00b7 \u00b7 \u00b7, G (m) t, we define a weighted return asGwt = m \u00b2 n = 1 w (n) G (n) t (1) Note that the n-step return G (n) t is defined as follows: G (n) t = n \u00b2 i \u2212 1rt + i + q nV (st + n) (2)"}, {"heading": "2.5 \u03bb-Returns", "text": "A specific case of Gwt is G \u03bb t (known as \u03bb returns), which is defined as follows: G\u03bbt = (1 \u2212 \u03bb) m \u2212 1 \u2211 n = 1 \u03bbn \u2212 1G (n) t + \u03bb m \u2212 1G (m) t (3) What we have defined here is a form of abbreviated \u03bb returns for TD learning. These are the only ways we experiment with in our paper. We use abbreviated \u03bb returns because the A3C algorithm is designed in such a way that it is suitable for an extension below abbreviated \u03bb returns. We leave the problem of generalizing our work to the full \u03bb returns as well as to eligibility traces (\u03bb returns are the predictive consideration of eligibility reductions) of future work."}, {"heading": "3 \u03bb-Returns and Beyond: Autodidactic Returns", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Autodidactic Returns", "text": "Autodidactic returns are a weighted average of n-step returns, where the weight vector is learned in addition to the approximate value functions. Since the self-didactic returns we propose are constructed using dynamic weight vectors (weights change with the state the agent encounters in the MDP), we refer to the weight vector as w (st). Autodidactic returns can be used to learn better approximations for the value functions using the updating equation based on the TD (0) learning rule: Vt (st) \u2190 Vt (st) + \u03b1 (G w (st) t \u2212 Vt (st))) (4) Unlike autodidactic returns, weights that are assigned by the learning process do not change."}, {"heading": "3.2 Confidence-based Autodidactic Returns", "text": "All n-step returns for the state st are estimates for V (st) with the value function of the corresponding n-th future state (V (st + n). All value functions are estimates for themselves. Therefore, it would be a natural method for the RL agent to weigh an n-step return (G (n) t), to calculate this weighting using the trust c (st + n) that the agent has in the value function estimate V (st + n), on the basis of which the n-step return was estimated. The agent weighs the n-step return based on how certain it is that this n-step return is a good estimate for V (st + n). Since the n-step return in turn depends on V (st + n), c (st + n) also reflects the confidence that the agent has the value function V (st + n), the estimate V (st + n), the weight vector w (w w w st), v (st), w w w w st, b (st), b (st."}, {"heading": "3.3 Using \u03bb-returns in A3C", "text": "We propose an easy way to integrate (truncated) \u03bb returns into the A3C framework. We call this combination LRA3C. The critic in A3C uses the TD (0) algorithm to make good estimates for the value function. However, note that the TD target can generally be based on any n-step returns (or a mixture thereof), and the A3C algorithm is particularly well suited to using weighted returns such as p returns, as the algorithm already uses n-step returns for boot strapping. Using eqs. (1) to (3) makes it very easy to integrate weighted returns into the A3C framework. The respective sample estimates for the gradients of the actor and critic are:"}, {"heading": "3.4 Using Autodidactic Returns in A3C", "text": "We propose to use autodidactic returns instead of the normal n-step returns in the A3C framework. We call this combination CARA3C. In a generic DRL configuration, a forward pass through the network is made to obtain the value function of the current state. Network parameters are continuously updated based on the gradient of the loss function and the value function estimate (in general) gets better. To predict the trust values, a unique neural network is created that shares all but the last layer with the value function network. Therefore, any forward movement of the network in the state now gives the value function V (st) and the network's confidence in its value function forecast c (st). Figure 1 visually illustrates how the trust values are calculated using an A3C network. Next, using eqs. (1) to (5) the weighted average of the n-step return calculated and used as an improvement (V)."}, {"heading": "3.5 Avoiding pitfalls in TD learning of Critic", "text": "The LSTM-A3C neural networks used to represent the policy and the value function share all but the last output layer. In particular, the LSTM Hochreiter & Schmidhuber (1997) controller, which summarizes the observations in time, is shared by the policy and the value networks. Figure 1 contains a demonstration of how w (s1) is calculated. Since all three outputs (policy, value function, reliance on value function) share all but the last layer, Gw (st) t depends on the network parameters used for the value function."}, {"heading": "4 Experimental Setup and Results", "text": "We conducted general gameplay experiments with CARA3C and LRA3C on 22 tasks in the Atari domain, training all networks for 100 million time steps, adjusting the hyperparameters for each of the methods to a subset of four tasks: Seaquest, Space Invaders, Gopher and Breakout. The same hyperparameters were used for the rest of the tasks. Baseline values were obtained from Sharma et al. (2017). All of our experiments were repeated three times with different random seeds to ensure that our results were robust against random initializations. All results reported are the average of the results obtained by using these three random seeds. As the A3C values are from Sharma et al. (2017), we also followed their training and testing regimes. Appendix A contains experimental details about the training and testing regimes. Appendix G documents the procedure we used to select important hyperparameters for our methods."}, {"heading": "4.1 General gameplay performance", "text": "Figure 2 shows the percentage improvement of our complex mix of n-step return methods (CARA3C and LRA3C) over A3C. If the values achieved by one of our methods and A3C in a task p or q, then the percentage improvement is calculated as follows: (p \u2212 q \u00b7 100). As we can see, CARA3C achieves an astonishing 67 times performance in the Kangaroo task. Table 1 (in Appendix B) compares the raw values obtained by our methods with the A3C baseline values. The development of the average performance of our methods in training progress is shown in Figure 3. An extended version of the chart for all tasks can be found in Appendix C."}, {"heading": "4.2 Analysis of the Evolution of Weights During Training", "text": "Figure 4 shows the weights assigned to different n-step returns (for n-step returns 1, 2, \u00b7 \u00b7, 20} are learned and therefore change to minimize the critical loss function. Figure 4 shows that the weights assigned to each n-step return value are initially random (all equal to 0.05) and slowly evolve to different values, depending on the corresponding confidence value predicted by the network. An extended version of the charts for all tasks can be found in Appendix D. We observe that the 1-step returns and the 20-step returns are assigned the most trust. This seems to indicate a bias variance trade-off between the different n-step returns, with both extremes advantageous, perhaps in different parts of the state. In contrast, the 1-step returns always give the highest weight."}, {"heading": "4.3 Analysis of the Evolution of Weights During an Episode", "text": "Figure 5 shows the evolution of weights assigned to different n-step returns over the duration of an episode. It can be seen that weights evolve dynamically for many tasks over the course of the episode, which seems to confirm our motivation for using dynamic autodidactic returns. An extended version of the diagrams for all tasks can be found in Appendix E. Figure 4: Evolution of weights assigned to each n-step return (where n \u2264 20) with the training time."}, {"heading": "4.4 Analysis of the Learned Value Function", "text": "In this paper, we propose two methods to learn value functions more refined than the use of n-step returns. Therefore, it is important to analyze the value functions learned through our methods and to understand whether our methods are actually capable of learning better value functions than basic methods or not. In this subsection, we trained A3C agents to serve as a baseline. To verify our claims about better learning value functions, we conducted the following experiment. We used trained CARA3C-LRA3C and A3C agents and calculated the L2 loss between the value function V (st) predicted by one method and the actual discounted sum of returns (\u2211 T \u2212 t k = 0 \u03b3krt + k)."}, {"heading": "5 Conclusion and Future Work", "text": "We propose an easy way to integrate \u03bb returns into the A3C algorithm and perform a large-scale benchmarking of the resulting LRA3C algorithm. Then, we propose a natural generalization of \u03bb returns, known as trust-based self-taught returns (CAR). In CAR, the agent learns to dynamically assign weights to the various n-step returns from which he can bootstrap. Our experiments demonstrate the effectiveness of a sophisticated mix of multi-level returns with at least one of CARA3C or LRA3C performing A3C in 18 of 22 tasks. In 9 of the tasks, CARA3C performs best, while in 9 of them LRA3C is the best. CAR gives the agent the freedom to learn and decide how much he wants to weigh these goals."}, {"heading": "Appendix A: Experimental Details", "text": "Since the baseline values used in this paper are from Sharma et al. (2017), we use the same training and evaluation system."}, {"heading": "On hyper-parameters", "text": "We used the LSTM variant of A3C [Mnih et al. (2016)] algorithm for the CARA3C and LRA3C experiments. The async-rmprop algorithms were used to update parameters with the same hyperparameters as in Mnih et al. (2016) The initial learning rate used was 10 \u2212 3 and resulted in 0 over 100 million time steps, which was the length of the training period. The n used in n-step returns was 20. Entropy regulation was used to promote exploration, similar to Mnih et al. (2016) The \u03b2 for entropy regulation was found to be 0.01 after hyper-parameter tuning, both for CARA parameter tuning and for LRA3C, separately."}, {"heading": "Appendix B: Table of Raw Scores", "text": "All evaluations were performed using the mean obtained after 100 million steps of training to be consistent with the evaluation paradigm presented in Sharma et al. (2017) and Mnih et al. (2016). Both the CARA3C and LRA3C values are determined by averaging three random seeds. A3C column values are taken from Table 4 by Sharma et al. (2017)."}, {"heading": "Appendix C: Training Graphs", "text": "The evaluation strategy described in Appendix A was executed to generate training curves for all 22 Atari tasks. This appendix contains all these training curves, which show how the performance of the CARA3C and LRA3C agents develops over time."}, {"heading": "Appendix D: Evolution of confidence-based weights with training time", "text": "This appendix reports on the results of an extended version of the experiments carried out in Section 4.3. If we assume that states are numbered by 1, then for the group of statesSt \u2261 1 (mod 20) = {st: t \u2261 1 (mod 20)} = {s1, s21, \u00b7 \u00b7 \u00b7 the confidence ratios were used to obtain weights wi, as demonstrated in Section 3.2. These weights were plotted as a function of training progress. Figure 8 shows that in general, confidence-based weights associated with n-step yields increase as training progresses. A similar diagram for all states in group St 8 (mod 20) is shown in Figure 9 that these states can only be bootstrap by 10 possible next states because of the way in which A3C is implemented. These states use a 10-size vector for trust-based weights and subsequently the diagrams of these weights against ten training lines."}, {"heading": "Appendix F: Algorithm for training CARA3C", "text": "The algorithm corresponding to CARA3C, our most important novel contribution, was presented in algorithm 8. A similar algorithm can be easily constructed for LRA3C. Algorithm 1 CARA3C 1: / / Let's assume global common parameter vectors. 2: / / Let's assume a global pedometer (together) T = 0 3: 4: K at maximum value of n in n-step yields 5: Tmax \u2190 Total number of training steps for CARA3C 6: \u03c0 \u2190 Agent policy 7: Initialize the local pedometer t \u2190 1 8: Let's repeat the local thread parameters 9: 10: 11: tinit = t 12: dhabi \u2190 0 13: Synchronization of local thread parameters. 7: Initialize local thread counter t \u2190 1 8: Let's let's call it the local thread parameters."}, {"heading": "36: R", "text": "\"\u2190 ri + \u03b3R \u2032 37: Gkj \u2190 Ckj.R \u2032 / / Assuming a 0-based indexing 38: R \u2190 V (si; \u03b8 \u2032) 39: 40: for i-based indexing 41: j-based indexing 42: TD target 26: K \u2212 1 j = 0 Gij43: Dependency-based dependency gradients (TD target \u2212 V (si;.))) + 1: Dependency-based gradients (TD target \u2212 V (si)) 44: Dependency-based dependency gradients (T target \u2212 V (si;.) + 2 45: Performing asynchronous dependency update using dependencies \u2032 46: to T > TmaxAlgorithm 2: Create a 2D weight matrix with the fix numbers 1: GET _ WEIGHTS _ MATRIX (C) 2: Dependencies-based Tmaxalgorithm > 2D update to a max."}], "references": [{"title": "Investigating recurrence and eligibility traces in deep q-networks", "author": ["Harb", "Jean", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1704.05495,", "citeRegEx": "Harb et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Harb et al\\.", "year": 2017}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Actor-critic algorithms. In Advances in neural information processing", "author": ["Konda", "Vijay R", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Konda et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2000}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Incremental multi-step q-learning", "author": ["Peng", "Jing", "Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Peng et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Peng et al\\.", "year": 1996}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q2014\\E", "shortCiteRegEx": "Puterman and L.", "year": 2014}, {"title": "On-line Q-learning using connectionist systems", "author": ["Rummery", "Gavin A", "Niranjan", "Mahesan"], "venue": "University of Cambridge, Department of Engineering,", "citeRegEx": "Rummery et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Rummery et al\\.", "year": 1994}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "True online td (lambda)", "author": ["Seijen", "Harm", "Sutton", "Rich"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Seijen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2014}, {"title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning", "author": ["Sharma", "Sahil", "Lakshminarayanan", "Aravind S", "Ravindran", "Balaraman"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Sharma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2017}, {"title": "Deterministic policy gradient algorithms", "author": ["Silver", "David", "Lever", "Guy", "Heess", "Nicolas", "Degris", "Thomas", "Wierstra", "Daan", "Riedmiller", "Martin A"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Silver", "David", "Huang", "Aja", "Maddison", "Chris J", "Guez", "Arthur", "Sifre", "Laurent", "Van Den Driessche", "George", "Schrittwieser", "Julian", "Antonoglou", "Ioannis", "Panneershelvam", "Veda", "Lanctot", "Marc"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Technical note: Q-learning", "author": ["Watkins", "Christopher J.C. H", "Dayan", "Peter"], "venue": "Mach. Learn.,", "citeRegEx": "Watkins et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watkins et al\\.", "year": 1992}, {"title": "Experimental Details Since the baseline scores used in this work are from Sharma et al. (2017), we use the same training and evaluation regime as well. On hyper-parameters We used the LSTM-variant of A3C [Mnih et al. (2016)] algorithm for the CARA3C and LRA3C", "author": ["A Appendix"], "venue": null, "citeRegEx": "Appendix,? \\Q2016\\E", "shortCiteRegEx": "Appendix", "year": 2016}, {"title": "The async-rmsprop algorithm [Mnih et al", "author": ["Mnih"], "venue": "Mnih et al", "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}, {"title": "\u03bb for the \u03bb-returns", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2017\\E", "shortCiteRegEx": "Sharma", "year": 2017}, {"title": "after every 1 million steps of training and followed the strategy", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2000\\E", "shortCiteRegEx": "Sharma", "year": 2000}, {"title": "Table 1 in Appendix B contains the raw scores obtained by CARA3C, LRA3C and A3C agents on 22 Atari 2600 tasks. The evaluation was done using the latest agent obtained after training for 100 million steps, to be consistent with the evaluation regime", "author": ["C. Appendix"], "venue": "Mnih et al", "citeRegEx": "Appendix,? \\Q2016\\E", "shortCiteRegEx": "Appendix", "year": 2016}, {"title": "Architecture details We used a low level architecture", "author": ["Mnih"], "venue": "Mnih et al", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "The first three layers of both the methods are convolutional layers with same filter sizes, strides, padding and number of filters", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2017\\E", "shortCiteRegEx": "Sharma", "year": 2017}, {"title": "2017) to ensure fair comparisons to the baseline A3C model and apply to both the CARA3C and LRA3C methods. Similar to Mnih et al. (2016) the Actor and Critic share all but the final layer. In the case of CARA3C, Each of the three functions: policy, value function and the confidence value are realized with a different final output layer, with the confidence and value function outputs having no non-linearity", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2016\\E", "shortCiteRegEx": "Sharma", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "The use of deep neural networks in conjunction with RL objectives has shown remarkable results such as learning to solve the Atari 2600 tasks from raw pixels (Bellemare et al., 2013; Mnih et al., 2015, 2016; Sharma et al., 2017; Jaderberg et al., 2017), learning to solve complex simulated physics tasks (Todorov et al.", "startOffset": 158, "endOffset": 252}, {"referenceID": 2, "context": "The use of deep neural networks in conjunction with RL objectives has shown remarkable results such as learning to solve the Atari 2600 tasks from raw pixels (Bellemare et al., 2013; Mnih et al., 2015, 2016; Sharma et al., 2017; Jaderberg et al., 2017), learning to solve complex simulated physics tasks (Todorov et al.", "startOffset": 158, "endOffset": 252}, {"referenceID": 18, "context": ", 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al.", "startOffset": 59, "endOffset": 128}, {"referenceID": 10, "context": ", 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al.", "startOffset": 59, "endOffset": 128}, {"referenceID": 4, "context": ", 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al.", "startOffset": 59, "endOffset": 128}, {"referenceID": 14, "context": ", 2015) and showing super-human performance on the ancient board game of Go (Silver et al., 2016).", "startOffset": 76, "endOffset": 97}, {"referenceID": 6, "context": "With the advent of deep RL, the use of multi-step returns has gained a lot of popularity (Mnih et al., 2016).", "startOffset": 89, "endOffset": 108}, {"referenceID": 2, "context": ", 2017; Jaderberg et al., 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al., 2016). Building accurate and powerful (in terms of generalization capabilities) state and action value function (Sutton & Barto, 1998) estimators is important for successful RL solutions. This is because many practical RL solutions (Q-Learning (Watkins & Dayan, 1992), SARSA (Rummery & Niranjan, 1994) and Actor-Critic Methods (Konda & Tsitsiklis, 2000)) use TD Learning (Sutton, 1988). The ability to build better estimates of the value functions directly results in better policy estimates as well as faster learning. \u03bb-returns (LR) (Sutton & Barto, 1998) are very effective in this regard. They are effective for faster propagation of delayed rewards and also result in more reliable learning. LR provide a trade-off between Monte Carlo and TD learning methods. They model the TD target using a mixture of n-step returns, wherein the weighs are exponentially decayed. With the advent of deep RL, the use of multi-step returns has gained a lot of popularity (Mnih et al., 2016). However, \u03bb-returns have not been explored extensively in the deep RL setting. Having said that, it is to be noted that the use of exponentially decaying weighting for various n-step returns seems to be an ad-hoc design choice made by LR. In this paper, we start off by extensively benchmarking \u03bb-returns (our experiments only use truncated \u03bb-returns due to the nature of the DRL algorithm (A3C) that we work with) and also propose a generalization of \u03bb-returns called the Confidence-based Autodidactic Returns (CAR), In CAR, the DRL agent learns in an end-to-end way, the weights to assign to the various n-step return based targets. Also note that in CAR, the weights assigned to various n-step returns change based on the current state that the DRL agent is in. In this sense, CAR weights are dynamic and using them represents a significant level of sophistication as compared to the usage of \u03bb-returns. In the Deep Reinforcement Learning (DRL) setting not much has been done in the context of using mixture of multi-step returns to build better value function estimators. An exception to that statement is the investigation done in Harb & Precup (2017). It shows promising results on two Atari 2600 tasks using the backward view of eligibility traces (\u03bb-returns constitute the forward view of eligibility traces) - Pong and Tennis.", "startOffset": 8, "endOffset": 2374}, {"referenceID": 17, "context": "They update the policy parameters using Policy Gradient Theorem (Sutton et al., 1999; Silver et al., 2014) based objective functions.", "startOffset": 64, "endOffset": 106}, {"referenceID": 13, "context": "They update the policy parameters using Policy Gradient Theorem (Sutton et al., 1999; Silver et al., 2014) based objective functions.", "startOffset": 64, "endOffset": 106}, {"referenceID": 5, "context": "Asynchronous Advantage Actor Critic(A3C) (Mnih et al. (2016)) introduced the first class of actor-critic algorithms which worked on high-dimensional complex visual input space.", "startOffset": 42, "endOffset": 61}, {"referenceID": 5, "context": "We believe that our proposed idea of CAR can be combined with any DRL algorithm (Mnih et al., 2015; Jaderberg et al., 2017; Sharma et al., 2017)wherein the TD-target is modeled in terms of n-step returns.", "startOffset": 80, "endOffset": 144}, {"referenceID": 2, "context": "We believe that our proposed idea of CAR can be combined with any DRL algorithm (Mnih et al., 2015; Jaderberg et al., 2017; Sharma et al., 2017)wherein the TD-target is modeled in terms of n-step returns.", "startOffset": 80, "endOffset": 144}, {"referenceID": 12, "context": "We believe that our proposed idea of CAR can be combined with any DRL algorithm (Mnih et al., 2015; Jaderberg et al., 2017; Sharma et al., 2017)wherein the TD-target is modeled in terms of n-step returns.", "startOffset": 80, "endOffset": 144}], "year": 2017, "abstractText": "Reinforcement Learning (RL) can model complex behavior policies for goaldirected sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state\u2019s value function. \u03bb-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While \u03bb-returns have been extensively studied in RL, they haven\u2019t been explored a lot in Deep RL. This paper\u2019s first contribution is an exhaustive benchmarking of \u03bb-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in \u03bb-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of \u03bb-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, \u03bb-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and \u03bb-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.", "creator": "LaTeX with hyperref package"}}}