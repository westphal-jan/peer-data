{"id": "1705.07460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Experience enrichment based task independent reward model", "abstract": "For most reinforcement learning approaches, the learning is performed by maximizing an accumulative reward that is expectedly and manually defined for specific tasks. However, in real world, rewards are emergent phenomena from the complex interactions between agents and environments. In this paper, we propose an implicit generic reward model for reinforcement learning. Unlike those rewards that are manually defined for specific tasks, such implicit reward is task independent. It only comes from the deviation from the agents' previous experiences.", "histories": [["v1", "Sun, 21 May 2017 15:19:20 GMT  (76kb,D)", "http://arxiv.org/abs/1705.07460v1", "4 pages, 1 figure"]], "COMMENTS": "4 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["min xu"], "accepted": false, "id": "1705.07460"}, "pdf": {"name": "1705.07460.pdf", "metadata": {"source": "CRF", "title": "Experience enrichment based task independent reward model", "authors": ["Min Xu"], "emails": ["mxu1@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Reinforcement Learning [7] focuses on the development of software agents that learn to operate in an environment to maximize long-term cumulative reward. In recent years, the field of Reinforcement Learning has evolved greatly thanks to the revolutions in big data machine learning techniques, in particular deep learning [1]. Supervised deep learning has enabled the learning of complex rules under large amounts of training data. Unsupervised deep learning techniques such as Generative Adversarial Networks (GAN) [2] have proven to be able to capture the intrinsic distribution among the images. [5] The use of deep learning for the value function, policy and model has greatly enhanced the learning field of reinforcement [3]. For example, deep-arcement learning techniques have been used to play Atari games and have surpassed human experts [6]. Most notable are Deep Learning techniques for spontaneous search gaming, which are also performed by GO World Champions. [6]"}, {"heading": "2 Construction of implicate reward model", "text": "We define the \"experience\" of an agent as a sub-sequence of action and observation pairs. For the operational advantage, the length of the sub-sequence is specified. Let's call the experiential space E. Let's call the experiential space a sample of previous experiences gained by the agent through his interactions with the environment. To model the enrichment of experience, we first learn a low-dimensional representation of S by means of unattended learning, such as generative adversarial networks (GAN) [2]. Let's call Rm a euclidean space for a reduced representation of S. Such unattended learning leads to a mapping of M: Rm \u2192 E, which maps a simple multidimensional distribution D (like m-dimensional Gaussian distribution) to the distribution, from the S. We then construct an inverse Figure M \u2212 1: E \u2192 Rm by regression using sample pairs (M (r) r \u00b2 Rm \u00b2), whereby a higher probability density Rexpands \u2212"}, {"heading": "3 Discussion", "text": "We expect that such a purely curious agent using such an implicit generic reward would be able to play certain computer games without knowing the beginning, end, and outcome of the game. This is because there is an intuitive correlation between the richness of the agent's experience and the agent's ability to play a game: in order to obtain an enriched experience, an agent must reach the current level and enter the next level. In order to imitate people who are getting richer in the real world, computer games are often designed to bring new (visual and other) experiences to the next level, and the difficulty of one level often correlates with the complexity of the experiences. Even if a computer game does not introduce a significantly new observation experience to the next level, the agent would learn to adapt to environmental influences and use different types of resources to survive in order to introduce a richer experience to the next level."}], "references": [{"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "http://www.deeplearningbook.org", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Deep reinforcement learning: An overview", "author": ["Yuxi Li"], "venue": "arXiv preprint arXiv:1701.07274,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Researchdoom and cocodoom: Learning computer vision with games", "author": ["A Mahendran", "H Bilen", "JF Henriques", "A Vedaldi"], "venue": "arXiv preprint arXiv:1610.02431,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}], "referenceMentions": [{"referenceID": 6, "context": "Reinforcement learning [7] focus on designing software agents that learn to take actions in an environment in order to maximize of long term cumulative reward.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "In recent years, the field of reinforcement learning has been greatly advanced thanks to the revolutions in big data machine learning techniques, especially deep learning [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Unsupervised deep learning techniques such as Generative Adversarial Networks (GAN) [2] has been shown to be able to capture the intrinsic distribution on the manifold among images.", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "The use of deep learning for learning value function, policy and model has greatly advanced reinforcement learning field [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "For example, deep reinforcement learning techniques have been used for playing Atari games and outperformed human experts [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "Most notably, deep reinforcement learning techniques have also outperformed world champion on playing GO game [6], which has been computationally infeasible through conventional searching techniques due to its huge search space.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "To model the enrichment of experiences, we first learn a low dimension representation of S using unsupervised learning, such as Generative Adversarial Networks (GAN) [2].", "startOffset": 166, "endOffset": 169}], "year": 2017, "abstractText": "For most reinforcement learning approaches, the learning is performed by maximizing an accumulative reward that is expectedly and manually defined for specific tasks. However, in real world, rewards are emergent phenomena from the complex interactions between agents and environments. In this paper, we propose an implicit generic reward model for reinforcement learning. Unlike those rewards that are manually defined for specific tasks, such implicit reward is task independent. It only comes from the deviation from the agents\u2019 previous experiences.", "creator": "LaTeX with hyperref package"}}}