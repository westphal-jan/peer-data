{"id": "1705.07485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Shake-Shake regularization", "abstract": "The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of applications. Code is available at", "histories": [["v1", "Sun, 21 May 2017 18:51:27 GMT  (1504kb,D)", "http://arxiv.org/abs/1705.07485v1", null], ["v2", "Tue, 23 May 2017 13:36:46 GMT  (1504kb,D)", "http://arxiv.org/abs/1705.07485v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xavier gastaldi"], "accepted": false, "id": "1705.07485"}, "pdf": {"name": "1705.07485.pdf", "metadata": {"source": "CRF", "title": "Shake-Shake regularization", "authors": ["Xavier Gastaldi"], "emails": ["xgastaldi.mba2011@london.edu"], "sections": [{"heading": "1 Introduction", "text": "In the ILSVRC & COCO 2015 competitions (Russakovsky et al., 2015; Lin et al., 2014), deep residual networks were introduced for the first time (He et al., 2016a), where they ranked first in the tasks ImageNet detection, ImageNet localization, COCO detection and COCO segmentation, and since then considerable efforts have been made to improve their performance. Scientists have studied the effects of depth shift (He et al., 2016b; Huang et al., 2016a), width (Zagoruyko & Komodakis, 2016) and cardinality (Xie et al., 2016; Szegedy et al., 2016; Abdi & Nahavandi, 2016). While residual networks are powerful models, they still fit into small data networks (Zagal sets, 2014- a large number of techniques, including weight decay (Nowlan & Hinton, 1992)."}, {"heading": "1.1 Motivation", "text": "Data enlargement techniques are traditionally applied only to input images, but for a computer there is no real difference between an input image and an intermediate representation. As a result, it may be possible to apply data enlargement techniques to internal representations.ar Xiv: 170 5.07 485v 1 [cs.L G] 21 May 201 7Shake Shake Regularization was created as an attempt to create this kind of effect by stochastically \"merging\" two usable sensors."}, {"heading": "1.2 Model description on 3-branch ResNets", "text": "Leave xi the tensor of the input into the residual block i. W (1) i andW (2) i are weight sets associated with the two residual units. F denotes the residual function, e.g. a stack of two 3x3 convolutionary layers. xi + 1 denotes the tensor of the outputs from residual block i. A typical pre-activation ResNet with 2 residual branches would follow this equation: xi + 1 = xi + F (xi, W (1) i) + F (xi, W (2) i) (1) suggested modification: If \u03b1i is a random variable after an even distribution between 0 and 1, then during training: xi + 1 = xi + \u03b1iF (xi, W (1) i) + (1 \u2212 \u03b1iF) F (xi, W (2) i) (2)."}, {"heading": "1.3 Training procedure", "text": "As shown in Figure 1, all scaling coefficients are overwritten with new random numbers before each forward pass. A key to this work is to repeat this coefficient update process before each backward pass, resulting in a stochastic mix of forward and backward flows during training, an idea that includes the work of An (1996) and Neelakantan et al. (2015). These authors showed that adding noise to the gradient during training contributes to the formation and generalization of complex neural networks. Shake-shake regularization can be considered an extension of this concept, where gradient noise is replaced by a form of gradient augmentation."}, {"heading": "2 Improving on the best single shot published results on CIFAR", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 CIFAR-10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1 Implementation details", "text": "The shake-shake code is based on fb.resnet.torch1 and is available at https: / / github.com / xgastaldi / shake-shake. The first layer is a 3x3 Conv with 16 filters, followed by 3 levels with 4 residual blocks each. The function board is 32, 16 and 8 for each level. The width is doubled during downsampling, the network ends with an 8x8 average pooling and a completely connected layer (a total of 26 layers deep). Remaining paths have the following structure: ReLU-Conv3x3-BN-ReLU-Conv3x3-BN-Mul. The skip connections represent the identity function except for downsampling, where a slightly adapted structure consisting of 2 linked rivers is used. Each of the two rivers has the following components: 1x1 average pooling with step 2 followed by a convolution. The input of the rivers is shifted by both pixels to the bottom and one pixel."}, {"heading": "2.1.2 Influence of Forward and Backward training procedures", "text": "The base network is a 26 2x32d ResNet (i.e. the network has a depth of 26, 2 residual branches and the first residual block has a width of 32). \"Shake\" means that all scaling coefficients are overwritten with new random numbers before the pass. \"Even\" means that all scaling coefficients are set to 0.5 before the pass. \"Keep\" means that we maintain the scaling coefficients used during the forward pass for the backward pass. \"Batch\" means that we apply the same scaling coefficient for each residual block i for all images in the mini batch. \"Image\" means that we apply a different scaling coefficient for each residual block i for each image in the mini batch (see Image Level Update Procedure below). Image level update procedure: Let x0 think the original input mini-batch tensor of dimensions 128x3x32x32 x32."}, {"heading": "2.2 CIFAR-100", "text": "The network architecture chosen for CIFAR-100 is a ResNeXt without pre-activation (this model yields slightly better results with CIFAR-100 than the model used for CIFAR-10).The hyperparameters are the same as in Xie et al. (2016), except for the learning rate annealed with a Cosine function and the number of epochs increased to 1800.The network in Table 2 is a ResNeXt-29 2x4x64d (2 residual branches with 4 grouped turns, each with 64 channels).Interestingly, due to the 1https: / / github.com / facebook / fb.resnet.torch combination of the larger model (34.4M parameters) and the long training time, fewer tests have been carried out than in CIFAR-10, one important hyperparameter for CIFAR-100 is the batch size, which compared to CIFAR-10 must be reduced from 128 to 32 if the PE does not have to reduce competitiveness by using the PE."}, {"heading": "2.3 Comparisons with state-of-the-art results", "text": "At the time of writing, the best single-shot model on CIFAR-10 is a DenseNet BC k = 40 (3.46% error rate) with 25.6M parameters. The second best model is a ResNeXt-29, 16x64d (3.58% error rate) with 68.1M parameters. A small 26 2x32d \"Shake Even Image\" model with 2.9M parameters achieves roughly the same error rate. This is about 9 times fewer parameters than the DenseNet model and 23 times less parameters than the ResNeXt model. A 26 2x96d \"Shake Shake Image\" ResNet with 26.2M parameters achieves a test error of 2.86% (average of 5 passes - median 2.87%, Min = 2.72%, Max = 2.95%).On CIFAR-100, a few Hyperparameter modifications reduce standard NeX29 run-time from 64x85% to no lead time (no 16.95%)."}, {"heading": "3 Correlation between residual branches", "text": "To check whether the regularization increases or decreases the correlation between the two residual branches, the following test was performed: 1. Forward a mini-batch tensor xi through the rest branch 1 (ReLU-Conv3x3-BN-ReLUConv3x3-BN-Mul (0.5))) and store the output tensor in y (1) i. Do the same for rest branch 2 and store the output in y (2) i.2. Flatten these 2 tensors into vectors flat (1) i and flat (2) i. Calculation the covariance between each corresponded item in the 2 vectors using an online version of the covariance algorithm.3. Calculate the variances of flat (1) i and flat (2) i and flat (2) is corignance algorithm.4 Repeat until all the images in the test set."}, {"heading": "4 Regularization strength", "text": "This section examines what would happen if we gave a branch that would have received a low weight during the forward pass (and vice versa) a high weight during the reverse pass. The first test (Method 1) is the coefficient that would be given during the forward pass for image j in the residual block i. Let \u03b2i.j be the coefficient that was used in the reverse pass for the same image at the same position in the network.The first test (Method 1) is the determination \u03b2i.j = 1 - \u03b1i.j. All tests in this section were performed on CIFAR-10, using 26 2x32d models at the image level. These models are compared to a 26 2x32d shake-keep-image model. The results of M1 can be seen in the left part of Figure 5 (blue curve).The effect is quite drastic and the training error remains really high. Tests M2 to M5 in Table 4 are designed to understand why method 1 (M1) has such a strong effect."}, {"heading": "5 Removing skip connections / Removing Batch Normalization", "text": "An interesting question is whether the skip connection plays a role. Many deep learning systems do not use ResNets too much and this type of regulation works without skip connections could increase the number of potential applications.Table 5 and Figure 6 show the results of eliminating the skip connection. The first variant (A) is exactly like the 26 2x32d used on CIFAR-10, but without the skip connection (i.e. 2 branches with the following components ReLU-Conv3x3-BN-ReLU-Conv3x3-Mul).The second variant (B) is the same as A, but with only one revolutionary layer per branch (ReLU-Conv3x3-BN-Mul) and twice as many blocks. Models using architecture A have been tested once and models using architecture B have been tested twice. The results of architecture A clearly show that shake-shake-shake-shake-shake-shake-shake-regulation can also work without skip connection."}, {"heading": "6 Conclusion", "text": "A number of experiments appear to indicate the ability to combat overmatch by decorating the branches of multi-branch networks, a method that yields state-of-the-art results on CIFAR datasets and could potentially improve the accuracy of architectures that do not use resnets or batch normalization. While these results are encouraging, questions remain about the precise dynamics at play, and understanding these dynamics could help broaden the scope to include a wider variety of complex architectures."}], "references": [{"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["Guozhong An"], "venue": "Neural Comput.,", "citeRegEx": "An.,? \\Q1996\\E", "shortCiteRegEx": "An.", "year": 1996}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ECCV,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Q. Weinberger"], "venue": "In ECCV,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Shakeout: A new regularized deep neural network training scheme", "author": ["Guoliang Kang", "Jun Li", "Dacheng Tao"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Kang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2016}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "venue": "In International Conference on Learning Representation (ICLR", "citeRegEx": "Keskar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Tech Report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Convergent learning: Do different neural networks learn the same representations", "author": ["Yixuan Li", "Jason Yosinski", "Jeff Clune", "Hod Lipson", "John Hopcroft"], "venue": "In International Conference on Learning Representation (ICLR", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "Lubomir D. Bourdev", "Ross B. Girshick", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Sgdr: stochastic gradient descent with restarts", "author": ["Ilya Loshchilov", "Frank Hutter"], "venue": "arXiv preprint arXiv:1608.03983,", "citeRegEx": "Loshchilov and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Loshchilov and Hutter.", "year": 2016}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Steven J. Nowlan", "Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Nowlan and Hinton.,? \\Q1992\\E", "shortCiteRegEx": "Nowlan and Hinton.", "year": 1992}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex A. Alemi"], "venue": "In ICLR 2016 Workshop,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Aggregated residual transformations for deep neural networks", "author": ["Saining Xie", "Ross Girshick", "Piotr Doll\u00e1r", "Zhuowen Tu", "Kaiming He"], "venue": "arXiv preprint arXiv:1611.05431,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Whiteout: Gaussian adaptive regularization noise in deep neural networks", "author": ["Li Yinan", "Xu Ruoyi", "Liu Fang"], "venue": "arXiv preprint arXiv:1612.01490v2,", "citeRegEx": "Yinan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yinan et al\\.", "year": 2016}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": ", 2016a) were first introduced in the ILSVRC & COCO 2015 competitions (Russakovsky et al., 2015; Lin et al., 2014), where they won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "startOffset": 70, "endOffset": 114}, {"referenceID": 19, "context": ", 2016a), width (Zagoruyko & Komodakis, 2016) and cardinality (Xie et al., 2016; Szegedy et al., 2016; Abdi & Nahavandi, 2016).", "startOffset": 62, "endOffset": 126}, {"referenceID": 18, "context": ", 2016a), width (Zagoruyko & Komodakis, 2016) and cardinality (Xie et al., 2016; Szegedy et al., 2016; Abdi & Nahavandi, 2016).", "startOffset": 62, "endOffset": 126}, {"referenceID": 16, "context": "A large number of techniques have been proposed to tackle this problem, including weight decay (Nowlan & Hinton, 1992), early stopping, and dropout (Srivastava et al., 2014).", "startOffset": 148, "endOffset": 173}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al.", "startOffset": 45, "endOffset": 83}, {"referenceID": 17, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al.", "startOffset": 45, "endOffset": 83}, {"referenceID": 10, "context": "Some of them noticed that, given the right conditions, it was possible to randomly drop some of the information paths during training (Huang et al., 2016b; Larsson et al., 2016).", "startOffset": 134, "endOffset": 177}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)).", "startOffset": 46, "endOffset": 274}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)). Pre-2015, most computer vision classification architectures used dropout to combat overfit but the introduction of Batch Normalization reduced its effectiveness (see Ioffe & Szegedy (2015); Zagoruyko & Komodakis (2016); Huang et al.", "startOffset": 46, "endOffset": 465}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)). Pre-2015, most computer vision classification architectures used dropout to combat overfit but the introduction of Batch Normalization reduced its effectiveness (see Ioffe & Szegedy (2015); Zagoruyko & Komodakis (2016); Huang et al.", "startOffset": 46, "endOffset": 495}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)). Pre-2015, most computer vision classification architectures used dropout to combat overfit but the introduction of Batch Normalization reduced its effectiveness (see Ioffe & Szegedy (2015); Zagoruyko & Komodakis (2016); Huang et al. (2016b)).", "startOffset": 46, "endOffset": 517}, {"referenceID": 10, "context": "This method can be seen as a form of drop-path (Larsson et al., 2016) where residual branches are scaled-down instead of being completely dropped (i.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "Replacing binary variables with enhancement or reduction coefficients is also explored in dropout variants like shakeout (Kang et al., 2016) and whiteout (Yinan et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 20, "context": ", 2016) and whiteout (Yinan et al., 2016).", "startOffset": 21, "endOffset": 41}, {"referenceID": 0, "context": "Related to this idea are the works of An (1996) and Neelakantan et al.", "startOffset": 38, "endOffset": 48}, {"referenceID": 0, "context": "Related to this idea are the works of An (1996) and Neelakantan et al. (2015). These authors showed that adding noise to the gradient during training helps training and generalization of complicated neural networks.", "startOffset": 38, "endOffset": 78}, {"referenceID": 9, "context": "Models were trained on the CIFAR-10 (Krizhevsky, 2009) 50k training set and evaluated on the 10k test set.", "startOffset": 36, "endOffset": 54}, {"referenceID": 9, "context": "Models were trained on the CIFAR-10 (Krizhevsky, 2009) 50k training set and evaluated on the 10k test set. Standard translation and flipping data augmentation is applied on the 32x32 input image. Due to the introduced stochasticity, all models were trained for 1800 epochs. Training starts with a learning rate of 0.2 and is annealed using a Cosine function without restart (see Loshchilov & Hutter (2016)).", "startOffset": 37, "endOffset": 406}, {"referenceID": 19, "context": "Hyperparameters are the same as in Xie et al. (2016) except for the learning rate which is annealed using a Cosine function and the number of epochs which is increased to 1800.", "startOffset": 35, "endOffset": 53}, {"referenceID": 11, "context": "One problem to be mindful of is the issue of alignment (see Li et al. (2016)).", "startOffset": 60, "endOffset": 77}], "year": 2017, "abstractText": "The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of applications. Code is available at https://github.com/xgastaldi/shake-shake.", "creator": "LaTeX with hyperref package"}}}