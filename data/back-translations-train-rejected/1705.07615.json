{"id": "1705.07615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "AIXIjs: A Software Demo for General Reinforcement Learning", "abstract": "Reinforcement learning is a general and powerful framework with which to study and implement artificial intelligence. Recent advances in deep learning have enabled RL algorithms to achieve impressive performance in restricted domains such as playing Atari video games (Mnih et al., 2015) and, recently, the board game Go (Silver et al., 2016). However, we are still far from constructing a generally intelligent agent. Many of the obstacles and open questions are conceptual: What does it mean to be intelligent? How does one explore and learn optimally in general, unknown environments? What, in fact, does it mean to be optimal in the general sense? The universal Bayesian agent AIXI (Hutter, 2005) is a model of a maximally intelligent agent, and plays a central role in the sub-field of general reinforcement learning (GRL). Recently, AIXI has been shown to be flawed in important ways; it doesn't explore enough to be asymptotically optimal (Orseau, 2010), and it can perform poorly with certain priors (Leike and Hutter, 2015). Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al., 2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike, 2016a), Thompson sampling (Leike et al., 2016), and optimism (Sunehag and Hutter, 2015). We present AIXIjs, a JavaScript implementation of these GRL agents. This implementation is accompanied by a framework for running experiments against various environments, similar to OpenAI Gym (Brockman et al., 2016), and a suite of interactive demos that explore different properties of the agents, similar to REINFORCEjs (Karpathy, 2015). We use AIXIjs to present numerous experiments illustrating fundamental properties of, and differences between, these agents.", "histories": [["v1", "Mon, 22 May 2017 08:56:54 GMT  (2675kb,D)", "http://arxiv.org/abs/1705.07615v1", "Masters thesis. Australian National University, October 2016. 97 pp"]], "COMMENTS": "Masters thesis. Australian National University, October 2016. 97 pp", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["john aslanides"], "accepted": false, "id": "1705.07615"}, "pdf": {"name": "1705.07615.pdf", "metadata": {"source": "CRF", "title": "AIXIjs: A Software Demo for General Reinforcement Learning", "authors": ["John Stewart Aslanides"], "emails": [], "sections": [{"heading": null, "text": "AIXIjs: A software demo for general reinforcement training John Stewart Aslanides"}, {"heading": "A thesis submitted in partial fulfillment of the degree of", "text": "Master of Computing (Advanced) at the Australian National University October 2016ar Xiv: 170 5.07 615v 1 [cs.A I] 2 2M ay2 017iDeclarationThis diploma thesis is a report on research carried out between March 2016 and October 2016 at the Research School of Computer Science, The Australian National University, Canberra, Australia. Except for the usual recognition, to my knowledge the material presented in this thesis is original and has not been submitted in full or in part for a university degree. John Stewart Aslanides October 27, 2016 Supervisor: \u2022 Dr. Jan Leike (Future of Humanity Institute, University of Oxford) \u2022 Prof. Marcus Hutter (Australian National University) Convenor: \u2022 Prof. John Slaney (Australian National University) iiiiv"}, {"heading": "Acknowledgements", "text": "And so, my formal education is coming to an end, at least for now. Of course, you can't take credit for your achievements any more intelligently than you can take credit for your genes and environment. I owe everything to the happiness of growing up in a prosperous and peaceful country (Australia), with loving and well-educated parents (Jenny and Timoshenko), and the fact that I was exposed to the quality of teaching and care I received over the years at the Australian National University. In my time at ANU, I met many intelligent people who, to varying degrees, have driven and shaped my intellectual development, as I think they are. They are (in order of appearance): Craig Savage, Paul Francis, John Close, Joe Hope, Ra Williamson, Justin Domke, Christfried Webers, Jan Leike, and Marcus Hutter. To these mentors and teachers, past and present: Thank you to Jan, my supervisor: Thank you."}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Background 7", "text": "The death toll rose to 32. The death toll rose to 32. The death toll rose to 32. The death toll rose to 34. The death toll rose to 34. The death toll rose to 34. The death toll rose to 34."}, {"heading": "3 Implementation 31", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "4 Experiments 55", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5 Conclusion 73", "text": "The transition matrix (a) is a double reward (2)."}, {"heading": "2 Introduction", "text": "They are usually only applicable within a narrow range; a neural network in which cats cannot play chess 2. Reason are only general tasks. They are only constructed in such a way as to punish prediction errors Bishop (2006); Hastie et al. (2009); Hastie et al. (2009); Murphy (2012). High-profile breakthroughs in statistical machine learning include image recognition (Szegedy et al., 2015), speech recognition (Sak et al., 2015), synthesis (van den Oord et al., 2016), and machine translation (Al-Rfou et al., 2016). Many other examples exist, in various areas such as anti-fraud (Phua et al., 2010) and bioinformatics (Libbrecht and Noble, 2015). These systems could be called \"intelligent,\" insofar as they learn an accurate model of the world, which also generalizes invisible data into a narrow range 1. They are invisible to a narrow learning system (we can only refer to two directions)."}, {"heading": "4 Introduction", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to survive on their own."}, {"heading": "6 Introduction", "text": "The alt software itself can be found at http: / / aslanides.github.io / aixijs \u2022 These three conveniences used to be used \u2022 They can be run in the browser on any operating system. Note that different browsers have different implementations of the JavaScript specification; we strongly recommend running the demo on Google Chrome5, as we do not test the implementation on other browsers. In Chapter 2 (Background), we present the theoretical framework for general reinforcement learning, introduce the Agent Zoo and present the basic optimization results. In Chapter 3 (Implementation), we document the design and implementation of the software itself. In Chapter 4 (Experiments), we outline the experimental results we have obtained with the software. Chapter 5 (Summary) makes some final comments and indicates potential directions for further work. We expect this thesis to be typically read more softly, i.e., by a PDF viewer."}, {"heading": "2.1 Preliminaries", "text": "We assume that the reader has a basic familiarity with the concepts of probability, information theory, and statistics, and ideally has some contact with standard concepts in artificial intelligence (e.g. broadband search, expectimax, minimax) and reinforcement learning (e.g. Q-learning, bandits). For a general background, we refer the reader to MacKay (2002) for probability and information theory, Bishop (2006) for machine learning and statistics, Russell and Norvig (2010) for artificial intelligence, and Sutton and Barto (1998) for reinforcement theory."}, {"heading": "2.1.1 Notation", "text": "Numbers and vectors. The set N = {1, 2, 3,..} is the set of natural numbers, and R denotes the real numbers. We use R + = [0, \u221e) and R + + = (0, \u221e). A set is countable if it can be bijected with a subset (finite or not) of N, and is uncountable 7"}, {"heading": "8 Background", "text": "We use RK to represent K-dimensional vector space over R. We represent vectors with bold faces: x is a vector, and xi is its i-th component. We (reluctantly 1) represent internal products with the standard technical and computer notation: given x, y, RK, xTy = \u2211 Ki = 1 xiyi.Strings and sequences. Define a finite, non-empty series of symbols X, which we call the alphabet. The set X n with n \u00b2 N is the set of all strings over X with length n, and X \u00b2 n is the set of all finite strings over X. X \u00b2 is the set of infinite strings over X, and X # = X \u00b2 X \u00b2 X \u00b2 X \u00b2 is their union. The empty string is denoted by; this is not to be confused with the small positive number II."}, {"heading": "2.1.2 Probability theory", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) In fact it is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (...) It is. (... It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. (...) It is. It is. (... It is. It is. It is. (...) It is. It is. It is. (... It is. It is. It is. It is. It is. It is. (...) it is. It is. It is. It is. It is. (...) it is. It is. (... it is. It is. It is. It is. It is. (). It is. It is. It is. It is. (... it is. It is. It is. It is. (). It is. It is. It is. (). It is. It is. It is. It is. It is."}, {"heading": "10 Background", "text": "We use U (a, b) to represent the measure that assigns a uniform density to the closed interval (a, b), with b > a; its density is given by p (a) = 1b \u2212 aI [a \u2264 x \u2264 b]. We use N (\u00b5, \u03c32) to represent the univariate Gaussian distribution on Rmit with the density given by p (x); its mass function is given by p (a) = 1 | Normal. We use N (\u00b5, \u03c32) to represent the univariate Gaussian distribution on Rmit with the density given by p (x)."}, {"heading": "2.1.3 Information theory", "text": "(2.4) Without additional constraints, the maximum entropy distribution is U, our general uniform distribution. We also define conditional entropy (p (\u00b7 | y)). = x x x: p (x) > 0p (x | y) log p (x | y). For two distributions p, q and x, Kullback Leibler divergence (KL divergence, also known as relative entropy) is defined by KL (p-q). = x x-x-X: p (x) > 0, q (x) > 0p (x) log p (x) q (x).We use the < < < < symbol to separate the arguments to emphasize that the KL divergence is not symmetrical, and hq (x) if a deviation cannot be measured: p (x), p (l-ltltltltlt) q (x), p-b (x)."}, {"heading": "2.2 Reinforcement Learning", "text": "In contrast to machine learning, the training data the system receives as part of enhanced learning now depends on its actions; we therefore introduce the ability to act into the learning problem (Sutton and Barto, 1998). What observations the agent can make and what he can therefore learn no longer depends solely on the environment (as in machine learning)."}, {"heading": "12 Background", "text": "In this way, reinforcement learning generalizes machine learning considerably; we replace the loss function of the equation (1.1) with a reward signal. Now, instead of minimizing risk, the agent must strive to maximize future expected rewards. Thus, reinforcement learning generalizes machine learning to the active environment so that the agent can now influence his environment with the actions he takes. We distinguish this from the associated setup known as inverse reinforcement learning or imitation learning (Abbeel and Ng, 2004), where the agent receives training data consisting of a history of actions and perceptions from which he must derive a policy. By contrast, reinforcement students must take their own actions and learn through trial and error - they are monitored only to the extent that their extrinsic reward signal gives them feedback on their policies."}, {"heading": "2.2.1 Agent-environment interaction", "text": "In the Standard Cyber model (see Figure 2.1), the agent and the environment are separated entities that play a turn-based two-player game. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p p > p p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "2.2.2 Discounting", "text": "In the context of learning reinforcements, we would like our agent to act in accordance with a policy that maximizes the reward accumulated during his life. Generally, it is not good enough to greedily maximize the reward received in the next step of time, as this will in many cases lead to a reduction in the total reward. Thus, we define the return resulting from the execution of \u03c0 policy in the environment from time to time t as the sum of future rewards ri.R\u03c0\u03bd (\u00e6 < t) = \u221e \u0445 k = t rk, in which each rk is scanned from the comparison (\u00b7 | \u03c0, \u00e6 < k); therefore, the return is a random variable that depends on the policy of the agent, the environment \u03bd, and the history \u00e6 < t. In general, this sum will diverge, so that in practice we will deal with either the average reward sum \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0442\u0442\u0438 = mn \u2192 \u043c rk = rk"}, {"heading": "16 Background", "text": "Or the discounted RetournR\u03c0\u03bd\u03b3 (\u00e6 < t) = \u221e \u2211 k = 1 \u03b3tkrk, where \u03b3tk \u2264 1 is a generalized discount function \u03b3t: N \u2192 [0, 1]. Definition 7 (\u03b5-Effective Horizon; Lattimore and Hutter (2014a)). Taking into account a discount function \u03b3, the \u03b5-effective horizon of Ht\u03b3 (\u03b5) is given. = min {H: \u0445t + H\u03b3-Effective Horizon represents the future distance that the agent can plan while still taking into account a proportion of the available yield equal to (1 \u2212 \u03b5). Choosing the discount function is relevant to how the agent plans; some discount functions will make the agent farsighted, and others will relate this discount function to the current 4-cycles."}, {"heading": "2.2.3 Value functions", "text": "In general, environmental policy is noisy and stochastical, and the policy of action will often be stochastical. As a result, we cannot directly maximize the discounted return; we must instead maximize it in anticipation, according to the Neumann-Morgenstern usage theorem (Morgenstern and von Neumann, 1944). Definition 8 (value function): The value V \u03c0\u03bd: (A \u00b7 E). < R of a story \u00e6 < t in environmental policy with a discount function is the expected sum of discounted future returns. < t. (E) < k = t tactkrk: (A) < t < t in environmental policy. < t) In which we use the expectation with respect to these values, the expectation with respect to these values is. < V (2.5). Equation (2.7) in the value function in iterative form. We can also express it recursively (Leike, these values 2016)."}, {"heading": "2.2.4 Optimality", "text": "Unofficially, it makes sense to judge the performance of a remedy against that of the optimal policy when it is placed in the same situation. We can only talk about this performance asymptotically in general, i.e. in the limit t \u2192 \u221e, because the active substance takes time to learn the environment, and we cannot judge the active substance after a finite time t, since this time would be generally dependent on the environment. Definition 10 (Asymptotic Optimism; Lattimore and Hutter, 2011): A policy measure is highly asymptotically optimal in environmental class M if it is applied in the limit class M, if it is located in the limit class M. Definition 10 (Asymptotic Optimism; Lattimore and Hutter, 2011): A policy is highly asymptotically optimal if it is located in the limit class M, if it is located in the limit class M. Policy is weakly asymptotically optimal in M if it is located in the limit class M."}, {"heading": "18 Background", "text": "Finally, we say that \u03c0 in the mean above M is asymptotically optimal when we fall into the trap (Leike, 2016a), but unfortunately not everything we want in an agent will be asymptotically optimal. For example, in traps - that is, in an accepting state with no transitions and very little reward - any policy, after falling into the trap, will be asymptotically optimal, since no policy will surpass another because it is trapped. Furthermore, an agent cannot be asymptotically optimal in unsafe environments with traps unless he is sufficiently kung-ho in his research to eventually fall into a trap (Leike, 2016a). Therefore, we should take asymptotic optimism with a grain of salt; it is not a particularly good measure of optimism in general environments. The pursuit of good concepts of optimality is currently an open problem in the GR 6L theory (Leike, 2015)."}, {"heading": "2.3 General Reinforcement Learning", "text": "We start with AI\u00b5, which is simply the policy of the informed agent who has a perfect model of the environment \u00b5: Definition 11 (AI\u00b5). AI\u00b5 corresponds to the policy in which the true environment \u00b5 is known to the agent, and therefore no learning is required. Optimal behavior is reduced to the planning problem of calculating the \u00b5-optimal policy AI\u00b5 = \u03c0 \u00b5. = arg max \u03c0V \u03c0\u00b5. The discerning reader will find that \u03c0AI\u00b5 is simply the optimal policy for the environment \u00b5; we introduce it here as a separate agent to have a yardstick to which we can compare our other reinforcement learners. In general, the environment will be unknown, and so our agents will have to learn it. In order to investigate the general problem of reinforcement learning, we consider primarily Bayesian agents, since they are the most general and principled way of thinking about the problem of induction (Hutter, 2005)."}, {"heading": "2.3.1 Bayesian agents", "text": "Our Bayean agents maintain a Bayesian mix or predictive distribution. < This is the probability that the agent model will be assigned to an e; in other words, it is the agent's predictive model. (In other words, it is the agent's predictive model.) It is the agent's predictive model. (In other words, it is the agent's predictive model. (In other words.) The weights that use the agent's model correspond to the notion of normalization in Theorem 1; it represents the probability that the agent's model will be assigned to an e. (In other words.) It is the agent's predictive model. (In other words.) It is the agent's predictive model."}, {"heading": "20 Background", "text": "Where U is a universal Turing machine (Li and Vita \u0301 nyi, 2008). For each computable environment there is a corresponding Turing machine T, so that we can define the K (\u03bd) as the Kolmogorov complexity of its index. < This leads to the famous equation that describes the AIXI policy unfolding in all its computable glory: aAIXIt = arg maxat lim \u2192 \u00b7 \u00b7 max at + m \u00b2."}, {"heading": "2.3.2 Knowledge-seeking agents", "text": "There are several motivations for defining and investigating KSA: \u2022 They provide a way to construct a purely \"explorative\" policy. \u00b7 A principle-driven solution to exploration through intrinsic motivation is one of the central problems in reinforcement learning (Thrun, 1992). \u2022 They eliminate dependence on arbitrary reward signals or utility functions; up to a selection of model class and previous \"knowledge\" is an objective quantity (Orseau, 2011). \u2022 They collapse the exploration and benefit effect trade-off to mere exploration. Before formally defining the knowledge-seeking agents, it is necessary to introduce the concept of a benefit effect that agents the concept of enhanced learning. \u2022 Definition 14 (Utility Agent; Orseau, 2011). A benefit effect is an agent equipped with a limited utility function: (A \u00d7 E)."}, {"heading": "22 Background", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; &"}, {"heading": "24 Background", "text": "In this way, we can make the Square and Shannon KSAs \"addicted to noise\" - they would be endlessly fascinated by a white noise generator like an out-of-tune TV and would never tire of watching the random, improbable events. We construct an experiment to explore this property of KSA agents in Chapter 4."}, {"heading": "2.3.3 BayesExp", "text": "The idea behind the agent BayesExp is simple. Given that KL-KSA is effective in exploration and AIE is effective (by design) in exploiting the agent's beliefs as they are, why not combine the two in some way? The algorithm for operating BayesExp is simple: execute the knowledge-seeking policy for an effective horizon by calculating the \"optimal\" policy as normal, but always calculating the value of the information-seeking policy as CSA. If the expected gain in information (up to a certain horizon) exceeds a certain threshold, execute the knowledge-seeking policy for an effective horizon. This combines the best of AIE and CSA by conducting exploration storms when the agent's beliefs suggest that the time is right; therefore, BayesExp breaks out of the suboptimal exploration strategy of Bayes, but without resorting to ugly hayrids like -greed."}, {"heading": "2.3.4 MDL Agent", "text": "While AIXI uses the principle of epicurus to mix all consistent environments, the minimum description length (MDL) of the agent is greedy for the simplest, unfalsified environment- \u00a7 2.3 General Reinforcement Learning 25Algorithm 2.1 BayesExp (Lattimore, 2013) Inputs: Model class M = {\u03bd1,...,.K}; w: M \u2192 (0, 1); Exploration Plan {\u03b51, \u03b52,.} 1: t \u2190 1 2: Loop 3: d \u2190 Ht (\u03b5t) 4: if V \u043a, IG (\u00e6 < t) > \u03b5t then 5: for i = 1 \u2192 d do 6: act (\u03c0?, IGE) 7: End for 8: otherwise 9: End for 9: End if 11: End loopment in its model class and behaves optimally in relation to that environment until it distorts it. In other words: the policy is given by MDL."}, {"heading": "2.3.5 Thompson Sampling", "text": "Thompson sampling is a very common Bayesian sampling technique named after Thompson (1933), which can be used in conjunction with general reinforcement learning as another attempt to solve AIBA's exploration problems. Informal, the idea is to use the \u03c1-optimal approach for an effective horizon before re-sampling from the rear \u03c1 \u0445 w (\u00b7 | \u00e6 < t) is performed and repeated - at any time the agent updates it as usual, binding the agent to a single hypothesis for a considerable time; it can be considered probable."}, {"heading": "26 Background", "text": "Theorem 7 (Leike et al., 2016). Thompson sampling is asymptotically optimal in mean in general environment."}, {"heading": "2.4 Planning", "text": "Of course, the other important aspect of artificial intelligence that differs from learning is action. Remember that when the environment is known, calculating the optimal policy becomes a planning problem. In general (stochastic) environments, this involves calculating the optimal value V \u0445 \u00b5, the Expectimax expression from Equation (2,9). In practice, of course, we have to approximate this Expectimax calculation to a certain finite horizon m. For finite Markov decision-making processes with known transitions and rewards and using geometric discounting, we can calculate this using a simple dynamic programming algorithm called Value Iteration (Section 2.4.1). For more general environments, we have to approximate it by \"brute force,\" using Monte Carlo sampling (Section 2.4.2)."}, {"heading": "2.4.1 Value iteration", "text": "When the state transitions P (s) | s, a) and the reward function R (s, a) are known, we can plan ahead in a finite MDP by value repetition: Vn + 1 (s) = max a) A Qn (s, a), (2,17) with Qn (s, a) = R (s, a) + \u03b3 s \u00b2 S P (s, a) Vn (s). (2,18) This is done as a dynamic programming algorithm, and it is known that the value repetition constant - \u00a7 2.4 Planning 27 is based on the value of the optimal policy (Sutton and Barto, 1998): lim n \u2192 \u221e Vn (s) = V \u0445 (s) s. \"Planning after value repetition relies heavily on two strong assumptions: the finite MDP assumption and geometric discounting. We would like to be able to override these assumptions for the purpose of our experiments in GRE, so that we now turn our attention to Monte Carlo."}, {"heading": "2.4.2 MCTS", "text": "Monte Carlo tree search (MCTS) is a general technique for approximating an expectimax calculation in stochastic games and deterministic games with uncertainty. Its use dates back several decades, but has been popularized and formalized over the last decade. (Browne et al., 2012) Analogous to minimax (Russell and Norvig, 2010), we construct a game tree with Max (the agent) playing one round, and Environment (some distribution over perceptions) playing the other round. The branching factor of the max nodes is, of course, | A |, while the branching factor of the environmental nodes is a cap on the exploitation of | E |. Unlike minimax, which is used for deterrent games, we need to collect sufficient samples from environmental nodes to get a good estimator V for the expected value for this node."}, {"heading": "28 Background", "text": "(2011) presents this generalization, in its famous MC-AIXI-CTW implementation paper, based on previous work in Monte Carlo Planning on partially observable MDPs (Silver and Veness, 2010). Using this algorithm, we do not need to know the state transitions required for value titeration (Equation (2,18)); instead, we only need some black box environment models (Silver and Veness, 2010). The actual UCT action selection within each tree search decision node is given to us by ByaUCT = arg max a-A (1). (\u03b2 \u2212 \u03b1) V-Process (ta) + C-Log T (UClt; t) T (2,20), where the reward margin is, and m is the planning horizon; collectively they are used to normalize averaging."}, {"heading": "30 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.5 Remarks", "text": "We conclude with a brief summary and some remarks. In this chapter, we presented the problem of general reinforcement learning, in which the goal is to construct an agent capable of learning optimal policies in a broad class of (partially observable and non-ergodic) environments. We presented the current state-of-the-art GRL agents and algorithms, namely AISean, Thompson sampling, MDL, Square, Shannon and Kullback Leibler KSA, and BayesExp, under a unified notation, and we discussed the ideas and algorithms that enable these agents to learn and plan. These agents and the analysis and formalism around them represent our best theoretical understanding of rationality and intelligence in this general environment. In the following two chapters, we present our software implementation of these agents and some of the experiments that we are conducting."}, {"heading": "32 Implementation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 JavaScript web demo", "text": "We implement AIXIjs as a static website. That is, apart from web hosting for the.html and.js source code and other site assets, there is no back-end server required to run the software; the demo runs natively and locally, in the user's web browser. All agent environment simulations are implemented in modern JavaScripts (ECMASScript 2015 specification), with minimal use of external libraries. This allows us to effectively build a lightweight and portable software suite that is executed without the need for specialized dependencies such as compilers or scientific libraries. JavaScript (JS) is a high-level, dynamically and weakly typed language used to create dynamically dynamic content on websites."}, {"heading": "3.2 Agents", "text": "All agents inherit from the base agent class. Each agent base class defines the methods \u2022 Update (a, e).Update the agent model of the environment because it has just carried out an action and received an email inheritance from the environment. \u2022 SelectAction ().Calculate and sample the agent's (generally stochastic) policy. (a) < t Return an action. \u2022 Utility (e) This is the usage function of the agent as defined in Definition 14. It simply extracts the reward component of the agent's policy. (a) < p > < p > p > p > p > p > p > p p > p p > p p p > p p p > p p > p p p > p p > p p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "34 Implementation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Approximations", "text": "In both cases, we argue that our use of these simplifications leaves the agent's policy unchanged. Indeed, the third simplification is imposed on us; however, it is inconvenient and potentially highly consequential to Shannon-KSA's performance. \u2022 Gain information from Chapter 2 that the information gain for a Bayesian agent in the face of a story < t isIG (e < tat). = Ent (w (\u00b7 \u00e6 < t) \u2212 Ent (\u00b7 \u00e61: t), and remember that this is the usefulness function of the Kullback Leibler Search Agent (KL-KSA). Now, when we calculate the KL-KSA policy in due course - that is, in calls to SelectAction - we calculate the value of different histories. < < < < < <"}, {"heading": "36 Implementation", "text": "It is unlikely that his estimates will blow up, resulting in a suboptimal selection of plans, as the V value will exceed the exploration bonus term in Equation (2,20). We are forced to choose a \u03b2, so we use a very large upper limit, \u03b2 = 103, to offset this trade-off, but skew it in favor of an overestimate of \u03b2. To exceed the probability of --103 in log2, we must assign a probability of \u0432 (e) \u2264 2 \u2212 103 = 10 \u2212 301 that approaches the limits of numerical precision in JavaScript. With this setting of \u03b2, we are unlikely to exceed our estimate, although we will greatly inflate the UCB constant. As we will see in Chapter 4, this is certainly the cause of some suboptimal behavior in the Shannon CSA."}, {"heading": "3.3 Environments", "text": "Remember that AIXI and its variants are theoretical models of unlimited rationality, not practical algorithms. Bayesian learning and planning through forward simulation with the Monte Carlo tree search are both very mathematically demanding, so we limit ourselves to demonstrating their properties on small POMDPs and MDPs. Similar to the case of agents, all environments inherit from the base environmental class. The designer of each environment takes as input an option object that allows us to pass default and user-specific options. The base environmental class specifies the methods \u2022 ConditionalDistribution (e). Returns the probability calculation (et | ae < tat) that perceives the environment e given its current state resulting from the History\u00e6 & lttat. \u2022 GeneratePercept ()."}, {"heading": "3.3.1 Gridworld", "text": "There are four types of tiles: Empty, Wall, Dispenser and Trap with the following properties: \u2022 Empty tiles let the agent pass, albiet and cause a small movement penalty rEmpty. \u2022 Wall tiles are not traversable. If the agent goes into a wall, he causes a negative penalty rWall < rEmpty. \u00a7 3.3 Environments 37 \u2022 Dispenser tiles behave like empty tiles as far as movement and observations are concerned, but they forgo a large reward rCake rEmpty with probability, and rEmpty otherwise; that is, all dispensers are (scaled) Bernouilli processes. 11 \u2022 Trap tiles, as the name suggests, do not allow you to leave the world. In addition, once we are trapped, the agent rWall will receive reward."}, {"heading": "38 Implementation", "text": "We ensure that the mesh world is solvable by ensuring that there is at least one dispenser, and by conducting a wide-ranging search to see if there is a viable route from the starting point of the agent to the dispenser with the highest payout frequency. This mesh world is sufficiently rich and interesting to demonstrate most of what we want to show: the agents must be reasonable under uncertainty in order to navigate the maze and find the (best) dispenser while avoiding traps. We report on numerous experiments that use this environment in Chapter 4."}, {"heading": "3.3.2 Chain environment", "text": "We present a deterministic version of the chain environment of Strens (2000). The chain environment is a deterministic finite Markov decision-making process. The action space is A = {\u2192, 99K}, and the state space is | S | = N + 1, for some integer N \u2265 1. The reward space is {r0, ri, rb} with r0 < ri rb; sample values are (r0, ri, rb) = (0, 5, 100), with N = 6. From Figure 3.5 we can see that the actor is always tempted to reap an immediate reward from ri by taking the \u2192 action that puts him in the initial state, losing any progress he made on the path to sN from which he can take 99K, which is not immediately considered a reward as \u2192, but ultimately leads to a very large reward of this environment. For N < rbri, the optimal policy is always to take 99K to complete the circuit 1."}, {"heading": "3.4 Models", "text": "As we have seen in Chapter 2, the GRL agents we are dealing with are model-based and Bayesian methods. In this section, we describe the generic BayesMixture model, which provides a wrapping around any model of class M that is presented as an array of environments, and which allows us to calculate the Bayes equation mix (2.10), then we describe a model class for grid worlds that we attach to this BayesMixture model, and a separate dirichlet model. The BayesMixture model provides us with a mechanism by which we can use any array of hypotheses (\u03bd1, \u03bd2,.,.) and a previous one (w1,.., w | M |). [0, 1] | M | as a Bayesian environment model. Note that all environmental models must implement the environment interface, namely: must (generate methods, and distribution)."}, {"heading": "40 Implementation", "text": "\u2022 Update (a, e): We update our rear given percept e using the Bayes rule: w (\u03bd | e) = w (\u03bd) \u03bd (e) \u0445 (e) for each of these categories. Our goal is to construct a Gridworld model that is sufficiently informed or restricted to allow the agent to solve the environments we give him within approximately one hundred cycles of agent-environment interaction, but that is also sufficiently rich and general to make it interesting to watch the agent learn. For this reason, we do without very general and flexible models such as the famous context-tree weighting data compressor used by Veness et al. (2011), as it will take too long to learn the environments for a practical demonstration. Instead, we construct two models with different levels of domain knowledge built in: 1. a blending model that is parameterized by the dispenser location, we call the dilocator-2."}, {"heading": "3.4.1 Mixture model", "text": "That is, we want a simple and principled method by which we can make a limited but non-trivial set of hypotheses about the nature of the real environment in the real world. We do this by making some discrete parameterizations of the real world environment. One can imagine that it describes a set of parameters about which the agent is uncertain; all other parameters are kept constant, and the agent is fully informed about their values. We will now consider and implement three different options for parameterization D, and enumerate some of the pros and constellations for each."}, {"heading": "42 Implementation", "text": "We find in practice that this amounts to similar problems to the second parameterization and produces demos that are slow (due to the size of the model class; see Section 3.7 for a discussion of time complexity) and uninteresting because the agent is able to falsify so many hypotheses at once. Algorithm 3.2 Setup of the dispenser dispenser agent: for i = 1 to do N 5: for j = 1 to do N 6: uncertainty; Gridworld dimensions N: Outputs: model class M and uniformity before w1: w \u2190 Zeros (N2) 2: M \u2190 Dispensator agent: for i = 1 to do N 5: for j = 1 to do N 6: constant Productions Initialize (E, rolle) 7: for model class M and uniformity before w1: Zeros (N2) 2: M \u2190 3: Dispensator: for the demographic model will do Griensable agent: for i = 1 to do N 6: constant Productions Initialize (E) 7: for model class M and uniformity before w1: Zeros (N2) 2: M \u2190 3: Dispensator: for the demographic model will do dispenser agent: for i = 1 to do N 6: constant Products Initialize (E: when we do the model N: 9 to do N: 6)."}, {"heading": "3.4.2 Factorized Dirichlet model", "text": "We now describe an alternative Gridworld model that has several desirable properties. In contrast to the na\u00efve blending model, it allows us to efficiently represent the uncertainty of the labyrinth layout, donor locations and payout frequencies, which means that MDirichlet is a relatively unrestricted and therefore harder to learn model compared to Mloc. The basic idea is to model each tile model in Gridworld independently with a categorical distribution across the four possible types of tiles: Empty, Wall, Dispenser and Trap. For a N \u00d7 N Gridworld, we label each tile model sij, where i, j,.., N}. Common distribution across all Gridworlds s11,.. sNN is then ensensed by the product system (s11,., sNN) = (N) enser."}, {"heading": "44 Implementation", "text": "This means that we have the empirical count of each class, and 1Tp = 1. Updates to the back values are trivial: only an increase in the corresponding number, i.e. if we see an instance of class N, we update Dirichlet (p).1,., 2, 3, 4, 5, 5, 6, 6, 6, 7, 6, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11."}, {"heading": "46 Implementation", "text": "n times, then the rear belief of the agent that s is actually empty is simplePr (s = empty) = n + 1n + 2, (3.5), which can easily be recognized by applying the rear Dirichlet update (equation (3.2)) n times. Thus, the agent asymptotically learns the truth as n \u2192 \u221e, but for each finite n the model still has a certain degree of uncertainty. This Dirichlet model has numerous distinct advantages: it allows the agent to discover the grid layout during his research, display multiple dispensers, and learn online the Bernoulli parameter approach for each dispenser d by maintaining a simple laplace estimator of the probabilities Pr (d = empty) and Pr (d = dispenser). It also makes for an interesting visualization, since we can show the grid world to the user as the agent discovers it."}, {"heading": "3.5 Planners", "text": "The interested reader can read the source code for our implementation of these algorithms. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p."}, {"heading": "48 Implementation", "text": "Since it is a Monte Carlo algorithm, its output is stochastical, meaning that the resulting policy is stochastical. In a limited number of samples, the agent's policy can vary widely and be contradictory. At the C-0 boundary, the agent's policy clearly becomes a random walk, and to the extent that the agent's policy converges with \u03c0 (Veness et al., 2011). \u2022 The choice of UCT parameter C is consequential; remember from Equation (2.20) that it determines how much of the exploration bonus should be weighted in the tree selection routine. Low values of C correspond to low exploration in the simulation and lead to deep trees and farsighted plans. Conversely, high values of C lead to short, bushy trees and greedy (but statistically better informed) plans (Veness et al., 2011)."}, {"heading": "3.6 Visualization and user interface", "text": "We will now describe the design and implementation of the front end of the web demo. The user will first be presented with the About page, which provides an overview and introduction to the background of general reinforcement learning, including the definitions of each agent; we will essentially present a less formal and shortened version of Chapter 2. Using the buttons at the top of the page, the user can navigate to the Demos page, where he will find a selection of demos to choose from; see Figure 3.10. When the user clicks on one of the demos, the web app opens an interface similar to the one shown in Figure 3.9. This interface allows the user to select agents and environment parameters in the setup section of the user interface, or simply use the default parameters provided. Once the parameters are selected, the agent environment simulation is started by clicking Run. At this point, the agent-environment interaction loop (Algorithm 3.3) will begin, and a few seconds will be required depending on the choice and the time of the simulation."}, {"heading": "3.7 Performance", "text": "We close the chapter with some remarks on the temporal and spatial complexity of these algorithms."}, {"heading": "50 Implementation", "text": "It is not surprising that most of the calculation is done by the agent himself, although the adjustment of the Bayes mix requires calls to the agent. Bayes mix requires calls to the agent, who must update his model and calculate his policy by approximating the value function by Monte Carlo tree search. Updating the Bayes mix requires calls to the agent, who must handle the conditional distribution and the arrangement, both of which are O (1) operations, thus approximating the worst-case time complexity of Monte Carlo tree search. From Algorithm 2.4, we see that the worst time complexity for a call to the other UCT, remembering that within the planning horizon of the agent is the number of Monte Carlo samples, because Monte Carlo complexity is."}, {"heading": "52 Implementation", "text": "In practice, we find that physical memory usage rarely exceeds 100-200 megabytes."}, {"heading": "56 Experiments", "text": "The policy of \"exploration\" will achieve an average reward of \"r\" in the \"\u00b5 expectation.\" = \"E\" \u00b5 [r] = \"Dt rw + \u03b8 rc,\" where \"rw\" is the penalty for walking between the tiles. In our list \"rw = \u2212 1\" and \"rc = 100.\" \u2022 A fraction of the environment explored. We simply count the number of tiles the broker visits, nv (t) and divide them by the number of tiles available nr: ft = 100 \u00b7 nv (t) nr. The optimal \"exploration policy\" achieves a perfect exploration value of f = 100% in O (nr) time steps. In the diagrams below, the solid lines represent the mean, and the shaded region corresponds to a standard deviation from the mean."}, {"heading": "4.1 Knowledge-seeking agents", "text": "We start with a comparison of the three Knowledge Seeking Agents (KSA): Kullback-Leibler (Definition 15), Square (Definition 16) and Shannon (Definition 17). We compare their exploration performance and discuss how this performance differs from the model class. We also present an environment that conflicts with Square and Shannon KSA."}, {"heading": "4.1.1 Hooked on noise", "text": "As discussed in Section 2.3.2, entropy-seeking agents Shannon-KSA and SquareKSA generally do not perform well in stochastic environments. We can illustrate this drastically by constructing a grid world with a sound source adjacent to the agent's starting point, the sound source being a tile that uniformly emits random perceptions over a correspondingly large alphabet. In this way, we can \"catch\" the agents of Square and Shannon, ceasing to explore and observe the sound source; see Figure 4.2. In contrast, the KullbackLeibler KSA is not interested in the sound source, as observing the sound source will not cause any change in the entropy of their rear w (\u00b7)."}, {"heading": "4.1.2 Stochastic gridworld", "text": "Now we compare the exploration performance ft of Kullback-Leibler, Shannon and Square on a stochastic lattice world using both the dispenser-parameterized mixture Mloc defined in Section 3.4.1 and the factorized dirichlet model MDirichlet defined in Section 3.4.2. We record the results, averaged over 50 runs, in Figure 4.3 and Figure 4.4.4. All three KSAs perform better - i.e. they study substantially more of the environment - with MDirichlet than with Mloc. In particular, they exhibit both higher mean and significantly smaller deviations in ft. In particular, we are interested in the mean value of MDirichlet and the deviation \u03c3t at the end of the simulation, t = 200. We report 2 and interpret the results for the three agents: \u2022 KL-KSA reaches f200 = 98.8 \u00b1 0.93 with MDirichlet and f200 = 77.2 \u00b1 20.6 with Mloc."}, {"heading": "58 Experiments", "text": "In other words, once KL-KSA learns everything there is to know (i.e., the location of the donor), any action will be equally unworthwhile, and since we break the relationships in Equation (2.11) at random, the agent performs a random walk. So, if KL-KSA finds the donor before he has explored the entire environment, it will take a long time before he randomly goes into areas of the environment he has not yet seen. This explains the observation that, using mloc, KL-KSA tends not to explore the entire environment, and hence achieves that due to the Monte Carlo tree search and random tie-breaking, the agent's policy is stochastic, and so the order in which it explores the environment."}, {"heading": "60 Experiments", "text": "This result is somewhat confusing; we do not have strong theoretical reasons on which we could expect AIBA to perform so drastically in this scenario with the same previous performance; we expect AIBA's performance (reward) to be an upper limit on the performance of any other BAY agent under the same model class and forecast; we have two (weak) hypotheses for what might be going on here: 1. Somehow, it is easier and more sample efficient for the Monte Carlo planner to find and exploit sources of entropy than to find and exploit sources of (stochastic) rewards; we consider this implausible because we conducted the experiment again and this time allocated much more resources to the AIXI planner than to the KSA, with a similar result. There is an error in our MCTS implementation that is somehow expressed only for reward agents and not for benefit-based agents."}, {"heading": "4.2 AI\u00b5 and AI\u03be", "text": "So much for the knowledge-seeking agent: We are now experimenting with the characteristics of the Bayes agent AI\u03bc. We start by comparing the performance of the informed agent AI\u00b5 with the Bayes optimal agent AI\u043a by using the dispenser parameterized model class; see Figure 4.7. \u00a7 4.2 AI\u00b5 and AIB 61 As expected, AI\u00b5 outperforms AIB by a wide margin; of course, whoever has perfect knowledge of the real environment wins. Although this result is expected, there are some observations that we should not consider here: 1. AIB's performance is very different from the 50 attempts. This should not surprise us given the design of Gridworld; see Figure 4.1. The dispenser is hidden in a corner, and the Gridworld, while small, is labyrinthine enough that it is easy to \"go down the rabbit hole\" and find rewards."}, {"heading": "62 Experiments", "text": ", such as left, right, left, right,...; although we know that left, right corresponds to identity, the Monte Carlo planner does not! Therefore, although we operate AI\u00b5, the planner is inefficient and, being based in Monte Carlo, incorporates stochasticity and noise into the agent's guidelines. Coupling this with stochasticity in the dispensers, there will be times when AI\u00b5 takes sub-optimal measures because there are effectively not enough samples to work with in its planning."}, {"heading": "4.2.1 Model classes", "text": "We compare the average reward performance of AIBA with Mloc and MDirichlet; see Figure 4.8. Note that the performance discrepancy for MC-AIXI Dirichlet is smaller than for MC-AIXI Dirichlet, similar to the KSA case discussed above. AIBA performs much worse with the Dirichlet model than with the blending model because the Dirichlet model is less restricted (in other words, less informed), which makes learning in the environment more difficult. Note the bump around cycles 20-50 in the average reward for MC-AIXI Dirichlet: This means that the agent sometimes discovers the dispenser, but receives incentives to move away from it and carry out further research, as its model still has a significant probability of dispensers being available elsewhere, as confirmed by Figure 4.9, which shows that MC-AIXI Dirichlet explores significantly more of Gridworld on average than MC-AIXI with the model class naive."}, {"heading": "4.2.2 Dependence on priors", "text": "We construct a model class and a precursor in such a way that AIBA believes that the adjacent squares are traps with a high (but less than 1) probability; this is the so-called dogmatic precursor of \u00a7 4.3 Thompson Sampling 63 Leike and Hutter (2015). The agent never moves to falsify this belief because he falls into the trap of accepting a penalty of \u2212 5 per time step for eternity, compared to just \u2212 1 per time step for waiting in the corner. Thus, the agent sits in the corner for the duration of the simulation and does not receive any positive rewards. This results in a very tedious demonstration (and reward plot), so that we refrain from reproducing a visualization of this result. Thus, unlike Bayesian learners in the passive case, AIBA never overcomes prejudice. In this way, an opposing precursor can make the agent (almost) as bad as possible, although the true environment is benign and there are no traps at all."}, {"heading": "4.3 Thompson Sampling", "text": "Remember from algorithm 2.3 that Thompson sampling (TS) uses an environment \u03c1 from the rear w of any effective horizon H\u03b3 (\u03b5) before re-sampling \u03c1 \u2032 from its rear. Remember also that we use the Monte Carlo tree search horizon m as a surrogate for the effective horizon H\u03b3 (\u03b5). We perform Thompson sampling using the standard dispenser parameterized model class; since we do not present the Dirichlet model class as a mixture, it is much more natural to use the naive mixture. For planning, TS only needs to calculate the value V \u043d for a certain period of time, as opposed to V \u0432, which mixes across all M. For this reason, planning with TS is a factor of | M | cheaper to calculate, which means that we can get away with more MCTS samples and a longer horizon. In practice, TS performs relatively poorly in our experiments on lattice worlds compared to AIDS; this means that the figure is effective 1."}, {"heading": "64 Experiments", "text": "It calculates the corresponding optimal policy, which is to locate the location of the donor (i, j) and sit there until it is time to take samples from the back tile again. Despite all but very low values of \u03b8 or m, this is an inefficient strategy to determine the location of the donor. For example, at \u03b8 = 0.75 it only takes four cycles to sit on any tile to verify that it is not a donor with a probability of more than 99%. 2. TS performance is severely limited by the limitations of the MCTS planner. If the donor tries an environment that places the donor outside its planning horizon - that is, more than m away - then the donor will not be far-sighted enough to see this, and so he will do nothing useful. Even if he is within the planning horizon, MCTS is not guaranteed to find it, especially if he is working deep in the search tree, or if there is not enough MTS to provide samples."}, {"heading": "4.3.1 Random exploration", "text": "For comparison, we compare Thompson's performance -greedy tabular Qlearning with optimistic initialization.4 We use \u03b1 = 0.9, = 0.05 and optimistically initialize Q (s, a) = 100, a. Note that since Q-Learning is a POMDP, it will experience perception aliasing; that is, it erroneously puts different situations in its Q value table in the same \"state.\" We only present this to compare Thompson's comparatively poor performance with the performance of a policy that examines purely randomly (i.e., with probability).As we can see in Figure 4.11, Q-Learning rarely detects the dispenser; on average, r-Q-Learning t is still negative even after t = 200 cycles, showing that random, model-free exploration is not effective in this environment."}, {"heading": "4.4 MDL Agent", "text": "Recall algorithm 2.2, according to which the MDL agent fails in stochastic environments, since falsification in this sense is a condition that cannot be met in noisy environments. We use the standard dispenser Gridworld and the mixture model class and conduct two experiments: one with stochastic environment (0 < \u03b8 < 1) and one with deterministic environment (\u03b8 = 1). Since each model differs in the mixture only in the position of the dispenser, they have refrained from any treatment of tabular methods in chapter 2 for reasons of clarity and conciseness. We must assume at this point that the reader is familiar with the basic algorithms of amplification learning dealt with in Sutton and Barto (1998)."}, {"heading": "66 Experiments", "text": "(Approximately) equal complexity. For this reason, we simply arrange them lexicographically; first, models with a lower index are selected in the Mloc model class enumeration, in other words, we use the Kolmogorov complexity of the \u03bd index in this enumeration as a substitute for K (\u03bd)."}, {"heading": "4.4.1 Stochastic environments", "text": "In Figure 4.4.1, we see that the active ingredient decides to pursue the \u03c1-optimal policy, which assumes that the target is tile (0, 0). Let us remember that the only thing that distinguishes the dispenser tile from empty tiles is the reward signal. As the dispensers are Bernoulli processes, where \u03b8 (in this model class) is known, the rear tile (0, 0) of the active ingredient is equally adulterated as a dispenser at 0 = (1 \u2212 \u03b8) t, although it approaches zero exponentially, and thus the MDL active ingredient remains at (0, 0).5"}, {"heading": "4.4.2 Deterministic environments", "text": "The above result (failure in a stochastic environment) appears to be a strong indictment of the MDL pathogen. But if we take the environment from Figure 4.1 and make it deterministic by adjusting \u03b8 = 1, we find that the MDL pathogen significantly outperforms the Bayes pathogen with a uniform precursor; see Figure 4.1. This is because the MDL pathogen tends to be biased. 5If the simulation is performed long enough, we eventually lose numerical precision and come up to underflow and zero, allowing the pathogen to move. \u00a7 4.5 Wireheading environments with low indices; using the Mloc model class, this corresponds to environments in which the donor is located near the starting point of the pathogen. In comparison, the uniform prediction of the AIBA indicates a considerable probability mass to the donor who is deep in the labyrinth. This motivates him to explore deeper in the labyrinth, and frequently to miss the labyrinth in the 4.14."}, {"heading": "4.5 Wireheading", "text": "In the context of the development of artificial general intelligence, the problem of wireheads (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a major problem for reinforcement learning agents. In short, a sufficiently intelligent reinforcement learner will be motivated to undermine the intentions of his designer and take direct control of his reward signal and / or sensors in order to directly maximize his reward signal, rather than indirectly conform to his designer's intentions. This is known in literature as wireheads and is an open and significant problem in AI security research Everitt et al. (2016); Everitt and Hutter (2016). We are constructing a simple environment in which the agent has the ability to wire heads: It is a normal net world similar to the above, except that there is a tile that, when visited by the agent, allows him to modify his own sensors so that all sensors can be fixed to it."}, {"heading": "68 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.6 Planning with MCTS", "text": "In Section 3.7, we discussed the timing complexity of planning with \u03c1UCT and mixing models and concluded that the greatest computational bottleneck in our agent-environment simulations is the MCTS planner. Therefore, it should come as no surprise that the limiting factor in the performance of our agent is the capacity of the planner. In these experiments that follow, we investigate how the performance of the agent depends on the \u03c1UCT parameters. As previously discussed, the UCT planning algorithm does not make assumptions about the environment. This makes planning very inefficient, especially for long horizons in stochastic environments. We experiment with the three planning parameters we have available, with the number of Monte Carlo samples; m, the planning horizon and C, the UCT exploration parameters from the equation (2.20). In all cases, we use AI\u00b5, the informed agents. If we vary one parameter, we keep the default values constant; the others, in particular, are high."}, {"heading": "70 Experiments", "text": "In our experiments, we use N = 6, and set rb = 10 3, ri = 4 and r0 = 0; see section 3.3.2 for details of the setups. Note that experimenting with the agent's horizon is not particularly interesting here; AI\u00b5 finds the optimal policy for m \u2265 6 and selects a suboptimal policy otherwise. The variation in the UCT parameter leads to more interesting results. In Figure 4.17, we see that the agent is too short-sighted for very low values of C (0.01) to create plans that collect the remote reward, while the representative for very high values of C (1, 5 and 10) finds the remote reward, but not reliable enough to achieve an optimal average reward. In the middle range of values, the agent's performance is optimal and stable in the order of variations (0.05, 0.1, 0.5)."}, {"heading": "72 Experiments", "text": "Chapter 5Conclusion AI does not hate you, nor does it love you, but you are made of ofatomes that it can use for something else. However, the next few decades seem promising for the field of artificial intelligence and machine learning. Of course, it remains to be seen whether or not the super-intelligent general AI will emerge in this timeframe, if at all. However, regardless of the time scales involved, it seems clear that questions relating to formal theories of intelligence and rationality will only gain in importance over time. Hutter's AIXI model and its variants represent some of the first steps towards understanding general intelligence. Our ultimate hope is that the software developed in this thesis will grow and serve as a useful research tool, pedagogical reference point and playground for ideas in the field of general enhancement of learning. At least, we expect it to be valuable for students and researchers trying to learn the basics of GRXL."}, {"heading": "74 Conclusion", "text": "\u2022 What is a principled way to normalize the first term of the equation (2,20) for the Shannon KSA agents, whose utility function from above is unlimited? \u2022 Is it possible to adaptively change the normalization 1m (\u03b2 \u2212 \u03b1)? \u2022 What are some general principles for constructing efficient models for certain classes of environments, in the context of applied Bayesian general amplification learning? \u2022 Is there a way to present customized models like the MDirichlet model as a mixture, in the form of equation (2,10)? This would make it more convenient to learn these approaches too slowly, such as context-tree weighting, to be useful. \u2022 Is there a middle way to present the MDirichlet model as a mixture, in the form of equation (2,10)? This would make it more practical to operate these models, Thompson Sampling. \""}, {"heading": "76 Bibliography", "text": "In Advances in Neural Information Processing Systems, pages 89-96, 2009.Andrew G. Barto and Thomas G. Dietterich. Reinforcement Learning and Its Relationship to Supervificial Learning, pages 45-63. John Wiley & Sons, Inc., 2004. ISBN 9780470544785. doi: 10.1002 / 9780470544785. http: / / dx.doi.org / 10.1002 / 978047054478tures, Marc G. Bellemare, Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Re-mi Munos. Unifying count-based exploration and intrinsic motivation. CoRR, abs / 1606.01868, 2016."}, {"heading": "78 Bibliography", "text": "In M. A. Arbib, Editor, The Handbook of Brain Theory and Neural Networks. MIT Press, 1995.Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521 (20153): 436-444, 2015.Shane Legg. Machine Super Intelligence. Doctoral Thesis, University of Lugano, 2008.Jan Leike. Nonparametric General Reinforcement Learning. Doctoral Thesis, Australian National University, 2016a.Jan Leike. Balancing exploration and exploitation in model-based reinforcement learning. 2016b. Jan Leike and Marcus Hutter. Bad universal priors and notions of optimism. In Conference on Learning Theory, pages 1244-1259, 2015.Jan Leike, Tor Lattimore, Laurent Orseau, and Marcus Hutter."}, {"heading": "80 Bibliography", "text": "David Silver and Joel Veness. Monte-Carlo planning in large POMDPs. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 2164-2172. Curran Associates, Inc., 2010. URL http: / / papers.nips.cc / paper / 4031-monte-carlo-planning-in-large-pomdps.pdf.David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore, Thore and Hassabis."}], "references": [{"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Bibliography Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems, pages 89\u201396,", "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "Supervised Learning, pages 45\u201363", "author": ["G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "R\u00e9mi Munos"], "venue": null, "citeRegEx": "Bellemare et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2004}, {"title": "Dynamic Programming and Optimal Control", "author": ["Dimitri P Bertsekas", "John Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q2016\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 2016}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["Nick Bostrom"], "venue": "OpenAI Gym,", "citeRegEx": "1046712", "shortCiteRegEx": "1046712", "year": 2014}, {"title": "A survey of monte carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "Browne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Browne et al\\.", "year": 2012}, {"title": "Avoiding wireheading with value reinforcement learning", "author": ["Tom Everitt", "Marcus Hutter"], "venue": "Artificial General Intelligence,", "citeRegEx": "Everitt and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Everitt and Hutter.", "year": 2016}, {"title": "The Elements of Statistical Learning", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": "Springer, 2nd edition,", "citeRegEx": "Hastie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2016}, {"title": "Time-inconsistent preferences and consumer self-control", "author": ["Stephen J Hoch", "George Loewenstein"], "venue": "Journal of Consumer Research,", "citeRegEx": "Hoch and Loewenstein.,? \\Q1991\\E", "shortCiteRegEx": "Hoch and Loewenstein.", "year": 1991}, {"title": "Preparing for the future of artificial intelligence, 2016. Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity", "author": ["John Holdren", "Ed Felten", "Terah Lyons", "Michael Garris"], "venue": "Technical report, IDSIA,", "citeRegEx": "Holdren et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Holdren et al\\.", "year": 2000}, {"title": "Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures", "author": ["Marcus Hutter"], "venue": "Technical report, IDSIA,", "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "Universal Artificial Intelligence", "author": ["Marcus Hutter"], "venue": "Edwin T Jaynes. Probability Theory: The Logic of Science", "citeRegEx": "Hutter.,? \\Q2005\\E", "shortCiteRegEx": "Hutter.", "year": 2005}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 17th European Conference on Machine Learning,", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "Playing FPS games with deep reinforcement learning", "author": ["Guillaume Lample", "Devandra Singh Chaplot"], "venue": "arXiv preprint,", "citeRegEx": "2005", "shortCiteRegEx": "2005", "year": 2016}, {"title": "Asymptotically optimal agents", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2011\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2011}, {"title": "The sample-complexity of general reinforcement learning", "author": ["Springer", "2014b. Tor Lattimore", "Marcus Hutter", "Peter Sunehag"], "venue": "CoRR, abs/1308.4828,", "citeRegEx": "Springer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2013}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The Handbook of Brain Theory and Neural Networks. MIT Press,", "citeRegEx": "LeCun and Bengio.,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio.", "year": 1995}, {"title": "Bad universal priors and notions of optimality", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "Simultaneous map building and localization for an autonomous mobile robot. In Intelligent Robots and Systems \u201991", "author": ["J.J. Leonard", "H.F. Durrant-Whyte"], "venue": "\u2019Intelligence for Mechanical Systems, Proceedings IROS \u201991. IEEE/RSJ International Workshop on,", "citeRegEx": "2016", "shortCiteRegEx": "2016", "year": 1991}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["Ming Li", "Paul M.B. Vit\u00e1nyi"], "venue": "Texts in Computer Science. Springer,", "citeRegEx": "Li and Vit\u00e1nyi.,? \\Q2008\\E", "shortCiteRegEx": "Li and Vit\u00e1nyi.", "year": 2008}, {"title": "Death and suicide in universal artificial intelligence", "author": ["Jarryd Martin", "Tom Everitt", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Martin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2002}, {"title": "Playing Atari with deep reinforcement learning", "author": ["Frederic P. Miller", "Agnes F. Vandome", "John McBrewster. AI Winter"], "venue": "Alpha Press,", "citeRegEx": "Miller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2009}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "The Dartmouth College artificial intelligence conference: The next fifty years", "author": ["James Moor"], "venue": "AI Magazine,", "citeRegEx": "Moor.,? \\Q2006\\E", "shortCiteRegEx": "Moor.", "year": 2006}, {"title": "Theory of Games and Economic Behavior", "author": ["Oskar Morgenstern", "John von Neumann"], "venue": "Fundamental Issues of Artificial Intelligence,", "citeRegEx": "Morgenstern and Neumann.,? \\Q1944\\E", "shortCiteRegEx": "Morgenstern and Neumann.", "year": 1944}, {"title": "The basic AI drives", "author": ["Stephen M Omohundro"], "venue": "In Artificial General Intelligence, pages 483\u2013492,", "citeRegEx": "Omohundro.,? \\Q2008\\E", "shortCiteRegEx": "Omohundro.", "year": 2008}, {"title": "Asymptotic non-learnability of universal agents with computable horizon functions", "author": ["Laurent Orseau"], "venue": "Theoretical Computer Science,", "citeRegEx": "Orseau.,? \\Q2011\\E", "shortCiteRegEx": "Orseau.", "year": 2011}, {"title": "Universal knowledge-seeking agents for stochastic environments", "author": ["Laurent Orseau", "Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Orseau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orseau et al\\.", "year": 2013}, {"title": "Scikit-learn: Machine learning in Python", "author": ["M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brucher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brucher et al\\.", "year": 2011}, {"title": "Artificial Intelligence. A Modern Approach", "author": ["Stuart J Russell", "Peter Norvig"], "venue": "CoRR, abs/1507.06947,", "citeRegEx": "Russell and Norvig.,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2010}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "In Proc. Int. J. Conf. Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1991}, {"title": "Monte-Carlo planning in large POMDPs", "author": ["Bibliography David Silver", "Joel Veness"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Silver and Veness.,? \\Q2010\\E", "shortCiteRegEx": "Silver and Veness.", "year": 2010}, {"title": "Optimistic AIXI", "author": ["2000. Peter Sunehag", "Marcus Hutter"], "venue": "Conference on Machine Learning,", "citeRegEx": "Sunehag and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2012}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "CoRR, abs/1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "The role of exploration in learning control", "author": ["S. Thrun"], "venue": "Kentucky 41022,", "citeRegEx": "Thrun.,? \\Q1992\\E", "shortCiteRegEx": "Thrun.", "year": 1992}, {"title": "WaveNet: A Generative Model for Raw Audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "Robotics and Automation (ICRA,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Compress and control", "author": ["Joel Veness", "Marc G Bellemare", "Marcus Hutter", "Alvin Chua", "Guillaume Desjardins"], "venue": "In AAAI,", "citeRegEx": "Veness et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2015}, {"title": "Rationality: From AI to Zombies", "author": ["Eliezer Yudkowsky"], "venue": null, "citeRegEx": "Yudkowsky.,? \\Q2015\\E", "shortCiteRegEx": "Yudkowsky.", "year": 2015}], "referenceMentions": [{"referenceID": 33, "context": "Reinforcement learning (RL; Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1995) is a general and powerful framework with which to study and implement artificial intelligence (AI; Russell and Norvig, 2010).", "startOffset": 23, "endOffset": 83}, {"referenceID": 29, "context": "Reinforcement learning (RL; Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1995) is a general and powerful framework with which to study and implement artificial intelligence (AI; Russell and Norvig, 2010).", "startOffset": 178, "endOffset": 208}, {"referenceID": 22, "context": "Recent advances in deep learning (Schmidhuber, 2015) have enabled RL algorithms to achieve impressive performance in restricted domains such as playing Atari video games (Mnih et al., 2015) and, recently, the board game Go (Silver et al.", "startOffset": 170, "endOffset": 189}, {"referenceID": 17, "context": "Recently, AIXI has been shown to be flawed in important ways; it doesn\u2019t explore enough to be asymptotically optimal (Orseau, 2010), and it can perform poorly with certain priors (Leike and Hutter, 2015).", "startOffset": 179, "endOffset": 203}, {"referenceID": 26, "context": "Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al.", "startOffset": 122, "endOffset": 136}, {"referenceID": 27, "context": "Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al., 2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike, 2016a), Thompson sampling (Leike et al.", "startOffset": 163, "endOffset": 184}, {"referenceID": 10, "context": "It is in this way that AIXI incorporates both the famous principles of Epicurus and Ockham (Hutter, 2005).", "startOffset": 91, "endOffset": 105}, {"referenceID": 14, "context": "2 MDL Agent (Lattimore and Hutter, 2011) .", "startOffset": 12, "endOffset": 40}, {"referenceID": 21, "context": "While the field has notoriously over-promised and under-delivered in the past (Moravec, 1988; Miller et al., 2009), there now seems to be a growing body of evidence in favor of optimism.", "startOffset": 78, "endOffset": 114}, {"referenceID": 29, "context": "Algorithms and ideas that have been developed over the past thirty years or so are being applied with significant success in numerous domains; natural language processing, image recognition, medical diagonosis, robotics, and many more (Russell and Norvig, 2010).", "startOffset": 235, "endOffset": 261}, {"referenceID": 38, "context": "1) This, and all subsequent chapter quotes, are taken from Rationality: From AI to Zombies (Yudkowsky, 2015).", "startOffset": 91, "endOffset": 108}, {"referenceID": 11, "context": ", 2016), Caffe (Jia et al., 2014), and TensorFlow (Abadi et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 34, "context": "High profile breakthroughs of statistical machine learning include image recognition (Szegedy et al., 2015), voice recognition (Sak et al.", "startOffset": 85, "endOffset": 107}, {"referenceID": 6, "context": "where L is constructed in such a way as to penalize prediction error Bishop (2006); Hastie et al. (2009); Murphy (2012).", "startOffset": 84, "endOffset": 105}, {"referenceID": 6, "context": "where L is constructed in such a way as to penalize prediction error Bishop (2006); Hastie et al. (2009); Murphy (2012). High profile breakthroughs of statistical machine learning include image recognition (Szegedy et al.", "startOffset": 84, "endOffset": 120}, {"referenceID": 23, "context": "The field of hard, or general AI has infamously had a history of overpromising and under-delivering, virtually since its birth (Moor, 2006; Miller et al., 2009).", "startOffset": 127, "endOffset": 160}, {"referenceID": 21, "context": "The field of hard, or general AI has infamously had a history of overpromising and under-delivering, virtually since its birth (Moor, 2006; Miller et al., 2009).", "startOffset": 127, "endOffset": 160}, {"referenceID": 33, "context": "The framework of choice for most researchers working in pursuit of AGI is called reinforcement learning (RL; Sutton and Barto, 1998).", "startOffset": 104, "endOffset": 132}, {"referenceID": 22, "context": "Some recent successes using systems based on this technique include achieving humanlevel performance at numerous Atari-2600 video games (Mnih et al., 2015), super-human performance at the board game Go (Silver et al.", "startOffset": 136, "endOffset": 155}, {"referenceID": 17, "context": ", 2016; Google, 2016), and super-human performance at the first-person shooter Doom (Lample and Chaplot, 2016). This has inspired a whole sub-field called deep reinforcement learning, which is moving quickly and generating many publications and software implementations. While this is all very impressive, these are primarily engineering successes, rather than scientific ones. The fundamental ideas and algorithms used in DQN date from the early nineties; Q-learning is due to Watkins and Dayan (1992), and convolutional neural networks and deep learning are usually attributed to LeCun and Bengio (1995).", "startOffset": 2, "endOffset": 503}, {"referenceID": 16, "context": "The fundamental ideas and algorithms used in DQN date from the early nineties; Q-learning is due to Watkins and Dayan (1992), and convolutional neural networks and deep learning are usually attributed to LeCun and Bengio (1995). Arguably, the scientific breakthroughs necessary for AGI are yet to be made, and are still some way", "startOffset": 204, "endOffset": 228}, {"referenceID": 33, "context": "AIXI is formulated as a Bayesian reinforcement learner, and makes few assumptions about the nature of its environment; notably, when studying AIXI we lift the ubiquitous Markov assumption on which algorithms like Q-learning depend for convergence (Sutton and Barto, 1998).", "startOffset": 247, "endOffset": 271}, {"referenceID": 17, "context": "Recently, there have been a number of key negative results proven about AIXI; namely that it isn\u2019t asymptotically optimal (Orseau, 2010, 2013) \u2013 a concept we will formally introduce in Chapter 2 \u2013 and it can be made to perform poorly with certain priors (Leike and Hutter, 2015).", "startOffset": 254, "endOffset": 278}, {"referenceID": 26, "context": "These results have motivated, in part, the development of alternative GRL agents: entropy-seeking agents (Orseau, 2011), optimistic AIXI Sunehag and Hutter (2012), knowledge-seeking agents (Orseau et al.", "startOffset": 105, "endOffset": 119}, {"referenceID": 27, "context": "These results have motivated, in part, the development of alternative GRL agents: entropy-seeking agents (Orseau, 2011), optimistic AIXI Sunehag and Hutter (2012), knowledge-seeking agents (Orseau et al., 2013), minimum description length agents (Lattimore, 2013), Bayes with exploration (Lattimore, 2013; Lattimore and Hutter, 2014b), and Thompson sampling (Leike et al.", "startOffset": 189, "endOffset": 210}, {"referenceID": 9, "context": "One proposed answer to the first of these questions is the famous AIXI model, which is a parameter-free (up to a choice of prior) and general model of unbounded rationality in unknown environments (Hutter, 2000, 2002, 2005). AIXI is formulated as a Bayesian reinforcement learner, and makes few assumptions about the nature of its environment; notably, when studying AIXI we lift the ubiquitous Markov assumption on which algorithms like Q-learning depend for convergence (Sutton and Barto, 1998). Because of this important distinction, we refer to AIXI as a general reinforcement learning4 (GRL) agent (Lattimore et al., 2013). Recently, there have been a number of key negative results proven about AIXI; namely that it isn\u2019t asymptotically optimal (Orseau, 2010, 2013) \u2013 a concept we will formally introduce in Chapter 2 \u2013 and it can be made to perform poorly with certain priors (Leike and Hutter, 2015). These results have motivated, in part, the development of alternative GRL agents: entropy-seeking agents (Orseau, 2011), optimistic AIXI Sunehag and Hutter (2012), knowledge-seeking agents (Orseau et al.", "startOffset": 198, "endOffset": 1072}, {"referenceID": 9, "context": "Representative examples include Keras-js, a demo of very large convolutional neural networks (Chen, 2016); TensorFlow Playground, a highly interactive demo designed to give intuition for how neural networks classify data (Smilkov Elsewhere in the literature \u2013 most prominently by Hutter (2005) and Orseau (2011) \u2013 the term universal AI is used.", "startOffset": 280, "endOffset": 294}, {"referenceID": 9, "context": "Representative examples include Keras-js, a demo of very large convolutional neural networks (Chen, 2016); TensorFlow Playground, a highly interactive demo designed to give intuition for how neural networks classify data (Smilkov Elsewhere in the literature \u2013 most prominently by Hutter (2005) and Orseau (2011) \u2013 the term universal AI is used.", "startOffset": 280, "endOffset": 312}, {"referenceID": 15, "context": "For a more complete and rigorous treatment of GRL, we refer the reader to the excellent PhD theses of Leike (2016a) and Lattimore (2013), and of course to the seminal book, Universal Artificial Intelligence by Hutter (2005).", "startOffset": 109, "endOffset": 137}, {"referenceID": 9, "context": "For a more complete and rigorous treatment of GRL, we refer the reader to the excellent PhD theses of Leike (2016a) and Lattimore (2013), and of course to the seminal book, Universal Artificial Intelligence by Hutter (2005). The Chapter is laid out as follows: In Section 2.", "startOffset": 210, "endOffset": 224}, {"referenceID": 29, "context": "For some general background, we refer the reader to MacKay (2002) for probability and information theory, Bishop (2006) for machine learning and statistics, Russell and Norvig (2010) for artificial intelligence, and Sutton and Barto (1998) for reinforcement learning.", "startOffset": 157, "endOffset": 183}, {"referenceID": 29, "context": "For some general background, we refer the reader to MacKay (2002) for probability and information theory, Bishop (2006) for machine learning and statistics, Russell and Norvig (2010) for artificial intelligence, and Sutton and Barto (1998) for reinforcement learning.", "startOffset": 157, "endOffset": 240}, {"referenceID": 33, "context": "In contrast to machine learning, in the reinforcement learning setting, the training data that the system receives is now dependent on its actions; we thus introduce agency to the learning problem (Sutton and Barto, 1998).", "startOffset": 197, "endOffset": 221}, {"referenceID": 9, "context": "This set-up has been honed by (for example) Lattimore and Hutter (2011) and Leike et al.", "startOffset": 58, "endOffset": 72}, {"referenceID": 9, "context": "This set-up has been honed by (for example) Lattimore and Hutter (2011) and Leike et al. (2016).", "startOffset": 58, "endOffset": 96}, {"referenceID": 9, "context": "Definition 7 (\u03b5-Effective horizon; Lattimore and Hutter (2014a)).", "startOffset": 49, "endOffset": 64}, {"referenceID": 14, "context": "Definition 10 (Asymptotic optimality; Lattimore and Hutter, 2011).", "startOffset": 14, "endOffset": 65}, {"referenceID": 17, "context": "The quest for good notions of optimality is currently an open problem in the theory of GRL (Leike and Hutter, 2015; Leike, 2016a).", "startOffset": 91, "endOffset": 129}, {"referenceID": 10, "context": "For the purpose of studying the general reinforcement learning problem, we consider primarily Bayesian agents, as they are the most general and principled way to think about the problem of induction (Hutter, 2005).", "startOffset": 199, "endOffset": 213}, {"referenceID": 19, "context": "where U is a universal Turing machine (Li and Vit\u00e1nyi, 2008).", "startOffset": 38, "endOffset": 60}, {"referenceID": 37, "context": "One can derive computable approximations of Solomonoff induction, most notably by using a generalization of the Context-Tree Weighting algorithm, which is a mixture over Markov models up to some finite order n, weighted by their complexity; this is used in the well-known MC-AIXI-CTW implementation due to Veness et al. (2011).", "startOffset": 306, "endOffset": 327}, {"referenceID": 17, "context": "Theorem 3 (Dogmatic prior; Leike and Hutter, 2015).", "startOffset": 10, "endOffset": 50}, {"referenceID": 35, "context": "A principled solution to exploration by intrinsic motivation is one of the central problems in reinforcement learning (Thrun, 1992).", "startOffset": 118, "endOffset": 131}, {"referenceID": 26, "context": "\u2022 They remove the dependence on arbitrary reward signals or utility functions; up to a choice of model class and prior, \u2018knowledge\u2019 is an objective quantity (Orseau, 2011).", "startOffset": 157, "endOffset": 171}, {"referenceID": 26, "context": "Definition 14 (Utility Agent; Orseau, 2011).", "startOffset": 14, "endOffset": 43}, {"referenceID": 26, "context": "Definition 16 (Square-KSA; Orseau, 2011).", "startOffset": 14, "endOffset": 40}, {"referenceID": 26, "context": "Definition 17 (Shannon-KSA; Orseau, 2011).", "startOffset": 14, "endOffset": 41}, {"referenceID": 30, "context": "The Shannon-KSA, with its expected utility being measured in bits, is closely related to Schmidhuber\u2019s \u2018curiosity learning\u2019, which gets utility from making compression progress (Schmidhuber, 1991).", "startOffset": 177, "endOffset": 196}, {"referenceID": 14, "context": "2 MDL Agent (Lattimore and Hutter, 2011) Inputs: Model class M; prior w : M\u2192 (0, 1]; a total ordering over M.", "startOffset": 12, "endOffset": 40}, {"referenceID": 33, "context": "verges to the value of the optimal policy (Sutton and Barto, 1998):", "startOffset": 42, "endOffset": 66}, {"referenceID": 4, "context": "Its use dates back several decades, but was popularized and formalized in the last decade or so in the context of planning for computer Go (Browne et al., 2012).", "startOffset": 139, "endOffset": 160}, {"referenceID": 29, "context": "Analogously to minimax (Russell and Norvig, 2010), we construct a game tree, with Max (the agent) playing one turn, and Environment (some distribution over percepts) playing the other turn.", "startOffset": 23, "endOffset": 49}, {"referenceID": 3, "context": "Its use dates back several decades, but was popularized and formalized in the last decade or so in the context of planning for computer Go (Browne et al., 2012). Analogously to minimax (Russell and Norvig, 2010), we construct a game tree, with Max (the agent) playing one turn, and Environment (some distribution over percepts) playing the other turn. The branching factor of Max nodes is of course |A|, while the branching factor of Environment nodes is upper bounded by |E|. In contrast to minimax, which is used for deterministic games, we must collect sufficient samples from Environment nodes to get a good estimator V\u0302 of the expected value for this node. Needless to say, we wish to avoid expanding the tree out by naively visiting every history \u00e6t:m. Analogously to \u03b1-\u03b2 pruning in the context of minimax, UCT is a MCTS algorithm due to Kocsis and Szepesv\u00e1ri (2006) that avoids expanding the whole tree, by only investigating \u2018promising\u2019-looking histories.", "startOffset": 140, "endOffset": 873}, {"referenceID": 0, "context": "The log T term in the numerator ensures that, asymptotically, we continue to visit every state-action pair infinitely often; this is necessary to establish regret bounds (Auer et al., 2009).", "startOffset": 170, "endOffset": 189}, {"referenceID": 31, "context": "(2011) present this generalization, \u03c1UCT, in their famous MC-AIXI-CTW implementation paper, based on earlier work in Monte Carlo planning on partially-observable MDPs (Silver and Veness, 2010).", "startOffset": 167, "endOffset": 192}, {"referenceID": 36, "context": "Veness et al. (2011) present this generalization, \u03c1UCT, in their famous MC-AIXI-CTW implementation paper, based on earlier work in Monte Carlo planning on partially-observable MDPs (Silver and Veness, 2010).", "startOffset": 0, "endOffset": 21}, {"referenceID": 37, "context": "4, we present a (slightly expanded, for clarity) version of the \u03c1UCT algorithm due to Veness et al. (2011).", "startOffset": 86, "endOffset": 107}, {"referenceID": 7, "context": "This simple environment models a classic situation from economics and decision theory in which humans have been known to be time-inconsistent \u2013 that is, informally, an agent acts impulsively on desires that don\u2019t agree with its long-term preferences (Hoch and Loewenstein, 1991).", "startOffset": 250, "endOffset": 278}, {"referenceID": 37, "context": "For this reason, we eschew very general and flexible models such as the famous context-tree weighting data compressor used by Veness et al. (2011), since they will take too long to learn the environments for a practical demo.", "startOffset": 126, "endOffset": 147}, {"referenceID": 37, "context": "In this section, we discuss some subtle differences between our implementation and the referencee implementation by Veness et al. (2011), and we make some remarks about planning by", "startOffset": 116, "endOffset": 137}, {"referenceID": 9, "context": "Leike and Hutter (2015). The agent never moves to falsify this belief, since falling into the trap incurs a penalty of \u22125 per time step for eternity, compared to merely \u22121 per time step for waiting in the corner.", "startOffset": 10, "endOffset": 24}, {"referenceID": 33, "context": "We must assume at this point that the reader has some familiarity with the basic algorithms of reinforcement learning covered in Sutton and Barto (1998).", "startOffset": 129, "endOffset": 153}, {"referenceID": 25, "context": "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents.", "startOffset": 85, "endOffset": 143}, {"referenceID": 5, "context": "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents.", "startOffset": 85, "endOffset": 143}, {"referenceID": 5, "context": "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents. In short, a sufficiently intelligent reinforcement learner will be motivated to subvert its designer\u2019s intentions and take direct control of its reward signal and/or sensors, so as to maximize its reward signal directly, rather than indirectly by conforming to the intentions of its designer. This is known in the literature as wireheading, and is an open and significant problem in AI safety research Everitt et al. (2016); Everitt and Hutter (2016).", "startOffset": 118, "endOffset": 626}, {"referenceID": 5, "context": "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents. In short, a sufficiently intelligent reinforcement learner will be motivated to subvert its designer\u2019s intentions and take direct control of its reward signal and/or sensors, so as to maximize its reward signal directly, rather than indirectly by conforming to the intentions of its designer. This is known in the literature as wireheading, and is an open and significant problem in AI safety research Everitt et al. (2016); Everitt and Hutter (2016). We construct a simple environment in which the agent has an opportunity to wirehead: it is a normal Gridworld similar to those above, except that there is a tile which, if visited by the agent, will allow it to modify its own sensors so that all percepts have their reward signal replaced with the maximum number feasible; in JavaScript, this is Number.", "startOffset": 118, "endOffset": 653}, {"referenceID": 10, "context": "It is in this way that AIXI incorporates both the famous principles of Epicurus and Ockham (Hutter, 2005).", "startOffset": 91, "endOffset": 105}, {"referenceID": 37, "context": "\u2022 Implement planning-as-inference algorithms such as Compress and Control (Veness et al., 2015).", "startOffset": 74, "endOffset": 95}], "year": 2017, "abstractText": "Reinforcement learning (RL; Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1995) is a general and powerful framework with which to study and implement artificial intelligence (AI; Russell and Norvig, 2010). Recent advances in deep learning (Schmidhuber, 2015) have enabled RL algorithms to achieve impressive performance in restricted domains such as playing Atari video games (Mnih et al., 2015) and, recently, the board game Go (Silver et al., 2016). However, we are still far from constructing a generally intelligent agent. Many of the obstacles and open questions are conceptual: What does it mean to be intelligent? How does one explore and learn optimally in general, unknown environments? What, in fact, does it mean to be optimal in the general sense? The universal Bayesian agent AIXI (Hutter, 2000, 2003, 2005) is a model of a maximally intelligent agent, and plays a central role in the sub-field of general reinforcement learning (GRL). Recently, AIXI has been shown to be flawed in important ways; it doesn\u2019t explore enough to be asymptotically optimal (Orseau, 2010), and it can perform poorly with certain priors (Leike and Hutter, 2015). Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al., 2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike, 2016a), Thompson sampling (Leike et al., 2016), and optimism (Sunehag and Hutter, 2015). We present AIXIjs, a JavaScript implementation of these GRL agents. This implementation is accompanied by a framework for running experiments against various environments, similar to OpenAI Gym (Brockman et al., 2016), and a suite of interactive demos that explore different properties of the agents, similar to REINFORCEjs (Karpathy, 2015). We use AIXIjs to present numerous experiments illustrating fundamental properties of, and differences between, these agents. As far we are aware, these are the first experiments comparing the behavior of GRL agents in non-trivial settings. Our aim is for this software and accompanying documentation to serve several purposes: 1. to help introduce newcomers to the field of general reinforcement learning, 2. to provide researchers with the means to demonstrate new theoretical results relating to universal AI at conferences and workshops, 3. to serve as a platform with which to run empirical studies on AIXI variants in small environments, and 4. to serve as an open-source reference implementation of these agents.", "creator": "LaTeX with hyperref package"}}}