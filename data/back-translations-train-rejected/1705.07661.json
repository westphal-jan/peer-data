{"id": "1705.07661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Streaming Binary Sketching based on Subspace Tracking and Diagonal Uniformization", "abstract": "In this paper, we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search. We present a new method for computing binary compressed representations -\\textit{sketches}- of high-dimensional real feature vectors. Given an expected code length $c$ and high-dimensional input data points, our algorithm provides a binary code of $c$ bits aiming at preserving the distance between the points from the original high-dimensional space. Our offline version of the algorithm outperforms the offline state-of-the-art methods regarding their computation time complexity and have a similar quality of the sketches. It also provides convergence guarantees. Moreover, our algorithm can be straightforwardly used in the streaming context by not requiring neither the storage of the whole dataset nor a chunk. We demonstrate the quality of our binary sketches through extensive experiments on real data for the nearest neighbors search task in the offline and online settings.", "histories": [["v1", "Mon, 22 May 2017 10:54:20 GMT  (444kb,D)", "http://arxiv.org/abs/1705.07661v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anne morvan", "antoine souloumiac", "c\\'edric gouy-pailler", "jamal atif"], "accepted": false, "id": "1705.07661"}, "pdf": {"name": "1705.07661.pdf", "metadata": {"source": "CRF", "title": "Streaming Binary Sketching based on Subspace Tracking and Diagonal Uniformization", "authors": ["Anne Morvan", "Antoine Souloumiac", "C\u00e9dric Gouy-Pailler", "Jamal Atif"], "emails": ["anne.morvan@cea.fr."], "sections": [{"heading": "1 Introduction", "text": "In large application fields such as computer vision or metagenomics, where similarity-preserving binary codes are critical for cost preservation to perform efficient indexing of large-scale high-dimensional data, memory requirements can be reduced and similarity searches speeded up by embedding high-dimensional data in a compact binary code. A classic method is locality-sensitive hashing (LSH) (Andoni and Indyk [2008]) to search for the nearest neighbor. (1) Input data with high dimension d is projected onto a lower c-dimensional space, which is randomly projected with i.i.d. Gaussian entries, 2) a hashing function is applied to the resulting vector to obtain the final binary codes. Two examples of the hashing function are crosspolytopic LSH (Terasawa and aka [2007], which returns the narrowest vector from the set {1}."}, {"heading": "2 Related work", "text": "There are two paradigms for constructing hash functions (Wang et al. [2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data-dependent methods. The latter learn the hash codes from a training set and perform better. Learning can go unsupervised (Wei\u00df et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpin a'n [2016]), which aim to maintain distances in the original space or (semi-) supervised the [binary] method (Leng et al.], which attempts to resemble the label (Wang et al. [2012], Liu et al.)."}, {"heading": "3 The proposed offline unsupervised model for binary quantiza-", "text": "The EU Commission has called on the EU Commission to reconsider compliance with the EU treaties."}, {"heading": "3.1 Common unsupervised offline binary quantization problem statement", "text": "Let us first introduce some notations. Our goal is to learn a binary code matrix B in which c denotes the length of the code in such a way that for each bit k = 1... c the binary encoding function is defined by hk (xt) = character (w \u00b2 T k), where w \u00b2 k is column vectors of hyperplane coefficients and characters (x) = 1, if x \u00b2 0 and \u2212 1 are otherwise applied componently to coefficients of vectors. B = characters (W \u00b2 T) in which each line k of W \u00b2 Tk is w \u00b2 Tk, with W \u00b2 Rc \u00b7 d.In ITQ and our model, W \u00b2 RW \u00b2 = RW, in which R \u00b2 c \u00b2 is an appropriate orthogonal matrix."}, {"heading": "3.2 UnifDiag: the proposed diagonal uniformization-based offline method for learning a suitable rotation", "text": "Leave the c \u00b7 c symmetrical matrix CovV = V V V T the covariance matrix of the projected data V = WX. In our offline model, R is learned to compensate the variance via the c directions given by the c main components of CovV. Consider the c diagonal coefficients of CovV: \u03c321,..., \u03c3 2 c s.t. \u03c3 2 1 \u2265... \u03c32c2. Since CovRT is symmetrical, Tr (CovV) = \u2211 c i = 1 \u03c3 2 i = 1 \u03bbi, where Tr stands for the trace application and \u03bb1..., \u03bbc are the c first eigenvalues of C s.t. Therefore, the variance via the c directions CovV can be regarded as equal to the diagonal coefficients of CovV."}, {"heading": "3.2.1 Variance uniformization", "text": "We propose to create the optimal orthogonal matrix R = \u2212 \u2212 \u2212 j = product of c \u2212 1 Givens rotations G (i, j, \u03b8) = sufficient = j = i = sufficient = i = necessary \u00b7 b = necessary (i, j, \u03b8) described by definition 3.1.1 In the following, we will use the term orthogonal matrix or rotation equally. 2If W is exactly the c first eigenvectors of C, the matrix of C - for example, if W is reached by PCA-, the condition i \u00b2 = 1 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 0... 2i =. Definition 3.1... 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 s \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 s Rotation G (i, j.) is a matrix of the form: G (i, j.) is a matrix of the form: G (i, j.)."}, {"heading": "3.2.2 Description of the diagonal uniformization algorithm", "text": "The mean of the CovV diagonal coefficients, which are equal, is not empty: Definition 3.2. Let iInf def = {l}. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3.2.3 Relation to existing works regarding complexities", "text": "Our algorithm for calculation R requires the storage of only the c \u00b7 c CovV matrix. In addition, most c \u2212 1 givens rotations are calculated, each implying four column or row multiplications, i.e. 4c flops. Thus, the final time complexity is O (c2).Spatial complexity ITQ requires the storage of two c \u00b7 n matrices: the After PCA projected data set V and the corresponding binary encoding matrix B. Since ITQ depends on the number of training data, it scales poorly w.r.t The size of the data sets and is of limited use for data sets larger than memory. Time complexity Learning R with our model is mathematically cheaper than a random rotation, as it implies a QR decomposition at a cost of O (c3).This is also the case for ITQ, where a certain number of Isoc \u00b7 c is calculated, the product should be followed by Li \u00b7 BO (c)."}, {"heading": "4 The proposed streaming unsupervised model for binary quan-", "text": "tizationIn the streaming context, the goal is to have bt = characters (W-txt) for t = 1... n, where bt is calculated and returned before xt + 1 is embedded by using xt and prior updating of W-t-1. W-tWt = RtWt, where Wt is linear dimension reduction, and Rt is a suitable c-c rotation.Online updating of the most important subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also called OPAST, is a method for quickly estimating and tracking the most important subspace of a data stream corresponding to matrix W. For each iteration, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O (c2) flops, while the storage only W and a c-c matrix. The pseudo-code is given in the appendix 3."}, {"heading": "4.1 Discussion about the number of passes over data", "text": "What matters is at what stage the data is to be projected and rotated. Should we make a first pass over the data to work on a reliable Wt? A pass over the data results in a worse performance than the other approaches with two or three passes, because the data is projected directly onto Wt, which is not correctly estimated during the first iterations. Consequently, Rt can also be used from an approximate estimate of the projected data covariance at step t: CovVt.If three passes over the data are possible, the results are optimal and much better than just one pass. One pass can be used to determine W and a second for CovV. Then, R can be calculated correctly and a third pass is used for the final binary embedding: xt is projected onto RtWt and the function character is applied. It is still a streaming algorithm that can reduce the memory requirements compared to ITQ and there is no alternative to a W for a loss of precision and two passes can be performed."}, {"heading": "4.2 Complexity analysis", "text": "Here we compare the spatial and temporal complexities of our method and occupational health and safety (Leng et al. [2015a]), which to our knowledge is the only online hashing method that is most similar to ours, i.e. unattended, hyperplane-based and reads one data point at a time. Despite the announcement, occupational health and safety is basically a mini-batch: the data stream is divided into data blocks for which a matrix S-Rd \u00b7 l is retained as a sketch of the entire data set X-Rd-n. Then, the main components are calculated from the updated sketch S. The projection of the data followed by random rotation can only be applied after this step. Therefore, there are actually two transitions over the data by reading data from each chunk twice. Spatial complexity Without counting the projection matrix and rotation, occupational safety is spatially maintained for the sketch S, which costs O-l."}, {"heading": "5 Experiments", "text": "All experiments were performed using the CIFAR-10 dataset 4, which contains 60000 32 \u00d7 32 color images evenly divided into 10 classes, from which 960-dimensional GIST descriptors were extracted."}, {"heading": "5.1 Nearest neighbor search task - Comparison to offline methods", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "5.2 Nearest neighbor search task - Comparison to OSH", "text": "In this section, we evaluate our online algorithm compared to Online Sketching Hashing (OSH) (Leng et al. [2015a]), the closest state-of-the-art approach. The training data set is evenly divided into 100 blocks (100 rounds) to perform a fair comparison, since OSH is a mini-batch and therefore performs two passes over the data: one to feed the chunk, another to perform the projection on Wt (calculated using an SVD on the sketched version of the data chunk), followed by random rotation application. Figure 3 shows the mean mean precision (MAP) on the CIFAR-10 data set of 100 rounds for a different value of c < l = 200, averaged to 5 experiments (c = 16 and c = 128 in the appendix). Our algorithm exceeds the occupational safety for c {16, 32, 64}. Finally, AP Figure 4 illustrates the average convergence of our first Msucker data using the points."}, {"heading": "6 Conclusion", "text": "Unlike traditional state-of-the-art methods, our algorithm is easily adaptable to the streaming context, where the entire dataset cannot be fully stored. In the streaming context, our online model enables the immediate extraction of a binary code as a new dataset point. Our approach shows promising results for both offline and online settings, as shown in the experimental part. It can achieve an accuracy that is at least similar, if not better, than modern methods, while saving considerable computing time and spatial requirements. Further work would be to investigate whether further rotation, which does not unify the diagonal of the covariance matrix of the projected data, could be more optimal. Another interesting perspective would be to evaluate the performance of the compact binary codes of machine learning: instead of using the original data, these binary embedding could be used directly to perform unattended learning without visualization."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 ITQ algorithm", "text": "In this section, ITQ pseudo-code in algorithm 2. RandomOrthogonalMatrix (c, c) is presented as a subroutine that simply generates a c \u00b7 c random matrix and then applies a QR decomposition with time complexity O (c3). Algorithm 2 ITQ algorithm Gong et al. [2013] 1: Inputs: Data: V = WX, V = Rc \u00b7 n PCA-projected X; code length: c; number of iterations: K 2: R \u2190 randomOrthogonalMatrix (c, c) / / Initialization 3: for i in 1... K do 4: B = characters (RV) / / Fix R and Update B 5: S, T = SV D (V TB) / / Fix B and Update R 6: R = S and ST 7: End for"}, {"heading": "7.2 Proof of theorem 2", "text": "In this section, theorem 2 is occupied. Theorem 2 If min (a, d) \u2264 \u03c4 a (b) \u2264 sin (a, d) (sufficient condition 5), then there is a \u03b8 (pi / 2; \u03c0 / 2) \u2212 such that a \u2032 (a \u2212 d 2) + b2, s1 = b (a \u2212 c2 \u2212 s1s2 and sin (\u03b8) = \u2212 c1s2 + c2s1 2 cos, c1 = (a \u2212 d 2) / + b2, s1 (a \u2212 d 2) 2 + b2, s1 = b (a \u2212 d 2) 2 + b2, s2 (a \u2212 d 2) / cos (a \u2212 d 2). Proof. As explained in Eq. 3, the problem is to find a suitable angle that can reduce the condition of CovV to the following 2-dimensional condition d: (a)."}, {"heading": "7.3 OPAST algorithm", "text": "The pseudo-code of our modified OPAST algorithm 3, which tracks the main subreoom and the covariance matrix CovV of the data projected onto this main subreoom online, is given here."}, {"heading": "7.4 Further experiments", "text": "Experiments were performed on a single processor computer (Intel Core i7-5600U CPU @ 2.60GHz, 4 hyper-threads) with 16GB RAM. In this section we present further experimental results. Algorithm 3 Modified OPAST algorithm 32 64 128 256 512ITQ 1.8 3.6 9.3 25.8 87.6 UnifDiag 3 \u00d7 10 \u2212 3 5 \u00d7 10 \u2212 3 1 \u00d7 10 \u2212 2 4 \u00d7 10 \u2212 2 0.14"}, {"heading": "7.5 Speedups with rotation computation in comparison with ITQ in offline setting", "text": "Table 1 shows the acceleration results of the rotation time calculation with our method UnifDiag, which is based on the unification of the diagonal of CovV (tol = 10 \u2212 14) instead of ITQ (number of iterations set to K = 50).This empirically confirms that our method of calculating R is faster than ITQ (for exact theoretical time complexities see Section 3.2.3) and represents a very interesting alternative to ITQ in the offline setting."}, {"heading": "7.5.1 Nearest neighbor search task in the offline setting", "text": "Figure 5 shows the complete results with the CIFAR 10 dataset for the precision recall curves, varying the length of the code from c = 32 to c = 512. Table 2 shows the corresponding average accuracy values. Both emphasize the competitive results of our method compared to ITQ."}, {"heading": "7.5.2 Nearest neighbor search task in the online setting", "text": "Figure 6 shows the complete online MAP results of the CIFAR 10 dataset from c = 32 to c = 128."}], "references": [{"title": "Fast orthonormal past algorithm", "author": ["K. Abed-Meraim", "A. Chkeif", "Y. Hua"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Abed.Meraim et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Abed.Meraim et al\\.", "year": 2000}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM,", "citeRegEx": "Andoni and Indyk.,? \\Q2008\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2008}, {"title": "Practical and optimal LSH for angular distance", "author": ["A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"], "venue": "In NIPS,", "citeRegEx": "Andoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2015}, {"title": "Structured adaptive and random spinners for fast machine learning computations", "author": ["M. Bojarski", "A. Choromanska", "K. Choromanski", "F. Fagan", "C. Gouy-Pailler", "A. Morvan", "N. Sakr", "T. Sarlos", "J. Atif"], "venue": "In AISTATS,", "citeRegEx": "Bojarski et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2017}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Online robust pca via stochastic optimization", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "In NIPS,", "citeRegEx": "Feng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Eigenvalue computation in the 20th century", "author": ["G.H. Golub", "H.A. van der Vorst"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Golub and Vorst.,? \\Q2000\\E", "shortCiteRegEx": "Golub and Vorst.", "year": 2000}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "In CVPR,", "citeRegEx": "Gong and Lazebnik.,? \\Q2011\\E", "shortCiteRegEx": "Gong and Lazebnik.", "year": 2011}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Kernelized locality-sensitive hashing", "author": ["K. Grauman", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Grauman and Kulis.,? \\Q2011\\E", "shortCiteRegEx": "Grauman and Kulis.", "year": 2011}, {"title": "Online hashing", "author": ["L.-K. Huang", "Q. Yang", "W.-S. Zheng"], "venue": "In IJCAI, pages 1422\u20131428,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "In CVPR,", "citeRegEx": "J\u00e9gou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "J\u00e9gou et al\\.", "year": 2010}, {"title": "Isotropic hashing", "author": ["W. Kong", "W.-j. Li"], "venue": "In NIPS, pages 1646\u20131654", "citeRegEx": "Kong and Li.,? \\Q2012\\E", "shortCiteRegEx": "Kong and Li.", "year": 2012}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "In CVPR,", "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Spherical hashing", "author": ["Y. Lee"], "venue": "In CVPR, pages 2957\u20132964,", "citeRegEx": "Lee.,? \\Q2012\\E", "shortCiteRegEx": "Lee.", "year": 2012}, {"title": "Online sketching hashing", "author": ["C. Leng", "J. Wu", "J. Cheng", "X. Bai", "H. Lu"], "venue": "In CVPR,", "citeRegEx": "Leng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2015}, {"title": "Hashing for distributed data", "author": ["C. Leng", "J. Wu", "J. Cheng", "X. Zhang", "H. Lu"], "venue": "In ICML,", "citeRegEx": "Leng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2015}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. fu Chang"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "In CVPR, pages 2074\u20132081,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Discrete graph hashing", "author": ["W. Liu", "C. Mu", "S. Kumar", "S.-F. Chang"], "venue": "In NIPS,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "In NIPS,", "citeRegEx": "Raginsky and Lazebnik.,? \\Q2009\\E", "shortCiteRegEx": "Raginsky and Lazebnik.", "year": 2009}, {"title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "author": ["R. Raziperchikolaei", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "In NIPS,", "citeRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.,? \\Q2016\\E", "shortCiteRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.", "year": 2016}, {"title": "Spherical LSH for approximate nearest neighbor search on unit hypersphere", "author": ["K. Terasawa", "Y. Tanaka"], "venue": "In WADS,", "citeRegEx": "Terasawa and Tanaka.,? \\Q2007\\E", "shortCiteRegEx": "Terasawa and Tanaka.", "year": 2007}, {"title": "Semi-supervised hashing for large-scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Learning to hash for indexing big data - a survey", "author": ["J. Wang", "W. Liu", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In NIPS, pages 1753\u20131760,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}, {"title": "Streaming sparse principal component analysis", "author": ["W. Yang", "H. Xu"], "venue": "In ICML, pages 494\u2013503,", "citeRegEx": "Yang and Xu.,? \\Q2015\\E", "shortCiteRegEx": "Yang and Xu.", "year": 2015}, {"title": "Circulant binary embedding", "author": ["F. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "In ICML,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.", "startOffset": 56, "endOffset": 80}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al.", "startOffset": 56, "endOffset": 450}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise.", "startOffset": 56, "endOffset": 598}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al.", "startOffset": 56, "endOffset": 846}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al. [2017]) leading to a reduced time complexity of O(d log c) thanks to fast Hadamard and Fourier transforms.", "startOffset": 56, "endOffset": 870}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al. [2017]) leading to a reduced time complexity of O(d log c) thanks to fast Hadamard and Fourier transforms. In order to increase the accuracy of the data sketches in the context of nearest neighbors search or classification, this projection can be also learned from data (Weiss et al. [2008], Wang et al.", "startOffset": 56, "endOffset": 1154}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al. [2017]) leading to a reduced time complexity of O(d log c) thanks to fast Hadamard and Fourier transforms. In order to increase the accuracy of the data sketches in the context of nearest neighbors search or classification, this projection can be also learned from data (Weiss et al. [2008], Wang et al. [2012], Gong \u2217To whom correspondence should be adressed: anne.", "startOffset": 56, "endOffset": 1174}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al.", "startOffset": 21, "endOffset": 60}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al. [2014]).", "startOffset": 21, "endOffset": 78}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al. [2014]). As Principal Component Analysis (PCA) is a common tool for reducing data dimensionality, PCA is often performed: data are then projected onto the first c principal components. But the PCA alone is not sufficient. Indeed, the c first principal components are chosen with a decreasing order of explained variance. It means that principal directions with higher variance carry more information. Consequently, associating each of the c directions to one of the c bits is equivalent to giving more weights to less informative directions and will lead to poor performance of the obtained sketches. To remedy this problem, after data have been projected on the first principal components of the covariance matrix, a solution consists in applying a suitable rotation on the projected data before performing the hashing function, in order to balance variance over the principal components. In work from J\u00e9gou et al. [2010], a random rotation is successfully applied giving quite good results.", "startOffset": 21, "endOffset": 994}, {"referenceID": 7, "context": "In ITerative Quantization (ITQ) (Gong and Lazebnik [2011], Gong et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 7, "context": "In ITerative Quantization (ITQ) (Gong and Lazebnik [2011], Gong et al. [2013]) or in Isotropic Hashing (IsoHash) (Kong and Li [2012]), the rotation is rather learned.", "startOffset": 33, "endOffset": 78}, {"referenceID": 7, "context": "In ITerative Quantization (ITQ) (Gong and Lazebnik [2011], Gong et al. [2013]) or in Isotropic Hashing (IsoHash) (Kong and Li [2012]), the rotation is rather learned.", "startOffset": 33, "endOffset": 133}, {"referenceID": 15, "context": "In any case, for only one pass over the data, our online algorithm gives better results than OSH (Leng et al. [2015a]) while being far less computationally demanding.", "startOffset": 98, "endOffset": 118}, {"referenceID": 7, "context": "Two paradigms exist for constructing hash functions (Wang et al. [2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 53, "endOffset": 72}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 27, "endOffset": 51}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 27, "endOffset": 81}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 27, "endOffset": 107}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al.", "startOffset": 27, "endOffset": 267}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al.", "startOffset": 27, "endOffset": 286}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al.", "startOffset": 27, "endOffset": 312}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al.", "startOffset": 27, "endOffset": 332}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al.", "startOffset": 27, "endOffset": 352}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al.", "startOffset": 27, "endOffset": 364}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al.", "startOffset": 27, "endOffset": 383}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al.", "startOffset": 27, "endOffset": 401}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al.", "startOffset": 27, "endOffset": 448}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al.", "startOffset": 27, "endOffset": 589}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]).", "startOffset": 27, "endOffset": 608}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al.", "startOffset": 27, "endOffset": 681}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]).", "startOffset": 27, "endOffset": 701}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al.", "startOffset": 27, "endOffset": 792}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al.", "startOffset": 27, "endOffset": 844}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al. [2015a]) have been developed.", "startOffset": 27, "endOffset": 865}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al. [2015a]) have been developed. Online Hashing (OKH) (Huang et al. [2013]) learns the hash functions from a stream of pair of data with a \u201dPassive-Aggressive\u201d method.", "startOffset": 27, "endOffset": 929}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al. [2015a]) have been developed. Online Hashing (OKH) (Huang et al. [2013]) learns the hash functions from a stream of pair of data with a \u201dPassive-Aggressive\u201d method. In Online Sketching Hashing (OSH) (Leng et al. [2015a]), the binary embeddings are learned from a maintained sketch of the dataset with a smaller size but preserving the property of interest.", "startOffset": 27, "endOffset": 1077}, {"referenceID": 8, "context": "For instance, W can be the matrix whose row vectors w k correspond to the c first principal components of the covariance matrix C = XX T (an other supervised approach is to perform Canonical Correlation Analysis (Gong et al. [2013])).", "startOffset": 213, "endOffset": 232}, {"referenceID": 12, "context": "So, similarly to IsoHash from work in (Kong and Li [2012]), we formulate the problem of finding R as the problem of equalizing the diagonal coefficients of CovV to the value \u03c4 = Tr(CovV )/c.", "startOffset": 39, "endOffset": 58}, {"referenceID": 12, "context": "In IsoHash (Kong and Li [2012]), authors have already formulated the problem as a variance-uniformization one.", "startOffset": 12, "endOffset": 31}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W .", "startOffset": 112, "endOffset": 138}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W . At each iteration t, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O(c ) flops while storing only W and a c\u00d7 c matrix. The pseudo-code is given by algorithm 3 in appendix. Online update of a suitable rotation The procedure to compute Rt relies exclusively on the c \u00d7 c covariance matrix of projected data CovVt and not on the whole V as for ITQ. CovVt is easy to update dynamically and this is performed while computing Wt with OPAST algorithm, so no need of adaptation for the Rt learning in the streaming setting. Remark also that this stage is completely independent from the first one to compute the principal subspace. One interest of UnifDiag algorithm for learning the rotation is the freedom to plug any other method for online PCA (Feng et al. [2013], Yang and Xu [2015]) or online subspace tracking( Abed-Meraim et al.", "startOffset": 112, "endOffset": 1051}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W . At each iteration t, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O(c ) flops while storing only W and a c\u00d7 c matrix. The pseudo-code is given by algorithm 3 in appendix. Online update of a suitable rotation The procedure to compute Rt relies exclusively on the c \u00d7 c covariance matrix of projected data CovVt and not on the whole V as for ITQ. CovVt is easy to update dynamically and this is performed while computing Wt with OPAST algorithm, so no need of adaptation for the Rt learning in the streaming setting. Remark also that this stage is completely independent from the first one to compute the principal subspace. One interest of UnifDiag algorithm for learning the rotation is the freedom to plug any other method for online PCA (Feng et al. [2013], Yang and Xu [2015]) or online subspace tracking( Abed-Meraim et al.", "startOffset": 112, "endOffset": 1071}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W . At each iteration t, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O(c ) flops while storing only W and a c\u00d7 c matrix. The pseudo-code is given by algorithm 3 in appendix. Online update of a suitable rotation The procedure to compute Rt relies exclusively on the c \u00d7 c covariance matrix of projected data CovVt and not on the whole V as for ITQ. CovVt is easy to update dynamically and this is performed while computing Wt with OPAST algorithm, so no need of adaptation for the Rt learning in the streaming setting. Remark also that this stage is completely independent from the first one to compute the principal subspace. One interest of UnifDiag algorithm for learning the rotation is the freedom to plug any other method for online PCA (Feng et al. [2013], Yang and Xu [2015]) or online subspace tracking( Abed-Meraim et al. [2000]).", "startOffset": 112, "endOffset": 1127}, {"referenceID": 15, "context": "2 Complexity analysis We compare here the spatial and time complexities of our method and OSH (Leng et al. [2015a]) which is to the best of our knowledge, the only online hashing method the most similar to ours, i.", "startOffset": 95, "endOffset": 115}, {"referenceID": 1, "context": "We compared our method to four baseline methods that follow the basic hashing scheme H(X) = sgn(W\u0303X), where the projection matrix W\u0303 \u2208 Rc\u00d7d is determined according to the chosen method: \u2022 LSH (Andoni and Indyk [2008]): W\u0303 is a Gaussian random matrix.", "startOffset": 193, "endOffset": 217}, {"referenceID": 15, "context": "2 Nearest neighbor search task - Comparison to OSH In this section, we evaluate our online algorithm in comparison with Online Sketching Hashing (OSH) (Leng et al. [2015a]), the nearest state-of-the-art approach.", "startOffset": 152, "endOffset": 172}], "year": 2017, "abstractText": "In this paper, we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search. We present a new method for computing binary compressed representations -sketchesof high-dimensional real feature vectors. Given an expected code length c and high-dimensional input data points, our algorithm provides a binary code of c bits aiming at preserving the distance between the points from the original high-dimensional space. Our offline version of the algorithm outperforms the offline state-of-the-art methods regarding their computation time complexity and have a similar quality of the sketches. It also provides convergence guarantees. Moreover, our algorithm can be straightforwardly used in the streaming context by not requiring neither the storage of the whole dataset nor a chunk. We demonstrate the quality of our binary sketches through extensive experiments on real data for the nearest neighbors search task in the offline and online settings.", "creator": "LaTeX with hyperref package"}}}