{"id": "1705.07663", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks", "abstract": "Recent advances in machine learning are paving the way for the artificial generation of high quality images and videos. In this paper, we investigate how generating synthetic samples through generative models can lead to information leakage, and, consequently, to privacy breaches affecting individuals' privacy that contribute their personal or sensitive data to train these models. In order to quantitatively measure privacy leakage, we train a Generative Adversarial Network (GAN), which combines a discriminative model and a generative model, to detect overfitting by relying on the discriminator capacity to learn statistical differences in distributions.", "histories": [["v1", "Mon, 22 May 2017 11:05:06 GMT  (1381kb,D)", "https://arxiv.org/abs/1705.07663v1", null], ["v2", "Sat, 12 Aug 2017 10:39:48 GMT  (4156kb,D)", "http://arxiv.org/abs/1705.07663v2", "Compared to v1, this version introduces experiments on a medical dataset as well as general improvements"]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["jamie hayes", "luca melis", "george danezis", "emiliano de cristofaro"], "accepted": false, "id": "1705.07663"}, "pdf": {"name": "1705.07663.pdf", "metadata": {"source": "CRF", "title": "LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks", "authors": ["Jamie Hayes", "Luca Melis", "George Danezis", "Emiliano De Cristofaro"], "emails": ["e.decristofaro}@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "We present attacks based on both white box and black box access to the target model, and show how to improve it using limited auxiliary knowledge of samples in the dataset. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN + VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). White box attacks are 100% successful when it comes to what samples were used to train the target model, and black box attacks can conclude that the training set is more than 80% accurate."}, {"heading": "1 Introduction", "text": "In the last few years, companies like Google, Microsoft, and Amazon have begun to give their customers access to APIs, allowing them to easily embed artificial intelligence into their applications. However, companies can also induce other users to use cloud models that rely on their data, possibly even at the expense of others. However, if malicious users can recover data that is used to train these models, it can lead to dangerous information leaks. Indeed, organizations do not have much control over the way the authors have contributed to this model. Models and training parameters used by the platform could lead to the model being overhauled (i.e.)"}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Attacks", "text": "It is time for a new beginning, \"he said in an interview with the German Press Agency."}, {"heading": "2.2 Defenses", "text": "Defense mechanisms based on secure multi-party computing and homomorphic encryption have been proposed to train supervised machine learning models such as decision trees [26], linear regressors [11], and neural networks [6, 10]. However, these mechanisms do not prevent an attacker from carrying out inference attacks on the privately trained models, since the final parameters remain unchanged. On the other hand, differential privacy [12] can mitigate inference attacks, and it has been widely applied to different models of machine learning [1, 23, 30, 37, 42]. Shokri and Shmatikov [37] support distributed training of deep learning networks in a privacy-friendly manner, i.e. independent units jointly build a model without sharing their training data, selectively dividing subsets of noisy model parameters during training. Abadi et al. [1] show how to combine deep neural networks with non-usable models (i.e. they are ineffective at differentiating private use during training)."}, {"heading": "3 Background", "text": "In this section, machine learning concepts for the production of realistic images are considered. Generative models are usually divided into discriminatory and generative networks. In the face of a supervised learning task, and in view of the characteristics (x) of a datapoint and the corresponding label (y), discriminatory models are attempted to project y onto future x by learning a discriminatory function f of pairs (x, y) that takes x as inputs and outputs as the most likely designation y. Discriminative models are unable to \"explain\" how the data points may have been generated. Generative models, on the other hand, describe how data is generated by learning the common probability distribution of p (X, Y), resulting in a score for the configuration determined jointly by pairs (x, y). Generative models are based on deep neural networks, such as generative Adversarial Networks (GAN) [17] and Variative Auto-Encoders (VAE) are considered to be state-of-the-art for the production of realistic images."}, {"heading": "4 Attacks Outline", "text": "In all of our attacks, an opponent tries to determine whether a single known data set has been included in the training set of a generative machine learning model. We distinguish between two settings: white box and black box attacks. In the former, the attacker can only make requests to the target model under attack (the \"cloud model\") and does not have access to the internal parameters of the model; in the latter, he also has access to the parameters of a trained cloud model. In both settings, we allow the opponent to know the size of the training set - but not its contents. Variants of the attack allow the opponent to access some additional side information.In our assessment of all attacks, we consider an attacker aiming to distinguish data points used to train the cloud model, and thus consider an attacker who has access to the data set. We assume that the attacker knows the size of the training set, Xtrain | = n, used to train the training points, but that the training points are not correctly divided into the training sets."}, {"heading": "4.1 White-Box Attack", "text": "Fig. 2 illustrates the intuition behind the white box attack. We assume that an attacker Awb has access to the trained cloud model, namely a GAN - i.e. a generator Gcloud and a discriminator Dcloud. The cloud model was trained to generate samples similar to the training sets. Awb creates a local copy of Dcloud, which we call Dwb. Then, as shown in Fig. 3, Awb enters all samples X = {x1,..., xm + n} from the original data set into Dwb, which returns the resulting probability vector p = [Dwb (x1),..., Dwb (xm + n). If the cloud model revised on the training data places a higher trust in samples that were part of the training set. Awb sorts their predictions p in descending order and takes samples associated with the greatest n probabilities than the predictions for the training members do not require access to the model."}, {"heading": "4.2 Black-Box Attack with No Auxiliary Knowledge", "text": "In the black box attack, we assume that the attacker Abb has no access to the parameters of the cloud model. Thus, Abb cannot steal the discriminator model directly from the cloud, as in the white box attack. Furthermore, in the white box attack, the attacker has no knowledge of how the original data set is divided into training and testing, i.e. no access to the true labels of the samples from the data set, and therefore cannot train a model using a discriminatory approach. Instead, Abb trains a GAN to recreate the cloud model locally, generating a discriminator Dbb that detects a match in the generative cloud model Gcloud. We illustrate the attack in Fig. 4a. Specifically, Abb forms a GAN (Gbb, Dbb) based on queries generated from the Abb cloud model by generating GAN points only on the cloud sample."}, {"heading": "4.3 Black-Box Attack with Limited Auxiliary Knowledge", "text": "It is indeed the case that we will be able to go in search of a solution that will enable us, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position where we are."}, {"heading": "5 Evaluation", "text": "In this section we present our experimental evaluation of the attacks described above. The experiments are carried out with PyTorch2 on a workstation running Ubuntu Server 16.04 LTS equipped with a 3.4 GHz CPU i7-6800K, 32 GB RAM and an NVIDIA Titan X GPU card. Source code is available on request.For white box attacks, we measure the precision of membership conclusions in successive training stages of the attack model, defining one training step as an iteration of training on a minibatch of inputs. 3 For black box attacks, we repair the cloud model and measure the precision of membership conclusions in successive training steps of the attack model, defining the training step as an iteration of training on a minibatch of inputs. The attack model is trained using soft and noisy labels as suggested in [36], i.e. we replace labels with random numbers in [0.7, 0.7] and [0.3] modes in samples."}, {"heading": "5.1 Experimental Setup", "text": "We first test our attacks with two standard machine learning datasets, namely, Labeled Faces in the Wild (LFW) and CIFAR-10, then, in Section 5.7, we also present a case study on a medical image dataset, i.e., diabetic retinopathy (DR) dataset.LFW includes 13,233 images of faces collected from the web5, while CIFAR-10 consists of 60,000 color images in 10 classes, with 6,000 images per class.6 For both, we randomly selected the dataset for training."}, {"heading": "5.2 Na\u0131\u0308ve Approaches", "text": "We begin our evaluation with a na\u00efve Euclidean distance-based attack that serves as motivation for the use of more complex machine learning techniques in our attacks. It proceeds as follows: Using a cloud-generated sample, the attacker calculates the Euclidean distance between the generated sample and each real sample in the dataset. If he repeats this several times for newly generated samples, the attacker calculates an average distance from each real sample, sorts the order of the average distances and takes the smallest n distances (and the associated real samples) as a guess for the training set, where n is the size of the training set. We perform this attack on a cloud model (DCGAN) that is trained on a random 10% subset of CIFAR-10 and a random 10% subset of LFW, noting that the attack does not perform any better than if the attacker randomly guessed which samples were real part of the original training sample."}, {"heading": "5.3 White-Box Attack", "text": "We present the results of our evaluation of the whitebox attack, which is described in Section 4.1 on LFW and CIFAR-10. For the LFW datasets, we build the training scenario either as a random 10% subset of the datasets, and in this section we will include a basic baseline (red line) corresponding to the success of an attacker. The samples we implement are DCGAN, DCGAN + VAE."}, {"heading": "5.4 Black-Box Attack with No Auxiliary Knowledge", "text": "We assume that the attacker has no knowledge of the training or test sets related to the size of the data set, but the cloud models we implement are both very vulnerable."}, {"heading": "5.5 Black-Box Attack with Limited Auxiliary Knowledge", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is not. (...) It is not. (...) It is not. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. It is. (...). It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. (...). (). It is. It is. It is. It is. It is. It is. (...). (). It is."}, {"heading": "5.6 Remarks", "text": "In order to better understand the relationship between membership conclusion and training performance, in Fig. 10 we show the attack accuracy and samples generated in different training phases by the cloud DCGAN generator in the whitebox attack (Fig. 10a) and the attacker DCGAN generator in the black box attack (Fig. 10b) in the first ten classes of the LFW dataset. The diagrams show that the accuracy correlates well with the visual quality of the samples generated. In particular, the samples generated by the target cloud provide a better visual quality than those generated by the attacker generator during the black box attack, resulting in higher membership inaccuracies. Overall, the samples generated by both attacks in later stages look visually plausible and look quite similar to the original ones, i.e. real samples and cloud samples.Our attacks are evaluated on datasets that consist of complex CW and FAR faces (FAR)."}, {"heading": "5.7 A Case Study on the Diabetic Retinopathy Dataset", "text": "We present a case study of our Attacks on Diabetic Retinopathy (DR) dataset, which consists of high-resolution retinopathy images, with a holistic label that assigns a score on how much the participant suffers from diabetic retinopathy. Diabetic retinopathy is a leading cause of blindness in the developed world, eliminating the need for the time-consuming process of manual detection. kaggle.com machine learning platform recently demonstrated that it is possible to classify cases of diabetic retinopathy with high accuracy using machine learning processes, eliminating the need for manual detection."}, {"heading": "6 Discussion", "text": "It is indeed the case that we are able to embark on a search for new paths that we must take in order to achieve our goals, \"he told the Deutsche Presse-Agentur.\" We must embark on a search for new paths, \"he said in an interview with the Deutsche Presse-Agentur.\" We must embark on a search for new paths, \"he told the Deutsche Presse-Agentur.\" We must embark on a search for new paths that we wish to tread. \""}, {"heading": "7 Conclusion", "text": "In this paper, we presented an initial evaluation of the information leaks in generative models, which show that a variety of models are indeed susceptible to inference attacks, i.e., the reconstruction of the data sets used during training. Attacks do not require information about the attacked model and generalize it. Our techniques can be used as a method to evaluate the quality of the data leak and decide how not to violate their models. In fact, generative models are becoming increasingly popular when the application is sensitive to privacy. Therefore, our attacks on the quality of the data leak can be used to evaluate the quality of the data leak."}, {"heading": "A Unsuccessful Attacks", "text": "In Fig. 14, we report on the results of the Euclidean attack presented in Section 5.2. This attack was carried out on a cloud model (DCGAN) trained on a random subset of 10% of CIFAR-10 and a random subset of 10% of LFW, but we found that the attack did not go much better than a random guess. We also report the results of a black box scenario in which 10% of the training set samples from LFW are used to train a shadow model - see Fig. 15. Samples generated by this model are then injected into the attack model along with samples generated by the cloud model. Specifically, each mini-batch consists of synthetic samples generated during training time, either by the cloud model or by the shadow model. However, somewhat inspired by the approach proposed in [38], this attack leads to only about 18% accuracy, with no improvements during training."}, {"heading": "B Additional Samples", "text": "Finally, in Figures 16-21 we report on further examples of samples deferred from Section 6."}], "references": [{"title": "Deep learning with differential privacy", "author": ["M. Abadi", "A. Chu", "I. Goodfellow", "H.B. McMahan", "I. Mironov", "K. Talwar", "L. Zhang"], "venue": "ACM CCS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers", "author": ["G. Ateniese", "L.V. Mancini", "A. Spognardi", "A. Villani", "D. Vitali", "G. Felici"], "venue": "International Journal of Security and Networks,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Membership Privacy in MicroRNA-based Studies", "author": ["M. Backes", "P. Berrang", "M. Humbert", "P. Manoharan"], "venue": "ACM CCS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "NIPS,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "BEGAN: Boundary Equilibrium Generative Adversarial Networks", "author": ["D. Berthelot", "T. Schumm", "L. Metz"], "venue": "arXiv preprint 1703.10717,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Practical secure aggregation for privacy preserving machine learning. https://eprint.iacr.org/2017/281", "author": ["K. Bonawitz", "V. Ivanov", "B. Kreuter", "A. Marcedone", "H.B. McMahan", "S. Patel", "D. Ramage", "A. Segal", "K. Seth"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "You Might Also Like:\u201d Privacy Risks of Collaborative Filtering", "author": ["J.A. Calandrino", "A. Kilzer", "A. Narayanan", "E.W. Felten", "V. Shmatikov"], "venue": "IEEE Security and Privacy,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating Multi-label Discrete Electronic Health Records using Generative Adversarial Networks", "author": ["E. Choi", "S. Biswal", "B. Malin", "J. Duke", "W.F. Stewart", "J. Sun"], "venue": "arXiv preprint 1703.06490,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards Adversarial Retinal Image Synthesis", "author": ["P. Costa", "A. Galdran", "M.I. Meyer", "M.D. Abr\u00e0moff", "M. Niemeijer", "A.M. Mendon\u00e7a", "A. Campilho"], "venue": "arXiv preprint 1701.08974,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy", "author": ["N. Dowlin", "R. Gilad-Bachrach", "K. Laine", "K. Lauter", "M. Naehrig", "J. Wernsing"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Privacy-preserving multivariate statistical analysis: Linear regression and classification", "author": ["W. Du", "Y.S. Han", "S. Chen"], "venue": "ICDM,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Differential privacy: A survey of results", "author": ["C. Dwork"], "venue": "Theory and Applications of Models of Computation,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalization in adaptive data analysis and holdout reuse", "author": ["C. Dwork", "V. Feldman", "M. Hardt", "T. Pitassi", "O. Reingold", "A. Roth"], "venue": "NIPS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust traceability from trace amounts", "author": ["C. Dwork", "A. Smith", "T. Steinke", "J. Ullman", "S. Vadhan"], "venue": "FOCS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training", "author": ["D. Erhan", "P.-A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "AISTATS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["M. Fredrikson", "S. Jha", "T. Ristenpart"], "venue": "CCS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde- Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning", "author": ["B. Hitaj", "G. Ateniese", "F. Perez-Cruz"], "venue": "arXiv preprint 1702.07464,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays", "author": ["N. Homer", "S. Szelinger", "M. Redman", "D. Duggan", "W. Tembe", "J. Muehling", "J.V. Pearson", "D.A. Stephan", "S.F. Nelson", "D.W. Craig"], "venue": "PLoS Genet,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "On your social network de-anonymizablity: Quantification and large scale evaluation with seed knowledge", "author": ["S. Ji", "W. Li", "N.Z. Gong", "P. Mittal", "R.A. Beyah"], "venue": "NDSS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative Models", "author": ["A. Karpathy", "P. Abbeel", "G. Brockman", "P. Chen", "V. Cheung", "R. Duan", "I. Goodfellow", "D. Kingma", "J. Ho", "R. Houthooft", "T. Salimans", "J. Schulman", "I. Sutskever", "W. Zaremba"], "venue": "https://blog.openai.com/generative-models/,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Differentially Private Bayesian Optimization", "author": ["M.J. Kusner", "J.R. Gardner", "R. Garnett", "K.Q. Weinberger"], "venue": "ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "H. Larochelle", "O. Winther"], "venue": "arXiv preprint 1512.09300,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Photorealistic single image super-resolution using a generative adversarial network", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Cunningham", "A. Acosta", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang"], "venue": "arXiv preprint 1609.04802,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Privacy preserving data mining", "author": ["Y. Lindell", "B. Pinkas"], "venue": "CRYPTO,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Statistical inference considered harmful", "author": ["F. McSherry"], "venue": "https://github.com/frankmcsherry/blog/blob/master/posts/2016- 06-14.md,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "De-anonymizing social networks", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "IEEE Security and Privacy,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Medical Image Synthesis with Context-Aware Generative Adversarial Networks", "author": ["D. Nie", "R. Trullo", "C. Petitjean", "S. Ruan", "D. Shen"], "venue": "arXiv preprint 1612.05362,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised knowledge transfer for deep learning from private training data", "author": ["N. Papernot", "M. Abadi", "\u00da. Erlingsson", "I. Goodfellow", "K. Talwar"], "venue": "ICLR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Practical Black-Box Attacks against Machine Learning", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint 1602.02697,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "The Limitations of Deep Learning in Adversarial Settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "IEEE EuroSP,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "IEEE Security and Privacy,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "De-anonymizing social networks and inferring private attributes using knowledge graphs", "author": ["J. Qian", "X.-Y. Li", "C. Zhang", "L. Chen"], "venue": "INFOCOM,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint 1511.06434,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved Techniques for Training GANs", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen", "X. Chen"], "venue": "NIPS,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Privacy-preserving deep learning", "author": ["R. Shokri", "V. Shmatikov"], "venue": "CCS,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Membership inference attacks against machine learning models", "author": ["R. Shokri", "M. Stronati", "C. Song", "V. Shmatikov"], "venue": "IEEE Security and Privacy,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. v. d. Oord", "M. Bethge"], "venue": "arXiv preprint 1511.01844,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Lossy image compression with compressive autoencoders", "author": ["L. Theis", "W. Shi", "A. Cunningham", "F. Husz\u00e1r"], "venue": "arXiv preprint 1703.00395,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Stealing machine learning models via prediction apis", "author": ["F. Tram\u00e8r", "F. Zhang", "A. Juels", "M.K. Reiter", "T. Ristenpart"], "venue": "USENIX Security,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Privacy aware learning", "author": ["M.J. Wainwright", "M.I. Jordan", "J.C. Duchi"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated Inference on Criminality using Face Images", "author": ["X. Wu", "X. Zhang"], "venue": "arXiv preprint 1611.04135,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "author": ["Y. Wu", "Y. Burda", "R. Salakhutdinov", "R. Grosse"], "venue": "https://openreview.net/forum?id=B1M8JF9xx,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic Image Inpainting with Perceptual and Contextual Losses", "author": ["R. Yeh", "C. Chen", "T.Y. Lim", "M. Hasegawa-Johnson", "M.N. Do"], "venue": "arXiv preprint 1607.07539,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "[38] at IEEE S&P\u201917.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "They are used in a plethora of applications, such as compression [40], denoising [4], inpainting [45], super-resolution [25], semisupervised learning [36], clustering [39], and deep neural networks pre-training [15] in cases where labeled data is expensive.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "They are used in a plethora of applications, such as compression [40], denoising [4], inpainting [45], super-resolution [25], semisupervised learning [36], clustering [39], and deep neural networks pre-training [15] in cases where labeled data is expensive.", "startOffset": 81, "endOffset": 84}, {"referenceID": 44, "context": "They are used in a plethora of applications, such as compression [40], denoising [4], inpainting [45], super-resolution [25], semisupervised learning [36], clustering [39], and deep neural networks pre-training [15] in cases where labeled data is expensive.", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "They are used in a plethora of applications, such as compression [40], denoising [4], inpainting [45], super-resolution [25], semisupervised learning [36], clustering [39], and deep neural networks pre-training [15] in cases where labeled data is expensive.", "startOffset": 120, "endOffset": 124}, {"referenceID": 35, "context": "They are used in a plethora of applications, such as compression [40], denoising [4], inpainting [45], super-resolution [25], semisupervised learning [36], clustering [39], and deep neural networks pre-training [15] in cases where labeled data is expensive.", "startOffset": 150, "endOffset": 154}, {"referenceID": 38, "context": "They are used in a plethora of applications, such as compression [40], denoising [4], inpainting [45], super-resolution [25], semisupervised learning [36], clustering [39], and deep neural networks pre-training [15] in cases where labeled data is expensive.", "startOffset": 167, "endOffset": 171}, {"referenceID": 14, "context": "They are used in a plethora of applications, such as compression [40], denoising [4], inpainting [45], super-resolution [25], semisupervised learning [36], clustering [39], and deep neural networks pre-training [15] in cases where labeled data is expensive.", "startOffset": 211, "endOffset": 215}, {"referenceID": 42, "context": "For instance, if images from a database of criminals are used to train a face generation algorithm [43], inference on training set membership might lead to learn-", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "Note that image synthesis is already a common technique for creating large datasets in the field of healthcare [29, 8, 9].", "startOffset": 111, "endOffset": 121}, {"referenceID": 7, "context": "Note that image synthesis is already a common technique for creating large datasets in the field of healthcare [29, 8, 9].", "startOffset": 111, "endOffset": 121}, {"referenceID": 8, "context": "Note that image synthesis is already a common technique for creating large datasets in the field of healthcare [29, 8, 9].", "startOffset": 111, "endOffset": 121}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In order to perform membership inference on generative machine learning models, we train a Generative Adversarial Network (GAN) model [17] on samples generated from the target model.", "startOffset": 134, "endOffset": 138}, {"referenceID": 34, "context": "We test our attacks on several state-of-the-art models such as Deep Convolutional GAN (DCGAN) [35], Boundary Equilibrium GAN (BEGAN) [5], and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE) [24], using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy), containing rich details both in the foreground and background.", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "We test our attacks on several state-of-the-art models such as Deep Convolutional GAN (DCGAN) [35], Boundary Equilibrium GAN (BEGAN) [5], and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE) [24], using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy), containing rich details both in the foreground and background.", "startOffset": 133, "endOffset": 136}, {"referenceID": 23, "context": "We test our attacks on several state-of-the-art models such as Deep Convolutional GAN (DCGAN) [35], Boundary Equilibrium GAN (BEGAN) [5], and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE) [24], using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy), containing rich details both in the foreground and background.", "startOffset": 210, "endOffset": 214}, {"referenceID": 18, "context": "A few efforts [19, 3, 14] focus on inferring the presence of genomic data of particular individuals within a dataset of complex genomic mixtures.", "startOffset": 14, "endOffset": 25}, {"referenceID": 2, "context": "A few efforts [19, 3, 14] focus on inferring the presence of genomic data of particular individuals within a dataset of complex genomic mixtures.", "startOffset": 14, "endOffset": 25}, {"referenceID": 13, "context": "A few efforts [19, 3, 14] focus on inferring the presence of genomic data of particular individuals within a dataset of complex genomic mixtures.", "startOffset": 14, "endOffset": 25}, {"referenceID": 18, "context": "[19] determine whether an individual is in a mixture by comparing a distance measure between the individual and the mixture versus the individual and the general population.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": ", on distributed recommender systems [7], infer which inputs cause output changes by looking at temporal patterns of the model.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "[2] present a few attacks against SVM and HMM classifiers aimed to reconstruct general statistics about training sets by exploiting knowledge of model parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "[38] present membership inference attacks on supervised models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "In fact, detecting overfitting in generative models is regarded as one of the most important research problems in machine learning [44].", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "In [16], an attacker relies on outputs from a given machine learning model to infer sensitive features used as inputs to the model itself.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "However, such an attack cannot be achieved by any statistical model that can generalize on inputs not seen at training time, thus, the attacker simply replies on statistical inference about the total population [27].", "startOffset": 211, "endOffset": 215}, {"referenceID": 40, "context": "[41] present a model extraction attack that aims to infer the parameters from a trained classifier, however, it cannot be applied to scenarios where the attacker does not have access to the probabilities returned for each class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] propose a white-box attack against the privacy-preserving distributed deep learning framework proposed in [37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[18] propose a white-box attack against the privacy-preserving distributed deep learning framework proposed in [37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 30, "context": ", malicious inputs modified to yield erroneous model outputs \u2013 in machine learning in [31].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "In particular, [32] study adversarial samples in deep neural networks, showing that inputs crafted by an adversary might result in deep neural networks to misclassify them.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "Defense mechanisms based on secure multiparty computation and homomorphic encryption have been proposed to train supervised machine learning models, such as decision trees [26], linear regressors [11], and neural networks [6, 10].", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "Defense mechanisms based on secure multiparty computation and homomorphic encryption have been proposed to train supervised machine learning models, such as decision trees [26], linear regressors [11], and neural networks [6, 10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 5, "context": "Defense mechanisms based on secure multiparty computation and homomorphic encryption have been proposed to train supervised machine learning models, such as decision trees [26], linear regressors [11], and neural networks [6, 10].", "startOffset": 222, "endOffset": 229}, {"referenceID": 9, "context": "Defense mechanisms based on secure multiparty computation and homomorphic encryption have been proposed to train supervised machine learning models, such as decision trees [26], linear regressors [11], and neural networks [6, 10].", "startOffset": 222, "endOffset": 229}, {"referenceID": 11, "context": "On the other hand, Differential Privacy [12] can mitigate inference attacks, and it has been widely applied to various machine learning models [1, 23, 30, 37, 42].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "On the other hand, Differential Privacy [12] can mitigate inference attacks, and it has been widely applied to various machine learning models [1, 23, 30, 37, 42].", "startOffset": 143, "endOffset": 162}, {"referenceID": 22, "context": "On the other hand, Differential Privacy [12] can mitigate inference attacks, and it has been widely applied to various machine learning models [1, 23, 30, 37, 42].", "startOffset": 143, "endOffset": 162}, {"referenceID": 29, "context": "On the other hand, Differential Privacy [12] can mitigate inference attacks, and it has been widely applied to various machine learning models [1, 23, 30, 37, 42].", "startOffset": 143, "endOffset": 162}, {"referenceID": 36, "context": "On the other hand, Differential Privacy [12] can mitigate inference attacks, and it has been widely applied to various machine learning models [1, 23, 30, 37, 42].", "startOffset": 143, "endOffset": 162}, {"referenceID": 41, "context": "On the other hand, Differential Privacy [12] can mitigate inference attacks, and it has been widely applied to various machine learning models [1, 23, 30, 37, 42].", "startOffset": 143, "endOffset": 162}, {"referenceID": 36, "context": "Shokri and Shmatikov [37] support distributed training of deep learning networks in a privacy-preserving way, i.", "startOffset": 21, "endOffset": 25}, {"referenceID": 0, "context": "[1] show how to train deep neural networks with non-convex objectives, under an acceptable privacy budget, while Papernot et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[30] combine multiple models trained with disjoint datasets without exposing the (possibly sensitive) models, with applications to non-convex models like deep neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Finally, [33] presents a mechanism called \u201cdefensive distillation\u201d to reduce the effectiveness of adversarial samples on deep neural networks.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": ", combining high utility and privacy levels) differentially private mechanisms that can be applied to complex deep learning generative models, such as Generative Adversarial Networks (GAN) [17] or Variational Auto-encoders (VAE) [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": ", combining high utility and privacy levels) differentially private mechanisms that can be applied to complex deep learning generative models, such as Generative Adversarial Networks (GAN) [17] or Variational Auto-encoders (VAE) [22].", "startOffset": 229, "endOffset": 233}, {"referenceID": 16, "context": "Generative models based on deep neural networks, such as Generative Adversarial Networks (GAN) [17] and Variational Auto-encoders (VAE) [22] are considered as the state-of-the-art for producing samples of realistic images [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "Generative models based on deep neural networks, such as Generative Adversarial Networks (GAN) [17] and Variational Auto-encoders (VAE) [22] are considered as the state-of-the-art for producing samples of realistic images [21].", "startOffset": 136, "endOffset": 140}, {"referenceID": 20, "context": "Generative models based on deep neural networks, such as Generative Adversarial Networks (GAN) [17] and Variational Auto-encoders (VAE) [22] are considered as the state-of-the-art for producing samples of realistic images [21].", "startOffset": 222, "endOffset": 226}, {"referenceID": 16, "context": "GANs [17] are neural networks trained in an adversarial manner to generate data mimicking some distribution.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "We also define a discriminator D(x; \u03b8d) that outputs D(x) \u2208 [0, 1], representing the probability that x was taken from the training set rather than from the generator G.", "startOffset": 60, "endOffset": 66}, {"referenceID": 16, "context": "After several steps of training, if G and D have enough capacity, they will reach a point at which both cannot improve [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "VAEs [22] consist of two neural networks (an encoder and a decoder) and a loss function.", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "[24] combine VAEs and GANs into an unsupervised generative model that simultaneously learns to encode and generate new samples, which contain more details, sampled from the training data-points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "For instance, social graph knowledge has been used to de-anonymize social networks [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "Overall, auxiliary/incomplete knowledge of sensitive datasets is a common assumption in prior work [34, 20].", "startOffset": 99, "endOffset": 107}, {"referenceID": 19, "context": "Overall, auxiliary/incomplete knowledge of sensitive datasets is a common assumption in prior work [34, 20].", "startOffset": 99, "endOffset": 107}, {"referenceID": 35, "context": "The attacker model is trained using soft and noisy labels as suggested in [36], i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "Since the introduction of GANs [17], several variants have been proposed aiming to improve training stability and sample quality.", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "In particular, deep convolutional generative adversarial networks (DCGANs) [35] combine the GAN training process with convolutional neural networks (CNNs).", "startOffset": 75, "endOffset": 79}, {"referenceID": 34, "context": "CNNs are considered the state-of-the-art for a range image recognition tasks and, by combining CNNs with the GAN training processes, DCGANs perform well at unsupervised learning tasks such as generating complex representations of objects and faces [35].", "startOffset": 248, "endOffset": 252}, {"referenceID": 23, "context": "GANs have also been combined with VAEs [24], by collapsing the generator (of the GAN) and decoder (of the VAE) into one, the model uses learned feature representations in the GAN discriminator as the reconstructive error term in the VAE.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "8 More recently, Boundary Equilibrium GAN (BEGAN) [5] have been proposed as an approximate measure of convergence.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "BEGAN produces realistic samples [5], and is simpler to train since loss convergence and sample quality is linked with one another.", "startOffset": 33, "endOffset": 36}, {"referenceID": 37, "context": "In the Appendix, we also discuss another unsuccessful technique, based on training a shadow model (somewhat inspired by techniques presented in [38]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 37, "context": "Defense strategies against membership inference discussed in [38], such as restricting the prediction vector to the top k classes, coarsening and increasing the entropy of the prediction vector, are not suitable to our attacks, since generative models do not output prediction vectors.", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "Advances in machine learning are paving the way for the artificial generation of high-quality images and videos. In this paper, however, we show that generating synthetic samples with generative models can lead to information leakage, i.e., an adversary might infer information about individuals whose data is used to train the models. To this end, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets by relying on the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of samples in the dataset. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), as well as medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones can infer training set membership with up to over 80% accuracy.", "creator": "LaTeX with hyperref package"}}}