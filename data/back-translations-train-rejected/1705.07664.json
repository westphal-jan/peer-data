{"id": "1705.07664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters", "abstract": "The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute localized regular filters on graphs that specialize on frequency bands of interest. Our model scales linearly with the size of the input data for sparsely-connected graphs, can handle different constructions of Laplacian operators, and typically requires less parameters than previous models. Extensive experimental results show the superior performance of our approach on various graph learning problems.", "histories": [["v1", "Mon, 22 May 2017 11:05:37 GMT  (539kb,D)", "http://arxiv.org/abs/1705.07664v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ron levie", "federico monti", "xavier bresson", "michael m bronstein"], "accepted": false, "id": "1705.07664"}, "pdf": {"name": "1705.07664.pdf", "metadata": {"source": "CRF", "title": "CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters", "authors": ["Ron Levie", "Federico Monti", "Xavier Bresson", "Michael M. Bronstein"], "emails": ["ronlevie@post.tau.ac.il", "federico.monti@usi.ch", "michael.bronstein@usi.ch", "xbresson@ntu.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who live in the US are able to survive themselves, and they are able to survive themselves, \"he told the German Press Agency.\" I don't think they are able to survive me, \"he said.\" I don't think they are able to survive me. \"He added,\" I don't think they are able to survive me. \"Indeed,\" I don't think they are able to survive themselves. \"He added,\" I don't think they are able to survive themselves. \"Indeed,\" I don't think they are able to survive me, but I don't think they are able to survive me. \""}, {"heading": "2 Spectral techniques for deep learning on graphs", "text": "The question that arises is whether this is a \"normal\" graph or a \"disordered\" graph represented by a symmetrical graph. (...) The unnormalized graphs are an n \"symmetrical positive-semi-definitory matrix.\" (...) The unnormalized graphs are an n \"symmetrical positive-semi-definitory matrix.\" (...) The normalized graphs are defined as \"n.\" (...) The normalized graphs are defined as \"D \u2212 1 / 2.\" (...) The normalized graphs are defined as D \u2212 1 / 2WD \u2212 1. (...) In the following, we will use the generic notation, on some laplacians.Both are standardized and normalized."}, {"heading": "3 Cayley filters", "text": "A key construction of this paper is a family of complex filters that enjoy the advantages of Chebyshev filters while avoiding some of their disadvantages. A Cayley filter of order r is a real rated function with complex coefficients, gc, h (\u03bb) = c0 + 2Re {r = 1 cj = 1 cj (h\u03bb \u2212 i) -j (5), where c = (c0, h) is a vector of a real coefficient and r complex coefficients and h > 0 is the spectral zoom parameter. A Cayley filter G is a spectral filter that is defined on real signals f byGf = gc, h (cr) is a vector of a real coefficient and h > 0 is the spectral zoom parameter. A Cayley filter G is a spectral filter that is defined on real signals f = gc."}, {"heading": "4 Results", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think they will be able to change the world. \""}, {"heading": "5 Conclusions", "text": "Our architecture is based on a new class of complex rational Cayley filters located in space that can represent any smooth spectral transmission function and are highly regular. A key feature of our model is its ability to specialize in narrow frequency bands with a small number of filter parameters while maintaining locality in the spatial range. We experimentally validated these theoretical properties and demonstrated the superior performance of our model in a wide range of graph learning problems. In future work, we will explore more modern linear solvers instead of the Jacobi method built into native CUDA as TensorFlow OP."}, {"heading": "Acknowledgment", "text": "FM and MB are partially supported by ERC Starting Grant No. 307047 (COMET), ERC Consolidator Grant No. 724228 (LEMAN), Google Faculty Research Award, Nvidia Equipment Grant, Radcliffe Fellowship from Harvard Institute for Advanced Study and TU Munich Institute for Advanced Study, funded by the German Excellence Initiative and the Seventh Framework Programme of the European Union under Funding Agreement No. 291763. XB is partially supported by NRF Fellowship NRFF2017-10."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Proposition 1", "text": "In our case it is necessary to observe that when we begin with the initial condition y (0) j = 0, the following iteration y (0) j = bj turns out that is the initial condition of our design. As we thus begin with the initial condition y (0) j = 0, the next iteration y (0) j = bj turns out that is the initial condition of our design. As we thus start with the initial condition yj (h) y (h) y (h) y (h) y (K) j = y (K) j (h) j (h) y (h) y (j) j (h) j (h) y (h) y (j) j (h) j (h) j) j (j) j (j) j (j) j (j) j (j) j) j (j) j (j) j (j) j) j (j) j (j) j (j) j) j (j) j (j) j) j (j) j (j) j (j)."}, {"heading": "Proof of Theorem 4", "text": "In this proof we approach G\u0441m by G \u0448\u0435\u0441m. Therefore, note that the signal \u03b4m is supported on a vertex, and in the calculation of G \u0441\u0435m each Jacobi iteration increases the support of the signal by 1 hops. Therefore, the support of G hops is the r (K + 1) hop neighbourhood no (K + 1), m of m. Characteristic of l = r (K + 1), and using Proposition 1 we get the indication G\u0441m-G\u0441m | Nl, m-2, m-G\u0441m-G-2, m-G\u0441m-G-2, m-G\u0441m-G-2, m-G\u0441m-G-2, m-G\u0441m-G-2, m-G-2 (13)."}], "references": [{"title": "Diffusion-convolutional neural networks", "author": ["J. Atwood", "D. Towsley"], "venue": "arXiv:1511.02136v2", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks", "author": ["D. Boscaini", "J. Masci", "S. Melzi", "M.M. Bronstein", "U. Castellani", "P. Vandergheynst"], "venue": "Computer Graphics Forum, 34(5):13\u201323", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning shape correspondence with anisotropic convolutional neural networks", "author": ["D. Boscaini", "J. Masci", "E. Rodol\u00e0", "M.M. Bronstein"], "venue": "Proc. NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Anisotropic diffusion descriptors", "author": ["D. Boscaini", "J. Masci", "E. Rodol\u00e0", "M.M. Bronstein", "D. Cremers"], "venue": "Computer Graphics Forum, 35(2):431\u2013441", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Geometric deep learning: going beyond euclidean data", "author": ["M.M. Bronstein", "J. Bruna", "Y. LeCun", "A. Szlam", "P. Vandergheynst"], "venue": "arXiv:1611.08097", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "Proc. ICLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Comm. ACM, 55(6):111\u2013119", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting Receptive Fields in Deep Networks", "author": ["A. Coates", "A. Ng"], "venue": "Proc. NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"], "venue": "Proc. NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Weighted graph cuts without eigenvectors a multilevel approach", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "Trans. PAMI, 29(11)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["D.K. Duvenaud"], "venue": "In Proc. NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A new model for learning in graph domains", "author": ["M. Gori", "G. Monfardini", "F. Scarselli"], "venue": "Proc. IJCNN", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "A generalization of convolutional neural networks to graph-structured data", "author": ["Y. Hechtlinger", "P. Chakravarti", "J. Qin"], "venue": "arXiv:1704.08165", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv:1506.05163", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Provable inductive matrix completion", "author": ["P. Jain", "I.S. Dhillon"], "venue": "arXiv:1306.0626", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion on graphs", "author": ["V. Kalofolias", "X. Bresson", "M.M. Bronstein", "P. Vandergheynst"], "venue": "arXiv:1408.1717", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv:1412.6980", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "arXiv:1609.02907", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Distance metric learning using graph convolutional networks: Application to functional brain networks", "author": ["S.I. Ktena", "S. Parisot", "E. Ferrante", "M. Rajchl", "M. Lee", "B. Glocker", "D. Rueckert"], "venue": "arXiv:1703.02161", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 86(11):2278\u20132324", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Gated graph sequence neural networks", "author": ["Y. Li", "D. Tarlow", "M. Brockschmidt", "R. Zemel"], "venue": "arXiv:1511.05493", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Geodesic convolutional neural networks on Riemannian manifolds", "author": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"], "venue": "Proc. 3DRR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "MovieLens unplugged: experiences with an occasionally connected recommender system", "author": ["B.N. Miller"], "venue": "In Proc. Intelligent User Interfaces,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Geometric deep learning on graphs and manifolds using mixture model CNNs", "author": ["F. Monti", "D. Boscaini", "J. Masci", "E. Rodol\u00e0", "J. Svoboda", "M.M. Bronstein"], "venue": "Proc. CVPR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Geometric matrix completion with recurrent multi-graph neural networks", "author": ["F. Monti", "M.M. Bronstein", "X. Bresson"], "venue": "arXiv:1704.06803", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["N. Rao", "H.-F. Yu", "P.K. Ravikumar", "I.S. Dhillon"], "venue": "Proc. NIPS", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "The graph neural network model", "author": ["F. Scarselli", "M. Gori", "A.C. Tsoi", "M. Hagenbuchner", "G. Monfardini"], "venue": "IEEE Trans. Neural Networks, 20(1):61\u201380", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Galligher", "T. Eliassi-Rad"], "venue": "AI Magazine, 29(3):93", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Sig. Proc. Magazine, 30(3):83\u201398", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multiagent communication with backpropagation", "author": ["S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv:1605.07736", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "author": ["M. Xu", "R. Jin", "Z.-H. Zhou"], "venue": "Proc. NIPS", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "The recent success of deep neural networks and, in particular, convolutional neural networks (CNNs) [20] have raised the interest in geometric deep learning techniques trying to extend these models to data residing on graphs and manifolds.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 1, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 3, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 2, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 23, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 18, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 153, "endOffset": 157}, {"referenceID": 4, "context": "For a comprehensive presentation of methods and applications of deep learning on graphs and manifolds, we refer the reader to the review paper [5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 11, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 114, "endOffset": 122}, {"referenceID": 29, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 114, "endOffset": 122}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "formulated convolutionlike operations in the spectral domain, using the graph Laplacian eigenbasis as an analogy of the Fourier transform [29].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "[14] used smooth parametric spectral filters in order to achieve localization in the spatial domain and keep the number of filter parameters independent of the input size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] proposed an efficient filtering scheme using recurrent Chebyshev polynomials applied on the Laplacian operator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Kipf and Welling [18] simplified this architecture", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "Atwood and Towsley [1] proposed a Diffusion CNN architecture based on random walks on graphs.", "startOffset": 19, "endOffset": 22}, {"referenceID": 23, "context": "[24] (and later, [13]) proposed a spatial-domain generalization of CNNs to graphs using local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[24] (and later, [13]) proposed a spatial-domain generalization of CNNs to graphs using local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "In [25], graph CNNs were extended to multiple graphs and applied to matrix completion and recommender system problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In this paper, we construct graph CNNs employing an efficient spectral filtering scheme based on Cayley polynomials that enjoys similar advantages of the Chebyshev filters [9] such as localization and linear complexity.", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "The main advantage of our filters over [9] is their ability to detect narrow frequency bands of importance during training, and to specialize on them while being well-localized on the graph.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "[6] used the spectral definition of convolution (1) to generalize CNNs on graphs, with a spectral convolutional layer of the form", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Third, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain (locality property simulates local reception fields [8]); assuming k = O(n) Laplacian eigenvectors are used, a spectral convolutional layer requires O(pqk) = O(n) parameters to train.", "startOffset": 169, "endOffset": 172}, {"referenceID": 13, "context": "[14] argued that smooth spectral filter coefficients result in spatially-localized filters (an argument similar to vanishing moments).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] used parametric functions of the form", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] used polynomial filters represented in the Chebyshev basis", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 17, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 23, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 23, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 8, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "SGD+Momentum and Adam [17] optimization methods were used to train the models in MNIST and the rest of the experiments, respectively.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": "Following [9, 24], we approached the classical MNIST digits classification as a learning problem on graphs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 23, "context": "Following [9, 24], we approached the classical MNIST digits classification as a learning problem on graphs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 9, "context": "We used a graph CNN architecture with two spectral convolutional layers based on Chebyshev and Cayley filters (producing 32 and 64 output features, respectively), interleaved with pooling layers performing 4-times graph coarsening using the Graclus algorithm [10], and finally a fully-connected layer (this architecture replicates the classical LeNet5 [20] architecture, which is shown for comparison).", "startOffset": 259, "endOffset": 263}, {"referenceID": 19, "context": "We used a graph CNN architecture with two spectral convolutional layers based on Chebyshev and Cayley filters (producing 32 and 64 output features, respectively), interleaved with pooling layers performing 4-times graph coarsening using the Graclus algorithm [10], and finally a fully-connected layer (this architecture replicates the classical LeNet5 [20] architecture, which is shown for comparison).", "startOffset": 352, "endOffset": 356}, {"referenceID": 27, "context": "Next, we address the problem of vertex classification on graphs using the popular CORA citation graph [28].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "We use the architecture introduced in [18, 24] (two spectral convolutional layers with 16 and 7 outputs, respectively) for solving the vertex classification task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 23, "context": "We use the architecture introduced in [18, 24] (two spectral convolutional layers with 16 and 7 outputs, respectively) for solving the vertex classification task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 6, "context": "Method RMSE MC [7] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "973 IMC [15, 31] 1.", "startOffset": 8, "endOffset": 16}, {"referenceID": 30, "context": "973 IMC [15, 31] 1.", "startOffset": 8, "endOffset": 16}, {"referenceID": 15, "context": "653 GMC [16] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "996 GRALS [26] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "945 sRGCNNCheby [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Method Accuracy DCNN [1] 86.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "60% ChebNet [9] 87.", "startOffset": 12, "endOffset": 15}, {"referenceID": 17, "context": "12% GCN [18] 87.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "In our final experiment, we applied CayleyNet to recommendation system, formulated as matrix completion problem on user and item graphs [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "[24] approached this problem as learning with a Recurrent Graph CNN (RGCNN) architecture, using an extension of ChebNets to matrices defined on multiple graphs in order to extract spatial features from the score matrix; these features are then fed into an RNN producing a sequential estimation of the missing scores.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Here, we repeated verbatim their experiment on the MovieLens dataset [23], replacing Chebyshev filters with Cayley filters.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Our version of sRGCNN outperforms all the competing methods, including the previous result with Chebyshev filters reported in [24].", "startOffset": 126, "endOffset": 130}], "year": 2017, "abstractText": "The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute localized regular filters on graphs that specialize on frequency bands of interest. Our model scales linearly with the size of the input data for sparsely-connected graphs, can handle different constructions of Laplacian operators, and typically requires less parameters than previous models. Extensive experimental results show the superior performance of our approach on various graph learning problems.", "creator": "LaTeX with hyperref package"}}}