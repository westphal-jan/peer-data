{"id": "1705.07706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "An Out-of-the-box Full-network Embedding for Convolutional Neural Networks", "abstract": "Transfer learning for feature extraction can be used to exploit deep representations in contexts where there is very few training data, where there are limited computational resources, or when tuning the hyper-parameters needed for training is not an option. While previous contributions to feature extraction propose embeddings based on a single layer of the network, in this paper we propose a full-network embedding which successfully integrates convolutional and fully connected features, coming from all layers of a deep convolutional neural network. To do so, the embedding normalizes features in the context of the problem, and discretizes their values to reduce noise and regularize the embedding space. Significantly, this also reduces the computational cost of processing the resultant representations. The proposed method is shown to outperform single layer embeddings on several image classification tasks, while also being more robust to the choice of the pre-trained model used for obtaining the initial features. The performance gap in classification accuracy between thoroughly tuned solutions and the full-network embedding is also reduced, which makes of the proposed approach a competitive solution for a large set of applications.", "histories": [["v1", "Mon, 22 May 2017 13:14:11 GMT  (182kb,D)", "http://arxiv.org/abs/1705.07706v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["dario garcia-gasulla", "armand vilalta", "ferran par\\'es", "jonatan moreno", "eduard ayguad\\'e", "jesus labarta", "ulises cort\\'es", "toyotaro suzumura"], "accepted": false, "id": "1705.07706"}, "pdf": {"name": "1705.07706.pdf", "metadata": {"source": "CRF", "title": "An Out-of-the-box Full-network Embedding for Convolutional Neural Networks", "authors": ["Dario Garcia-Gasulla", "Armand Vilalta"], "emails": ["dario.garcia@bsc.es", "armand.vilalta@bsc.es", "ferran.pares@bsc.es"], "sections": [{"heading": "1 Introduction", "text": "The key to the success of these methods lies in the rich representations of models generated after an exhaustive and computationally complex learning process. For any domain or application in which one of these factors is a problem, the formation of a deep model is fundamentally unfeasible. Both authors have equally contributed to improving Xiaomi's functioning by focusing on workforce and optimal hyperparameterization."}, {"heading": "2 Related Work", "text": "However, to mitigate this limitation, it was proposed to use carefully selected parts of the t0 dataset in the fine-tuning process alongside t1 (i.e., selective joint fine-tuning), and also to use large amounts of noisy web images alongside clean curated data [10]. In the fine-tuning process, it was proposed which layers of weights should be transferred from the t0 model and which should be transferred and maintained unchanged to the t1 training phase. Extensive research on this topic has shown that the optimal policy largely depends on the properties of both t0 and t1."}, {"heading": "3 Full-network Embedding", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "3.1 Feature Discretization", "text": "The embedding vectors we create consist of a large number of characteristics. Exploring a representation space of such high-dimensional characteristics is problematic for most machine learning algorithms, as it can lead to overadjustments and other problems associated with the curse of dimensionality. A common solution is to use dimensionality reduction techniques such as PCA [14, 7]. We propose an alternative approach that maintains the same number of characteristics (and thus the size of the representational language defined by embedding), but reduces their expressiveness. We discredit any standardized characteristic value to represent either an atypically low value (-1), a typical value (0), or an atypically high value (1). This discreditation occurs by assigning characteristic values to the {\u2212 1, 0, 1} domain by defining two thresholds to define two thresholds."}, {"heading": "4 Datasets", "text": "One of the objectives of this paper is to identify a complete network extraction method that provides the above-mentioned results in practice. To this end, we evaluate the embedding in the system in a series of 9 data sets that present a challenge for all types of objects, including the classification into the category of fine-grained objects that are able to be located."}, {"heading": "5 Classification Experiments", "text": "In this section, we analyze the performance gap between thoroughly tuned models (those that currently provide state-of-the-art results) and the approach described in \u00a7 3. To evaluate the consistency of our method out-of-the-box, we decide not to use additional data if it is available on the data set (e.g. image segmentation, regions of interest, or other metadata), or perform any other type of problem-specific adjustment (e.g. tuning hyperparameters). As a source model for the function extraction process, we use the classic VGG16 CNN architecture [18], which is pre-trained on the Places2 scene detection data sets [29] for the 67 experiments and the same VGG16 architecture that is pre-trained on the ImageNet 2012 classification data set [30] for the rest (these define our t0 tasks). As a result, the proposed embedding in the data set of 12.416 years is put together."}, {"heading": "5.1 Classification Results", "text": "The results of our study are listed in Table 3. Performance is measured by average per-class knowledge. For each database we provide, the accuracy with which we orient ourselves to the basics depends on our methodology. We show that we use external data to improve the quality of the data. (This means that we are able to trump ourselves.) It is the best way we use it. (This means that we are able to trump ourselves.) The best way we use it is the same. (This means that we are able to trump ourselves.) \"It is the best way we can trump ourselves.\" (...) \"It is the world to trump ourselves.\""}, {"heading": "5.2 Study of Variants", "text": "In this section, we will consider removing and modifying some of the components of the whole network to understand their effects."}, {"heading": "6 Conclusions", "text": "In this paper, we describe a process of feature extraction that effectively utilizes the information encoded in all features of a deep CNN. Full network embedding introduces the use of feature standardization and a novel feature discretization method. The former provides context-dependent embedding that adapts the representations to the problem at hand. The latter reduces noise and regulates the embedding space while maintaining the size of the original representation language (i.e. the pre-schooled model used as the source). Significantly, the discretization of the feature limits the resulting computational effort in processing much larger embedding when training an SVM. Our experiments also show that the full network is more robust than single-layer embedding when a suitable source model is not available. The resulting full network embedding shows that it delivers the single-layer embedding task in these reported classification tasks (and the best wood)."}, {"heading": "Acknowledgements", "text": "This work is partially supported by the Joint Study Agreement No. W156463 under the IBM / BSC Deep Learning Center Agreement, by the Spanish Government through Programa Severo Ochoa (SEV-2015-0493), by the Spanish Ministry of Science and Technology through the TIN2015-65316-P project, and by the Generalitat de Catalunya (Contracts 2014-SGR-1051), and by the Core Research for Evolutional Science and Technology (CREST) program of the Japanese Science and Technology Agency (JST)."}], "references": [{"title": "Augmenting strong supervision using web data for fine-grained categorization", "author": ["Zhe Xu", "Shaoli Huang", "Ya Zhang", "Dacheng Tao"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Bird species categorization using pose normalized deep convolutional nets", "author": ["Steve Branson", "Grant Van Horn", "Serge Belongie", "Pietro Perona"], "venue": "arXiv preprint arXiv:1406.2952,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Deepfood: Deep learning-based food image recognition for computer-aided dietary assessment", "author": ["Chang Liu", "Yu Cao", "Yan Luo", "Guanling Chen", "Vinod Vokkarane", "Yunsheng Ma"], "venue": "In International Conference on Smart Homes and Health Telematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning", "author": ["Weifeng Ge", "Yizhou Yu"], "venue": "arXiv preprint arXiv:1702.08690,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Neural activation constellations: Unsupervised part model discovery with convolutional networks", "author": ["Marcel Simon", "Erik Rodner"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Factors of transferability for a generic convnet representation", "author": ["Hossein Azizpour", "Ali Sharif Razavian", "Josephine Sullivan", "Atsuto Maki", "Stefan Carlsson"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Yunchao Gong", "Liwei Wang", "Ruiqi Guo", "Svetlana Lazebnik"], "venue": "In European conference on computer vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "The unreasonable effectiveness of noisy data for finegrained recognition", "author": ["Jonathan Krause", "Benjamin Sapp", "Andrew Howard", "Howard Zhou", "Alexander Toshev", "Tom Duerig", "James Philbin", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "author": ["Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael I Jordan"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In Icml,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deep convolutional features for image based retrieval and scene categorization", "author": ["Arsalan Mousavian", "Jana Kosecka"], "venue": "arXiv preprint arXiv:1509.06033,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A generic deep-learning-based approach for automated surface inspection", "author": ["Ruoxu Ren", "Terence Hung", "Kay Chen Tan"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "On the behavior of convolutional nets for feature extraction", "author": ["Dario Garcia-Gasulla", "Ferran Par\u00e9s", "Armand Vilalta", "Jonatan Moreno", "Eduard Ayguad\u00e9", "Jes\u00fas Labarta", "Ulises Cort\u00e9s", "Toyotaro Suzumura"], "venue": "arXiv preprint arXiv:1703.01127,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["Mircea Cimpoi", "Subhransu Maji", "Andrea Vedaldi"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Recognizing indoor scenes", "author": ["Ariadna Quattoni", "Antonio Torralba"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Automated flower classification over a large number of classes", "author": ["Maria-Elena Nilsback", "Andrew Zisserman"], "venue": "In Computer Vision, Graphics & Image Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Cats and dogs", "author": ["Omkar M Parkhi", "Andrea Vedaldi", "Andrew Zisserman", "CV Jawahar"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Novel dataset for fine-grained image categorization: Stanford dogs", "author": ["Aditya Khosla", "Nityananda Jayadevaprakash", "Bangpeng Yao", "Fei-Fei Li"], "venue": "In Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["Li Fei-Fei", "Rob Fergus", "Pietro Perona"], "venue": "Computer vision and Image understanding,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Food-101\u2013mining discriminative components with random forests", "author": ["Lukas Bossard", "Matthieu Guillaumin", "Luc Van Gool"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Describing textures in the wild", "author": ["Mircea Cimpoi", "Subhransu Maji", "Iasonas Kokkinos", "Sammy Mohamed", "Andrea Vedaldi"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Wood inspection with non-supervised clustering", "author": ["Olli Silv\u00e9n", "Matti Niskanen", "Hannu Kauppinen"], "venue": "Machine Vision and Applications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Places: An image database for deep scene understanding", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Antonio Torralba", "Aude Oliva"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Wood defect recognition with self-organizing feature selection. In Photonics for Industrial Applications, pages 385\u2013395", "author": ["Jouko Lampinen", "Seppo Smolander"], "venue": "International Society for Optics and Photonics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 135, "endOffset": 144}, {"referenceID": 1, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 135, "endOffset": 144}, {"referenceID": 2, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 135, "endOffset": 144}, {"referenceID": 3, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 219, "endOffset": 225}, {"referenceID": 4, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 219, "endOffset": 225}, {"referenceID": 5, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 308, "endOffset": 317}, {"referenceID": 6, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 308, "endOffset": 317}, {"referenceID": 7, "context": "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].", "startOffset": 308, "endOffset": 317}, {"referenceID": 3, "context": ", selective joint fine-tuning) [5], and also to use large amounts of noisy web imagery alongside with clean curated data [10].", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": ", selective joint fine-tuning) [5], and also to use large amounts of noisy web imagery alongside with clean curated data [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 5, "context": "Extensive research on that regard has shown that the optimal policy depends mostly on the properties of both t0 and t1 [7, 11, 12].", "startOffset": 119, "endOffset": 130}, {"referenceID": 9, "context": "Extensive research on that regard has shown that the optimal policy depends mostly on the properties of both t0 and t1 [7, 11, 12].", "startOffset": 119, "endOffset": 130}, {"referenceID": 10, "context": "Extensive research on that regard has shown that the optimal policy depends mostly on the properties of both t0 and t1 [7, 11, 12].", "startOffset": 119, "endOffset": 130}, {"referenceID": 5, "context": "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].", "startOffset": 125, "endOffset": 146}, {"referenceID": 6, "context": "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].", "startOffset": 125, "endOffset": 146}, {"referenceID": 7, "context": "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].", "startOffset": 125, "endOffset": 146}, {"referenceID": 11, "context": "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].", "startOffset": 125, "endOffset": 146}, {"referenceID": 12, "context": "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].", "startOffset": 125, "endOffset": 146}, {"referenceID": 13, "context": "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].", "startOffset": 125, "endOffset": 146}, {"referenceID": 11, "context": ", most convolutional layers) are discarded because these \"are unlikely to contain a richer semantic representation than the later feature\" [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "So far, this choice has been supported by performance comparisons of single-layer embeddings, where high-level layer embeddings have been shown to consistently outperform low-level layer embeddings [7, 8].", "startOffset": 198, "endOffset": 204}, {"referenceID": 6, "context": "So far, this choice has been supported by performance comparisons of single-layer embeddings, where high-level layer embeddings have been shown to consistently outperform low-level layer embeddings [7, 8].", "startOffset": 198, "endOffset": 204}, {"referenceID": 14, "context": "However, it is also known that all layers within a deep network, including low-level ones, can contribute to the characterization of the data in different ways [16].", "startOffset": 160, "endOffset": 164}, {"referenceID": 5, "context": "Some of those are evaluated in [7], which includes parameters related with the architecture and training of the initial CNN (e.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].", "startOffset": 90, "endOffset": 96}, {"referenceID": 6, "context": "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].", "startOffset": 90, "endOffset": 96}, {"referenceID": 5, "context": "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].", "startOffset": 176, "endOffset": 185}, {"referenceID": 6, "context": "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].", "startOffset": 176, "endOffset": 185}, {"referenceID": 7, "context": "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].", "startOffset": 176, "endOffset": 185}, {"referenceID": 5, "context": "The quality of the resultant embedding is typically evaluated by the performance of an SVM, trained using the embedding representations, to solve a classification task [7, 8].", "startOffset": 168, "endOffset": 174}, {"referenceID": 6, "context": "The quality of the resultant embedding is typically evaluated by the performance of an SVM, trained using the embedding representations, to solve a classification task [7, 8].", "startOffset": 168, "endOffset": 174}, {"referenceID": 4, "context": "Recently, extracted features have also been combined with more sophisticated computer vision techniques, such as the constellation model [6] and Fisher vectors [17], with significant success.", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "Recently, extracted features have also been combined with more sophisticated computer vision techniques, such as the constellation model [6] and Fisher vectors [17], with significant success.", "startOffset": 160, "endOffset": 164}, {"referenceID": 5, "context": "To tackle this issue, a recurrent solution in the field is to perform a spatial average pooling on each convolutional filter, such that a single value per filter is obtained by averaging all its spatially-depending activations [7, 8].", "startOffset": 227, "endOffset": 233}, {"referenceID": 6, "context": "To tackle this issue, a recurrent solution in the field is to perform a spatial average pooling on each convolutional filter, such that a single value per filter is obtained by averaging all its spatially-depending activations [7, 8].", "startOffset": 227, "endOffset": 233}, {"referenceID": 16, "context": "In the case of the well-known VGG16 architecture [18] this embedding vector is composed by 12,416 features.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "Figure 2: For the mit67 dataset, distribution of average standardized feature values for those features belonging to the sets identified in [16].", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": ", batch normalization) [19], but this is the first time this technique has been applied to a feature extraction solution.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "A common solution is to use dimensionality reduction techniques like PCA [14, 7].", "startOffset": 73, "endOffset": 80}, {"referenceID": 5, "context": "A common solution is to use dimensionality reduction techniques like PCA [14, 7].", "startOffset": 73, "endOffset": 80}, {"referenceID": 14, "context": "[16], who use a supervised statistical approach to evaluate the importance of CNN features for characterization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] to our corresponding feature/image activations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We do so on the datasets explored in [16]: mit67, flowers102 and cub200, by computing the average values of the features belonging to each of the three sets.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "Clearly, a strong correlation exists between the supervised statistic feature relevance defined in [16] and the standardized feature values generated by the full-network embedding, as features in the characteristic by absence set correspond to activations which are particularly low, while features in the characteristic by presence set correspond to activations which are particularly high.", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "The MIT Indoor Scene Recognition dataset [20] (mit67) consists of different indoor scenes to be classified in 67 categories.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "The Oxford Flower dataset [22] (flowers102) is a fine-grained dataset consisting of 102 flower categories.", "startOffset": 26, "endOffset": 30}, {"referenceID": 20, "context": "The Oxford-IIIT-Pet dataset [23] (cats-dogs) is a fine-grained dataset covering 37 different breeds of cats and dogs.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "The Stanford Dogs dataset [24] (sdogs) contains images from the 120 breeds of dogs found in ImageNet.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "The Caltech 101 dataset [25] (caltech101) is a classical dataset of 101 object categories containing clean images with low level of occlusion.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "The Food-101 dataset [26] (food101) is a large dataset of 101 food categories.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "The Describable Textures Dataset [27] (textures) is a database of textures categorized according to a list of 47 terms inspired from human perception.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "The Oulu Knots dataset [28] (wood) contains knot images from spruce wood, classified according to Nordic Standards.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "A specific case is caltech101 where, following the dataset authors instructions [25], we randomly choose 30 training examples per class and a maximum of 50 for test, and repeat this experiment 5 times.", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "As source model for the feature extraction process we use the classical VGG16 CNN architecture [18] pre-trained on the Places2 scene recognition dataset [29] for the mit67 experiments, and the same VGG16 architecture pre-trained on the ImageNet 2012 classification dataset [30] for the rest (these define our t0 tasks).", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "As source model for the feature extraction process we use the classical VGG16 CNN architecture [18] pre-trained on the Places2 scene recognition dataset [29] for the mit67 experiments, and the same VGG16 architecture pre-trained on the ImageNet 2012 classification dataset [30] for the rest (these define our t0 tasks).", "startOffset": 153, "endOffset": 157}, {"referenceID": 27, "context": "As source model for the feature extraction process we use the classical VGG16 CNN architecture [18] pre-trained on the Places2 scene recognition dataset [29] for the mit67 experiments, and the same VGG16 architecture pre-trained on the ImageNet 2012 classification dataset [30] for the rest (these define our t0 tasks).", "startOffset": 273, "endOffset": 277}, {"referenceID": 5, "context": "As discussed in \u00a72, a popular embedding is obtained by extracting the activations of one of the fully connected layers (fc6 or fc7 for the VGG16 model) and applying a L2 normalization per data instance [7, 8, 13].", "startOffset": 202, "endOffset": 212}, {"referenceID": 6, "context": "As discussed in \u00a72, a popular embedding is obtained by extracting the activations of one of the fully connected layers (fc6 or fc7 for the VGG16 model) and applying a L2 normalization per data instance [7, 8, 13].", "startOffset": 202, "endOffset": 212}, {"referenceID": 11, "context": "As discussed in \u00a72, a popular embedding is obtained by extracting the activations of one of the fully connected layers (fc6 or fc7 for the VGG16 model) and applying a L2 normalization per data instance [7, 8, 13].", "startOffset": 202, "endOffset": 212}, {"referenceID": 3, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 19, "endOffset": 22}, {"referenceID": 28, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 28, "endOffset": 31}, {"referenceID": 15, "context": "5 [5] [10] [5] [6] [5] [31] [4] [17] -", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "[10] achieve a remarkable state-of-the-art performance by using lots of additional data (roughly 5 million additional images of birds) to train a deep network from scratch, and then fine-tune the model using the cub200 dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[4] use the complete training set for fine-tuning, while we only use a subset of the test set (see \u00a74 for details).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "The work of [17] obtains the best results for the textures dataset by using a combination of bag-of-visual-words, Fisher vectors and convolutional filters.", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "The wood dataset is designed to be particularly challenging, even for human experts; according to the dataset authors the global accuracy of an experienced human sorter is about 75-85% [32, 28].", "startOffset": 185, "endOffset": 193}, {"referenceID": 25, "context": "The wood dataset is designed to be particularly challenging, even for human experts; according to the dataset authors the global accuracy of an experienced human sorter is about 75-85% [32, 28].", "startOffset": 185, "endOffset": 193}, {"referenceID": 13, "context": "[15], which are 94.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15], which resulted in 71.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Finally, we also considered using different network depths, a parameter also analyzed in [7].", "startOffset": 89, "endOffset": 92}], "year": 2017, "abstractText": "Transfer learning for feature extraction can be used to exploit deep representations in contexts where there is very few training data, where there are limited computational resources, or when tuning the hyper-parameters needed for training is not an option. While previous contributions to feature extraction propose embeddings based on a single layer of the network, in this paper we propose a full-network embedding which successfully integrates convolutional and fully connected features, coming from all layers of a deep convolutional neural network. To do so, the embedding normalizes features in the context of the problem, and discretizes their values to reduce noise and regularize the embedding space. Significantly, this also reduces the computational cost of processing the resultant representations. The proposed method is shown to outperform single layer embeddings on several image classification tasks, while also being more robust to the choice of the pre-trained model used for obtaining the initial features. The performance gap in classification accuracy between thoroughly tuned solutions and the full-network embedding is also reduced, which makes of the proposed approach a competitive solution for a large set of applications.", "creator": "LaTeX with hyperref package"}}}