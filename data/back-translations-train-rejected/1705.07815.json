{"id": "1705.07815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Minimax Statistical Learning and Domain Adaptation with Wasserstein Distances", "abstract": "As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.", "histories": [["v1", "Mon, 22 May 2017 15:50:47 GMT  (16kb)", "http://arxiv.org/abs/1705.07815v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jaeho lee", "maxim raginsky"], "accepted": false, "id": "1705.07815"}, "pdf": {"name": "1705.07815.pdf", "metadata": {"source": "CRF", "title": "Minimax Statistical Learning and Domain Adaptation with Wasserstein Distances\u2217", "authors": ["Jaeho Lee", "Maxim Raginsky"], "emails": ["jlee620@illinois.edu", "maxim@illinois.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.07 815v 1 [cs.L G] 22 May 2"}, {"heading": "1 Introduction and problem set-up", "text": "iSe \"s,\" so iri.P iDe \"iSe,\" so iri.P iDe \"iSe\" n, \"so i.P\" iSe, \"so i.P,\" so i.P, \"so i.P,\" so i.P, \"so i.P,\" so i.P, \"so i.P,\" so i.P, \"so i.P, so i.P,\" so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.D, so i.P, so i.D, so i.D, so i.D, so i.P, so i.D, so i.P, so i.D, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so i.P, so"}, {"heading": "1.1 Wasserstein ambiguity sets and local minimax risk", "text": "We assume that instance space Z is a Polish space (i.e., a fully separable metric space) with metric dZ. We denote by P (Z) the space of all Borel probabilities on Z, and by Pp (Z) with p \u2265 1 the space of all P-P (Z) with finite pth moments: Pp (Z): = {P-P (Z): EP [dpZ (Z, z0)] < p (Z) for some z0-Z}.The metric structure of Z (Z) can be used to define a family of metrics on the spaces Pp (Z) [8]: 1.1. For p-Waterstone distance between P, Q-Pp (Z) isWp (P, Q): = inf M (\u00b7 Z) = P M (Z \u00b7 \u00b7) = Q (M) = Q (EM [d p, Z \u2032)]))."}, {"heading": "1.2 Motivating problem: domain adaptation", "text": "One of the attractive features of ambiguity sets based on Waterstone distances is that due to their close connection to the metric geometry of the instance space (i.e. they provide a natural mechanism for dealing with uncertainty due to (possibly randomized) transformations affecting the problem cases. To illustrate this point, we briefly discuss a motivating example of domain adaptation. In contrast to the standard statistical learning framework, in which the risk of the learned hypothesis is assessed based on the same unknown distribution used to generate the training examples, the problem of domain adaptation [9] arises when the training data is generated according to an unknown distribution P, but the learned hypothesis is assessed based on another unknown distribution Q. However, it is assumed that these distributions (commonly referred to as problem domains) are somehow related, and some partial information about Q are also available during the training. In the context of the supervised learning labels, the instances X and Y are random (examples for Y)."}, {"heading": "2 Local worst-case risk vs. statistical risk", "text": "In some situations (see e.g. section 4) it is interesting that we convert back and forth between the local worst-case risks (or local minimax risks) and the usual statistical risks. In this section we give a few inequalities in relation to these quantities. The first is a simple consequence of Kantorovich's duality theory (or local minimax risks): Suppose f is L-Lipschitz, i.e. f (z) - f (z) - f (z) - f (z) - LdZ (z) for all z, z (z) - z (z) - Z (z) - Z (z) - Z (f), p (P), R (Q) - R\u03b5, p (P, f) - R (Q) + 2L\u03b5.As an example, we consider the problem of binary classification with hinge loss: Z = X \u00b7 Y, where X is an arbitrary attribute space, Y = {\u2212 1}, and the hypothesis of space F exists."}, {"heading": "3 Guarantees for empirical risk minimization", "text": "In this section we will analyze the performance of the ERM method. \u2212 p The results of the following strong duality results due to Gao and Kleywegt [7] will be instrumental: Proposition 3.1. For each upper semi-continuous function f: Z \u2192 R and for each Q question Pp (Z), R\u03b5, p (Q, f) = minimum duality results f: 0 (Z), where the upper semi-continuous function f: Z \u2192 R and for each Q question Pp (Z), R\u03b5, p (F) = minimum dispositives 0 (F), f (Z), f (Z), where the upper semi-continuous function f: Z (Z). \u2212 Z {f (Z)."}, {"heading": "3.1 Example bounds", "text": "In this section we illustrate the use of Theorem 3.1 when (upper limits on) the coverage numbers for the hypothesis class F (1) are available. (r) In this section we leave Z = X \u00b7 Y, where the attribute space X = {x \u00b2 Rd: 2 \u2264 r0} is a ball of radius r0 in Rd with center at origin, the label space Y [\u2212 B, + B] for some 0 < B < p = 2. We equip Z with the Euclidean metricdZ (z, z \u2032) = dZ (x \u00b2), (x \u00b2, y \u00b2): (x \u00b2, y \u00b2 s): 1 x \u2212 x \u00b2 s for some 0 < B < p = 2. We first consider a simple neural network class F, consisting of functions of the form f (z) = f (x, y \u2212 s (fT0 x), where s (fT0 x))."}, {"heading": "3.2 Technical lemmas for the proof of Theorem 3.1", "text": "Lemma 3.1. Correct some Q-Pp (Z). Define f-Pp (F) and f-Pp (Z). Define f-Pp and f-Pp (Z): = argmin f-F R\u03b5, p-F (F, f, f)]. The expected Rademacher complexity of the function class \"satisfiesRn\" (N) \u2264 12 \"n\" 0 \"logN\" (F, E, A / 2) du + 12C0 (2 \"D (Z)) p\" n \"(1 + (D (Z)) p.\""}, {"heading": "4 Application to domain adaptation", "text": "As discussed in Section 1.2, the problem of domain adaptation arises when we want to transfer the data or knowledge from one source domain P-P (Z) to another but related target domain P-Z (Z). Suppose it is possible to estimate the Waterstone distance Wp (P, Q) between the two domain distributions. Then, as we show below, we can generalize for the target domain by estimating guarantees for Wp (P, Q) with risk imbalances of Section 2. We work in the environment considered by Courty et al. [10]: Leave Z = X \u00b7 Y where (X, dX) is the attribute space and (Y) the attribute space. We endow Z with the product metricdZ (z) = dZ (z), y), (x \u00b2, y): (x \u00b2, y \u00b2): (dX) + pdY."}, {"heading": "A Proofs", "text": "The result results from the fact that W1 (Q), Q (Q), Q (Q), Q (F), Q (F), Q (F), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (G), Q (Q), Q (G), Q (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G, G, G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G, G (G), G (G), G (G), G (G), G (G, G (G), G (G), G (G), G (G, G (G), G (G), G (G), G (G, G (G), G (G), G (G, G), G (G (G), G (G), G (G, G (G), G (G), G (G (G), G (G, G), G (G, G, G (G), G (G, G (G), G (G), G (G, G (G), G (G, G (G), G (G, G (G), G (G), G (G (G), G, G (G (G), G (G (G), G, G (G, G), G (G (G, G), G (G, G (G), G (G), G (, G (G (, G), G), G ("}], "references": [{"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["V. Koltchinskii"], "venue": "volume 2033 of Lecture Notes in Mathematics. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A minimax approach to supervised learning", "author": ["F. Farnia", "D. Tse"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Statistics of robust optimization: a generalized empirical likelihood approach", "author": ["J.C. Duchi", "P.W. Glynn", "H. Namkoong"], "venue": "arXiv preprint 1610.03425", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations", "author": ["P. Mohajerin Esfahani", "D. Kuhn"], "venue": "arXiv preprint 1505.05116", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributionally robust logistic regression", "author": ["S. Shafieezadeh-Abadeh", "P. Mohajerin Esfahani", "D. Kuhn"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Distributionally robust stochastic optimization with Wasserstein distance", "author": ["R. Gao", "A.J. Kleywegt"], "venue": "arXiv preprint 1604.02199", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Topics in optimal transportation", "author": ["C. Villani"], "venue": "American Mathematics Society, Providence, RI", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning, 79:151\u2013175", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal transport for domain adaptation", "author": ["N. Courty", "R. Flamary", "D. Tuia", "A. Rakotomamonjy"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient Flows in Metric Spaces and in the Space of Probability Measures", "author": ["L. Ambrosio", "N. Gigli", "G. Savar\u00e9"], "venue": "Birkh\u00e4user", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems", "author": ["M. Talagrand"], "venue": "Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning theory: an approximation theory viewpoint", "author": ["F. Cucker", "D. Zhou"], "venue": "Cambridge University Press, Cambridge, MA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Covering numbers of gaussian reproducing kernel hilbert spaces", "author": ["T. K\u00fchn"], "venue": "Journal of Complexity, pages 489\u2013499", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "On the rate of convergence in wasserstein distance of the empirical measure", "author": ["N. Fournier", "A. Guillin"], "venue": "Probability Theory and Related Fields, pages 1\u201332", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction and problem set-up In the traditional paradigm of statistical learning [1], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z \u2192 R+.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:", "startOffset": 112, "endOffset": 118}, {"referenceID": 1, "context": "Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 77, "endOffset": 86}, {"referenceID": 5, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 77, "endOffset": 86}, {"referenceID": 6, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 77, "endOffset": 86}, {"referenceID": 2, "context": "However, with the exception of the recent work by Farnia and Tse [3], the minimizer of (1.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "(Recently, this modification was also proposed by Farnia and Tse [3] for ambiguity sets defined by a finite number of moment constraints.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "The metric structure of Z can be used to define a family of metrics on the spaces Pp(Z) [8]: Definition 1.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius \u03b5 \u2265 0 centered at P : A(P ) = B \u03b5,p(P ) := { Q \u2208 Pp(Z) : Wp(P,Q) \u2264 \u03b5 } ,", "startOffset": 10, "endOffset": 19}, {"referenceID": 5, "context": "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius \u03b5 \u2265 0 centered at P : A(P ) = B \u03b5,p(P ) := { Q \u2208 Pp(Z) : Wp(P,Q) \u2264 \u03b5 } ,", "startOffset": 10, "endOffset": 19}, {"referenceID": 6, "context": "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius \u03b5 \u2265 0 centered at P : A(P ) = B \u03b5,p(P ) := { Q \u2208 Pp(Z) : Wp(P,Q) \u2264 \u03b5 } ,", "startOffset": 10, "endOffset": 19}, {"referenceID": 8, "context": "In contrast to the standard statistical learning framework, where the risk of the learned hypothesis is evaluated on the same unknown distribution that was used for generating the training examples, the problem of domain adaptation [9] arises when the training data are generated according to an unknown distribution P , but the learned hypothesis is evaluated on another unknown distribution Q.", "startOffset": 232, "endOffset": 235}, {"referenceID": 9, "context": "[10] have introduced an algorithmic framework for domain adaptation based on optimal transport.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10], our approach completely bypasses the problem of estimating the transport map T .", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The first one is a simple consequence of the Kantorovich duality theorem from the theory of optimal transport [8]: Proposition 2.", "startOffset": 110, "endOffset": 113}, {"referenceID": 10, "context": "Since we are working with general metric spaces that may lack an obvious differentiable structure, we need to first introduce some concepts from metric geometry [11].", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "A metric space (Z,dZ) is a geodesic space if for every two points z, z\u2032 \u2208 Z there exists a path \u03b3 : [0, 1] \u2192 Z, such that \u03b3(0) = z, \u03b3(1) = z\u2032, and dZ(\u03b3(s), \u03b3(t)) = (t\u2212 s) \u00b7 dZ(\u03b3(0), \u03b3(1)) for all 0 \u2264 s \u2264 t \u2264 1 (such a path is called a constant-speed", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "A functional F : Z \u2192 R is geodesically convex if for any pair of points z, z\u2032 \u2208 Z there is a constant-speed geodesic \u03b3, so that F (\u03b3(t)) \u2264 (1\u2212 t)F (\u03b3(0)) + tF (\u03b3(1)) = (1\u2212 t)F (z) + tF (z), \u2200t \u2208 [0, 1].", "startOffset": 195, "endOffset": 201}, {"referenceID": 6, "context": "The following strong duality result due to Gao and Kleywegt [7] will be instrumental: Proposition 3.", "startOffset": 60, "endOffset": 63}, {"referenceID": 11, "context": "1) is the Dudley entropy integral [12].", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "The excess risk bound of Farnia and Tse [3] has the same behavior, where in that case \u03b5 is the slack in the moment constraints defining the ambiguity set.", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "1] (which was later shown by K\u00fchn [14] to be asymptotically exact up to the double logarithmic factor): Proposition 3.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "2, the problem of domain adaptation arises when we want to transfer the data or knowledge from a source domain P \u2208 P(Z) to a different but related target domain Q \u2208 P(Z) [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 9, "context": "[10]: Let Z = X \u00d7 Y, where (X , dX ) is the feature space and (Y, dY ) is the label space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "invertible \u2013 in fact, its inverse is equal to the optimal transport map from \u03bd to \u03bc [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "Here, Wp(\u03bcn, \u03bdm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(\u03bc, \u03bcn) and Wp(\u03bd, \u03bdm) rapidly converge to zero as n,m \u2192 \u221e:", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "Here, Wp(\u03bcn, \u03bdm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(\u03bc, \u03bcn) and Wp(\u03bd, \u03bdm) rapidly converge to zero as n,m \u2192 \u221e:", "startOffset": 166, "endOffset": 170}, {"referenceID": 7, "context": "For p = 1, the result follows immediately from the Kantorovich dual representation of W1(\u00b7, \u00b7) [8]: W1(Q,Q ) = sup \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 |EQF \u2212EQ\u2032F | : sup z,z\u2032\u2208Z z 6=z\u2032 |F (z) \u2212 F (z\u2032)| dZ(z, z\u2032) \u2264 1 \uf8fc \uf8f4\uf8fd \uf8f4\uf8fe and from the fact that, for Q,Q\u2032 \u2208 BW \u03b5,1(P ), W1(Q,Q) \u2264 2\u03b5 by the triangle inequality.", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "Hence, X is subgaussian with respect to d\u03a6, and therefore the Rademacher average Rn(\u03a6) can be upper-bounded by the Dudley entropy integral [12]: Rn(\u03a6) \u2264 12 \u221a n \u222b \u221e", "startOffset": 139, "endOffset": 143}], "year": 2017, "abstractText": "As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.", "creator": "LaTeX with hyperref package"}}}