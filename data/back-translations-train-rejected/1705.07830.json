{"id": "1705.07830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning", "abstract": "We propose an active question answering agent that learns to reformulate questions and combine evidence to improve question answering. The agent sits between the user and a black box question-answering system and learns to optimally probe the system with natural language reformulations of the initial question and to aggregate the evidence to return the best possible answer. The system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. Our agent improves F1 by 11% over a state-of-the-art base model that uses the original question/answer pairs.", "histories": [["v1", "Mon, 22 May 2017 16:19:21 GMT  (71kb,D)", "https://arxiv.org/abs/1705.07830v1", null], ["v2", "Wed, 12 Jul 2017 12:21:14 GMT  (66kb,D)", "http://arxiv.org/abs/1705.07830v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["christian buck", "jannis bulian", "massimiliano ciaramita", "wojciech gajewski", "rea gesmundo", "neil houlsby", "wei wang"], "accepted": false, "id": "1705.07830"}, "pdf": {"name": "1705.07830.pdf", "metadata": {"source": "CRF", "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning", "authors": ["Christian Buck", "Jannis Bulian"], "emails": ["cbuck@google.com", "jbulian@google.com", "massi@google.com", "wgaj@google.com", "agesmundo@google.com", "neilhoulsby@google.com", "wangwe@google.com"], "sections": [{"heading": null, "text": "Ask the right questions: Active Question Reformulation with Reinforcement LearningChristian Buck cbuck @ google.com Jannis Bulian jbulian @ google.com Massimiliano Ciaramita massi @ google.com Wojciech Gajewski wgaj @ google.comAndrea Gesmundo agesmundo @ google.com Neil Houlsby neilhoulsby @ google.comWei Wang wangwe @ google.comGoogleJuly 13, 2017We frame Question Answering as a Reinforcement Learning task, an approach we call Active Question Answering. We propose an agent sitting between the user and a black box question system who learns to reformulate questions in order to find the best possible answers. The agent probes the system with potentially many natural language reformulations of a source question and aggregates the evidence returned to provide the best answer. The reformulation system at the end is a complex response system based on Q1 paxiare to find the answers at the end of Q1."}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 Active Question Answering", "text": "In this section, we describe the components of the active answering of questions as shown in Figure 1."}, {"heading": "2.1 The Agent-Environment Framework", "text": "Instead of passively sending a user's question to a QA system, the QA system actively reformulates the question several times and delivers the reformulations; the QA system acts as a black box environment into which QA sends questions and receives answers; the environment returns one or more answers from which the final answer is selected; QA has no access to the interior of the environment and the environment is not trained as part of the AQA agent; the agent must learn to communicate optimally with the environment in order to maximize the likelihood of receiving the correct answer; the environment can in principle include multiple sources of information that may provide feedback in different modes: images, structured data from knowledge databases, unstructured text, search results, etc. Here, the information source consists of a pre-programmed question response system that accepts question strings as input and returns strings as answers."}, {"heading": "2.2 Active Question Answering Agent", "text": "The architecture of the model consists of a multi-layered bidirectional LSTM with attention-based decoding. The main departure from the standard MT setting is that our model formulates statements in the same language. Unlike in MT, few high-quality training data are available. 1We rely on the public implementation2 by Britz et al. [2017]. 2https: / / github.com / google / seq2seq / monolingual paraphrasing. Effective training of highly parameterized neural networks relies on a wealth of data, so our setting presents an additional challenge. We assume that the model is generated first by pre-training on a related task and second by using the endmetric signals."}, {"heading": "2.3 Question-Answering Environment", "text": "For this purpose, we use a competitive model for answering neural questions, Bidirectional Attention Flow (BiDAF) [Seo et al., 2017a].3 BiDAF is an extractive QA system that takes a question and a document as input and returns a continuous span from the document in response. It contains a bidirectional attention mechanism to evaluate snippets of documents in relation to the question, implemented with multi-layered LSTMs and other components. The environment is opaque, the agent does not have access to its internals: parameters, activations, gradients, etc. AQA can only send and receive questions to it. This scenario allows us to design a general framework that allows the use of any backend. However, it means that feedback on the quality of the question formulations is loud and indirect, which poses a challenge for the training."}, {"heading": "3 Training", "text": "To train AQA, we use a combination of reinforcement and supervised learning. We also present a strategy for overcoming the data shortage in monolingual transcription."}, {"heading": "3.1 Question Answering Environment", "text": "We treat BiDAF [Seo et al., 2017a] like a static black box QA system. We train the model on the training set for the present QA task, see Section 4.5 for details. Thereafter, BiDAF becomes part of the environment and its parameters are not updated during the training of the AQA agent. In principle, we could train both the agent and the environment together to further improve performance. However, this is not our desired task: our goal is 3https: / / allenai.github.io / bi-att-flow / so that the agent learns to communicate with an environment over which there is no control. This setting generalizes the interaction with any information source."}, {"heading": "3.2 Policy Gradient Training of the Reformulation Model", "text": "The answer a = f (q) is an unknown function of a question q, which is calculated by the environment. Note that the reward is calculated taking into account the original question q0, while the answer is created using a question q0. The question q (q) is an unknown function of a question q, which is calculated by the parameters of the policy. Politics, in this case a sequence-to-sequence model, assigns a probability check (q | q0) = p (wt | w1,."}, {"heading": "3.3 Initialization of the Reformulation Model", "text": "While parallel corpora are available for many language pairs, English-English corpora are rare, so we cannot train the monolingual model directly. Instead, we first produce a multilingual translation system that translates between multiple languages [Johnson et al., 2016], which allows us to use available bilingual corpora. Multilingual training requires no more than adding two special tokens to the data that specify the source and target languages. 4 However, the encoder decoder architecture of the translation model remains unchanged. As Johnson et al. [2016] show, this model can be used for zero-shot translations, i.e. translations between language pairs for which there have been no training examples. For example, after the training, English-Spanish, French-English, and Spanish-English, the model can be used first by translating a single encoder, English, Spanish, and French, and then twice by translating Spanish for the respective Spanish and then the same model."}, {"heading": "3.4 Answer Selection", "text": "When searching for a beam or sample, we can perform many reformulations of a single question from our reformulation system. We output each rewrite of the QA environment and receive a series of (queries, rewrite, answers) tuples from which we must select the best instance. We train a different neural network to select the best answer from the candidates. Although this is a ranking problem, we render it as a binary classification, distinguishing between above and below average performance. In training, we calculate the F1 score of the answer for each instance. If the rewrite produces an answer with an F1 score greater than the average of the other, the instance is assigned a positive label. We ignore questions where all reformulations yield equally good / bad answers. For the classifier, we evaluated FFNNs, LSTMs and CNNs and found that the performance of all systems was comparable. Since the inputs are triple < the latter two, <"}, {"heading": "4 Experiments", "text": "We are experimenting with a new and challenging set of questionnaires, SearchQA [Dunn et al., 2017]. We show that our environment, BiDAF, is already performing well relative to the published baseline and improving on its own. However, a low absolute performance indicates that the task is difficult. A trained reformulator improves end-to-end performance through a single rewrite, i.e. without aggregation. Our end-to-end system improves relative to BiDAF by 11 F1 points (32%)."}, {"heading": "4.1 Question Answering Data", "text": "SearchQA is a recently released dataset based on a series of Jeopardy! clues. Clues are veiled queries like This \"Father of Our Country\" that didn't really cut down a cherry tree. Each clue is associated with the correct answer, such as George Washington, and a list of snippets from Google's top search results. SearchQA contains over 140k question-answer pairs and 6.9m snippets. We train our model on the predefined training split, perform model selection and validation split matching, and report on the results of the validation and test splits. The training, validation and test sets contain 99,820, 13,393, and 27,248 examples, respectively."}, {"heading": "4.2 Sequence to Sequence Pre-training", "text": "For the pre-training of the reformer, we use the multilingual United Nations Parallel Corpus v1.0 [Ziemski et al., 2016]. This dataset contains 11.4M sentences, which are fully aligned with six UN languages: Arabic, English, Spanish, French, Russian and Chinese. From all bilingual pairs, we generate a multilingual training corpus of 30 language pairs. This results in 340M training examples, which we use to train the zero-shot neural MT system [Johnson et al., 2016] as decoder. We tokenize our data using 16k sets. 5 Following [Britz et al., 2017] we use a bidirectional LSTM as encoder and a 4-layer LSTM with attention [Bahdanau et al., 2016] as decoder. The model we have converted to 400M instances after the training is the Adam optimizer with a learner rate of 0.001 and a batch-128 systematically related source."}, {"heading": "4.3 RL Training of the Reformulator", "text": "After pre-training the reformulator, we switch the optimizer from Adam to SGD and train for 100k RL steps of lot size 64 with a low learning rate of 0.001. We use an entropy regularization weight of \u03bb = 0.001. As a stop criterion, we monitor the reward from the best single paraphrase generated by greedy decoding on the validation set. Unlike our initial training on GPUs, this training phase is dominated by the latency of the QA system and we perform inferences and updates on CPU and the BiDAF environment on GPU."}, {"heading": "4.4 Training the Aggregator", "text": "For the aggregator, we use supervised learning: First, we train the reformulator, then we generate N = 20 paraphrases for each question in the SearchQA training and validation sets. After we send these to the environment, we have about 2M (question, paraphrase, answer) -threefold to train the aggregator. We remove queries where all paraphrases provide identical rewards, removing about half of the aggregation training data. We use pre-formed 100-dimensional embedding [Pennington et al., 2014] for the tokens. Our CNN-based aggregator encodes the three strings into 100-dimensional vectors, with a 1D CNN with core width 3 and output dimension 100 using the embedded token, followed by max pooling. The vectors are then linked and fed through a feed-forward network that produces the binary output, indicating whether we will find triple the average response to each of the other responses for the first two exercises, and the lower one for the second."}, {"heading": "4.5 Baselines and Benchmarks", "text": "As a starting point, we repeat the results of a deep learning system developed for SearchQA Dunn et al. [2017]. This is a modified pointer network called Attention Sum Reader.77 Dunn et al. [2017] also provides a simpler starting point, in which unigrams from the search snippets are evaluated based on their TF-IDF score. This starting line is not comparable to our experiments, as it is only unigramThe BiDAF environment can be used without the reformulator to answer the original question. This corresponds to the raw performance of BiDAF and is our second starting line. We train BiDAF directly on the basis of the SearchQA training data. In the SearchQA task, the answers are extended by several snippets (an average of 50), which are returned by a Google search for the question. We join the snippets to form the context from which BiDAF matches the answer snippets most closely to the top 10 of the response text we select from the Google."}, {"heading": "4.6 AQA Variants", "text": "We evaluate several variants of AQA. For each query q in the evaluation, we generate a list of reformulations qi, for i = 1... N, from the AQA reformulator trained as described in Section 3. In these experiments, we specify N = 20. AQA Top Hyp. First, we omit aggregation and simply select the top hypothesis generated by the sequence model q1.AQA voting. In addition to the most likely response span, BiDAF also reports a model score that we use for a heuristically weighted voting scheme to implement a deterministic aggregator. Let's assume the answer that BiDAF returns for query q with the associated score (a). We select the answer accordingly argmaxa \u2032 = a (a).AQA Max Conf. We implement a second heuristic aggregator that selects the answer with the highest BiDAF score over formulations."}, {"heading": "4.7 Results", "text": "Table 1 shows the results. We report on exact matches and formula 1 ratios that are calculated at symbol level between the predicted answer and the golden answer. We present results for full validation and test sets (referred to as n-gram in [Dunn et al., 2017]), including questions that have both Unigram and longer answers. Answers. the performance of BiDAF appears to be more difficult than other current QA tasks such as SQuAD [Rajpurkar et al., 2016] and CNN / Daily Mail [Hermann et al., 2015], both for machines and for mankind. The performance of BiDAF decreases by 40 F1 points compared to SQuAD and CNN / Daily Mail. However, BiDAF is still competitive with respect to SeachQA and improves compared to the base network Attention Sum 13.7 F1 points. Using the top hypothesis alone already leads to an improvement in performance of 2.2 F1 in the test. This improvement, without a hay aggregation, shows that the questions can be answered more easily today, if the intuition of the Q."}, {"heading": "5 Related work", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6 Conclusion", "text": "We have investigated a first system of its kind, which consists of three components: a question reformulator, a black box QA system and a candidate response aggregator. The reformulator and aggregator form a traceable agent that tries to elicit the best answers from the QA system. It is important that the agent can only query the environment with natural language questions. Experimental results show that the approach is highly effective. We are improving a sophisticated deep QA system by 11% absolute F1, 32% relative F1 on a difficult dataset of long, semantically complex questions."}, {"heading": "6.1 Future Work", "text": "A direct extension consists of connecting multiple different environments, such as additional documents or a knowledge base; the reformulator can choose which questions to send to which environments, and the aggregator must now collect evidence for different modalities; this setting naturally handles multi-task scenarios where the same AQA agent is trained to interact with multiple backends to solve multiple QA tasks; as a longer-term extension, we will explore the sequential, iterative aspect of QA and include the problem as an end-to-end RL task, closing the loop between the reformer and the aggregator; Figure 2 represents the generalized AQA environment framework; the Agent (AQA) interacts with the environment (E) to answer a question (q0); and the environment includes a Q & A that sends out formative system, observations, and rewards."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["D. Bahdanau", "P. Brakel", "K. Xu", "A. Goyal", "R. Lowe", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["C. Bannard", "C. Callison-Burch"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Bannard and Callison.Burch.,? \\Q2005\\E", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Massive exploration of neural machine translation architectures", "author": ["D. Britz", "A. Goldie", "M.-T. Luong", "Q. Le"], "venue": null, "citeRegEx": "Britz et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "Aspects of the Theory of Syntax", "author": ["N. Chomsky"], "venue": null, "citeRegEx": "Chomsky.,? \\Q1965\\E", "shortCiteRegEx": "Chomsky.", "year": 1965}, {"title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "author": ["M. Dunn", "L. Sagun", "M. Higgins", "U. Guney", "V. Cirik", "K. Cho"], "venue": null, "citeRegEx": "Dunn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dunn et al\\.", "year": 2017}, {"title": "Paraphrase-Driven Learning for Open Question Answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation", "author": ["J. Ganitkevitch", "C. Callison-Burch", "C. Napoles", "B. Van Durme"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2011}, {"title": "Ppdb: The paraphrase database", "author": ["J. Ganitkevitch", "B. Van Durme", "C. Callison-Burch"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Comutational Linguistics: Human Language Technologies,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["E. Greensmith", "P.L. Bartlett", "J. Baxter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Ko\u010disk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Joshua: An open source toolkit for parsing-based machine translation", "author": ["Z. Li", "C. Callison-Burch", "C. Dyer", "J. Ganitkevitch", "S. Khudanpur", "L. Schwartz", "W.N. Thornton", "J. Weese", "O.F. Zaidan"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation,", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["C. Liang", "J. Berant", "Q. Le", "K.D. Forbus", "N. Lao"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Generating phrasal and sentential paraphrases: A survey of data-driven methods", "author": ["N. Madnani", "B.J. Dorr"], "venue": "Computational Linguistics,", "citeRegEx": "Madnani and Dorr.,? \\Q2010\\E", "shortCiteRegEx": "Madnani and Dorr.", "year": 2010}, {"title": "Exploratory search: From finding to understanding", "author": ["G. Marchionini"], "venue": "Commun. ACM,", "citeRegEx": "Marchionini.,? \\Q2006\\E", "shortCiteRegEx": "Marchionini.", "year": 2006}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": null, "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "End-to-end goal-driven web navigation", "author": ["R. Nogueira", "K. Cho"], "venue": "In Proceedings of the 30th International Conference on Neural Information Processing Systems,", "citeRegEx": "Nogueira and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Nogueira and Cho.", "year": 2016}, {"title": "Task-oriented query reformulation with reinforcement learning", "author": ["R. Nogueira", "K. Cho"], "venue": null, "citeRegEx": "Nogueira and Cho.,? \\Q2017\\E", "shortCiteRegEx": "Nogueira and Cho.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Statistical machine translation for query expansion in answer retrieval", "author": ["S. Riezler", "A. Vasserman", "I. Tsochantaridis", "V. Mittal", "Y. Liu"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Riezler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2007}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "ICML, Williamstown,", "citeRegEx": "Roy and McCallum.,? \\Q2001\\E", "shortCiteRegEx": "Roy and McCallum.", "year": 2001}, {"title": "Bidirectional Attention Flow for Machine Comprehension", "author": ["M. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Query-reduction networks for question answering", "author": ["M. Seo", "S. Min", "A. Farhadi", "H. Hajishirzi"], "venue": "In Proceedings of ICLR 2017,", "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "In Proceedings of the 12th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Mach. Learn.,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["R.J. Williams", "J. Peng"], "venue": "Connection Science,", "citeRegEx": "Williams and Peng.,? \\Q1991\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Computational fact checking through query perturbations", "author": ["Y. Wu", "P.K. Agarwal", "C. Li", "J. Yang", "C. Yu"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "Wu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2017}, {"title": "Dual learning for machine translation", "author": ["Y. Xia", "D. He", "T. Qin", "L. Wang", "N. Yu", "T.-Y. Liu", "W.-Y. Ma"], "venue": "In Proceedings of the 30th International Conference on Neural Information Processing Systems,", "citeRegEx": "Xia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2016}, {"title": "The united nations parallel corpus v1.0", "author": ["M. Ziemski", "M. Junczys-Dowmunt", "B. Poliquen"], "venue": "In Proceedings of Language Resources and Evaluation (LREC),", "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Such search sessions may require multiple iterations, critical assessment and synthesis [Marchionini, 2006].", "startOffset": 88, "endOffset": 107}, {"referenceID": 4, "context": "The productivity of natural language yields a myriad of ways to formulate a question [Chomsky, 1965].", "startOffset": 85, "endOffset": 100}, {"referenceID": 26, "context": "Our method resembles active learning [Settles, 2010].", "startOffset": 37, "endOffset": 52}, {"referenceID": 5, "context": "We evaluate on a dataset of complex questions taken from Jeopardy!, the SearchQA dataset [Dunn et al., 2017].", "startOffset": 89, "endOffset": 108}, {"referenceID": 3, "context": "1We build upon the public implementation2 of Britz et al. [2017]. 2https://github.", "startOffset": 45, "endOffset": 65}, {"referenceID": 29, "context": "We optimize the reward directly with respect to parameters of the policy using the Policy Gradient algorithm [Sutton and Barto, 1998].", "startOffset": 109, "endOffset": 133}, {"referenceID": 32, "context": "To compute gradients for training we use REINFORCE [Williams and Peng, 1991], \u2207Eq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))] = Eq\u223c\u03c0\u03b8( \u00b7 |q0)\u2207\u03b8 log(\u03c0\u03b8(q|q0))R(f(q)) (3) \u2248 1 N N \u2211", "startOffset": 51, "endOffset": 76}, {"referenceID": 9, "context": "This estimator is often found to have high variance, leading to unstable training [Greensmith et al., 2004].", "startOffset": 82, "endOffset": 107}, {"referenceID": 31, "context": "We reudce the variance by adding a baseline: B(q0) = Eq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))] [Williams, 1992].", "startOffset": 76, "endOffset": 92}, {"referenceID": 5, "context": "We experiment on a new and challenging question answering dataset, SearchQA [Dunn et al., 2017].", "startOffset": 76, "endOffset": 95}, {"referenceID": 35, "context": "0 [Ziemski et al., 2016].", "startOffset": 2, "endOffset": 24}, {"referenceID": 3, "context": "5 Following [Britz et al., 2017] we use a bidirectional LSTM as encoder and an 4-layer LSTM with attention [Bahdanau et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 1, "context": ", 2017] we use a bidirectional LSTM as encoder and an 4-layer LSTM with attention [Bahdanau et al., 2016] as decoder.", "startOffset": 82, "endOffset": 105}, {"referenceID": 6, "context": "The monolingual data is extracted from the Paralex database of question paraphrases [Fader et al., 2013].", "startOffset": 84, "endOffset": 104}, {"referenceID": 19, "context": "We use pre-trained 100-dimensional embeddings [Pennington et al., 2014] for the tokens.", "startOffset": 46, "endOffset": 71}, {"referenceID": 5, "context": "5 Baselines and Benchmarks As a baseline, we repeat the results reported for a deep learning system developed for SearchQA Dunn et al. [2017]. This is a modified pointer network, called Attention Sum Reader.", "startOffset": 123, "endOffset": 142}, {"referenceID": 5, "context": "5 Baselines and Benchmarks As a baseline, we repeat the results reported for a deep learning system developed for SearchQA Dunn et al. [2017]. This is a modified pointer network, called Attention Sum Reader.7 7 Dunn et al. [2017] also provide a simpler baseline that ranks unigrams from the search snippets by their TF-IDF score.", "startOffset": 123, "endOffset": 230}, {"referenceID": 5, "context": "The first is human performance reported in [Dunn et al., 2017] based on a sample of the test set.", "startOffset": 43, "endOffset": 62}, {"referenceID": 5, "context": "We present results on the full validation and test sets (referred to as n-gram in [Dunn et al., 2017]).", "startOffset": 82, "endOffset": 101}, {"referenceID": 20, "context": "SearchQA appears to be harder than other recent QA tasks such as SQuAD [Rajpurkar et al., 2016] and CNN/Daily Mail [Hermann et al.", "startOffset": 71, "endOffset": 95}, {"referenceID": 10, "context": ", 2016] and CNN/Daily Mail [Hermann et al., 2015], for both machines and humans.", "startOffset": 27, "endOffset": 49}, {"referenceID": 14, "context": "Bilingual corpora and machine translation have been used to generate paraphrases by pivoting through a second language[Madnani and Dorr, 2010].", "startOffset": 118, "endOffset": 142}, {"referenceID": 2, "context": "Early work extracted word or phrase pairs (e, e\u2032) from a phrase table such that e translates into a foreign phrase f and f translates back to e\u2032 [Bannard and Callison-Burch, 2005].", "startOffset": 145, "endOffset": 179}, {"referenceID": 34, "context": "Sequence level reward functions based on language models and reconstruction errors are used to bootstrap MT with fewer resources [Xia et al., 2016].", "startOffset": 129, "endOffset": 147}, {"referenceID": 26, "context": "Our approach also resembles active learning [Settles, 2010] because our agent optimizes the input to an environment from which it collects data to receive the most useful responses.", "startOffset": 44, "endOffset": 59}, {"referenceID": 13, "context": "Riezler et al. [2007] propose a phrase-based paraphrasing system for queries which they use to extract synonyms of terms in a given query.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "For example, Narasimhan et al. [2015] use RL to learn control policies for multi-user dungeon games where the state of the game is summarized by a textual description, and Li et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 9, "context": "[2015] use RL to learn control policies for multi-user dungeon games where the state of the game is summarized by a textual description, and Li et al. [2017] use RL for dialogue generation.", "startOffset": 141, "endOffset": 158}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al.", "startOffset": 0, "endOffset": 485}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al. [2017b] who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning.", "startOffset": 0, "endOffset": 564}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al. [2017b] who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning. The work of Nogueira and Cho [2016] is most related to ours.", "startOffset": 0, "endOffset": 727}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al. [2017b] who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning. The work of Nogueira and Cho [2016] is most related to ours. Their goal is to identify a document containing an answer to a question by following links on a document graph. Evaluating on a set of questions from the game \u201cJeopardy!\u201d, they learn to walk the Wikipedia graph using an RNN until they reach the predicted article/answer. In a recent follow-up Nogueira and Cho [2017] improve document retrieval with an approach inspired by relevance feedback in combination with RL.", "startOffset": 0, "endOffset": 1069}, {"referenceID": 33, "context": "Finally, Active QA is related to recent research on fact-checking: Wu et al. [2017] propose to perturb database queries in order to estimate the support of quantitative claims.", "startOffset": 67, "endOffset": 84}, {"referenceID": 23, "context": "8Bayes optimal active learning also tries to optimize end-to-end performance, however, these techniques are often computationally prohibitive in large-scale settings [Roy and McCallum, 2001].", "startOffset": 166, "endOffset": 190}], "year": 2017, "abstractText": "We frame Question Answering as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box question-answering system an which learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. Our agent improves F1 by 11% over a state-of-the-art base model that uses the original question/answer pairs.", "creator": "LaTeX with hyperref package"}}}