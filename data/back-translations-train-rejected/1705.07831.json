{"id": "1705.07831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Stabilizing GAN Training with Multiple Random Projections", "abstract": "Training generative adversarial networks is unstable in high-dimensions when the true data distribution lies on a lower-dimensional manifold. The discriminator is then easily able to separate nearly all generated samples leaving the generator without meaningful gradients. We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. We show that individual discriminators then provide stable gradients to the generator, and that the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.", "histories": [["v1", "Mon, 22 May 2017 16:23:26 GMT  (5559kb,D)", "http://arxiv.org/abs/1705.07831v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["behnam neyshabur", "srinadh bhojanapalli", "ayan chakrabarti"], "accepted": false, "id": "1705.07831"}, "pdf": {"name": "1705.07831.pdf", "metadata": {"source": "CRF", "title": "Stabilizing GAN Training with Multiple Random Projections", "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "Ayan Chakrabarti"], "emails": ["bneyshabur@ttic.edu", "srinadh@ttic.edu", "ayanc@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "Indeed, it is the case that most people who are in a position to surpass themselves, to surpass themselves, to surpass themselves, by surpassing themselves, by surpassing themselves, by surpassing themselves, by surpassing themselves, by surpassing themselves, by surpassing themselves, by surpassing, by surpassing themselves, by surpassing themselves, by surpassing themselves, by surpassing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing themselves, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overcoming, overtaxing, overtaxing, overcoming, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, overtaxing, over"}, {"heading": "1.1 Related Work", "text": "Instead of optimizing the divergence between Jensen and Shannon, as proposed in the original GAN framework, Energy based GAN [12] and Wasserstein GAN [11] show an improvement in stability by optimizing the total variation distance or earth-shift distance, respectively, along with adjusting the discriminator to limit discrimination capacity. [10] Further extended GAN training to target any choice of f-divergence. [8] Several heuristics have been proposed to improve the stability of the training, including modification of the objective function, virtual batch normalization, historical averaging of parameters, semi-monitored learning, etc. All of these methods are designed to improve the quality of the gradients and provide additional monitoring to the generator, so they are complementary and can probably be used in combination with our approach."}, {"heading": "2 Learning with Multiple Discriminators on Random Projections", "text": "GANs [1] traditionally include a generator G that learns to generate samples from a data distribution Px by turning against a single discriminator D), (1) for x-th Rd, Px = 1, and with Pz a fixed distribution (typically uniform or Gaussian) for the sound vectors z \u00b2, d \u00b2 n for x-th Rd, Px: SGD = 1, and with Pz a fixed distribution. While this approach works amazingly well, instability is common during training, especially when generating high-dimensional data."}, {"heading": "2.1 Stability", "text": "If the support of the true distribution Px in relation to the range of possible values of x occupies a small volume, it is difficult for the generator to generate samples within this support, which makes it easy for a discriminator to quickly learn to classify all such samples as fake. At this point, we argue that this problem is alleviated by training discriminators on low-dimensional projections of x and G (z). For simplicity's sake, we assume that the range of x is centered on the d-dimensional ball Bd of radius 1 with 0. We call the support supp (Px) and Bd of a distribution Px as the set where the density is greater than a small threshold. Let's assume W-Rd \u00d7 m to be an initial random Gaussian projection matrix. We call BdW the projection of the range to W, and PWT x the limit of Px along W. Theorem 2.1. Let's say that Px is a projection of Px."}, {"heading": "2.2 Consistency", "text": "The following two results, shown in the supplement, show that with discriminators acting on a sufficiently large number of K random projections, generator G will learn to produce samples under a distribution corresponding to the true data distribution Px. Theorem 2.2. Let Pg denote the distribution of generator outputs G (z), where z-Pz, and let PWTk g be the margins of Pg along Wk. For fixed G, the optimal {Dk} of Dk (y) = PWTk x (y) + PWTk g (y), (3) is given for all k-Pg {1, \u00b7 \u00b7 \u00b7, K}. The optimal G minimized (2) is iff PWTk x = PWTk g g, k. The above result implies that the margins of the distribution of the optimal generator x x agree with the distribution of Wx."}, {"heading": "2.3 Projections for Generating Image Data", "text": "While individual discriminators see projected inputs in our framework that are lower dimensional than the overall picture, their dimensionality is still large enough (a very small m would require a large number of discriminators) to make it difficult to train discriminators with only fully connected layers. Therefore, it is desirable to use for each discriminator revolutionary architectures (as shown in Fig. 1), by selecting the projection matrices WTk to generate \"image-like\" data. Accordingly, we use downsampled convolutions with random filters to embody the projections WTk (as shown in Fig. 1). Elements of these convolution filters are generally extracted from a Gaussian distribution, and the filters are then scaled to have a Unit '2 standard."}, {"heading": "3 Experimental Results", "text": "In this section, we evaluate our approach with experiments compared to generators that have gone against a single real discriminator who considers the entire image as input, and show that it leads to greater stability during training, and that the latter ultimately leads to higher discrimination that results in higher quality samplers. In fact, we primarily use the datasets of the faces we collect, which we use empirically to achieve better results (both the default discrimination settings and our multiple discrimination settings). First, we use different batches of generated images to update the DC-GAN implementation, as well as to achieve the results (both the default discrimination settings and our multiple settings)."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed a new framework for training GANs with high-dimensional results. Our approach employs multiple discriminators on random projections of the data to stabilize the training, with enough projections to ensure that the generator learns the true data distribution. Experimental results show that the generators trained with our method are actually more stable - they continue to learn during the training, while those trained with a single discriminator often diverge and produce higher-quality samples that correspond to the true data distribution. Source code and trained models for our implementation are available on the project page http: / / www.ttic.edu / chakrabarti / rpgan /. In our current framework, the number of discriminators is limited by computational costs. In future work, we plan to study the training with a much larger group of discriminators, using only a small subset of them in each iteration, or each group of iterations. We are also interested in using multiple discriminators with modified modes [e.g., 12 and 18-type targets] in common."}, {"heading": "A Proofs", "text": "The proof of the theory 2.1. We first show that we can assume that the columns of the projection W = = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = px = p"}, {"heading": "B Additional Experimental Results", "text": "Face Pictures: Proposed Method (K = 48) Face Pictures: Proposed Method (K = 24) Face Pictures: Proposed Method (K = 12) Face Pictures: Traditional DC-GAN (Iter. 40k) Face Pictures: Traditional DC-GAN (Iter. 100k) Random Pictures: Proposed Method"}], "references": [{"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Semi-supervised learning with context-conditional generative adversarial networks", "author": ["Emily Denton", "Sam Gross", "Rob Fergus"], "venue": "[cs.CV],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Husz\u00e1r", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "venue": "[cs.CV],", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A Efros"], "venue": "[cs.CV],", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumit Chintala"], "venue": "In ICLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Towards principled methods for training generative adversarial networks", "author": ["Martin Arjovsky", "L\u00e9on Bottou"], "venue": "In NIPS Workshop on Adversarial Training,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Wasserstein gan", "author": ["Martin Arjovsky", "Soumith Chintala", "L\u00e9on Bottou"], "venue": "[stat.ML],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "[cs.LG],", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Ensembles of generative adversarial networks", "author": ["Yaxing Wang", "Lichao Zhang", "Joost van de Weijer"], "venue": "[cs.CV],", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Generative multi-adversarial networks", "author": ["Ishan Durugkar", "Ian Gemp", "Sridhar Mahadevan"], "venue": "[cs.LG],", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Experiments with a new boosting algorithm", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Methods for combining experts\u2019 probability assessments", "author": ["Robert A Jacobs"], "venue": "Neural computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Sliced and radon wasserstein barycenters of measures", "author": ["Nicolas Bonneel", "Julien Rabin", "Gabriel Peyr\u00e9", "Hanspeter Pfister"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Foundations of Computer Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In ICCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Generative adversarial networks (GANs), introduced by [1], endow neural networks with the ability to express distributional outputs.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting [2], and super-resolution [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting [2], and super-resolution [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Recently, [4] demonstrated that GANs can be used to produce plausible mappings between a variety of domains\u2014including sketches and photographs, maps and aerial views, segmentation masks and images, etc.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks [2, 5, 6].", "startOffset": 189, "endOffset": 198}, {"referenceID": 4, "context": "GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks [2, 5, 6].", "startOffset": 189, "endOffset": 198}, {"referenceID": 5, "context": "GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks [2, 5, 6].", "startOffset": 189, "endOffset": 198}, {"referenceID": 6, "context": "[7] proposed explicitly factorizing generating an image into a sequence of conditional generations of levels of a Laplacian", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "pyramid, while [6] demonstrated that specific architecture choices and parameter settings led to higher-quality samples.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 190, "endOffset": 197}, {"referenceID": 10, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 190, "endOffset": 197}, {"referenceID": 11, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 190, "endOffset": 197}, {"referenceID": 8, "context": "In this work, we seek to provide a framework that addresses a key source of instability [9] when training GANs\u2014when the true distribution of high-dimensional data is concentrated in a small fraction of the ambient space.", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN [12] and Wasserstein GAN [11] show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity.", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN [12] and Wasserstein GAN [11] show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity.", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "[10] further extended GAN training to any choice of f -divergence as objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] proposed several heuristics to improve the stability of training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Note that prior methods have involved training ensembles of GANs [13], or ensembles of discriminators [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Note that prior methods have involved training ensembles of GANs [13], or ensembles of discriminators [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": ", with boosting [15], or mixtureof-experts [16], is a common approach to learning.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": ", with boosting [15], or mixtureof-experts [16], is a common approach to learning.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "[17], who also use low-dimensional projections to deal with high-dimensional probability distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "GANs [1] traditionally comprise of a generator G that learns to generate samples from a data distribution Px, through adversarial training against a single discriminator D as:", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "As described in [9], this instability is due to the fact that the distributions Px corresponding to natural data often have very limited support in the ambient domain of x and G(z).", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "Assume Px = \u2211 j \u03c4jN (x|\u03bcj ,\u03a3j) is a mixture of Gaussians, such that the individual components are sufficiently well separated (in the sense that there is no overlap between their supports or the projections thereof, see [18]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 5, "context": "Prior work [6] has shown that using convolutional architectures for both the generator and discriminator networks is key to generating images with GANs.", "startOffset": 11, "endOffset": 14}, {"referenceID": 18, "context": "For evaluation, we primarily use the dataset of celebrity faces collected by [19]\u2014we use the cropped and aligned 64\u00d7 64-size version of the images\u2014and the DC-GAN [6] architectures for the generator and discriminator.", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "For evaluation, we primarily use the dataset of celebrity faces collected by [19]\u2014we use the cropped and aligned 64\u00d7 64-size version of the images\u2014and the DC-GAN [6] architectures for the generator and discriminator.", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "As suggested in [6], we use Adam [20] with learning rate 2 \u00d7 10\u22124, \u03b21 = 0.", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "As suggested in [6], we use Adam [20] with learning rate 2 \u00d7 10\u22124, \u03b21 = 0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "Finally, we also use our framework to train a generator on a subset of the Imagenet-1K database [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": ", [10\u201312]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", [10\u201312]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": ", [10\u201312]).", "startOffset": 2, "endOffset": 9}], "year": 2017, "abstractText": "Training generative adversarial networks is unstable in high-dimensions when the true data distribution lies on a lower-dimensional manifold. The discriminator is then easily able to separate nearly all generated samples leaving the generator without meaningful gradients. We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. We show that individual discriminators then provide stable gradients to the generator, and that the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.", "creator": "LaTeX with hyperref package"}}}