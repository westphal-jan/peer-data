{"id": "1705.07867", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "SmartPaste: Learning to Adapt Source Code", "abstract": "Deep Neural Networks have been shown to succeed at a range of natural language tasks such as machine translation and text summarization. While tasks on source code (ie, formal languages) have been considered recently, most work in this area does not attempt to capitalize on the unique opportunities offered by its known syntax and structure. In this work, we introduce SmartPaste, a first task that requires to use such information. The task is a variant of the program repair problem that requires to adapt a given (pasted) snippet of code to surrounding, existing source code. As first solutions, we design a set of deep neural models that learn to represent the context of each variable location and variable usage in a data flow-sensitive way. Our evaluation suggests that our models can learn to solve the SmartPaste task in many cases, achieving 58.6% accuracy, while learning meaningful representation of variable usages.", "histories": [["v1", "Mon, 22 May 2017 17:16:06 GMT  (34kb)", "http://arxiv.org/abs/1705.07867v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SE", "authors": ["miltiadis allamanis", "marc brockschmidt"], "accepted": false, "id": "1705.07867"}, "pdf": {"name": "1705.07867.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["t-mialla@microsoft.com", "mabrocks@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.07 867v 1 [cs.L G] 22 May 2Deep Neural Networks have proved successful in a number of natural language tasks such as machine translation and text summary. Although source code (i.e. formal languages) tasks have been considered lately, most work in this area does not attempt to take advantage of the unique possibilities offered by the known syntax and structure of the source code. In this work, we present SMARTPASTE, a first task that requires the use of such information, which is a variant of the repair problem of the program that requires adapting a given (inserted) snippet of code to the surrounding existing source code. As an initial solution, we design a set of deep neural models that learn to present the context of each variable position and variable use in a data flow-sensitive manner. Our evaluation suggests that our models can learn to solve the SMARTPASTE task in many cases, while learning to a full representation of 8.6% accuracy in many cases."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2 The SMARTPASTE Task", "text": "We consider a task beyond standard source code completion, where we insert a snippet of code into an existing program and adapt variable identifiers in the snippet to the target program (Figure 1). This is a common scenario in software development [4], when developers copy a piece of code from a website (e.g. StackOverflow) or from an existing project into a new context. Furthermore, this task differs in a number of important aspects. First, only variable identifiers need to be completed, while many code completion systems focus on a broader task (e.g. predicting each next code character). Second, multiple identifiers need to be completed simultaneously, and therefore all decisions need to be made synchronously, reflecting interdependencies."}, {"heading": "3 Models", "text": "Subsequently, we will discuss a sequence of models designed for the SMARTPASTE task, in which we know more and more familiar semantics of the underlying programming language. (All models share the definition of a context that refers in a certain way to the way it is used to correctly assign the inner product of c (t, v) to variable mapping v to t.Notation We use Vt-V to refer to the set of all variables to the extent of t. (i.e.) These variables can be used legally in t. (t) In addition, we use Up (t, v) T-V to mark the last occurrence of variables in T (resp.) Un (t, v) for the next occurrence of variables in t."}, {"heading": "3.1 Learning to Paste", "text": "With the context representation c (t) of the placeholder t (t) many different mappings that we select during training. (t, v) We can now formulate the probability of a single placeholder t (t) in such a way that we find an mapping between the two vectors: p (T). (t) We have maximized the probability of the acquired code by replacing all placeholders t (t). (1) As in all structured predictive models, applying the model directly to Equation 1 is computationally intractable, because the normalization constant requires an exponential calculation."}, {"heading": "4 Evaluation", "text": "Dataset We collected a dataset for the SMARTPASTE task from open source C # projects on GitHub. To select projects, we selected the most high-profile (non-forked) projects in GitHub. We then filtered out projects that we could not (easily) fully compile with Roslyn3 because we needed a compilation to extract precise type information for the code (including those available in external libraries).Our final dataset contains 27 projects from a diverse set of domains (compilers, databases,...) with about 4.8 million non-empty lines of code. A complete table is appendix D. We then created SMARTPASTE examples by selecting snippets of up to 80 syntax tokens (in practice, this means that snippets are about 10 statements long) from the source files of a project that are either a single AST string (e.g. a continuous node or a block)."}, {"heading": "4.1 Quantitative Evaluation", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.2 Qualitative Evaluation", "text": "We show an example of the SMART PASTE task in Figure 3, where we can observe that the model learns both to distinguish between variables of different types (elements of the IHTML element are not confused with string variables) and to assign finer-grained semantics (url and path are treated separately), as implied by the results for our similar scenario. In Figure 4, we show placeholders that have highly similar context representations u (t, v). Qualitatively, Figure 4 and the visualizations in Appendix B suggest that the learned representations can be used as learned similarity metrics for the semantics of variable use. These representations learn protocols and conventions such as \"after accessing X, we should access Y\" or the need to conditionally verify a property, as in Figure 4.We observed a number of common problems. Above all, variables that are explained but not explicitly initialized (e.g., root information is available as the cause)."}, {"heading": "5 Related Work", "text": "Our work builds on the recent field of using machine learning for source code artifacts. However, recent research has led to language models of the code that attempt to model the entire code. Bhoopchand et al. [6], Hindle et al. [10] model the code as a sequence of tokens, while Maddison and Tarlow [11], Raychev et al. [17] model the syntax tree structure of the code. All work on language models of the code finds that predicting variable and method structures is one of the biggest challenges in the task. However, we are not aware of any models that attempt to use data flow information for different purposes. Note that the local context is not used when (?.HasValue) computingu (t, v) but data flow information of the other uses is used (highlighted in yellow). In this example, the model learns to use a common representation of labels and then conditional for the variables."}, {"heading": "6 Discussion & Conclusions", "text": "Although source code is well understood and explored in other disciplines such as programming language research, it is a relatively new domain for deep learning. It offers new possibilities compared to text or perceptual data because its (local) semantics are clearly defined and additional information can be extracted using known, efficient program analysis. On the other hand, integrating this wealth of structured information presents an interesting challenge. Our SMARTPASTE task opens up these possibilities and goes beyond simple tasks such as completing the code. We consider it to be the first representative of the core challenge of learning the meaning of the source code, as it probably needs to refine standard information contained in type systems. We see a wealth of opportunities in the field of research. To improve our performance in the SMARTPASTE task, we would like to expand our models to include additional identification names that are obviously rich in information."}, {"heading": "Acknowledgments", "text": "We would like to thank Alex Gaunt for his valuable comments and suggestions."}, {"heading": "A Per Placeholder Suggestion Samples", "text": "Do you see how much time we need to find a solution? \"he said in an interview with the German Press Agency."}, {"heading": "B Nearest Neighbor of Usage Representations", "text": "Here we show pairs based on the learned similarity (t, v). Each placeholder is marked as? and all uses of V."}, {"heading": "C Full Snippet Pasting Samples", "text": "The variables displayed with each placeholder correspond to the basic truth. Underlined symbols represent UNK symbols. The three most important assignments are displayed in the superscript next to the basic truth (if not included in the top 3 suggestions). Example 1... CharsLeft1 = 0; while (p\u03bb2.IsRightOf (selection\u03bb3.Start)) {charsLeft5% symbols that need to be filled in during insertion. All other placeholders are marked in the superscript next to the relevant variables (_ MOVEUNIT _ ACTION.MOVEUNIT _ PREVCHAR);}..."}, {"heading": "D Dataset", "text": "The collected data set and its properties are listed in Table 2."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Deep Neural Networks have been shown to succeed at a range of natural lan-<lb>guage tasks such as machine translation and text summarization. While tasks on<lb>source code (i.e., formal languages) have been considered recently, most work in<lb>this area does not attempt to capitalize on the unique opportunities offered by its<lb>known syntax and structure. In this work, we introduce SMARTPASTE, a first task<lb>that requires to use such information. The task is a variant of the program repair<lb>problem that requires to adapt a given (pasted) snippet of code to surrounding,<lb>existing source code. As first solutions, we design a set of deep neural models<lb>that learn to represent the context of each variable location and variable usage<lb>in a data flow-sensitive way. Our evaluation suggests that our models can learn<lb>to solve the SMARTPASTE task in many cases, achieving 58.6% accuracy, while<lb>learning meaningful representation of variable usages.", "creator": "LaTeX with hyperref package"}}}