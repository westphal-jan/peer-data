{"id": "1705.07962", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "abstract": "Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites and mobile applications. In this paper, we show that Deep Learning techniques can be leveraged to automatically generate code given a graphical user interface screenshot as input. Our model is able to generate code targeting three different platforms (i.e. iOS, Android and web-based technologies) from a single input image with over 77% of accuracy.", "histories": [["v1", "Mon, 22 May 2017 19:32:20 GMT  (1094kb,D)", "http://arxiv.org/abs/1705.07962v1", "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA"], ["v2", "Tue, 19 Sep 2017 11:27:47 GMT  (1079kb,D)", "http://arxiv.org/abs/1705.07962v2", null]], "COMMENTS": "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.CV cs.NE", "authors": ["tony beltramelli"], "accepted": false, "id": "1705.07962"}, "pdf": {"name": "1705.07962.pdf", "metadata": {"source": "CRF", "title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "authors": ["Tony Beltramelli"], "emails": ["tony@uizard.io"], "sections": [{"heading": "1 Introduction", "text": "However, implementing GUI code is time-consuming and prevents developers from devoting most of their time to implementing the actual features and logic of the software they are building. In addition, we describe a system that can automatically generate platform-specific computer code when a GUI screenshot is available for input. We anticipate that a scaled version of our method could potentially eliminate the need for manually programmed GUI code. Our first contribution is pix2code, a novel approach based on Convolutional and Recurrent Neural Networks, which allow computer code to be generated from a single GUI screenshot as input. Our model is capable of generating computer code from different image platforms to demonstrate the interactivity of our system."}, {"heading": "2 Related Work", "text": "A current example is DeepCoder [2], a system capable of generating computer programs by providing statistical 1https: / / uizard.io / research # pix2code 2https: / / github.com / tonybeltramelli / pix2codeSubmitted to 31. Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.ar Xiv: 170 5.07 962v 1 [cs.L G] 22 May 2predictions to augment traditional search techniques. In another paper by Gaunt et al. [4], source code generation is enabled by learning the relationships between input-output examples using differentiated interpreters. Neurtual Ling et al."}, {"heading": "3 pix2code", "text": "The task of generating code with a GUI screenshot as input can be compared to the task of generating English text descriptions with a scene photograph as input. We can therefore divide our problem into three sub-problems: First, a problem of computer vision to understand the given scene (i.e. in this case the GUI screenshot image) and derive from it the existing objects, their identities, positions and poses (i.e. buttons, labels, element containers); second, a language modeling problem in understanding text (i.e. in this case computer code) and generating syntactically and semantically correct examples. Finally, the final challenge is to use the solutions to both previous sub-problems by using the latent variables derived from understanding the scene to generate corresponding text descriptions (i.e. computer code instead of English text) of the variable objects represented by these variables."}, {"heading": "3.1 Vision Model", "text": "CNNs are currently the method of choice for solving a wide range of visual problems, thanks to their topology that allows them to learn rich latent representations from the images on which they are trained [14, 10]. We used a CNN to perform unattended feature learning by mapping an input image to a learned fixed-length vector; and thus act as an encoder as shown in Figure 1. The input images are initially reduced to 256 x 256 pixels (aspect ratio is not preserved) and the pixel values are normalized before they are fed into CNN. No further pre-processing takes place. To encode each input image to a fixed-size output vector, we used only small 3 x 3 receptive fields associated with step 1, as used by Simonyan and Zisserman for VGNet [15]."}, {"heading": "3.2 Language Model", "text": "In this work, we are only interested in the GUI layout, the various graphical components and their relationships; therefore, the actual textual value of the labels is ignored by our DSL. In addition to reducing the size of the search space, DSL simplicity also reduces the size of the vocabulary (i.e. the total number of tokens + supported by DSL).As a result, our language model can avoid token-level language modeling with discrete input from using one-hot-coded vectors; eliminating the need for word embedding techniques such as word2vec [12], which can lead to costly calculations. In most programming languages and markup languages, an element is declared with an opening token; if child elements or instructions are included within a block, a closing token is usually required for the interpreter or the compiler."}, {"heading": "3.3 Combining Models", "text": "As shown in Figure 1, a CNN-based vision model encodes the input image I into a vector representation p. The input token xt is encoded by an LSTM-based language model into an intermediate representation qt, which allows the model to focus more on certain tokens and less on others [7]. This first language model is implemented as a stack of two LSTM layers of 128 cells each. The vision-encoded vector p and the language-encoded vector qt are linked to a single vector rt, which is then fed into a second LSTM-based model."}, {"heading": "3.4 Training", "text": "The length T of the sequences used for training is important to model long-term dependencies; for example, to close a block of code that has been opened. According to empirical experiments, the DSL code input file used for training was segmented with a 48-size sliding window; in other words, we unroll the recursive neural network for 48 steps. This has proven to be a satisfactory trade-off between long-term dependencies, learning and computational costs. Therefore, for each token in the input DSL file, the model is fed both an input image and a sequence of T = 48 tokens. While the sequence of tokens used for training is updated at each step of the time (i.e. each token) by moving the window, the exact same input image I is reused for samples associated with the same GUI. The special tokens < START and < END are used to comply with DSL and END."}, {"heading": "3.5 Sampling", "text": "To generate DSL code, we feed the GUI image I and a sequence X of T = 48 characters, where the characters xt... xT \u2212 1 are first blank and the last character of the sequence xT is set to the special < START > character. The predicted character yt is then used to update the next sequence of input marks, i.e., xt... xT \u2212 1 are set to xt + 1.. xT (the text is thus discarded), where xT is set to yt. The process repeats until the character < END > is generated by the model, and the generated DSL code can then be compiled into the desired target language using conventional compilation methods."}, {"heading": "4 Experiments", "text": "Access to consistent datasets is a typical bottleneck in deep neural network training. To our knowledge, no dataset consisting of both GUI screenshots and source code was available at the time this work was written. As a result, we synthesized our own data, leading to the three open datasets described in Table 1. Our data synthesis algorithm is designed to synthesize GUIs written in our DSL, which are then rendered in the desired target language. Data synthesis also enables us to demonstrate our model's ability to generate computer code for three different platforms. Our model has approximately 109 x 106 optimization parameters, and all experiments are performed with the same model without specific coordination; only the training datasets differ, as shown in Figure 3."}, {"heading": "5 Conclusion and Discussions", "text": "In this paper, we have a novel method of generating a single GUI screenshot. While our work has the potential to automate GUI programming, we are only scratching the surface of what is possible. Our model consists of relatively few parameters and was trained on a relatively small dataset, and the quality of the code generated could be drastically improved by a larger model over an extended number of epochs."}], "references": [{"title": "COURSERA: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Training with the RMSProp algorithm [17] gave the best results with a learning rate set to 1e \u2212 4 and by clipping the output gradient to the range [\u22121.", "startOffset": 36, "endOffset": 40}], "year": 2017, "abstractText": "Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites and mobile applications. In this paper, we show that Deep Learning techniques can be leveraged to automatically generate code given a graphical user interface screenshot as input. Our model is able to generate code targeting three different platforms (i.e. iOS, Android and web-based technologies) from a single input image with over 77% of accuracy.", "creator": "LaTeX with hyperref package"}}}