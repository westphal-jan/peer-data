{"id": "1705.08011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Convergence Analysis of Batch Normalization for Deep Neural Nets", "abstract": "Batch normalization (BN) is very effective in accelerating the convergence of a neural network training phase that it has become a common practice. We propose a generalization of BN, the diminishing batch normalization (DBN) algorithm. We provide an analysis of the convergence of the DBN algorithm that converges to a stationary point with respect to trainable parameters. We analyze a two layer model with linear activation. The main challenge of the analysis is the fact that some parameters are updated by gradient while others are not. In the numerical experiments, we use models with more layers and ReLU activation. We observe that DBN outperforms the original BN algorithm on MNIST, NI and CIFAR-10 datasets with reasonable complex FNN and CNN models.", "histories": [["v1", "Mon, 22 May 2017 21:31:10 GMT  (1147kb,D)", "http://arxiv.org/abs/1705.08011v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yintai ma", "diego klabjan"], "accepted": false, "id": "1705.08011"}, "pdf": {"name": "1705.08011.pdf", "metadata": {"source": "CRF", "title": "Convergence Analysis of Batch Normalization for Deep Neural Nets", "authors": ["Yintai Ma", "Diego Klabjan"], "emails": ["yintaima2020@u.northwestern.edu,", "d-klabjan@northwestern.edu"], "sections": [{"heading": null, "text": "This year it is more than ever before in the history of the city."}], "references": [{"title": "Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks", "author": ["Devansh Arpit", "Yingbo Zhou", "Bhargava U. Kota", "Venu Govindaraju"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A Survey. Optimization for Machine", "author": ["Dimitri P. Bertsekas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Gradient Convergence in Gradient Methods with Errors", "author": ["Dimitri P. Bertsekas", "John N. Tsitsiklis"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Optimization Methods for Large-Scale Machine Learning", "author": ["L\u00e9on Bottou", "Frank E. Curtis", "Jorge Nocedal"], "venue": "arXiv preprint arXiv:1606.04838,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Recurrent Batch Normalization", "author": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "Aaron Courville"], "venue": "arXiv preprint arXiv:1603.09025,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Yoram Duchi", "John", "Hazan", "Elad", "Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "A Class of Unconstrained Minimization Methods for Neural Network Training", "author": ["L. Grippo"], "venue": "Optimization Methods and Software,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Knowledge Matters: Importance of Prior Information for Optimization", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky", "Geoffrey E. Hinton"], "venue": "PhD thesis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Imagenet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio"], "venue": "Proceedings of the IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Neural Networks: Tricks of the Trade", "author": ["Genevieve B. Orr", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Introduction to optimization. Translations series in mathematics and engineering", "author": ["B.T. Polyak"], "venue": "Optimization Software,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1987}, {"title": "Pseudogradient Adaption and Training Algorithms", "author": ["B.T. Polyak", "Y.Z. Tsypkin"], "venue": "Automation and Remote Control,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1973}], "referenceMentions": [{"referenceID": 9, "context": "Ioffe and Szegedy [12] identified an important problem involved in training deep networks, internal covariate shift, and then proposed batch normalization (BN) to decrease this phenomenon.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "It is now a standard practice [6, 11].", "startOffset": 30, "endOffset": 37}, {"referenceID": 8, "context": "It is now a standard practice [6, 11].", "startOffset": 30, "endOffset": 37}, {"referenceID": 3, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "In fact, Orr and M\u00fcller [16] show that preprocessing the data by subtracting the mean, normalizing the variance, and decorrelating the input has various beneficial effects for back-propagation.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "[14] propose a method called local response normalization which is inspired by computational neuroscience and acts as a form of lateral inhibition, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "G\u00fcl\u00e7ehre and Bengio [10] propose a standardization layer that bears significant resemblance to batch normalization, except that the two methods are motivated by very different goals and perform different tasks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Normalization propagation [2] uses data-independent estimations for the mean and standard deviation in every layer to reduce the internal covariate shift and make the estimation more accurate for the validation phase.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "[7] propose a new way to apply batch normalization to RNN and LSTM models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "For classical convergence results with a nonconvex objective function and diminishing learning rate, we refer to survey papers [4, 5, 6].", "startOffset": 127, "endOffset": 136}, {"referenceID": 2, "context": "For classical convergence results with a nonconvex objective function and diminishing learning rate, we refer to survey papers [4, 5, 6].", "startOffset": 127, "endOffset": 136}, {"referenceID": 3, "context": "For classical convergence results with a nonconvex objective function and diminishing learning rate, we refer to survey papers [4, 5, 6].", "startOffset": 127, "endOffset": 136}, {"referenceID": 2, "context": "Bertsekas and Tsitsiklis [5] provide a convergence result with deterministic gradient with errors.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "[6] provide a convergence result with stochastic gradient.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The classic analyses showing the norm of gradients of the objective function going to zero date back to [9, 18, 17].", "startOffset": 104, "endOffset": 115}, {"referenceID": 15, "context": "The classic analyses showing the norm of gradients of the objective function going to zero date back to [9, 18, 17].", "startOffset": 104, "endOffset": 115}, {"referenceID": 14, "context": "The classic analyses showing the norm of gradients of the objective function going to zero date back to [9, 18, 17].", "startOffset": 104, "endOffset": 115}, {"referenceID": 3, "context": "For strongly convex objective functions with a diminishing learning rate, we learn the classic convergence results from [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "This result is similar to the classical convergence rate analysis for the non-convex objective function with diminishing stepsizes, which can be found in [6].", "startOffset": 154, "endOffset": 157}, {"referenceID": 3, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "We use MNIST [15], CIFAR-10 [13] and Network Intrusion (NI) [1] datasets to compare the performance between DBN and the original BN algorithm.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "We use MNIST [15], CIFAR-10 [13] and Network Intrusion (NI) [1] datasets to compare the performance between DBN and the original BN algorithm.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "We use AdaGrad [8] to update the learning rates \u03b7 for trainable parameters, starting from 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "[2] Devansh Arpit, Yingbo Zhou, Bhargava U.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[4] Dimitri P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[5] Dimitri P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[6] L\u00e9on Bottou, Frank E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Tim Cooijmans, Nicolas Ballas, C\u00e9sar Laurent, and Aaron Courville.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] Yoram Duchi, John and Hazan, Elad and Singer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[10] \u00c7aglar G\u00fcl\u00e7ehre and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] Sergey Ioffe and Christian Szegedy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Alex Krizhevsky and Geoffrey E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Yann LeCun, L\u00e9on Bottou, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Genevieve B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "This is a known result of the Lipschitz-continuous condition that can be found in [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "8 This proof is similar to the the proof by Bertsekas and Tsitsiklis [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "This lemma has been proven by Bertsekas and Tsitsiklis [5].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "[6].", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Batch normalization (BN) is very effective in accelerating the convergence of a neural network training phase that it has become a common practice. We propose a generalization of BN, the diminishing batch normalization (DBN) algorithm. We provide an analysis of the convergence of the DBN algorithm that converges to a stationary point with respect to trainable parameters. We analyze a two layer model with linear activation. The main challenge of the analysis is the fact that some parameters are updated by gradient while others are not. In the numerical experiments, we use models with more layers and ReLU activation. We observe that DBN outperforms the original BN algorithm on MNIST, NI and CIFAR-10 datasets with reasonable complex FNN and CNN models.", "creator": "LaTeX with hyperref package"}}}