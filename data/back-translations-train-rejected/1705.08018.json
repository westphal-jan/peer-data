{"id": "1705.08018", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Use of Knowledge Graph in Rescoring the N-Best List in Automatic Speech Recognition", "abstract": "With the evolution of neural network based methods, automatic speech recognition (ASR) field has been advanced to a level where building an application with speech interface is a reality. In spite of these advances, building a real-time speech recogniser faces several problems such as low recognition accuracy, domain constraint, and out-of-vocabulary words. The low recognition accuracy problem is addressed by improving the acoustic model, language model, decoder and by rescoring the N-best list at the output of the decoder. We are considering the N-best list rescoring approach to improve the recognition accuracy. Most of the methods in the literature use the grammatical, lexical, syntactic and semantic connection between the words in a recognised sentence as a feature to rescore. In this paper, we have tried to see the semantic relatedness between the words in a sentence to rescore the N-best list. Semantic relatedness is computed using TransE~\\cite{bordes2013translating}, a method for low dimensional embedding of a triple in a knowledge graph. The novelty of the paper is the application of semantic web to automatic speech recognition.", "histories": [["v1", "Mon, 22 May 2017 21:53:05 GMT  (724kb,D)", "http://arxiv.org/abs/1705.08018v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ashwini jaya kumar", "camilo morales", "maria-esther vidal", "christoph schmidt", "s\\\"oren auer"], "accepted": false, "id": "1705.08018"}, "pdf": {"name": "1705.08018.pdf", "metadata": {"source": "CRF", "title": "Use of Knowledge Graph in Rescoring the N-best List in Automatic Speech Recognition", "authors": ["Ashwini Jaya Kumar", "Camilo Morales", "Maria-Esther Vidal", "Christoph Schmidt", "S\u00f6ren Auer"], "emails": ["ashwini.jaya.kumar@iais.fraunhofer.de"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people are able to recognize themselves and understand what they are doing."}, {"heading": "2. Background", "text": "In this section, various methods are used to revive the N-best list and are relevant to the work in this thesis. A query-specific search result is used as a feature to revive the n-best list. Search results contain domain-specific knowledge that is difficult to acquire with simple n-gram-based language models. The authors reported that search domain knowledge can be used to classify an unambiguous query and reorder it in the N-best list, resulting in improved recognition accuracy. The default selection criteria for a speech recognition hypothesis are to maximize the rear probability of a hypothesis W against the observation sequence X - an algorithm to revive N-best list where detection probabilities occur, compressing the N-best list and then the expected word error rate for each hypothesis."}, {"heading": "3. Knowledge graph based N-best list rescoring", "text": "Knowledge diagrams can be considered a rich source of information that can be used to interpret a sentence in a logical form or in a form understandable to a computer, e.g. by semantic marking of the sentence. Example: \"Chelsea Clinton is a daughter of Bill Clinton and was born in Arkansas\" is represented in logical / triple form as (< ChelseaClinton, daughter, Billclinton >) and (< ChelseaClinton, born, Arkansas >). The units in the above sentence are \"Chelsea Clinton,\" \"Bill Clinton\" and \"Arkansas\" and associated with properties like \"daughter\" and \"born.\" The above example shows how the units and relationships in Dbpedia knowledge diagrams are interconnected. This type of structured information storage makes it easy to retrieve the stored information and store new information. This type of knowledge source is useful in all language processing applications to make a logical interpretation of a sentence by understanding the units and their relationships."}, {"heading": "3.1. Obtaining the N-best list", "text": "The N-best list of sentences results from a caldi-based speech recognition system. A grid can be defined as a labeled, weighted, directed acyclic graph (Weighted Finite State Transducers with Word Labels) [7]. The N-best paths are calculated by the grid, where the Viterbi search algorithm uses only one tunable parameter: the truncated bar and prints the result as a grid, but with a special structure. The start state in the Viterbi search algorithm shows (up to) n slurs, each of which leads to the beginning of a separate path [9]."}, {"heading": "3.2. Annotating the mentions of Dbpedia in the N-best list", "text": "DBpedia Spotlight is a tool for automatically annotating mentions of DBpedia resources in text, which provides a solution for linking unstructured information sources to the Linked Open Data Cloud via DBpedia. DBpedia Spotlight detects that names of concepts or entities have been mentioned (e.g. \"Michael Jordan\"), and then assigns these names to unique identifiers (e.g. dbpedia: Michael I. Jordan, the Professor of Machine Learning, or dbpedia: Michael Jordan the Basketball Player). [6] There is a web service to locate, clarify, and comment on the entities / concepts. In this work, we only use the annotation. Annotation takes text as input, recognizes entities / concepts for commenting, and selects an identification in Dbpedia for each recognized entity / concept in context."}, {"heading": "3.3. Obtaining the RDF molecules corresponding to annotated entities/relations in Step 2", "text": "We need to see the relationship between the entities / relationships in a set in the diagram. Properties or relationships are stored only in the triple form in the diagram. Entities / relationships can be associated with subject or object or property of a triple combination in the diagram. Let's call the entity / relations as the reference entity / relationships. With Apache Jena API, the other triples that have the reference entity are shortlisted. The number is limited to 500. So here we get 500 EBS molecules for an entity. Likewise, we get 500 EBS molecules for all commented entities / relationships in a set."}, {"heading": "3.4. Calculating the semantic relatedness cost using TransE embeddings", "text": "In TransE, relationships are represented as translations in the embedding space: in (h, l, t), the embedding of tail t should be close to the embedding of head h plus a vector that depends on the relationship l. In other words, the TransE encoding for the same subject is similar, then the TransE encoding has the same vector values and therefore the distance value will be zero. TransE encoding takes care of the repetitive subject or object or predicate in a threefold way. In other words, the TransDF encoding for the same subject in different RDF molecules is the same. TransE embedding is achieved for all the RDF molecules of the entities in one sentence."}, {"heading": "3.5. Rescoring the N-best list based on the semantic relatedness cost computed in Step 4", "text": "The semantic relationship cost (SRC) calculated for a set is used as a feature to revive the N leaderboard. The lower the SRC, the higher the relationship between the units / relationships in a set. Since the semantic relationship cost is calculated from the true information in the knowledge diagram, it can be considered a true cost."}, {"heading": "4. Discussion and Conclusion", "text": "The training database is TED-LIUM (Tedlium-1) with a training date of 118 hours, it consists of TED conversations with purified automatic transcripts 1. Mel frequency receiver coefficients are used as features to train a deep neural network based acoustic model and an n-gram based speech model. The decoding curve is created using the weighted finite state converter [10]. The best list of sentences is N = 30. The linking of the entities is done with a confidence value of 0.3 on Dbpedia annotation tool. The Apache Jena API is used to fetch the RDF molecules from Dbpedia knowledge diagram with LIMIT = 500. TransE is exercised with the RDF molecules at the record level derived from Dbpedia. The method discussed above showed average results on Tbpedia knowledge chart with LIMIT = 500. TransE is exercised with the RDF molecules at the record level derived from Dbpedia."}, {"heading": "5. Acknowledgement", "text": "Parts of this work were funded under the European Union's Horizon 2020 research and innovation programme under the Marie1http: / / www-lium.univ-lemans.fr / en / content / ted-lium-corpusSklodowska-Curie grant agreement No. 642795 (WDAqua project)."}, {"heading": "6. References", "text": "[1] Peng, Fuchun, Scott Roy, Ben Shahshahani, and Franoise Beau-fays. \"Search results are based on N-best hypothesis rescoring with maximum entropy classification.\" In ASRU, pp. 422-427. 2013. [2] Rayner, Manny, David Carter, Vassilios Digalakis, and Patti Price. \"Combining knowledge sources to reorder n-best speech hypothesis lists.\" In Proceedings of the workshop on Human Language Technology, pp. 217-221. Association for Computational Linguistics, 1994. [3] Stolcke, Yochai Konig, and Mitchel Weintraub. \"Explicit word error minimization in n-best list rescoring.\" In Eurospeech, vol. 97, pp."}], "references": [{"title": "Search results based N-best hypothesis rescoring with maximum entropy classification.", "author": ["Peng", "Fuchun", "Scott Roy", "Ben Shahshahani", "Franoise Beaufays"], "venue": "In ASRU,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Combining knowledge sources to reorder n-best speech hypothesis lists.", "author": ["Rayner", "Manny", "David Carter", "Vassilios Digalakis", "Patti Price"], "venue": "In Proceedings of the workshop on Human Language Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Explicit word error minimization in n-best list rescoring.", "author": ["Stolcke", "Andreas", "Yochai Konig", "Mitchel Weintraub"], "venue": "In Eurospeech,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "A Study on Knowledge Source Integration for Candidate Rescoring in Automatic Speech Recognition.", "author": ["Li", "Jinyu", "Yu Tsao", "Chin-Hui Lee"], "venue": "In ICASSP", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "N-best rescoring based on pitch-accent patterns.", "author": ["Jeon", "Je Hun", "Wen Wang", "Yang Liu"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Improving efficiency and accuracy in multilingual entity extraction.", "author": ["Daiber", "Joachim", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes"], "venue": "In Proceedings of the 9th International Conference on Semantic Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Generating exact lattices in the WFST framework.", "author": ["Povey", "Daniel", "Mirko Hannemann", "Gilles Boulianne", "Luk Burget", "Arnab Ghoshal", "Milo Janda", "Martin Karafit"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data.", "author": ["Bordes", "Antoine", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "The Kaldi speech recognition toolkit.", "author": ["Povey", "Daniel", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann"], "venue": "In IEEE 2011 workshop on automatic speech recognition and understanding,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1925}, {"title": "Speech recognition with weighted finite-state transducers.", "author": ["Mohri", "Mehryar", "Fernando Pereira", "Michael Riley"], "venue": "In Springer Handbook of Speech Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "Semantic relatedness is computed using TransE [8], a method for low dimensional embedding of a triple in a knowledge graph.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "TransE [8] is a method to model the relationships between entities by interpreting them as translations operating on the low-dimensional embeddings of the entities.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "A query specific search result is used as a feature to rescore the n-best list in [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "In [3] an algorithm to rescore N-best list is proposed, where it approximates posterior probabilities using the N-best list and then the expected word error rate is computed for each hypothesis with respect to posterior probability distribution.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In [1], various knowledge sources such as recogniser score, linguistic analysis, grammar construction, semantic discrimination score is used to rescore the N-best list.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Articulatory based feature is used as a knowledge source to rescore the N-best list in [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "In [5] the acoustic and lexical prosodic models are applied to each n-best hypothesis to obtain its prosody score, and combined with ASR scores to find the top hypothesis.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "TransE [8] aims at embedding entities and relationships in a relational database (knowledge graphs) into a lower dimensional vector space.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "A lattice can be defined as a labelled, weighted, directed acyclic graph (Weighted Finite State Transducers with word labels) [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "The start state in the viterbi beam search algorithm will have (up to) n arcs out of it, each one to the start of a separate path [9].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "Jordan, the machine learning professor or dbpedia:Michael Jordan the basketball player) [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "The connection information stored in a triple is represented in vector space using TransE [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "A kaldi based speech recogniser is used in this work [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "Decoding graph is created using the weighted finite state transducers [10].", "startOffset": 70, "endOffset": 74}], "year": 2017, "abstractText": "With the evolution of neural network based methods, automatic speech recognition (ASR) field has been advanced to a level where building an application with speech interface is a reality. Inspite of these advances, building a real-time speech recogniser faces several problems such as low recognition accuracy, domain constraint and out-of-vocabulary words. The low recognition accuracy problem is addressed by improving the acoustic model, language model, decoder and by rescoring the N-best list at the output of decoder. We are considering the N-best list rescoring approach to improve the recognition accuracy. Most of the methods in literature uses the grammatical, lexical, syntactic and semantic connection between the words in a recognised sentence as a feature to rescore. In this paper, we have tried to see the semantic relatedness between the words in a sentence to rescore the N-best list. Semantic relatedness is computed using TransE [8], a method for low dimensional embedding of a triple in a knowledge graph. The novelty of the paper is the application of semantic web to automatic speech recognition.", "creator": "LaTeX with hyperref package"}}}