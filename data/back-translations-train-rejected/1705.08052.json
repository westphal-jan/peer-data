{"id": "1705.08052", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Compressing Recurrent Neural Network with Tensor Train", "abstract": "Recurrent Neural Network (RNN) are a popular choice for modeling temporal and sequential tasks and achieve many state-of-the-art performance on various complex problems. However, most of the state-of-the-art RNNs have millions of parameters and require many computational resources for training and predicting new data. This paper proposes an alternative RNN model to reduce the number of parameters significantly by representing the weight parameters based on Tensor Train (TT) format. In this paper, we implement the TT-format representation for several RNN architectures such as simple RNN and Gated Recurrent Unit (GRU). We compare and evaluate our proposed RNN model with uncompressed RNN model on sequence classification and sequence prediction tasks. Our proposed RNNs with TT-format are able to preserve the performance while reducing the number of RNN parameters significantly up to 40 times smaller.", "histories": [["v1", "Tue, 23 May 2017 01:14:22 GMT  (418kb,D)", "http://arxiv.org/abs/1705.08052v1", "Accepted at IJCNN 2017"]], "COMMENTS": "Accepted at IJCNN 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andros tjandra", "sakriani sakti", "satoshi nakamura"], "accepted": false, "id": "1705.08052"}, "pdf": {"name": "1705.08052.pdf", "metadata": {"source": "CRF", "title": "Compressing Recurrent Neural Network with Tensor Train", "authors": ["Andros Tjandra", "Sakriani Sakti", "Satoshi Nakamura"], "emails": ["andros.tjandra.ai6@is.naist.jp,", "ssakti@is.naist.jp,", "s-nakamura@is.naist.jp"], "sections": [{"heading": null, "text": "In recent years, the number of those who are able to put it into practice has decreased significantly. In recent years, the number of those who are able to put it into practice has soared. In the last ten years, the number of those who are able to put it into practice has soared. In the last ten years, the number of those who are able to put it into practice has soared. In the last ten years, the number of those who are able to put it into practice has soared. In the last ten years, the number of those who are able to put it into practice has soared."}, {"heading": "A. Simple Recurrent Neural Network", "text": "Typically, we define the input sequence x = (x1,..., xT), the hidden vector sequence h = (h1,..., hT) and the output vector sequence y = (y1,..., yT). As shown in Fig. 1, a simple RNN at present t can be formulated as follows: ht = f (Wxhxt + Whhht \u2212 1 + bh) (1) yt = g (Whyht + by). (2) where Wxh represents the weight parameters between the input and the hidden layer, Whh represents the weight parameters between the hidden and the hidden layer, Why represents the weight parameters between the hidden and the output layer, and bh represents by bias vectors for the hidden and the output layers. Functions f (\u00b7) and g (\u00b7) are non-linear activation functions, such as the signature parameters between the output layer and the output layer."}, {"heading": "B. Gated Recurrent Neural Network", "text": "This problem is caused by the effect of the limited activation functions and their derivatives. Therefore, the formation of a simple RNN is more complicated than the formation of an unrestricted neural network. Some research has addressed the difficulties of forming simple RNNNs. [18] They have replaced the activation function, which causes the disappearance of gradients with a rectifier linear (ReLU) function. With an unrestricted activation function and identity initialization, they have optimized a simple RNN dependency for long-term dependencies. [19] They used a second-order Hessian-free (RF) optimization method instead of the first-order method such as gradients."}, {"heading": "A. Tensor Train (TT) format", "text": "Before defining Tensor Train (TT) format, we explain the notations that we borrow from [13], [14] used in later sections. Generally, one-dimensional arrays are called vectors, two-dimensional arrays are called matrices, and all higher multidimensional arrays are commonly referred to as tensors. We represent vectors with lowercase letters (e.g. b), matrices with uppercase letters (e.g. W), and tensors with calligraphic uppercase letters (e.g. W). Each element of the vectors, matrices, and tensors is explicitly represented with indexing in each dimension."}, {"heading": "B. Representing Linear Transformation using TT-format", "text": "In most cases, the matrix W has many more parameters than the matrix W, in which we can use the TT format for optimizing our neural networks. (1) In most cases, the matrix W is much more parameters than the matrix W, where M = 1 mk = 1 mk and N = 1 nk as tensor W by using the bijective functions fi: Z + \u2192 Zd + and f j: Z + \u2192 Zd +. We have each line p (1.., M) in fi (p), i1 (p), i (p) and f (c)."}, {"heading": "C. Compressing Simple RNN with TT-format", "text": "We present a simple RNN in TT format and call this model TT-SRNN for the rest of this work. From Section II-A, we focus our attention on two dense weight matrices: (Wxh, Whh). Previously, we defined Wxh-RM-N as inputto-hidden parameter and Whh-RM-M as hidden to hidden parameters. First, we factored the matrix form M into the form dk = 1 mk and N into the form d = 1 nk. Next, we determined the TT rank {rk} dk = 0 for our model and replaced Wxh with the tensor Wxh \u2212 and Whh with the tensor Wxh. Tensor Wxh is represented by a series of TT cores {Gxhk} dk = 1 where the formula k-1, d-xi \u2212 h \u2212 Ghh \u2212 xxxxxxxx rk \u2212 xh is replaced by the tensor Wxh. Whht-Cores is represented by a series of TT cores {Gxhk} dk = 1, dh = x1, dk-1, dk = 1."}, {"heading": "D. Compressing GRU RNN with TT-format", "text": "In this section we use TT format to represent a gated RNN."}, {"heading": "A. Sequence Classification on Sequential MNIST", "text": "In fact, we are able to assert ourselves, we are able to adapt, we are able to adapt to the needs of the people and we are able to adapt to the needs of the people."}, {"heading": "B. Sequence Prediction on Polyphonic Music", "text": "This year, we will be in a position to start looking for a new partner who will be able to help us find a new partner."}], "references": [{"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "arXiv preprint arXiv:1609.08144, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition for mobile devices at Google", "author": ["M. Schuster"], "venue": "Pacific Rim International Conference on Artificial Intelligence. Springer, 2010, pp. 8\u201310.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": "Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Recurrent neural network training with dark knowledge transfer", "author": ["Z. Tang", "D. Wang", "Z. Zhang"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5900\u20135904.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 442\u2013450.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing, vol. 33, no. 5, pp. 2295\u20132317, 2011. [Online]. Available: http://dx.doi.org/10.1137/090752286", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning recurrent neural networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1033\u20131040.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.-r. Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["A. Graves"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-lewandowski", "Y. Bengio", "P. Vincent"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), J. Langford and J. Pineau, Eds. New York, NY, USA: ACM, 2012, pp. 1159\u20131166. [Online]. Available: http://icml.cc/2012/papers/590.pdf", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of multiplef0 estimation and tracking systems.", "author": ["M. Bay", "A.F. Ehmann", "J.S. Downie"], "venue": "in 2009 International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 2348\u2013 2356.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["G.E. Hinton", "D. Van Camp"], "venue": "Proceedings of the sixth annual conference on Computational learning theory. ACM, 1993, pp. 5\u201313.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1993}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, 2015, pp. 1737\u20131746.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Training deep neural networks with low precision multiplications", "author": ["M. Courbariaux", "J.-P. David", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.7024, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 3123\u2013 3131.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/ 1605.02688", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning compact recurrent neural networks", "author": ["Z. Lu", "V. Sindhwani", "T.N. Sainath"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5960\u20135964.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Although RNNs have been researched for about two decades [1], [2], their recent resurgence reflects improvements in computer hardware and the growth of available datasets.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "Although RNNs have been researched for about two decades [1], [2], their recent resurgence reflects improvements in computer hardware and the growth of available datasets.", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Especially for state-of-the-art models on speech recognition [4] and machine translation [5], such huge models can only be implemented in high-end cluster environments because they need massive computation power and millions of parameters.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Especially for state-of-the-art models on speech recognition [4] and machine translation [5], such huge models can only be implemented in high-end cluster environments because they need massive computation power and millions of parameters.", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "This limitation hinders the creation of efficient RNN models that are fast enough for massive real-time inference or small enough to be implemented in low-end devices like mobile phones [8] or embedded systems with limited memory.", "startOffset": 186, "endOffset": 189}, {"referenceID": 8, "context": "[9] and Ba et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] successfully compressed a large deep neural network into a smaller neural network by training the latter on the transformed softmax outputs from the former.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Distilling knowledge from larger neural networks has also been successfully applied to recurrent neural network architecture by [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "[12] utilized low-rank matrix decomposition of the weight matrices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] replaced the dense weight matrices with Tensor Train (TT) format [14] inside convolutional neural network (CNN) model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] replaced the dense weight matrices with Tensor Train (TT) format [14] inside convolutional neural network (CNN) model.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "An RNN is a kind of neural network architecture that models sequential and temporal dependencies [15].", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Simple RNNs cannot easily be used for modeling datasets with long sequences and long-term dependency because the gradient can easily vanish or explode [16], [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Simple RNNs cannot easily be used for modeling datasets with long sequences and long-term dependency because the gradient can easily vanish or explode [16], [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "[18] replaced the activation function that causes the vanishing gradient with a rectifier linear (ReLU) function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] used a second-order Hessian-free (HF) optimization method rather than the first-order method such as gradient descent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The additional gating layers control the information flow from the previous states and the current input [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "2, the LSTM hidden layer values at time t are defined by the following equations [20]:", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "[21] as an alternative to LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "First, a GRU does not have memory cells [22].", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "3, the GRU hidden layer at time t is defined by the following equations [21]:", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "GRU can match LSTM\u2019s performance and its convergence speed sometimes surpasses LSTM, despite having one fewer gating layer [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "We start with the description of Tensor Train [14] and then represent the linear transformation operation in the TT-format", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Before defining Tensor Train (TT) format, we will explain the notations which we borrow from [13], [14] that will be used in later sections.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Before defining Tensor Train (TT) format, we will explain the notations which we borrow from [13], [14] that will be used in later sections.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Based on previous description [13], we assume that d-dimensional array (tensor) W is represented in TT-format [14] if for each k \u2208 {1, .", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "Based on previous description [13], we assume that d-dimensional array (tensor) W is represented in TT-format [14] if for each k \u2208 {1, .", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "Therefore, we can utilize the TT-format for optimizing our neural networks by replacing weight matrix W with tensor W in TT-format [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "Table I compares the forward and backward propagation times and the memory complexity between the fully connected layer and the TT-layer in Big-O notation [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "Especially for our RNN with TT-format that has many mini-tensors and several multiplications, the TTRNN will have a longer matrix multiplication chain than a standard RNN, and the hidden layer value will quickly saturate [23].", "startOffset": 221, "endOffset": 225}, {"referenceID": 22, "context": "In our implementation, we follow Glorot initialization [23] to keep the same variance of weights gradients across layers and time-steps to avoid the vanishing gradient problem.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "We conducted the experiments on sequence classification tasks, where each input sequence was assigned a single class, and sequence prediction tasks, where we predicted the next time-step based on previous information [24].", "startOffset": 217, "endOffset": 221}, {"referenceID": 24, "context": "We used Adam algorithm [25] to optimize our model parameters.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "We evaluated our proposed model TT-SRNN and TT-GRU for classification task using the MNIST dataset [18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "This task is very challenging even for an RNN with gating mechanism because the RNN needs to model very long sequences [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "In the last experiment, we used the most difficult task [18] to push the limits of the gated RNN model.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "For the sequential modeling tasks, we used four polyphonic music datasets [26]: Piano-midi.", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "To calculate the accuracy, we followed the evaluation metric proposed by [27] where ACC = T P/(T P + FP + FN).", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "[10] and Hinton et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] \u201cdistilled\u201d the knowledge from a deep neural network into a shallow neural network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] utilized a similar approach for training RNN with a trained DNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] proposed a variational inference method for learning the mean and variance of Gaussian distribution for each weight parameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "They reformulated the variational inference as the optimization of a Minimum Description Length [29].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "[30] and Courbariaux et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] minimized the performance loss while using fewer bits (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] proposed BinaryConnect to constrain the weight possible values to -1 or +1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Most of these ideas can be easily applied with our proposed model since several deep-learning frameworks have built-in low-precision floating point options [33], [34].", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "Most of these ideas can be easily applied with our proposed model since several deep-learning frameworks have built-in low-precision floating point options [33], [34].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Model compression using low-rank matrix has also been reported [12], [35].", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "Model compression using low-rank matrix has also been reported [12], [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "[36] used low-rank matrix ideas to reduce the number of parameters in an RNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] utilized TT-format to represent weight matrices on feedforward neural networks.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Recurrent Neural Network (RNN) are a popular choice for modeling temporal and sequential tasks and achieve many state-of-the-art performance on various complex problems. However, most of the state-of-the-art RNNs have millions of parameters and require many computational resources for training and predicting new data. This paper proposes an alternative RNN model to reduce the number of parameters significantly by representing the weight parameters based on Tensor Train (TT) format. In this paper, we implement the TT-format representation for several RNN architectures such as simple RNN and Gated Recurrent Unit (GRU). We compare and evaluate our proposed RNN model with uncompressed RNN model on sequence classification and sequence prediction tasks. Our proposed RNNs with TT-format are able to preserve the performance while reducing the number of RNN parameters significantly up to 40 times smaller.", "creator": "LaTeX with hyperref package"}}}