{"id": "1705.08063", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge", "abstract": "Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to reflect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the effectiveness of our model by significantly outperforming the state-of-the-art. We furthermore demonstrate how an effective contextualization method results in improving citation-based summarization of the scientific articles.", "histories": [["v1", "Tue, 23 May 2017 02:55:56 GMT  (106kb,D)", "http://arxiv.org/abs/1705.08063v1", "SIGIR 2017"]], "COMMENTS": "SIGIR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["arman cohan", "nazli goharian"], "accepted": false, "id": "1705.08063"}, "pdf": {"name": "1705.08063.pdf", "metadata": {"source": "META", "title": "Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge", "authors": ["Arman Cohan", "Nazli Goharian"], "emails": ["arman@ir.cs.georgetown.edu", "nazli@ir.cs.georgetown.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "KEYWORDS Text Summary, Scienti c Text, Information Gathering"}, {"heading": "1 INTRODUCTION", "text": "The literature often refers to a short description of this work, which we call quotation texts. Citation texts usually highlight certain contributions of the cited paper and a number of quotation texts to a reference work. Therefore, quotation texts have been used to improve many downstream tasks, such as searching and summarizing (e.g. [2, 15, 16]). While useful quotation texts lack the appropriate context from the reference article, details of the methods, assumptions or conditions for the results obtained are often not mentioned. Furthermore, the quoting author may misunderstand and assign the reference texts that are not intended in this form."}, {"heading": "2 CONTEXTUALIZING CITATIONS", "text": "The goal in LM is to evaluate a document d according to the conditional probability p (d). (d) The conceptual plurality for IR (LM) extends to the conceptual plurality of IR (LM) by including the conceptual plurality and the domain ontology for this research purpose. (d) The goal in LM is to evaluate a document d according to the conditional probability p (d). (d) The conceptual plurality of the conceptual plurality p (d) extends to the conceptual plurality p (d). (d) The conceptual plurality of the conceptual plurality p (d) extends to the conceptual plurality p (d). (d), where the conceptual plurality of the query q (d) is shown. (d) is often achieved by the maximum probability that the conceptual plurality is estimated with some kind of smoothing. (d) We have the use of Smoothing + 21 (d)."}, {"heading": "3 EXPERIMENTS", "text": "We use the TAC 2014 Biomedical Summarization benchmark3. This dataset contains 220 scientific articles in biomedical journals and 313 citation texts in total, in which the relevant contexts for each citation text are commented on by 4 experts. To our knowledge, the only published results for TAC 2014 [4] are those in which the authors used a Query Reformulation (QR) based on UMLS ontology. In addition to [4], we also implement several other strong baselines to better evaluate the environmental impact of our model: 1) BM25; 2) VSM: Vector Space Model used in [4]; 3) DESM: Dual Embedding Space Model, which is a current embedding model based on available models [12]; and 4) LMD-LDA: Language Modeling with LDA smoothing mddomas, which is a recent extension of the MLD in general to include the MLD."}, {"heading": "3.1 Intrinsic Evaluation", "text": "In fact, the fact is that most of us are able to go in search of a solution that is capable, in which they are able to find a solution that they are able to find, in which they are able to find a solution."}, {"heading": "3.2 External evaluation", "text": "The results of the external evaluation are illustrated by the use of citation texts. [15] However, as argued in Section 1, citation texts do not always exactly correspond to the original paper. We show how adding a context from the original paper can accommodate this concern while maintaining the results of the citation-based summary. Specifically, we compare how using without contextualization affects the quality of the summary compared to various suggested contextualization approaches. We apply the following well-known summation algorithms to the set of citation texts and the citation contexts obtained: LexRank, LSAbased, SumBasic, and KL divergence (for space limitations, we will not explain these approaches here; refer to [14] for details. We then compare the effect of our proposed contextualization methods using the standard Rouge-N summary metrics."}, {"heading": "4 RELATEDWORK", "text": "In this paper, we focus on extracting the relevant context from the reference paper in view of the citation texts. Related work has also shown that citation texts can be used in common applications such as summaries [2, 3, 9, 11, 15, 20]. Our proposed model uses word embedding and domain knowledge. Embedding was recently used in general information retrieval models. Vuli\u0107 and Moens [19] proposed an architecture for embedding words in multilingual contexts and used it in document and query presentation. Mitra et al. [12] proposed a dual embedding model that predicts the flow of documents by comparing the core of word vectors with terms. Ganguly et al. [7] used embedding to transform term weights into a query translation model."}, {"heading": "5 CONCLUSIONS", "text": "Citation texts are passages of text in a quoting article that explain certain contributions of a reference paper. We presented an effective model for contextualizing citation texts (by linking them to the corresponding context from the reference paper), achieved statistically significant improvements in multiple evaluation metrics across multiple strong baselines, and matched the accuracy of human annotations. We demonstrated that embedding embedding and domain knowledge in language modeling-based restoration is beneficial for situations where there are large terminological differences between source and target (such as citations and their reference context). Citation contextualization can help readers not only to better understand the citation texts, but also, as we have shown, improve other downstream applications such as the summary of Scienti-c documents. Overall, our results show that citation contextualization enables us to better understand the merits of citation texts while making use of the original citations and their claims."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank the three anonymous reviewers for their helpful comments and suggestions. This work was partially supported by the National Science Foundation (NSF) with funding from CNS-1204347."}], "references": [{"title": "Reference scope identi cation in citing sentences", "author": ["Amjad Abu-Jbara", "Dragomir Radev"], "venue": "In NAACL-HLT. ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Scienti c Article Summarization Using Citation-Context and Article\u2019s Discourse Structure", "author": ["Arman Cohan", "Nazli Goharian"], "venue": "In EMNLP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Scienti c document summarization via citation contextualization and scienti c discourse", "author": ["Arman Cohan", "Nazli Goharian"], "venue": "International Journal on Digital Libraries", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach", "author": ["Arman Cohan", "Luca Soldaini", "Nazli Goharian"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Epistemic modality and knowledge attribution in scienti c discourse: A taxonomy of types and overview of features", "author": ["Anita de Waard", "Henk Pander Maat"], "venue": "In Workshop on Detecting Structure in Scholarly Discourse", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Retro tting Word Vectors to Semantic Lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Kumar Sujay Jauhar", "Chris Dyer", "Eduard Hovy", "A. Noah Smith"], "venue": "In NAACL-HLT. Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Word Embedding Based Generalized Language Model for Information Retrieval", "author": ["Debasis Ganguly", "Dwaipayan Roy", "Mandar Mitra", "Gareth J.F. Jones"], "venue": "In SIGIR", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Simlex-999: Evaluating semantic models with similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Overview of the CL-SciSumm 2016 Shared Task", "author": ["Kokil Jaidka", "Muthu Kumar Chandrasekaran", "Sajal Rustagi", "Min-Yen Kan"], "venue": "In BIRNDL@ JCDL", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A simple enhancement for ad-hoc information retrieval via topic modelling", "author": ["Fanghong Jian", "Jimmy Xiangji Huang", "Jiashu Zhao", "Tingting He", "Po Hu"], "venue": "In SIGIR", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Generating Impact-Based Summaries for Scienti c Literature", "author": ["Qiaozhu Mei", "ChengXiang Zhai"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A dual embedding space model for document ranking", "author": ["Bhaskar Mitra", "Eric Nalisnick", "Nick Craswell", "Rich Caruana"], "venue": "CoRR arXiv:1602.01137", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Classify or Select: Neural Architectures for Extractive Document Summarization", "author": ["Ramesh Nallapati", "Bowen Zhou", "Mingbo Ma"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "A survey of text summarization techniques", "author": ["Ani Nenkova", "Kathleen McKeown"], "venue": "In Mining text data", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Scienti c Paper Summarization Using Citation Summary Networks (COLING", "author": ["Vahed Qazvinian", "Dragomir R. Radev"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Comparing Citation Contexts for Information Retrieval", "author": ["Anna Ritchie", "Stephen Robertson", "Simone Teufel"], "venue": "In CIKM", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Identifying claimed knowledge updates in biomedical research articles", "author": ["\u00c1gnes S\u00e1ndor", "Anita De Waard"], "venue": "In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Summarizing scienti c articles: experiments with relevance and rhetorical status", "author": ["Simone Teufel", "Marc Moens"], "venue": "Computational linguistics", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Whetting the appetite of scientists: Producing summaries tailored to the citation context", "author": ["Stephen Wan", "C\u00e9cile Paris", "Robert Dale"], "venue": "JCDL", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["Chengxiang Zhai", "John La erty"], "venue": "TOIS 22,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "[2, 15, 16]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[2, 15, 16]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 15, "context": "[2, 15, 16]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 3, "context": "While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18].", "startOffset": 91, "endOffset": 101}, {"referenceID": 4, "context": "While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18].", "startOffset": 91, "endOffset": 101}, {"referenceID": 17, "context": "While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18].", "startOffset": 91, "endOffset": 101}, {"referenceID": 16, "context": "the citation text is not su ciently informative or in other cases, even inaccurate [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Using Dirichlet smoothing [21], we have:", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "On the other hand, domain ontologies and lexicons that are built by experts include some information that might not be captured by embedding methods [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "[6] proposed a model that uses the constraints on WordNet lexicon to modify the word vectors and pull synonymous words closer to each other.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "where \u03b3 \u2208 [0, 1] is a parameter and qi \u2248 dj shows that there is an is-synonym relation in ontology between qi and dj 2.", "startOffset": 10, "endOffset": 16}, {"referenceID": 3, "context": "To our knowledge, the only published results on TAC 2014 is [4], where the authors utilized query reformulation (QR) based on UMLS ontology.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 173, "endOffset": 176}, {"referenceID": 11, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 265, "endOffset": 269}, {"referenceID": 9, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 401, "endOffset": 405}, {"referenceID": 3, "context": "VSM [4] 20.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "2 DESM [12] 20.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "5 LMD-LDA [10] 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "7 QR [4] 22.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "by [4] which improves over other baselines.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "This further con rms the inferior performance of the out-of-domain word embeddings in capturing correct word-level semantics [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "Citation-based summarization can e ectively capture various contributions and aspects of the paper by utilizing citation texts [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "based, SumBasic, and KL-Divergence (For space constraints, we will not explain these approaches here; refer to [14] for details).", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "7 VSM [4] 35.", "startOffset": 6, "endOffset": 9}, {"referenceID": 11, "context": "6 DESM [12] 36.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "9 LMD-LDA [10] 38.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "9 QR [4] 39.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 2, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 8, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 10, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 14, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 19, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 18, "context": "Vuli\u0107 and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "[12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] used embeddings to transform term weights in a translation model for retrieval.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations.", "startOffset": 40, "endOffset": 43}], "year": 2017, "abstractText": "Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to re ect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the e ectiveness of our model by signi cantly outperforming the state-of-the-art. We furthermore demonstrate how an e ective contextualization method results in improving citation-based summarization of the scienti c articles.", "creator": "LaTeX with hyperref package"}}}