{"id": "1705.08076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Learning from partial correction", "abstract": "We introduce a new model of interactive learning in which an expert examines the predictions of a learner and partially fixes them if they are wrong. Although this kind of feedback is not i.i.d., we show statistical generalization bounds on the quality of the learned model.", "histories": [["v1", "Tue, 23 May 2017 05:07:52 GMT  (57kb,D)", "https://arxiv.org/abs/1705.08076v1", null], ["v2", "Wed, 24 May 2017 16:39:52 GMT  (57kb,D)", "http://arxiv.org/abs/1705.08076v2", "10 pages"], ["v3", "Thu, 25 May 2017 16:50:52 GMT  (57kb,D)", "http://arxiv.org/abs/1705.08076v3", "10 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sanjoy dasgupta", "michael luby"], "accepted": false, "id": "1705.08076"}, "pdf": {"name": "1705.08076.pdf", "metadata": {"source": "CRF", "title": "Learning from partial correction", "authors": ["Sanjoy Dasgupta", "Michael Luby"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is a question of whether and in what form people will be able to identify themselves in their totality, and the question of whether they will be able to identify themselves in their totality. (...) It is a question of whether they will be able to identify themselves in their totality. (...) It is a question of whether they will be able to identify themselves in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality, in their totality"}, {"heading": "1.1 Learning procedure", "text": "In the tree example above, Q is all subsets of six species and \u00b5 is the even distribution to Q. At step t = 1, 2,... of learning, the learner selects something that corresponds to all the feedback received so far. Select q-\u00b5 Q, where q has c atomic components, (q, c)... 3. The learner shows q and ht (q) to the expert 4. If ht (q) is correct: \u2022 The expert returns that ht (q) is correct \u2022 The feedback implicitly returns h-ig (q, j) = ht (q, j). Otherwise ht (q) is wrong: \u2022 The expert selects 1-j-c, for which ht (q, j) 6 h-j (q, j) \u2022 Expert feeds and j-j (h)."}, {"heading": "1.2 Results", "text": "The error of a hypothesis h H can be measured in two ways: in relation to complete questions q-Q, err (h) = Prq-Q [h) = Prq-Q [h, j) 6 = h-Q [h, j) 6 = h-Q (h, j) 6 = h-Q (h, j) 6 = h-Q (h, j) 6 = h-Q (h, j) 6 = h-Q (h, j) 6 = h-Q (h) \u2264 err (h) \u2264 err (h) c (h) c (h) -S (h, j) 6 = h-Q (h) -S (h) -S (h) -S (h) -S (h) -S (h) -S (h) -M (h) -S (h) -S (h) -S (h) -S (h) -S (h) s) s (h) s) s (h) s) s s s (h) s s (h) s) s s s (h) s s (h) s s (s) s) s (s) s (h) s (h) s (h) s (s) s) s (s) s (h) s (s) s) s (h s) s (h s) s (h) s (s) s (h) s (s) s) s (h s) s (h s) s (h s) s (h s) s (h s) s (s) s) s (h s) s (h s) s (h s) s (h s) s (h s) s (s) s (h s) s (h s) s (h s) s (h (h s) s) s (s) s (h s (h s) s (h s) s (h s (h s) s (h s) s) s (h (h s) s (h s) s (h s) s (h s (s) s (h s) s) s (h s (h s) s (h s (h s) s (h s) s (h s) s (h s) s (h s (h s) (h s ("}, {"heading": "2 An illustrative example", "text": "Suppose X = [0, 1] and the goal is to learn a threshold classifier: H = {hv: v [0, 1]}, hv (x) = 1 (x > v). Suppose the target threshold is 0 (i.e. h \u0445 = h0), so that the correct name for all points in (0, 1] equals 1. If we learned from random examples (x, h \u0445 (x), then according to O (1 /) samples, regardless of the distribution on X, all remaining consistent hypotheses (with a probability close to one) would consist exclusively of classifiers h with Err (h) \u2264. Thus, the error according to O (1) instances would be lower than any previously determined constant."}, {"heading": "2.1 Uniformly distributed, component independent queries", "text": "This is the case when the rate of convergence from 0 to zero of c and is influenced by the policy of the expert label, for which errors have to be corrected, where is the threshold determined by the learner according to t-steps, i.e., vt is the smallest component value for which the feedback was provided by the expert and therefore only classifiers with the threshold have to be corrected. We consider two expert guidelines: \u2022 \"The largest,\" \"the largest,\" \"the largest,\" \"the largest,\" \"the largest,\" \"the largest,\" \"the largest,\" \"the largest,\" \"the smallest,\" \"the largest,\" \"the largest.\""}, {"heading": "2.2 A lower bound on component-level error", "text": "We continue with the one-dimensional example, but now turn to distributions that are not component-independent. We look at a learner who starts with a threshold of 1 and at a given time chooses the largest threshold that is consistent with all previous feedback: namely, the smallest marked point he has received."}, {"heading": "2.2.1 A single query, repeated", "text": "To start with a particularly simple case: Suppose that the \u00b5 distribution over Q is supported at a single point (1 / c, 2 / c,..., 1). Suppose the labeler behaves as follows: when confronted with a label on points 1 / c, 2 / c,..., 1, he always decides to \"correct the most glaring error,\" that is, the highest value for which a 0 label is proposed. It is clear that x = 1 is labeled in the first round, x = (c \u2212 1) / c in the second round, x = (c \u2212 2) / c in the third round, etc. The behavior of the labeler is hardly pathological."}, {"heading": "2.2.2 Lower bound", "text": "Select a > 0 and now look at a distribution \u00b5 over Q that is supported only in two points: (1 2c, 2 2c,..., 1 2) probability 2 (1 2 + 1 2c, 1 2 + 2 2c,..., 1) probability 1 \u2212 2Any hypothesis with errc (hv) \u2264 must be v \u2264 1 / 4. To achieve this, the learner must see the first point at least c / 2 times, which requires that he sees the (c /) samples as a whole with a high probability. We found the following. Theorem 1 There is a concept class H of VC dimension 1, so that for each > 0 it is necessary that there is round of feedback to guarantee with a high probability that all hypotheses have h in accordance with this feedback errc (h) \u2264."}, {"heading": "3 Main result", "text": "Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2: Theorem 2 2: Theorem 2: Theorem 2: Theorem 2"}, {"heading": "3.1 Effective sampling distribution", "text": "At the beginning of step t, a hypothesis ht =.q = q = q q values (q) = q values (q, j) is defined as follows: \u2022 For all (q, j) \u00b2 B \u00b2 (ht), the conditional probability is given that the expert will give feedback on (q, j) when querying q. Define wt (q, j) = \u00b5 (q) \u00b7 \u03b3 (q, j). \u2022 For all q \u00b2 G (ht), calculate wt (q, 1), wt (q, c), sum up to \u00b5 (q), as indicated below in Lemma 3.Finally, leave Wt (q, j) = w1 (q, j) + \u00b7 + wt (q, j) the sum of the individual distributions up to step. Note that at each step t wt, for each q q q-Q, we wt = wt (q, [c] =.Wt (q) and hence Wt (j), q = q-q (q-q) (1)."}, {"heading": "3.2 Eliminating inconsistent hypotheses", "text": "Lemma 4 With a probability of at least 1 \u2212 \u03b4, the following applies to all h-H: If there is a step t where Wt (B-H) \u2265 'is given, then h is not consistent with the feedback received up to this step. Proof: Select any h-H. It is eliminated if feedback is received on one (q, j) and B-H (h). The probability that this will happen at step t is at least wt (B-H). Let us select the first step where Wt (B-H) has been received. The probability that h is not eliminated until the end of step t is at most (1 \u2212 w1 (B-H)) \u00b7 (1 \u2212 w2 (B-H)) \u00b7 \u00b7 \u00b7 (1 \u2212 wt (B-H)) \u2264 exp (\u2212 ltp (B-H))."}, {"heading": "3.3 Analysis for Phase 1", "text": "Consider a first phase consisting of the first N-steps."}, {"heading": "3.4 Analysis for Phase 2", "text": "s leave ht to be the current hypothesis for one of these steps. If \u00b5 (B (ht)) \u2265 2 \u00b7 \"then \u00b5 (B (ht) \u2212\" and the first half of the proof of term 6 implies that W-t (Q) at least \"increases\" in this step. However, since W-t (Q-T) \u2264 N and W-N (Q-N) \u2265 (1 \u2212) \u00b7 N at the beginning of the second phase of episode 7 can give at most N steps in the second phase in which W-t (Q-T) at least \"increases.\" This concludes the proof for theorem 2: During one of the steps in the second phase \u00b5 (B (ht) \u2264 2 \u00b7 \"=, at which the basic algorithm can select and terminate as -good hypothesis."}, {"heading": "4 Generalization bound", "text": "The following generalization limit applies at the end of phase 1.Theorem 8 With a probability of at least 1 \u2212 \u03b4, each h-H remaining in the release space at the end of phase 1 (i.e., according to N = c \u00b7 ('+ 1) feedback rounds, has errc (h) <. Proof: Let me be the distribution over Q corresponding to picking q from \u00b5, and then select a random feature: \u00b5 (q, j) = \u00b5 (q) / c. Thus, for each h-H round we have errc (h) = \u00b5 (B-H).At the end of phase 1, W-N (Q-N) \u2265 (1 \u2212 \u2032) \u00b7 N. Thus, for each h-H, W-N (B-H) \u2012 (q, j) \u2012 B-N (q, j) \u2012 B-N (b-H) \u2012 B-H (c) \u2012 (h-N-H) \u2012 (h-H-H-H) \u2012 (h-H-H-H-H)."}, {"heading": "5 Stick-with-it algorithm", "text": "There are some practical problems with the base algorithm of Section 1.1. One problem is that at the beginning of each step q = a current hypothesis must be selected that is consistent with all previous steps, resulting in a number of selections equal to the number of steps. A second problem is that a separate procedure is required to assess whether a given hypothesis -is good to end the base algorithm with a hypothesis that proves to be -good, since a current hypothesis can be the current hypothesis for only one step. A third problem is that the analysis is obviously not applied to the case where H = | H | is unlimited, but instead slows down the VC dimension of H. We are introducing the stick-with-it algorithm, a variant of the base algorithm that addresses all these problems. We are using an integer k-1 to describe the following changes to the base algorithm: \u2022 Instead of selecting a current hypothesis at each step, selecting current one at each hypothesis."}, {"heading": "6 Lower bound on number of steps and selected hypotheses", "text": "An example showing the number of steps required can be proportional to c \u00b7 log | H | is as follows. Let Q be a fixed subset of Q, so that | Q | = \u00b7 | Q |. Let h * be the correct hypothesis. For i = 1,.. c / 2, let Hi consist of all classifiers h with the following properties: \u2022 h (q, j) = h * (q, j) if q 6 x Q or j / x {2i, 2i \u2212 1}. \u2022 For each q \u00b2 Q, either h (q, 2i \u2212 1) 6 = h * (q, 2i \u2212 1) and h (q, 2i) = h * (q, 2i), or h (q, 2i)."}, {"heading": "7 Acknowledgments", "text": "This work is a direct result of the Foundations of Machine Learning program at the Simons Institute, UC Berkeley."}], "references": [{"title": "A theory of the learnable", "author": ["L. Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "In the standard supervised learning model [1], labeled data is provided in advance, after which a consistent hypothesis is sought.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "Here we have formalized an interactive protocol that is quite natural and intuitive in terms of human-computer interface, but breaks the statistical assumptions that underlie generalization results in other settings like the PAC model [1].", "startOffset": 235, "endOffset": 238}, {"referenceID": 0, "context": "Suppose X = [0, 1] and the goal is to learn a threshold classifier:", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "H = {hv : v \u2208 [0, 1]}, hv(x) = 1(x > v).", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "Thus, for any v \u2208 [0, 1], errc(hv) = v and err(hv) = 1\u2212 (1\u2212 v), and thus errc(h) \u2248 err(h)/c if err(h) is small.", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": ", xc \u2208R [0, 1].", "startOffset": 8, "endOffset": 14}], "year": 2017, "abstractText": "We introduce a new model of interactive learning in which an expert examines the predictions of a learner and partially fixes them if they are wrong. Although this kind of feedback is not i.i.d., we show statistical generalization bounds on the quality of the learned model.", "creator": "LaTeX with hyperref package"}}}