{"id": "1705.08080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Visual Semantic Planning using Deep Successor Representations", "abstract": "A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross-task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment. The supplementary video can be accessed at the following link:", "histories": [["v1", "Tue, 23 May 2017 05:22:47 GMT  (7454kb,D)", "https://arxiv.org/abs/1705.08080v1", null], ["v2", "Tue, 15 Aug 2017 21:13:49 GMT  (7305kb,D)", "http://arxiv.org/abs/1705.08080v2", "ICCV 2017 camera ready"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["yuke zhu", "daniel gordon", "eric kolve", "dieter fox", "li fei-fei", "abhinav gupta", "roozbeh mottaghi", "ali farhadi"], "accepted": false, "id": "1705.08080"}, "pdf": {"name": "1705.08080.pdf", "metadata": {"source": "CRF", "title": "Visual Semantic Planning using Deep Successor Representations", "authors": ["Yuke Zhu", "Daniel Gordon", "Eric Kolve", "Dieter Fox", "Li Fei-Fei", "Abhinav Gupta", "Roozbeh Mottaghi", "Ali Farhadi"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "2. Related Work", "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move."}, {"heading": "3. Interactive Framework", "text": "In order to facilitate interaction with objects and the environment, we extend the THOR framework [54], which has been used to learn visual navigation tasks. Our extension includes new object states and a discrete description of the scene in a planning language [13]."}, {"heading": "3.1. Scenes", "text": "In this work, we focus on kitchen scenes as they allow a variety of tasks with objects from many categories. Our extended THOR framework consists of 10 individual kitchen scenes. Each scene contains an average of 53 different objects with which the agent can interact, and the scenes are developed using the Unity 3D game engine."}, {"heading": "3.2. Objects and Actions", "text": "We categorize the objects by their affinity [15], i.e., the plausibility of the actions that can be carried out. To perform the tasks of interest, we focus on two types of objects: 1) objects that are small objects (cups, apples, etc.) that can be picked up, and 2) containers that are large objects (table, sink, etc.) that are stationary and have a fixed capacity of objects. A subset of containers, such as refrigerators and cabinets, are containers. These containers have doors that can be opened and closed."}, {"heading": "3.3. Planning Language", "text": "The problem of generating a sequence of actions leading to the target state has been formally investigated in the area of automated planning [14]. Planning languages provide a standard method of expressing an automated planning problem that can be solved by a standard planner. We use STRIPS [13] as the planning language to describe our visual planning problem. In STRIPS, a planning problem consists of the description of an initial state, a specification of the target state (the target states) and a series of actions. In visual planning, the initial state corresponds to the initial configuration of the scenario. The specification of the target state is a Boolean function that applies to states in which the task is completed. Each action is defined by its prerequisite (conditions that must be met before the action is performed) and subsequent condition (changes caused by the action)."}, {"heading": "4. Our Approach", "text": "In paragraph 4.1, we first outline the basics of political learning. Next, we formulate the visual semantic planning problem as a problem of political learning and describe our model on the basis of the successor presentation. Later, we propose two protocols for training this model by means of imitation learning (IL) and reinforcement learning (RL). To this end, we use IL to launch our model and RL to further improve its performance."}, {"heading": "4.1. Successor Representation", "text": "We formulate the interactions of the actor with an environment as a Markov decision-making process (MDP = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = S and A are the sets of states and actions. For s \u00b2 S and a = A, the reward of a measure to be taken in a state is the likelihood of the transition from state s \u00b2 s to the next state s \u00b2 s \u00b2 S by taking action. [0, 1] is a real-value function that defines the expected immediate reward for taking action in a state. For a state action, we define the future discounted yield R = D = D = D = D = D = D = 0 z \u00b2 s, in which we take action in which the discount factor is specified that measures the importance of the immediate rewards versus future rewards. A policy: S \u2192 A defines the assignment of political actions to the optimal political learning objective consists of the states."}, {"heading": "4.2. Our Model", "text": "We formulate the problem of visual semantic planning as a political learning problem. Formally, we call a task by a Boolean function t: S \u2192 {0, 1}, in which a state s the task t iff t (s) = 1. The goal is to find an optimal policy approach \u043c, so that at a starting state s0, \u03c0 * a state action path T = {(si, ai) | i = 0. T} is generated that maximizes the sum of immediate rewards \u2211 T \u2212 1 i = 0 r (si, ai), where t (s0... T \u2212 1) = 0 and t (sT) = 1. We parameterize such a policy using the successor model (SR) from the previous section. We develop a new neural network architecture to learn about the state, vice versa and w. The network architecture is illustrated in Figure 3: The agent's observations come from a first-person RGB camera."}, {"heading": "4.3. Imitation Learning", "text": "Our SR-based policy can be learned in two ways. First, it can be learned by imitation learning (IL) under the supervision of the trajectories of an optimal planner. Second, it can be learned by trial and error through reinforcement learning (RL). In practice, we find that the large scope of action in THOR makes RL fundamentally insoluble due to the challenge of exploration. \u2212 The best model performance is generated by IL bootstrapping followed by RL fine-tuning. In the face of a task, we create a state-action path: T = {(s0, a0), {s1, a1),. (sT-1, aT \u2212 1), (sT-5) using the planner from the initial state-action pair (s0, a0) to the target state sT (no action is carried out in the final states)."}, {"heading": "4.4. Reinforcement Learning", "text": "In training our SR model using RL, we can still use the mean quadratic loss in Equation (6) to monitor learning the reward branch for \u03c6 and w. However, in the absence of expert paths, we would need an iterative way to learn the successor characteristics of the match. (2) If we paraphrase the Bellman equation in Equation (2) with SR factorization, we can achieve an equality in relation to \u03c6 (s, a) = equality. (8) Similar to DQN [35], we minimize the \"2 loss between both sides of the equation.\" (8): LSR = equality (s, a)] (8), where a) = Argmaxa (s, a) equality exists as a function of both sides of the match. (8): LSR = equality (s, a) + equality (s, a) (8)."}, {"heading": "4.5. Transfer with Successor Features", "text": "A major advantage of the succession functions is their ability to delegate tasks by taking advantage of the structure divided by the tasks. In view of the fixed representation of state action in the context of the i-th task, M\u03c6 is the totality of all possible MDPs produced by \u03c6 and all instances of reward prediction vectors. Suppose that Mn + 1 is the optimal policy of the i-th task within the framework of the set Mn + 1. Suppose that Mn + 1 is a new task. Suppose that Mn + 1 is the value function of executing the optimal policy of the task Mi to the new task Mn + 1 and Q + 1 to the new task Mn + 1 as an approximation of the Qp + 1 according to our SR model."}, {"heading": "4.6. Implementation Details", "text": "We use time costs of \u2212 0.01 to promote shorter plans and a reward for completing tasks of 10.0. We train our model with imitation acquisition for 500k iterations with a lot size of 32 and a learning rate of 1e-4. We also include the succession loss in Equation (9) in imitation acquisition, which helps to learn better succession functions. We then refine the network with 10,000 episodes of reinforcement learning."}, {"heading": "5. Experiments", "text": "We evaluate our model using the extended THOR framework on a variety of household tasks. We compare our method with traditional amplification learning methods as well as non-sequential depth models. The tasks compare the learning abilities of the different methods over different time horizons. We also demonstrate the ability of the SR network to adapt efficiently to new tasks. Finally, we show that our model can acquire an idea of object affordability by interacting with the scene."}, {"heading": "5.1. Quantitative Evaluation", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.2. Task Transfer", "text": "A major advantage of the successor model is its ability to take on new tasks while only the reward prediction vector w is retrained while the successor functions are frozen. We are investigating the sample efficiency of adapting a trained SR model to several new tasks in the same scene. We are investigating the transfer of strategies in the hard tasks as the scene dynamics of the search policy are maintained, even if the objects to be searched are different. We are evaluating the speed at which the SR model approaches a new task by redesigning the w vector from scratch compared to the training of the model. We are pursuing a strategy to search a dish in the scene and replace four new elements (lettuce, egg, container and apple) in each new task. Fig. 5 shows the episode success rates (bar chart) and the successful action rate (line chart). By fine-linking to the video, the Qinivw / oogw model will quickly complement both: https / googb) and the successful action rate."}, {"heading": "5.3. Learning Affordances", "text": "We expect our SR model to learn the pros and cons of actions through interaction, so that it develops an idea of affordability [15], i.e., what actions can be performed under certain circumstances. In the real world, such knowledge could help prevent damage to the agent and the environment caused by unexpected or invalid actions. We first evaluate the ability of each network to implicitly learn affordability when we are trained in the 5.1 tasks. In these tasks, we punish unnecessary actions with a small time penalty, but we do not explicitly tell the network which actions are successful and which fail. Fig. 6 shows that a standard learning method to amplify unnecessary actions cannot filter out unnecessary actions, especially in the face of delayed rewards. Imitated learning methods result in significantly less failed actions because they can directly assess whether each action is getting closer to the goal."}, {"heading": "6. Conclusions", "text": "In this paper, we argue that visual semantic planning is an important next task in computer vision. Our proposed solution shows promising results in predicting a sequence of actions that change the current state of the visual world to a desired target state. We have examined several different tasks with varying degrees of difficulty and show that our proposed model achieves near-optimal results based on deep follow-up presentations in the challenging THOR environment. We also show promising cross-task knowledge transfer results, a critical component of any generalizable solution. Our qualitative results show that our learned successor has knowledge of the affordability of objects, as well as prerequisites for action and after-effects. Our next steps include exploring the transfer of knowledge from THOR to real environments and exploring the possibilities of more complex tasks with a richer set of measures."}, {"heading": "Acknowledgements", "text": "This work is partially supported by ONR N00014-13-10720, ONR MURI N00014-16-1-2007, NSF IIS-1338054, NSF-1652052, NRI-1637479, NSF IIS-1652052, a Siemensgrant, the Intel Science and Technology Center for Pervasive Computing (ISTC-PC), Allen Distinguished Investigator Award, and the Allen Institute for Artificial Intelligence."}, {"heading": "A. Experiment Details", "text": "We use the Adam Optimizer from (Kingma and Ba) to learn our Successor Representation (SR) model with a learning rate of 1e-4 and a mini-batch size of 32. To amplify learning experiments, we use the discounted factor \u03b3 = 0.99 and a replay buffer size of 100,000. The exploration term is annealed from 1.0 to 0.1 during the training process. We maintain a -greedy policy (= 0.1) during the evaluation. We use soft target updates after each episode. For the simple and medium tasks, we assign an immediate reward for completing tasks, \u2212 5.0 for unsuccessful actions and \u2212 1.0 for other actions (to promote a shorter plan). For the hard task, we train our SR model to imitate a plan that mimics all recipe activities for an object in a fixed order of visitation."}, {"heading": "B. Algorithm Details", "text": "We describe the reinforcement learning procedure of the SR model in Algorithm 1. This training method closely follows the previous work on Deep Q-Learning [35] and Deep SR model [24]. Similar to these two works, replay buffers and target networks are used to stabilize the training."}, {"heading": "C. Action Space", "text": "The number of actions in a scene is determined by the variety of objects in the scene. - Each scene has 53 objects (a subset of them is interactive) and the agent is able to perform 80 actions. - Here we provide a sample scene to illustrate the interactive objects and the action spaces. - Scene 1: 16 points, 23 recipes (in 11 unique places), and 15 containers (a subset of recipes): apple, bowl, bread, butter knife, glass bottle, egg, fork, knife, Lettuce, Mug 1-3, plate, potatoes, spoons, tomato receptacles: Cabinet 1-13, refrigerator, garbage can, dishwasher 1-4, table top containers: Cabinet 1-13, refrigerator, microwave actions: 80 actions in total, including 11 actions, 15 Open actions, 15 Close actions, 14 Pick Up algorithm 1 Reinforcement for the procedure of representation."}, {"heading": "D. Tasks", "text": "We list all tasks that we evaluated in the experiments in Table 2. In summary, we evaluated tasks from three levels of difficulty, with 10 simple tasks, 8 medium tasks and 7 difficult tasks."}], "references": [{"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recent work has enabled faster learning and produced more robust visual representations [1, 32, 39] through interaction.", "startOffset": 88, "endOffset": 99}, {"referenceID": 0, "context": "For a state-action trajectory, we define the future discounted return R = \u2211\u221e i=0 \u03b3 r(si, ai), where \u03b3 \u2208 [0, 1] is called the discount factor, which trades off the importance of immediate rewards versus future rewards.", "startOffset": 104, "endOffset": 110}], "year": 2017, "abstractText": "A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.", "creator": "LaTeX with hyperref package"}}}