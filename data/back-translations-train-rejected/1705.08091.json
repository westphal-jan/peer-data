{"id": "1705.08091", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Local Monotonic Attention Mechanism for End-to-End Speech Recognition", "abstract": "Recently, sequence-to-sequence model by using encoder-decoder neural network has gained popularity for automatic speech recognition (ASR). The architecture commonly uses an attentional mechanism which allows the model to learn alignments between source speech sequence and target text sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in speech recognition task. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results demonstrate that encoder-decoder based ASR with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.", "histories": [["v1", "Tue, 23 May 2017 06:32:36 GMT  (483kb,D)", "http://arxiv.org/abs/1705.08091v1", "12 pages, 2 figures"]], "COMMENTS": "12 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andros tjandra", "sakriani sakti", "satoshi nakamura"], "accepted": false, "id": "1705.08091"}, "pdf": {"name": "1705.08091.pdf", "metadata": {"source": "CRF", "title": "Local Monotonic Attention Mechanism for End-to-End Speech Recognition", "authors": ["Andros Tjandra", "Sakriani Sakti", "Satoshi Nakamura"], "emails": ["andros.tjandra.ai6@is.naist.jp", "ssakti@is.naist.jp", "s-nakamura@is.naist.jp"], "sections": [{"heading": null, "text": "Index terms: speech recognition, neural encoder decoder network, global and local attention mechanism"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves how they want to behave."}, {"heading": "2 Related Work", "text": "Selective attention, which is a crucial attribute in human perception, allows attention to focus on specific information while filtering out a range of other information. [10] In the case of the Cocktail Party effect, humans can selectively focus their attentive hearing on a single speaker between different sources of conversation and background noise. [11] The deep learning attention mechanism has been studied for many years, but only recently have attention mechanisms found their way into the sequence of deep learning tasks proposed to solve machine translation tasks. Such mechanisms provide a model with the ability to align and translate them jointly."}, {"heading": "3 Attention-based Encoder Decoder Neural Net-", "text": "The encoder decoder model is a neural network that directly models the conditional probability p (y | x), where x = [x1,..., xS] is the source sequence with the length S and y = [y1,..., yT] is the target sequence with the length T. Figure 1 shows the overall structure of the attention-based encoder decoder model, which consists of furnace coder, decoder, and attention modules. The encoder task processes input sequence x and prints representative information that it = [he1,..., h e S] for the decoder. The attention module is an extension scheme for supporting the decoder to find relevant information on the encoder side, based on the current decoder hidden states of the bank [1, 9]. Usually, attention modules produce context information at the time t based on the encoder and the hidden states of the encoder."}, {"heading": "4 Locality and Monotonicity Properties", "text": "In the previous section, we explained the standard global attention-based encoder models (see Figure 1). However, we cannot explicitly control these mechanisms because we cannot explicitly maintain the probability generated by the current attention. (Figure 2 illustrates the general mechanisms of our proposed local monotonicity attention, and the details are described until the end of the source sequence.) In this section, we discuss and explain how we modulate the locality and monotonicity properties on the attention. Figure 2 illustrates the general mechanisms of our proposed local monotonicity attention. (Figure 2) Assume we have source sequence with the length S, which is illustrated by the stack of the Bi-LSTM (see Figure 1) of the next central position."}, {"heading": "5 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Speech Data", "text": "For our experiments, we used the data set TIMIT 1 [12] with the same setup for training, development and testing as defined in the Kaldi s5 recipe [13]. The training set contains 3696 sets of 462 speakers. We also used another set of 50 speakers for the development set and the test set contains 192 statements from 24 speakers each. For each experiment, we used a 40-dimensional delta and acceleration database (a total of 120-dimensional feature vector) extracted from the Kaldi toolkit. Input functions were normalized by subtracting the mean and divided by the standard deviation from the training set. For our decoder goal, we remapped the original target set of 61 into 39 phoneme classes plus the end of the sequence mark (eos). 1https: / / catalog.ldc.upenn.edu / ldc93s1"}, {"heading": "5.2 Model Architectures", "text": "On the encoder side, we projected our input functions with a linear plane with 512 hidden units followed by a tanh activation function. We used three bi-directional LSTMs (Bi-LSTM) for our encoder with 256 hidden units for each LSTM (a total of 512 hidden units for Bi-LSTM). To reduce computing time, we used a 64-dimensional embedding matrix to transform the input phones into a continuous vector, followed by two unidirectional LSTMs with 512 hidden units. For each local monotonic model, we used a MLP with 128 hidden units to generate the embedding matrix."}, {"heading": "6 Result and Discussion", "text": "Table 1 summarizes our experiments on our proposed local attention models and compares them with the base model based on several possible scenarios."}, {"heading": "6.1 Constrained vs Unconstrained Position Prediction", "text": "Taking into account the use of restricted and unrestricted position forecast \u2206 pt, our results show that the model with the unrestricted position forecast model (Exp) delivers better results than one based on the model of restricted position forecast (Sigmoid) based on both MLP and bilinear scorers. We conclude that it is more advantageous to use the formulation of the unrestricted position forecast as it provides better performance and we do not have to deal with the additional hyperparameter Cmax."}, {"heading": "6.2 Alignment Scorer vs Non-Scorer", "text": "Next, we will examine the importance of the goalscorer module by comparing the results between a model with and without a goalscorer. Our results show that by relying only on the Gaussian alignment aNt and setting a S t = 1, the performance of our model was worse than that which used both the goalscorer and the Gaussian alignment. This may be because the goalscorer modules are able to correct the details of the Gaussian alignment based on the relevance of the encoder states in the current decoder states. Consequently, we conclude that alignment with the goalscorer is essential for our proposed models."}, {"heading": "6.3 Overall comparison to the baseline", "text": "Overall, our proposed encoder decoder model with local monotonous attention significantly improved performance and reduced computational complexity compared to a model that used standard global attention mechanisms (we cannot directly compare it to [3], as it is pre-trained with HMM state adjustment).The best performance our proposed model achieved with full position prediction and bilinear scorers resulted in a 12.2% reduction in the relative error rate compared to our baseline."}, {"heading": "7 Conclusion", "text": "This work demonstrated a novel attention mechanism for encoder decoding models that ensures monotonicity and locality characteristics. We explored several ways to control these characteristics, including monotonicity-based position prediction and local alignment generation. Results show that our proposed encoder decoder model with local monotonous attention significantly improved performance and reduced computing complexity more than one that used the standard global attention architecture. In the future, we will further explore our proposed approach in a task for speech recognition with large vocabulary."}, {"heading": "8 Acknowledgements", "text": "Part of this work was supported by JSPS KAKENHI Grant Numbers JP17H00747 and JP17K00237."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence-to-Sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent NN: First results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.1602, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4960\u20134964.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, 2015, pp. 2048\u20132057.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3128\u20133137.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "Syntax, Semantics and Structure in Statistical Translation, p. 103, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "arXiv preprint arXiv:1508.04025, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The dynamic representation of scenes", "author": ["R.A. Rensink"], "venue": "Visual cognition, vol. 7, no. 1-3, pp. 17\u201342, 2000. 11", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Some experiments on the recognition of speech, with one and with two ears", "author": ["E.C. Cherry"], "venue": "The Journal of the acoustical society of America, vol. 25, no. 5, pp. 975\u2013979, 1953.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1953}, {"title": "Darpa TIMIT acoustic-phonetic continous speech corpus cd-rom. NIST speech disc 1-1.1", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett"], "venue": "NASA STI/Recon technical report n, vol. 93, 1993.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised sequence labelling", "author": ["A. Graves"], "venue": "Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012, pp. 5\u201313.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Endto-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4945\u20134949.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Chainer: a nextgeneration open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015. [Online]. Available: http://learningsys.org/papers/ LearningSys 2015 paper 33.pdf", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularizing neural networks by penalizing confident output distributions", "author": ["G. Pereyra", "G. Tucker", "J. Chorowski", "L. Kaiser", "G. Hinton"], "venue": "arXiv preprint arXiv:1701.06548, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning online alignments with continuous rewards policy gradient", "author": ["Y. Luo", "C.-C. Chiu", "N. Jaitly", "I. Sutskever"], "venue": "arXiv preprint arXiv:1608.01281, 2016. 12", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).", "startOffset": 24, "endOffset": 30}, {"referenceID": 1, "context": ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).", "startOffset": 24, "endOffset": 30}, {"referenceID": 2, "context": ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).", "startOffset": 56, "endOffset": 62}, {"referenceID": 3, "context": ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "The earlier version of an encoderdecoder model is built with only two different components [2, 6]: (1) an encoder that processes the source sequence and encodes them into a fixed-length vector; and (2) a decoder that produces the target sequence based on information from fixed-length vector given by encoder.", "startOffset": 91, "endOffset": 97}, {"referenceID": 5, "context": "The earlier version of an encoderdecoder model is built with only two different components [2, 6]: (1) an encoder that processes the source sequence and encodes them into a fixed-length vector; and (2) a decoder that produces the target sequence based on information from fixed-length vector given by encoder.", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "This architecture has been applied in many applications such as machine translation [2, 6], image captioning [7], and so on.", "startOffset": 84, "endOffset": 90}, {"referenceID": 5, "context": "This architecture has been applied in many applications such as machine translation [2, 6], image captioning [7], and so on.", "startOffset": 84, "endOffset": 90}, {"referenceID": 6, "context": "This architecture has been applied in many applications such as machine translation [2, 6], image captioning [7], and so on.", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "[8] demonstrated a decrease in the performance of the encoder-decoder model associated with an increase in the length of the input sentence sequence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] introduced attention mechanism to address these issues.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Most attention-based encoder-decoder model used today has a \u201cglobal\u201d property [1, 9].", "startOffset": 78, "endOffset": 84}, {"referenceID": 8, "context": "Most attention-based encoder-decoder model used today has a \u201cglobal\u201d property [1, 9].", "startOffset": 78, "endOffset": 84}, {"referenceID": 9, "context": "The biological structure of the eye and the eye movement mechanism is one part of visual selective attention that provides the ability to focus attention selectively on parts of the visual space to acquire information when and where it is needed [10].", "startOffset": 246, "endOffset": 250}, {"referenceID": 10, "context": "In the case of the cocktail party effect, humans can selectively focus their attentive hearing on a single speaker among various conversation and background noise sources [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "Such mechanisms provide a model with the ability to jointly align and translate [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].", "startOffset": 120, "endOffset": 126}, {"referenceID": 8, "context": "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].", "startOffset": 120, "endOffset": 126}, {"referenceID": 2, "context": "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].", "startOffset": 174, "endOffset": 180}, {"referenceID": 3, "context": "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].", "startOffset": 174, "endOffset": 180}, {"referenceID": 8, "context": "[9] which provided the capability to only focus small subset of the encoder sides.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] also proposed a soft constraint to encourage monotonicity by invoking a penalty based on the current alignment and previous alignments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The attention module is an extension scheme for assisting the decoder to find relevant information on the encoder side based on the current decoder hidden states [1, 9].", "startOffset": 162, "endOffset": 168}, {"referenceID": 8, "context": "The attention module is an extension scheme for assisting the decoder to find relevant information on the encoder side based on the current decoder hidden states [1, 9].", "startOffset": 162, "endOffset": 168}, {"referenceID": 8, "context": "[9] to generate alignment at only within [pt \u2212 2\u03c3, pt + 2\u03c3]: at (s) = Align(h e s, h d t ),\u2200s \u2208 [pt \u2212 2\u03c3, pt + 2\u03c3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "1 Speech Data We conducted our experiments on the TIMIT 1 [12] dataset with the same setup for training, development, and test sets as defined in the Kaldi s5 recipe [13].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "1 Speech Data We conducted our experiments on the TIMIT 1 [12] dataset with the same setup for training, development, and test sets as defined in the Kaldi s5 recipe [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "To reduce the computational time, we used hierarchical subsampling [14, 15], applied it to the top two Bi-LSTM layers, and reduced their length by a factor of 4.", "startOffset": 67, "endOffset": 75}, {"referenceID": 14, "context": "To reduce the computational time, we used hierarchical subsampling [14, 15], applied it to the top two Bi-LSTM layers, and reduced their length by a factor of 4.", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "We used an Adam [16] optimizer with a learning rate of 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "All of our models were implemented on the Chainer framework [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "Model Test PER (%) Global Attention Model (Baseline) Att Enc-Dec (pretrained with HMM align)[3] 18.", "startOffset": 92, "endOffset": 95}, {"referenceID": 17, "context": "6 Att Enc-Dec [18] 23.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "2 Att Enc-Dec [19] 24.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "Overall, our proposed encoder-decoder model with local monotonic attention significantly improved the performance and reduced the computational complexity in comparison with one that used standard global attention mechanism (we cannot compare directly with [3] since its pretrained with HMM state alignment).", "startOffset": 257, "endOffset": 260}], "year": 2017, "abstractText": "Recently, sequence-to-sequence model by using encoder-decoder neural network has gained popularity for automatic speech recognition (ASR). The architecture commonly uses an attentional mechanism which allows the model to learn alignments between source speech sequence and target text sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in speech recognition task. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results demonstrate that encoder-decoder based ASR with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.", "creator": "LaTeX with hyperref package"}}}