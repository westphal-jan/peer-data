{"id": "1705.08110", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Semi-Bandits with Knapsacks", "abstract": "This paper unifies two lines of work on multi-armed bandits, Bandits with Knapsacks (BwK) and semi-bandits. The former concerns scenarios with limited \"resources\" consumed by the algorithm, e.g., limited inventory in a dynamic pricing problem. The latter has a huge number of actions, but there is combinatorial structure and additional feedback which makes the problem tractable. Both lines of work has received considerable recent attention, and are supported by numerous application examples. We define a common generalization, and design a general algorithm for this model. Our regret rates are comparable with those for BwK and semi-bandits in general, and essentially optimal for important special cases.", "histories": [["v1", "Tue, 23 May 2017 07:46:32 GMT  (46kb)", "http://arxiv.org/abs/1705.08110v1", null], ["v2", "Mon, 16 Oct 2017 01:53:46 GMT  (1525kb,D)", "http://arxiv.org/abs/1705.08110v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["karthik abinav sankararaman", "aleksandrs slivkins"], "accepted": false, "id": "1705.08110"}, "pdf": {"name": "1705.08110.pdf", "metadata": {"source": "CRF", "title": "Semi-Bandits with Knapsacks", "authors": ["Karthik Abinav Sankararaman"], "emails": ["kabinav@cs.umd.edu", "slivkins@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.08 110v 1 [cs.L G] 23 May 2"}, {"heading": "1 Introduction", "text": "It is an elegant model for investigating the problem between acquiring and using information that relates to the manner of work that has taken place over the last decade. (Work on MAB is progressing in several directions, such as: what would have happened if another action had been chosen in this round.) MAB problems have been continuously investigated since 1930, with a huge increase in interest in the last ten years. Work on MAB is progressing in several directions, such as: what auxiliary information, if any, will be uncovered before or after it has to be made a decision; the \"processes\" are the rewards that come out of the world, they have some known structures that can be used, there are global restrictions on the algorithms, etc. Many of these directions led to prominent lines of two working papers."}, {"heading": "2 Our model and preliminaries", "text": "Our model, called Semi-Bandits with Knapsacks (SemiBwK), is a generalization of multi-armed bandits (henceforth, MAB) with i.e. rewards. As such, in each turn t = 1,.., an algorithm selects an action starting from a fixed set of actions F, and receives a reward for that action, which is drawn independently of a fixed distribution, which only depends on the chosen action. The number of rounds T, a.k.a. the time horizon is known to the algorithm. There are d resources consumed by the algorithm. The algorithm starts with a budgetBj distribution, which only depends on the chosen action. All budgets are known to the algorithm. If in round t the action S is selected, the result of this round is not only the reward (S), but the consumption Ct (S, j) of each resource y."}, {"heading": "2.1 Probability and concentration.", "text": "Let's call X = (X1, X2,., Xm) a collection of random variables that include values in [0, 1]. Let's call X: = 1 x x x = 1 Xi is the average, and \u00b5: = E [X] is its expectation. Negative correlation. Family X is negatively correlated iffE [2], (2,2) Note: Independent random variables are negatively correlated. In addition, a family remains negatively correlated with non-negative scaling. Furthermore: Claim 2.1. If family X is negatively correlated, then families (1 + Xi \u2212 E [Xi] are 2: i [m] and (1 \u2212 Xi]."}, {"heading": "3 Main algorithm", "text": "The algorithm is based on an arbitraryRRS for the action framework F + 1. It is parameterized by the manner described in (2.7), and resolves the following linear program: maximize \u00b5 (j) x x \u2264 B (1 \u2212 1) T, j = 1, d x x x (0, 1) P x x (0, 1) P x (0, 2). The linear program defines a linear relativization of the original problem, which is \"optimistic\" in the sense that it uses upper confidence for rewards and lower confidence for consumption."}, {"heading": "3.1 Proof overview of Theorem 3.1", "text": "First, we argue that LPALG provides a good yardstick that we can use instead of OPT. Fix round t and let OPTALG, t denote the optimal value for LPALG in this round. Then: Lemma 3.4. OPTALG, t \u2265 1T (1 \u2212 \u0445) OPT with a probability of at least 1 \u2212 \u03b4.We prove this by constructing a series of LPs starting with a general linear relaxation for BwK and ending with LPALG, and showing that along the series the optimal value does not decrease w.h.p.Further, we define a series of events that most likely occur as clean events. \u2212 Our first \"clean event\" concerns total rewards and compares our algorithm with the LPALG qualities:."}, {"heading": "4 Applications and special cases", "text": "We assume that these examples can be applied exponentially beyond a broader range of parameters. However, compared to what can be deduced from some special cases, our results may have a much better dependence on the parameters, which require a much better per-round runtime and can be applied to a wider range of parameters. However, we leave open the possibility that regrets can be improved for some special cases. A more detailed discussion, including the framing examples in the SemiBwK framework and comparisons to previous work, can be found in the supplement, see Section 7. Dynamic pricing application is as follows: for simplicity, say B units each. Following Besbes and Zeevi [14], we also allow constellations that go beyond products, such as limited stocks of a \"part\" that goes into multiple productions."}, {"heading": "5 Proof of the main result (Theorem 3.1)", "text": "In this section, a detailed and self-contained proof for the main result is presented: Theorem 3.1. Let us first remember the statement of the theorem (Theorem 3.1). Let us consider the SemiBwK problem with a linearizable action set F, which permits a negatively correlated RRRS. Then the algorithm reaches SemiBwK-RRS with this RRS Regret \u2264 O (log (ndT / \u03b4)) \u221a n (OPT / \u221a B + \u221a T + OPT) (5.1) with a probability of at least 1 \u2212 \u03b4, for each probability. Here, T is the time horizon, n is the number of atoms and B is the budget. The result lasts as long as B > \u03b1 = log (ndT / \u03b4)."}, {"heading": "5.1 Linear programs", "text": "We argue that LPALG provides a good benchmark that we can use instead of OPT. (Fix round t) and let OPTALG, t denote the optimal value for LPALG in this round. Then: Lemma (Lemma 3.4). We show that along the series the optimal value does not decrease with high probability. The first LP that has been adjusted by [11] has a decision variable for each action and applies generically to each BwK problem.maximize S (S) x (S) is subject to the high probability. (S) x x (S) x."}, {"heading": "5.2 Concentration bounds", "text": "Our analysis is based on several concentration limits based on those in Section 2.1.First, we extend theorem 2.2 to a random process that develops over time, and only assumes that the property (2.3) has random variables within each round that depend on the history. Let's leave Zt \u2212 1 and have expectation 12 given Zt \u2212 1, for each round. Let's leave Z = 1nT random variables that assume values in [0, 1] random variables, a: a) are negatively correlated with Zt \u2212 1 and have expectation 12 given Zt \u2212 1, for each round. Let's leave Z = 1nT random variables that are an average constant in [T]."}, {"heading": "5.3 Analysis of the \u201cclean event\"", "text": "Let us list several events that will henceforth be called clean events, and prove that they are highly probable, and then the rest of the analysis can take place depending on the intersection of these events. [3] The clean events are similar to those in SemiBwK, but are somewhat \"stronger,\" essentially because our algorithm has access to atomic feedback and our analysis can take advantage of the negative correlation properties of the RRRS. In the following, it is convenient to consider a version of SemiBwK in which the algorithm does not stop, so that we can argue about what happens when our algorithm runs over the full T rounds. Then, we show that our algorithm actually runs for the full T rounds."}, {"heading": "5.3.1 \u201cClean event\" for rewards", "text": "For the abbreviation, for each round, let us leave \u00b5t = (a): a): a): b): c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c)) c))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))"}, {"heading": "5.3.2 \u201cClean event\" for resource consumption", "text": "We define a similar \"clean event\" for the consumption of each resource j. The proof is similar and will be postponed to a later date. By slight misuse of notation, the consumption of resource j. for each round j. \u2212 j. \u2212 j. for each round j. \u2212 j. for each round j. \u2212 j. for each round j. for each round j. for each round j. \u2212 j. for each round j. \u2212 j. for each round j. \u2212 j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j. for each round j."}, {"heading": "5.4 Putting it all together", "text": "Similar to [3], we will handle the hard budget constraint by selecting a reasonable value of \"B.\" We will then combine the above lemmas on \"rewards\" clean event to compare the algorithm's reward with the optimal value of \"LP\" to obtain the regret tied in Equation 3.1. Furthermore, we will use the lemmas on \"consumption\" clean event to argue that the algorithm will match the resource budget before round T. Formally, consider the following. Let's remember that starting with lemmas 3.4 we have OPTALG, \u2265 1T (1 \u2212) OPT. Let's define the performance of the algorithm as ALG = \u2211 t \u2264 T rt. Starting with lemmas 5.6, that we have probability of at least 1 \u2212 ndT e \u2212 (\u03b1) ALG \u2265 (1 \u2212) OPT \u2212 (\u03b1) ALG (1 \u2212) OPT \u2212 nT before assuming that we do not have the probability, and the possibility, that we do not have the probability, that we do not have (T \u2212 nT)."}, {"heading": "6 Proof sketch for an extension (Theorem 3.3)", "text": "Here we will give a sketch to prove the regret in Equation 3.2. Note that the RRS does not guarantee any form of concentration between the atoms. Therefore, we analyze it atom by atom and proceed from a union that is bound across all atoms. We will probably get 1 \u2212 n2T e \u2212 (\u03b1) for the reward event. If we transfer this to the final calculation of regret, we will get OPT \u2212 ALG \u2264 T \u2212 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 rt (a) + O (\u03b1n2). If we transfer these arguments to the final calculation of regret, we will get OPT \u2212 ALG \u00b2 O (OPT \u00b2 n \u00b2 n \u00b2 a = 1 \u00b2 n \u00b2 n \u00b2 n \u00b2). Now we want to maximize the resources na = 1 \u00b2 rt (a) on the RHS. Let us not transfer the resources in the form T = 1 \u00b2 n \u00b2 n \u00b2."}, {"heading": "7 A detailed discussion of examples", "text": "This section deals with the discussion of applications and specific cases in Section 4. For the simplicity of presentation, this section can only be read independently of the brief discussion in Section 4. [Note] For quotes on the previous work on these particular cases, see the corresponding subsection of Introduction.Dynamic pricing. Consider the dynamic pricing problem in a general formulation in which the algorithm has products for sale with limited supply of each individual product. [Note] For simplicity, we say that there may be limitations that extend across products, such as limited inventory of a \"part\" that goes into multiple products (e.g., the same type of nut that goes into many furniture products). Thus, we have consumed at most one unit of each product and assume that the inventory of that part will arrive at least B.In each round, an agent will choose a vector of prices (pt, 1, pt, 2, 2)."}, {"heading": "A Probability and concentration: some proofs", "text": "Our analysis was based on some facts about probability and concentration that we have given in the preliminaries (section 2.1) (section 2.1). Although we believe that these results are known, we provide the following evidence for completeness. In what follows, we allow Xi Xi (Xi) = (X1, X2,.., Xm) a collection of random variables, the values in [0, 1], and allow X (1): 1m \u00b2 m = 1 Xi (Xi). (X1, X2,.) Satisfactory (2,3), i.e., E [2). (S)."}, {"heading": "B Combinatorial constraints: formal discussion", "text": "It is not as if we are able to define a number of important special cases that we have listed in Appendices B.1 and B.2, respectively, the extension in Theorem 3.3 considers linearizable action sets. We provide an example in Appendix B.3, namely: We define constraints specified by independent propositions and show that they are linearizable.B.1 Matroid conintsTo be consistent with the literature on the ground.Family F of the propositions of E is a graph, and they are linearizable.B.1 Matroid conintsTo be consistent with the literature on the ground.B.1 Matroid conintsTo be consistent with the literature on the ground."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper unifies two lines of work on multi-armed bandits, Bandits with Knap-<lb>sacks (BwK) and semi-bandits. The former concerns scenarios with limited \u201cre-<lb>sources\" consumed by the algorithm, e.g., limited inventory in a dynamic pricing<lb>problem. The latter has a huge number of actions, but there is combinatorial struc-<lb>ture and additional feedback which makes the problem tractable. Both lines of<lb>work has received considerable recent attention, and are supported by numerous<lb>application examples. We define a common generalization, and design a general<lb>algorithm for this model. Our regret rates are comparable with those for BwK and<lb>semi-bandits in general, and essentially optimal for important special cases.", "creator": "LaTeX with hyperref package"}}}