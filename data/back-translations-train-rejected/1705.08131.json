{"id": "1705.08131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Black-Box Attacks against RNN based Malware Detection Algorithms", "abstract": "Recent researches have shown that machine learning based malware detection algorithms are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers have begun to use recurrent neural networks (RNN) to detect malware based on sequential API features. This paper proposes a novel algorithm to generate sequential adversarial examples, which are used to attack a RNN based malware detection system. It is usually hard for malicious attackers to know the exact structures and weights of the victim RNN. A substitute RNN is trained to approximate the victim RNN. Then we propose a generative RNN to output sequential adversarial examples from the original sequential malware inputs. Experimental results showed that RNN based malware detection algorithms fail to detect most of the generated malicious adversarial examples, which means the proposed model is able to effectively bypass the detection algorithms.", "histories": [["v1", "Tue, 23 May 2017 08:51:37 GMT  (299kb,D)", "http://arxiv.org/abs/1705.08131v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["weiwei hu", "ying tan"], "accepted": false, "id": "1705.08131"}, "pdf": {"name": "1705.08131.pdf", "metadata": {"source": "CRF", "title": "Black-Box Attacks against RNN based Malware Detection Algorithms", "authors": ["Weiwei Hu", "Ying Tan"], "emails": ["ytan}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it has been shown that it is a purely reactionary project, which is an experiment, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project. \""}, {"heading": "2 Adversarial Examples", "text": "In fact, most people who are able to survive themselves have to survive themselves because they are not able to survive themselves, \"he said in an interview with Welt am Sonntag. He added,\" I don't think we will be able to survive ourselves. \"He added,\" I don't think we will be able to survive ourselves. \"He added,\" I don't think we will be able to survive ourselves. \"He added,\" I don't think we will be able to survive ourselves if we are able to survive ourselves. \"He added,\" I don't think we will be able to survive ourselves if we are able to survive ourselves. \""}, {"heading": "3 RNN for Malware Detection", "text": "In this section we will show how to use RNN to detect malware. Malware detection is regarded as a sequential classification problem [25, 30, 14]. RNN is used to classify whether an API sequence emanates from a harmless program or malware. We will also introduce some variants of RNN in this section. Malware detection model is usually a black box for malware authors, and they must take into account the potential variants when developing attack algorithms. Each API is represented as a one-hot vector. Assuming that there are total M-APIs, these APIs are numbered from 0 to M \u2212 1. The feature vector x of an API is an M-dimensional binary vector. If the number of the API is i, the i-th dimension of x, and other dimensions are all zero. A sequence x of an API is represented as an AxT, where a T-length is..."}, {"heading": "4 Attacking RNN based Malware Detection Algorithms", "text": "Papernot et al. [23] migrated the opposing sample generation algorithms for forward-looking neural networks to attack RNN by unrolling RNN over time and viewing it as a special type of forward-looking neural network. However, such a model can only replace existing elements in the sequence with other elements, since the disturbances are not really sequential, and this algorithm cannot insert disrespectful APIs into the original sequences.The main contribution of this paper is that we have proposed a generative RNN-based approach to generating sequentially contradictory examples, capable of effectively minimizing the weak points in the sequential patterns. [28] The proposed algorithm consists of a generative RNN and a replacement RNN, as shown in Figure 1 and Figure 2. The generative model is based on a modified version of the sequence to the sequence model [28] that takes the sequence as a sequence input and PI."}, {"heading": "4.1 The Generative RNN", "text": "The generative RNN generates a small piece of API sequence after each API and tries to insert the sequence piece after the APPI. For the input sequence x1, x2,..., xT, the hidden states of the recursive plane are h1, h2,..., hT, a small sequence of Gumbel Softmax output gt1, gt2,..., gtL with the length L is based on ht, where L is a hyper-parameter.Sequence decoder [5] is used to generate the small sequence.The decoder RNN uses the formula hD2 = Dec (x D), h D to update hidden states, where x D is the hidden states."}, {"heading": "4.2 The Substitute RNN", "text": "Malware authors usually do not know the detailed structure of the victim RNN. They do not know whether the victim uses RNN bidirectional connection, average pooling and the attention mechanism. Therefore, the victim RNN weights are not available to malware authors either. In order to adapt such victim RNN with unknown structure and weights, a neural network with strong representation capability should be used. Therefore, the replacement RNN uses bidirectional RNN with attention mechanism, as it is able to learn complex sequential patterns. Bidirectional connection contains both the forward connection and the backward connection, and therefore it has the ability to represent the unidirectional connection. The attention mechanism is able to focus on different positions of the sequence. Therefore, RNN with attention mechanism can represent cases without attention mechanism, such as average pooling and the use of the last state to replace the victim RNN sequence to replace the NN."}, {"heading": "4.3 Training", "text": "The training objective of the generative RNN is to minimize the predicted probability of damage pS on SGumbel. We also add a regularization term to limit the number of APIs inserted in the opposing sequence by maximizing the probability of the zero API. The final loss function of the generative RNN is defined in formula 7.LG = log (pS) \u2212 \u03b3Et = 1 \u0445 T, \u03c4 = 1 \u0445 L\u03c0Mt\u03c4, (7), where \u03b3 is the regulatory coefficient and M is the index of the zero API. The training process of the proposed model is summarized in algorithm 1.Algorithm 1 Training of the proposed model 1: while the final state is not met 2: Sample of a minibatch of data containing malware and benign programs. 3: Calculate the results of the generative RNN for malware. 4: Get the outputs of the replacement model RNN on malware max and the rubber outputs on the malware."}, {"heading": "5 Experiments", "text": "Adam [13] was used to train all models. LSTM unit was used for all RNNs presented in the experiments due to their good performance in processing long sequences [10, 7]."}, {"heading": "5.1 Dataset", "text": "We combed through 180 programs with corresponding behavioral reports from a malware analysis website (https: / / malwr.com /). On the website, users can upload their programs and the website executes the programs on virtual machines. Afterwards, the API sequences called by the uploaded programs are published on the website. 70% of the programs searched are malware. In real applications, the opposing example generation model and the victim RNN should be trained by malware authors or virus vendors. The data sets they collect cannot be identical. Therefore, we use different training sets for the two models. Then, we selected 30% of our data set as the training set of the opposing example generation model (i.e. the generative RNN and the replacement RNN) and chose 10% as the validation set of the opposing example generation model."}, {"heading": "5.2 The Victim RNNs", "text": "The suffixes \"Average\" and \"Attention\" in the last four lines indicate the use of average pooling and attention mechanism to represent the sequence. We first adjusted the hyperparameters of the BiLSTM Attention to the validation set, the final learning rate was set at 0.001, the number of recurring hidden layers and the number of hidden layers of attention were both set at one, and the layer sizes were both set at 128. We applied the resulting hyperparameters directly to other victim models, we tried to set the hyperdirectional parameters of the LNM separately to the hyperdirectional parameters of the LNM, but performance did not improve significantly compared to the use of BiLSTM Attentional information."}, {"heading": "5.3 Experimental Results of the Proposed Model", "text": "The hyperparameters of the generative RNN and the substitute RNN were adjusted separately for each black box victim RNN. Learning rate and regulation coefficient were chosen by line search along the direction 0.01, 0.001, et al.. Gumbel Softmax temperature was sought in the range [1, 100]. In fact, the decoder length L in the generative RNN is also a kind of regulation coefficient. A large L strongly represents the generative RNN, but the entire opposing sequences become too long, and the generative RNN size may exceed the capacity of the GPU memory. Therefore, we rely on the experimental results in Table 2.After conciliatory attacks, all victims RNNN are not recognized to detect most of the malware."}, {"heading": "6 Conclusions and Future Works", "text": "We use Gumbel-Softmax to approximate the discrete APIs generated that are able to propagate the gradients from replacement RNN to generative RNN. The proposed model has successfully enabled most of the generated adversarial examples to bypass multiple black box victim RNs with different structures. Previous research on adversarial examples has focused mainly on images that have a fixed input dimension. We have shown that sequential machine models are not safe even under adversarial attacks, and the problem of adversarial examples is getting more serious when it comes to detecting malware. Robust defense models are needed to deal with adversarial attacks."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y-Lan Boureau", "Jean Ponce", "Yann LeCun"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Defensive distillation is not robust to adversarial examples", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Evaluation of defensive methods for dnns against multiple adversarial evasion models, 2016", "author": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["Emil Julius Gumbel", "Julius Lieblein"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1954}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Generating adversarial malware examples for black-box attacks based on gan", "author": ["Weiwei Hu", "Ying Tan"], "venue": "arXiv preprint arXiv:1702.05983,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "venue": "arXiv preprint arXiv:1611.01144,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deep learning for classification of malware system call sequences", "author": ["Bojan Kolosnjaji", "Apostolis Zarras", "George Webster", "Claudia Eckert"], "venue": "In Australasian Joint Conference on Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Learning to detect and classify malicious executables in the wild", "author": ["J Zico Kolter", "Marcus A Maloof"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Sequential short-text classification with recurrent and convolutional neural networks", "author": ["Ji Young Lee", "Franck Dernoncourt"], "venue": "arXiv preprint arXiv:1603.03827,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "A general retraining framework for scalable adversarial classification", "author": ["Bo Li", "Yevgeniy Vorobeychik", "Xinyun Chen"], "venue": "arXiv preprint arXiv:1604.02606,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song"], "venue": "arXiv preprint arXiv:1611.02770,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Simple black-box adversarial perturbations for deep networks", "author": ["Nina Narodytska", "Shiva Prasad Kasiviswanathan"], "venue": "arXiv preprint arXiv:1612.06299,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Security and Privacy (EuroS&P),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Crafting adversarial input sequences for recurrent neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ananthram Swami", "Richard Harang"], "venue": "In Military Communications Conference, MILCOM 2016-2016", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Security and Privacy (SP),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Malware classification with recurrent networks", "author": ["Razvan Pascanu", "Jack W Stokes", "Hermineh Sanossian", "Mady Marinescu", "Anil Thomas"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Data mining methods for detection of new malicious executables", "author": ["Matthew G Schultz", "Eleazar Eskin", "Erez Zadok", "Salvatore J Stolfo"], "venue": "In Security and Privacy,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Malware detection with deep neural network using process behavior", "author": ["Shun Tobiyama", "Yukiko Yamaguchi", "Hajime Shimada", "Tomonori Ikuse", "Takeshi Yagi"], "venue": "In Computer Software and Applications Conference (COMPSAC),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Adversarial feature selection against evasion attacks", "author": ["Fei Zhang", "Patrick PK Chan", "Battista Biggio", "Daniel S Yeung", "Fabio Roli"], "venue": "IEEE transactions on cybernetics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "However, recent researches on adversarial examples show that many machine learning algorithms are not robust at all when someone want to crack them on purpose [29, 6].", "startOffset": 159, "endOffset": 166}, {"referenceID": 5, "context": "However, recent researches on adversarial examples show that many machine learning algorithms are not robust at all when someone want to crack them on purpose [29, 6].", "startOffset": 159, "endOffset": 166}, {"referenceID": 14, "context": "Existing machine learning based malware detection algorithms mainly represent programs as feature vectors with fixed dimension and classify them between benign programs and malware [15].", "startOffset": 181, "endOffset": 185}, {"referenceID": 25, "context": "application programming interfaces) in a program [26].", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "[8] and Hu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] have shown that fixed dimensional feature based malware detection algorithms are very vulnerable under the attack of adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Recently, as recurrent neural networks (RNN) become popular, some researchers have tried to use RNN for malware detection and classification [25, 30, 14].", "startOffset": 141, "endOffset": 153}, {"referenceID": 29, "context": "Recently, as recurrent neural networks (RNN) become popular, some researchers have tried to use RNN for malware detection and classification [25, 30, 14].", "startOffset": 141, "endOffset": 153}, {"referenceID": 13, "context": "Recently, as recurrent neural networks (RNN) become popular, some researchers have tried to use RNN for malware detection and classification [25, 30, 14].", "startOffset": 141, "endOffset": 153}, {"referenceID": 11, "context": "Gumbel-Softmax [12] is used to smooth the API symbols and deliver gradient information between the generative RNN and the substitute RNN.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "used a box-constrained L-BFGS to search for an appropriate perturbation which can make a neural network misclassify an image [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "proposed the \u201cfast gradient sign method\u201d where added perturbations are determined by the gradients of the cost function with respect to inputs [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] to add some adversarial perturbations to Android malware on about 545 thousand binary features [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[22] to add some adversarial perturbations to Android malware on about 545 thousand binary features [8].", "startOffset": 100, "endOffset": 103}, {"referenceID": 22, "context": "migrated these algorithms to attack RNN [23].", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "first got the outputs from the victim neural network on their training data, and then trained a substitute neural network to fit the victim neural network [21].", "startOffset": 155, "endOffset": 159}, {"referenceID": 19, "context": "They also showed that other kinds of machine learning models such as decision trees can also be attacked by using the substitute network to fit them [20].", "startOffset": 149, "endOffset": 153}, {"referenceID": 18, "context": "adopted a greedy local search to find a small set of pixels by observing the probability outputs of the victim network after applying perturbations [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 17, "context": "used an ensemble-based algorithm to generate adversarial examples and the adversarial examples are able to attack other black-box models due to the transferability of adversarial examples [18] .", "startOffset": 188, "endOffset": 192}, {"referenceID": 31, "context": "Several defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 23, "context": "Several defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Several defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17].", "startOffset": 153, "endOffset": 157}, {"referenceID": 7, "context": "However, it is found that the effectiveness of these defensive algorithms is limited, especially under repeated attacks [8, 4, 3].", "startOffset": 120, "endOffset": 129}, {"referenceID": 3, "context": "However, it is found that the effectiveness of these defensive algorithms is limited, especially under repeated attacks [8, 4, 3].", "startOffset": 120, "endOffset": 129}, {"referenceID": 2, "context": "However, it is found that the effectiveness of these defensive algorithms is limited, especially under repeated attacks [8, 4, 3].", "startOffset": 120, "endOffset": 129}, {"referenceID": 24, "context": "Malware detection is regarded as a sequential classification problem [25, 30, 14].", "startOffset": 69, "endOffset": 81}, {"referenceID": 29, "context": "Malware detection is regarded as a sequential classification problem [25, 30, 14].", "startOffset": 69, "endOffset": 81}, {"referenceID": 13, "context": "Malware detection is regarded as a sequential classification problem [25, 30, 14].", "startOffset": 69, "endOffset": 81}, {"referenceID": 1, "context": "The first variant of the RNN model introduced here is average pooling [2], which uses the average states across h1 to hT as the representation of the sequence, instead of the last state hT .", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Attention mechanism [1] is another variant, which uses weighted average of the hidden states to represent the sequence.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "It has shown to be very useful in machine translation [1] and image caption [31].", "startOffset": 54, "endOffset": 57}, {"referenceID": 30, "context": "It has shown to be very useful in machine translation [1] and image caption [31].", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "Bidirectional RNN tries to learn patterns from both directions [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "[23] migrated the adversarial example generation algorithms for feed-forward neural networks to attack RNN by unrolling RNN along time and regarding it as a special kind of feedforward neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The generative model is based on a modified version of the sequence to sequence model [28], which takes malware\u2019s API sequence as input and generates an adversarial API sequence.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "The substitute RNN is trained on benign sequences and the Gumbel-Softmax [12] outputs of the generative RNN, in order to fit the black-box victim RNN.", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "Sequence decoder [5] is used to generate the small sequence.", "startOffset": 17, "endOffset": 20}, {"referenceID": 11, "context": "Gumbel-Softmax is recently proposed to approximate one-hot vectors with differentiable representations [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "where zi is a random number sampled from the Gumbel distribution [9] and temp is the temperature of Gumbel-Softmax.", "startOffset": 65, "endOffset": 68}, {"referenceID": 12, "context": "Adam [13] was used to train all of the models.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "LSTM unit was used for all of the RNNs presented in the experiments due to its good performance in processing long sequences [10, 7].", "startOffset": 125, "endOffset": 132}, {"referenceID": 6, "context": "LSTM unit was used for all of the RNNs presented in the experiments due to its good performance in processing long sequences [10, 7].", "startOffset": 125, "endOffset": 132}, {"referenceID": 0, "context": "The Gumbel-Softmax temperature was searched in the range [1, 100].", "startOffset": 57, "endOffset": 65}, {"referenceID": 32, "context": "In future works we will use the proposed model to attack convolutional neural network (CNN) based malware detection algorithms, since many researchers have begun to use CNN to process sequential data recently [33, 16].", "startOffset": 209, "endOffset": 217}, {"referenceID": 15, "context": "In future works we will use the proposed model to attack convolutional neural network (CNN) based malware detection algorithms, since many researchers have begun to use CNN to process sequential data recently [33, 16].", "startOffset": 209, "endOffset": 217}], "year": 2017, "abstractText": "Recent researches have shown that machine learning based malware detection algorithms are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers have begun to use recurrent neural networks (RNN) to detect malware based on sequential API features. This paper proposes a novel algorithm to generate sequential adversarial examples, which are used to attack a RNN based malware detection system. It is usually hard for malicious attackers to know the exact structures and weights of the victim RNN. A substitute RNN is trained to approximate the victim RNN. Then we propose a generative RNN to output sequential adversarial examples from the original sequential malware inputs. Experimental results showed that RNN based malware detection algorithms fail to detect most of the generated malicious adversarial examples, which means the proposed model is able to effectively bypass the detection algorithms.", "creator": "LaTeX with hyperref package"}}}