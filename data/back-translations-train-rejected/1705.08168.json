{"id": "1705.08168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Look, Listen and Learn", "abstract": "We consider the question: what can be learnt by looking at and listening to a large amount of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel \"Audio-Visual Correspondence\" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good vision and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.", "histories": [["v1", "Tue, 23 May 2017 10:37:54 GMT  (6032kb,D)", "http://arxiv.org/abs/1705.08168v1", null], ["v2", "Tue, 1 Aug 2017 12:04:50 GMT  (5737kb,D)", "http://arxiv.org/abs/1705.08168v2", "Appears in: IEEE International Conference on Computer Vision (ICCV) 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["relja arandjelovi\\'c", "rew zisserman"], "accepted": false, "id": "1705.08168"}, "pdf": {"name": "1705.08168.pdf", "metadata": {"source": "META", "title": "Look, Listen and Learn", "authors": ["Relja Arandjelovi\u0107", "Andrew Zisserman"], "emails": ["relja@google.com", "zisserman@google.com"], "sections": [{"heading": "1. Introduction", "text": "This is the question that arises, to what extent we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we in which we are in which we are in which we are in which we in which we are in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we in which we are in which we in which we in which we are in which we in which we in which we are in which we in which we in which we are in which we in which we in which we in which we are in which we in which we in which we in which we are in which we in which we are in which we in which we are in which we are in which we in which we are in which we in which we in which we in which we are in which we are in which we in which we are in which we are in which we are in which we in which we are in which we are in which we are in which we in which we are in which we in which we are in which we are in which we in which we are in which we in which we are in which we are in which we in which we are in which we in which we are in which we are in which we are in which we in which we are in which we are in which we in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we in which we are in which we in which we are in which we are in which we are in which we in which we in which we in which"}, {"heading": "2. Audio-visual correspondence learning", "text": "The main idea is to use a valuable but as yet untapped source of information contained in the video itself - the correspondence between visual and audiovisual streams available due to their common appearance in the same video. By seeing and listening to many examples of a person barking a violin and a dog, and never, or at least very rarely, seeing a violin being played while a dog barks and vice versa, it should be possible to infer what a violin and a dog look and sound like without ever being explicitly taught what a violin or a dog is. We use this to learn through an audiovisual correspondence (AVC) illustrated in Figure 1. The AVC task is a simple binary classification task: a sample video and a short audio clip - decide whether or not they correspond."}, {"heading": "2.1. Network architecture", "text": "To tackle the AVC task, we propose the network structure shown in Figure 2, which consists of three distinct parts: the vision and the audio subnetworks, which each extract visual and audiographic features, and the fusion network, which takes these features into account to make the final decision on whether the visual and audio signals match. We follow the design style of the VGG network [29] with 3 x 3 Convolutionary Filters and 2 x 2 Max Pooling layers, with step 2 and no padding. The network can be segmented into four blocks of conv + conv + Pool layers, so that within each block the two Conv layers have the same number of filters, while successive blocks have a doubling of filter numbers: 64, 128, 256 and 512."}, {"heading": "2.2. Implementation details", "text": "Training Data Sampling. A non-corresponding frame audio pair is composed by randomly sampling two different videos and selecting a random image from one and a random 1-second audio clip from the other. A corresponding frame audio pair is generated by sampling a random video, selecting a random image in that video, and then selecting a random 1-second audio clip that coincides with the sampled image. This provides additional training samples compared to simply sampling the 1-second audio with the image in the center of the image. We use standard data augmentation techniques for images: Each training image is scaled uniformly so that the smallest dimension is equal to 256, followed by random cropping at 224 x 224 x 224 x, random horizontal rotation and brightness and saturation jitter. Audio is only expanded by sampling the volume to 10% across the same sample frequency, but across primacy."}, {"heading": "3. Results and discussion", "text": "Our network approach \"Look, Listening and Learning\" (L3-Net) is evaluated and examined in many ways. First, the performance of the network in the audiovisual correspondence task itself is examined and compared with supervised baselines. Second, the quality of the learned visual and audiovisual characteristics is tested in a transfer learning environment, in visual and audiovisual classification tasks. Finally, we perform a qualitative analysis of what the network has learned. We start with the introduction of the data sets used for training."}, {"heading": "3.1. Datasets", "text": "Two video datasets are used for training the networks: Flickr-SoundNet and Kinetics-Sounds. Flickr-SoundNet [2]. This is a large, unlabeled dataset of fully unrestricted videos from Flickr, compiled by searching for popular tags, but no tags or any kind of additional information are used except for the videos themselves. It contains over 2 million videos, but for practical reasons we use a random subset of 500k videos (400k training, 50k validation and 50k test) and use only the first 10 seconds of each video. This is the dataset used for training the L3 network to transmit learning tools in sections 3.3 and 3.4. Kinetics sounds. While our goal is to learn from fully unrestricted videos, a labeled dataset is useful for quantitative analysis."}, {"heading": "3.2. Audio-visual correspondence", "text": "First, we evaluate the performance of our method on the task that needs to be solved - deciding whether a frame and a second audio clip match (Section 2). For the Kinetics Sounds Dataset, which contains the described videos, we also evaluate two monitored baselines to determine how well the AVC training is compared to the monitored training (Section 2.1). For both baselines, we first train the monitoring and the audio networks independently of each other, and then combine them in two ways. The monitoring network has identical functionality as a subnet (Section 2.1), on which two fully connected layers are attached (Size: 512 x 128 and 128 x 34) to perform the classification into the 34 Kinetics Sounds classes."}, {"heading": "3.3. Audio features", "text": "In this section, we evaluate the power of audio representation resulting from the L3-Net approach. Namely, the audio subnetwork that is trained on Flickr-SoundNet to extract functions from 1 second audio clips, and the effectiveness of these functions is evaluated on two standard sound benchmarks: ESC-50 and DCASE.Environmental sound classification (ESC-50). This dataset contains 2000 audio clips that are equally balanced between 50 classes, including animal sounds, natural soundscapes, human non-speaking noises, and external / or urban noises. The data is divided into 5 predefined folds and performances."}, {"heading": "3.4. Visual features", "text": "In this section, we evaluate the power of visual representation resulting from the L3-Net approach."}, {"heading": "3.5. Qualitative analysis", "text": "In this section, we analyze what the network has learned. We visualize the results on the test set of Kinetics sounds and Flickr SoundNet records, so that the network has not seen the videos during the training."}, {"heading": "3.5.1 Vision features", "text": "To explore what the visionary network has learned, we select a specific \"unit\" in Pool4 (i.e. a component of the 512-dimensional Pool4 vector) and evaluate the test images by order of magnitude. Figure 3 shows the images of KineticsSounds that most activate certain units in Pool4 (i.e. they are ranked highest by order of magnitude). As you can see, the visionary subnetwork has learned automatically, without explicit supervision, to recognize semantic units such as guitars, chords, keyboards, clarinets, bowling alleys, lawnmowers, or lawnmowers; it has learned finergrained categories how to distinguish between acoustic and bass guitars (\"finger picking\" is largely associated with acoustic grids). Figure 4 shows heatmaps for the kinetics sounds images in Figure 3, by simply connecting the corresponding activation to the component."}, {"heading": "3.5.2 Audio features", "text": "Figure 7 shows what certain audio units in the Kinetics Sounds dataset are sensitive to. Instead of displaying the tone, we show the video image corresponding to the sound to visualize it. You can see that the audio subnetwork, also unsupervised, manages to learn different semantic units and perform a fine-grained classification (\"finger picking\" versus \"playing bass guitar\"). Note that some units are inherently confused - the \"tap dance\" unit also responds to \"feather tapping,\" while the \"saxophone\" unit is sometimes confused with a \"trombone.\" These are reasonable mistakes, especially considering that the sound input is only one second long. Audio concepts learned on the Flickr SoundNet follow the same pattern as the visual ones - the network learns to distinguish different scene categories such as water, underwater, outdoors and windy scenes, as well as recognizing human concepts such as baby-soundnet-picking (8) and the human voices (8) for picking and their spatialization."}, {"heading": "3.5.3 Versus random features", "text": "It is unlikely that there are only 512 units in Pool4, many of which are associated with a semantic concept. However, we have repeated the same experiment with a random network (i.e. a network that has not been trained), and we have failed to find such a correlation. In this case, we have many of the action classes in Pool4 that have a high preference for the class."}, {"heading": "4. Discussion", "text": "We have shown that the network trained for the AVC task achieves superior results in terms of sound classification in order to develop additional methods to prepare and repair the visual networks (one for ImageNet and one for Scenes), and we suspect that this is because the additional freedom of the visual network enables learning to make better use of the possibilities offered by the diversity of visual information in the video (rather than being limited to seeing only through the eyes of the trained network); in addition, the visual features that emerge from the L3 net are equivalent to the state of the art under self-supervised approaches; in addition, it has been shown that the network automatically learns fine-grained distinctions in both modalities, such as bass versus acoustic guitar or saxophone versus clarinet; the localization results are reminiscent of the classical highlighted pixels in [13]; except in our case, we do not just learn the few pixels that move (simultaneously with learning additional events, but rather are motivated by the location of the regions)."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "SoundNet: Learning sound representations from unlabeled video", "author": ["Y. Aytar", "C. Vondrick", "A. Torralba"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Neural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: An argument for multiple comparisons correction", "author": ["C.M. Bennett", "M.B. Miller", "G.L. Wolford"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Computational learning theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Out of time: Automated lip sync in the wild", "author": ["J.S. Chung", "A. Zisserman"], "venue": "In Workshop on Multi-view Lip-reading,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "In Proc. ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Audio Set: An ontology and human-labeled dataset for audio", "author": ["J.F. Gemmeke", "D.P.W. Ellis", "D. Freedman", "A. Jansen", "W. Lawrence", "R.C. Moore", "M. Plakal", "M. Ritter"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["D. Harwath", "A. Torralba", "J.R. Glass"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proc. ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "The Kinetics human action video", "author": ["W. Kay", "J. Carreira", "K. Simonyan", "B. Zhang", "C. Hillier", "S. Vijayanarasimhan", "F. Viola", "T. Green", "T. Back", "P. Natsev", "M. Suleyman", "A. Zisserman"], "venue": "dataset. CoRR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Pixels that sound", "author": ["E. Kidron", "Y.Y. Schechner", "M. Elad"], "venue": "In Proc. CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In Proc. ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Datadependent initializations of convolutional neural networks", "author": ["P. Kr\u00e4henb\u00fchl", "C. Doersch", "J. Donahue", "T. Darrell"], "venue": "In Proc. ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Auditory scene classification using machine learning techniques", "author": ["D. Li", "J. Tam", "D. Toub"], "venue": "IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Shuffle and learn:  Unsupervised learning using temporal order verification", "author": ["I. Misra", "C.L. Zitnick", "M. Herbert"], "venue": "In Proc. ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "In Proc. ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Visually indicated sounds", "author": ["A. Owens", "P. Isola", "J. McDermott", "A. Torralba", "E. Adelson", "W. Freeman"], "venue": "In Proc. CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "W. Jiajun", "J. McDermott", "W. Freeman", "A. Torralba"], "venue": "In Proc. ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "In IEEE Workshop on Machine Learning for Signal processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "ESC: Dataset for environmental sound classification", "author": ["K.J. Piczak"], "venue": "In Proc. ACMM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Histogram of gradients of time-frequency representations for audio scene classification", "author": ["A. Rakotomamonjy", "G. Gasso"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Recurrence quantification analysis features for environmental sound recognition", "author": ["G. Roma", "W. Nogueira", "P. Herrera"], "venue": "In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "S. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A. Berg", "F. Li"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Learning the speech front-end with raw waveform CLDNNs", "author": ["T.N. Sainath", "R.J. Weiss", "A.W. Senior", "K.W. Wilson", "O. Vinyals"], "venue": "In INTERSPEECH,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Proc. ICLR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Detection and classification of acoustic scenes and events", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley"], "venue": "In IEEE Transactions on Multimedia,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Rethinking the Inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In Proc. CVPR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "WaveNet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "In Proc. ICCV,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "In Proc. ECCV,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 5, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 7, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 17, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 18, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 21, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 33, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 34, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 1, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 9, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 19, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 20, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 19, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 136, "endOffset": 144}, {"referenceID": 20, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 136, "endOffset": 144}, {"referenceID": 1, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 208, "endOffset": 215}, {"referenceID": 9, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 208, "endOffset": 215}, {"referenceID": 12, "context": "In earlier, pre deep-learning, approaches the observation was used to beautiful effect in [13] showing \u201cpixels that sound\u201d (e.", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "In contrast, we train both visual and audio networks and, somewhat surprisingly, show that this is beneficial \u2013 in that our performance improves substantially over that of [2] when trained on the same data.", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "In terms of prior work, the most closely related deep learning approach that we know of is \u2018SyncNet\u2019 in [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "However, [5] is aimed at learning to synchronize lip-regions and speech for lip-reading, rather than the more general video and audio material considered here for learning semantic representations.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "More generally, the AVC task is a form of co-training [4], where there are two \u2018views\u2019 of the data, and each view provides complementary information.", "startOffset": 54, "endOffset": 57}, {"referenceID": 28, "context": "We follow the VGG-network [29] design style, with 3 \u00d7 3 convolutional filters, and 2\u00d72 max-pooling layers with stride 2 and no padding.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "Each conv layer is followed by batch normalization [11] and a ReLU nonlinearity.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "Each convolutional layer is followed by batch normalization [11] and a ReLU nonlinearity, and the first fully connected layer (fc1) is followed by ReLU.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "The response map is passed through a logarithm and global scaling is applied such that values fit into the [0, 1] range, before feeding it into the audio subnetwork.", "startOffset": 107, "endOffset": 113}, {"referenceID": 13, "context": "We use the Adam optimizer [14], weight decay 10\u22125, and perform a grid search on the learning rate, although 10\u22124 usually works well.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "Flickr-SoundNet [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "For this purpose we took a subset (much smaller than Flickr-SoundNet) of the Kinetics dataset [12], which contains YouTube videos manually annotated for human actions using Mechanical Turk,", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Environmental sound classification (ESC-50) [24].", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "Detection and classification of acoustic scenes and events (DCASE) [30].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "[2], we follow the same experimental setup.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "SVM-MFCC [24] 39.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "6% Autoencoder [2] 39.", "startOffset": 15, "endOffset": 18}, {"referenceID": 23, "context": "9% Random Forest [24] 44.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "3% Piczak ConvNet [23] 64.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "5% SoundNet [2] 74.", "startOffset": 12, "endOffset": 15}, {"referenceID": 23, "context": "[24] 81.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 65, "endOffset": 68}, {"referenceID": 21, "context": "[22] 22.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] 24.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] 31.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] 31.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[35] (init: [15]) 32.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[35] (init: [15]) 32.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "6% Noroozi and Favaro [19] 34.", "startOffset": 22, "endOffset": 26}, {"referenceID": 34, "context": "Following [35], our features are evaluated by training a linear classifier on the ImageNet training set and measuring the classification accuracy on the validation set.", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "All performance numbers apart from ours are provided by authors of [35], showing only the best performance for each method over all parameter choices (e.", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "[7] achieve 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "On both benchmarks we convincingly beat the previous state-of-the-art, SoundNet [2], by 5.", "startOffset": 80, "endOffset": 83}, {"referenceID": 26, "context": "Namely, the L-Net vision subnetwork trained on Flickr-SoundNet is used to extract features from images, and the effectiveness of these features is evaluated on the ImageNet large scale visual recognition challenge 2012 [27].", "startOffset": 219, "endOffset": 223}, {"referenceID": 34, "context": "[35] where features are extracted from 256 \u00d7 256 images and used to perform linear classification on ImageNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "As in [35], we take conv4 2 features after ReLU and perform max-pooling with equal kernel and stride sizes until feature dimensionality is below 10k; in our case this results in 4\u00d74\u00d7512 = 8192-D features.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "The training procedure (data augmentation, learning rate schedule, label smoothing) is identical to [31], the only differences being that we use the Adam optimizer instead of RMSprop, and a 256\u00d7256 input image instead of 299 \u00d7 299 as it fits our architecture better and to be consistent with [35].", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "The training procedure (data augmentation, learning rate schedule, label smoothing) is identical to [31], the only differences being that we use the Adam optimizer instead of RMSprop, and a 256\u00d7256 input image instead of 299 \u00d7 299 as it fits our architecture better and to be consistent with [35].", "startOffset": 292, "endOffset": 296}, {"referenceID": 5, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 6, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 18, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 34, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 14, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 206, "endOffset": 210}, {"referenceID": 15, "context": "It should be noted that these methods use the AlexNet [16] architecture which is different to ours, so the results are not fully comparable.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "[7] drops from 31.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Could the results in Figures 3, 4, 5, 6, 7 8, and 9 simply be obtained by chance due to examining a large number of units, as colourfully illustrated by the dead salmon experiment [3]? It is unlikely as there are only 512 units in pool4 to choose from, and many of those were found to be highly correlated with a semantic concept.", "startOffset": 180, "endOffset": 183}, {"referenceID": 32, "context": "network with random weights) L-Net representations for the visual and the audio modalities, on the Kinetics-Sounds dataset, using the t-SNE visualization [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "The localization visualization results are reminiscent of the classic highlighted pixels in [13], except in our case we do not just learn the few pixels that move (concurrent with the sound) but instead are able to learn extended regions corresponding to the instrument.", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "over spectrograms [2, 28, 32].", "startOffset": 18, "endOffset": 29}, {"referenceID": 27, "context": "over spectrograms [2, 28, 32].", "startOffset": 18, "endOffset": 29}, {"referenceID": 31, "context": "over spectrograms [2, 28, 32].", "startOffset": 18, "endOffset": 29}, {"referenceID": 8, "context": "Additionally, it would be interesting to learn from the recently released large dataset of videos curated according to audio, rather than visual, events [9] and see what subtle visual semantic categories are discovered.", "startOffset": 153, "endOffset": 156}], "year": 2017, "abstractText": "We consider the question: what can be learnt by looking at and listening to a large amount of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself \u2013 the correspondence between the visual and the audio streams, and we introduce a novel \u201cAudio-Visual Correspondence\u201d learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good vision and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.", "creator": "LaTeX with hyperref package"}}}