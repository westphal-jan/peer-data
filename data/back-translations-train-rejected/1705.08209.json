{"id": "1705.08209", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Unbiasing Truncated Backpropagation Through Time", "abstract": "Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT.", "histories": [["v1", "Tue, 23 May 2017 12:32:48 GMT  (458kb,D)", "http://arxiv.org/abs/1705.08209v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["corentin tallec", "yann ollivier"], "accepted": false, "id": "1705.08209"}, "pdf": {"name": "1705.08209.pdf", "metadata": {"source": "CRF", "title": "Unbiasing Truncated Backpropagation Through Time", "authors": ["Corentin Tallec", "Yann Ollivier"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1 Related Work", "text": "BPTT [Wer90] and its truncated counterpart [Jae05] are almost undisputed in the area of recurring learning processes. Nevertheless, BPTT is hardly applicable to very long training sequences, since it requires storage and backpropagation by a network with as many layers as there are timestamps [Sut13]. Storage problems can be partially solved as in [GMD + 16], but with increased computing costs. Backpropagation by very long sequences also implies the implementation of fewer downward steps, which significantly slows learning [Sut13]. Cut BPTT heuristically solves BPTT deficits by splitting the original sequence into equally large sequences. Cut BPTT separates gradient flows between contiguous sub-sequences, but not the recursive hidden states of the network. Truncation tends to gradients, eliminating any theoretical guarantee of contingent."}, {"heading": "2 Background on recurrent models", "text": "The goal of the recurrent learning algorithms is to optimize a parametric dynamic system so that its output sequence (or predictions) is as close as possible to any target sequence. (1) The goal is to find a total loss in relation to the target outputs. (2) A typical case is that of a standardized neural network (RNN). In this case, the activation of the output layer (ot, ht) is where the activation of the output layer (encoding the predictions), and it is the activation of the hidden recurrent layer. (2) The activation of the hidden recurrent layer (encoding the predictions), and it is the activation of the hidden recurrent layer. (2) The dynamic system takes the activation of the output layer (encoding the predictions), and is the activation of the hidden recurrent layer."}, {"heading": "3 Anticipated Reweighted Backpropagation Through Time: unbiasedness through reweighted stochastic truncation lengths", "text": "Like the full BPTT, BPTT is unable to detect short-term favors."}, {"heading": "4 Choice of ct and memory/variance tradeoff", "text": "ARTBP requires the indication of the probability of a temporal abbreviation t of given previous abbreviations =. Intuitively, the c's regulate the average abbreviation lengths. For example, with a constant abbreviation length of g \u2212 c, the lengths of the sub-sequences followed between two abbreviations of a geometric distribution, with the average abbreviation length being 1c. Abbreviated BPTT with fixed abbreviation length L and ARTBP with fixed sequence L. Small values of the sequence lead to long sub-sequences and sequences that are closer to the exact value, while large values lead to shorter sequences, but larger compensation factors 11 \u2212 ct and rougher estimates. In particular, the product of the 11 \u2212 ct factors can grow rapidly within a sub-sequence. For example, a constant deviation leads to exponential growth of the cumulative 11 \u2212 ct factors during iteration (11).To mitigate this effect, we suggest values so that the probability of a decrease is achieved."}, {"heading": "5 Online implementation", "text": "Importantly, ARTBP can be applied directly online to provide unbiased gradient estimates for recurring networks. In fact, not all truncated points need to be plotted in advance: ARTBP can be applied by scanning the first truncated point, performing both forward and backward transitions of the BPTT up to that point, and performing a partial update of the gradient based on the resulting gradient on that sub-sequence, then proceeding to the next partial sequence and the next truncated point, etc. (Fig. 1c)."}, {"heading": "6 Experimental validation", "text": "The following experimental setup aims both to demonstrate the theoretical properties of ARTBP in comparison to truncated BPTT and to test the solidity of ARTBP on real data."}, {"heading": "6.1 Influence balancing", "text": "The influence of the experiment is a typical example of such a problem, the effect of which is optimized after various delays. The meaning of being is largely unbiased. Intuitively, a parameter has a positive short-term influence, but a negative long-term influence, which exceeds the short-term effect. In practice, we look at a series of agents, numbered from left to right from 1 to p + n, which receive a negative signal at each step, which leads to the agent directly to the left depending on the parameter and diffuse part of their current state. p-Leftmost agents each receive a positive signal, and n-Rightmost agents receive a negative signal. The training goal is to control the state of the leftmost agent state. The first p-agents contribute positively to the first agent state, while the next one contributes negatively to it. However, Agent 1 only feels the contribution of the agent k after k-time. If the optimization is blind to dependencies over k, the effect is never noticeable."}, {"heading": "7 Conclusion", "text": "We have shown that the distortion caused by truncation in the back propagation by the time algorithm can be compensated by the simple mathematical trick of randomizing the truncation points and introducing compensating factors into the back propagation equation. It is experimentally practicable and provides an appropriate balance of the effects of different time scales in the training of recurring models."}, {"heading": "8 Proof of Proposition 1", "text": ", x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x"}], "references": [{"title": "Memory-efficient backpropagation through time", "author": ["Audrunas Gruslys", "R\u00e9mi Munos", "Ivo Danihelka", "Marc Lanctot", "Alex Graves"], "venue": null, "citeRegEx": "Gruslys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gruslys et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen A. Schmidhuber", "Fred A. Cummins"], "venue": "Neural Comput.,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \"echo state network", "author": ["Herbert Jaeger"], "venue": null, "citeRegEx": "Jaeger.,? \\Q2005\\E", "shortCiteRegEx": "Jaeger.", "year": 2005}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1s\u0306 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Le Hai-Son", "Stefan Kombrink", "Jan C\u0306ernock\u00fd"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Training recurrent networks online without backtracking", "author": ["Yann Ollivier", "Corentin Tallec", "Guillaume Charpiat"], "venue": "arXiv preprint arXiv:1507.07680,", "citeRegEx": "Ollivier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ollivier et al\\.", "year": 2015}, {"title": "Unbiased online recurrent optimization", "author": ["Corentin Tallec", "Yann Ollivier"], "venue": "arXiv preprint arXiv:1702.05043,", "citeRegEx": "Tallec and Ollivier.,? \\Q2017\\E", "shortCiteRegEx": "Tallec and Ollivier.", "year": 2017}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "In Proceedings of IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}], "referenceMentions": [], "year": 2017, "abstractText": "Truncated Backpropagation Through Time (truncated BPTT, [Jae05]) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT [Wer90]) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling [MSD+12], ARTBP slightly outperforms truncated BPTT. Backpropagation Through Time (BPTT) [Wer90] is the de facto standard for training recurrent neural networks. However, BPTT has shortcomings when it comes to learning from very long sequences: learning a recurrent network with BPTT requires unfolding the network through time for as many timesteps as there are in the sequence. For long sequences this represents a heavy computational and memory load. This shortcoming is often overcome heuristically, by arbitrarily splitting the initial sequence into subsequences, and only backpropagating on the subsequences. The resulting algorithm is often referred to as Truncated Backpropagation Through Time (truncated BPTT, see for instance [Jae05]). This comes at the cost of losing long term dependencies. We introduce Anticipated Reweighted Truncated BackPropagation (ARTBP), a variation of truncated BPTT designed to provide an unbiased gradient estimate, accounting for long term dependencies. Like truncated BPTT, ARTBP splits the initial training sequence into subsequences, and only 1 ar X iv :1 70 5. 08 20 9v 1 [ cs .N E ] 2 3 M ay 2 01 7 backpropagates on those subsequences. However, unlike truncated BPTT, ARTBP splits the training sequence into variable size subsequences, and suitably modifies the backpropagation equation to obtain unbiased gradients. Unbiasedness of gradient estimates is the key property that provides convergence to a local optimum in stochastic gradient descent procedures. Stochastic gradient descent with biased estimates, such as the one provided by truncated BPTT, can lead to divergence even in simple situations and even with large truncation lengths (Fig. 3). ARTBP is experimentally compared to truncated BPTT. On truncated BPTT failure cases, typically when balancing of temporal dependencies is key, ARTBP achieves reliable convergence thanks to unbiasedness. On small-scale but real world data, ARTBP slightly outperforms truncated BPTT on the test case we examined. ARTBP formalizes the idea that, on a day-to-day basis, we can perform short term optimization, but must reflect on long-term effects once in a while; ARTBP turns this into a provably unbiased overall gradient estimate. Notably, the many short subsequences allow for quick adaptation to the data, while preserving overall balance.", "creator": "LaTeX with hyperref package"}}}