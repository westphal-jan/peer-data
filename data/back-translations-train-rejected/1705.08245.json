{"id": "1705.08245", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Enhanced Experience Replay Generation for Efficient Reinforcement Learning", "abstract": "Applying deep reinforcement learning (RL) on real systems suffers from slow data sampling. We propose an enhanced generative adversarial network (EGAN) to initialize an RL agent in order to achieve faster learning. The EGAN utilizes the relation between states and actions to enhance the quality of data samples generated by a GAN. Pre-training the agent with the EGAN shows a steeper learning curve with a 20% improvement of training time in the beginning of learning, compared to no pre-training, and an improvement compared to training with GAN by about 5% with smaller variations. For real time systems with sparse and slow data sampling the EGAN could be used to speed up the early phases of the training process.", "histories": [["v1", "Tue, 23 May 2017 13:36:00 GMT  (726kb,D)", "http://arxiv.org/abs/1705.08245v1", null], ["v2", "Mon, 29 May 2017 14:24:08 GMT  (726kb,D)", "http://arxiv.org/abs/1705.08245v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vincent huang", "tobias ley", "martha vlachou-konchylaki", "wenfeng hu"], "accepted": false, "id": "1705.08245"}, "pdf": {"name": "1705.08245.pdf", "metadata": {"source": "CRF", "title": "Enhanced Experience Replay Generation for Efficient Reinforcement Learning", "authors": ["Vincent Huang", "Tobias Ley", "Wenfeng Hu"], "emails": ["vincent.a.huang@ericsson.com", "tobias.ley@ericsson.com", "martha.vlachou-konchylaki@ericsson.com", "wenfeng.hu@ericsson.com"], "sections": [{"heading": "1 Introduction", "text": "In 5G telecommunications systems, network functionality must meet new network-specific requirements, such as ultra-low latency, high robustness, rapid response to changing capacity needs, and dynamic functionality allocation. As cloud computing and data centers rise, more and more network functionality will be virtualized and shifted to the cloud. Self-optimized and self-sustaining dynamic systems with fast and efficient scaling, workload optimization, and new functionality such as self-healing, parameter-free, and zero-touch systems will ensure SLA (Service Level Agreements) and reduce TCO (Total Cost of Ownership). Strengthening learning, where an agent learns how to best manage the system, government information, and a reward function, is a promising technology to solve such an optimization problem. Strengthening learning is a technology to develop self-learning SW agents that learn and optimize policies based on environmental conditions and an observed reward system."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reinforcement Learning", "text": "A reinforcement learning agent obtains an observation from the environment it interacts in the state st and selects an action to maximize the total expected discounted reward Gt. The action taken from the action space A is calculated by a policy \u03c0 (at | st). Each time the policy is executed, a scalar reward value Ras is returned from the environment, and the transitions of the agent to the next state, st + 1, following the state transitional probabilities P ass = P (s, a). We can define the state value function V (s) as the expected return in state strategies, and the action value function Q\u03c0 (s) as the expected return that takes action, while in the state s, the policies follow. The reinforcement learning agent attempts to maximize the expected return by maximizing the value function V."}, {"heading": "2.2 Generative Adversarial Networks", "text": "A second trend in deep learning research is generative models, in particular Generative Adversarial Nets (GAN) [Goodfellow et al., 2014], and the link to reinforcement learning [Finn et al., 2016, Yu et al., 2017]. GANs are used to synthesize data samples that can be used to train an RL agent. In our case, these synthesized data samples are used to pre-train a Reinforcement Learning Agent to speed up training time in the real production system. We will compare this method with various pre-training alternatives. The key behind Generative Adversarial Nets is an aversion between a generative model G, which learns true data distribution, and a discriminatory model D, which evaluates the likelihood of a sample originating from true distribution rather than having been generated by G. The generator, modeled as a multilayer perceptron, is amplified from a noise distribution."}, {"heading": "3 Enhanced GAN", "text": "The object of GAN may be considered a minmax game. The discriminator tries to maximize a value function (\"Q\") pairs (\"Q\") pairs (\"Q\" pairs), while the generator tries to minimize these, as shown below. \"Dr.\" (\"Dr.\") Dr. \"(\" Dr. \") Dr.\" (\"Dr.\") \"Dr.\" (\"Dr.\") \"Dr.\" (\"Dr.\") \"Dr.\" (\"Dr.\") \"Dr.\" (\"Dr.\") Dr. \"(\" D. \")\" Dr. \"(\" Dr. \")\" Dr. \"(\" Dr. \")\" Dr. \"(\" Dr. \")\" Dr. \"(\" Dr.. \")\" (Dr..) \"(Dr..)\" (Dr..) \"(Dr..)\" (Dr..) \"(Dr.. (.)\" (Dr..) \"(Dr..)\" (Dr..) \"(Dr.. (.)\" (Dr..) \"(Dr..)\" (Dr.. (Dr..) \"(Dr..)\" (Dr.. (.) \"(Dr..)\" (Dr.. (.) \"(Dr..)\" (Dr.. (.) \"(Dr..)\" (Dr.. (.) \"(Dr..)\" (Dr.. (.) \"(Dr..)\" (Dr.. (.) \"(Dr.. (.)\" (Dr..) \"(Dr.. (.) (.)\" (Dr.. (.) (Dr.. (.) (Dr..) (Dr.. (.) (.) (Dr..) (. (Dr..) (.) (. (Dr..) (.) (Dr..) (. (. (.) (\") (Dr.. (.) (Dr..) (\") (Dr.. (. (. (.) (\"Dr.. (.). (.). (. (.)) (\" Dr.. (. (\").) (\") (\"Dr.. (.) (\" Dr.. (\").). (\"). (\") (\" Dr.. (\"). (\" Dr.. (.). (\").) (\" Dr.. (\""}, {"heading": "4 Results", "text": "We use the CartPole environment of OpenAI Gym to evaluate the EGAN performance as shown in Figure 2, with parameter settings listed in Table 1. The figure shows the training of the PG agent after he has been preschooled, so we observe a small offset of the upstream agents on the X axis (500 episodes), while the agent without preschooling starts at 0. The black line is the 100-episode average reward of a PG agent, without preschooling mechanisms. The red line and the blue line represent the performance of the PG agent with GAN and EGAN preschooling. The black line uses 500-episode average reward for the preschoolers, without preschooling mechanisms."}, {"heading": "5 Conclusions", "text": "In this paper, we deal with a basic problem of reinforcement learning applied to a real environment. Training usually takes a long time and requires many samples. First, we collected a small set of data samples from the environment to train a GAN. GAN is then used to generate unlimited synthesized data, to train an RL agent before training, so that the agent learns the basic characteristics of the environment. Using a GAN, we can cover larger variations of randomly collected data. We further improve the GAN with an enhancer that uses the state-action relationships in the reproduction of experiential data to improve the quality of the synthesized data.By using the extended structure (EGAN), we can learn 20% faster than no pre-training, 5% faster than pre-training with a GAN, and achieve a more robust system in terms of standard deviations."}], "references": [{"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Decentralized POMDPs, pages 471\u2013503", "author": ["Frans A. Oliehoek"], "venue": "ISBN 978-3-642-27645-3", "citeRegEx": "Oliehoek.,? \\Q2012\\E", "shortCiteRegEx": "Oliehoek.", "year": 2012}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E Turner", "Sergey Levine"], "venue": "arXiv preprint arXiv:1611.02247,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael Jordan", "Philipp Moritz"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["Lerrel Pinto", "Abhinav Gupta"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Pinto and Gupta.,? \\Q2016\\E", "shortCiteRegEx": "Pinto and Gupta.", "year": 2016}, {"title": "Generative Adversarial Networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models", "author": ["Chelsea Finn", "Paul Christiano", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1611.03852,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": "In Thirty-First AAAI Conference on Artificial Intelligence,", "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "[Sutton and Barto, 1998].", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "The agent has access only to partial, local information, which can be formalized as a Decentralized Partial-Observable Markov Decision Process (Dec-POMDPs) [Oliehoek, 2012].", "startOffset": 156, "endOffset": 172}, {"referenceID": 1, "context": "Finally, sparse data, low data sampling rate, and slow reaction time to actions greatly limit the possibility to train an agent in an acceptable time frame [Duan et al., 2016].", "startOffset": 156, "endOffset": 175}, {"referenceID": 4, "context": "New, sample efficient algorithms such as Q-Prop [Gu et al., 2016] have been proposed, that provide substantial gains in sample efficiency over trust region policy optimization (TRPO) [Schulman et al.", "startOffset": 48, "endOffset": 65}, {"referenceID": 5, "context": ", 2016] have been proposed, that provide substantial gains in sample efficiency over trust region policy optimization (TRPO) [Schulman et al., 2015].", "startOffset": 125, "endOffset": 148}, {"referenceID": 6, "context": "Methods such as actor critic algorithms [Mnih et al., 2016], as well as combinations of on-policy and off policy algorithms [O\u2019Donoghue et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 7, "context": "Other approaches using supervised learning have been also tested [Pinto and Gupta, 2016].", "startOffset": 65, "endOffset": 88}, {"referenceID": 8, "context": "A second trend in deep learning research has been generative models, especially Generative Adversarial Nets (GAN) [Goodfellow et al., 2014], and the connection to reinforcement learning [Finn et al.", "startOffset": 114, "endOffset": 139}], "year": 2017, "abstractText": "Applying deep reinforcement learning (RL) on real systems suffers from slow data sampling. We propose an enhanced generative adversarial network (EGAN) to initialize an RL agent in order to achieve faster learning. The EGAN utilizes the relation between states and actions to enhance the quality of data samples generated by a GAN. Pre-training the agent with the EGAN shows a steeper learning curve with a 20% improvement of training time in the beginning of learning, compared to no pre-training, and an improvement compared to training with GAN by about 5% with smaller variations. For real time systems with sparse and slow data sampling the EGAN could be used to speed up the early phases of the training process.", "creator": "LaTeX with hyperref package"}}}