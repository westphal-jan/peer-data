{"id": "1705.08378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Detecting Adversarial Examples in Deep Networks with Adaptive Noise Reduction", "abstract": "Deep neural networks (DNNs) play a key role in many applications. Unsurprisingly, they also became a potential attack target of adversaries. Some studies have demonstrated DNN classifiers can be fooled by the adversarial example, which is crafted via introducing some perturbations into an original sample. Accordingly, some powerful defense techniques were proposed against adversarial examples. However, existing defense techniques require modifying the target model or depend on the prior knowledge of attack techniques to different degrees. In this paper, we propose a straightforward method for detecting adversarial image examples. It doesn't require any prior knowledge of attack techniques and can be directly deployed into unmodified off-the-shelf DNN models. Specifically, we consider the perturbation to images as a kind of noise and introduce two classical image processing techniques, scalar quantization and smoothing spatial filter, to reduce its effect. The image two-dimensional entropy is employed as a metric to implement an adaptive noise reduction for different kinds of images. As a result, the adversarial example can be effectively detected by comparing the classification results of a given sample and its denoised version. Thousands of adversarial examples against some state-of-the-art DNN models are used to evaluate the proposed method, which are crafted with different attack techniques. The experiment shows that our detection method can achieve an overall recall of 93.73% and an overall precision of 95.45% without referring to any prior knowledge of attack techniques.", "histories": [["v1", "Tue, 23 May 2017 15:50:32 GMT  (1440kb,D)", "https://arxiv.org/abs/1705.08378v1", null], ["v2", "Mon, 19 Jun 2017 02:28:53 GMT  (1440kb,D)", "http://arxiv.org/abs/1705.08378v2", null], ["v3", "Tue, 20 Jun 2017 01:15:17 GMT  (1440kb,D)", "http://arxiv.org/abs/1705.08378v3", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["bin liang", "hongcheng li", "miaoqiang su", "xirong li", "wenchang shi", "xiaofeng wang"], "accepted": false, "id": "1705.08378"}, "pdf": {"name": "1705.08378.pdf", "metadata": {"source": "META", "title": "Detecting Adversarial Examples in Deep Networks with Adaptive Noise Reduction", "authors": ["Bin Liang", "Hongcheng Li", "Miaoqiang Su", "Xirong Li", "Wenchang Shi", "Xiaofeng Wang"], "emails": ["liangb@ruc.edu.cn", "owenlee@ruc.edu.cn", "sumiaoqiang@ruc.edu.cn", "xirong@ruc.edu.cn", "wenchang@ruc.edu.cn", "xw7@indiana.edu"], "sections": [{"heading": null, "text": "KEYWORDS Contradictory examples, deep neural networks, recognition"}, {"heading": "1 INTRODUCTION", "text": "This year it has come to the point that it is a purely reactionary project, which is a reactionary, reactionary, reactionary and reactionary solution."}, {"heading": "2 BACKGROUND", "text": "In this section, we provide some preliminary work on DNNs and the attack techniques used to create opposing examples."}, {"heading": "2.1 Deep Neural Networks", "text": "As illustrated in Figure 3, a DNN consists of a sequence of neural layers. Each neural layer serves as a parametric function for modelling the new representation obtained from the previous layer. Gradually, from the low layers to the high layers, the network can efficiently perform feature extractions. Each neural layer is assigned a weight vector that indicates the activation of each neuron. It is updated during the training phase using the back propagation algorithm. Generally, the features imported into the low layers are the raw data that describe the basic original characteristics of the problem cases. After multi-layer abstraction, the features extracted from the high layers have more semantic information about the input.Depending on the type of output expected by the network, DNN can fall into two categories: supervised learning and unattended learning. The former is mainly used for classification. The network is equipped with labeled data sets to form connections between this network and inputs [17 is used to learn]."}, {"heading": "2.2 Crafting Adversarial Example", "text": "In most cases, the attacker can misunderstand the resulting image, while a human observer can still correctly classify it without noticing the existence of the introduced perturbation. In practice, the adversarial examples can be generated directly. [13, 49, 61] In this paper, we choose the following three attack techniques to perform detection techniques: imperceptible perturbation.Fast sign method."}, {"heading": "3 METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "3.2 Computing Entropy", "text": "Traditional image entropy (1-D entropy) concerns only the concentration of the pixel value distribution. To capture the spatial correlation between the pixels, we use two-dimensional entropy (2-D entropy) to measure the information of an image. Without loss of generality, for an M \u00b7 N image with 256 pixel levels (0 \u0445 255), we first calculate the average pixel value of the neighborhood for each pixel. In this study, we use the average filter mask shown in Figure 10 to calculate the average pixel value of the neighborhood, resulting in a pair (i, j), the pixel value i, and the average of the neighborhood j. The frequency of the pair is called f i, j, and a common probability mass function pi, j is calculated as an equation (3). On the basis of the 2-D entropy of the image can be calculated as equation (4)."}, {"heading": "3.3 Scalar Quantization", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3.4 Spatial Smoothing Filter", "text": "In fact, it is as if it were a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project, a reactionary project."}, {"heading": "3.5 Detection Filter", "text": "For this purpose, we design a combination filter based on the two above techniques, rather than simply concatenating them. As discussed in Section 2, the attacker often wants to minimize the interference introduced into the opposing sample to render it imperceptible. In other words, the interference in pixel intensity is often limited to a small range. If the intensity of a pixel is too blurred by the smoothing filter, the smoothing may be unnecessary. Based on the above intuition, our detection filter is determined by the following equation '(x, y) = {fSQ (x, y), if the strength of a pixel (x, y) \u2212 f (x, y) \u2212 an optimization filter (x, y) it is unnecessary to determine the original SQ \u2212 f (x, y)."}, {"heading": "4 EVALUATION", "text": "We evaluate the effectiveness of our method by applying it to detect enemy examples created by the attack techniques described in Section 2.2. The retrieval rate and precision rate are used to quantify the detection performance defined as: Recall = TPTP + FN (7) Precision = TPTP + FP (8), where TP is the number of correctly detected enemy examples (true positives), FN is the number of opposing samples surviving from our detection (false negatives), and FP is the number of benign images detected as opposing examples (false positives)."}, {"heading": "4.1 Detecting FGSM Examples", "text": "In fact, it is a matter of a way in which it is a matter of a way in which it is a model that has come into contact with the MNIST dataset [51] and which has been designed to test the FGSM attack. ImageNet [31] is a dataset of over 15 million described high-resolution images belonging to about 22,000 categories, while MNIST [4] is a small scale dataset of simple gray handwritten digitalisms. ImageNet [31] is a dataset of over 15 million described high-resolution images belonging to about 22,000 categories, while MNIST [4] is a small scale dataset of simple gray handwritten digitalisms. We randomly choose four classes (zebra, panda, cab, and pineapple) of ImageNet images belonging to FGSM adversarial examples."}, {"heading": "4.2 Detecting DeepFool Examples", "text": "Moosavi-Dezfooli et al. used two state-of-the-art CaffeNet and GoogLeNet models trained with ImageNet data sets to test their DeepFool attack, and the two models are available in [1]. We still select the same four image classes (i.e. Zebra, Panda, Cab and Pineapple) to generate contrary examples. Using the DeepFool algorithm provided in [1], 1,234 effective contrary examples are generated for the CaffeNet model and 1,032 for the GoogLeNet model. The generated examples and the corresponding original images form our detection test set. The detection results of these samples are listed in Table 4. An average recall rate of 95.62% and an average precision of 91.12% is achieved for samples targeting CaffeNet; 93.22% and 2.15% for the Google net targets."}, {"heading": "4.3 Detecting CW Examples", "text": "Carlini and Wagner also use MNIST and ImageNet to train two DNN models as targets. In practice, the use of the two trained models is more expensive than other attack techniques. In our computer, creating an ImageNet counter-example with L2 and L \u221e takes about 30 minutes and 4.5 hours, respectively. For this reason, we chose only the first 1,000 images in MNIST and 30 random images for each of the four ImageNet classes (listed in Table 3) as an experiment data set. Finally, L2 attack successfully generated 991 effective counter-examples and 110 from the 120 ImageNet images; and L \u00b2 attack provides 991 and 67 effective examples from the two image groups, respectively. The examples generated and the associated original images form our recognition settings. As listed in Table N5, the proposed net values are compared with very high precision M9 and the proposed net samples with only 3.64 and 3.10 pixels, respectively."}, {"heading": "CW MNIST L2 98.79% 99.49%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4 Summary", "text": "A total of 18,322 samples are used to evaluate our method in the above experiments, half of which are counterproductive and the other half benign. We achieve an overall memory rate of 93.73% and an overall accuracy of 95.47% in detecting the contrary examples generated by the three attack techniques."}, {"heading": "5 DISCUSSION AND LIMITATIONS", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6 RELATEDWORK", "text": "This year, the time has come to ask only one question: \"What is it?,\" he asked. \"What is it?,\" he asked. \"What is it?,\" he asked. \"What is it?,\" he asked. \"What is it?,\" he asked. \"What is it?,\" he asked. \"What is it?,\" he asked. \"\" What is it?, \"he asked.\" \"What is it?,\" he asked. \"\" What is it?, \"he asked."}, {"heading": "7 CONCLUSION", "text": "Our work represents a simple and effective method of detecting enemy image samples, the opposing interference is considered as a kind of noise and the proposed method is used as a filter to reduce its effect.Image 2-D entropy is used to automatically adjust the detection strategy for certain samples. Our method offers two important features (1) without requiring prior knowledge of attacks, and (2) can be directly integrated into unmodified models.The experiment shows that our method can achieve high recall and precision in detecting enemy examples generated by the different attack techniques and in selectively detecting different models. Our method is also compatible with other defense techniques. Better performance can be achieved by combining them together.Our research has shown that the opposing images can be effectively analyzed with classical image processing techniques. In the future, we will find more effective image processing techniques to detect them."}, {"heading": "8 ACKNOWLEDGMENTS", "text": "The authors thank the anonymous reviewers for their insightful comments. The work is supported by XXX and YYY.ACKNOWLEDGMENTS REFERENCES [1]. A simple and precise method for processing deep neural networks. [3] Robust evasion against neural network to find adversarial examples.https: / / github.com / carlini. [4] The MNIST database of the handwritten digital world. [5] Marco Barreno, Blaine Nelson, Russell Sears, AnthonyD Joseph, and J Doug Tygar.Can machine learning be secure?"}], "references": [{"title": "Can machine learning be secure", "author": ["Marco Barreno", "BlaineNelson", "Russell Sears", "AnthonyD Joseph", "J Doug Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, computer and communications security,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Evasion attacks against machine learning at test time", "author": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Multiple classifier systems for adversarial classification tasks", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the International Workshop on Multiple Classifier Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "International Journal of Machine Learning and Cybernetics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Multiple classifier systems under attack", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the International Workshop on Multiple Classifier Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Design of robust classifiers for adversarial environments", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the 2011 IEEE International Conference on Systems, Man, and Cybernetics (SMC),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["Michael Br\u00fcckner", "Christian Kanzow", "Tobias Scheffer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["Michael Br\u00fcckner", "Tobias Scheffer"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "On short interval expansion of r\u00e9nyi entropy", "author": ["Bin Chen", "Jia-ju Zhang"], "venue": "Journal of High Energy Physics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Multicolumn deep neural network for traffic sign classification", "author": ["Dan Cire\u015fAn", "Ueli Meier", "Jonathan Masci", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu"], "venue": "In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Detecting adversarial samples from artifacts", "author": ["Reuben Feinman", "Ryan R Curtin", "Saurabh Shintre", "Andrew B Gardner"], "venue": "arXiv preprint arXiv:1703.00410,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["Matt Fredrikson", "Somesh Jha", "Thomas Ristenpart"], "venue": "In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 28th international conference on Machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Digital image processing", "author": ["Rafael C Gonzalez", "Richard E Woods"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Measuring invariances in deep networks", "author": ["Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "On the (statistical) detection of adversarial examples", "author": ["Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1702.06280,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "On detecting adversarial perturbations", "author": ["Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff"], "venue": "arXiv preprint arXiv:1702.04267,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng Jia", "Dong Wei", "Socher Richard", "Li-Jia Li", "Li Kai", "Fei-Fei Li"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Caffe: Convolutional 12  architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Approaches to adversarial drift", "author": ["Alex Kantchelian", "Sadia Afroz", "Ling Huang", "Aylin Caliskan Islam", "Brad Miller", "Michael Carl Tschantz", "Rachel Greenstadt", "Anthony D Joseph", "JD Tygar"], "venue": "In Proceedings of the 2013 ACM workshop on Artificial intelligence and security,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Deep learning and music adversaries", "author": ["Corey Kereliuk", "Bob L Sturm", "Jan Larsen"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Feature weighting for improved classifier robustness", "author": ["Aleksander Ko\u0142cz", "Choon Hui Teo"], "venue": "In Proceedings of the 6th Conference on Email and Anti-Spam,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Adversarial examples in the physical world", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["Pavel Laskov"], "venue": "In Proceedings of the 2014 IEEE Symposium on Security and Privacy (S&P),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le"], "venue": "In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1989}, {"title": "Convolutional networks and applications in vision", "author": ["Yann LeCun", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In Proceedings of the 2010 IEEE International Symposium on Circuits and Systems (ISCAS),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Digital image enhancement and noise filtering by use of local statistics", "author": ["Jong-Sen Lee"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1980}, {"title": "Deep text classification can be fooled", "author": ["Bin Liang", "Hongcheng Li", "Miaoqiang Su", "Pan Bian", "Xirong Li", "Wenchang Shi"], "venue": "arXiv preprint arXiv:1704.08006,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2017}, {"title": "Cracking classifiers for evasion: A case study on the google\u2019s phishing pages filter", "author": ["Bin Liang", "Miaoqiang Su", "Wei You", "Wenchang Shi", "Gang Yang"], "venue": "In Proceedings of the 25th International Conference on World Wide Web,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Good word attacks on statistical spam filters", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the 2nd Conference on Email and Anti-Spam,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2005}, {"title": "Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious pdf files detection", "author": ["Davide Maiorca", "Igino Corona", "Giorgio Giacinto"], "venue": "In Proceedings of the 8th ACM SIGSAC symposium on Information, computer and communications security,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber"], "venue": "Artificial Neural Networks and Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Exploiting machine learning to subvert your spam", "author": ["Blaine Nelson", "Marco Barreno", "Fuching Jack Chi", "Anthony D Joseph", "Benjamin IP Rubinstein", "Udam Saini", "Charles A Sutton", "J Doug Tygar", "Kai Xia"], "venue": "filter. LEET,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "cleverhans v1.0.0: an adversarial machine learning library", "author": ["Nicolas Papernot", "Ian Goodfellow", "Ryan Sheatsley", "Reuben Feinman", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Practical black-box attacks against machine learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2017}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2016 IEEE European Symposium on Security and Privacy (EuroS&P),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Proceedings of the 2016 IEEE Symposium on Security and Privacy (S&P),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Robustness of multimodal biometric fusion methods against spoof attacks", "author": ["Ricardo N Rodrigues", "Lee Luan Ling", "Venu Govindaraju"], "venue": "Journal of Visual Languages & Computing,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K Reiter"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Membership inference attacks against machine learning models", "author": ["Reza Shokri", "Marco Stronati", "Congzheng Song", "Vitaly Shmatikov"], "venue": "In Proceedings of the 2017 IEEE Symposium on Security and Privacy (S&P)", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2017}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Yi Sun", "Yuheng Chen", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In Proceedings of the 2014 International Conference on Learning Representations,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2014}, {"title": "Adversary resistant deep neural networks with an application to malware detection", "author": ["Qinglong Wang", "Wenbo Guo", "Kaixuan Zhang", "Alexander G Ororbia II", "Xinyu Xing", "C. Lee Giles", "Xue Liu"], "venue": "arXiv preprint arXiv:1610.01239,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Learning adversary-resistant deep neural networks", "author": ["Qinglong Wang", "Wenbo Guo", "Kaixuan Zhang", "Alexander G Ororbia II", "Xinyu Xing", "C Lee Giles", "Xue Liu"], "venue": "arXiv preprint arXiv:1612.01401,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2016}, {"title": "Amethodology for formalizing model-inversion attacks", "author": ["XiWu", "Matthew Fredrikson", "Somesh Jha", "Jeffrey F Naughton"], "venue": "In Proceedings of the 2016 IEEE Computer Security Foundations Symposium (CSF),", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2016}, {"title": "Feature squeezing: Detecting adversarial examples in deep neural networks", "author": ["Weilin Xu", "David Evans", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1704.01155,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2017}, {"title": "Automatically evading classifiers", "author": ["Weilin Xu", "Yanjun Qi", "David Evans"], "venue": "In Proceedings of the 2016 Network and Distributed Systems Symposium,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 98, "endOffset": 106}, {"referenceID": 36, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 98, "endOffset": 106}, {"referenceID": 13, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 127, "endOffset": 135}, {"referenceID": 25, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 127, "endOffset": 135}, {"referenceID": 11, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 169, "endOffset": 177}, {"referenceID": 62, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 169, "endOffset": 177}, {"referenceID": 55, "context": "DNNs have exhibited very impressive performance in these tasks, especially in the image classification [60].", "startOffset": 103, "endOffset": 107}, {"referenceID": 53, "context": "Some DNN-based classifiers achieved even higher performance than human [58, 59].", "startOffset": 71, "endOffset": 79}, {"referenceID": 54, "context": "Some DNN-based classifiers achieved even higher performance than human [58, 59].", "startOffset": 71, "endOffset": 79}, {"referenceID": 21, "context": "Some recent studies [26, 49, 61] demonstrate that DNN-based image classifiers can be fooled by adversarial examples, which are well-crafted to cause a trained model to misclassify the instances it is given.", "startOffset": 20, "endOffset": 32}, {"referenceID": 44, "context": "Some recent studies [26, 49, 61] demonstrate that DNN-based image classifiers can be fooled by adversarial examples, which are well-crafted to cause a trained model to misclassify the instances it is given.", "startOffset": 20, "endOffset": 32}, {"referenceID": 56, "context": "Some recent studies [26, 49, 61] demonstrate that DNN-based image classifiers can be fooled by adversarial examples, which are well-crafted to cause a trained model to misclassify the instances it is given.", "startOffset": 20, "endOffset": 32}, {"referenceID": 21, "context": "presented in [26].", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "image [26].", "startOffset": 6, "endOffset": 10}, {"referenceID": 55, "context": "Consequently, a famous DNN classifier GoogLeNet [60] will misclassify the resultant image, while a human observer can still correctly classify it and without noticing the existence of the introduced perturbations.", "startOffset": 48, "endOffset": 52}, {"referenceID": 47, "context": "As shown in [52], a stop sign, after being crafted, will be incorrectly classified as a yield sign.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 29, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 48, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 49, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 21, "context": "For example, the adversarial training is a straightforward defense technique which uses as many adversarial samples as possible during training process as a kind of regularization [26, 34, 53].", "startOffset": 180, "endOffset": 192}, {"referenceID": 29, "context": "For example, the adversarial training is a straightforward defense technique which uses as many adversarial samples as possible during training process as a kind of regularization [26, 34, 53].", "startOffset": 180, "endOffset": 192}, {"referenceID": 48, "context": "For example, the adversarial training is a straightforward defense technique which uses as many adversarial samples as possible during training process as a kind of regularization [26, 34, 53].", "startOffset": 180, "endOffset": 192}, {"referenceID": 49, "context": "s [54] introduced a defense technique named defensive distillation to adversarial sample.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 22, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 24, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 60, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 15, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 219, "endOffset": 223}, {"referenceID": 22, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 307, "endOffset": 311}, {"referenceID": 60, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 428, "endOffset": 432}, {"referenceID": 20, "context": "In fact, some studies [2, 25, 40] have shown that the state-of-the-art classifier is invariant to different input transformations, such as translation, rotation, scale and etc.", "startOffset": 22, "endOffset": 33}, {"referenceID": 35, "context": "In fact, some studies [2, 25, 40] have shown that the state-of-the-art classifier is invariant to different input transformations, such as translation, rotation, scale and etc.", "startOffset": 22, "endOffset": 33}, {"referenceID": 55, "context": "Weemploy some state-of-the-art DNNmodels and popular datasets, such as GoogLeNet [60], CaffeNet [32], ImageNet [31] andMNIST [4] to evaluate the effectiveness of the proposed method.", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Weemploy some state-of-the-art DNNmodels and popular datasets, such as GoogLeNet [60], CaffeNet [32], ImageNet [31] andMNIST [4] to evaluate the effectiveness of the proposed method.", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "Weemploy some state-of-the-art DNNmodels and popular datasets, such as GoogLeNet [60], CaffeNet [32], ImageNet [31] andMNIST [4] to evaluate the effectiveness of the proposed method.", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": ", FGSM [26], DeepFool [49], and CW attacks [13], are used to craft adversarial examples.", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": ", FGSM [26], DeepFool [49], and CW attacks [13], are used to craft adversarial examples.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": ", FGSM [26], DeepFool [49], and CW attacks [13], are used to craft adversarial examples.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 12, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 18, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 31, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 43, "context": "The latter is often used for feature extraction [48] and network pre-training [19], which is trained with unlabeled dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "The latter is often used for feature extraction [48] and network pre-training [19], which is trained with unlabeled dataset.", "startOffset": 78, "endOffset": 82}, {"referenceID": 56, "context": "[61] first made the intriguing discovery that various machine learning models, including DNNs [36, 39], are vulnerable to adversarial samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[61] first made the intriguing discovery that various machine learning models, including DNNs [36, 39], are vulnerable to adversarial samples.", "startOffset": 94, "endOffset": 102}, {"referenceID": 34, "context": "[61] first made the intriguing discovery that various machine learning models, including DNNs [36, 39], are vulnerable to adversarial samples.", "startOffset": 94, "endOffset": 102}, {"referenceID": 21, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 67, "endOffset": 79}, {"referenceID": 44, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 67, "endOffset": 79}, {"referenceID": 56, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 67, "endOffset": 79}, {"referenceID": 21, "context": "[26] proposed a straightforward strategy named fast gradient sign method (FGSM) to craft adversarial samples against GoogLeNet [60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[26] proposed a straightforward strategy named fast gradient sign method (FGSM) to craft adversarial samples against GoogLeNet [60].", "startOffset": 127, "endOffset": 131}, {"referenceID": 44, "context": "[49] devised the DeepFool algorithm to find very small perturbations that are sufficient to change the classification result.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "performed some attack experiments against several DNN image classifiers, such as CaffeNet [32] and GoogLeNet [60], and so on.", "startOffset": 90, "endOffset": 94}, {"referenceID": 55, "context": "performed some attack experiments against several DNN image classifiers, such as CaffeNet [32] and GoogLeNet [60], and so on.", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "Carlini and Wagner [13] also employed an optimization algorithm to seek as small as possible perturbations.", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "Using some public datasets, such as MNIST [4] and ImageNet [31], Carlini and Wagner trained some deep network models to evaluate their attack methods.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "As demonstrated in [13], CW attacks can find closer adversarial examples than the other attack techniques and never fail to find an adversarial example.", "startOffset": 19, "endOffset": 23}, {"referenceID": 49, "context": "Besides, Carlini and Wagner also illustrated their attacks can effectively break the defensive distillation [54].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "[25] found that the features learned by deep networks are invariant to different input transformations, such as translation, rotation, scale and etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[2, 40] also demonstrated LeNet-5 classifier is robust to translation, scale, rotation, squeezing, and stroke width.", "startOffset": 0, "endOffset": 7}, {"referenceID": 37, "context": "For example, Lee filtering [42], a very effective algorithm to filter noise.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "There are two types of scalar quantization techniques, uniform quantization and non-uniform quantization [24].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "In uniform quantization, the input will be separated into the same size intervals, and in non-uniform quantization they are usually of different sizes chosen with an optimization algorithm to minimize the distortion [24].", "startOffset": 216, "endOffset": 220}, {"referenceID": 44, "context": "Interval [0,49] [50,99] [100,149] [150,199] [200,249] [250,255]", "startOffset": 9, "endOffset": 15}, {"referenceID": 45, "context": "Interval [0,49] [50,99] [100,149] [150,199] [200,249] [250,255]", "startOffset": 16, "endOffset": 23}, {"referenceID": 19, "context": "According to the suggestion of Safe RGB Colors [24], we separate each color plane (R, G and B) into six intervals and set the step to 50.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "Besides, in practice, some features of interest can be emphasized by giving more importance (weight) to some pixels in the mask at the expense of others [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "One is a GoogLeNet model trained with the ImageNet dataset [31], which has been taken as the attack target of FGSM in [26].", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "One is a GoogLeNet model trained with the ImageNet dataset [31], which has been taken as the attack target of FGSM in [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 46, "context": "The other is a DNN model trained with the MNIST dataset, which is from an adversarial machine learning library [51] and trained for testing the FGSM attack.", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "ImageNet [31] is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories, while MNIST [4] is a small-scalar dataset of simple gray handwritten digits.", "startOffset": 9, "endOffset": 13}, {"referenceID": 60, "context": "We can compare the predict vectors to find a difference for detection as done in [65], if a trained threshold is available.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "Some attack techniques, such as CW L0 [13], may introduce the large-amplitude perturbation.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Some of them may can be leveraged to further improve our detection method, such as R\u00e9nyi entropy [14], image segmentation [24], etc.", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "Some of them may can be leveraged to further improve our detection method, such as R\u00e9nyi entropy [14], image segmentation [24], etc.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "Many studies have investigated the security of traditional machine learning methods [5] and proposed some attack methods.", "startOffset": 84, "endOffset": 87}, {"referenceID": 40, "context": "Lowd and Meek conduct an attack that minimizes a cost function [45].", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "They further propose attacks against statistical spam filters that add the words indicative of nonspam emails to spam emails [46].", "startOffset": 125, "endOffset": 129}, {"referenceID": 45, "context": "The same strategy is employed in [50].", "startOffset": 33, "endOffset": 37}, {"referenceID": 42, "context": "In [47], a methodology, called reverse mimicry, is designed to evade structural PDF malware detection systems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "In [38], an online learning-based system for detection of PDF malware, PDFRATE, was used as a case to investigate the effectiveness of evasion attacks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "In [6], an algorithm is proposed for evasion of classifiers with differentiable discriminant functions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 39, "context": "[44] demonstrated that client-side classifiers are also vulnerable to evasion attacks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[66] presented a general approach to find evasive variants by stochastically manipulate a malicious sample seed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[21, 64] developed a new form of model inversion attack which can infer sensitive features used in decision tree models and recover images from some facial recognition models by exploiting confidence values revealed by the target models.", "startOffset": 0, "endOffset": 8}, {"referenceID": 59, "context": "[21, 64] developed a new form of model inversion attack which can infer sensitive features used in decision tree models and recover images from some facial recognition models by exploiting confidence values revealed by the target models.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "The proposed attack may cause serious privacy disclosure problems [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 59, "context": "More model inversion attacks can be found in [64].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 91, "endOffset": 99}, {"referenceID": 7, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 91, "endOffset": 99}, {"referenceID": 5, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 124, "endOffset": 132}, {"referenceID": 50, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 124, "endOffset": 132}, {"referenceID": 2, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 243, "endOffset": 248}, {"referenceID": 3, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 243, "endOffset": 248}, {"referenceID": 4, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 243, "endOffset": 248}, {"referenceID": 17, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 321, "endOffset": 329}, {"referenceID": 30, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 321, "endOffset": 329}, {"referenceID": 6, "context": "Game-theoretical approaches [11, 12] model the interactions between the adversary and the classifier as a game.", "startOffset": 28, "endOffset": 36}, {"referenceID": 7, "context": "Game-theoretical approaches [11, 12] model the interactions between the adversary and the classifier as a game.", "startOffset": 28, "endOffset": 36}, {"referenceID": 2, "context": "MCSs [7\u20139], as the name suggests, uses multiple classifiers rather than only one to improve classifier\u2019s robustness.", "startOffset": 5, "endOffset": 10}, {"referenceID": 3, "context": "MCSs [7\u20139], as the name suggests, uses multiple classifiers rather than only one to improve classifier\u2019s robustness.", "startOffset": 5, "endOffset": 10}, {"referenceID": 4, "context": "MCSs [7\u20139], as the name suggests, uses multiple classifiers rather than only one to improve classifier\u2019s robustness.", "startOffset": 5, "endOffset": 10}, {"referenceID": 28, "context": "[33] present family-based ensembles of classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In [22], the method weight evenness via feature selection optimization is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [35], the features are reweighted inversely proportional to their corresponding importance, making it difficult for the adversary to exploit the features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 121, "endOffset": 133}, {"referenceID": 44, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 121, "endOffset": 133}, {"referenceID": 56, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 121, "endOffset": 133}, {"referenceID": 29, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 163, "endOffset": 167}, {"referenceID": 38, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 189, "endOffset": 193}, {"referenceID": 23, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 216, "endOffset": 220}, {"referenceID": 29, "context": "[34] proposed a method to craft adversarial audio examples using the gradient information of the model\u2019s loss function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[43] proposed a method to craft adversarial text examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[28] presented a method to craft adversarial examples on neural networks for malware classification, by adapting the method originally proposed in [53].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[28] presented a method to craft adversarial examples on neural networks for malware classification, by adapting the method originally proposed in [53].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "[37] demonstrated that the adversarial images obtained from a cell-phone camera can still fool an ImageNet classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[56] presented an attack method to fool facial biometric systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[57] developed a novel black-box membership inference attack against machine learning models, including DNN and non-DNN models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 29, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 44, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 48, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 58, "context": "[63] integrated a data transformation module right in front of a standard DNN to improve the model\u2019s resistance to adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[62] also proposed another method, named random feature nullification, for constructing adversary resistant DNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "in a very recent study [65] proposed a method, called Feature Squeezing, to detect adversarial examples in a similar way as ours.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "[27] put forward a defense to detect adversarial examples using statistical tests.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[29] used a large number of adversarial examples to train a detector to identify unknown adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "To a large extent, the performance of the above two detection techniques [27, 29] also depends on how much effectual adversarial examples are available.", "startOffset": 73, "endOffset": 81}, {"referenceID": 24, "context": "To a large extent, the performance of the above two detection techniques [27, 29] also depends on how much effectual adversarial examples are available.", "startOffset": 73, "endOffset": 81}, {"referenceID": 15, "context": "[20] devised two novel features to detect adversarial examples based on the idea that adversarial examples deviate the true data manifold.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Deep neural networks (DNNs) play a key role in many applications. Unsurprisingly, they also became a potential attack target of adversaries. Some studies have demonstrated DNN classifiers can be fooled by the adversarial example, which is crafted via introducing some perturbations into an original sample. Accordingly, some powerful defense techniques were proposed against adversarial examples. However, existing defense techniques requiremodifying the target model or depend on the prior knowledge of attack techniques to different degrees. In this paper, we propose a straightforward method for detecting adversarial image examples. It doesn\u2019t require any prior knowledge of attack techniques and can be directly deployed into unmodified off-the-shelf DNN models. Specifically, we consider the perturbation to images as a kind of noise and introduce two classical image processing techniques, scalar quantization and smoothing spatial filter, to reduce its effect. The image two-dimensional entropy is employed as a metric to implement an adaptive noise reduction for different kinds of images. As a result, the adversarial example can be effectively detected by comparing the classification results of a given sample and its denoised version. Thousands of adversarial examples against some state-of-the-art DNN models are used to evaluate the proposed method, which are crafted with different attack techniques. The experiment shows that our detection method can achieve an overall recall of 93.73% and an overall precision of 95.47% without referring to any prior knowledge of attack techniques.", "creator": "LaTeX with hyperref package"}}}