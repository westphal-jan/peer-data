{"id": "1705.08386", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Better Text Understanding Through Image-To-Text Transfer", "abstract": "Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.", "histories": [["v1", "Tue, 23 May 2017 16:06:32 GMT  (2837kb,D)", "https://arxiv.org/abs/1705.08386v1", null], ["v2", "Fri, 26 May 2017 08:08:20 GMT  (2837kb,D)", "http://arxiv.org/abs/1705.08386v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["karol kurach", "sylvain gelly", "michal jastrzebski", "philip haeusser", "olivier teytaud", "damien vincent", "olivier bousquet"], "accepted": false, "id": "1705.08386"}, "pdf": {"name": "1705.08386.pdf", "metadata": {"source": "CRF", "title": "Better Text Understanding Through Image-To-Text Transfer", "authors": ["Karol Kurach", "Sylvain Gelly", "Michal Jastrzebski", "Philip Haeusser", "Olivier Teytaud", "Damien Vincent", "Olivier Bousquet"], "emails": [], "sections": [{"heading": null, "text": "1 IntroductionThe problem of finding good representations of text data is a very actively researched area of research. Many models are able to learn representations directly by optimizing the end-to-end task in a monitored manner. However, this often requires an enormous amount of labeled data, which are not available in many practical applications and collecting such data can be very costly. A common solution that requires an order of magnitude is the reuse of upstream embedding. A large part of the work in this space focuses on the formation of embedding from pure text data. However, there are many types of relationships and coexistence that are difficult to grasp from plain text. Instead, they appear naturally in other modalities, such as images. In particular, from a similarity measure in the image space and pairs of images and sentences, a similarity measure for sentences can be induced, as in Figure 1.In this work, we examine how embedding text that are good."}, {"heading": "2 Related Work", "text": "Many papers explore the use of multimodal data, especially image and text pairs. Most of them explore how these data pairs can be used for tasks that require knowledge of both modalities, such as captions or image retrievals [1, 7, 11, 12, 29, 8, 26]. While this work line is very interesting, since joint embedding can be applied directly to image captions or image retrieval tasks, the direct use of text embedding for NLP tasks, where images are used only as supplementary training data, is less explored. [12] Propose expanding the Skip-gram algorithm [16] to integrate image data. [4] Also, in the original Skip-gram algorithm, each word has been optimized to increase the probability of adjacent words tied to the central word. In addition to predicting contextual words, [12] s models maximize the similarity between image and word embedding."}, {"heading": "3 VETE Models", "text": "Our setup aims at the direct transfer of knowledge between image and text, and the main goal is to find reusable sentence embeddings. (...) We use the paired data, consisting of images and text, to describe them. (...) We propose a model that consists of two separate encoders - one for images and another for text. (...) An overview of the archaic is shown in Figure 2. For the text encoder, we look at three families of models that combine words in text representations. (BOW) model, the sentence is simply a normalized sum of vectors corresponding to each word. (...) For the RNN model, we create a stacked recurrent neural network encoder (LSTM or GRU). Finally, the CNN model includes a revolutionary layer followed by a fully connected layer, as described in [9]."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Training Datasets", "text": "In our experiments, we are looking at three training datasets: MS COCO, SBU and Pinterest5M. They are described in detail below. An important note is that we are modifying datasets that contain multiple captions (MS COCO and Pinterest5M) in order to retain only one caption, but this was done to prevent the network from \"cheating\" by using the image feature vector only as a way to connect text pairs. To our knowledge, we are the first to notice this problem when evaluating the quality of multimodal image-text models. It is well known that training similarity models produce good results directly on text-text pairs [30], but here we just want to examine the effect of knowledge transfer from captions. MS COCO The MS COCO dataset [13] contains 80 image categories. Five high-quality captions are provided for each image. We are using the MS COCO 2014 dataset as a data set with the same test / validation."}, {"heading": "4.2 Hyperparameter selection and training", "text": "The performance of each machine learning model depends to a large extent on the choice of its hyperparameters. To compare our approach fairly with previous work, we follow the same hyperparameter search protocol for all models. We choose the average value of SemEval 2016 (c.f. Section 4.3) as a validation metric and call it \"avg2016.\" Algorithm 1: Hyperparameter search protocol. For i = 1.2,..., 100 doSample a set of hyperparameters in the allowed ranges; conduct training and evaluate them on \"avg2016\"; finish reporting the results of all benchmarks of the model, which has the highest score on \"avg2016.\" If a hyperparameter in two models has a similar meaning (e.g. learning rate, initialization scale, LZ decay, etc.), the ranges searched for have been set to the same values. Furthermore, we have ensured that the parameters reported by the authors are contained in ranges COB 10, SO 5, SB, SO 5, we train with all models."}, {"heading": "4.3 Evaluation", "text": "Our goal is to create good text embedding that encodes knowledge from corresponding images. To evaluate the effect of this knowledge transfer from images to text, we use a set of textual semantic similarity data sets from the SemEval competitions in 2014 and 2015 [18]. Unfortunately, we were unable to directly compare our models with the Gold RP10K data set introduced by [15] because it was not publicly published. In addition, we use two additional custom test sets: COCO test and Pin test. These were created from the MS COCO or Pinterest 5M test sets by randomly stitching 1,000 semantically related captions (from the same image) and 1,000 unrelated captions from different images. Unlike the SemEval data sets, the similarity value in this case is binary. The goal was to verify performance using the same task as SemEval, but with data from the same word distribution as our training data sets."}, {"heading": "4.4 Results", "text": "This allows us to compare only the algorithms, not the data used. In Section 5.3, we analyze the robustness of our methods on two additional data sets (SBU and Pinterest5M).As a direct comparison, we set ModelA as in Section 5.3, which we call PinModelA. Our implementation uses a pre-built InceptionV3 network for visual feature extraction, as in the VETE models. To understand the effects of adding information from images to text data, we also evaluate two models that are purely text-based: \u2022 RNN-based language model learns sentence embedding via an RNN-based language model. It corresponds to the PureTextRnn baseline of Word2Vec We have Word2Vec Word2Vec word embedding [21], where the corpus consists of sentences from MS COCOCOCOCOCOCOCOCOM."}, {"heading": "5 Ablation studies", "text": "In order to analyze the effects of various components of our architecture, we perform ablation studies on the text encoder used, the loss type and the training data set. We also examine the effects of the training at the word or sentence level. In all cases, we follow a protocol similar to Section 4.2: Algorithm 2: Protocol for Hyperparameter Ablation Studies. Randomly, we generate 100 combination sets for all hyperparameters.; for hyperparameters p (e.g. \"loss type\") dofor v within the permitted value range for p do Perform the training using the 100 hyperparameter sets, leaving p = v unchanged.; finish selecting the best ones using the validation metric \"avg2016\" and report the results.;"}, {"heading": "5.1 Encoder", "text": "The results are summarized in Table 3. \"RNN-GRU\" and \"RNN-LSTM\" denote RNN encoders with GRU [2] and LSTM [5] cells, respectively. For BOW, we try two options: either we use the sum or the mean of the word embedding. Both bag-of-words encoders perform better than RNN encoders, although RNNs on the test data, which have the same distribution as the training data, are slightly better. Encoder images2014 images2015 COCO-Test Pin-Test RNN-GRU 0.834 0.821 0.906 0.507RNN-LSTM 0.838 0.835 0.901 0.549 BOW-SUM 0.860 0.853 0.873 BOW-MEAN 0.861 0.579Table 3: Results of the application of this text loss on the model VCOW-SUM 0.853 0.573 BOW-Tank-MEAN 0.8570.5794."}, {"heading": "5.2 Loss type", "text": "In this section we describe different loss types with which we trained our model. \u2022 Pearson correlation: measures the linearity of the connection between two variables, which is estimated on the basis of a sample; it is defined as \u03c1 (x, y) = Cov (x, y) Std (y) Std (y). \u2022 Surrogate Kendall: The Pearson correlation only takes linear dependencies into account. To mitigate this, we experimented with the Kendall correlation, which is only disease-dependent. Unfortunately, it is not differentiable, so we used its differentiable approximation: SKT\u03b1 [6], defined as SKT (x, y) = j (1 > j)."}, {"heading": "5.3 Dataset", "text": "The results of the training of the model on MS COCO, SBU and Pinterest5M datasets are shown in Table 5. Each cell of the table contains an average of 4 evaluation datasets (Figures 2014, Figures 2014, COCO-Test, Pin-Test).The quality of the captions varies considerably between the datasets, as can be seen in Figure 3. However, we conclude that the relationship between the models is maintained, that is: regardless of the dataset used for training, PinModelA is always worse than VETE-RNN, which is worse than VETE-BOW."}, {"heading": "5.4 Sentence-level vs word-level embedding", "text": "In our work, we learn sentence embeddings as a whole, but the most powerful text encoder turned out to be BOW. This raises the following question: could the model work equally well if we train it at the word level and then only combine word embeddings during the inference? Comparison of these two approaches is presented in Table 6, which clearly shows the benefits of sentence level training. This effect should be investigated further, but while we train word embeddings separately, each of them forces us to be close to the corresponding images, sentence level training offers the possibility of the word embeddings complementing each other, each of them explaining a part of the picture and capturing coexistences."}, {"heading": "6 Conclusion", "text": "We have shown that VETE, a simple approach that directly optimizes text embedding to fit corresponding image representations, outperforms previous multimodal approaches, which are sometimes more complex and optimize word embedding as opposed to sentence embedding. We have also shown that our proposed models can generate very competitive embedding even for relatively complex similarity tasks at sentence level, even compared to more complex models trained to scale more text data, especially when vocabulary is associated with visual concepts. To our initial surprise, state-of-the-art encoder models, such as LSTMs, perform significantly worse than much simpler encoders, such as bag-of-word models. Although they achieve better results when evaluated on the same data distribution, their embedding does not translate well to other text distributions."}], "references": [{"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. d. Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "Journal of machine learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Flickr", "author": ["D. Defreyne"], "venue": "https://www.flickr.com/photos/denisdefreyne/1091487059,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what i mean", "author": ["F. Hill", "A. Korhonen"], "venue": "EMNLP, pages 255\u2013265,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Content-based medical image retrieval with metric learning via rank correlation", "author": ["W. Huang", "K.L. Chan", "H. Li", "J. Lim", "J. Liu", "T.Y. Wong"], "venue": "F. Wang, P. Yan, K. Suzuki, and D. Shen, editors, Machine Learning in Medical Imaging, First International Workshop, MLMI 2010, Held in Conjunction with MICCAI 2010, Beijing, China, September 20, 2010. Proceedings, volume 6357 of Lecture Notes in Computer Science, pages 18\u201325. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2407\u20132414. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Empirical Methods in Natural Language Processing, pages 1746\u20131751,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "CoRR, abs/1501.02598,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision, pages 740\u2013755. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations), pages 55\u201360,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Training and evaluating multimodal word embeddings with large-scale web annotated images", "author": ["J. Mao", "J. Xu", "K. Jing", "A.L. Yuille"], "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 442\u2013450. Curran Associates, Inc.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "In Hlt-naacl,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Flickr", "author": ["J. Moes"], "venue": "https://www.flickr.com/photos/jeroenmoes/4265223393,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Flickr", "author": ["F. Rosa"], "venue": "https://www.flickr.com/photos/kairos_of_tyre/6318245758,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["C. Shallue"], "venue": "https://github.com/ tensorflow/models/tree/master/im2txt,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818\u20132826,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Order-embeddings of images and language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR, abs/1609.06647,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep structure-preserving image-text embeddings", "author": ["L. Wang", "Y. Li", "S. Lazebnik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5005\u20135013,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "arXiv preprint arXiv:1511.08198,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Photo credit: [17, 22] The problem of finding good representations of text data is a very actively studied area of research.", "startOffset": 14, "endOffset": 22}, {"referenceID": 20, "context": "Photo credit: [17, 22] The problem of finding good representations of text data is a very actively studied area of research.", "startOffset": 14, "endOffset": 22}, {"referenceID": 11, "context": "This extends previous works, for example [12] or [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "This extends previous works, for example [12] or [15].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "Despite its simplicity, the model significantly outperforms pure-text models and the best multimodal model from [15] on a set of well established text similarity benchmarks from the SemEval competition [18].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 6, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 10, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 11, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 27, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 7, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 24, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 11, "context": "[12] propose to extend the skip-gram algorithm [16] to incorporate image data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[12] propose to extend the skip-gram algorithm [16] to incorporate image data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "[4] also took a similar approach before.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "In addition to predicting contextual words, [12]\u2019s models maximize the similarity between image and word embeddings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "To inject visual information, [12] add a max margin loss between the image embedding and the word embedding:", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "[11] also use a max margin loss to co-embed images and corresponding text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Photo credit: [3, 17, 22]", "startOffset": 14, "endOffset": 25}, {"referenceID": 16, "context": "Photo credit: [3, 17, 22]", "startOffset": 14, "endOffset": 25}, {"referenceID": 20, "context": "Photo credit: [3, 17, 22]", "startOffset": 14, "endOffset": 25}, {"referenceID": 14, "context": "For example, [15] use an Recurrent Neural Network (RNN) as a language model in order to learn word embeddings which are then combined to create a phrase embedding.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "For their model A, they use a similar setup as the captioning model from [27], with an RNN decoder conditioned on a pre-trained CNN embedding.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "The initial state is set to a transformation of the last internal layer of a pre-trained VGGNet [24].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Finally, model C develops the multimodal skip-gram [12] by adding an additional loss measuring the distance between the word embeddings and vimage.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Finally, for the CNN model, the encoder includes a convolutional layer followed by a fully connected layer, as described in [9].", "startOffset": 124, "endOffset": 127}, {"referenceID": 23, "context": "For encoding images, we use a pre-trained InceptionV3 network [25] which provides a 2048dimensional feature vector for each image in the dataset (images are rescaled and cropped to 300 x 300 px).", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "It is known that training similarity models directly on text-text pairs yields good results [30] but here we want to investigate only the effect of knowledge transfer from images.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "MS COCO The MS COCO dataset [13] contains 80 image categories.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "We use the MS COCO 2014 dataset using the same train/validation/test split as the im2txt [23] Tensorflow implementation of [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 26, "context": "We use the MS COCO 2014 dataset using the same train/validation/test split as the im2txt [23] Tensorflow implementation of [28].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "SBU The Stony Brook University dataset[19] consists of 1M image-caption pairs collected from Flickr, with only one caption per image.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Pinterest5M The original Pinterest40M dataset [15] contains 40M images.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "The training data in all datasets is lowercased and tokenized using the Stanford Tokenizer [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "In all models, we train using the Adam optimizer[10], for 10 (MS COCO, SBU) or 5 epochs (Pinterest5M).", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Unfortunately, we could not compare our models directly on the Gold RP10K dataset introduced by [15] as it was not publicly released.", "startOffset": 96, "endOffset": 100}, {"referenceID": 14, "context": "As a direct comparison, we implementModelA as described in [15], which we refer to as PinModelA.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "It corresponds to the PureTextRnn baseline from [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "\u2022 Word2Vec We trained Word2Vec word embeddings [21] where the corpus consists of sentences from MS COCO.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Similarly to [30], we observed that RNN-based encoders are outperformed by a simpler BOW model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "\u2022 Glove: embeddings proposed in [20], trained on a Common Crawl dataset with 840 billion tokens.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "\u2022 M-Skip-Gram: embeddings proposed in [12], trained on Wikipedia and a set of images from ImageNet.", "startOffset": 38, "endOffset": 42}, {"referenceID": 28, "context": "\u2022 PP-XXL: the best embeddings from the [30], trained on 9M phrase pairs from PPDB.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "\u201cRNN-GRU\u201d and \u201cRNN-LSTM\u201d denote RNN encoders with GRU [2] and LSTM [5] cells, respectively.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "\u201cRNN-GRU\u201d and \u201cRNN-LSTM\u201d denote RNN encoders with GRU [2] and LSTM [5] cells, respectively.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "We therefore used its differentiable approximation: SKT\u03b1 [6] defined as SKT\u03b1(x, y) = \u2211 i,j tanh(\u03b1(xi\u2212xj)(yi\u2212yj)) n(n\u22121)/2 for some \u03b1 > 0.", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "We follow closely the definition in [11].", "startOffset": 36, "endOffset": 40}], "year": 2017, "abstractText": "Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.", "creator": "LaTeX with hyperref package"}}}