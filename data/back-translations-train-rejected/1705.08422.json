{"id": "1705.08422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach", "abstract": "Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient's physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient's physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6% over observed clinical policies, from a baseline mortality of 13.7%. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.", "histories": [["v1", "Tue, 23 May 2017 17:13:11 GMT  (154kb,D)", "http://arxiv.org/abs/1705.08422v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aniruddh raghu", "matthieu komorowski", "leo anthony celi", "peter szolovits", "marzyeh ghassemi"], "accepted": false, "id": "1705.08422"}, "pdf": {"name": "1705.08422.pdf", "metadata": {"source": "CRF", "title": "Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach", "authors": ["Aniruddh Raghu", "Matthieu Komorowski", "Leo Anthony Celi", "Peter Szolovits"], "emails": ["ARAGHU@MIT.EDU", "MKOMO@MIT.EDU", "LCELI@MIT.EDU", "PSZ@MIT.EDU", "MGHASSEM@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules that they are able to play by."}, {"heading": "2. Background and Related Work", "text": "In this section, we outline important reinforcement learning algorithms that are used in the work and motivate our approach compared to previous work."}, {"heading": "2.1 Reinforcement Learning", "text": "Reinforcement learning (RL) models time-variating state spaces with a Markov Decision Process (Q = Q = Q), where Q (MDP), in which at any time an agent observes the current state of the environment, takes an action at the permitted set of measures A = {1,.., M}, receives a reward rt, and then transitions to a new state st + 1. The agent selects actions at any point in time that maximize his expected discounted future reward, or return, defined as Rt = T \u2032 = t \"\u2212 trt,\" where he picks the compromise between immediate and future rewards, and T is the terminal fair value that maximizes the expected future reward, or return that is achieved after the execution of a measure in a state; that is, the execution of a measure in a state s and progression is optimal."}, {"heading": "2.2 Reinforcement Learning in Health", "text": "Many previous work in the field of clinical machine learning has focused on supervised learning methods for diagnosis (Esteva et al., 2017) and risk stratification (N.Razavian et al., 2015).The inclusion of time in a supervised environment could be implicit within the functional spatial design (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or be captured with multiple models at different times (Fialho et al., 2013; Ghassemi et al., 2014).We prefer RL for sepsis treatment over supervised learning, as the basic truth of the \"good\" treatment strategy is unclear in the medical literature (Marik, 2015).Importantly, RL algorithms derive optimal strategies from training examples that do not represent optimal behaviour.RL is well suited for identifying ideal septic treatment strategies as clinicians deal with a sparse, time-delayed reward signal in patients."}, {"heading": "3. Data and Preprocessing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Cohort", "text": "The data for these patients comes from the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC-III v1.4) database (Johnson et al., 2016), which is publicly available and includes hospitalizations of approximately 38,600 adults (at least 15 years old). We identified a group of patients who meet the sepsis 3 criteria (Singer et al., 2016) and found that the summary information on populations among sepsis survivors and mortality rates is similar (Table 1)."}, {"heading": "3.2 Feature Preprocessing", "text": "For each patient, we extracted relevant physiological parameters, including demographics, laboratory values, vital signs, and intake / output events. Data were aggregated into 4-hour windows, recording the mean or sum (as required) when multiple data points were present in a window. Excessive absence variables were removed, and any remaining missing values were offset against k-nearest neighbors, creating a 47 x 1 trait vector for each patient in each time period. Values that exceeded clinical limits were capped, and capped data per feature were normalized to zero mean and variance per unit. See Appendix 8.3 for a full feature list."}, {"heading": "3.3 Action Discretization", "text": "We defined a 5 \u00d7 5 action room for the medical interventions that covered the space of the intravenous (intravenous) fluid (volume adjusted for fluid tonicity) and the maximum dosage of the vasopressor (VP) within a given time frame of 4 hours. The action space was limited to these two interventions because both drugs are extremely important for the management of septic patients, but there is no agreement on when and how much of each drug should be administered (Marik, 2015). We determined the action space into quarters per drug based on all non-zero dosages of the two drugs and converted each drug at any point in time into an integer representing its quarter container."}, {"heading": "4. Methods", "text": "The challenge in applying RL to the optimal drug dosage is that all available data is scanned offline; that is, data is collected beforehand and models can only be adapted to a retrospective dataset. In an RL context, this restricts the exploration of the state space in question and makes it difficult to learn the truly \"optimal\" policy. This constraint motivates the trial of different approaches with different modeling constraints to determine the best drug strategy for patients. We focus on non-political RL algorithms that learn optimal policy based on data generated by adhering to alternative policies. This makes sense to our problem because the available data is generated from a policy pursued by physicians, but our goal is to learn a different, optimal policy, rather than evaluating physician policies. We propose deep models with continuous state spaces and discredited action to retain more of the underlying state representation."}, {"heading": "4.1 Discretized State-space and Discretized Action-space", "text": "Following Komorowski et al. (2016), we create a basic model with discredited states and spaces of action with the aim of grasping the underlying representation and simplifying the learning procedure. We use this approach to evaluate the performance of other techniques and to understand the importance of the Q values learned. We use the SARSA algorithm (Rummery and Niranjan, 1994) to learn Q\u03c0 (s, a) and the action value function for medical policy (more detailed in Appendix 8.4)."}, {"heading": "4.2 Continuous State-spaces", "text": "Continuous state models directly capture the physiological state of a patient and enable us to discover high-quality treatment strategies. To learn an optimal strategy with continuous state vectors, we use neural networks to approximate the optimal action value function, Q \u043a (s, a)."}, {"heading": "4.2.1 MODEL ARCHITECTURE", "text": "Our model is based on a variant of Deep Q Networks (Mnih et al., 2015). Deep Q Networks try to minimize a square error loss between the output of the network, Q (s, a; \u03b8), and the desired target, Qtarget = r + \u03b3maxa \u2032 Q (s, \"a,\" \u03b8), with tuples of the form < s, a, r, s \">. The network has results for all the different actions that can be taken - for all a\" A = {1, \"., M.\" Specifically, the parameters that we use are found so that the results of the actions that we have taken turn out to be such that the results of the actions that we can take. [(Qtarget \u2212 Q (s, a \";.) 2] In practice, the expected loss due to stochastic batch descent is minimized. However, this method may be unstable due to the non-stationary nature of the target values."}, {"heading": "4.3 Autoencoder Latent State Representation", "text": "We studied both conventional autoencoders (Bengio, 2009) and sparse autoencoders (Ng, 2011) to generate latent state representations of physiological state vectors and simplify the learning problem. Sparse autoencoders were trained with an additional term in the loss function to promote scarcity. Our autoencoder models all had a single hidden layer that was used as latent state representation, and these latent state representations were used as inputs for dueling DQN (Section 4.2.1)."}, {"heading": "5. Evaluation", "text": "The evaluation of models outside politics is difficult because it is difficult to estimate whether the introduction of learned policies (based on learned policies to determine actions in each state) would ultimately lead to lower patient mortality. Furthermore, the direct comparison of Q values on non-policy data, as in previous applications of RL in health care (Komorowski et al., 2016), can lead to incorrect performance estimates (Jiang and Li, 2015). The evaluation of the average Q value as in Komorowski et al. (2016) is suboptimal because the Q\u03c0 used for evaluation represents the expected return if one acts optimally at the state border, but then proceeds according to medical policy. In this paper, we propose to evaluate the learned strategies with several approaches."}, {"heading": "5.1 Discounted Returns vs. Mortality", "text": "To understand how the expected discounted yield affects mortality, we assign the Q values obtained via SARSA on the test kit in separate buckets, and assign a 1-1 label to each bucket if it is part of a patient's history of death. If the patient survives, we assign a 0 label. These labels represent the basic truth, since we know the actual outcome of patients if the doctor's guidelines are followed. We calculate the average mortality in each bucket, which enables us to establish an empirically derived function of the ratio of mortality to expected yield (Figure 1)."}, {"heading": "5.2 Off-Policy Evaluation", "text": "We use the Doubly Robust Off-Policy Value Evaluation method (Jiang and Li, 2015) to obtain an unbiased estimate of the value of the optimal policies learned, using data collected outside of politics (the trajectories in our training set); for each trajectory, we calculate an unbiased estimate of the value of the policies learned, V-HDR, and the average of the results obtained over the trajectories observed; for more details, see Jiang and Li (2015); and we can also calculate the mean discounted return of selected interventions within the framework of medical policy; using these estimates and the empirically determined ratio of mortality to expected return, we can evaluate the potential improvement that our policies could bring in reducing patient mortality; this method allows us to accurately compare the non-policy returns obtained through various methods and estimate the mortality that we would observe if we learned the outcomes from a direct comparison (the one Li is likely to follow)."}, {"heading": "5.3 Qualitative Examination of Treatment Policies", "text": "We examine the overall choice of treatment proposed by the optimal policy to gain a better clinical understanding and compare these decisions with those made by physicians to understand how differences in the policies chosen contribute to patient mortality."}, {"heading": "6. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Fully Discretized Models are Well-calibrated with Test Set Mortality", "text": "Figure 1 shows the share of mortality in the expected return for the medical policy on the test set. Note that Rmax = 15 is the reward given at terminal times. As shown, we observe high mortality at low yield and low mortality at high yield. We also confirm that the empirically derived mortality for the medical policy corresponds to the actual share of mortality in the test set. For empirically derived mortality, we calculate the expected return for the physician on the test set to an average of 13.9 \u00b1 0.5%, which corresponds to the actual share of mortality on the test set (13.7%)."}, {"heading": "6.2 Continuous State-space Models", "text": "We present the results for the two proposed networks: the Dueling Double-Deep Q Network (Dueling DDQN) and the Sparse Autoencoder Dueling DDQN. For clarity reasons, these are referred to as the normal Q-N model or the autoencoded Q-N model."}, {"heading": "6.2.1 QUANTITATIVE VALUE ESTIMATE OF LEARNED POLICIES", "text": "As described in Section 5.2, we first obtain unbiased estimates of the value of our learned guidelines based on the test data. The expected results are V-PhysicianDR, V-Normal Q-N DR and V-Normal Q-N DR. We estimate mortality among the individual guidelines based on Figure 1. As shown, autocoding Q-N guidelines has the lowest estimated mortality and could reduce patient mortality by up to 4%. We examine a histogram of mortality against the first two main components of the sparse representation (Figure 2) and observe a clear gradient of mortality numbers indicating how the hidden state of the autocoder can provide a rich representation of the physiological state that leads to better guidelines."}, {"heading": "6.2.2 QUALITATIVE EXAMINATION OF LEARNED POLICIES", "text": "Figure 3 shows what the three policies - physician, normal Q-N, and autoencoded Q-N. The action figures indicate the various discrete actions selected over a certain period of time, and the diagrams show aggregated actions taken across all patient histories. Action 0 does not refer to medications administered to the patient at that time, and increasing actions refer to higher drug doses, with drug doses represented by quarterly numbers. As shown, this result is reasonable; although vasopressors are often used in intensive care to increase mean arterial pressure (note the high density of measures corresponding to the vasopressor dose = 0), many patients with sepsis are not hypotensive and therefore do not need vasopressors. In addition, there are few controlled clinical trials that increase average drug pressure (drug pressure, but no medication)."}, {"heading": "6.2.3 QUANTIFYING OPTIMALITY OF LEARNED POLICIES", "text": "Figure 4 shows the correlation between 1) the observed mortality and 2) the difference between the optimal doses proposed by the policy and the actual doses given by physicians. Dosage differences at individual times have been pooled and mortality rates aggregated. We observe consistently low mortality rates when the optimal dose and the actual dose match, i.e. at a difference of 0, indicating the validity of the policy learned. Then, the observed mortality rate increases with the difference between the optimal dose and the true dose. Less reliable results are when the optimal dose and the doctor's dose differ by larger quantities.Both models seem to learn useful strategies for vasopressors, with a large increase in observed mortality under the Q-N autocoding due to relatively fewer cases in the test kit where the optimal dose and the given dose positively differed by a large amount."}, {"heading": "7. Conclusion", "text": "In this paper, we explored methods of applying deep reinforcement learning (RL) to the problem of deriving optimal medical treatments for patients with sepsis. There remain many interesting areas that need to be studied. Firstly, the credit allocation in this model is quite sparse, with rewards / penalties issued only on terminal conditions. Here, there is scope for improvement; one idea could be to use a clinically informed reward function based on patient blood counts to learn better strategies. Another approach could be to use inverse RL techniques (Abbeel and Ng, 2010) to derive an appropriate reward function based on the actions of experts (the doctors). As our database of patient pathways is collected to capture the actions of many different physicians, this approach could allow us to derive a more appropriate reward function and, in turn, learn a better model. Our contributions build on recent work by Komowski et al to investigate the optimal outcome of treatment al, 2016, 2016 and 2017."}, {"heading": "Acknowledgments", "text": "This research was partially funded by the Intel Science and Technology Center for Big Data and the National Library of Medicine Biomedical Informatics Research Training Grant (NIH / NLM 2T15 LM007092-22)."}, {"heading": "8. APPENDICES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Cohort definition", "text": "In accordance with the latest guidelines, sepsis was defined as suspected infection (prescribing antibiotics and taking bodily fluids for microbiological culture), combined with evidence of organ dysfunction, defined by a sequential organ failure assessment (SOFA) value greater than or greater than 2 (Singer et al., 2016). We assumed a baseline SOFA of zero for all patients. In defining the cohort, we respected the time criteria for diagnosing sepsis: the first microbiological sample must have been administered within 72 hours, and the first antibiotic dose must have been taken within 24 hours (Singer et al., 2016). The earliest event defined the onset of sepsis. We excluded patients who did not receive intravenous fluid, and those with missing data for 8 or more of the 47 variables. This method yielded a cohort of 17,898 patients."}, {"heading": "8.2 Data extraction", "text": "MIMIC-III was queried using pgAdmin 4. Raw data was extracted for all 47 features and processed in Matlab (version 2016b). Data was collected from up to 24 hours before to 48 hours after the onset of sepsis to capture the early stage of treatment including initial resuscitation, i.e. the period of time that is of interest. Features were converted into multidimensional time series with a time resolution of 4 hours. As a result of the interest, mortality in hospital was recorded."}, {"heading": "8.3 Model Features", "text": "The physiological characteristics used in our model are presented below: Demography / Static Shock Index, Elixhauser, SIRS, Gender, Resumption, GCS - Glasgow Coma Scale, SOFA - Sequential Organ Failure Assessment, AgeLab Values Albumin, Arterial pH, Calcium, Glucose, Hemoglobin, Magnesium, PTT - Partial Thromboplastin Time, Potassium, SGPT - Serum Glutamic-Pyruvic Transaminase, Arterial Blood Gases, BUN - Blood-urea-Nitrogen, Chloride, Bicarbonate, INR - International Normalized Ratio, Sodium, Arterial Lactate, CO2, Creatinine, Ionized Calcium, PT - Prothrombin Time, FileSignet Count, SGOT - Serum Glutamic-Oxygen-Oxygen-Oxygen Ratio, Sputamic-Oxygen, Sputamic Acid, Arthrolytic, Arthritis, Arthrombin Time, Output, Oxygen-oxygen Amount."}, {"heading": "8.4 Discretized State and Action Space Model", "text": "Here we present how the discredited model was built."}, {"heading": "8.4.1 STATE DISCRETIZATION", "text": "As in the continuous case, the data is divided into a training set (80%) and a sustained test set (20%) by selecting a proportional number of patient histories for each set. These sets have been reviewed to ensure that they provide an accurate representation of the entire data set, in terms of the distribution of results and some demographic characteristics. We apply k-mean clusters to the training set and discredit the states in 1250 clusters. As in Komorowski et al. (2016), we use a simple, sparse reward function that issues a reward of + 15 at a given time if a patient survives, -15 if he dies, and 0 otherwise."}, {"heading": "8.4.2 SARSA FOR PHYSICIAN POLICY", "text": "To learn the action value function associated with the model, we used an offline SARSA approach using the Bellman optimality equation by randomly sampling trajectories from our training set and using tuples of the form < s, a, r, s, a \u2032 > to update the action value function: Q (s, a) \u2190 Q (s, a) + \u03b1 * [r + \u03b3Q (s \u2032, a \u2032) - Q (s, a)] Here is (s, a) the tuple currently under consideration (s, a) a tuple representing the next state and action, \u03b1 is the learning rate and \u03b3 is the discount factor. Since our state and action space in this model are both finite, we represent the Q function using a table of rows for each tupel (s, a). This learned function was then used in the model evaluation - after convergence it represents \u03c0 (Qs \u00b2, a \u00b2, a \u00b2, a \u00b2, T is where a)."}, {"heading": "8.5 Continuous Model Architecture and Implementation Details", "text": "Our final network architecture had two hidden layers of size 128, using batch normalization (Ioffe and Szegedy, 2015) after each leaky ReLU activation function, a division into equally large benefit and value flows, and a projection onto the action space by combining these two flows. Mathematically, the activation function is described by: f (z) = max (z, 0.5z), where z is the input to a neuron. This choice of activation function is motivated by the fact that Q values can be positive or negative, and standard ReLU, tanh, and sigmoid activations appear to lead to saturation and \"activated neurons\" in the network. Appropriate function scaling helped alleviate this problem, as did the output of rewards of \u00b1 15 at terminal times to stabilize the model.We added a regulation term to the standard Q network loss that allowed network losses (the network losses were punished)."}, {"heading": "8.6 Autoencoder Implementation Details", "text": "For the autoencoder, a desired sparseness \u03c1 is selected and the weights of the autoencoder are adjusted in such a way that Lsparse (\u03b8) = Lreconstruction (\u03b8) + \u03b2 \u2211 n j = 1 KL (\u03c1 | | \u03c1j) is minimized. Here, n is the total number of hidden neurons in the network, \u03c1j is the actual output of neuron j, \u03b2 is a hyperparameter controlling the thickness of the sparseness date, KL (\u00b7 | | \u00b7) is the KL divergence, and Lreconstruction is the loss for a normal autoencoder."}], "references": [{"title": "Inverse Reinforcement Learning, pages 554\u2013558", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "ISBN 978-0-387-30164-8", "citeRegEx": "Abbeel and Ng.,? \\Q2010\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2010}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Sepsis: a roadmap for future research", "author": ["J. Cohen", "J.-L. Vincent", "N.K.J. Adhikari", "F.R. Machado", "D.C. Angus", "T. Calandra", "K. Jaton", "S. Giulieri", "J.Delaloye", "S. Opal", "K. Tracey", "T. van der Poll", "E. Pelfrene"], "venue": "Lancet Infectious Diseases,", "citeRegEx": "Cohen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2006}, {"title": "Blood pressure targets for vasopressor therapy: A systematic review", "author": ["Frederick D\u2019Aragon", "Emilie P Belley-Cote", "Maureen O Meade", "Fran\u00e7ois Lauzier", "Neill KJ Adhikari", "Matthias Briel", "Manoj Lalu", "Salmaan Kanji", "Pierre Asfar", "Alexis F Turgeon"], "venue": "Shock, 43(6):530\u2013539,", "citeRegEx": "D.Aragon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "D.Aragon et al\\.", "year": 2015}, {"title": "Dermatologistlevel classification of skin cancer", "author": ["A. Esteva", "B. Kuprel", "R.A. Novoa", "J. Ko", "S.M. Swetter", "H.M. Blau", "S. Thrun"], "venue": null, "citeRegEx": "Esteva et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Esteva et al\\.", "year": 2017}, {"title": "Diseasebased modeling to predict fluid response in intensive care units", "author": ["AS Fialho", "LA Celi", "F Cismondi", "SM Vieira", "SR Reti", "JM Sousa", "SN Finkelstein"], "venue": "Methods Inf Med,", "citeRegEx": "Fialho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fialho et al\\.", "year": 2013}, {"title": "Unfolding physiological state: Mortality modelling in intensive care units", "author": ["Marzyeh Ghassemi", "Tristan Naumann", "Finale Doshi-Velez", "Nicole Brimmer", "Rohit Joshi", "Anna Rumshisky", "Peter Szolovits"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Ghassemi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ghassemi et al\\.", "year": 2014}, {"title": "Icu acuity: real-time models versus daily models", "author": ["Caleb W Hug", "Peter Szolovits"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "Hug and Szolovits.,? \\Q2009\\E", "shortCiteRegEx": "Hug and Szolovits.", "year": 2009}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["S. Ioffe", "C. Szegedy"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Doubly Robust Off-policy Evaluation for Reinforcement Learning", "author": ["N. Jiang", "L. Li"], "venue": "CoRR, abs/1511.03722,", "citeRegEx": "Jiang and Li.,? \\Q2015\\E", "shortCiteRegEx": "Jiang and Li.", "year": 2015}, {"title": "MIMIC-III, a freely accessible critical care database", "author": ["A.E.W. Johnson", "T.J. Pollard", "L. Shen", "L.-W.H. Lehman", "M. Feng", "M. Ghassemi", "B. Moody", "P. Szolovits", "L. Anthony Celi", "R.G. Mark"], "venue": "Scientific Data,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Prognostic physiology: modeling patient severity in intensive care units using radial domain folding", "author": ["Rohit Joshi", "Peter Szolovits"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "Joshi and Szolovits.,? \\Q2012\\E", "shortCiteRegEx": "Joshi and Szolovits.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A Markov Decision Process to suggest optimal treatment of severe infections in intensive care", "author": ["M. Komorowski", "A. Gordon", "L.A. Celi", "A. Faisal"], "venue": "In Neural Information Processing Systems Workshop on Machine Learning for Health,", "citeRegEx": "Komorowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Komorowski et al\\.", "year": 2016}, {"title": "Fluid administration in severe sepsis and septic shock, patterns and outcomes: an analysis of a large national database", "author": ["Paul E Marik", "Walter T Linde-Zwirble", "Edward A Bittner", "Jennifer Sahatjian", "Douglas Hansell"], "venue": "Intensive care medicine,", "citeRegEx": "Marik et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Marik et al\\.", "year": 2017}, {"title": "The demise of early goal-directed therapy for severe sepsis and septic shock", "author": ["P.E. Marik"], "venue": "Acta Anaesthesiologica Scandinavica,", "citeRegEx": "Marik.,? \\Q2015\\E", "shortCiteRegEx": "Marik.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "Wierstra D", "S. Legg", "D. Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Vasopressors for shock", "author": ["Marcus M\u00fcllner", "Bernhard Urbanek", "Christof Havel", "Heidrun Losert", "Gunnar Gamper", "Harald Herkner"], "venue": "The Cochrane Library,", "citeRegEx": "M\u00fcllner et al\\.,? \\Q2004\\E", "shortCiteRegEx": "M\u00fcllner et al\\.", "year": 2004}, {"title": "Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach", "author": ["S. Nemati", "M.M. Ghassemi", "G.D. Clifford"], "venue": "In 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),", "citeRegEx": "Nemati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nemati et al\\.", "year": 2016}, {"title": "URL https://web.stanford.edu/class/cs294a/ sparseAutoencoder.pdf", "author": ["A.Y. Ng"], "venue": "Sparse autoencoder,", "citeRegEx": "Ng.,? \\Q2011\\E", "shortCiteRegEx": "Ng.", "year": 2011}, {"title": "Populationlevel prediction of type 2 diabetes from claims data and analysis of risk factors", "author": ["N.Razavian", "S.Blecker", "A.M Schmidt", "A.Smith-McLallen", "S. Nigam", "D.Sontag"], "venue": "Big Data,", "citeRegEx": "N.Razavian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "N.Razavian et al\\.", "year": 2015}, {"title": "A reinforcement learning approach to weaning of mechanical ventilation in intensive care", "author": ["N. Prasad", "L. Cheng", "C. Chivers", "M. Draugelis", "B.E. Engelhardt"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2017}, {"title": "Surviving sepsis campaign: International guidelines for management of sepsis and septic shock: 2016", "author": ["Andrew Rhodes", "Laura E Evans", "Waleed Alhazzani", "Mitchell M Levy", "Massimo Antonelli", "Ricard Ferrer", "Anand Kumar", "Jonathan E Sevransky", "Charles L Sprung", "Mark E Nunnally"], "venue": "Intensive care medicine,", "citeRegEx": "Rhodes et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Rhodes et al\\.", "year": 2017}, {"title": "On-line q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical report,", "citeRegEx": "Rummery and Niranjan.,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan.", "year": 1994}, {"title": "Prioritized Experience Replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "CoRR, abs/1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Informing sequential clinical decision-making through reinforcement learning: an empirical study", "author": ["Susan M Shortreed", "Eric Laber", "Daniel J Lizotte", "T Scott Stroup", "Joelle Pineau", "Susan A Murphy"], "venue": "Machine learning,", "citeRegEx": "Shortreed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shortreed et al\\.", "year": 2011}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "The third international consensus definitions for sepsis and septic shock", "author": ["M. Singer", "C.S. Deutschman", "C. Seymour"], "venue": "(sepsis-3). JAMA,", "citeRegEx": "Singer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singer et al\\.", "year": 2016}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "CoRR, abs/1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Sepsis in European intensive care units: results of the SOAP study", "author": ["J.-L. Vincent", "Y. Sakr", "C.L. Sprung", "V.M. Ranieri", "K. Reinhart", "H. Gerlach", "R. Moreno", "J. Carlet", "J.-R. Le Gall", "D. Payen"], "venue": "Critical Care Medicine,", "citeRegEx": "Vincent et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2006}, {"title": "Cooperative Antimicrobial Therapy of Septic Shock Database Research Group, et al. Interaction between fluids and vasoactive agents on mortality in septic shock: a multicenter, observational study", "author": ["Jason Waechter", "Anand Kumar", "Stephen E Lapinsky", "John Marshall", "Peter Dodek", "Yaseen Arabi", "Joseph E Parrillo", "R Phillip Dellinger", "Allan Garland"], "venue": "Critical care medicine,", "citeRegEx": "Waechter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Waechter et al\\.", "year": 2014}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "CoRR, abs/1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Sepsis (severe infections with organ failure) is a dangerous condition that costs hospitals billions of pounds in the UK alone (Vincent et al., 2006), and is a leading cause of patient mortality (Cohen et al.", "startOffset": 127, "endOffset": 149}, {"referenceID": 2, "context": ", 2006), and is a leading cause of patient mortality (Cohen et al., 2006).", "startOffset": 53, "endOffset": 73}, {"referenceID": 31, "context": "Various fluids and vasopressor treatment strategies have been shown to lead to extreme variations in patient mortality, which demonstrates how critical these decisions are (Waechter et al., 2014).", "startOffset": 172, "endOffset": 195}, {"referenceID": 22, "context": "While international efforts attempt to provide general guidance for treating sepsis, physicians at the bedside still lack efficient tools to provide individualized real-term decision support (Rhodes et al., 2017).", "startOffset": 191, "endOffset": 212}, {"referenceID": 16, "context": "While RL has been used successfully in complex decision making tasks (Mnih et al., 2015; Silver et al., 2016), its application to clinical models has thus far been limited by data availability (Nemati et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 26, "context": "While RL has been used successfully in complex decision making tasks (Mnih et al., 2015; Silver et al., 2016), its application to clinical models has thus far been limited by data availability (Nemati et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 18, "context": ", 2016), its application to clinical models has thus far been limited by data availability (Nemati et al., 2016) and the inherent difficulty of defining clinical state and action spaces (Prasad et al.", "startOffset": 91, "endOffset": 112}, {"referenceID": 21, "context": ", 2016) and the inherent difficulty of defining clinical state and action spaces (Prasad et al., 2017; Komorowski et al., 2016).", "startOffset": 81, "endOffset": 127}, {"referenceID": 13, "context": ", 2016) and the inherent difficulty of defining clinical state and action spaces (Prasad et al., 2017; Komorowski et al., 2016).", "startOffset": 81, "endOffset": 127}, {"referenceID": 16, "context": "1 We focus on continuous state-space modeling, represent a patient\u2019s physiological state at a point in time as a continuous vector (using either raw physiological data or sparse latent state representations), and find optimal actions with Deep-Q Learning (Mnih et al., 2015).", "startOffset": 255, "endOffset": 274}, {"referenceID": 28, "context": "Learning proceeds either with value iteration (Sutton and Barto, 1998) or by directly approximating Q\u2217(s, a) using a function approximator (such as a neural network) and learning via stochastic gradient descent.", "startOffset": 46, "endOffset": 70}, {"referenceID": 23, "context": "An alternative to Q-learning is the SARSA algorithm (Rummery and Niranjan, 1994); an on-policy method to learn Q\u03c0(s, a), which is the action-value function when taking action a in state s at time t, and then proceeding according to policy \u03c0 afterwards.", "startOffset": 52, "endOffset": 80}, {"referenceID": 4, "context": "Much prior work in clinical machine learning has focused on supervised learning techniques for diagnosis (Esteva et al., 2017) and risk stratification (N.", "startOffset": 105, "endOffset": 126}, {"referenceID": 20, "context": ", 2017) and risk stratification (N.Razavian et al., 2015).", "startOffset": 32, "endOffset": 57}, {"referenceID": 7, "context": "The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al.", "startOffset": 106, "endOffset": 158}, {"referenceID": 11, "context": "The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al.", "startOffset": 106, "endOffset": 158}, {"referenceID": 5, "context": "The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al., 2013; Ghassemi et al., 2014).", "startOffset": 218, "endOffset": 262}, {"referenceID": 6, "context": "The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al., 2013; Ghassemi et al., 2014).", "startOffset": 218, "endOffset": 262}, {"referenceID": 15, "context": "We prefer RL for sepsis treatment over supervised learning, because the ground truth of \u201cgood\u201d treatment strategy is unclear in medical literature (Marik, 2015).", "startOffset": 147, "endOffset": 160}, {"referenceID": 28, "context": "Their work applied on-policy SARSA learning to fit an action-value function to the physician policy and value-iteration techniques to find an optimal policy (Sutton and Barto, 1998).", "startOffset": 157, "endOffset": 181}, {"referenceID": 4, "context": "Much prior work in clinical machine learning has focused on supervised learning techniques for diagnosis (Esteva et al., 2017) and risk stratification (N.Razavian et al., 2015). The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al., 2013; Ghassemi et al., 2014). We prefer RL for sepsis treatment over supervised learning, because the ground truth of \u201cgood\u201d treatment strategy is unclear in medical literature (Marik, 2015). Importantly, RL algorithms also allow us to infer optimal strategies from training examples that do not represent optimal behavior. RL is well-suited to identifying ideal septic treatment strategies, because clinicians deal with a sparse, time-delayed reward signal in septic patients, and optimal treatment strategies may differ. Nemati et al. (2016) applied deep RL techniques to modeling ICU heparin dosing as a Partially Observed Markov Decision Process (POMDP), using both discriminative Hidden Markov Models and Q-networks to discover the optimal policy.", "startOffset": 106, "endOffset": 956}, {"referenceID": 4, "context": "Much prior work in clinical machine learning has focused on supervised learning techniques for diagnosis (Esteva et al., 2017) and risk stratification (N.Razavian et al., 2015). The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al., 2013; Ghassemi et al., 2014). We prefer RL for sepsis treatment over supervised learning, because the ground truth of \u201cgood\u201d treatment strategy is unclear in medical literature (Marik, 2015). Importantly, RL algorithms also allow us to infer optimal strategies from training examples that do not represent optimal behavior. RL is well-suited to identifying ideal septic treatment strategies, because clinicians deal with a sparse, time-delayed reward signal in septic patients, and optimal treatment strategies may differ. Nemati et al. (2016) applied deep RL techniques to modeling ICU heparin dosing as a Partially Observed Markov Decision Process (POMDP), using both discriminative Hidden Markov Models and Q-networks to discover the optimal policy. Their investigation was made more challenging by the relatively small amount of available data. Shortreed et al. (2011) learned optimal treatment policies for schizophrenic patients, and quantified the uncertainty around the expected outcome for patients who followed the policies.", "startOffset": 106, "endOffset": 1285}, {"referenceID": 4, "context": "Much prior work in clinical machine learning has focused on supervised learning techniques for diagnosis (Esteva et al., 2017) and risk stratification (N.Razavian et al., 2015). The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al., 2013; Ghassemi et al., 2014). We prefer RL for sepsis treatment over supervised learning, because the ground truth of \u201cgood\u201d treatment strategy is unclear in medical literature (Marik, 2015). Importantly, RL algorithms also allow us to infer optimal strategies from training examples that do not represent optimal behavior. RL is well-suited to identifying ideal septic treatment strategies, because clinicians deal with a sparse, time-delayed reward signal in septic patients, and optimal treatment strategies may differ. Nemati et al. (2016) applied deep RL techniques to modeling ICU heparin dosing as a Partially Observed Markov Decision Process (POMDP), using both discriminative Hidden Markov Models and Q-networks to discover the optimal policy. Their investigation was made more challenging by the relatively small amount of available data. Shortreed et al. (2011) learned optimal treatment policies for schizophrenic patients, and quantified the uncertainty around the expected outcome for patients who followed the policies. Prasad et al. (2017) use off-policy reinforcement learning algorithms to determine ICU strategies for mechanical ventilation administration and weaning, but focus on simpler learning algorithms and a heuristic action space.", "startOffset": 106, "endOffset": 1468}, {"referenceID": 4, "context": "Much prior work in clinical machine learning has focused on supervised learning techniques for diagnosis (Esteva et al., 2017) and risk stratification (N.Razavian et al., 2015). The incorporation of time in a supervised setting could be implicit within the feature space construction (Hug and Szolovits, 2009; Joshi and Szolovits, 2012), or captured with multiple models for different timepoints (Fialho et al., 2013; Ghassemi et al., 2014). We prefer RL for sepsis treatment over supervised learning, because the ground truth of \u201cgood\u201d treatment strategy is unclear in medical literature (Marik, 2015). Importantly, RL algorithms also allow us to infer optimal strategies from training examples that do not represent optimal behavior. RL is well-suited to identifying ideal septic treatment strategies, because clinicians deal with a sparse, time-delayed reward signal in septic patients, and optimal treatment strategies may differ. Nemati et al. (2016) applied deep RL techniques to modeling ICU heparin dosing as a Partially Observed Markov Decision Process (POMDP), using both discriminative Hidden Markov Models and Q-networks to discover the optimal policy. Their investigation was made more challenging by the relatively small amount of available data. Shortreed et al. (2011) learned optimal treatment policies for schizophrenic patients, and quantified the uncertainty around the expected outcome for patients who followed the policies. Prasad et al. (2017) use off-policy reinforcement learning algorithms to determine ICU strategies for mechanical ventilation administration and weaning, but focus on simpler learning algorithms and a heuristic action space. We experiment with using a sparse autoencoder to generate latent representations of the state of a patient, likely leading to an easier learning problem. We also propose neural network architectures that obtain more robust methods for optimal policy deduction. Optimal sepsis treatment strategy was tackled most recently by Komorowski et al. (2016), using a discretized state and action-space to deduce optimal treatment policies for septic patients.", "startOffset": 106, "endOffset": 2020}, {"referenceID": 13, "context": "We also propose a novel evaluation metric, different from ones used in Komorowski et al. (2016). We focus on in-hospital mortality instead of 90-day mortality (used in Komorowski et al.", "startOffset": 71, "endOffset": 96}, {"referenceID": 13, "context": "We also propose a novel evaluation metric, different from ones used in Komorowski et al. (2016). We focus on in-hospital mortality instead of 90-day mortality (used in Komorowski et al. (2016)) because of the other unobserved factors that could affect mortality in a 3-month timeframe.", "startOffset": 71, "endOffset": 193}, {"referenceID": 10, "context": "4) database (Johnson et al., 2016), which is publicly available, and contains hospital admissions from approximately 38,600 adults (at least 15 years old).", "startOffset": 12, "endOffset": 34}, {"referenceID": 27, "context": "We extracted a cohort of patients fulfilling the Sepsis-3 criteria (Singer et al., 2016), and note that summary information about the populations is similar in sepsis survivors and mortalities (Table 1).", "startOffset": 67, "endOffset": 88}, {"referenceID": 15, "context": "The action space was restricted to these two interventions as both drugs are extremely important in the management of septic patients, but there is no agreement on when, and how much, of each drug to give (Marik, 2015).", "startOffset": 205, "endOffset": 218}, {"referenceID": 23, "context": "We use the SARSA algorithm (Rummery and Niranjan, 1994) to learn Q\u03c0(s, a), and the action-value function for the physician policy (more detail in Appendix 8.", "startOffset": 27, "endOffset": 55}, {"referenceID": 13, "context": "1 Discretized State-space and Discretized Action-space Following Komorowski et al. (2016), we create a baseline model with discretized state and action spaces, aiming to capture the underlying representation while simplifying the learning procedure.", "startOffset": 65, "endOffset": 90}, {"referenceID": 16, "context": "1 MODEL ARCHITECTURE Our model is based on a variant of Deep Q Networks (Mnih et al., 2015).", "startOffset": 72, "endOffset": 91}, {"referenceID": 32, "context": "To this end, we use a Dueling Q Network (Wang et al., 2015), where the action-value function for a given (s, a) pair, Q(s, a), is split into separate value and advantage streams, where the value represents the quality of the current state, and the advantage represents the quality of the chosen action.", "startOffset": 40, "endOffset": 59}, {"referenceID": 24, "context": "We use Prioritized Experience Replay (Schaul et al., 2015) to accelerate learning by sampling a transition from the training set with probability proportional to the previous error observed.", "startOffset": 37, "endOffset": 58}, {"referenceID": 8, "context": "The network has two hidden layers of size 128, uses batch normalization (Ioffe and Szegedy, 2015) after each, Leaky-ReLU activation functions, a split into equally sized advantage and value streams, and a projection onto the action space by combining these two streams.", "startOffset": 72, "endOffset": 97}, {"referenceID": 1, "context": "We examined both ordinary autoencoders (Bengio, 2009) and sparse autoencoders (Ng, 2011) to produce latent state representations of the physiological state vectors and simplify the learning problem.", "startOffset": 39, "endOffset": 53}, {"referenceID": 19, "context": "We examined both ordinary autoencoders (Bengio, 2009) and sparse autoencoders (Ng, 2011) to produce latent state representations of the physiological state vectors and simplify the learning problem.", "startOffset": 78, "endOffset": 88}, {"referenceID": 13, "context": "Furthermore, directly comparing Q values on off-policy data, as done in prior applications of RL to healthcare (Komorowski et al., 2016) can provide incorrect performance estimates (Jiang and Li, 2015).", "startOffset": 111, "endOffset": 136}, {"referenceID": 9, "context": ", 2016) can provide incorrect performance estimates (Jiang and Li, 2015).", "startOffset": 52, "endOffset": 72}, {"referenceID": 9, "context": ", 2016) can provide incorrect performance estimates (Jiang and Li, 2015). Finding the average Q-value as in Komorowski et al. (2016) is suboptimal because theQ\u03c0 used for assessment represents the expected return when acting optimally at state st, but then proceeding according to \u03c0physician, the physician policy.", "startOffset": 53, "endOffset": 133}, {"referenceID": 9, "context": "2 Off-Policy Evaluation We use the method of Doubly Robust Off-policy Value Evaluation (Jiang and Li, 2015) to obtain an unbiased estimate of the value of the learned optimal policy using off-policy sampled data (the trajectories in our training set).", "startOffset": 87, "endOffset": 107}, {"referenceID": 9, "context": "Directly comparing returns without the use of such an estimator is likely to give invalid results (Jiang and Li, 2015).", "startOffset": 98, "endOffset": 118}, {"referenceID": 9, "context": "2 Off-Policy Evaluation We use the method of Doubly Robust Off-policy Value Evaluation (Jiang and Li, 2015) to obtain an unbiased estimate of the value of the learned optimal policy using off-policy sampled data (the trajectories in our training set). For each trajectoryH we compute an unbiased estimate of the value of the learned policy, V H DR, and average the results obtained across the observed trajectories. More details are provided in Jiang and Li (2015). We can also compute the mean discounted return of chosen actions under the physician policy.", "startOffset": 88, "endOffset": 465}, {"referenceID": 17, "context": "In addition, there have been few controlled clinical trials that have documented improved outcomes from their use (M\u00fcllner et al., 2004).", "startOffset": 114, "endOffset": 136}, {"referenceID": 0, "context": "Another approach could be to use inverse RL techniques (Abbeel and Ng, 2010) to derive a suitable reward function based on the actions of experts (the physicians).", "startOffset": 55, "endOffset": 76}, {"referenceID": 0, "context": "Another approach could be to use inverse RL techniques (Abbeel and Ng, 2010) to derive a suitable reward function based on the actions of experts (the physicians). As our dataset of patient trajectories is collected from recording the actions of many different physicians, this approach may allow us to infer a more appropriate reward function and in turn learn a better model. Our contributions build on recent work by Komorowski et al. (2016), investigating a variety of techniques to find optimal treatment policies that improve patient outcome.", "startOffset": 56, "endOffset": 445}, {"referenceID": 3, "context": "Our policies learned that vasopressors may not be favored as a first response to sepsis, which is sensible given that vasopressors may be harmful in some populations (D\u2019Aragon et al., 2015).", "startOffset": 166, "endOffset": 189}, {"referenceID": 14, "context": "Our learned policy of intermediate fuild dosages also fits well with recent clinical work finding that large fluid dosages on first ICU day are associated with increased hospital costs and risk of death (Marik et al., 2017).", "startOffset": 203, "endOffset": 223}, {"referenceID": 3, "context": "Our policies learned that vasopressors may not be favored as a first response to sepsis, which is sensible given that vasopressors may be harmful in some populations (D\u2019Aragon et al., 2015). Our learned policy of intermediate fuild dosages also fits well with recent clinical work finding that large fluid dosages on first ICU day are associated with increased hospital costs and risk of death (Marik et al., 2017). The learned policies are also clinically interpretable, and could be used to provide clinical decision support in the ICU. To our knowledge, this is the first extensive application of novel deep reinforcement learning techniques to medical informatics, building significantly on the findings of Nemati et al. (2016). Acknowledgments This research was funded in part by the Intel Science and Technology Center for Big Data and the National Library of Medicine Biomedical Informatics Research Training grant (NIH/NLM 2T15 LM007092-22).", "startOffset": 167, "endOffset": 732}], "year": 2017, "abstractText": "Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient\u2019s physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient\u2019s physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6% over observed clinical policies, from a baseline mortality of 13.7%. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.", "creator": "LaTeX with hyperref package"}}}