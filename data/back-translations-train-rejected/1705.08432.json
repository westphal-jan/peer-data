{"id": "1705.08432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Deep Learning of Grammatically-Interpretable Representations Through Question-Answering", "abstract": "We introduce an architecture in which internal representations, learned by end-to-end optimization in a deep neural network performing a textual question-answering task, can be interpreted using basic concepts from linguistic theory. This interpretability comes at a cost of only a few percentage-point reduction in accuracy relative to the original model on which the new one is based (BiDAF [1]). The internal representation that is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Through extremely detailed, fine-grained analysis, we find specific correspondences between the learned roles and parts of speech as assigned by a standard parser [2], and find several discrepancies in the model's favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means to represent using symbols and roles and an inductive bias favoring use of these in an approximately discrete manner.", "histories": [["v1", "Tue, 23 May 2017 17:40:14 GMT  (856kb,D)", "http://arxiv.org/abs/1705.08432v1", null], ["v2", "Mon, 25 Sep 2017 23:49:18 GMT  (343kb,D)", "http://arxiv.org/abs/1705.08432v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hamid palangi", "paul smolensky", "xiaodong he", "li deng"], "accepted": false, "id": "1705.08432"}, "pdf": {"name": "1705.08432.pdf", "metadata": {"source": "CRF", "title": "Deep Learning of Grammatically-Interpretable Representations Through Question-Answering", "authors": ["Hamid Palangi", "Paul Smolensky", "Xiaodong He", "Li Deng"], "emails": ["hpalangi@microsoft.com", "psmo@microsoft.com", "xiaohe@microsoft.com", "l.deng@ieee.org"], "sections": [{"heading": null, "text": "This work was done while PS was on leave from Johns Hopkins University, and is currently at Citadel.ar Xiv: 170 5.08 432v 1 [csto stands for the use of symbols and rolls, and for an inductive bias that favors their use roughly discreetly."}, {"heading": "1 Introduction: Minding the gap", "text": "The difficulty of interpreting internal representations within deep neural networks stems from the inconsistency between, on the one hand, and, on the other, that this difference can be reduced in principle if deep neural networks are embedded in discrete structures; the categories and relationships of these concepts could then be comprehensible."}, {"heading": "2 The Model", "text": "The proposed TPRNarchitecture in TensorFlow [8] is based on the BiDAF model proposed in [1]. BiDAF consists of 6 levels: a character embedding layer using CNNs, a word embedding layer using GLOVE [9] vectors, a phrase embedding layer using bidirectional LSTMs for sentence embedding [10], an attention flow layer using a special attention mechanism, a modeling layer using LSTMs, and an output layer that generates pointers to the beginning and end of an answer in the paragraph. (See Fig. 1 of [1].) TPRN replaces the LSTM cells forming the bidirectional RNN with the phrase embedding layer with recurring TPR cells, which are described next: see Fig. 1."}, {"heading": "3 TPR: distributed representation of structures", "text": "Although the approach is generalized in an obvious way, for conciseness, here we describe the model whose performance is described in Sec. 4 and whose representations are interpreted in Sec. 5. (This TPRNmodel allows the phrase embedding layer of the model to decide for each word how that word will be encrypted by selection from among 100 symbols, each of which will be interpretable in terms of the lexical content of the word (e.g. Australia points to a place, while the slots will be interpretable as grammatical roles, such as subject / agent / patient, question-constraint phrase). In Sec. 5 we will test this hypothesis; we will refer to \"roles\" rather than \"slots."}, {"heading": "4 Experiments", "text": "In this section, we describe the details of the experiments for the proposed TPRN model in response to questions answering questions related to the use of Stanford's SQuAD Dataset [3]. Results of interest are the interpretations of the learned presentations, which will be discussed in detail in Sec. 5. 1Please note that the goal of this work is not to beat the state-of-the-art system on SQuAD (at the time of writing this paper, r-net, [12]), but to create a question that is interpretable as long as we have a system of TPR cells that work reasonably well, we can use it to demonstrate the interpretation capability of claims in this work. SQuAD is a reading comprehension to answer questions that are answered. It consists of more than 500 Wikipedia articles and more than 100,000 question pairings about it that are significantly larger than previous reader datasets."}, {"heading": "5 Interpretation of learned TPRs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Interpreting learned TPR Roles", "text": "Here we offer an interpretation of the TPR roles aR (t) associated with the words w (t) of query input in the forward TPR RNN of the TPRN. Just as well-learned neural network models in vision typically acquire similar early types of representations of an input image (e.g. [14]), the hypothesis is reasonable that well-learned neural network models in language typically acquire low-level input representations that are generally similar to each other. Therefore, we can hope for some universality of the types of interpretation discussed here. Convergence in common input representations is to be expected, as these representations capture the regularities between the input data that are useful for many tasks processing such inputs. Linguists have been studying the types of regularities to be captured in speech input for years, so there is reason to expect a convergence between general well-learned neural concepts and network representations."}, {"heading": "5.1.1 Grammatical role concepts learned by the model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A grammatical category\u2014Part of Speech: Determiner \u223c Role #9", "text": "The network assigns the following words to role 9: a significant proportion of characters for: the (76%), to (52%), a (46%), to be (36%) and a few characters for (8%) and the century (3%). The dominant words for role 9 (that, an, a, to be) are all determinants. Although they are not determinators, of is also an important function word; the 3% of characters of the century that activate role 9 can be set aside. Quantitatively, p (w is a determinator | w activates role 9 to > 0.65) = 0.96. This interpretation does not claim that # 9 is the only role for determinants; e.g. p (w activates role 9 | w, a, a, die) = 0.70."}, {"heading": "A semantic category: Predicate (verbs and adjectives) \u223c Role #17:", "text": "The words assigned to role # 17 are overwhelmingly predictable, a semantic category that corresponds to the syntactic categories of verbs and adjectives [e.g., under semantic interpretation, J runs \u2192 runs (J); J is large \u2192 large (J)]. While the English word orders of these two types of prediction are often opposites (the girl runs against the big girl), the model represents them as both fulfilling the same role, which can be interpreted as semantic rather than syntactic. Quantitatively, p (w is a verb or adjective that selects role # 17) = 0.82. Unlike role # 9, which concerns only a small (\"closed\") class of words, the class of predicates is large (\"open\"), and role # 17 is assigned only to a rather small fraction of predicate signs: e.g. p (w is assigned to role # 17 | w) is a verb = 4.1."}, {"heading": "5.1.2 Correcting the Stanford Parser\u2019s POS labeling using learned roles", "text": "In fact, most people who are able to see themselves as being able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry, to parry."}, {"heading": "5.2 Interpreting learned TPR symbols", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Meaning of learned symbols: Lexical-semantic coherence of symbol assignments", "text": "To interpret the lexical-semantic content of the TPR symbols s (t) learned from the TPRN network, we calculate: 1. s (t) = SaS (t) \u2022 R10 for all (120,950) word marks w (t) in the validation level.2. Cosinal similarity is calculated between aS (t) and the embedding vector of each symbol.3. However, the symbol with the maximum (cosinal) similarity is assigned to the corresponding symbol.4. For each symbol, all symbols assigned to it are sorted based on their similarity to it; symbols of the same type are removed, and the uppermost symbols from this list are examined to assess the semantic coherence of the symbol assignments (see Tables 3-5).The results provide significant support for our hypothesis that each symbol corresponds to a specific meaning assigned to a cloud semantic word combination 0.775% multiplied with a \"429\" symbols."}, {"heading": "5.2.2 Polysemy", "text": "Each symbol of the same word, for example, who, generates its own symbol / role TPR in TPRN, and if our hypothesis is correct, characters with different meanings should choose different symbols. In fact, we see three general patterns of symbol selection for whom. Who is the producer of Dr. Who? illustrates the meaning of the main word or proper name in its two uses by who. Third, the relative pronoun is meant, illustrated by... the actor who... The three symbol selection patterns associated with these three meanings are in Table 6. We can interpret the symbols with IDs 25, 52 and 98 to correspond to the meaning of a relative pronoun, a main question and a correct noun. Then, the symbols with bold number sequence are correct, while the other counts are errors. Of interest are the other facts that all 18 of the non-sentence main question word tokens are correctly identified as such (52 symbols assigned) and the question number 1 is clearly identified with the correct half of the number 1 to 27."}, {"heading": "5.2.3 Predicting output errors from internal mis-representation", "text": "When processing the test query Which type / genre of a TV show is Doctor Who? [7632] The model assigns symbol 52 to the Who, which we interpreted as an error, since symbol 52 is assigned to each of the 1062 occurrences of Who as the main question word. Although the model tends to give answers in the right category, it responds to Time Lord, an appropriate way of answering a real Who question, but not to the actual question. The model makes 4 errors of this type, out of the total 9 errors made in the assignment of symbol 25; this 44% rate contrasts with the 9% rate when assigning the \"proper word symbol\" 98 to the Who correctly. Although such an error analysis with TPRNmodels is still in its infancy, it is already beginning to reveal its potential, enabling us for the first time to attribute total output errors of a DNN to specific errors in the internal representation."}, {"heading": "6 Related work", "text": "In recent years, we have encountered a number of problems characterized by the reintroduction of elements of symbolic computation as peripheral modules, including: (i) the memory bank, a discrete set of addressed storage media, each with a neural activation vector, and (ii) the sequential program that links them to the central control mechanisms of the system. (i) The discretion in these peripheral modules is software-driven. (i)"}, {"heading": "7 Conclusion", "text": "This new model, TPRN, uses Tensor Product Representations in recurring networks to encode input words. Through holistic learning, the model learns how to use 100 symbols in 20 structural roles; the symbols and roles have no meaning before learning them. We hypothesize that the symbols would acquire lexical meanings and the roles would acquire grammatical meanings. We interpret the learned symbols and roles by observing which of them the trained model chooses to encode individual words in context. We observe that the words assigned to a particular symbol tend to be semantically related, and the words assigned to a given role correlate with abstract notions of grammatical roles from linguistic theory. Thus, the TPRN model illustrates how learning to question a natural language can lead to representations that can be interpreted to represent an abstract structure without children ever being exposed to conceptual theory."}, {"heading": "Acknowledgments", "text": "The work reported here builds on work supported by the NSF INSPIRE Scholarship BCS-1344269 at PS and conducted at Johns Hopkins University and Northwestern University. We thank NSF, the JHU Department of Cognitive Science, and co-researchers Matthew Goldrick, Geraldine Legendre, Akira Omaki, Kyle Rawlins, Ben Van Durme, and Colin Wilson for their support. We thank collaborator Paul Tupper for proposing the form of Q function used here."}, {"heading": "8 Supplementary Materials", "text": "In this section, we present further examples that support the lexical-semantic coherence of the words associated with symbols described in Section 5.2.1.8.1 Symbol 29: The \"Month of the Year\" symbol 29 is attracted by the months of the year. By counting the total number of times per month of the year in the validation set and then counting how many of them are attracted by symbol 29, we get results in Table 7. The 30 most attracted symbols are shown in Table 8 (sorted by cosmic similarity, duplicate symbols removed)."}, {"heading": "8.2 Symbol 26: \u201cwhat\u201d symbol", "text": "100% of the \"what\" and \"what\" characters are attracted by symbol 26."}, {"heading": "8.3 Symbol 20: \u201cdirectional / causal\u201d symbol", "text": "75.8% of the tokens from \"to\" and 81.7% of the tokens from \"to\" are attracted by symbol 20."}, {"heading": "8.4 Symbol 55: \u201cfinance / property\u201d symbol", "text": "Most of the symbols attracted by symbol 55 are financial or asset related symbols. Table 9 shows the full list of symbols attracted by symbol 55."}, {"heading": "8.5 Symbol 43: \u201chow\u201d symbol", "text": "Symbol 43 attracts 100% of the \"How\" chips and 62.6% of the \"How\" chips."}, {"heading": "8.6 Symbol 22: \u201claw\u201d symbol", "text": "Most symbols attracted by symbol 22 are law-related symbols. Table 10 shows the full list of symbols attracted by symbol 22."}, {"heading": "8.7 Symbol 44: \u201cterritory\u201d symbol", "text": "Most symbols attracted by symbol 44 are territorial symbols. Table 11 shows the full list of symbols attracted by symbol 44."}, {"heading": "8.8 Symbol 61: \u201cyear\u201d symbol", "text": "Most of the symbols attracted by symbol 61 are year symbols. Table 12 shows the full list of symbols attracted by symbol 61."}, {"heading": "8.9 Symbol 36: \u201ca / an / about\u201d symbol", "text": "Symbol 36 attracts 70.8% of the \"a\" tokens, 69.7% of the \"an\" tokens and 87% of the \"about\" tokens."}, {"heading": "8.10 Symbol 30: \u201c?\u201d symbol", "text": "74% of \"?\" chips are attracted by symbol 30."}], "references": [{"title": "Bidirectional attention flow for machine comprehension", "author": ["M.J. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": "5th International Conference for Learning Representations, San Juan, Puerto Rico, 2016. [Online]. Available: https://arxiv.org/abs/1611.01603", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Featurerich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning", "Y. Singer"], "venue": "Proceedings of HLT-NAACL 2003, 2003, pp. 252\u2013259. [Online]. Available: https://nlp.stanford.edu/software/tagger.shtml", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP, Austin, Texas, USA, 2016. [Online]. Available: http://arxiv.org/abs/1606.05250", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist networks", "author": ["P. Smolensky"], "venue": "Artificial Intelligence, vol. 46, pp. 159\u2013216, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar", "author": ["P. Smolensky", "G. Legendre"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Optimization and quantization in gradient symbol systems: A framework for integrating the 16  continuous and the discrete in cognition", "author": ["P. Smolensky", "M. Goldrick", "D. Mathis"], "venue": "Cognitive Science, vol. 38, pp. 1102\u20131138, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, October 2014, pp. 1532\u20131543.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 4, pp. 694\u2013707, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Symbolic functions from neural computation", "author": ["P. Smolensky"], "venue": "Philosophical Transactions of the Royal Society \u2014 A: Mathematical, Physical and Engineering Sciences, vol. 370, pp. 3543\u20133569, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Gated selfmatching networks for reading comprehension and question answering", "author": ["W. Wang", "N. Yang", "F. Wei", "B. Chang", "M. Zhou"], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "2012. [Online]. Available: http://arxiv.org/abs/1212.5701", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 2528\u20132535. 17", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Tracking the world state with recurrent entity networks", "author": ["M. Henaff", "J. Weston", "A. Szlam", "A. Bordes", "Y. LeCun"], "venue": "6th International Conference for Learning Representations, Toulon, France, 2017.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "Advances in neural information processing systems, 2015, pp. 2440\u20132448.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "5th International Conference for Learning Representations, San Juan, Puerto Rico, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 577\u2013585.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "International Conference on Machine Learning, 2015, pp. 2048\u20132057.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska- Barwi\u0144ska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou"], "venue": "Nature, vol. 538, no. 7626, pp. 471\u2013476, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "This interpretability comes at a cost of only a few percentage-point reduction in accuracy relative to the original model on which the new one is based (BiDAF [1]).", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "Through extremely detailed, fine-grained analysis, we find specific correspondences between the learned roles and parts of speech as assigned by a standard parser [2], and find several discrepancies in the model\u2019s favor.", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "Specifically, the task we address is question answering for the SQuAD dataset [3], in which a text passage and a question are presented as input, and the model\u2019s output identifies a stretch within the passage that contains the answer to the question (see Sec.", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "The capacity for distributed representations of structure is provided by Tensor Product Representations, TPRs, in which a discrete symbol structure is encoded as a vector systematically built\u2014through vector addition and the tensor product\u2014from vectors encoding symbols and vectors encoding the roles each symbol plays in the structure as a whole [4, 5, 6].", "startOffset": 346, "endOffset": 355}, {"referenceID": 4, "context": "The capacity for distributed representations of structure is provided by Tensor Product Representations, TPRs, in which a discrete symbol structure is encoded as a vector systematically built\u2014through vector addition and the tensor product\u2014from vectors encoding symbols and vectors encoding the roles each symbol plays in the structure as a whole [4, 5, 6].", "startOffset": 346, "endOffset": 355}, {"referenceID": 5, "context": "The capacity for distributed representations of structure is provided by Tensor Product Representations, TPRs, in which a discrete symbol structure is encoded as a vector systematically built\u2014through vector addition and the tensor product\u2014from vectors encoding symbols and vectors encoding the roles each symbol plays in the structure as a whole [4, 5, 6].", "startOffset": 346, "endOffset": 355}, {"referenceID": 0, "context": "The new model proposed here is built from the BiDAF model proposed in [1] for question answering.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "It replaces a bidirectional RNN built from LSTM units [7] with one built from TPR units; the architecture is called the Tensor Product Recurrent Network, TPRN.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The proposed TPRNarchitecture is built in TensorFlow [8] on the BiDAF model proposed in [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The proposed TPRNarchitecture is built in TensorFlow [8] on the BiDAF model proposed in [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "BiDAF is constructed from 6 layers: a character embedding layer using CNNs, a word embedding layer using GLOVE [9] vectors, a phrase embedding layer using bidirectional LSTMs for sentence embedding [10], an attention flow layer using a special attention mechanism, a modeling layer using LSTMs, and an output layer that generates pointers to the start and end of an answer in the paragraph.", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "BiDAF is constructed from 6 layers: a character embedding layer using CNNs, a word embedding layer using GLOVE [9] vectors, a phrase embedding layer using bidirectional LSTMs for sentence embedding [10], an attention flow layer using a special attention mechanism, a modeling layer using LSTMs, and an output layer that generates pointers to the start and end of an answer in the paragraph.", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "1 of [1].", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "The present TPRNmodel incorporates TPR to only a modest degree, but it is a proof-of-concept system that paves the way for future models that can import the power of general symbol-structure processing, proven to be within the scope of full-blown TPR architectures [5, 11].", "startOffset": 265, "endOffset": 272}, {"referenceID": 10, "context": "The present TPRNmodel incorporates TPR to only a modest degree, but it is a proof-of-concept system that paves the way for future models that can import the power of general symbol-structure processing, proven to be within the scope of full-blown TPR architectures [5, 11].", "startOffset": 265, "endOffset": 272}, {"referenceID": 2, "context": "In this section, we describe details of the experiments for the proposed TPRNmodel on question answering task using the Stanford\u2019s SQuAD dataset [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 11, "context": "1 Please note that the goal of this work is not to beat the state-of-the-art system on SQuAD (at the time of writing this paper, r-net, [12]), but to create a question answering system that is interpretable, by exploiting TPR.", "startOffset": 136, "endOffset": 140}, {"referenceID": 2, "context": "It consists of more than 500 Wikipedia articles and more than 100,000 questionanswer pairs about them, which is significantly larger than previous reading comprehension datasets [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "For the experiments, we used the same settings reported in [1] for all layers of TPRNexcept the phrase embedding layer.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "\u2022 The concatenation of word embedding using GLOVE [9] and character embedding using Convolutional Neural Networks (CNNs) was used to represent each word.", "startOffset": 50, "endOffset": 53}, {"referenceID": 12, "context": "\u2022 The optimizer used was AdaDelta [13] with 12 epochs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "Performance results of our single model compared with a single model in [1] are presented in Table 1.", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "From this table we observe that our proposed TPR based model underperforms [1] by about 2 points.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "2 mentions a few selected, highly targeted performance comparisons against the POS tagging provided by the Stanford parser [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "Table 1: Performance of the proposed TPRNmodel compared to BiDAF proposed in [1]", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "86 BiDAF [1] 68.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": ", [14]), it is reasonable to hypothesize that good learned neural network models in language will typically learn low-level input representations that are generally similar to one another.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 109, "endOffset": 121}, {"referenceID": 15, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 109, "endOffset": 121}, {"referenceID": 16, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 109, "endOffset": 121}, {"referenceID": 17, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 275, "endOffset": 283}, {"referenceID": 18, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 275, "endOffset": 283}, {"referenceID": 19, "context": "The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank (\u2018attention\u2019 [20, 21]); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program [22].", "startOffset": 274, "endOffset": 282}, {"referenceID": 20, "context": "The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank (\u2018attention\u2019 [20, 21]); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program [22].", "startOffset": 274, "endOffset": 282}, {"referenceID": 21, "context": "The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank (\u2018attention\u2019 [20, 21]); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program [22].", "startOffset": 407, "endOffset": 411}, {"referenceID": 4, "context": "Computation over TPRs is massively parallel [5].", "startOffset": 44, "endOffset": 47}], "year": 2017, "abstractText": "We introduce an architecture in which internal representations\u2014 learned by end-to-end optimization in a deep neural network performing a textual question-answering task\u2014can be interpreted using basic concepts from linguistic theory. This interpretability comes at a cost of only a few percentage-point reduction in accuracy relative to the original model on which the new one is based (BiDAF [1]). The internal representation that is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Through extremely detailed, fine-grained analysis, we find specific correspondences between the learned roles and parts of speech as assigned by a standard parser [2], and find several discrepancies in the model\u2019s favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means \u2217This work was carried out while PS was on leave from Johns Hopkins University. LD is currently at Citadel. 1 ar X iv :1 70 5. 08 43 2v 1 [ cs .C L ] 2 3 M ay 2 01 7 to represent using symbols and roles and an inductive bias favoring use of these in an approximately discrete manner.", "creator": "LaTeX with hyperref package"}}}