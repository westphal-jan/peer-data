{"id": "1705.08480", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit", "abstract": "Recurrent Neural Networks architectures excel at processing sequences by modelling dependencies over different timescales. The recently introduced Recurrent Weighted Average (RWA) unit captures long term dependencies far better than an LSTM on several challenging tasks. The RWA achieves this by applying attention to each input and computing a weighted average over the full history of its computations. Unfortunately, the RWA cannot change the attention it has assigned to previous timesteps, and so struggles with carrying out consecutive tasks or tasks with changing requirements. We present the Recurrent Discounted Attention (RDA) unit that builds on the RWA by additionally allowing the discounting of the past.", "histories": [["v1", "Tue, 23 May 2017 18:57:50 GMT  (375kb)", "https://arxiv.org/abs/1705.08480v1", null], ["v2", "Mon, 19 Jun 2017 08:53:04 GMT  (375kb)", "http://arxiv.org/abs/1705.08480v2", "Updated results of RDA-exp-tanh unit for the wikipedia char prediction task"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["brendan maginnis", "pierre h richemond"], "accepted": false, "id": "1705.08480"}, "pdf": {"name": "1705.08480.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["b.maginnis@imperial.ac.uk", "pierre.richemond@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.08 480v 2 [cs.L G] 19 JuWe compare our model empirically with RWA, LSTM and GRU units for multiple challenging tasks. For tasks with a single output, the RWA, RDA and GRU units learn much faster than the LSTM and with better performance. For tasks with multiple sequences, our RDA unit learns the task three times faster than the LSTM or GRU units, while the RWA does not learn at all. In the Wikipedia prediction task, the LSTM performs best, but closely followed by our RDA unit. Overall, our RDA unit performs well and is sample efficient on a variety of sequence tasks."}, {"heading": "1 Introduction", "text": "rE \"s tis rf\u00fc eid rf\u00fc rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "2 Related Work", "text": "Sutskever et al. [20] first tried to do this on the Wikipedia data sets of the Hutter Prize using the MRNN architecture. Since then, many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have performed impressively in this task, which is close to the bit-per-character limits of adapted compression algorithms. Many of the above architectures are very complex, and so the Gated Recurrent Unit (GRU) is a much simpler design that achieves a similar performance to the LSTM. Our experiments confirm previous literature [4], which reports that it works very well.Attention mechanisms were used in the neural machine translation by Bahdanau et al. [3] to give attention to the local information distribution. Xu et al. [21] experimented with hard attention to the image, where a single location from a multinomial distribution is selected to give extra attention to the local information [11]."}, {"heading": "3 Recurrent Weighted Average", "text": "At each time step, the Recurrent Weighted Average model uses its current hidden state ht \u2212 1 and the input xt to calculate two quantities: 1. The characteristics zt of the current input: ut = Wu \u00b7 xt + bu gt = Wg \u00b7 [xt, ht \u2212 1] + bgzt = ut-tanh gtwhere ut is an unlimited vector that depends only on the input text, and tanh gt is a limited vector that depends on the input text and the hidden state \u2212 1. Notation: W are weights, b are distortions, (\u00b7) is matrix multiplication, (\u00b7) is the elementary product. 2. Attention to the characteristics zt: at = e Wa \u00b7 [xt, ht \u2212 1] + baThe hidden state ht is then the average of the characteristics zt, weighted by the attention at and squeezed by the hyperbolic tangent function: ht = tanh (pt i = 1 zi \u2212 ai = 1)."}, {"heading": "4 Properties of the Recurrent Weighted Average", "text": "The RWA shows superior experimental results compared to the LSTM for the following tasks: 1. Classifying whether a sequence is longer than 500 elements; 2. Storing short random sequences of symbols and retrieving them at any point in the subsequent 1000 times.3. Adding two numbers randomly separated from each other in a sequence of 1000.4. Let's classify MNIST images pixel by pixel. All of these tasks require combining the complete sequences of inputs into a single output. It makes absolute sense that an average over all time pisses. On the other hand, we can imagine tasks where an average over all time pisses would not work effectively: 1. Copying many input sequences from input to output. It will be necessary to forget the sequences once they are output. 2. Predict the next character in a text body."}, {"heading": "5 The Recurrent Discounted Attention unit", "text": "The RDA uses its current hidden state ht \u2212 1 and the input xt to calculate three quantities: 1. The characteristics zt of the current input are identical to the RWA: ut = Wu \u00b7 xt + bugt = Wg \u00b7 [xt, ht \u2212 1] + bgzt = ut tanh gt2. Attention to the characteristics: ztat = fa (Wa \u00b7 [xt, ht \u2212 1] + ba) Here we generalize attention to any function fa that is not negative and monotonically increasing. If we choose fa = exp, then we recover the RWA attention function.3. The discount factor targets the previous values on average t = n."}, {"heading": "6 Experiments", "text": "We conducted experiments to examine the following questions: 1. Which form of RDA works best? (Section 6.2) 2. The RWA unit works remarkably well for sequences with a single task. Does the RDA unit retain this strength? (Section 6.3) 3. We expect the RWA unit to struggle with successive independent tasks. Does this happen in practice and does the RDA solve this problem? (Section 6.4) 4. How does the RDA unit scale up to very long sequences? We test character predictions on the Wikipedia dataset of the Hutter Prize (Section 6.5) 5. How does the RDA unit compare with RWA, LSTM and GRU units? Are some units better suited to certain types of tasks than others? (Section 7) We provide descriptions of the training process in Appendix B."}, {"heading": "6.1 Implementation details", "text": "For all tasks except the task of predicting the Wikipedia characters, we use 250 recurring units. The weights are initialized with the Xavier initialization [8] and the distortions are initialized with 0, with the exception of Forge Gates and Discount Gates, which are initialized with 1 [7]. We use mini-batches with 100 examples and multiply over the full sequence length. We train the models with Adam [16] at a learning rate of 0.001. Gradients are truncated between -1 and 1. For the Wikipedia task, we use a character embedding of 64 dimensions, followed by a single layer of 1800 recurring units, and a final softmax layer for predicting the characters. We apply all 250 time periods truncated back propagation and use the last hidden state of the sequence as the initial hidden state of the next sequence to approximate full backpropagation. All our experiments are implemented in Tensorlow [1]."}, {"heading": "6.2 Empirical evaluation of RDA activation functions", "text": "We conducted our experiments with different combinations of fa and fo and found the following: \u2022 Using a ReLU for attention function fa almost always fails to train. Using a Softplus for fa is much more stable than using a ReLU. However, it does not work as well as using sigmoid or exponential attention.Classify long model steps to accuracy = 1.0.GRU 71 LSTM 776 RDA-exp-tanh 164 RDA-sigmoid-414RWA 133Table 2: Classify: Steps to Accuracy = 1.0.MNISTModel Test Set AccuracyGRU 0.985 LSTM 0.114 RDA-exp-tanh 0.985 RDA-sigmoid-id-id 0.987RWA 0.979Table 3: MNIST Test Set Accuracy = MNIST-Attention (MNIST PermutedModel Permuted Test Set AccurmutacyGR944-9STDA-130.9STDA-Attention)"}, {"heading": "6.3 Single task sequences", "text": "Here we examine whether sequences with a single task can be performed with both the RDA > and the RDA > NWA. Each of the four tasks listed below requires the RNN to store some or all of the input sequences before printing a single result, many steps later. 1. Addition - Input consists of two sequences. The first is a sequence of numbers, each uniformly scanned from [0, 1], and the second consists of all zeros except two, which add the two numbers of the first sequence. (Table 1) 2. Classify the length - A sequence from 1 to 1000 is entered. The goal is to classify whether the sequence is longer than 500. All RNN architectures could learn their initial hidden state for this task, which has improved performance for all of them. (Table 2) 3. MNIST - The task is replaced by the superior classification of the MIST digits."}, {"heading": "6.4 Multiple sequence copy task", "text": "Here we examine whether the different RNN units can perform the same task repeatedly. The tasks consist of several copy tasks, all of which run within the same sequence. Instead of placing the retrieval symbol randomly over the entire sequence, it always appears a few steps after the sequence to be saved. This gives space for 50 consecutive copy tasks with a length of 1000 input sequences. (Table 6)"}, {"heading": "6.5 Wikipedia character prediction task", "text": "The standard test for RNN models is character level language modeling. We evaluate our models based on the Hutter Price Wikipedia dataset enwik8, which contains 100M characters from 205 different symbols, including XML markup and special characters. We divide the data into the first 90M characters for the training set, the next 5M for validation, and the last 5M for the test set. (Table 7)"}, {"heading": "7 Discussion", "text": "We begin our discussion by describing the performance of each unit. Our analysis of the RWA unit showed that it should only work well on the individual task areas, and we confirm this experimentally. It learns the individual task areas quickly, but is unable to learn the multiple task definition and Wikipedia prediction. Our experiments show that the RDA unit is a consistent performer for all kinds of tasks. As expected, it learns individual task areas more slowly than the RWA, but it actually achieves a better generalization of the MNIST test sets. We speculate that the cause of this improvement is that it compresses the information it has previously processed."}, {"heading": "8 Conclusion", "text": "We analyzed the unit RecurrentWeighted Average (RWA) and identified its weakness as the inability to forget the past. By adding this ability to forget the past, we arrived at the unit Recurrent Discounted Attention (RDA). We implemented several variants of the RDA and compared them with the units RWA, LSTM, and GRU for different tasks. We showed that in almost all cases, the RDA is preferable to the RWA and represents a flexible RNN unit that performs well for all types of tasks. We also determined which types of tasks are more suitable for any other RNN unit. For tasks with a single output, the units RWA, RDA, and GRU performed best, while the unit LSTM performs best in the task of predicting Wikipedia characters. We recommend that these results be taken into account when selecting a unit for real-world applications."}, {"heading": "9 Acknowledgements", "text": "Thanks to Elisabeth Adlington, Neil Maginnis and Nick Thapen for many helpful comments and suggestions. Thanks also to Jared Ostmeyer, the author of the RWA unit, who contacted us to discuss our work and pointed out a bug in the implementation of the RMA-exp-tanh unit."}, {"heading": "A Mathematical Proofs", "text": "Lemma 1 Let the task be to ht = \u2212 1 tc for 0 < c < c < 1 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Gregory S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian J. Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal J\u00f3zefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Gordon Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "CoRR, abs/1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "CoRR, abs/1609.01704,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1603.08983,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Machine learning on sequential data using a recurrent weighted average", "author": ["Jared Ostmeyer", "Lindsay Cowell"], "venue": "arXiv preprint arXiv:1703.01253,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Recurrent memory array structures", "author": ["Kamil Rocki"], "venue": "CoRR, abs/1607.03085,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "A specific RNN architecture, known as Long Short-Term Memory (LSTM) [14], is the benchmark against which other RNNs are compared.", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "[3] for neural machine translation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "The Recurrent Weighted Average (RWA) unit, recently introduced by Ostmeyer and Cowell [18], can apply attention to sequences of any length.", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "[20] first attempted this on the Hutter Prize Wikipedia datasets using the MRNN archtecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 3, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 12, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 16, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 4, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 14, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 86, "endOffset": 93}, {"referenceID": 2, "context": "Our experiments confirm previous literature [4] that reports it performing very well.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[21] experimented with hard-attention on image where a single location is selected from a multinomial distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] introduced the global and local attention to refer to attention applied to the whole input and hard attention applied to a local subset of the input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Graves [10] shows that this yields insight into the distribution of information in the input data itself.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "Several RNN architectures have attempted to deal with long term dependencies by storing information in an external memory [11, 12].", "startOffset": 122, "endOffset": 130}, {"referenceID": 10, "context": "Several RNN architectures have attempted to deal with long term dependencies by storing information in an external memory [11, 12].", "startOffset": 122, "endOffset": 130}, {"referenceID": 6, "context": "Weights are initialized using Xavier initialization [8] and biases are initialized to 0, except for forget gates and discount gates which are initialized to 1 [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Weights are initialized using Xavier initialization [8] and biases are initialized to 0, except for forget gates and discount gates which are initialized to 1 [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 13, "context": "We train the models using Adam [16] with a learning rate of 0.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "All of our experiments are implemented in TensorFlow [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The first is a sequence of numbers each uniformly sampled from [0, 1], and the second consists of all zeros except for two ones which indicate the two numbers of the first sequence to be added together.", "startOffset": 63, "endOffset": 69}], "year": 2017, "abstractText": "Recurrent Neural Networks architectures excel at processing sequences by modelling dependencies over different timescales. The recently introduced Recurrent Weighted Average (RWA) unit captures long term dependencies far better than an LSTM on several challenging tasks. The RWA achieves this by applying attention to each input and computing a weighted average over the full history of its computations. Unfortunately, the RWA cannot change the attention it has assigned to previous timesteps, and so struggles with carrying out consecutive tasks or tasks with changing requirements. We present the Recurrent Discounted Attention (RDA) unit that builds on the RWA by additionally allowing the discounting of the past. We empirically compare our model to RWA, LSTM and GRU units on several challenging tasks. On tasks with a single output the RWA, RDA and GRU units learn much quicker than the LSTM and with better performance. On the multiple sequence copy task our RDA unit learns the task three times as quickly as the LSTM or GRU units while the RWA fails to learn at all. On the Wikipedia character prediction task the LSTM performs best but it followed closely by our RDA unit. Overall our RDA unit performs well and is sample efficient on a large variety of sequence tasks.", "creator": "LaTeX with hyperref package"}}}