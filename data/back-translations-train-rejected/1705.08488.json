{"id": "1705.08488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features", "abstract": "We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.", "histories": [["v1", "Tue, 23 May 2017 19:12:05 GMT  (36kb,D)", "http://arxiv.org/abs/1705.08488v1", "Submitted to NIPS 2017. (8 pages + 4 reference)"]], "COMMENTS": "Submitted to NIPS 2017. (8 pages + 4 reference)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["denis newman-griffis", "eric fosler-lussier"], "accepted": false, "id": "1705.08488"}, "pdf": {"name": "1705.08488.pdf", "metadata": {"source": "CRF", "title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features", "authors": ["Denis R. Newman-Griffis", "Eric Fosler-Lussier"], "emails": ["newman-griffis.1@osu.edu,", "fosler@cse.ohio-state.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are in a position to go into another world, to go into another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Alternate features for training embeddings", "text": "Recent research on improving word embedding in downstream tasks has examined a number of different directions. Levy and Goldberg [11] used syntactic dependencies as context and found improved functional similarity and decreased topical sensitivity. Morphological information on target and context words were also included to improve the robustness of language models in a number of studies, both at word level [20, 21, 22] and implicitly at sign level [23]. In addition, multilingual data was extensively studied to improve language models: Upadhyay et al. [24] reviewed several newer methods using multilingual corpora, while Faruqui and Dyer [25] and Lu et al. [26] used canonical correlation analyses to learn cross-language embeddings."}, {"heading": "2.2 Absolute positioning in the embedding space", "text": "Considerable research has also been invested in the analysis and interpretation of neural models for NLP, especially word embedding. Several studies have found correlations between individual dimensions in an example embedding set and semantic groupings [16, 33], as well as predictive results [17, 18]. Similar to the random initializations used in most embedding trainings, correlations of specific characteristics vary between studies and direct interpretability. In addition, Li et al. [34] showed a sensitivity of neural NLP models to noise in the entrance area and present regulation methods for compositional models to deal more robustly with input disturbances. Several studies have also shown a large variability in the reliability of semantic and syntactic information encoded linearly in vector space [35, 36, 37]. Linzen [19] has shown that many of the successes are based on cosmic structure or more consistent with similar tasks."}, {"heading": "3 Second-order embeddings", "text": "We present a method for generating second-order word embeddings from the nearest neighborhood structure, which is calculated using an upstream embeddings vocabulary. As we perform a series of upstream word embeddings V, NNkv is to denote the k nearest neighbors of each word v, as calculated by cosinal similarity. We use these k nearest neighborhood embeddings to induce a graph GV over the vocabulary in which each word v is a vertex, and the directed edge (v, w) is added for each w-word NNNkv. 2 An example embeddings is shown in Figure 1. Finally, we use node2vec [38], a newer method for learning unsupervised embeddings of graphs based on weighted random strokes to learn second-order embeddings for each word."}, {"heading": "4 Experiments", "text": "To explore the properties of second-order embedding, we apply them as input characteristics to proven algorithms for three tasks: Entity Recognition (NER), textual impact recognition, and paraphrase recognition. For all three tasks, we use existing methods that use word embedding as input characteristics; for the first two tasks, we use deep and highly nonlinear neural models, while the paraphrase recognition model is a simple logistical regression. Finally, we compare the closest neighborhood structure of first and second-order embedding. To control corpus and hyperparameter effects in the various tasks, we use the same sets of vectoral embedding in all applications. Our initial word embedding is trained on Gigaword; following Lample et al. [40] we remove the New York Times and LA Times parts of the corpus and then train the embedding of Skip-gram embedding with 4, with 2-dimensional [4], 2-dimensional [4] with 2-dimensional embedding."}, {"heading": "4.1 Named entity recognition", "text": "We first evaluate our embedding on the English NER data from the well-studied CoNLL 2003 shared task with an accuracy of only 0.5%. The goal of the task is to take as input uncommented documents, identifying individuals (PER), locations (LOC), organizations (ORG), or other entities that do not fit into one of these three categories (MISC). We adopt the NER system from Lample et al. [40], which is based on a bidirectional short-term memory (LSTM) network with a conditional random field (CRF) above the baseline, using character embedding learned during the training in addition to pre-initialized word embedding; for our experiments, we vary the embedding of words, but do not change the character behavior. Table 2 gives precision, recall, and F-score results for the full test set."}, {"heading": "4.2 Recognizing textual entailment", "text": "The data set consists of 570,152 pairs of sentences (550,152 for training, 10k for development, and 10k for testing), each marked with the label \"Impailment\" (sentence 1 contains sentence 2), \"Opposition\" (sentence 2 contradicts sentence 1), or \"Neutral\" (sentence 1 does not inform about sentence 2). To evaluate our embedding for this task, we use the Long Short-Term Memory Network Model proposed by Cheng et al. [43], which they evaluated for several machine reading tasks, including SNLI. We use the publicly available implementation of their model, 4, and compare second-order embedding with second-order embedding with second-order embedding as pre-initialization. We use the default settings of their implementation of a package size of 40, set embedding 450 dimensions, second-order embedding with results other than embedding 0.001."}, {"heading": "4.3 Paraphrase recognition", "text": "We also evaluate our embedding using the task of paraphrase recognition: in view of two sentences, the task is to decide whether sentence 2 is a paraphrase of sentence 1 or not. We use the well-examined Microsoft4 https: / / github.com / cheng6076 / SNLI-attentionResearch Paraphrase Corpus (MSRPC) [44], consisting of 5,801 sentence pairs (4,076 for training, 1,725 for the test), each of which is described as \"equivalent\" (3,900 pairs, 67%) or \"not equivalent.\" For this task, we follow the methodology of Blacoe and Lapata [45]. Specifically, we present each sentence as the sum of the embedding of its in-vocabulary words; the characteristic vector for a sentence pair is then either the concatenation or difference of the two sentence embedding in the order, but the concatenation in the sequence. For the classification of the LIN1, we use comlogistic regression of 001, as it is implied by IBAR in the second sentence."}, {"heading": "4.4 Neighborhood analysis", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 Discussion", "text": "The consistency of second-order embedding performance compared to first-order embedding, which typically differs by only 1 to 2 points in absolute terms in the deep and linear models, suggests that the closest neighbourhood topology of an embedding space contains the lion's share of the information important for these tasks, regardless of the values of the individual features. This is true despite the greater density of second-order embedding and the different semantic correlations that we observe between nearby points. However, some specific performance differences are evident when comparing unchained first-order and second-order embedding. In the task of the NER, the marked increase in precision of MISC units (which are very heterogeneous in their textual realizations) is an indication that the first-order and second-order embedding merely exhibit greater precision, that the second-order embedding signals are not linear, but that the embedding patterns of second-order embedding patterns are more effectively mirrored by second-order embedding patterns (where the second-order embedding signals are very textual)."}, {"heading": "6 Conclusion", "text": "We introduced second-order word embedding, derived from the closest neighbourhood topology of context-based word representations. We analyzed the effects of using these embedding in existing models for identifying named entities, detecting textual impacts, and paraphrase recognition, both as second-order information alone and linked to first-order contextual embedding. Our analysis showed that second-order embedding performs similar to their first-order counterparts, and we found that a high second-order similarity indicates a broad context rather than a contextual similarity. Our results suggest that second-order embedding is an interesting area for further research. In particular, the higher second-order similarity indicates that this second-order combination contains valuable information obtained through direct embedding of second-order information."}, {"heading": "Acknowledgments", "text": "The authors thank Adam Stiff and Chunxiao Zhou for helpful discussions and the Ohio Supercomputer Center [47] for using experimental resources. Denis is a Pre-Doc Fellow at the National Institutes of Health, Clinical Center."}], "references": [{"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "author": ["William L. Hamilton", "Jure Leskovec", "Dan Jurafsky"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "The Interplay of Semantics and Morphology in Word Embeddings", "author": ["Oded Avraham", "Yoav Goldberg"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The Role of Context Types and Dimensionality in Learning Word Embeddings. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT \u201916", "author": ["Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Dependency-Based Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308", "author": ["Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Symmetric Patterns and Coordinations: Fast and Enhanced Representations of Verbs and Adjectives", "author": ["Roy Schwartz", "Roi Reichart", "Ari Rappoport"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Retrofitting Word Vectors to Semantic Lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Intent detection using semantically enriched word embeddings", "author": ["Joo-Kyung Kim", "Gokhan Tur", "Asli Celikyilmaz", "Bin Cao", "Ye-Yi Wang"], "venue": "IEEE Spoken Language Technology Workshop (SLT),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Cross-Lingual Syntactically Informed Distributed Word Representations. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 408\u2013414", "author": ["Ivan Vuli\u0107"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "A Compositional and Interpretable Semantic Space", "author": ["Alona Fyshe", "Leila Wehbe", "Partha P Talukdar", "Brian Murphy", "Tom M Mitchell"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Visualizing and Understanding Neural Models in NLP", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Understanding Neural Networks through Representation Erasure", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1612.08220,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Issues in evaluating semantic spaces using word analogies", "author": ["Tal Linzen"], "venue": "In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Thang Luong", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Morphological Word-Embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Implicitly Incorporating Morphological Information into Word Embedding", "author": ["Yang Xu", "Jiawei Liu"], "venue": "arXiv preprint arXiv:1701.02481,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Character-aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "author": ["Shyam Upadhyay", "Manaal Faruqui", "Chris Dyer", "Dan Roth"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Deep Multilingual Correlation for Improved Word Embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Counter-fitting Word Vectors to Linguistic Constraints", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Brian MacWhinney", "Chris Dyer"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Ultradense Word Embeddings by Orthogonal Transformation", "author": ["Sascha Rothe", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Learning Syntactic Categories Using Paradigmatic Representations of Word Context", "author": ["Mehmet Ali Yatbaz", "Enis Sert", "Deniz Yuret"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Probabilistic Modeling of Joint-context in Distributional Similarity", "author": ["Oren Melamud", "Ido Dagan", "Jacob Goldberger", "Idan Szpektor", "Deniz Yuret"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Modeling Word Meaning in Context with Substitute Vectors", "author": ["Oren Melamud", "Ido Dagan", "Jacob Goldberger"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Sparse Overcomplete Word Vector Representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Learning Robust Representations of Text", "author": ["Yitong Li", "Trevor Cohn", "Timothy Baldwin"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance", "author": ["Billy Chiu", "Anna Korhonen", "Sampo Pyysalo"], "venue": "Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Analogy-based Detection of Morphological and Semantic Relations With Word Embeddings: What Works and What Doesn\u2019t", "author": ["Anna Gladkova", "Aleksandr Drozd", "Satoshi Matsuoka"], "venue": "Proceedings of the NAACL Student Research Workshop,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen", "author": ["Aleksandr Drozd", "Anna Gladkova", "Satoshi Matsuoka"], "venue": "In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Node2Vec: Scalable Feature Learning for Networks", "author": ["Aditya Grover", "Jure Leskovec"], "venue": "In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition", "author": ["Erik F Tjong Kim Sang", "Fien De Meulder"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2003}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Long Short-Term Memory-Networks for Machine Reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In Proceedings of Coling", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "A Comparison of Vector-based Representations for Semantic Composition", "author": ["William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP- CoNLL \u201912),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "LIB- LINEAR: A Library for Large Linear Classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Word embeddings are dense, low-dimensional vector representations of words that are commonly used as input features in a variety of natural language processing (NLP) tasks [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 2, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 3, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 4, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 5, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 162, "endOffset": 171}, {"referenceID": 6, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 162, "endOffset": 171}, {"referenceID": 7, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 162, "endOffset": 171}, {"referenceID": 8, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 269, "endOffset": 276}, {"referenceID": 9, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 269, "endOffset": 276}, {"referenceID": 10, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 11, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 12, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 256, "endOffset": 264}, {"referenceID": 13, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 256, "endOffset": 264}, {"referenceID": 14, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 274, "endOffset": 278}, {"referenceID": 15, "context": "A number of other recent studies have approached embeddings from another angle, by trying to analyze exactly what is encoded in the space characterized by the embedded representations [16, 17, 18].", "startOffset": 184, "endOffset": 196}, {"referenceID": 16, "context": "A number of other recent studies have approached embeddings from another angle, by trying to analyze exactly what is encoded in the space characterized by the embedded representations [16, 17, 18].", "startOffset": 184, "endOffset": 196}, {"referenceID": 17, "context": "A number of other recent studies have approached embeddings from another angle, by trying to analyze exactly what is encoded in the space characterized by the embedded representations [16, 17, 18].", "startOffset": 184, "endOffset": 196}, {"referenceID": 18, "context": "However, a recent study by Linzen [19] illustrated that in some semantic tasks, what matters most is neighborhood structure in the embedding space.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "Levy and Goldberg [11] utilized syntactic dependencies as context, and found improved functional similarity and decreased topical sensitivity.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 168, "endOffset": 180}, {"referenceID": 20, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 168, "endOffset": 180}, {"referenceID": 21, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 168, "endOffset": 180}, {"referenceID": 22, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 219, "endOffset": 223}, {"referenceID": 23, "context": "[24] review several recent methods with multilingual corpora, while Faruqui and Dyer [25] and Lu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[24] review several recent methods with multilingual corpora, while Faruqui and Dyer [25] and Lu et al.", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "[26] used canonical correlation analysis to learn cross-lingual embeddings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Recently, Vuli\u0107 [15] combined multilingual corpora with syntactic dependencies in embedding training, and observed improvements in both monolingual and cross-lingual tasks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "[13], Mrks\u0306i\u0107 et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27], and Kim et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] enrich pre-trained embeddings with semantic knowledge via lexical constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] find benefits from tailoring the learning curriculum or embedding training to specific downstream tasks, and Rothe et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] project pre-trained embeddings into task-specific subspaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] use second-order contexts in the form of possible lexical substitutions for word representations; Melamud et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31, 32, 10] adapt this approach for embedding learning, by using lexical substitutions to incorporate the joint contexts of two words and extend contextual information in training.", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "[31, 32, 10] adapt this approach for embedding learning, by using lexical substitutions to incorporate the joint contexts of two words and extend contextual information in training.", "startOffset": 0, "endOffset": 12}, {"referenceID": 9, "context": "[31, 32, 10] adapt this approach for embedding learning, by using lexical substitutions to incorporate the joint contexts of two words and extend contextual information in training.", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 32, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 165, "endOffset": 173}, {"referenceID": 17, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 165, "endOffset": 173}, {"referenceID": 33, "context": "[34] demonstrated sensitivity of neural NLP models to noise in the input space, and present regularization methods for compositional models to more robustly handle perturbations in input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Several studies have also shown wide variability in the reliability of semantic and syntactic information as encoded linearly in the vector space [35, 36, 37].", "startOffset": 146, "endOffset": 158}, {"referenceID": 35, "context": "Several studies have also shown wide variability in the reliability of semantic and syntactic information as encoded linearly in the vector space [35, 36, 37].", "startOffset": 146, "endOffset": 158}, {"referenceID": 36, "context": "Several studies have also shown wide variability in the reliability of semantic and syntactic information as encoded linearly in the vector space [35, 36, 37].", "startOffset": 146, "endOffset": 158}, {"referenceID": 18, "context": "Linzen [19] illustrated that many of the successes on similar tasks have relied more on nearest neighborhood structure than consistent affine transformations.", "startOffset": 7, "endOffset": 11}, {"referenceID": 37, "context": "Finally, we use node2vec [38], a recent method for learning unsupervised embeddings of graphs nodes based on weighted random walks, to learn second-order embeddings for each word in the vocabulary.", "startOffset": 25, "endOffset": 29}, {"referenceID": 38, "context": "[40], we remove the New York Times and LA Times portions of the corpus, and train skip-gram embeddings with word2vec [4] for 10 iterations, with vector dimensionality of 100, window size of 8, and minimum word frequency of 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[40], we remove the New York Times and LA Times portions of the corpus, and train skip-gram embeddings with word2vec [4] for 10 iterations, with vector dimensionality of 100, window size of 8, and minimum word frequency of 4.", "startOffset": 117, "endOffset": 120}, {"referenceID": 39, "context": "We first evaluate our embeddings on the English NER data from the well-studied CoNLL 2003 shared task [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 38, "context": "[40], which is based on a bidirectional long short-term memory (LSTM) network with a conditional random field (CRF) over the output layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "For textual entailment, we use the Stanford Natural Language Inference (SNLI) dataset [42].", "startOffset": 86, "endOffset": 90}, {"referenceID": 41, "context": "[43], which they evaluated on several machine reading tasks, including SNLI.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "Research Paraphrase Corpus (MSRPC) [44], consisting of 5,801 sentence pairs (4,076 for training, 1,725 for test), each of which is labeled as \u201cequivalent\u201d (3,900 pairs, 67%) or \u201cnot equivalent.", "startOffset": 35, "endOffset": 39}, {"referenceID": 43, "context": "We follow the methodology of Blacoe and Lapata [45] for this task.", "startOffset": 47, "endOffset": 51}, {"referenceID": 44, "context": "For classification, we use logistic regression as implemented in LIBLINEAR [46], with a cost parameter of 0.", "startOffset": 75, "endOffset": 79}], "year": 2017, "abstractText": "We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.", "creator": "LaTeX with hyperref package"}}}