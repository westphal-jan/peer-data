{"id": "1705.08492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Uplift Modeling with Multiple Treatments and General Response Types", "abstract": "Randomized experiments have been used to assist decision-making in many areas. They help people select the optimal treatment for the test population with certain statistical guarantee. However, subjects can show significant heterogeneity in response to treatments. The problem of customizing treatment assignment based on subject characteristics is known as uplift modeling, differential response analysis, or personalized treatment learning in literature. A key feature for uplift modeling is that the data is unlabeled. It is impossible to know whether the chosen treatment is optimal for an individual subject because response under alternative treatments is unobserved. This presents a challenge to both the training and the evaluation of uplift models. In this paper we describe how to obtain an unbiased estimate of the key performance metric of an uplift model, the expected response. We present a new uplift algorithm which creates a forest of randomized trees. The trees are built with a splitting criterion designed to directly optimize their uplift performance based on the proposed evaluation method. Both the evaluation method and the algorithm apply to arbitrary number of treatments and general response types. Experimental results on synthetic data and industry-provided data show that our algorithm leads to significant performance improvement over other applicable methods.", "histories": [["v1", "Tue, 23 May 2017 19:20:18 GMT  (594kb,D)", "http://arxiv.org/abs/1705.08492v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yan zhao", "xiao fang", "david simchi-levi"], "accepted": false, "id": "1705.08492"}, "pdf": {"name": "1705.08492.pdf", "metadata": {"source": "CRF", "title": "Uplift Modeling with Multiple Treatments and General Response Types", "authors": ["Yan Zhao", "Xiao Fang", "David Simchi-Levi"], "emails": ["zhaoyanmit@gmail.com", "ustcfx@gmail.com", "dslevi@mit.edu"], "sections": [{"heading": null, "text": "Accepted: 2017 SIAM International Conference on Data Mining (SDM2017)"}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Evaluation of Uplift Models", "text": "Before introducing the evaluation method, we first describe the mathematical formulation of the survey problems and the actual notation used throughout this paper (2.1 Problem formulation and notation We use uppercase letters to mark random variables and lowercase letters, their realizations. We use bold face for vectors and normal font for scalers. \u2022 X represents the attribute vector and its realization. \u2022 X represents the treatment. We assume that there are different treatments that are given as {1,.,. K} the control group is given as T = 0. \u2022 Let Y indicate the answer and its realization the d-dimensional attribute space. \u2022 T represents the treatment. We assume that there are different treatments that are given as {1,.,., K} the control group is given as T = 0. \u2022 Let Y show the answer and its realization."}, {"heading": "3 The CTS Algorithm", "text": "In combination with ensembles, they prove to be one of the most powerful algorithms for general classification and regression problems. Even for the relatively new upgrade modeling problem, there are some reports of the excellent performance of tree groups [12]. The algorithm we present in this section also generates a tree ensemble. We refer to it as the CTS algorithm, which stands for contextual treatment selection. Unique to CTS is its splitting criterion, which directly maximizes the expected response from the tree as measured on the basis of education. 3.1 Splitting criterion We take the recursive binary splitting approach. Each partition creates two new branches further down the tree. Let's leave the feature space associated with a Leaf node. The best we can do for subjects falling into the subspace is to assign optimal treatment."}, {"heading": "4 Experimental Evaluation", "text": "In this section, we present an experimental comparison between the proposed CTS algorithm and other applicable application factors for the allocation of treatments: Different treatment methods are used for several benchmark datasets; the first dataset is generated using a 50-dimensional artificial data model; knowledge of the true data model allows us to compare methods without worrying about average treatment accuracy; and next, we compare the methods against two large-scale industry1Exact values of data model parameters and datasets that can be found at this Dropbox link: / / www.dropbox.com / sf7nu2uw8tcwreu / AAAhqQnaUpR5vCfxSsYsYsMda? dl = 0Algorithm 1 CTS - Contextual Treatment SelectionInput: training data sN, number of samples in each tree B (B \u2264 N), the number of variables to be considered for dividing."}, {"heading": "5 Conclusion", "text": "The uplift model initially attracted attention through its successful application in marketing and insurance, but it does not have to be limited to these areas. Any situation where personalized treatment selection is desired and a randomized experiment is possible can be a potential use case for uplift modeling. In Section 4, for example, we described how it can be applied to customized pricing. This paper's contribution to uplift modeling is threefold: First, we present a way to obtain an unbiased estimate of the expected response under an uplift model that was previously unavailable in the literature; second, we design a tree ensemble algorithm with a splitting criterion based on the new estimation methodology. Both the unbiased estimate and the algorithm naturally apply to multiple treatments and a continuous response, significantly expanding the current focus of the uplift algorithms on binary reaction cases with only one treatment; and finally, we showed that our algorithm leads to 15% - 40% more customization revenue than customization algorithms reserve on individual seat elevators."}, {"heading": "Acknowledgment", "text": "This work was partially supported by Accenture through the Accenture-MIT Alliance in Business Analytics."}], "references": [{"title": "Incremental value modeling", "author": ["B. Hansotia", "B. Rukstales"], "venue": "Journal of Interactive Marketing, 16.3 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A proportional hazards approach to campaign list selection", "author": ["C. Manahan"], "venue": "SAS User Group International (SUGI) 30 Proceedings", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "The true lift model - A novel data mining approach to response modeling in database management", "author": ["V.S.Y. Lo"], "venue": "ACM SIGKDD Explorations Newsletter, 4.2 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Real-world uplift modelling with significance-based uplift trees", "author": ["N.J. Radcliffe", "P.D. Surry"], "venue": "White Paper TR-2011-1, Stochastic Solutions ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Support vector machines for uplift modeling", "author": ["L. Zaniewicz", "S. Jaroszewicz"], "venue": "2013 IEEE 13th International Conference on Data Mining Workshops (ICDMW), IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved Statistical Methods Are Needed to Advance Personalized Medicine, The open translational medicine journal", "author": ["F. Alemi"], "venue": "PMC. Web", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Facilitating score and causal inference trees for large observational studies", "author": ["X. Su", "J. Kang", "J. Fan", "R.A. Levine", "X. Yan"], "venue": "The Journal of Machine Learning Research, 13.1 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A decision theoretic approach to targeted advertising", "author": ["D.M. Chickering", "D. Heckerman"], "venue": "Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (pp. 82-88). Morgan Kaufmann Publishers Inc.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Decision trees for uplift modeling", "author": ["P. Rzepakowski", "S. Jaroszewicz"], "venue": "Proceedings of the 10th IEEE International Conference on Data Mining (ICDM), Sydney, Australia,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A survey of personalized treatment models for pricing strategies in insurance", "author": ["L. Guelman", "M. Guillen", "A.M. Perez-Marin"], "venue": "Insurance: Mathematics and Economics, 58 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification and regression trees", "author": ["L. Brieman", "J. Friedman", "R. Olshen", "C. Stone"], "venue": "Wadsworth Inc", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1984}, {"title": "Ensemble methods for uplift modeling", "author": ["M. Sotys", "S. Jaroszewicz", "P. Rzepakowski"], "venue": "Data mining and knowledge discovery 29.6 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Decision trees for uplift modeling with single and multiple treatments", "author": ["P. Rzepakowski", "S. Jaroszewicz"], "venue": "Knowledge and Information Systems 32.2 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "uplift: Uplift Modeling. R package version 0.3.5", "author": ["Leo Guelman"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Using control groups to target on predicted lift: Building and assessing uplift models", "author": ["N.J. Radcliffe"], "venue": "Direct Market J Direct Market Assoc Anal Council 1 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "S", "author": ["M. Fernandez-Delgado", "E. Cernadas"], "venue": "Barro, and D.Amorim, Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?, Journal of Machine Learning Research 15 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Random forests.", "author": ["L. Breiman"], "venue": "Machine learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Optimal personalized treatment rules for marketing interventions: A review of methods", "author": ["L. Guelman", "M. Guillen", "A.M. Prez-Marn"], "venue": "a new proposal, and an insurance case study ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical selection of SVM parameters and noise estimation for SVM regression", "author": ["V. Cherkassky", "Y. Ma"], "venue": "Neural networks, 17.1 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Applications of SMA include direct marketing [1] and customer retention [2].", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "Applications of SMA include direct marketing [1] and customer retention [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "However, the Separate Model Approach, while simple and correct in principle, does not always perform well in real-world situation [3][4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "However, the Separate Model Approach, while simple and correct in principle, does not always perform well in real-world situation [3][4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "See [4] for an illustrative example.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "A logistic regression formulation is proposed which explicitly includes interaction terms between features and the treatment [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "Support Vector Machine is adapted for uplift modeling to predict whether a subject will be positively, neutrally, or negatively affected by the treatment [5].", "startOffset": 154, "endOffset": 157}, {"referenceID": 5, "context": "The adaption of K-Nearest Neighbors for uplift modeling is briefly mentioned in both [6] and [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "The adaption of K-Nearest Neighbors for uplift modeling is briefly mentioned in both [6] and [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Several tree-based algorithms have been proposed for uplift modeling, each with a different splitting criterion [8] [1] [9] [4] [10].", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "Several tree-based algorithms have been proposed for uplift modeling, each with a different splitting criterion [8] [1] [9] [4] [10].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "Several tree-based algorithms have been proposed for uplift modeling, each with a different splitting criterion [8] [1] [9] [4] [10].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Several tree-based algorithms have been proposed for uplift modeling, each with a different splitting criterion [8] [1] [9] [4] [10].", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "Several tree-based algorithms have been proposed for uplift modeling, each with a different splitting criterion [8] [1] [9] [4] [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "In [8], the authors modify the standard decision tree construction procedure [11] by forcing a split on the treatment at each leaf node.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In [8], the authors modify the standard decision tree construction procedure [11] by forcing a split on the treatment at each leaf node.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "In [1] a splitting criterion is employed that maximizes the difference between the difference between the treatment and control probabilities in the left and right child nodes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [9], splitting points are chosen that maximize the distributional difference between two child nodes as measured by a weighted Kullback-Leibler divergence and a weighted squared Euclidean distance.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [4], a linear model is fitted to each candidate split and the significance of the interaction term is used as the measure of the split quality.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In [10], the variable that has the smallest p-value in the hypothesis test on the interaction between the response and itself is selected as the splitting variable.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "It is demonstrated experimentally that the use of Bagging or Random Forest on uplift trees often results in significant improvement in performance [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "Rare exceptions include [13] and [14].", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "In [13], the tree-based algorithm described in [9] is extended to multiple treatment cases by using a weighted sum of pairwise distributional divergence as the splitting criterion.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [13], the tree-based algorithm described in [9] is extended to multiple treatment cases by using a weighted sum of pairwise distributional divergence as the splitting criterion.", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "It is worth mentioning that the causal K-nearest neighbors originally intended for single treatment can be naturally generalized to multiple treatments [6] [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "It is worth mentioning that the causal K-nearest neighbors originally intended for single treatment can be naturally generalized to multiple treatments [6] [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 13, "context": "This algorithm is implemented in a R package called uplift by Leo Guelman [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "For single treatment cases, qini curves and uplift curves have been used to serve the purpose [16] [9].", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "For single treatment cases, qini curves and uplift curves have been used to serve the purpose [16] [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "In [9], the authors explained that they use uplift curves because there does not seem to be a better option at the time.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "On all of the data sets, CTS demonstrates superior performance compared to other applicable methods which include Separate Model Approach with Random Forest/Support Vector Regression/K-Nearest Neighbors/AdaBoost, and Uplift Random Forest (upliftRF) as implemented in the R uplift package [15].", "startOffset": 288, "endOffset": 292}, {"referenceID": 10, "context": "Tree-based methods are time-tested tools in Machine Learning [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "When combined into ensembles, they prove to be among the most powerful algorithms for general classification and regression problems [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "Even for the relatively new uplift modeling problem, there have been some reports on the excellent performance of tree ensembles [12].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "3 The Algorithm To mitigate the overfitting problem commonly associated with a single tree, we formulate CTS in a form similar to Random Forest [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "1 Synthetic Data The feature space is the fiftydimensional hyper-cube of length 10, or X = [0, 10].", "startOffset": 91, "endOffset": 98}, {"referenceID": 9, "context": ", Xd \u223c U[ 0, 10 ], for d = 1, .", "startOffset": 8, "endOffset": 17}, {"referenceID": 13, "context": "They are the separate model approach with Random Forest (SMA-RF), Support Vector Machine (SMA-SVM), Adaboost (SMA-Ada), K-Nearest Neighbors (SMA-KNN), as well as the uplift Random Forest method implemented in [15], and CTS.", "startOffset": 209, "endOffset": 213}], "year": 2017, "abstractText": "Randomized experiments have been used to assist decisionmaking in many areas. They help people select the optimal treatment for the test population with certain statistical guarantee. However, subjects can show significant heterogeneity in response to treatments. The problem of customizing treatment assignment based on subject characteristics is known as uplift modeling, differential response analysis, or personalized treatment learning in literature. A key feature for uplift modeling is that the data is unlabeled. It is impossible to know whether the chosen treatment is optimal for an individual subject because response under alternative treatments is unobserved. This presents a challenge to both the training and the evaluation of uplift models. In this paper we describe how to obtain an unbiased estimate of the key performance metric of an uplift model, the expected response. We present a new uplift algorithm which creates a forest of randomized trees. The trees are built with a splitting criterion designed to directly optimize their uplift performance based on the proposed evaluation method. Both the evaluation method and the algorithm apply to arbitrary number of treatments and general response types. Experimental results on synthetic data and industry-provided data show that our algorithm leads to significant performance improvement over other applicable methods. Accepted: 2017 SIAM International Conference on Data", "creator": "LaTeX with hyperref package"}}}