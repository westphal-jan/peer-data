{"id": "1705.08498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Clinical Intervention Prediction and Understanding using Deep Networks", "abstract": "Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are noisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper, we integrate data from all available ICU sources (vitals, labs, notes, demographics) and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner to enable \"real-time\" performance, and predictions are made with a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on our predictive tasks using deep architectures. We explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines in intervention prediction, and provide insight into model learning, which is crucial for the adoption of such models in practice.", "histories": [["v1", "Tue, 23 May 2017 19:42:20 GMT  (760kb,D)", "http://arxiv.org/abs/1705.08498v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harini suresh", "nathan hunt", "alistair johnson", "leo anthony celi", "peter szolovits", "marzyeh ghassemi"], "accepted": false, "id": "1705.08498"}, "pdf": {"name": "1705.08498.pdf", "metadata": {"source": "CRF", "title": "Clinical Intervention Prediction and Understanding using Deep Networks", "authors": ["Harini Suresh", "Nathan Hunt", "Alistair Johnson", "Leo Anthony Celi"], "emails": ["HSURESH@MIT.EDU", "NHUNT@MIT.EDU", "AEWJ@MIT.EDU", "LCELI@MIT.EDU", "PSZ@MIT.EDU", "MGHASSEM@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2. Background and Related Work", "text": "Clinical decision-making often takes place in situations of limited knowledge and high uncertainty; for example, only 10 of the 72 ICU interventions evaluated in randomized controlled trials (RCTs) are associated with improved outcomes (Ospina-Tasco \u0301 n et al., 2008). Our goal is to gain insights from health data previously collected for the primary purpose of facilitating patient care. Recent studies have used relapsing neural networks (RNNNs) to model sequential EHR data to label ICU signals with billing codes (Che et al., 2016; Lipton et al., 2015; Choi et al.) to identify the effects of various anti-diabetes drugs (Krishnan et al., 2015). Razavian et al al al al al al al al al al al al al al al al al (2016) compared CNNs with LSTMs for longitudinal billing codes using laboratory tests."}, {"heading": "3. Data and Preprocessing", "text": "See Figure 1 for a general description of the data flow."}, {"heading": "3.1 Data Source", "text": "We use data from the Multiparameter Intelligent Monitoring in Intensive Care database (MIMIC-III v1.4) (Johnson et al., 2016). MIMIC is publicly available and contains over 58,000 hospital admissions of approximately 38,600 adults. We look at patients aged 15 years and older who have had intensive care stays of 12 to 240 hours and consider only the first intensive care stay of each patient, resulting in 34,148 unique intensive care stays."}, {"heading": "3.2 Data Extraction and Preprocessing", "text": "For each patient we extract: 1. 5 static variables such as sex and age 2. 29 time-varying vital signs and laboratories such as oxygen saturation and blood urea nitrogen 3. All available unidentified clinical notes for each patient as time series over their entire stay (see Table 3 for a full listing of variables) Static variables were replicated over all time periods for each patient. Vital and laboratory measurements receive time stamps rounded to the next hour. If one hour has multiple measurements for a signal, these measurements are averaged."}, {"heading": "3.3 Representation of Notes and Vitals", "text": "Clinical narration notes were processed to generate a 50-dimensional vector of the theme proportions for each note using latent Dirichlet allocation (Lead et al., 2003; Griffiths and Steyvers, 2004), which is replicated forward and aggregated over time (Ghassemi et al., 2014). For example, if a patient had a note A to hour 3 and a note B to hour 7, hours 3-6 would contain the topic distribution of A, while hours 7 starting from this variable would contain the aggregated topic distribution of A and B. We compare raw physiological data with physiological words, where we categorize the vital data by first converting each value into a z-score based on the population mean and the standard deviation for that variable, and then rounding up that score to the nearest integer and then categorizing it between -4 and 4. Each z-score value then becomes its own column, allowing an explicit misrepresentation."}, {"heading": "3.4 Prediction Task", "text": "When predicting ventilation, non-invasive ventilation or vasopressors, the model classifies the prediction window as one of four possible outcomes: 1) insertion, 2) weaning, 3) staying with the intervention, 4) discontinuation of the intervention. A prediction window is a beginning when there is a transition from a marking of 0 to 1 for the patient during this window; weaning is the opposite: a transition from 1 to 0. A window is classified as \"Stay tuned\" if the label for the entire window is 1 or \"Stay away\" if 0. When predicting colloid or crystalloid bolts, we classify the prediction window into one of two classes: 1) insertion or 2) no insertion as these interventions are not administered for the duration of time. After dividing the patient records into 1.15101 individual examples each for the intervention class, we will go up to 1.151)."}, {"heading": "4. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Long Short-Term Memory Network (LSTM)", "text": "We use long-term short-term memory networks (LSTM) as our first model. After seeing the input sequence x1... xt of a given example, we predict y-t, a probability distribution over the results, with the target result yt: h1... ht = LSTM (x1.. xt) (1) y-t = Softmax (Wyht + by) (2), where xi-RV, Wy-RNC \u00b7 L2, ht-RL2, by-RNC, where V is the dimensionality of the input (number of variables), NC is the number of classes we predict, and L2 the second hidden layer size. For a schematic model, see Figure 3a and for more details on implementing the model, see Appendix."}, {"heading": "4.2 Convolution Neural Network (CNN)", "text": "We use a similar CNN architecture to Razavian et al. (2016), except that we do not at first summarize the features into an intermediate representation. We present features as channels and execute 1D-temporal turns instead of treating the input as a 2D image. Our architecture consists of temporal turns in three different temporal granularities, each with 64 filters. The dimensions of the filters are 1 \u00d7 i, where i-shaped. We cushion the inputs so that the outputs from the wave layers are equal in size, and use a step of 1. Each conversion is followed by a max pooling layer with a pooling size of 3. The outputs from all three temporal granularities are linked and flattened, followed by 2 fully connected layers with dropouts in between and a softmax above the output (Figure 3b)."}, {"heading": "4.3 Experimental Settings", "text": "We use a pull / validation / test split of 70 / 10 / 20 and stratify the splits based on the result. For the LSTM, we use dropout with a probability of 0.8 during training (on stacked layers only) and L2 regularization with lambda = 0.0001. We use 2 hidden LSTM layers of 512 nodes each. For CNN, we use dropout between fully connected layers with a probability of 0.5. We use a weighted loss function during optimization to account for class imbalances. All parameters were determined by cross-validation with the validation set. We implemented all models in TensorFlow version 1.0.1 with the Adam Optimizer on mini-batches of 128 examples. We determine when we finish training with an early stop based on the AUC based on the validation set."}, {"heading": "4.4 Evaluation", "text": "If there are K classes with one AUC per class of AUCk, then the macro AUC is defined as the average of AUCS per class, AUCmacro = 1K \u2211 k AUCk. We use the macro AUC as a total score because it weights the AUCs of all classes equally regardless of their class size (Manning et al., 2008). This is important due to the large class imbalance in the data. We use the L2 regulated logistic regression (LR) as the baseline for comparison with neural networks (Pedregosa et al., 2011)."}, {"heading": "4.5 Interpretibility", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.5.1 LSTM FEATURE-LEVEL OCCLUSIONS", "text": "Zeiler and Fergus (2013) use occlusion to understand how models process images: They remove a region of the image (by setting all values in that region to 0) and compare the prediction of that hidden image with the original prediction of the model. A large shift in the prediction implies that the hidden region contains important information for correct prediction. With our LSTM model, we remove characteristics one by one from the patients (by replacing the given characteristic with noise obtained from a uniform distribution in [0.1). Then, we compare the predictiveness of the model with and without each characteristic; if this difference is large, the model relies heavily on this characteristic to make the prediction."}, {"heading": "4.5.2 CNN FILTER/ACTIVATION VISUALIZATION", "text": "To understand how CNN uses patient data to predict specific tasks, we find and compare the ten best real-world examples of which our model is the most likely and least likely to predict a particular outcome. Since our gap time is 6 hours, this means that the model predicts a high probability of starting the task 6 hours after the trajectory has been identified. Second, we generate \"hallucinations\" from the model that maximize the predicted probability for a particular task (Erhan et al., 2009) by creating an objective function that maximizes the activation of a particular starting node, and by backpropagating gradients back into the input image, adjusting the image to maximize the output node."}, {"heading": "5. Results", "text": "The AUCs for each of our five intervention types and 4 prediction tasks are presented for all models in Table 2. All models use 6-hour \"raw data,\" either transformed into a range of 0-1 (normalized and implied) or discredited into physiological words (Section 3.3)."}, {"heading": "5.1 Physiological Words Improve Predictive Task Performance With High Class Imbalance", "text": "We observed a significant increase in AUC in some interventions when we used physiological words - particularly for the start of ventilation (from 0.61 to 0.75) and the start of the colloid bolus (from 0.52 to 0.72), which have the lowest proportion of insertion examples (Table 1). This could be because physiological words have a smoothing effect. By rounding the z-value for each value to the next integer, they are likely to be represented as the same word in a patient with a heart rate of 87 at one hour and then 89 at the next. This could make the model immutable to small fluctuations in patient data and more resistant to excessive small classes. In addition, the physiological word representation has an explicit coding for missing data. This is in contrast to the raw data that was prefilled in and intermediated, making it difficult for the model to know how safe it is in the measurements that exist (Che et, 2016)."}, {"heading": "5.2 Feature-Level Occlusions Identify Important Per-Class Features", "text": "We are able to interpret the predictions of the LSTM using functional closures (Section 4.5.1). We note that vital systems, laboratories, topics and static data are important for various interventions (Figure 4). Table 5 provides a full listing of the most likely words for each topic mentioned. For mechanical ventilation, the five most important features for weaning and insertion are consistent (pH, sodium, lactate, hemoglobin and potassium). This is useful because all important laboratory values are used to assess a patient's physiological stability, and ventilation is an aggressive intervention. However, ventilation additionally puts a patient's importance on the Coma Score (GCS) and Topic 4 (Assessment of Patient Consciousness), probably because patient seditions are a critical part of mechanical ventilation, and the physical difference between ventilation and weaning is."}, {"heading": "5.3 Convolutional Filters Target Short-term Trajectories", "text": "We are able to understand CNN by examining maximum activating patient pathways (Section 4.5.2). Figure 5 shows the mean with standard deviation error bars for four of the most differentiated features of the 10 real patient pathways, which represent the highest and lowest activation for each task. Trends suggest that patients requiring ventilation in the future have higher diastolic blood pressure, a higher respiratory rate, a lower heart rate, and lower oxygen saturation, possibly corresponding to patients experiencing hyperventilation. For vasopressor attacks, we see a decrease in systolic blood pressure, heart rate, and oxygen saturation, which could indicate either altered peripheral perfusion or stressed hyperglycemia. Topic 3, which was important for the vasopressor approach, is also a decrease in systolic blood pressure, heart rate, and oxygen saturation."}, {"heading": "6. Conclusion", "text": "To our knowledge, our model is the first to use deep neural networks to predict both the onset and weaning of interventions using all available modalities of ICU data. In our tasks, deep learning methods beat the state of the art of modern medical AUCs reported in previous work on intervention prediction tasks, which is useful because previous work focused on single targets with smaller data sets (Wu et al., 2016) or on unattended representations prior to supervised training (Ghassemi et al., 2017). We also point out that LSTM has significantly improved the permeability of the two intervention tasks with the lowest incidence rate across physiological words - possibly because this representation encodes important information about what is \"normal\" for each physiological value (Ghassemi et al., 2017)."}, {"heading": "B. Physiological Word Generation", "text": "See Figure 7."}, {"heading": "C. LSTM Model Details", "text": "LSTM performs the following update equations for a single layer, taking into account its previous hidden state and the new input: ft = \u03c3 (Wf [ht \u2212 1, xt] + bf) (3) it = \u03c3 (Wi [ht \u2212 1, xt] + bi) (4) c-t = tanh (Wc [ht \u2212 1, xt] + bc) (5) ct = ft ct \u2212 1 + it c-t (6) ot = \u03c3 (Wo [ht \u2212 1, xt] + bo) (7) ht = ot-tanh (ct) (8), where Wf, Wi, Wc, Wo-RL1 \u00b7 (L1 + V), bf, bi, bc, bo-RL1 are learned parameters, and ft, it, c-t, ct, ht-t, ht-RL1. In these equations it represents an elementary application of the sigmoid function (logistically) and is an elementary product."}], "references": [{"title": "Therapeutic strategies for highdose vasopressor-dependent shock", "author": ["Estev\u00e3o Bassi", "Marcelo Park", "Luciano Cesar Pontes Azevedo"], "venue": "Critical care research and practice,", "citeRegEx": "Bassi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bassi et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Adoption of electronic health record systems among us non-federal acute care hospitals: 2008-2012", "author": ["Dustin Charles", "Meghan Gabriel", "Michael F Furukawa"], "venue": "ONC data brief,", "citeRegEx": "Charles et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Charles et al\\.", "year": 2013}, {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "venue": "arXiv preprint arXiv:1606.01865,", "citeRegEx": "Che et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Che et al\\.", "year": 2016}, {"title": "Doctor AI: predicting clinical events via recurrent neural networks", "author": ["Edward Choi", "Mohammad Taha Bahadori", "Jimeng Sun"], "venue": "CoRR, abs/1511.05942,", "citeRegEx": "Choi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2015}, {"title": "Retain: An interpretable predictive model for healthcare using reverse time attention mechanism", "author": ["Edward Choi", "Mohammad Taha Bahadori", "Jimeng Sun", "Joshua Kulas", "Andy Schuetz", "Walter Stewart"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Choi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Blood pressure targets for vasopressor therapy: A systematic review", "author": ["Frederick DAragon", "Emilie P Belley-Cote", "Maureen O Meade", "Fran\u00e7ois Lauzier", "Neill KJ Adhikari", "Matthias Briel", "Manoj Lalu", "Salmaan Kanji", "Pierre Asfar", "Alexis F Turgeon"], "venue": "Shock, 43(6):530\u2013539,", "citeRegEx": "DAragon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "DAragon et al\\.", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Technical report, University of Montreal,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Predicting clinical events by combining static and dynamic information using recurrent neural networks", "author": ["Crist\u00f3bal Esteban", "Oliver Staeck", "Stephan Baier", "Yinchong Yang", "Volker Tresp"], "venue": "In Healthcare Informatics (ICHI),", "citeRegEx": "Esteban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Esteban et al\\.", "year": 2016}, {"title": "Diseasebased modeling to predict fluid response in intensive care units", "author": ["AS Fialho", "LA Celi", "F Cismondi", "SM Vieira", "SR Reti", "JM Sousa", "SN Finkelstein"], "venue": "Methods Inf Med,", "citeRegEx": "Fialho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fialho et al\\.", "year": 2013}, {"title": "A multivariate timeseries modeling approach to severity of illness assessment and forecasting in icu with sparse, heterogeneous clinical data", "author": ["Marzyeh Ghassemi", "Marco AF Pimentel", "Tristan Naumann", "Thomas Brennan", "David A Clifton", "Peter Szolovits", "Mengling Feng"], "venue": "In Proc. Twenty-Ninth AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Ghassemi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghassemi et al\\.", "year": 2015}, {"title": "Predicting intervention onset in the icu with switching state space models", "author": ["Marzyeh Ghassemi", "Mike Wu", "Michael Hughes", "Finale Doshi-Velez"], "venue": "In Proceedings of the AMIA Summit on Clinical Research Informatics (CRI),", "citeRegEx": "Ghassemi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ghassemi et al\\.", "year": 2017}, {"title": "Finding scientific topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "In PNAS,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Office-based physician electronic health record adoption", "author": ["E Yang N Jamoom", "E. Hing"], "venue": "Office of the National Coordinator for Health Information Technology,", "citeRegEx": "Jamoom and Hing,? \\Q2016\\E", "shortCiteRegEx": "Jamoom and Hing", "year": 2016}, {"title": "MIMIC-III, a freely accessible critical care database", "author": ["Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "Li-wei H Lehman", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark"], "venue": "Scientific data,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Deep kalman filters", "author": ["Rahul G Krishnan", "Uri Shalit", "David Sontag"], "venue": "arXiv preprint arXiv:1511.05121,", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "Learning to diagnose with lstm recurrent neural networks", "author": ["Zachary C Lipton", "David C Kale", "Charles Elkan", "Randall Wetzell"], "venue": "arXiv preprint arXiv:1511.03677,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Fluid overload, de-resuscitation, and outcomes in critically ill or injured patients: a systematic review with suggestions for clinical practice", "author": ["ML Malbrain", "Paul E Marik", "Ine Witters", "Colin Cordemans", "Andrew W Kirkpatrick", "Derek J Roberts", "Niels Van Regenmortel"], "venue": "Anaesthesiol Intensive Ther,", "citeRegEx": "Malbrain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malbrain et al\\.", "year": 2014}, {"title": "Introduction to Information Retrieval", "author": ["Christopher Manning", "Prabhakar Raghavan", "Hinrich Schtze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Vasopressors for shock", "author": ["Marcus M\u00fcllner", "Bernhard Urbanek", "Christof Havel", "Heidrun Losert", "Gunnar Gamper", "Harald Herkner"], "venue": "The Cochrane Library,", "citeRegEx": "M\u00fcllner et al\\.,? \\Q2004\\E", "shortCiteRegEx": "M\u00fcllner et al\\.", "year": 2004}, {"title": "Multicenter, randomized, controlled trials evaluating mortality in intensive care: Doomed to fail", "author": ["Gustavo A Ospina-Tasc\u00f3n", "Gustavo Luiz B\u00fcchele", "Jean-Louis Vincent"], "venue": "Critical care medicine,", "citeRegEx": "Ospina.Tasc\u00f3n et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ospina.Tasc\u00f3n et al\\.", "year": 2008}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Multi-task prediction of disease onsets from longitudinal lab tests", "author": ["Narges Razavian", "Jake Marcus", "David Sontag"], "venue": "In JMLR (Journal of Machine Learning Research): MLHC Conference Proceedings,", "citeRegEx": "Razavian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2016}, {"title": "Ensemble fuzzy models in personalized medicine: Application to vasopressors administration", "author": ["C\u00e1tia M Salgado", "Susana M Vieira", "Lu\u0131\u0301s F Mendon\u00e7a", "Stan Finkelstein", "Jo\u00e3o MC Sousa"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "Salgado et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salgado et al\\.", "year": 2016}, {"title": "Principles and practice of mechanical ventilation", "author": ["Martin J Tobin"], "venue": null, "citeRegEx": "Tobin.,? \\Q2006\\E", "shortCiteRegEx": "Tobin.", "year": 2006}, {"title": "Critical care-where have we been and where are we going", "author": ["Jean-Louis Vincent"], "venue": "Critical Care, 17 (Suppl 1):S2,", "citeRegEx": "Vincent.,? \\Q2013\\E", "shortCiteRegEx": "Vincent.", "year": 2013}, {"title": "Understanding vasopressor intervention and weaning: Risk prediction in a public heterogeneous clinical time series database", "author": ["Mike Wu", "Marzyeh Ghassemi", "Mengling Feng", "Leo A Celi", "Peter Szolovits", "Finale Doshi-Velez"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "CoRR, abs/1311.2901,", "citeRegEx": "Zeiler and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "As Intensive Care Units (ICUs) play an increasing role in acute healthcare delivery (Vincent, 2013), clinicians must anticipate patient care needs in a fast-paced, data-overloaded setting.", "startOffset": 84, "endOffset": 99}, {"referenceID": 2, "context": "The widespread availability of electronic healthcare data (Charles et al., 2013; Jamoom E and E, 2016) allows new investigations into evidence-based decision support, where we can learn when patients need a given intervention.", "startOffset": 58, "endOffset": 102}, {"referenceID": 27, "context": "Mechanical ventilation is commonly used for breathing assistance, but has many potential complications (Yang and Tobin) and small changes in ventilation settings can have large impact in patient outcomes (Tobin, 2006).", "startOffset": 204, "endOffset": 217}, {"referenceID": 22, "context": "Vasopressors are a common ICU medication, but there is no robust evidence of improved outcomes from their use (M\u00fcllner et al., 2004), and some evidence they may be harmful (DAragon et al.", "startOffset": 110, "endOffset": 132}, {"referenceID": 7, "context": ", 2004), and some evidence they may be harmful (DAragon et al., 2015).", "startOffset": 47, "endOffset": 69}, {"referenceID": 20, "context": "Both are often considered as less aggressive alternatives to vasopressors, but there are no multi-center trials studying whether fluid bolus therapy should be given to critically ill patients, only studies trying to distinguish which type of fluid should be given (Malbrain et al., 2014).", "startOffset": 264, "endOffset": 287}, {"referenceID": 15, "context": "We use long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), which have been shown to effectively model complicated dependencies in timeseries data (Bengio et al.", "startOffset": 46, "endOffset": 80}, {"referenceID": 1, "context": "We use long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), which have been shown to effectively model complicated dependencies in timeseries data (Bengio et al., 1994).", "startOffset": 169, "endOffset": 190}, {"referenceID": 14, "context": "Previously, LSTMs have achieved state-of-the-art results in many different applications, such as machine translation (Hermann et al., 2015), dialogue systems (Chorowski et al.", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": ", 2015), dialogue systems (Chorowski et al., 2015) and image captioning (Xu et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 30, "context": ", 2015) and image captioning (Xu et al., 2015).", "startOffset": 29, "endOffset": 46}, {"referenceID": 25, "context": "We compare the LSTM models to a convolutional neural network (CNN) architecture that has previously been explored for longitudinal laboratory data (Razavian et al., 2016).", "startOffset": 147, "endOffset": 170}, {"referenceID": 23, "context": "Clinical decision-making often happens in settings of limited knowledge and high uncertainty; for example, only 10 of the 72 ICU interventions evaluated in randomized controlled trials (RCTs) are not associated with improved outcomes (Ospina-Tasc\u00f3n et al., 2008).", "startOffset": 234, "endOffset": 262}, {"referenceID": 3, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al.", "startOffset": 137, "endOffset": 195}, {"referenceID": 19, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al.", "startOffset": 137, "endOffset": 195}, {"referenceID": 4, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al.", "startOffset": 137, "endOffset": 195}, {"referenceID": 18, "context": ", 2015), to identify the impact of different drugs for diabetes (Krishnan et al., 2015).", "startOffset": 64, "endOffset": 87}, {"referenceID": 11, "context": ", 2014) or patient physiological signals to predict mortality (Ghassemi et al., 2015).", "startOffset": 62, "endOffset": 85}, {"referenceID": 3, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al., 2015). Razavian et al. (2016) compared CNNs to LSTMs for longitudinal outcome prediction on billing codes using lab tests.", "startOffset": 138, "endOffset": 300}, {"referenceID": 3, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al., 2015). Razavian et al. (2016) compared CNNs to LSTMs for longitudinal outcome prediction on billing codes using lab tests. With regard to interpretability, Choi et al. (2016) used temporal attention to identify important features in early diagnostic prediction of chronic diseases from time-ordered billing codes.", "startOffset": 138, "endOffset": 445}, {"referenceID": 10, "context": "79 in patients receiving fluid resuscitation (Fialho et al., 2013), 0.", "startOffset": 45, "endOffset": 66}, {"referenceID": 26, "context": "85 in septic shock patients (Salgado et al., 2016), and 0.", "startOffset": 28, "endOffset": 50}, {"referenceID": 29, "context": "71 for weaning, only trained on patients who did receive a vasopressor (Wu et al., 2016).", "startOffset": 71, "endOffset": 88}, {"referenceID": 12, "context": "78 (vasopressor) for vasopressor onset prediction after a 4 hour gap (Ghassemi et al., 2017).", "startOffset": 69, "endOffset": 92}, {"referenceID": 17, "context": "4) database (Johnson et al., 2016).", "startOffset": 12, "endOffset": 34}, {"referenceID": 13, "context": "3 Representation of Notes and Vitals Clinical narrative notes were processed to create a 50-dimensional vector of topic proportions for each note using Latent Dirichlet Allocation (Blei et al., 2003; Griffiths and Steyvers, 2004).", "startOffset": 180, "endOffset": 229}, {"referenceID": 29, "context": ", all columns for a particular variable zeroed) that does not require imputation (Figure 7 in Appendix B) (Wu et al., 2016).", "startOffset": 106, "endOffset": 123}, {"referenceID": 9, "context": "The physiological variables, topic distribution, and static variables for each patient are concatenated into a single feature vector per patient per hour (Esteban et al., 2016).", "startOffset": 154, "endOffset": 176}, {"referenceID": 25, "context": "2 Convolution Neural Network (CNN) We employ a similar CNN architecture to Razavian et al. (2016), except that we do not initially convolve the features into an intermediate representation.", "startOffset": 75, "endOffset": 98}, {"referenceID": 21, "context": "We use the macro AUC as an aggregate score because it weights the AUCs of all classes equally, regardless of class size (Manning et al., 2008).", "startOffset": 120, "endOffset": 142}, {"referenceID": 24, "context": "We use L2 regularized logistic regression (LR) as a baseline for comparison with the neural networks (Pedregosa et al., 2011).", "startOffset": 101, "endOffset": 125}, {"referenceID": 31, "context": "Zeiler and Fergus (2013) use occlusion to understand how models process images: they remove a region of the image (by setting all values in that region to 0) and compare the model\u2019s prediction of this occluded image with the original prediction.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Second, we generate \u201challucinations\u201d from the model which maximize the predicted probability for a given task (Erhan et al., 2009).", "startOffset": 110, "endOffset": 130}, {"referenceID": 3, "context": "This is in contrast to the raw data that has been forward-filled and mean-imputed, introducing noise and making it difficult for the model to know how confident to be in the measurements it is given (Che et al., 2016).", "startOffset": 199, "endOffset": 217}, {"referenceID": 0, "context": "In vasopressor onset prediction, physiological variables such as potassium and hematocrit are consistently important, which agrees with clinical assessment of cardiovascular state (Bassi et al., 2013).", "startOffset": 180, "endOffset": 200}, {"referenceID": 29, "context": "This is consistent with previous work that demonstrated weaning to be a more difficult task in general for vasopressors (Wu et al., 2016).", "startOffset": 120, "endOffset": 137}, {"referenceID": 0, "context": "In vasopressor onset prediction, physiological variables such as potassium and hematocrit are consistently important, which agrees with clinical assessment of cardiovascular state (Bassi et al., 2013). Similarly, Topic 3 (noting many physiological values) is also important for both onset and weaning. Note that the overall difference in AUC for onset ranges up to 0.16, but there is no significant decrease in AUC for weaning (< 0.02). This is consistent with previous work that demonstrated weaning to be a more difficult task in general for vasopressors (Wu et al., 2016). We also note that weaning prediction places importance on time of day. As noted by Wu et al. (2016), this could be a side-effect of patients being left on interventions longer than necessary.", "startOffset": 181, "endOffset": 676}, {"referenceID": 29, "context": "In our tasks, deep learning methods beat state-of-the-art AUCs reported in prior work for intervention prediction tasks \u2014 this is sensible given that prior works have focused on single targets with smaller datasets (Wu et al., 2016) or unsupervised representations prior to supervised training (Ghassemi et al.", "startOffset": 215, "endOffset": 232}, {"referenceID": 12, "context": ", 2016) or unsupervised representations prior to supervised training (Ghassemi et al., 2017).", "startOffset": 69, "endOffset": 92}, {"referenceID": 25, "context": "As in prior work, we found that RNNs often have similar or improved performance as compared to CNNs Razavian et al. (2016). However, it is possible that more complex models would perform better as they uncover more long and short-term dependencies.", "startOffset": 100, "endOffset": 123}], "year": 2017, "abstractText": "Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are noisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper, we integrate data from all available ICU sources (vitals, labs, notes, demographics) and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner to enable \u201creal-time\u201d performance, and predictions are made with a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on our predictive tasks using deep architectures. We explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines in intervention prediction, and provide insight into model learning, which is crucial for the adoption of such models in practice.", "creator": "LaTeX with hyperref package"}}}