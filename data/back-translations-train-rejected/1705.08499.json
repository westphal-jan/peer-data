{"id": "1705.08499", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "The Prediction Advantage: A Universally Meaningful Performance Measure for Classification and Regression", "abstract": "We introduce the Prediction Advantage (PA), a novel performance measure for prediction functions under any loss function (e.g., classification or regression). The PA is defined as the performance advantage relative to the Bayesian risk restricted to knowing only the distribution of the labels. We derive the PA for well-known loss functions, including 0/1 loss, cross-entropy loss, absolute loss, and squared loss. In the latter case, the PA is identical to the well-known R-squared measure, widely used in statistics. The use of the PA ensures meaningful quantification of prediction performance, which is not guaranteed, for example, when dealing with noisy imbalanced classification problems. We argue that among several known alternative performance measures, PA is the best (and only) quantity ensuring meaningfulness for all noise and imbalance levels.", "histories": [["v1", "Tue, 23 May 2017 19:43:37 GMT  (225kb,D)", "https://arxiv.org/abs/1705.08499v1", null], ["v2", "Fri, 26 May 2017 20:45:54 GMT  (225kb,D)", "http://arxiv.org/abs/1705.08499v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ran el-yaniv", "yonatan geifman", "yair wiener"], "accepted": false, "id": "1705.08499"}, "pdf": {"name": "1705.08499.pdf", "metadata": {"source": "CRF", "title": "The Prediction Advantage: A Universally Meaningful Performance Measure for Classification and Regression", "authors": ["Ran El-Yaniv", "Yonatan Geifman"], "emails": ["rani@cs.technion.ac.il", "yonatang@cs.technion.ac.il", "yair@jether-energy.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Prediction Advantage", "text": "In this section, we present the Bayesian Marginal Prediction (BMP), which is retrospectively defined as the optimal prediction based on the marginal distribution of the markers, P (Y). For each given loss function, the risk of BMP is taken as the reference point for meaningless predictions that we want to exceed. To this end, we define the predictive advantage (PA) as the (additive) interaction of the power ratio between our prediction function and the BMP. We then begin the PA with several important loss functions."}, {"heading": "2.1 Bayesian Marginal Prediction", "text": "We define the Bayean Marginal Prediction Function (BMP) as the optimal prediction function with respect to the marginal distribution of Y, P (Y), and refer to it as f0.1 The BMP predicts a constant prediction function.1In this way, we expect the BMP to achieve only the complexity of the problem latently in P (Y), we will select it arbitrarily among the optimal, as we are interested in its risk distribution over Y.Value / class, while we expect the best prediction function in B.For each prediction function g B, and each loss function g, we have R '(g) = EY' (Y, g).This can be easily determined by determining that g is independent of X: EX, Y '(Y, g)."}, {"heading": "2.2 The Prediction Advantage", "text": "The predictive advantage (PA) of a predictive function f is defined as the advantage of the expected power of f over the BMP: PA '(f) = \u2206 1 \u2212 R' (f) R '(f0) = 1 \u2212 EX, Y (' f (X), Y))) EX, Y ('(f0 (X), Y).The following basic properties of the PA measure are intentional: 1. Preservation of order: PA forms a weak order of the functions f \u00b2 F (in the presence of a function class F for our problem).Since the previous probabilities about the classes of a problem are constant, the PA maintains an order inverse to the order formed by the risk. PA' (f1) > PA '(f2) iff R' (f1) < R '(f2).2."}, {"heading": "2.3 Prediction Advantage for the Cross-Entropy Loss", "text": "We are now considering a multi-class classification problem using the cross-sectional entropy loss, defined as \", (f (X) Q = Q = Q = Q = Q.\" (F) Q = Q (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), F (F), F (F), F (F), F (F), F (F, F, F, F (F), F (F), F (F), F (F), F (F), F (F), F (F), F (F), F (F), F (F (F), F (F), F (F), F (F), F (F (F), F (F), F (F (F), F (F), F (F), F (F (F), F (F), F (F (F), F (F), F (F (F), F (F), F (F (F), F (F), F (F (F), F (, F), F (F (, F), F (F (, F), F (, F), F (, F (F), F (F (F), F (, F (, F), F (, F (, F), F (, F (, F), F (, F (, F), F (, F (F), F (, F (F), F (, F (, F (, F), F (, F (, F), F (, F (, F), F (, F (, F), F (, F ("}, {"heading": "2.4 Prediction Advantage for 0-1 Loss", "text": "Consider a 0-1 multi-class classification problem, where the set of classes C = Q = Q = Q (Q = Q = Q = Q = Q) is. In this case (and in contrast to the case of cross-entropy discussed above), the classifier f predicts a nominal value, f (x): X \u2192 C. The 0-1 loss is defined as \"(y, y) = I (y = y), where I is the indicator function. If the 0-1 loss function is used, the resulting risk clearly corresponds to the probability of misclassification. To derive the PA for this setting, we now show that the BMP strategy is f0 = \u2206 argmax i (Pr {Y = i}); that is, the BMP gives the most probable class in P (Y) for each X-.lemma 2.3 (BMP for 0-1 loss) f0-1 loss class."}, {"heading": "2.5 Prediction Advantage for Squared Loss in Regression", "text": "For the sake of simplicity, we look at the univariate result model, where Y = R, but the results can easily be extended to multivariate result models Y = Rm. Thus, the predicted variable is a real number, Y-R, and the function of the quadratic loss is defined as \"(r, s) = (r \u2212 s) 2. We now show that the BMP strategy for this setting is f0 = \u2206 E (Y). Lemma 2.4 (BMP for the quadratic loss in the regression) f0 = \u2206 E [Y] is the BMP for the regression of the quadratic losses. The proof of this problem is the known result of a minimum average quadratic error in the signal processing field, for further evaluations see [20] (Chapter 8). After identifying the BMP and observing that R '(f0) = EY [(Y \u2212 f0) PA) 2] = EY (Y \u2212 Y (2), we get the exact expression (R \u2212 Y = (var)."}, {"heading": "2.6 Prediction Advantage for Absolute Loss in Regression", "text": "Consider a univariate (multiple) regression setting below absolute loss. The predicted variable is a real number, Y-R, and the absolute loss function is defined as \"(r, s) = | r-s |. We now show that the BMP strategy for this setting is f0 =\" median (Y). \"Lemma 2.5 (BMP for absolute loss in regression) is f0 =\" Median (Y) is the BMP for regression among square losses. Advantage: According to Lemma 2.1, the BMP is a constant function in regression, if we consider an arbitrary constant function fa = a \"R,\" is the risk of a fa isR \"(fa) = EY | Y -a | Y -a | Y-b dP (Y) =\" a \u2212 \"a\" a \"a\" a \"a\" a \"a\" a \"a\" a \"a\" a \"a\" a \"a \u2212 a \u2212 a \u2212 a \u2212 b a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 b (Y-b a a a) (Y a a a) a (Y a a) a \u2212 b \u2212 b \u2212 a \u2212 a \u2212 a \u2212 b \u2212 b \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 b \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 a \u2212 \u2212 a \u2212 a \u2212 a \u2212"}, {"heading": "2.7 Prediction Advantage for Cost-Sensitive Loss", "text": "Consider a multi-class classification problem where classes C = \u2206 {1, 2,.,., k} are, and the loss function is defined with specific costs for each type of misclassification (see Elkan, (2001), for more details). For 1 \u2264 i, j \u2264 k, bi, j are the costs of predicting the label i, while the true label is j. It is clear that the BMP in this setting is isf0 = \u0445 argmini.C \u0445 j.C bi, j \u00b7 Pr (Y = j).We pass over the proof of the optimality of this proposed BMP, which is similar to the proof of Lemma 2.3. the risk of the BMP is R'c (f0) = min i-C-J-C bi, j \u00b7 Pr (Y = j), and therefore the predictive advantage for the cost-sensitive loss isPA'c (f) = 1 \u2212 R'c (f) f'c (c) (0) mini (J-J) \u2212 C (f)."}, {"heading": "3 Related Measures", "text": "False positive and false negative rates (also referred to as type 1 and type 2 errors) are typically used in statistics in the context of hypotheses testing. One can meaningfully compare two classifiers by looking (separately) at their false positive and false negative rates. Furthermore, Brodersen et al. [3] suggests comparing the mean of true positive and true negative rates and defining it as the balanced accuracy. [4] Emerging from the information retrieval, precision and recall are two popular measures that can be used to meaningfully measure performance in unbalanced problems. [5] In the context of a binary classification problem with a target minority class, precision is the percentage of instances that are classified to be truly in the target. Recall this is the fraction of true target instances that are correctly classified. [17], the use of these measures prevents meaningless assessments in unbalanced minority classes."}, {"heading": "4 Analysis", "text": "If the PA of a predictive function is not positive, the BMP exceeds our function (f = Y). In this case, our function may not be better than trivial. Therefore, using PA allows us to detect such trivial cases. Thus, most alternative methods mentioned in Section 3 are defined as 0-1 loss function in a binary classification setting, for which we can show that the PA falls below the lower limits of all other measurements, so if the PA is zero, all alternative measurements are qualified as positive and false as meaningless.The formal introduction of all the measurements discussed above can be simplified using the Venn chart in Figure 2. Let f be a classifier whose performance we want to quantify. The ranges in this graph are defined as follows. (a) (b) (c) Noting that the balanced accuracy (see Section 3) is defined as the arithmetic value of the following TN and we can define the TP."}, {"heading": "5 Numerical Examples", "text": "In this area, we are able to see the advantage of using plasma in certain cases, in the way that they are able to find a solution, in the way that they are able to find a solution."}, {"heading": "6 Concluding Remarks", "text": "Unlike previous methods, the proposed measure is defined for each loss function, and we derived simple formulas for all popular loss functions. This attractive property is achieved by normalizing the risk of the regression function through the variance of the marginal result distribution. One reason for the popularity of the R-quadratic measure in regression analysis is its ability to measure performance in a \"unified\" manner across all problems. This attractive property is achieved by normalizing the regression function through the variance of the marginal result distribution. PA generalizes this attractive property of the R-quadratic measure for each monitored learning problem and enjoys the same uniformity characteristic over the problems. Generally, the normalization factor of each problem is determined by the performance of the Bavarian marginal prediction."}, {"heading": "Acknowledgments", "text": "This research was supported by the Israel Science Foundation (grant no. 1890 / 14)."}, {"heading": "A Prediction Advantage on Haberman Dataset", "text": "The widely used Haberman dataset includes cases from a study conducted between 1958 and 1970 at the Billings Hospital of the University of Chicago on the survival of patients undergoing breast cancer surgery. [16] The dataset presents a small binary classification problem, where the minority class is 26.47%. To convincingly demonstrate that this is indeed a widespread problem, we briefly detail some such results in this appendix. [23] We used SVM with a reject option and reported 0.27 0 / 1 error without any rejection; this result is equivalent to a PA of -0.02. [14] We trained a classifier whose error of 0.3 with a corresponding PA of -0.13. [24] reported a classifier whose error is 0.273 (PA = \u2212 0.0313), compared to 0.4 [27.4], whose negative method was given almost exactly by 712 [27.1]."}], "references": [{"title": "Support vector machines with indefinite kernels", "author": ["Ibrahim Alabdulmohsin", "Xin Gao", "Xiangliang Zhang Zhang"], "venue": "In Proceedings of the Sixth Asian Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "On randomization in on-line computation", "author": ["Allan Borodin", "Ran El-Yaniv"], "venue": "Information and Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "The balanced accuracy and its posterior distribution", "author": ["Kay Henning Brodersen", "Cheng Soon Ong", "Klaas Enno Stephan", "Joachim M Buhmann"], "venue": "In 20th international conference on Pattern recognition (ICPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A robust algorithm for classification using decision trees", "author": ["B Chandra", "V Pallath Paul"], "venue": "In IEEE Conference on Cybernetics and Intelligent Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Fuzzifying gini index based decision trees", "author": ["B Chandra", "P Paul Varghese"], "venue": "Expert Systems with Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "On optimum recognition error and reject tradeoff", "author": ["C Chow"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "A coefficient of agreement for nominal scales", "author": ["Jacob Cohen"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1960}, {"title": "Regression and anova with zero-one data: Measures of residual variation", "author": ["Bradley Efron"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1978}, {"title": "On the foundations of noise-free selective classification", "author": ["Ran El-Yaniv", "Yair Wiener"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Comparison of evaluation metrics in classification applications with imbalanced datasets", "author": ["Mehrdad Fatourechi", "Rabab K Ward", "Steven G Mason", "Jane Huggins", "Alois Schl\u00f6gl", "Gary E Birch"], "venue": "In International Conference on Machine Learning and Applications (ICMLA),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "The Relationship Between Agnostic Selective Classification and Active", "author": ["R. Gelbhart", "R. El-Yaniv"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "An effective integrated method for learning big imbalanced data", "author": ["Mojgan Ghanavati", "Raymond K Wong", "Fang Chen", "Yang Wang", "Chang-Shing Perng"], "venue": "In IEEE International Congress on Big Data (BigData Congress),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "On the interpretation and use of r2 in regression analysis", "author": ["Inge S Helland"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1987}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Jiayuan Huang", "Arthur Gretton", "Karsten M Borgwardt", "Bernhard Sch\u00f6lkopf", "Alex J Smola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Missing values: how many can they be to preserve classification reliability", "author": ["Martti Juhola", "Jorma Laurikkala"], "venue": "Artificial Intelligence Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Introduction to Information Retrieval, volume 1", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Building predictors from vertically distributed data", "author": ["Sabine McConnell", "David B Skillicorn"], "venue": "In Proceedings of the 2004 conference of the Centre for Advanced Studies on Collaborative Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Conditional logit analysis of qualitative choice behavior", "author": ["D McFadden"], "venue": "Frontiers in Econometrics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1974}, {"title": "Signals, systems and inference", "author": ["Alan V Oppenheim", "George C Verghese"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "A critical investigation of recall and precision as measures of retrieval system performance", "author": ["Vijay Raghavan", "Peter Bollmann", "Gwang S Jung"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "Agnostic pointwise-competitive selective classification", "author": ["Y. Wiener", "R. El-Yaniv"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Agnostic selective classification", "author": ["Yair Wiener", "Ran El-Yaniv"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Random forests for metric learning with implicit pairwise position dependence", "author": ["Caiming Xiong", "David Johnson", "Ran Xu", "Jason J Corso"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge Discovery and Data Mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Probabilistic computations: Toward a unified measure of complexity", "author": ["Andrew Chi-Chin Yao"], "venue": "In Proceedings of the 18th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1977}], "referenceMentions": [{"referenceID": 21, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 13, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 22, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 4, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 11, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 16, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 14, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 3, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 0, "context": "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].", "startOffset": 53, "endOffset": 86}, {"referenceID": 23, "context": "By Yao\u2019s principle [25, 2] (which follows from von Neumann\u2019s minimax theorem), we can restrict attention only to deterministic BMPs (i.", "startOffset": 19, "endOffset": 26}, {"referenceID": 1, "context": "By Yao\u2019s principle [25, 2] (which follows from von Neumann\u2019s minimax theorem), we can restrict attention only to deterministic BMPs (i.", "startOffset": 19, "endOffset": 26}, {"referenceID": 18, "context": "The proof for this lemma is the known result of minimum mean squared error in the field of signal processing, for further reading see [20] (chapter 8).", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "Apparently, the PA in regression is precisely the well-known R-squared measure in regression analysis (also known as the coefficient of determination) [13].", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "[3] proposed to compare the mean of the true positive and true negative rates and defined it as the balanced accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Emerging from information retrieval, precision and recall are two popular measures that can be used to meaningfully measure performance in imbalanced problems [21].", "startOffset": 159, "endOffset": 163}, {"referenceID": 15, "context": "[17], the use of these measures prevents meaningless assessments in imbalance problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "This quantity is called the F -measure and it approximates the so-called break-even point, defined to be the equilibrium of the trade-off between precision and recall, Another performance measure, which emerged from experimental psychology, is Cohen\u2019s kappa [7].", "startOffset": 258, "endOffset": 261}, {"referenceID": 9, "context": "[10] to tackle imbalanced classification problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "R-squared is probably the most popular measure of fit in statistical modeling and particularly in the context of regression analysis [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 7, "context": "For example, Efron [8] extended the R-squared measure by calculating the squared loss over the predicted probabilities divided by the variance measured in the probability space and McFadden [19] extended the R-squared measure by replacing the loss with the log-likelihood of the model, and the variance with the log-likelihood of the intercept.", "startOffset": 19, "endOffset": 22}, {"referenceID": 17, "context": "For example, Efron [8] extended the R-squared measure by calculating the squared loss over the predicted probabilities divided by the variance measured in the probability space and McFadden [19] extended the R-squared measure by replacing the loss with the log-likelihood of the model, and the variance with the log-likelihood of the intercept.", "startOffset": 190, "endOffset": 194}, {"referenceID": 5, "context": "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.", "startOffset": 240, "endOffset": 254}, {"referenceID": 8, "context": "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.", "startOffset": 240, "endOffset": 254}, {"referenceID": 20, "context": "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.", "startOffset": 240, "endOffset": 254}, {"referenceID": 10, "context": "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.", "startOffset": 240, "endOffset": 254}], "year": 2017, "abstractText": "We introduce the Prediction Advantage (PA), a novel performance measure for prediction functions under any loss function (e.g., classification or regression). The PA is defined as the performance advantage relative to the Bayesian risk restricted to knowing only the distribution of the labels. We derive the PA for well-known loss functions, including 0/1 loss, cross-entropy loss, absolute loss, and squared loss. In the latter case, the PA is identical to the well-known R-squared measure, widely used in statistics. The use of the PA ensures meaningful quantification of prediction performance, which is not guaranteed, for example, when dealing with noisy imbalanced classification problems. We argue that among several known alternative performance measures, PA is the best (and only) quantity ensuring meaningfulness for all noise and imbalance levels.", "creator": "LaTeX with hyperref package"}}}