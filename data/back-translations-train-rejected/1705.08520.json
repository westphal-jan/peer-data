{"id": "1705.08520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "An effective algorithm for hyperparameter optimization of neural networks", "abstract": "A major challenge in designing neural network (NN) systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand. Examples of parameters are the number of layers and nodes, the learning rates, and the dropout rates. Typically, these parameters are chosen based on heuristic rules and manually fine-tuned, which may be very time-consuming, because evaluating the performance of a single parametrization of the NN may require several hours. This paper addresses the problem of choosing appropriate parameters for the NN by formulating it as a box-constrained mathematical optimization problem, and applying a derivative-free optimization tool that automatically and effectively searches the parameter space. The optimization tool employs a radial basis function model of the objective function (the prediction accuracy of the NN) to accelerate the discovery of configurations yielding high accuracy. Candidate configurations explored by the algorithm are trained to a small number of epochs, and only the most promising candidates receive full training. The performance of the proposed methodology is assessed on benchmark sets and in the context of predicting drug-drug interactions, showing promising results. The optimization tool used in this paper is open-source.", "histories": [["v1", "Tue, 23 May 2017 20:17:44 GMT  (530kb)", "http://arxiv.org/abs/1705.08520v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["gonzalo diaz", "achille fokoue", "giacomo nannicini", "horst samulowitz"], "accepted": false, "id": "1705.08520"}, "pdf": {"name": "1705.08520.pdf", "metadata": {"source": "CRF", "title": "An effective algorithm for hyperparameter optimization of neural networks", "authors": ["G. I. Diaz"], "emails": [], "sections": [{"heading": "An effective algorithm for hyperparameter optimization of neural networks1", "text": "G. I. Diaz, A. Fokoue, G. Nannicini, H. SamulowitzA major challenge in the development of neural network systems (NN) is to determine the best structure and parameters for the network given the available data for the problem of machine learning. Examples of parameters are the number of layers and nodes, learning rates and drop-out rates. Typically, these parameters are selected on the basis of heuristic rules and fine-tuned manually, which can be very time-consuming, since evaluating the performance of a single parameterization of the NN can take several hours. This paper addresses the problem of selecting suitable parameters for the NN by formulating it as a box-bound mathematical optimization problem and applying a derivative-free optimization tool that automatically and effectively searches the parameter space. The optimization tool uses a radial basic function model of the objective function (the pre-exposure of the remedy contexts to the N) to show the universality of the remedy results."}, {"heading": "Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "The hyperparameter optimization problem", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "Derivative-free optimization: overview", "text": "Rather, they will be able to live in the United States, where they will be able to move, to travel the world, to travel the world and to travel the world."}, {"heading": "Parallelization of RBFOpt", "text": "The evaluation of objective function f is by far the most time-consuming operation in solving the (HPO): training and assessing the performance of a single NN can take several hours, and for this reason we are expanding the optimization algorithm implemented in RBFOpt to perform this task in parallel. Note that the training time of a predictive model is not a deterministic quantity, and this leads to difficulties in parallelism - in fact, many parallel derived optimization algorithms make the simplistic assumption of a synchronous, objective function evaluation (e.g., 20), but our approach allows to be asynchronous. The idea is that a set of tasks is derived. We keep a set of tasks that contain evaluations, evaluations that are evaluated."}, {"heading": "Computational experiments", "text": "We will now describe in more detail the empirical evaluation conducted in this study. First, we will discuss experiments with a Modified National Institute of Standards and Technology Dataset (MNIST) with publicly available code, and then we will describe a specific application that uses proprietary code and is evaluated on both publicly available datasets (the Wordnet and FreeBase datasets) and non-public datasets."}, {"heading": "Experiments on the MNIST dataset", "text": "It is not as if people in the US have the same problems as people in most other countries in the world."}, {"heading": "Conclusion", "text": "We presented a methodology for the problem of hyperparameter optimization and a corresponding software implementation.The problem of hyperparameter optimization is to find values of the parameters of a machine learning algorithm in order to achieve the greatest prediction accuracy on a dataset. Our methodology relies on mapping the problem as a derivative-free optimization problem and solving it as such. Although the methodology cannot guarantee to find the global optimum in a limited number of iterations, we empirically show that it finds values of the hyperparameters that have a higher prediction accuracy than those determined by a domain expert. Furthermore, using a number of benchmark instances from the literature, we obtain results that are at least comparable and sometimes better than popular existing algorithms such as random search and sequential model-based algorithm configuration. This indicates that the attempt to use derivative-free optimization techniques for the problem is a potential of addressing the problem in the hyper parameter.11"}, {"heading": "3. M. Feurer, J. T. Springenberg, and F. Hutter, \u201cInitializing Bayesian hyperparameter", "text": "Optimization through Meta-Learning, \"in Proc. of the 29th AAAI Conf. on Artificial Intelligence, 2015."}, {"heading": "4. H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, and F. Hutter, \u201cTowards automatically", "text": "In the US, the number of people able to integrate is at an all-time high."}], "references": [{"title": "Introduction to Derivative-Free Optimization, Society for Industrial and Applied Mathematics (SIAM)", "author": ["A.R. Conn", "K. Scheinberg", "L.N. Vicente"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Kegl, \u201cAlgorithms for hyperparameter optimization,", "author": ["J. Bergstra", "R. Bardenet", "Y. Bengio"], "venue": "in: Proc. Adv. Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Initializing Bayesian hyperparameter optimization via meta-learning,", "author": ["M. Feurer", "J.T. Springenberg", "F. Hutter"], "venue": "in Proc. of the 29th AAAI Conf. on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Towards automatically tuned neural networks,", "author": ["H. Mendoza", "A. Klein", "M. Feurer", "J.T. Springenberg", "F. Hutter"], "venue": "J. Mach. Learn. Res,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,", "author": ["R.S. Olson", "N. Bartley", "R.J. Urbanowicz", "J.H. Moore"], "venue": "Proceedings of GECCO 2016,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Multi-task neural networks for QSAR predictions", "author": ["G. Dahl", "N. Jaitly", "R. Salakhutdinov"], "venue": "in: arXiv 1406.1231,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Practical bayesian optimization of machine learning algorithms,", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "in: 26th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Fast bayesian optimization of machine learning hyperparameters on large datasets,", "author": ["A. Klein", "S. Falkner", "S. Bartels", "P. Hennig", "F. Hutter"], "venue": "in: arXiv", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves,", "author": ["T. Domhan", "J. Springenberg", "F. Hutter"], "venue": "in: Proc. 24th International Joint Conference Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Embracing the random", "author": ["B. Recht"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms,", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Proceedings of the 19th ACM SIGKDD, International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "RBFOpt: an open-source library for black-box optimization with costly evaluations,", "author": ["A. Costa", "G. Nannicini"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A radial basis function method for global optimization,", "author": ["H.M. Gutmann"], "venue": "J. of Global Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "A stochastic radial basis function method for the global optimization of expensive black-box functions,", "author": ["R.G. Rommel", "C.A. Shoemaker"], "venue": "INFORMS J. of Comp.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Symmetry in integer linear programming,", "author": ["F. Margot"], "venue": "Years of Integer Programming", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1958}, {"title": "SOP: parallel surrogate global optimization with Pareto center selection for computationally expensive single objective problems,", "author": ["T. Krityakierne", "T. Akhtar", "C.A. Shoemaker"], "venue": "J. of Global Optimization, vo. 66,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "SO-MI: a surrogate model algorithm for computationally expensive nonlinear mixed-integer black-box global optimization problems,", "author": ["J. Mueller", "C.A. Shoemaker", "R. Piche"], "venue": "Computers and Operations Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Sequential model-based optimization for general algorithm configuration,", "author": ["F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "Proc. 5 Conf. Learning and Intelligent Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Rosebraugh, \u201cPreventable adverse drug reactions: a focus on drug interactions,", "author": ["D.A. Flockhart", "P. Honig", "C.S.U. Yasuda"], "venue": "Centers for Education & Research on Therapeutics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Predicting drug-drug interaction through large-scale similarity-based link prediction,", "author": ["A. Fokoue", "M. Sadoghi", "O. Hassazadeh", "P. Zhang"], "venue": "in: Proc. 13th Int. Conf. on the Semantic  14  Web,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "A review of relational machine learning for knowledge graphs,", "author": ["M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "2 given in [1].", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": ", see references [2-6]).", "startOffset": 17, "endOffset": 22}, {"referenceID": 2, "context": ", see references [2-6]).", "startOffset": 17, "endOffset": 22}, {"referenceID": 3, "context": ", see references [2-6]).", "startOffset": 17, "endOffset": 22}, {"referenceID": 4, "context": ", see references [2-6]).", "startOffset": 17, "endOffset": 22}, {"referenceID": 5, "context": "Optimization of hyperparameters has also been studied, for example, using a random search (RS) approach [7], Bayesian optimization [8-10], weighted probabilistic extrapolation [11], and other approaches [12,13].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Optimization of hyperparameters has also been studied, for example, using a random search (RS) approach [7], Bayesian optimization [8-10], weighted probabilistic extrapolation [11], and other approaches [12,13].", "startOffset": 131, "endOffset": 137}, {"referenceID": 7, "context": "Optimization of hyperparameters has also been studied, for example, using a random search (RS) approach [7], Bayesian optimization [8-10], weighted probabilistic extrapolation [11], and other approaches [12,13].", "startOffset": 131, "endOffset": 137}, {"referenceID": 8, "context": "Optimization of hyperparameters has also been studied, for example, using a random search (RS) approach [7], Bayesian optimization [8-10], weighted probabilistic extrapolation [11], and other approaches [12,13].", "startOffset": 131, "endOffset": 137}, {"referenceID": 9, "context": "Optimization of hyperparameters has also been studied, for example, using a random search (RS) approach [7], Bayesian optimization [8-10], weighted probabilistic extrapolation [11], and other approaches [12,13].", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "Optimization of hyperparameters has also been studied, for example, using a random search (RS) approach [7], Bayesian optimization [8-10], weighted probabilistic extrapolation [11], and other approaches [12,13].", "startOffset": 203, "endOffset": 210}, {"referenceID": 11, "context": "Optimization of hyperparameters has also been studied, for example, using a random search (RS) approach [7], Bayesian optimization [8-10], weighted probabilistic extrapolation [11], and other approaches [12,13].", "startOffset": 203, "endOffset": 210}, {"referenceID": 12, "context": "Second, we describe an extension of the derivativefree optimization algorithms implemented in the open-source library RBFOpt [14] to allow parallel, asynchronous evaluations of the function to be optimized, i.", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "We note that RBFOpt allows the exploitation of a computationally cheaper but inaccurate version for the objective function (see [14]), and this capability could be used to accelerate convergence of the hyperparameter optimization algorithm by varying the number of training epochs, but this direction is left for future research.", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "reader to [1] for an overview.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "We now briefly describe one of the approaches implemented in the opensource library RBFOpt [14], which as of version 2.", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "The approach that we describe, and that we use in our numerical experiments, is based on the metric stochastic response surface method of [16], with several modifications.", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "We note that in [14] and in the software implementation, the problem is assumed to be in minimization form, but here we discuss a maximization problem for consistency and without loss of generality, since a maximization problem can be transformed into a minimization problem by negating the objective function.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The vast majority of derivative-free optimization algorithms only apply to box-constrained problems, due to the difficulty of handling general constraints [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 14, "context": "Several types of surrogate models are possible, but by default RBFOpt uses a radial basis function model with thin plate splines, combined with a polynomial tail of degree 1 [17].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": "The resulting single-objective optimization problem for the choice of y can be solved in several ways: the authors of [16] recommend random sampling, but RBFOpt employs a simple genetic algorithm by default.", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": ", see [18]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "[19, 20]), but our approach, described next, allows them to be asynchronous.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[19, 20]), but our approach, described next, allows them to be asynchronous.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "Whenever a task of type 2 is completed, we check if the newly determined search point is to be discarded because of several criteria also employed in the serial version of the optimization algorithm (see [14]), and if the search point is accepted, we queue a task of type 1 to evaluate f at it.", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "It is important to note that several decisions taken by the optimization algorithm depend on the difference between the largest and smallest function value among the interpolation nodes; see [14] for details.", "startOffset": 191, "endOffset": 195}, {"referenceID": 5, "context": "The log scale is used to sample these two hyperparameters in RS as well, following [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 18, "context": "We compare RBFOpt to RS and Sequential Model-based Algorithm Configuration (SMAC) [21], using the same hyperparameter space for all methodologies.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "epochs divided by two; this criterion was also adopted in [7].", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "Aside from the results in the previous section, our hyperparameter optimization algorithm is also applied to a software system called Tiresias [22] that was built to receive various sources of", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "DDIs are a major cause of preventable adverse drug reactions, causing a significant burden on the patients\u2019 health and the healthcare system [23].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "In previous work [22], the similarity features were limited in two ways.", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": "We address the first limitation of [22] by exploring the use of a multi-layer perceptron to directly predict DDIs given the fingerprints of two drugs.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "For an overview of relational machine learning on knowledge graphs, we refer to the recent survey [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "We first evaluate the performance of the hyperparameter optimization methodology on the WN and FB15k datasets of [16] employing the same methodology used in that paper.", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "However, to more accurately estimate the speed improvement that can be achieved by the parallel run, we report statistics on the benchmark set of [14] on a cluster of identical machines with 8 CPUs each and no user-submitted jobs other than ours.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "36 instances of (DFO) coming from various sources (see [14] for details), and the location and value of the true optimum of each function are known but are kept hidden from the optimization algorithm.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "[11, 26].", "startOffset": 0, "endOffset": 8}], "year": 2017, "abstractText": "A major challenge in designing neural network (NN) systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand. Examples of parameters are the number of layers and nodes, the learning rates, and the dropout rates. Typically, these parameters are chosen based on heuristic rules and manually fine-tuned, which may be very time-consuming, because evaluating the performance of a single parametrization of the NN may require several hours. This paper addresses the problem of choosing appropriate parameters for the NN by formulating it as a box-constrained mathematical optimization problem, and applying a derivative-free optimization tool that automatically and effectively searches the parameter space. The optimization tool employs a radial basis function model of the objective function (the prediction accuracy of the NN) to accelerate the discovery of configurations yielding high accuracy. Candidate configurations explored by the algorithm are trained to a small number of epochs, and only the most promising candidates receive full training. The performance of the proposed methodology is assessed on benchmark sets and in the context of predicting drug-drug interactions, showing promising results. The optimization tool used in this paper is open-source.", "creator": "Microsoft\u00ae Word 2013"}}}