{"id": "1705.08619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Dictionary-based Monitoring of Premature Ventricular Contractions: An Ultra-Low-Cost Point-of-Care Service", "abstract": "While cardiovascular diseases (CVDs) are prevalent across economic strata, the economically disadvantaged population is disproportionately affected due to the high cost of traditional CVD management. Accordingly, developing an ultra-low-cost alternative, affordable even to groups at the bottom of the economic pyramid, has emerged as a societal imperative. Against this backdrop, we propose an inexpensive yet accurate home-based electrocardiogram(ECG) monitoring service. Specifically, we seek to provide point-of-care monitoring of premature ventricular contractions (PVCs), high frequency of which could indicate the onset of potentially fatal arrhythmia. Note that a traditional telecardiology system acquires the ECG, transmits it to a professional diagnostic centre without processing, and nearly achieves the diagnostic accuracy of a bedside setup, albeit at high bandwidth cost. In this context, we aim at reducing cost without significantly sacrificing reliability. To this end, we develop a dictionary-based algorithm that detects with high sensitivity the anomalous beats only which are then transmitted. We further compress those transmitted beats using class-specific dictionaries subject to suitable reconstruction/diagnostic fidelity. Such a scheme would not only reduce the overall bandwidth requirement, but also localising anomalous beats, thereby reducing physicians' burden. Finally, using Monte Carlo cross validation on MIT/BIH arrhythmia database, we evaluate the performance of the proposed system. In particular, with a sensitivity target of at most one undetected PVC in one hundred beats, and a percentage root mean squared difference less than 9% (a clinically acceptable level of fidelity), we achieved about 99.15% reduction in bandwidth cost, equivalent to 118-fold savings over traditional telecardiology.", "histories": [["v1", "Wed, 24 May 2017 06:00:57 GMT  (1335kb)", "http://arxiv.org/abs/1705.08619v1", "19 pages, 9 figures and 5 tables"]], "COMMENTS": "19 pages, 9 figures and 5 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bollepalli s chandra", "challa s sastry", "laxminarayana anumandla", "soumya jana"], "accepted": false, "id": "1705.08619"}, "pdf": {"name": "1705.08619.pdf", "metadata": {"source": "CRF", "title": "Dictionary-based Monitoring of Premature Ventricular Contractions: An Ultra-Low-Cost Point-of-Care Service", "authors": ["Bollepalli S. Chandra", "Challa S. Sastry", "Laxminarayana Anumandla", "Soumya Jana"], "emails": ["bschandra@iith.ac.in", "csastry@iith.ac.in", "laxmin56@gmail.com", "jana@iith.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.08 619v 1 [cs.L G] 24 May 2While cardiovascular disease (CVD) is widespread in all economic strata, the economically disadvantaged population is disproportionately affected by the high cost of traditional CVD management, which includes consultation, testing and monitoring in medical institutions. Accordingly, the development of an ultra-low-cost alternative, which is also affordable for groups at the lower end of the economic pyramid, has emerged as a societal imperative. Against this background, we propose a low-cost but accurate electrocardiogram (ECG). Specifically, we seek to ensure monitoring of premature ventricular contradictions (PVCs), a high frequency of which could indicate potentially fatal arrhythmia."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2. Motivation and Envisaged System", "text": "We begin by placing the current problem in a medical and social context."}, {"heading": "2.1. Clinical Imperative", "text": "Continuous monitoring has often proved effective in the timely detection of such abnormalities, particularly monitoring of PVCs, which represent early depolarization of the heart muscle originating in the ventricle [3], although such beats are found in subjects with or without structural heart disease. [4] In healthy individuals, a PVCs prevalence of less than 1% is common and has no prognostic significance. In contrast, more frequent PVCs may indicate (or lead to) structural heart disease. Specifically, 90% of patients report PVCs after acute MI [24], and the risk of sudden death in such patients is related to the complexity and frequency of PVCs. Recent studies also indicate the role of PVCs in the development of cardiomyopathy [25]."}, {"heading": "2.2. Technological Imperative", "text": "A conventional telecardiological system, illustrated in Figure 1a, records and transmits the entire user ECG to the diagnostic center. Such a system not only uses the available bandwidth inefficiently, but also burdens healthcare professionals with processing the entire recording to identify abnormalities. In this context, telephone-based ECGs and the associated clinical experience were studied decades ago [28]. Given the increasing ubiquity of mobile phones in recent years, both cellular network and ZigBee-based wireless systems have been developed [29, 30, 31]. However, despite technological advances, the inefficient telecardiological architecture has largely been avoided. Against this background, we propose a novel architecture that uses bandwidth sensibly and supports healthcare professionals by localizing potential anomalies without compromising the quality of care. In this context, efforts have already been made to provide telecardiological services in remote rural and unsafe communities."}, {"heading": "2.3. Social Imperative: Representative Scenario", "text": "As mentioned above, we are aiming for a cost-effective telecardiological solution for individuals with an average daily income of approximately $1.25. Consider a person living on the economic threshold of this segment of the population who has recently suffered a heart attack and been successfully treated (see [34] for various treatment options)."}, {"heading": "2.3.1. Cost incurred in traditional telecardiology", "text": "Such an assumption is realistic in various developing and underdeveloped countries, where free health care is provided in government facilities [35], and this welfare paradigm is currently being extended to the broader context of telemedicine [36], so the cost would only be the cost of data transmission; at a sampling rate of 360 Hz and an 11-bit word length (used in the MIT / BIH arrhythmia database [37]), about 1.78 MB of data would be generated per hour. As we plan to use existing cellular networks, transferring all the data to the diagnostic center would cost about $27 per hour, at a cost of $1.5 per 100 KB of data1. At this rate, the cost of monitoring a single-channel ECG for ten hours would be $2.7."}, {"heading": "2.3.2. Affordability as necessity", "text": "Assuming a household size of four people (equivalent to the average family size in India [39]), household income amounts to about $5 a day. Assuming zero savings, the 10-hour PVC surveillance costs calculated in paragraph 2.3.1 could account for 54% of household expenditure and would clearly be prohibitively prohibitive. In this situation, where catastrophic health conditions are rarely detected, the household may be tempted to consider surveillance expenditure to be insignificant, but in reality, detecting a life-threatening condition in time is highly likely to save lives, and therefore regular monitoring remains crucial for long-term survival. Therefore, it is imperative to drastically reduce surveillance costs to such an affordable level that even an economically disadvantaged person would find little incentive to forego it."}, {"heading": "2.4. Outline of Envisaged System", "text": "To comply with the above-mentioned imperative, we strive to reduce the volume of data transmitted to the diagnostic center. To compress the data prior to transmission, we suggest detecting abnormal beats and communicating a compressed version of only these beats. Specifically, we form beat trios, each consisting of a PVC beat and normal beats before and after it (see Figure 2a for an illustrative example).A representative beat vector for the normal beat and the PVC beat are shown in Figure 2b. If a PVC beat is not isolated but occurs as a sequence of PVC beats (two or more), the normal beats that precede and follow the run are used as comforters. Such beat trios (and delivered PVC beats) are then communicated to the diagnostic center along with the timing information. Although this scheme adds a worst case of beats, two beats are added to the timings known earlier for each one of the beats."}, {"heading": "3. Classification, Compression and Dictionary Learning", "text": "As already mentioned, signal processing in this paper includes the classification and compression of ECG signals. In this section we present the associated technical problems and provide the necessary mathematical preparatory work."}, {"heading": "3.1. ECG classification", "text": "A desired classifier specifies two mutually exclusive and exhaustive subsets of \"1\" and \"2\" of a possible ECG beat x as follows: Each beat x \"1\" is declared normal, while each beat x \"2\" is declared PVC. Currently, we would like to find \"2\" (and thus \"1\") so that for a given sensitivity, \"Se,\" i.e. a fraction of the PVC beats, is correctly recognized as PVC beats, the specificity \"Sp,\" i.e. the fraction of the normal beats is maximized [40]. Next, we examine the bandwidth requirement of the aforementioned classifier \"p.,\" provided that only beats that are recognized as anomalous (PVC) are transmitted to the diagnostic center that has sufficient resources to validate the normal beats and, if necessary, correct the class of each beat it receives is classified \"SL,\" in other words, only if it is classified as actual."}, {"heading": "3.2. ECG compression", "text": "In the same vein, assuming that the set signal consists only of normal and PVC impacts with a compression ratio of \u03b2N (\u2265 1) or \u03b2V (\u2265 1) for normal and PVC impacts, bandwidth usage is a function of prevalence and is given by Bco = \u03c1 \u00d7 1\u03b2V + (1 \u2212 \u03c1) \u00d7 1\u03b2N. (2) Further bandwidth savings can be achieved by using a hybrid scheme in which beat trios are formed around detected PVC impacts, which are then compressed and communicated to the diagnostic center. (3) In general, reconstruction fidelity differs inversely from the compression ratio and the target conflict is impact specific. We will determine reconstruction fidelity based on the percentage of PRINT PRINT Squared D (D + 2\u03b2N) in relation to the widely used signals (EK1 \u2212 4x), whereby the reconstruction fidelity (further in relation to the original Ex) is \u2212 4x."}, {"heading": "3.3. Dictionary-based Technique", "text": "So far, we have envisaged a system with some target classification accuracy and reconstruction accuracy. Now, we need technology that will enable us to achieve these goals. In this regard, we propose a dictionary-based solution. First, we need mathematical preconditions for compressed sampling and dictionary learning."}, {"heading": "3.3.1. Compressive sampling paradigm", "text": "Compressive sampling (CS) restores a high-dimensional, sparse vector \u03b1-Rn from some of its measurements x = \u03a6\u03b1, x-Rm, m < n, \u03a6 denoting the measurement matrix [43]. Formally, we aim to use solvemin \u03b1-\u03b1-0, subject to \u03a6\u03b1 = x, (5) where 0 denotes the l0 (counting) standard. In general, (5) is insoluble. Fortunately, under certain technical conditions, the solution of (5) remains unchanged when replaced by the l1 standard. As L1 solver, we will use orthogonal matching struts (OMP) for its simplicity, empirical effectiveness (despite its greed) and relatively low computational complexity of O (m2n) [44]."}, {"heading": "3.3.2. Dictionary learning", "text": "The dictionary learning method identifies a tunable selection of base vectors that provide a sparse representation. In the face of a series of signals {xi} M i = 1, KSVD obtains the dictionary D, which provides the sparsest representation for each example in this sentence. [45] It is a two-step procedure. In the first step, for a given dictionary D we obtain the matrix with sparse columns by solving the following optimization problem: (D) l 1 subject under X = D, (6) where \"l\" is the l-th column of, \"and\" X \"is the matrix whose columns are\" xi. \"Using the above-mentioned thresholds, the pair (D) is then updated as\" D. \""}, {"heading": "4. Proposed Dictionary-based Solution", "text": "At this point, we are ready to propose a dictionary-based solution to achieve the desired classification and compression objectives."}, {"heading": "4.1. Dictionary-based classification", "text": "Consider the dataset {xil} Ml i = 1} K l = 1. Here l indicates the class name: l = \"N\" stands for normal, and l = \"V\" stands for PVC in a two-class problem (K = 2). In addition, i indicates the beat index that records values up to Ml, the number of strokes present in class l. On the basis of this dataset, we learn the dictionary Dl = min-Rm \u00b7 n for class l. When a beat x is presented to achieve the beat classification, we first find the sparest representation of x based on each dictionary Dl, l, l, l, V}, by solving the dictionary whose labeling is subject to l = min-Rp-1, we are subject to the x-Rp classification 2 < (8) where \"0\" represents the labeling of Dl, the labeling of Dl, the labeling of Dl, and the labeling of Dl, the labeling of Dl, the labeling of Dl, l, l, \"l,\" L, \"V."}, {"heading": "4.2. Dictionary-based compression", "text": "Remember that each beat is labeled as abnormal, as is any delimitation of normal beats, compressed and forwarded to the diagnostic center. We intend to maximize the compression ratio for a given reconstruction target PRDcompr by using a dictionary-based method as shown in Figure 3b. Specifically, we first project the test beat onto the class-specific dictionary subject to an intermediate PRD constraint, PRDcompr, and calculate the corresponding dictionary coefficients that are expected to be non-zero. These non-zero coefficients are then quantified so that the PRD constraints are sufficiently degraded to meet the general constraint PRDcompr. Here, PRDint remains an internal compression subsystem, and if we set PRDint significantly smaller than PRDcompr, the number of non-zero coefficients would be large overall, which would then increase the quantization by PRD."}, {"heading": "4.3. End-to-end System", "text": "At this point, we turn to the completion of an end-to-end system that uses the classification and compression subsystems previously discussed. A flowchart of the proposed system is shown in Figure 4, and consists of the following modules. Data reading: We start with the acquisition of ECG samples from the subject and store them in a buffer. At the same time, we read stored samples from the buffer to form a beat vector Bn \u2212 Time Tn of occurrence of corresponding beat is recorded and the communication status flag Sn is set to zero, which would later indicate whether a specific beat is transmitted to the diagnostic center. Classification and Compression: First, to detect the anomaly, each beat vector is projected onto the prefabricated dictionaries of the normal and PVC classes to obtain correspondingly sparse representations."}, {"heading": "5. Framework for Performance Evaluation", "text": "We now turn to the performance evaluation of the proposed classification subsystem, the compression subsystem and the complete end-to-end system. As usual, we evaluate the classification and compression subsystems based on the trade-off between sensitivity (reliability) versus specificity or compression ratio versus reconstruction fidelity. We also evaluate the end-to-end system in terms of bandwidth cost savings under clinically motivated reliability and fidelity limitations. To evaluate various performance indices, we use the MIT / BIH arrhythmy database available from the PhysioBank archives [37]. Specifically, we first divide the database into training and testing kits performance and form a common dictionary based on both the classification and compression subsystems. Later, we test the performance of these subsystems and the end-to-end system and compare it with the performance of the reported algorithms in the telardiological context."}, {"heading": "5.1. Patient-specific Partitioning", "text": "Traditionally, the database is divided into training and test sets, either class-oriented or subject-oriented [47]. In the first case, the division is based solely on the heartbeat label, which allows significant amounts of data from the same patient to be presented in both training and test sets, leading to overly optimistic performance estimates [18, 48]. More recently, however, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17] in which a topic-oriented approach is adopted with the following modification. Some patient-specific beats (generally segmented from the first 5 minutes of each data set) are added to the training set. Such a patient-specific approach often provides an appropriate performance estimate, which is less optimistic than the performance estimated using paradigmatic sequences of steps, and less conservative than the approach based on target and target results, with target and target results often being compared with target 102 / 17."}, {"heading": "5.2. Hand-picked Partitioning", "text": "Traditionally, a research group would hand-pick such a partition based on subjective criteria. Examples are Partition-1, Partition-2, and Partition-3, which are given in Table 3. In any case, the total number of normal and relative PVC beats for each workout and the test sets are mentioned. Specifically, Partition-1 only takes into account records that contain at least one PVC beat. Subsequently, Partition-2 includes 13 records with indexes in the range of 100-124 and 20 records with indexes in the range of 200-234 and [10], respectively. Such a partition has a relatively small fraction (18.3%) of PVC beats for training. Subsequently, Partition-1 generalizes by taking into account all records with indexes in the range of 100-124 for training, and those with indexes in the range of 200-234 for training, without paying attention to the occurrence of PVC beats."}, {"heading": "5.3. Randomized Partitioning with Even Split", "text": "In response, we are launching another Proposal-2, in which performance is averaged over permitted evenly distributed partitions. In particular, 22 out of 44 subjects are randomly selected for training and the data of the remaining subjects for testing purposes. In addition, permitted partitions maintain the numerical equilibrium that 45% to 55% of the total PVC beats are included in the test set. As the total number of permitted partitions is extremely high (> 1011), we have chosen the Monte Carlo Cross Validation Approach (MCCV), in which performance is averaged over multiple (in our case 100) random partitions. Compared to Proposal-1, the randomization in Proposal-2 takes more satisfactorily into account patient data that is invisible in practice."}, {"heading": "5.4. Randomized Partitioning with Training Set Larger than Test Set", "text": "In our domestic PVC monitoring context, we have extensive historical data and some potential subjects that need to be taken into account. Consequently, partitioning the current database seems realistic enough that the training set is larger than the test set, compared to the even distribution as in Proposal-1 and Proposal-2. Accordingly, we modify Proposal-2 to include each partition to include 40 subjects for training and 4 for testing, and call for the new Proposal-3. In Proposal-3, we also update the numerical equilibrium between PVC and normal beats so that only those partitions are taken into account in which the test set accounts for 10% - 20% of the total number of PVC beats. As permitted partitions still have a large proportion of 42,294, we apply the MCCV approach over 100 randomly selected partitions as before. Of course, historical (training) data should be weighted even more in practice given the overwhelming superiority of the first data."}, {"heading": "5.5. Recommendation", "text": "In summary, Proposal-1 is the peak performance that is overly optimistic and should not be used as a practical guide. Proposal-2 delivers an average performance that is in some respects more satisfactory than peak performance and helps highlight the significant gap between the two. However, the even distribution in Proposal-2 does not reflect the superiority of historical data and is therefore too conservative to guide the practical design. We recommend performance indicators that correspond to Proposal-3 and include both a more realistic distribution and randomization as well as (somewhat conservative) design guidance."}, {"heading": "6. Experimental Setup", "text": "At this point, we are conducting simulation experiments to prove the effectiveness of the proposed system."}, {"heading": "6.1. Preprocessing", "text": "Remember that the assumed MIT / BIH arrhythmia database consists of 30-minute snippets of two-channel outpatient ECG recordings of 48 subjects [37]. Each channel collects 360 samples per second with a dynamic range of 10 mV peak-to-peak and digitizes to 11-bit words. Furthermore, each beat is commented on per recognized clinical practice.For our experiments, we used only the modified Limb Lead II channel (MLII).In each recording, we performed the following steps: First, the baseline walker was sequentially removed using two median filters of the respective window sizes 200ms and 600ms [11]. Next, the annotated R-Peak position was noted in each beat, and 150 samples before, 150 samples after that, and the Rpeak sample were collected in a vector of length 301 [21]. Such a signal vector contained most of the heart peak position in each beat, and 150 samples after that were collected in a vector of length 301 [21]."}, {"heading": "6.2. Dictionary Size", "text": "In the proposed dictionary-based classification / compression approach, we trained an over-complete dictionary for each of the normal and PVC classes using the K-SVD algorithm. In this respect, the dictionary size became more important as (i) smaller size required less calculation and (ii) larger column size led to a sparser representation, both of which are desirable. However, the column size saturates with increasing column size and has a negligible effect on classification performance beyond a certain threshold [50]. Accordingly, we strive to select the smallest dictionary that offers an acceptable level of thrift. Empirically, \"good\" overcomplete dictionaries have proven to be a ratio of column size to row size between about 2 and 5. [45] Fortunately, we have achieved satisfactory classification and compression performance, even if we operate at the lower limit of the aforementioned time range, we have used the 30-size dictionaries, in particular, by remembering that the 30-size of the 1."}, {"heading": "7. Experimental Results", "text": "In this section, we present experimental results and performance analyses for the classification subsystem and compression subsystem separately, as well as for the overall system. To this end, we used the MIT / BIH arrhythmia database [37], selected a patient-specific partitioning, evaluated the performance of our dictionary-based method according to proposal-1, proposal-2, and proposal-3, and compared it with the performance of known algorithms as appropriate. Note the different beat morphologies and the way in which these are captured by the displayed dictionary while the performance was evaluated using the test set. See Figure 5 for a typical normal beat, a typical PVC beat, and the three most commonly used atoms of each dictionary. Note the different beat morphologies and how these are captured by the mapped dictionary atoms. For simulations, we used MATLAB v.2014b on a desktop computer with a core Intel processor with a full 3.7-B6 GHz classification, as well as the millisecond required for approximately 0.6-BGB full processing."}, {"heading": "7.1. Classification Performance", "text": "As already mentioned, the classification performance depends on the reconstruction fidelity of the target PRDclass, which is internally compared with the subsystem of classification that we select first. 7.1.1. Key compromises and PRDclass To this end, we investigated the relationship between sensitivity, specificity and PRDclass of the classifier for Proposal-3. Specifically, in Figure 6a we plotted the trade-off between specificity and PRDclass at different sensitivity levels. At the sensitivity target (reliability) of 99%, we observed that the specificity should be maximized at approximately PRDclass = 9%. To assess the phenomenon from a different perspective, we plotted different ROC curves (Se versus 1 \u2212 Sp) in Figure 6b, where we found that the optimal ROC curve achieved by variation of PRDclass is well approximated by the ROC curve (RODemc curve at the fixed value of PRD9%, we reached the target class equivalent to PRD9%)."}, {"heading": "7.1.2. Performance statistics", "text": "Now, in operation at PRDclass = 9%, we compared the performance of the proposed classifier with various algorithms that have adopted patient-specific evaluation schemes. Specifically, in Table 4, we report sensitivity and specificity of the existing classifiers and the proposed classifiers along with specific information on training data. Let's remember that we have set a sensitivity target of 99% for our proposals. Now, when comparing peak performance, our Proposal-1 is better than most of the reported algorithms. However, Proposal-1 represents overly optimistic performance focused on Partition-4, and cannot capture the performance variation due to the randomly chosen partitioning. In return, we have the performance of our Proposal-2 and Proposal-3, where uncertainty is handled more realistically. Specifically, operating at the target sensitivity of 99%, we report the mean and standard deviation of specificity over 100 randomly selected suggestions."}, {"heading": "7.2. Compression Performance", "text": "Recall from Sec 4.2 that compression performance is determined by the number of non-zero elements in the dictionary coefficient, which in turn is dictated by the intermediate reconstruction fidelity PRDint. We now choose PRDint that maxizes compression ratio for different PRDint values for PVC beats (Figure 8a). Key trade offs and choice of PRDintFor this purpose, we first considered Proposal-3 and plotted PRDcompr versus compression ratio for different PRDint values for PVC beats (Figure 8a). As previously mentioned, the number of non-zero elements of dictionary coefficient, and plots of dictionary coefficient is large. In this setting, for a small increase in quantization step size \u0435, which produce an increment in compresint ratio \u03b2V, quantization error from all these coefficient daculations, DDcompots to result in steep crement in PRDcompr."}, {"heading": "7.2.2. Performance statistics", "text": "Compression performance also depends on the partitioning between training and test data. To eliminate this dependence, we again took the MCCV approach to evaluate our compression algorithm and reported on the performance statistics for Proposal-1, Proposal-2 and Proposal-3 (Table 5). Specifically, in PRDcompr = 9% we reported on the mean and standard deviation of \u03b2N and \u03b2V, compression rates corresponding to normal beats or PVC beats. Not unexpectedly, compression performance increased for larger training data (Proposal-3). Interestingly, the standard deviation of compression performance also increased. Unlike classification, the proposed compression technique cannot be fairly compared with the state of the art here. To be more precise, note that certain algorithms achieve a high compression rate for a signal consisting of several beats before it is compressed [19]. In contrast, we avoid that a particular compression system is selected to avoid higher delays."}, {"heading": "7.3. System Performance", "text": "We present the overall performance of the system in terms of savings in bandwidth costs. Operating at the target reliability of Se = 99% and reconstruction fidelity of PRDcompr = 9%, the proposed Beattrio communication system would achieve approximately 57.6% savings in original bandwidth if the classification were used alone. Compression alone increases bandwidth savings to 97.99%, while ignoring the representative scenarios presented in Section 2 and remembering that conventional telecardiology achieves a cost of $2.7 for 10-hour healthcare monitoring. Against this background, we have plotted the performance of our system in Figure 9we and other reported algorithms in the telecardiological context."}, {"heading": "8. Discussion", "text": "We conclude with a summary of our contributions, comments on the expected user experience and reflections on the broader implications of our work."}, {"heading": "8.1. Summary", "text": "In this paper, we presented an extremely cost-effective POC service for PVC monitoring that ensures high accuracy, in particular, we proposed a dictionary-based technique that achieves highly sensitive classification and high-precision compression, demonstrating the effectiveness of our method using Monte Carlo cross-validation on the MIT / BIH arrhythmia database [37, 51], and in particular characterizing the threefold trade-off between bandwidth, reliability, and reconstruction fidelity: with a reliability target of no more than one undetected PVC in a hundred beats, and a reconstruction fidelity of 9% PRD, we achieved approximately forty times the savings in bandwidth and associated costs, and our service would cost only $2.3 for ten-hour monitoring, which we believe should be attractive to economically marginalized people."}, {"heading": "8.2. User Experience", "text": "When using our service, it is expected that the experience of users (both subjects and medical professionals) will remain essentially the same as that of conventional telecardiology. Specifically, the same sensors are still used at the end of the test to record the patient's ECG signals. In fact, from the medical professionals \"point of view, the conclusion to be drawn from electronic recordings must be of essentially the same quality (PRD \u2264 9%, from Table 1 [42]) as the gold standard (quality standard) of unprocessed signals. In fact, it is expected that the time and workload of medical professionals will be less than in the traditional situation, since the proposed method would automatically identify PVCs and represent only delimited abnormal beats. In short, subjects familiar with conventional telecardiology would not require additional training, while medical professionals would focus only on the beats presented (beat trios) and would have to ignore blanks that would merely display (non-informative beats)."}, {"heading": "8.3. Broader Impact", "text": "In particular, high PVC exposure could anticipate unwanted heart disease even in people without prior structural heart disease [5]. In such a context, our technique could facilitate screening with minor modifications. Furthermore, in addition to PVCs, the proposed dictionary-based method could be extended to other abnormal indicators such as supraventricular arrhythmias and atrial fibrillation [52]. Furthermore, the inclusion of feedback from health professionals and adaptively learned personalized dictionaries could potentially improve both classification and compression performance [53]."}, {"heading": "Acknowledgment", "text": "This work was partially supported by the Department of Electronics and Information Technology (DeitY), Govt. of India, within the Cyber Physical Systems Innovation Project: 13 (6) / 2010 - CC & BT."}], "references": [{"title": "Cardiac arrhythmia: mechanisms, diagnosis, and management", "author": ["P.J. Podrid", "P.R. Kowey"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "The ECG: a two-step approach to diagnosis", "author": ["M. Gertsch"], "venue": "Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The prognostic significance of ventricular premature contractions in healthy people and in people with coronary heart disease., Acta cardiologica", "author": ["L. Hinkle", "S. Carver", "D. Argyros"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1974}, {"title": "Treating patients with ventricular ectopic beats, Heart", "author": ["G.A. Ng"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "How mobile devices are transforming healthcare, Issues in technology innovation", "author": ["D. West"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The Fortune at the Bottom of the Pyramid", "author": ["C.K. Prahalad"], "venue": "Pearson Education India,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Frugal engineering: An emerging innovation paradigm", "author": ["N. Kumar", "P. Puranam"], "venue": "Ivey Business Journal", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A patientadaptable ecg beat classifier using a mixture of experts approach, IEEE transactions on biomedical engineering", "author": ["Y.H. Hu", "S. Palreddy", "W.J. Tompkins"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "A patient-adapting heartbeat classifier using ecg morphology and heartbeat interval features", "author": ["P. de Chazal", "R.B. Reilly"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Classification of electrocardiogram signals with support vector machines and particle swarm optimization", "author": ["F. Melgani", "Y. Bazi"], "venue": "IEEE Transactions on Information Technology in Biomedicine", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Block-based neural networks for personalized ecg signal classification", "author": ["W. Jiang", "S.G. Kong"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Automated patientspecific classification of premature ventricular contractions", "author": ["T. Ince", "S. Kiranyaz", "M. Gabbouj"], "venue": "in: 2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Heartbeat classification using disease-specific feature selection, Computers in biology and medicine", "author": ["Z. Zhang", "J. Dong", "X. Luo", "K.S. Choi", "X. Wu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Comparison of four methods for premature ventricular contraction and normal beat clustering, in: Computers in Cardiology", "author": ["G. Bortolan", "I. Jekova", "I. Christov"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "A 2-d ecg compression method based on wavelet transform and modified spiht", "author": ["S.-C. Tai", "C. Sun", "W.-C. Yan"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Ecg signal compression using analysis by synthesis coding", "author": ["Y. Zigel", "A. Cohen", "A. Katz"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "An ecg signals compression method and its validation using nns", "author": ["C.M. Fira", "L. Goras"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Ecg compression retaining the best natural basis k-coefficients via sparse decomposition", "author": ["A. Adamo", "G. Grossi", "R. Lanzarotti", "J. Lin"], "venue": "Biomedical Signal Processing and Control", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Optimality of klt for high-rate transform coding of gaussian vector-scale mixtures: Application to reconstruction, estimation, and classification", "author": ["S. Jana", "P. Moulin"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Coronary Heart Disease: Clinical, Pathological, Imaging, and Molecular Profiles", "author": ["Z. Vlodaver", "R.F. Wilson", "D. Garry"], "venue": "Springer Science & Business Media,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Premature ventricular contraction-induced cardiomyopathy a treatable condition, Circulation: Arrhythmia and Electrophysiology", "author": ["Y.-M. Cha", "G.K. Lee", "K.W. Klarich", "M. Grogan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Prognostic significance of frequent premature ventricular contractions originating from the ventricular outflow tract in patients with normal left ventricular function, Heart", "author": ["S. Niwano", "Y. Wakisaka", "H. Niwano", "H. Fukaya", "S. Kurokawa", "M. Kiryu", "Y. Hatakeyama", "T. Izumi"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Relationship between burden of premature ventricular complexes and left ventricular function, Heart Rhythm", "author": ["T.S. Baman", "D.C. Lange", "K.J. Ilg", "S.K. Gupta", "T.- Y. Liu", "C. Alguire", "W. Armstrong", "E. Good", "A. Chugh", "K. Jongnarangsin"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Pervasive healthcare and wireless health monitoring, Mobile Networks and Applications", "author": ["U. Varshney"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "We-care: an intelligent mobile telecardiology system to enable mhealth applications, IEEE journal of biomedical and health informatics", "author": ["A. Huang", "C. Chen", "K. Bian", "X. Duan", "M. Chen", "H. Gao", "C. Meng", "Q. Zheng", "Y. Zhang", "B. Jiao"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "A lossless ecg data compression technique using ascii character encoding, Computers & Electrical Engineering", "author": ["S.K. Mukhopadhyay", "S. Mitra", "M. Mitra"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Esc guidelines for the management of acute myocardial infarction in patients presenting with st-segment elevation, European heart journal (2012) ehs215", "author": ["P.G. Steg", "S.K. James", "D. Atar", "L.P. Badano", "C.B. Lundqvist", "M.A. Borger", "C. Di Mario", "K. Dickstein", "G. Ducrocq", "F. Fernandez-Aviles"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Private initiatives and policy options: recent health system experience in india, Health policy and planning", "author": ["B.C. Purohit"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Saberwal, In ehealth in india today, the nature of work, the challenges and the finances: an interview-based study, BMC medical informatics and decision making", "author": ["G.S. Jaros lawski"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Physiobank, physiotoolkit, and physionet components of a new research resource for complex physiologic signals, Circulation", "author": ["A.L. Goldberger", "L.A. Amaral", "L. Glass", "J.M. Hausdorff", "P.C. Ivanov", "R.G. Mark", "J.E. Mietus", "G.B. Moody", "C.-K. Peng", "H.E. Stanley"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "On the problem of the most efficient tests of statistical hypotheses", "author": ["J. Neyman", "E.S. Pearson"], "venue": "in: Breakthroughs in Statistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1992}, {"title": "Bioelectrical signal processing", "author": ["L. S\u00f6rnmo", "P. Laguna"], "venue": "in cardiac and neurological applications,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "The weighted diagnostic distortion (wdd) measure for ecg signal compression", "author": ["Y. Zigel", "A. Cohen", "A. Katz"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2000}, {"title": "Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing, 1st Edition", "author": ["M. Elad"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Efficient implementation of the k-svd algorithm using batch orthogonal matching pursuit, CS Technion", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation, IEEE Transactions on signal processing", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "Heartbeat classification using morphological and dynamic features of ecg signals", "author": ["C. Ye", "B.V. Kumar", "M.T. Coimbra"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Reliable low-cost telecardiology: High-sensitivity detection of ventricular beats using dictionaries", "author": ["B.S. Chandra", "C.S. Sastry", "S. Jana"], "venue": "in: 16th IEEE Healthcom,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "A survey of cross-validation procedures for model selection, Statistics surveys", "author": ["S. Arlot", "A. Celisse"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Asymptomatic arrhythmias in patients with symptomatic paroxysmal atrial fibrillation and paroxysmal supraventricular tachycardia., Circulation", "author": ["R.L. Page", "W.E. Wilkinson", "W.K. Clair", "E.A. Mc- Carthy", "E. Pritchett"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1994}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "in: Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Especially, for patients who have suffered myocardial infarction (MI), or developed left ventricular dysfunction (LVD), continuous monitoring has proven essential in promptly detecting sudden deterioration in cardiac functions, and hence preventing mortality [2].", "startOffset": 259, "endOffset": 262}, {"referenceID": 1, "context": "The aforementioned as well as various related conditions are associated with premature ventricular contractions (PVCs) that briefly interrupt the normal rhythm of the heart [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "Although PVCs occur in healthy individuals as well, high frequency of PVCs is known to foretell serious arrhythmic conditions [4], and significantly correlate with events of mortality [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Although PVCs occur in healthy individuals as well, high frequency of PVCs is known to foretell serious arrhythmic conditions [4], and significantly correlate with events of mortality [5].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "Fortunately, high penetration of mobile phones even in remote communities has mitigated such barriers in certain scenarios [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "In the present case, can the mobile network be leveraged to provide reliable PVC monitoring at an attractive cost to the communities living at the bottom of the economic pyramid [8]?", "startOffset": 178, "endOffset": 181}, {"referenceID": 6, "context": "In response, we take a frugal engineering approach [9], and propose an ultra-low-cost POC service.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 122, "endOffset": 130}, {"referenceID": 9, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 11, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 12, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 13, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 14, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 15, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 16, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 198, "endOffset": 206}, {"referenceID": 17, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 198, "endOffset": 206}, {"referenceID": 18, "context": "The proposed approach, however, is different from the (symmetric) joint classification/reconstruction framework [23], where a combination of classification and (class-oblivious) reconstruction indices is minimized subject to a rate constraint.", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "In particular, monitoring PVCs, which are an early depolarization of the myocardium originating in the ventricle [3], assumes significance, even though such beats are found in subjects with as well as without structural heart diseases [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "In particular, monitoring PVCs, which are an early depolarization of the myocardium originating in the ventricle [3], assumes significance, even though such beats are found in subjects with as well as without structural heart diseases [4].", "startOffset": 235, "endOffset": 238}, {"referenceID": 19, "context": "Specifically, 90% of patients experience PVCs after acute MI [24], and the risk of sudden death in such patients is related to the complexity and frequency of the PVCs.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Recent studies also indicate the role of PVCs in inducing cardiomyopathy [25].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "Recommended lower threshold for the high-risk subjects, such as those with a history of MI or LVD varies between 10,000 and 20,000 in a 24-hour window [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "Another recommendation sets 10% as the threshold PVC burden [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "Besides frequency of PVCs, run of two or more PVCs and their complexity could also indicate an adverse heart condition [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 23, "context": "With growing ubiquity of mobile phones in recent years, cellular network based as well as ZigBee based wireless systems have been developed [29, 30, 31].", "startOffset": 140, "endOffset": 152}, {"referenceID": 24, "context": "With growing ubiquity of mobile phones in recent years, cellular network based as well as ZigBee based wireless systems have been developed [29, 30, 31].", "startOffset": 140, "endOffset": 152}, {"referenceID": 25, "context": "In particular, a method to encode ECG signals into ASCII characters to enable communication via SMS (short message service) has been reported [32].", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": "Consider an individual living at the economic threshold of this target population segment, who suffered myocardial infarction in the recent past, and was successfully treated (see [34] for various treatment options).", "startOffset": 180, "endOffset": 184}, {"referenceID": 27, "context": "Such an assumption is realistic in various developing and underdeveloped countries, where free healthcare is dispensed from government-run facilities [35].", "startOffset": 150, "endOffset": 154}, {"referenceID": 28, "context": "This welfare paradigm is currently being extended even to the broader context of telemedicine [36].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Considering a sampling rate of 360Hz and word length of 11 bits (used in MIT/BIH arrhythmia database [37]), one would generate about 1.", "startOffset": 101, "endOffset": 105}, {"referenceID": 30, "context": ", fraction of normal beats correctly detected as normal beats is maximized [40].", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": "Table 1: Relation between PRD and the diagnostic content of the ECG signal [42].", "startOffset": 75, "endOffset": 79}, {"referenceID": 31, "context": "where x and x\u0302 stand respectively for the original and the reconstructed signals [41].", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "Further, from a diagnostic perspective, a PRD of no more than 9% has been found to be \u201cgood\u201d (Table 1) [42].", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "Compressive sampling (CS) recovers a high dimensional sparse vector \u03b1 \u2208 R from a few of its measurements x = \u03a6\u03b1, x \u2208 R, m < n, where \u03a6 denotes the measurement matrix [43].", "startOffset": 166, "endOffset": 170}, {"referenceID": 33, "context": "As l1 solver, we shall use orthogonal matching pursuit (OMP) in view of its simplicity, empirical effectiveness (despite its being greedy) [43], and relatively low computational complexity of O(mn) [44].", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "As l1 solver, we shall use orthogonal matching pursuit (OMP) in view of its simplicity, empirical effectiveness (despite its being greedy) [43], and relatively low computational complexity of O(mn) [44].", "startOffset": 198, "endOffset": 202}, {"referenceID": 35, "context": "Given a set of signals {xi} M i=1, KSVD obtains the dictionary D that provides the sparsest representation for each example in this set [45].", "startOffset": 136, "endOffset": 140}, {"referenceID": 34, "context": "M training data (signals) is O(mnM) [44].", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "For evaluation of various performance indices, we made use of MIT/BIH Arrhythmia database available from the PhysioBank archives [37].", "startOffset": 129, "endOffset": 133}, {"referenceID": 36, "context": "Traditionally, partitioning of database into training and test sets is performed either in a classoriented or in a subject-oriented manner [47].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "In the former, partitioning is based only on the heartbeat label, which allows significant amounts of data from the same patient to be represented in both training and test sets, resulting in overly optimistic performance estimates [18, 48].", "startOffset": 232, "endOffset": 240}, {"referenceID": 12, "context": "In contrast, the latter seeks to account for inter-subject variability, and constitutes training and test sets with beats from distinct subsets of records, leading to an overly conservative estimate of performance [16].", "startOffset": 214, "endOffset": 218}, {"referenceID": 7, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 8, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 10, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 11, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 7, "context": "Here, the training and the test sets consist of 13 records with indices in the range 100\u2013124 and 20 records with indices in the range 200\u2013234, respectively [10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Subsequently, Partition2 generalizes Partition-1 by considering all records with indices in the range 100\u2013124 for training, and those with indices in the range 200\u2013234 for testing, without paying attention to occurence of PVC beats [14].", "startOffset": 232, "endOffset": 236}, {"referenceID": 8, "context": "Finally, Partition-3 possesses the property that training and test sets enjoy approximately equal representation from the rival classes of beats [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "Recall that the adopted MIT/BIH Arrhythmia database consists of 30-minute excerpts of two channel ambulatory ECG recordings of 48 subjects [37].", "startOffset": 139, "endOffset": 143}, {"referenceID": 8, "context": "First, the baseline wander was removed using two median filters of respective window sizes 200ms and 600ms in a sequential manner [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "Next the annotated R-peak location in each beat was noted, and 150 samples before, 150 samples after and the Rpeak sample were collected in a vector of length 301 [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 37, "context": "However, sparsity saturates with increasing column size, and has negligible effect on classification performance beyond certain threshold [50].", "startOffset": 138, "endOffset": 142}, {"referenceID": 35, "context": "Empirically, \u201cgood\u201d overcomplete dictionaries have been shown to possess a ratio of column to row size between approximately 2 and 5 [45].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "To this end, we made use of MIT/BIH Arrhythmia database [37], adopted patient specific partitioning, evaluated the performance of our dictionary based method according to Proposal-1, Proposal-2 and Proposal3, and compared with the performance of known algorithms, when relevant.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": ", 1997 [10] 82.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": ", 2006 [11] 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": ", 2007 [14] 86.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": ", 2009 [15] 84.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "preciate this, note that certain algorithms achieve high compression ratio for a signal consisting of several beats by stacking such beats before compressing [19].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "Further, specific fixed partitioning is sometimes chosen so as to maximize reported compression ratio [21, 22].", "startOffset": 102, "endOffset": 110}, {"referenceID": 17, "context": "Further, specific fixed partitioning is sometimes chosen so as to maximize reported compression ratio [21, 22].", "startOffset": 102, "endOffset": 110}, {"referenceID": 8, "context": "[11], requires 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "We demonstrated the efficacy of our method using Monte Carlo cross validation on the MIT/BIH arrhythmia database [37, 51].", "startOffset": 113, "endOffset": 121}, {"referenceID": 38, "context": "We demonstrated the efficacy of our method using Monte Carlo cross validation on the MIT/BIH arrhythmia database [37, 51].", "startOffset": 113, "endOffset": 121}, {"referenceID": 32, "context": "From the medical professionals\u2019 perspective, the inference has to be made from the electronic records at essentially the same quality (PRD \u2264 9%, from Table 1 [42]) as the gold (quality) standard of unprocessed signals.", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "Specifically, high PVC burden could presage adverse heart conditions even in individuals without prior structural heart disease [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 39, "context": "Further, apart from PVCs, the proposed dictionary-based method could be extended to other anomalous indicators such as supraventricular arrhythmias and atrial fibrillation [52].", "startOffset": 172, "endOffset": 176}, {"referenceID": 40, "context": "In addition, incorporating medical professionals\u2019 feedback and adaptively learning personalized dictionaries could potentially improve both classification and compression performance levels [53].", "startOffset": 190, "endOffset": 194}], "year": 2017, "abstractText": "While cardiovascular diseases (CVDs) are prevalent across economic strata, the economically disadvantaged population is disproportionately affected due to the high cost of traditional CVD management, involving consultations, testing and monitoring at medical facilities. Accordingly, developing an ultra-low-cost alternative, affordable even to groups at the bottom of the economic pyramid, has emerged as a societal imperative. Against this backdrop, we propose an inexpensive yet accurate home-based electrocardiogram (ECG) monitoring service. Specifically, we seek to provide point-of-care monitoring of premature ventricular contractions (PVCs), high frequency of which could indicate the onset of potentially fatal arrhythmia. Note that a traditional telecardiology system acquires the ECG, transmits it to a professional diagnostic center without processing, and nearly achieves the diagnostic accuracy of a bedside setup, albeit at high bandwidth cost. In this context, we aim at reducing cost without significantly sacrificing reliability. To this end, we develop a dictionary-based algorithm that detects with high sensitivity the anomalous beats only which are then transmitted. We further compress those transmitted beats using class-specific dictionaries subject to suitable reconstruction/diagnostic fidelity. Such a scheme would not only reduce the overall bandwidth requirement, but also localizing anomalous beats, thereby reducing physicians\u2019 burden. Finally, using Monte Carlo cross validation on MIT/BIH arrhythmia database, we evaluate the performance of the proposed system. In particular, with a sensitivity target of at most one undetected PVC in one hundred beats, and a percentage root mean squared difference less than 9% (a clinically acceptable level of fidelity), we achieved about 99.15% reduction in bandwidth cost, equivalent to 118-fold savings over traditional telecardiology. In the process, our algorithm outperforms known algorithms under various measures in the telecardiological", "creator": "LaTeX with hyperref package"}}}