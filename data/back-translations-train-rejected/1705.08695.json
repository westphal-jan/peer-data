{"id": "1705.08695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Stochastic Sequential Neural Networks with Structured Inference", "abstract": "Unsupervised structure learning in high-dimensional time series data has attracted a lot of research interests. For example, segmenting and labelling high dimensional time series can be helpful in behavior understanding and medical diagnosis. Recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models (e.g., Hidden Markov Models). This combination can model not only the long term dependency in sequential data, but also the uncertainty included in the hidden states. Inheriting these advantages of stochastic neural sequential models, we propose a structured and stochastic sequential neural network, which models both the long-term dependencies via recurrent neural networks and the uncertainty in the segmentation and labels via discrete random variables. For accurate and efficient inference, we present a bi-directional inference network by reparamterizing the categorical segmentation and labels with the recent proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient Variational Bayes. We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition. Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods.", "histories": [["v1", "Wed, 24 May 2017 10:52:19 GMT  (2257kb,D)", "http://arxiv.org/abs/1705.08695v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["hao liu", "haoli bai", "lirong he", "zenglin xu"], "accepted": false, "id": "1705.08695"}, "pdf": {"name": "1705.08695.pdf", "metadata": {"source": "CRF", "title": "Stochastic Sequential Neural Networks with Structured Inference", "authors": ["Hao Liu", "Haoli Bai", "Lirong He", "Zenglin Xu"], "emails": ["liuhaosater@gmail.com", "haolibai@gmail.com", "he@std.uestc.edu.cn", "zenglin@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Recursive Neural Networks, Hidden Semi-Markov Models, Sequential Data"}, {"heading": "1. Introduction", "text": "In this sense, it is very useful for physicians to understand the underlying behavior or activity of people engaged in sequential data analysis such as recursive neural networks (RNNs), Rumelhart et al. (1988) and hidden Markov models (HMMs) by Rabiner (1989). Recentar Xiv: 170 5.08 695v 1 [cs.L Gliterature have approaches of combining probable generative models and recurrent neural networks for the benefit of their complementary strengths in nonlinear representation and effective estimation of parameters Johnson et al."}, {"heading": "2. Preliminaries", "text": "In this section, we present background related to generative sequential models. Specifically, we first introduce RNNs, followed by HMMs and HSMMs.Recursive neural networks (RNNs), as a wide range of sequential models for time series, have been applied in a number of applications that depend on the inputs. (2014) Here, we present basic properties and notations of RNNs. Let us consider a sequence of time sequences of vectors x1: T = [x1, x2,..., xT], which depend on the inputs u1: T = [u1, u2, uT], where the time sequence of vectors x1: T = [x2, xT], which differs from the input steps in the time steps, and T is the maximum time steps. RNN introduces further hidden states h1: T = [h1, h2, hT], where is located."}, {"heading": "3. Model", "text": "In this section, we present our stochastic sequential neural network model. Notations and settings generally agree with Section 2 of the HSMM, as shown in Figure 1. For convenience, we present our model on a single sequence. It is easy to apply the model to multiple sequences."}, {"heading": "3.1 Generative Model", "text": "In order to model the long-term temporal dependencies and uncertainty in the segmentation and labeling of multiple time series, we strive to take advantage of RNN and HSMM and learn categorical information and representation information from the observed data on a recurring basis. As shown in Figure 2 (a), we design a stochastic sequential neural network (SSNN) with a sequence of consecutive latent variables that model the recursive hidden states, and two sequences of discrete variables that each characterize the duration and designation of the segments. The common probability can be factored as: p\u03b8 (x1: T, z1: T \u2212 \u2212 Si: T \u2212 si si: T, d1: si si si: T, the discrete variables that each characterize the duration and designation of the segments. The common probability can be factored as: dt (dt), t (zt), t (dt), t (zt), t (t (zt)."}, {"heading": "3.2 Structured Inference", "text": "We are interested in maximizing the marginal log probability protocol p (x), but this is usually insoluble, since the complicated posterior distributions cannot be universally integrated. Newer Bayesian learning methods, such as the score function (or REINFORCE) Archer et al. (2015) and the stochastic gradient variational Bayes (SGVB) Kingma and Welling (2013), are common black box methods that lead to tractable solutions. We use the SGVB because it was able to efficiently learn the approximation with relatively small discrepancies between Kingma and Welling (2013), while the score function suffers from high discrepancies and high computational costs. We are now focusing on maximizing the evidence, also known as ELBO, log-ledge (x) \u2265 L (x1: T; \u043d, \u03c6) = equilibrium distribution (z1: T, d1: quadrilature | xT: T: T-T: adricating and T: T: T: T)."}, {"heading": "3.2.1 Bi-directional Inference", "text": "To find a more informative approach to the posterior level, we supplement both random variables dt, zt with bidirectional information in the inference network. Such experiments have been studied in many previous papers (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), but focus mainly on continuous variables, and little attention is paid to the discrete variable. Specifically, we first learn a bidirectional deterministic variable h, t = BiRNN (x1: t, xt: T), where BiRNN is a bidirectional RNN, with each unit implemented as LSTM Hochreiter and Schmidhuber (1997). Similar to Fraccaro et al. (2016), we continue to use a backward recurring function qit = g\u03c6I (It + 1, [xt, h, t]) to explicitly capture forward and backward information in the posterior level."}, {"heading": "3.2.2 Gumbel-Softmax Reparameterization", "text": "Gumbel-Softmax repair proposes an alternative to reverse propagation by discrete random variables (max.), bypassing the non-differentiable categorical distribution. To use Gumbel-Softmax repair, we must first change the discrete pair (zt, dt) to a N-dimensional vector (t) to represent the Gumbel-Softmax distribution variable (t): yi (t). (t) is a N-dimensional vector on the simplex and N = K \u00b7 D. Then we use y (t) to represent the Gumbel-Softmax distribution variable: yi (t) = exp (log))."}, {"heading": "4. Related Work", "text": "In this section, we will conduct research on generative sequential data models in relation to state spatial models and recurrent neural networks. In the following, we will review some recent work on sequential latent variable models. In relation to the linkage of the two and SSM and RNs, the work will mostly be located close to our work. (2016) Krishnan et al. (2016); Archer et al. (2016); Fraccaro et al. (2016) There will be a number of different models that deal with continuous state models, highlighting the relationship to linear dynamic systems. (2016) Inference networks will be designed that include both future and past hidden variables that Deep Kalman filters. (2015)"}, {"heading": "5. Experiment", "text": "In this section, we evaluate SSNN using multiple datasets across multiple scenarios: First, we evaluate its performance in finding complex structures and estimating the probability of data using a synthetic dataset and two language datasets (TIMIT & Blizard), then we test SSNN using learning segmentations and latent labels on human activity Reyes-Ortiz et al. (2016) dataset, Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al. (2016) Challenge dataset and compare the results with HSMM and its variants. Finally, we offer an additional challenging test on the problem of multi-object recognition using the generated Multi-MNIST dataset. All models of the experiment use the optimizer Adam Kingma and Ba (2014). The temperatures of Gumbel-Softmax were determined throughout the training. We implement the proposed model based on TheAl-Rfou Van & Fuel (2015) and Ma (2015)."}, {"heading": "5.1 Synthetic Experiment", "text": "To confirm that our method is able to model high-dimensional data with complex dependencies, we simulated a complex dynamic, torque-controlled pendulum controlled by a differential equation to generate non-Markovian observations from a dynamic system: ml2 d 2\u03c6 (t) dt2 = \u2212 \u00b5d\u03c6 (t) dt + mgl sin\u03c6 (t) + u (t). To be fair to Karl et al. (2016), we use m = l = 1, \u00b5 = 0.5 and g = 9.81. We convert the generated basic truth angles into image observations. The system can be fully described by angle and angular velocity. We compare our method with Deep Variational Bayes Filters (DVBF-LL) Karl et al. (2016) and Deep Kalman Filters (DKF) Krishnan et al. (2015). The usual results of square regression are shown in Table 1."}, {"heading": "5.2 Speech Modeling", "text": "In fact, the majority of people who support the rationalization of rationalization are in favor of the rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of rationalization of ration"}, {"heading": "5.3 Segmentation and Labeling of Time Series", "text": "To show the advantages of SSNN over HSMM and its variants as we learn the segmentation and latent markings of sequences, we take experiments on human activity Reyes-Ortiz et al. (2016), Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al. (2016) challenge dataset.Both Human Activity and Drosophila dataset are used for segmentation prediction.Human activity is a 6 dimensional vector.Drosophila dataset records the time series movement of fruit flies' legs t, xt is a 45-dimension vector, which consists of the raw and some higher order attributes. We is the maximum time series movement of fruit flies' legs t, xt is a 45-dimension vector, which consists of the raw and some higher order."}, {"heading": "5.4 Sequential Multi-objects Recognition", "text": "In order to further verify the ability to model complex spatial dependencies, we test SSNN for the problem of detecting multiple objects. This problem is interesting, but difficult, because the model must capture the dependence of pixels in images and detect the objects in images. Specifically, we construct a small image dataset with 3000 images, called multi-MNIST. Each image contains three non-overlapping random MNIST digits with equal probability. Our goal is to detect each digit in the image sequentially. In the experiment, we train our model with 2500 images and test the remaining 500 images. First, we determine the maximum time steps T = 3 and feed the same image sequentially as input to SSNN. We interpret the latent variable dt as intensity and then as a location variable in the training images. Then, we train SSNN with random initialized parameters on 60,000 multi-MNIST images. The NIST model can only run through a higher curriculum without a curriculum."}, {"heading": "6. Conclusion", "text": "In order to learn the structures (e.g. segmentation and labeling) of high-dimensional time series unsupervised, we have proposed a stochastic sequential neural network (SSNN) with structured inference. To better interpret the model, we continue to limit the labeling and the duration of segmentation to two sequences of discrete variables each. To use forward and backward time information, we design carefully structured inferences, and to overcome the difficulties of deriving discrete latent variables in deep neural networks, we resort to the recently proposed Gumbel Softmax functions. The benefits of the proposed inference method in SSNN have been demonstrated in both synthetic and real sequential benchmarks."}, {"heading": "Acknowledgments", "text": "This work was partially supported by grants from the Natural Science Foundation of China (No 61572111), the National High Technology Research and Development Program of China (No 2015AA015408), a 985 Project of UESTC (No A1098531023601041) and two Basic Research Funds for the Central Universities of China (No ZYGX2016J078 and ZYGX2016Z003)."}], "references": [{"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov", "Alexander Belopolsky"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2016}, {"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah A Smith"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Black box variational inference for state space models", "author": ["Evan Archer", "Il Memming Park", "Lars Buesing", "John Cunningham", "Liam Paninski"], "venue": "arXiv preprint arXiv:1511.07367,", "citeRegEx": "Archer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Archer et al\\.", "year": 2015}, {"title": "Learning stochastic recurrent networks", "author": ["Justin Bayer", "Christian Osendorfer"], "venue": "arXiv preprint arXiv:1411.7610,", "citeRegEx": "Bayer and Osendorfer.,? \\Q2014\\E", "shortCiteRegEx": "Bayer and Osendorfer.", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Stochastic generative hashing", "author": ["Bo Dai", "Ruiqi Guo", "Sanjiv Kumar", "Niao He", "Le Song"], "venue": "arXiv preprint arXiv:1701.02815,", "citeRegEx": "Dai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2017}, {"title": "Recurrent hidden semimarkov model", "author": ["Hanjun Dai", "Bo Dai", "Yan-Ming Zhang", "Shuang Li", "Le Song"], "venue": null, "citeRegEx": "Dai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2017}, {"title": "Inference in hidden markov models with explicit state duration distributions", "author": ["Michael Dewar", "Chris Wiggins", "Frank Wood"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Dewar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dewar et al\\.", "year": 2012}, {"title": "Variational recurrent auto-encoders", "author": ["Otto Fabius", "Joost R van Amersfoort"], "venue": "arXiv preprint arXiv:1412.6581,", "citeRegEx": "Fabius and Amersfoort.,? \\Q2014\\E", "shortCiteRegEx": "Fabius and Amersfoort.", "year": 2014}, {"title": "Bayesian nonparametric inference of switching dynamic linear models", "author": ["Emily Fox", "Erik B Sudderth", "Michael I Jordan", "Alan S Willsky"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Fox et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2011}, {"title": "Sequential neural models with stochastic layers", "author": ["Marco Fraccaro", "S\u00f8ren Kaae S\u00f8nderby", "Ulrich Paquet", "Ole Winther"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fraccaro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fraccaro et al\\.", "year": 2016}, {"title": "Deep temporal sigmoid belief networks for sequence modeling", "author": ["Zhe Gan", "Chunyuan Li", "Ricardo Henao", "David E Carlson", "Lawrence Carin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1310.8499,", "citeRegEx": "Gregor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Neural adaptive sequential monte carlo", "author": ["Shixiang Gu", "Zoubin Ghahramani", "Richard E Turner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Muprop: Unbiased backpropagation for stochastic neural networks", "author": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Categorical reparameterization with gumbelsoftmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "venue": null, "citeRegEx": "Jang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "Stochastic variational inference for bayesian time series models", "author": ["Matthew Johnson", "Alan Willsky"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Johnson and Willsky.,? \\Q2014\\E", "shortCiteRegEx": "Johnson and Willsky.", "year": 2014}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Matthew Johnson", "David K Duvenaud", "Alex Wiltschko", "Ryan P Adams", "Sandeep R Datta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Bayesian nonparametric hidden semi-markov models", "author": ["Matthew J Johnson", "Alan S Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Johnson and Willsky.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Willsky.", "year": 2013}, {"title": "Leg-tracking and automated behavioural classification in drosophila", "author": ["Jamey Kain", "Chris Stokes", "Quentin Gaudry", "Xiangzhi Song", "James Foley", "Rachel Wilson", "Benjamin De Bivort"], "venue": "Nature communications,", "citeRegEx": "Kain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kain et al\\.", "year": 2013}, {"title": "Deep variational bayes filters: Unsupervised learning of state space models from raw data", "author": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "venue": "arXiv preprint arXiv:1605.06432,", "citeRegEx": "Karl et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karl et al\\.", "year": 2016}, {"title": "Kullback-leibler proximal variational inference", "author": ["Mohammad E Khan", "Pierre Baqu\u00e9", "Fran\u00e7ois Fleuret", "Pascal Fua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Khan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2015}, {"title": "Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models", "author": ["Mohammad Emtiyaz Khan", "Wu Lin"], "venue": "arXiv preprint arXiv:1703.04265,", "citeRegEx": "Khan and Lin.,? \\Q2017\\E", "shortCiteRegEx": "Khan and Lin.", "year": 2017}, {"title": "Faster stochastic variational inference using proximal-gradient methods with general divergence functions", "author": ["Mohammad Emtiyaz Khan", "Reza Babanezhad", "Wu Lin", "Mark Schmidt", "Masashi Sugiyama"], "venue": "arXiv preprint arXiv:1511.00146,", "citeRegEx": "Khan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Non-conjugate variational message passing for multinomial and binary regression", "author": ["David A Knowles", "Tom Minka"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Knowles and Minka.,? \\Q2011\\E", "shortCiteRegEx": "Knowles and Minka.", "year": 2011}, {"title": "Deep kalman filters", "author": ["Rahul G Krishnan", "Uri Shalit", "David Sontag"], "venue": "arXiv preprint arXiv:1511.05121,", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "Structured inference networks for nonlinear state space models", "author": ["Rahul G Krishnan", "Uri Shalit", "David Sontag"], "venue": "arXiv preprint arXiv:1609.09869,", "citeRegEx": "Krishnan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": "arXiv preprint arXiv:1611.00712,", "citeRegEx": "Maddison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "Variational inference for monte carlo objectives", "author": ["Andriy Mnih", "Danilo J Rezende"], "venue": "arXiv preprint arXiv:1602.06725,", "citeRegEx": "Mnih and Rezende.,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende.", "year": 2016}, {"title": "The blizzard challenge 2013\u2013indian language task", "author": ["Kishore Prahallad", "Anandaswarup Vadapalli", "Naresh Elluru", "G Mantena", "B Pulugundla", "P Bhaskararao", "HA Murthy", "S King", "V Karaiskos", "AW Black"], "venue": "In Blizzard Challenge Workshop,", "citeRegEx": "Prahallad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prahallad et al\\.", "year": 2013}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner.", "year": 1989}, {"title": "Transition-aware human activity recognition using smartphones", "author": ["Jorge-L Reyes-Ortiz", "Luca Oneto", "Albert Sama", "Xavier Parra", "Davide Anguita"], "venue": null, "citeRegEx": "Reyes.Ortiz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reyes.Ortiz et al\\.", "year": 2016}, {"title": "Discrete variational autoencoders", "author": ["Jason Tyler Rolfe"], "venue": "arXiv preprint arXiv:1609.02200,", "citeRegEx": "Rolfe.,? \\Q2016\\E", "shortCiteRegEx": "Rolfe.", "year": 2016}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Logistic regression-hsmm-based heart sound segmentation", "author": ["David B Springer", "Lionel Tarassenko", "Gari D Clifford"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "Springer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "Unsupervised structure learning in high-dimensional sequential data is an important research problem in a number of applications, such as machine translation, speech recognition, computational biology, and computational physiology Sutskever et al. (2014); Dai et al.", "startOffset": 231, "endOffset": 255}, {"referenceID": 6, "context": "(2014); Dai et al. (2017b). For example, in medical diagnosis, learning the segment boundaries and labeling of complicated physical signals is very useful for doctors to understand the underlying behavior or activity types.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "(2014); Dai et al. (2017b). For example, in medical diagnosis, learning the segment boundaries and labeling of complicated physical signals is very useful for doctors to understand the underlying behavior or activity types. Models for sequential data analysis such as recurrent neural networks (RNNs)Rumelhart et al. (1988) and hidden Markov models (HMMs)Rabiner (1989) are widely used.", "startOffset": 8, "endOffset": 324}, {"referenceID": 6, "context": "(2014); Dai et al. (2017b). For example, in medical diagnosis, learning the segment boundaries and labeling of complicated physical signals is very useful for doctors to understand the underlying behavior or activity types. Models for sequential data analysis such as recurrent neural networks (RNNs)Rumelhart et al. (1988) and hidden Markov models (HMMs)Rabiner (1989) are widely used.", "startOffset": 8, "endOffset": 370}, {"referenceID": 14, "context": "literature have investigated approaches of combining probabilistic generative models and recurrent neural networks for the sake of their complementary strengths in nonlinear representation learning and effective estimation of parameters Johnson et al. (2016); Dai et al.", "startOffset": 237, "endOffset": 259}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al.", "startOffset": 8, "endOffset": 51}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al. (2017). However, most of existing models are designed primarily for continuous situations and do not extend to discrete latent variables Johnson et al.", "startOffset": 8, "endOffset": 261}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al. (2017). However, most of existing models are designed primarily for continuous situations and do not extend to discrete latent variables Johnson et al. (2016); Krishnan et al.", "startOffset": 8, "endOffset": 413}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al. (2017). However, most of existing models are designed primarily for continuous situations and do not extend to discrete latent variables Johnson et al. (2016); Krishnan et al. (2015); Archer et al.", "startOffset": 8, "endOffset": 437}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks.", "startOffset": 8, "endOffset": 53}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework.", "startOffset": 8, "endOffset": 189}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework.", "startOffset": 8, "endOffset": 260}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density.", "startOffset": 8, "endOffset": 438}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al.", "startOffset": 8, "endOffset": 810}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al.", "startOffset": 8, "endOffset": 833}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.", "startOffset": 8, "endOffset": 874}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al.", "startOffset": 8, "endOffset": 1492}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al. (2017b), and is composed with a continuous sequence (i.", "startOffset": 8, "endOffset": 1530}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al. (2017b), and is composed with a continuous sequence (i.e., hidden states in RNN) as well as two discrete sequences (i.e., segmentation variables and labels in SSM). The inference network(as will shown in Figure 2(b)) can take the advantages of bi-directional temporal information by augmented variables, and efficiently approximate the categorical variables in segmentation and segment labels via the recently proposed Gumbel-Softmax approximation Jang et al. (2017); Maddison et al.", "startOffset": 8, "endOffset": 1989}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al. (2017b), and is composed with a continuous sequence (i.e., hidden states in RNN) as well as two discrete sequences (i.e., segmentation variables and labels in SSM). The inference network(as will shown in Figure 2(b)) can take the advantages of bi-directional temporal information by augmented variables, and efficiently approximate the categorical variables in segmentation and segment labels via the recently proposed Gumbel-Softmax approximation Jang et al. (2017); Maddison et al. (2016). Thus, SSNN can model the complex and long-range dependencies in sequential data, but also maintain the structure learning ability of SSMs with efficient inference.", "startOffset": 8, "endOffset": 2013}, {"referenceID": 37, "context": "Recurrent neural networks (RNNs), as a wide range of sequential models for time series, have been applied in a number of applications Sutskever et al. (2014). Here we introduce basic properties and notations of RNNs.", "startOffset": 134, "endOffset": 158}, {"referenceID": 6, "context": "State space models such as hidden Markov models (HMMs) and hidden Semi-Markov models (HSMMs) are also widely-used methods in sequential learning Dai et al. (2017b); Chiappa et al.", "startOffset": 145, "endOffset": 164}, {"referenceID": 6, "context": "State space models such as hidden Markov models (HMMs) and hidden Semi-Markov models (HSMMs) are also widely-used methods in sequential learning Dai et al. (2017b); Chiappa et al. (2014); Dewar et al.", "startOffset": 145, "endOffset": 187}, {"referenceID": 6, "context": "State space models such as hidden Markov models (HMMs) and hidden Semi-Markov models (HSMMs) are also widely-used methods in sequential learning Dai et al. (2017b); Chiappa et al. (2014); Dewar et al. (2012). In HMM, given an observed sequence x1:T , each xt is generated based on the hidden state zt \u2208 {1, 2, .", "startOffset": 145, "endOffset": 208}, {"referenceID": 19, "context": "There are many variants of HSMMs such as the Hierarchical Dirichlet-Process HSMM (HDP-HSMM) Johnson and Willsky (2013) and subHSMM Johnson and Willsky (2014).", "startOffset": 92, "endOffset": 119}, {"referenceID": 19, "context": "There are many variants of HSMMs such as the Hierarchical Dirichlet-Process HSMM (HDP-HSMM) Johnson and Willsky (2013) and subHSMM Johnson and Willsky (2014). The subHSMM and HDP-HSMM extend their HMM counterparts by allowing explicit modeling of state duration lengths with arbitrary distributions.", "startOffset": 92, "endOffset": 158}, {"referenceID": 2, "context": "Recent methods in Bayesian learning, such as the score function (or REINFORCE) Archer et al. (2015) and the Stochastic Gradient Variational Bayes (SGVB) Kingma and Welling (2013), are common black-box methods that lead to tractable solutions.", "startOffset": 79, "endOffset": 100}, {"referenceID": 2, "context": "Recent methods in Bayesian learning, such as the score function (or REINFORCE) Archer et al. (2015) and the Stochastic Gradient Variational Bayes (SGVB) Kingma and Welling (2013), are common black-box methods that lead to tractable solutions.", "startOffset": 79, "endOffset": 179}, {"referenceID": 2, "context": "Recent methods in Bayesian learning, such as the score function (or REINFORCE) Archer et al. (2015) and the Stochastic Gradient Variational Bayes (SGVB) Kingma and Welling (2013), are common black-box methods that lead to tractable solutions. We resort to the SGVB since it could efficiently learn the approximation with relatively low variances Kingma and Welling (2013), while the score function suffers from high variances and heavy computational costs.", "startOffset": 79, "endOffset": 372}, {"referenceID": 31, "context": "Such attempts have been explored in many previous work (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), however they mainly focus on continuous variables, and little attention is paid to the discrete variable.", "startOffset": 55, "endOffset": 121}, {"referenceID": 25, "context": "Such attempts have been explored in many previous work (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), however they mainly focus on continuous variables, and little attention is paid to the discrete variable.", "startOffset": 55, "endOffset": 121}, {"referenceID": 30, "context": "Such attempts have been explored in many previous work (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), however they mainly focus on continuous variables, and little attention is paid to the discrete variable.", "startOffset": 55, "endOffset": 121}, {"referenceID": 16, "context": "Specifically, we first learn a bi-directional deterministic variable \u0125t = BiRNN(x1:t, xt:T ) , where BiRNN is a bi-directional RNN with each unit implemented as an LSTM Hochreiter and Schmidhuber (1997). Similar to Fraccaro et al.", "startOffset": 169, "endOffset": 203}, {"referenceID": 11, "context": "Similar to Fraccaro et al. (2016), we further use a backward recurrent function It = g\u03c6I (It+1, [xt, \u0125t]) to explicitly capture forward and backward information in the sequence via \u0125t, where [xt, \u0125t] is the concatenation of xt and \u0125t.", "startOffset": 11, "endOffset": 34}, {"referenceID": 5, "context": "However, the reparameterization tricks and their extensions (Chung et al., 2016) are not directly applicable due to the discrete random variables, i.", "startOffset": 60, "endOffset": 80}, {"referenceID": 18, "context": "Thus we turn to the recently proposed Gumbel-Softmax reparameterization trick (Jang et al., 2017; Maddison et al., 2016), as shown in the following.", "startOffset": 78, "endOffset": 120}, {"referenceID": 32, "context": "Thus we turn to the recently proposed Gumbel-Softmax reparameterization trick (Jang et al., 2017; Maddison et al., 2016), as shown in the following.", "startOffset": 78, "endOffset": 120}, {"referenceID": 32, "context": "Via the Gumbel Softmax transformation, we set y(t) \u223c Concrete(\u03c0(t), \u03c4) according to (Maddison et al., 2016).", "startOffset": 84, "endOffset": 107}, {"referenceID": 27, "context": "Now we can sample y(t) from the Gumbel-Softmax posterior in replacement of the categorically distributed \u03b3(t), and use the back-propagation gradient with the ADAM Kingma and Ba (2014) optimizer to learn parameters \u03b8 and \u03c6.", "startOffset": 163, "endOffset": 184}, {"referenceID": 10, "context": "In terms of combining both and SSM and RNNs, the papers mostly close to our paper include Johnson et al. (2016); Krishnan et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al.", "startOffset": 14, "endOffset": 59}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems.", "startOffset": 14, "endOffset": 94}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering.", "startOffset": 14, "endOffset": 246}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning.", "startOffset": 14, "endOffset": 381}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference.", "startOffset": 14, "endOffset": 580}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables.", "startOffset": 14, "endOffset": 665}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network.", "startOffset": 14, "endOffset": 1161}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance.", "startOffset": 14, "endOffset": 1439}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients.", "startOffset": 14, "endOffset": 1612}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances.", "startOffset": 14, "endOffset": 1864}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner.", "startOffset": 14, "endOffset": 1970}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables.", "startOffset": 14, "endOffset": 2128}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn.", "startOffset": 14, "endOffset": 2510}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn. In Dai et al. (2017a), the discrete optimization is replaced by the maximization over the negative Helmholtz free energy.", "startOffset": 14, "endOffset": 2645}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn. In Dai et al. (2017a), the discrete optimization is replaced by the maximization over the negative Helmholtz free energy. In contrast to linearizing intractable terms around the current iteration as used in the above approaches, we handle intractable terms via recognition networks and amortized inference(with the aid of Gumbel-Softmax reparameterization Jang et al. (2017); Maddison et al.", "startOffset": 14, "endOffset": 2998}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn. In Dai et al. (2017a), the discrete optimization is replaced by the maximization over the negative Helmholtz free energy. In contrast to linearizing intractable terms around the current iteration as used in the above approaches, we handle intractable terms via recognition networks and amortized inference(with the aid of Gumbel-Softmax reparameterization Jang et al. (2017); Maddison et al. (2016)) in this paper.", "startOffset": 14, "endOffset": 3022}, {"referenceID": 35, "context": "Then we test SSNN with learning segmentations and latent labels on Human activity Reyes-Ortiz et al. (2016) dataset, Drosophila dataset Kain et al.", "startOffset": 82, "endOffset": 108}, {"referenceID": 22, "context": "(2016) dataset, Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 26, "context": "All models in the experiment use the Adam Kingma and Ba (2014) optimizer.", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "We implement the proposed model based on Theano Al-Rfou et al. (2016) and Block & Fuel Van Merri\u00ebnboer et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "We implement the proposed model based on Theano Al-Rfou et al. (2016) and Block & Fuel Van Merri\u00ebnboer et al. (2015).", "startOffset": 48, "endOffset": 117}, {"referenceID": 23, "context": "For fair comparison with Karl et al. (2016), we set m = l = 1, \u03bc = 0.", "startOffset": 25, "endOffset": 44}, {"referenceID": 23, "context": "For fair comparison with Karl et al. (2016), we set m = l = 1, \u03bc = 0.5, and g = 9.81. We convert the generated ground-truth angles to image observations. The system can be fully described by angle and angular velocity. We compare our method with Deep Variational Bayes Filter(DVBF-LL) Karl et al. (2016) and Deep Kalman Filters(DKF) Krishnan et al.", "startOffset": 25, "endOffset": 304}, {"referenceID": 23, "context": "For fair comparison with Karl et al. (2016), we set m = l = 1, \u03bc = 0.5, and g = 9.81. We convert the generated ground-truth angles to image observations. The system can be fully described by angle and angular velocity. We compare our method with Deep Variational Bayes Filter(DVBF-LL) Karl et al. (2016) and Deep Kalman Filters(DKF) Krishnan et al. (2015). The ordinary least square regression results are shown in Table 1.", "startOffset": 25, "endOffset": 356}, {"referenceID": 29, "context": ", Blizzard and TIMIT datasets Prahallad et al. (2013). Blizzard records the English speech with 300 hours by a female speaker.", "startOffset": 30, "endOffset": 54}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al.", "startOffset": 124, "endOffset": 144}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al.", "startOffset": 124, "endOffset": 178}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al. (2015a); Gan et al.", "startOffset": 124, "endOffset": 197}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al. (2015a); Gan et al. (2015); Sutskever et al.", "startOffset": 124, "endOffset": 216}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al. (2015a); Gan et al. (2015); Sutskever et al. (2014).", "startOffset": 124, "endOffset": 241}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.", "startOffset": 87, "endOffset": 131}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM).", "startOffset": 87, "endOffset": 1229}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM). We also compare to VRNN-I in which the latent variables in VRNN are constrained to be independent across time steps. For SRNN Fraccaro et al. (2016), we compare with the smoothing and filtering performance denoted as SRRR (smooth), SRNN (filt) and SRNN (smooth+ Resq) respectively.", "startOffset": 87, "endOffset": 1514}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM). We also compare to VRNN-I in which the latent variables in VRNN are constrained to be independent across time steps. For SRNN Fraccaro et al. (2016), we compare with the smoothing and filtering performance denoted as SRRR (smooth), SRNN (filt) and SRNN (smooth+ Resq) respectively. The results of VRNN-GMM, VRNN-Gauss and VRNN-I-Gauss are taken from Chung et al. (2015), and those of SRNN (smooth+Resq), SRNN (smooth) and SRNN (filt) are taken from Fraccaro et al.", "startOffset": 87, "endOffset": 1735}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM). We also compare to VRNN-I in which the latent variables in VRNN are constrained to be independent across time steps. For SRNN Fraccaro et al. (2016), we compare with the smoothing and filtering performance denoted as SRRR (smooth), SRNN (filt) and SRNN (smooth+ Resq) respectively. The results of VRNN-GMM, VRNN-Gauss and VRNN-I-Gauss are taken from Chung et al. (2015), and those of SRNN (smooth+Resq), SRNN (smooth) and SRNN (filt) are taken from Fraccaro et al. (2016). From Table 5.", "startOffset": 87, "endOffset": 1837}, {"referenceID": 35, "context": "To show the advantages of SSNN over HSMM and its variants when learning the segmentation and latent labels from sequences, we take experiments on Human activity Reyes-Ortiz et al. (2016), Drosophila dataset Kain et al.", "startOffset": 161, "endOffset": 187}, {"referenceID": 22, "context": "(2016), Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 22, "context": "(2016), Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al. (2016) Challenge dataset.", "startOffset": 27, "endOffset": 83}, {"referenceID": 16, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 77, "endOffset": 104}, {"referenceID": 16, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 77, "endOffset": 141}, {"referenceID": 1, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al.", "startOffset": 149, "endOffset": 169}, {"referenceID": 1, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al. (2017b).", "startOffset": 149, "endOffset": 201}, {"referenceID": 16, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 38, "endOffset": 65}, {"referenceID": 16, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 38, "endOffset": 102}, {"referenceID": 1, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al.", "startOffset": 110, "endOffset": 130}, {"referenceID": 1, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al. (2017b). For the HDP-HSMM and subHSMM, the observed sequences x1:T are generated by standard multivariate Gaussian distributions.", "startOffset": 110, "endOffset": 162}, {"referenceID": 13, "context": "We compare the proposed model to DRAW Gregor et al. (2015) and visualize our learned latent representations in Figure 5.", "startOffset": 38, "endOffset": 59}], "year": 2017, "abstractText": "Unsupervised structure learning in high-dimensional time series data has attracted a lot of research interests. For example, segmenting and labelling high dimensional time series can be helpful in behavior understanding and medical diagnosis. Recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models (e.g., Hidden Markov Models). This combination can model not only the long term dependency in sequential data, but also the uncertainty included in the hidden states. Inheriting these advantages of stochastic neural sequential models, we propose a structured and stochastic sequential neural network, which models both the long-term dependencies via recurrent neural networks and the uncertainty in the segmentation and labels via discrete random variables. For accurate and efficient inference, we present a bi-directional inference network by reparamterizing the categorical segmentation and labels with the recent proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient Variational Bayes. We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition. Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}