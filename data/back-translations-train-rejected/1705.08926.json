{"id": "1705.08926", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Counterfactual Multi-Agent Policy Gradients", "abstract": "Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.", "histories": [["v1", "Wed, 24 May 2017 18:52:17 GMT  (301kb,D)", "http://arxiv.org/abs/1705.08926v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.MA", "authors": ["jakob foerster", "gregory farquhar", "triantafyllos afouras", "nantas nardelli", "shimon whiteson"], "accepted": false, "id": "1705.08926"}, "pdf": {"name": "1705.08926.pdf", "metadata": {"source": "CRF", "title": "Counterfactual Multi-Agent Policy Gradients", "authors": ["Jakob N. Foerster", "Gregory Farquhar", "Triantafyllos Afouras", "Nantas Nardelli", "Shimon Whiteson"], "emails": ["jakob.foerster@cs.ox.ac.uk", "gregory.farquhar@cs.ox.ac.uk", "afourast@robots.ox.ac.uk", "nantas@robots.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that there will only be one time that there will be such a process, in which there will be such a process."}, {"heading": "2 Related Work", "text": "In recent years, it has been shown that people are able to survive on their own, and that they are able to survive on their own. (...) In recent years, it has been shown that people are able to survive on their own. (...) In recent years, the situation of people has changed. (...) In recent years, the number of people who are able to survive on their own has increased dramatically. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the world has changed."}, {"heading": "3 Background", "text": "We consider a fully cooperative multi-agent task that can be described as a stochastic game defined by a tuple G = 2016 (1)."}, {"heading": "4 Methods", "text": "In this section, we describe approaches to extending political gradients to our multi-agent environment."}, {"heading": "4.1 Independent Actor-Critic", "text": "The simplest way to apply political gradients to multiple agents is for each agent to learn independently, with their own actor and critic, from their own action observation history. This is essentially the idea behind independent Q-Learning (Tan, 1993), which is perhaps the most popular multi-agent learning algorithm, but with an actor-critic instead of Q-Learning. Therefore, we call this approach Independent Actor-Critic (IAC). In our implementation of the IAC, we accelerate learning by sharing parameters among agents, i.e., we learn only one actor and one critic used by all agents. Agents can still behave differently because they receive different observations, including an agent-specific ID, and thus develop different hidden states. Learning remains independent in the sense that the critic of each agent values only one local function, i.e. one that is tied to inter alia, not to u. Although we are aware of earlier applications of this specific algorithm, we consider it merely a basic contribution, but rather than an algorithm."}, {"heading": "4.2 Counterfactual Multi-Agent Policy Gradients", "text": "The difficulties discussed above arise because, beyond parameter distribution, IAC does not take advantage of the fact that learning is centralized in our environment. In this section, we propose counterfactual multi-agents (COMA) that overcome this limitation. However, three main ideas are subject to the centralization of the critic, 2) the use of a counterfactual baseline and 3) the use of a critical representation that enables an efficient assessment of the baseline. However, the rest of this section describes these ideals. However, first, COMA uses a centralized critic, 2) the use of a counterfactual baseline, and 3) the use of a critical representation that enables an efficient assessment of the baseline. However, the rest of this section uses a centralized critic. Note that in IAC each actor (among others) and each critic (among others) has the ability to apply the actions of the critic (among others) or V (perca) only on the basis of the agent's own observation history of action. However, the critic is only one that is needed during the course and only the actualization of the actuator is true."}, {"heading": "5 Experimental Setup", "text": "This year it is so far that it will only take a year before it will be able to leave the country in order to leave it again."}, {"heading": "6 Results", "text": "The results show that COMA is superior to IAC baselines in all scenarios. Interestingly, the IAC methodologies could ultimately learn sound strategies in m5v5, although they require much more episodes to do so. This may seem counterproductive, since in IAC methodologies the actor and the critical networks share parameters in their early shifts (see Section 5). These results suggest that the improved accuracy of policy assessment enabled by the conditioning of the global state is counterproductive, as the actor and the critical networks share parameters in their early shifts (see Section 5)."}, {"heading": "7 Conclusions & Future Work", "text": "This paper presented COMA Policy Gradients, a method in which a centralized critic is used to estimate a counterfactual advantage for decentralized strategies in multi-agent lending. COMA addresses the challenges of multi-agent lending by using a counterfactual baseline that marginalizes the actions of a single agent while keeping the actions of the other agents fixed. Our results on a decentralized StarCraft scale for micro-management show that COMA significantly improves final performance and training speed compared to other methods of multi-agent stakeholder criticism and remains competitive with state-of-the-art centralized controllers within the framework of best-performance reporting. Future work will expand COMA to scenarios with a large number of agents where centralized critics are more difficult to train and exploration more difficult to coordinate. We also aim to develop more model-efficient variants that are feasible for real-world applications such as self-driving cars."}, {"heading": "Acknowledgements", "text": "This work was supported by the Oxford-Google DeepMind Graduate Scholarship, the UK EPSRC CDT in Autonomous Intelligent Machines and Systems and a generous grant from Microsoft for their Azure Cloud Computing Services. We thank Nando de Freitas, Yannis Assael and Brendan Shillingford for helpful comments and discussions. We also thank Gabriel Synnaeve, Zeming Lin and the rest of the TorchCraft team at FAIR for their work on the interface."}, {"heading": "A Training Details and Hyperparameters", "text": "The training is carried out in batch mode, with a batch size of 30. Due to the sharing of parameters, all agents can be processed in parallel, with each agent occupying one batch entry for each episode and time step. The training cycle runs in three steps (completion of all three steps represents an episode in our diagrams): 1) Collect data: collect 30 episodes; 2) Train critic: apply one gradient step to the critic for each time step, starting at the end of the episode; and 3) Train actor: fully unroll the recurring part of the actor, apply aggregate gradient in reverse over all time steps, and apply a gradient upward.We use a target network for the critic that updates all 150 training steps for the critic and improves all 50 steps for the recurring critics of the IAC. The forward critic receives more learning steps as there is a parameter update for each absolute time step. We use a target network for the critic that updates all 150 training steps for the critic and improves all 50 steps for the recurring critics of the IAC."}, {"heading": "B Algorithm", "text": "Algorithm 1 Counterfactual Multi-Agent (COMA) Policy GradientsInitialise (1 = 1, 1 = 1, 1 = 1, 0 = 0 for each agent for a while st 6 = Terminal and t < T do t = t + 1 for each agent a do hat = Actor (haat, h a t \u2212 1, u a \u2212 1, a, u; \u03b8i) Example uat from \u03c0 (h a t, (e) Get reward rt and next state st + 1 Add episode to buffer episodes in buffer into single batch for t = 1 to T do / / / / from now processing all agents in parallel via single batch unroll RNN using states, actions and rewards (h a t, (e) Get reward rt and next state st + 1 Add episode to buffer episodes in buffer for t = 1 to T do / / / / from now processing all agents in parallel via single batch unroll RNN using states, actions and rewards Calculate for Tcy and next state st + 1 Add episode to buffer for t do / / / / from now processing all agents in parallel via single batch unroll RNN using rewards (h a t, (a) Get reward rt and next state st + 1 Add episode to buffer episodes in buffer for t do / / / / / from now processing all agents in parallel via single batch unroll RNNN using states, actions and rewards Calculate for TNNN at a \u2212 a \u2212 Q and reactions)."}], "references": [{"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Busoniu", "Lucian", "Babuska", "Robert", "De Schutter", "Bart"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "An overview of recent progress in the study of distributed multi-agent coordination", "author": ["Cao", "Yongcan", "Yu", "Wenwu", "Ren", "Wei", "Chen", "Guanrong"], "venue": "IEEE Transactions on Industrial informatics,", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "All learning is local: Multi-agent learning in global reward games", "author": ["Chang", "Yu-Han", "Ho", "Tracey", "Kaelbling", "Leslie Pack"], "venue": "In NIPS,", "citeRegEx": "Chang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2003}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Das", "Abhishek", "Kottur", "Satwik", "Moura", "Jos\u00e9 MF", "Lee", "Stefan", "Batra", "Dhruv"], "venue": "arXiv preprint arXiv:1703.06585,", "citeRegEx": "Das et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Nardelli", "Nantas", "Farquhar", "Gregory", "Torr", "Philip", "Kohli", "Pushmeet", "Whiteson", "Shimon"], "venue": "In Proceedings of The 34th International Conference on Machine Learning,", "citeRegEx": "Foerster et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2017}, {"title": "Cooperative multi-agent control using deep reinforcement learning", "author": ["Gupta", "Jayesh K", "Egorov", "Maxim", "Kochenderfer", "Mykel"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Reinforcement learning in feedback control", "author": ["Hafner", "Roland", "Riedmiller", "Martin"], "venue": "Machine learning,", "citeRegEx": "Hafner et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hafner et al\\.", "year": 2011}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning to play guess who? and inventing a grounded language as a consequence", "author": ["Jorge", "Emilio", "K\u00e5geb\u00e4ck", "Mikael", "Gustavsson", "Emil"], "venue": "arXiv preprint arXiv:1611.03218,", "citeRegEx": "Jorge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jorge et al\\.", "year": 2016}, {"title": "An analysis of actor-critic algorithms using eligibility traces: reinforcement learning with imperfect value functions", "author": ["Kimura", "Hajime", "Kobayashi", "Shigenobu"], "venue": "Journal of Japanese Society for Artificial Intelligence,", "citeRegEx": "Kimura et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kimura et al\\.", "year": 2000}, {"title": "Multi-agent reinforcement learning as a rehearsal for decentralized planning", "author": ["Kraemer", "Landon", "Banerjee", "Bikramjit"], "venue": null, "citeRegEx": "Kraemer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kraemer et al\\.", "year": 2016}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["Lazaridou", "Angeliki", "Peysakhovich", "Alexander", "Baroni", "Marco"], "venue": "arXiv preprint arXiv:1612.07182,", "citeRegEx": "Lazaridou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2016}, {"title": "Multi-agent reinforcement learning in sequential social dilemmas", "author": ["Leibo", "Joel Z", "Zambaldi", "Vinicius", "Lanctot", "Marc", "Marecki", "Janusz", "Graepel", "Thore"], "venue": "arXiv preprint arXiv:1702.03037,", "citeRegEx": "Leibo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Leibo et al\\.", "year": 2017}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Emergence of grounded compositional language in multi-agent populations", "author": ["Mordatch", "Igor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1703.04908,", "citeRegEx": "Mordatch et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2017}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["Oliehoek", "Frans A", "Spaan", "Matthijs T. J", "Vlassis", "Nikos"], "venue": null, "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "Deep decentralized multi-task multi-agent rl under partial observability", "author": ["Omidshafiei", "Shayegan", "Pazis", "Jason", "Amato", "Christopher", "How", "Jonathan P", "Vian", "John"], "venue": "arXiv preprint arXiv:1703.06182,", "citeRegEx": "Omidshafiei et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Omidshafiei et al\\.", "year": 2017}, {"title": "Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games", "author": ["Peng", "Yuan", "Quan", "Wen", "Ying", "Yang", "Yaodong", "Tang", "Zhenkun", "Long", "Haitao", "Wang", "Jun"], "venue": "arXiv preprint arXiv:1703.10069,", "citeRegEx": "Peng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "CoRR, abs/1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown", "year": 2009}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sukhbaatar", "Sainbayar", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Torchcraft: a library for machine learning research on real-time strategy games", "author": ["Synnaeve", "Gabriel", "Nardelli", "Nantas", "Auvolat", "Alex", "Chintala", "Soumith", "Lacroix", "Timoth\u00e9e", "Lin", "Zeming", "Richoux", "Florian", "Usunier", "Nicolas"], "venue": "arXiv preprint arXiv:1611.00625,", "citeRegEx": "Synnaeve et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Synnaeve et al\\.", "year": 2016}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["Tampuu", "Ardi", "Matiisen", "Tambet", "Kodelja", "Dorian", "Kuzovkin", "Ilya", "Korjus", "Kristjan", "Aru", "Juhan", "Jaan", "Vicente", "Raul"], "venue": "arXiv preprint arXiv:1511.08779,", "citeRegEx": "Tampuu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tampuu et al\\.", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Tan", "Ming"], "venue": "In Proceedings of the tenth international conference on machine learning,", "citeRegEx": "Tan and Ming.,? \\Q1993\\E", "shortCiteRegEx": "Tan and Ming.", "year": 1993}, {"title": "Distributed agent-based air traffic flow management", "author": ["Tumer", "Kagan", "Agogino", "Adrian"], "venue": "In Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Tumer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tumer et al\\.", "year": 2007}, {"title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks", "author": ["Usunier", "Nicolas", "Synnaeve", "Gabriel", "Lin", "Zeming", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1609.02993,", "citeRegEx": "Usunier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2016}, {"title": "Sample efficient actor-critic with experience", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": "replay. arXiv preprint arXiv:1611.01224,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "The optimal reward baseline for gradient-based reinforcement learning", "author": ["Weaver", "Lex", "Tao", "Nigel"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Weaver et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Weaver et al\\.", "year": 2001}, {"title": "The packet-world: A test bed for investigating situated multi-agent systems. In Software Agent-Based Applications, Platforms and Development Kits", "author": ["Weyns", "Danny", "Helleboogh", "Alexander", "Holvoet", "Tom"], "venue": null, "citeRegEx": "Weyns et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weyns et al\\.", "year": 2005}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Multiagent reinforcement learning for multi-robot systems: A survey", "author": ["Yang", "Erfu", "Gu", "Dongbing"], "venue": "Technical report, tech. rep,", "citeRegEx": "Yang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2004}, {"title": "A multi-agent framework for packet routing in wireless sensor", "author": ["Ye", "Dayong", "Zhang", "Minjie", "Yang", "Yun"], "venue": "networks. sensors,", "citeRegEx": "Ye et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2015}, {"title": "Multi-agent framework for third party logistics in e-commerce", "author": ["Ying", "Wang", "Dayong", "Sang"], "venue": "Expert Systems with Applications,", "citeRegEx": "Ying et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2005}, {"title": "Empirically evaluating multiagent learning algorithms", "author": ["E. Zawadzki", "A. Lipson", "K. Leyton-Brown"], "venue": "arXiv preprint 1401.8074,", "citeRegEx": "Zawadzki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zawadzki et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Many complex reinforcement learning (RL) problems such as the coordination of autonomous vehicles (Cao et al., 2013), network packet delivery (Ye et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 38, "context": ", 2013), network packet delivery (Ye et al., 2015), and distributed logistics (Ying & Dayong, 2005) are naturally modelled as cooperative multi-agent systems.", "startOffset": 33, "endOffset": 50}, {"referenceID": 20, "context": "This centralised training of decentralised policies is a standard paradigm for multi-agent planning (Oliehoek et al., 2008; Kraemer & Banerjee, 2016) and has recently been picked up by the deep RL community (Foerster et al.", "startOffset": 100, "endOffset": 149}, {"referenceID": 7, "context": ", 2008; Kraemer & Banerjee, 2016) and has recently been picked up by the deep RL community (Foerster et al., 2016; Jorge et al., 2016).", "startOffset": 91, "endOffset": 134}, {"referenceID": 13, "context": ", 2008; Kraemer & Banerjee, 2016) and has recently been picked up by the deep RL community (Foerster et al., 2016; Jorge et al., 2016).", "startOffset": 91, "endOffset": 134}, {"referenceID": 2, "context": "Another crucial challenge is multi-agent credit assignment (Chang et al., 2003): in cooperative settings, joint actions typically generate only global rewards, making it difficult for each agent to", "startOffset": 59, "endOffset": 79}, {"referenceID": 32, "context": "Previous works (Usunier et al., 2016; Peng et al., 2017) have made use of a centralised control policy that conditions on the entire state and can use powerful macro-actions, using StarCraft\u2019s built-in planner, that combine movement and attack actions.", "startOffset": 15, "endOffset": 56}, {"referenceID": 22, "context": "Previous works (Usunier et al., 2016; Peng et al., 2017) have made use of a centralised control policy that conditions on the entire state and can use powerful macro-actions, using StarCraft\u2019s built-in planner, that combine movement and attack actions.", "startOffset": 15, "endOffset": 56}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments.", "startOffset": 66, "endOffset": 105}, {"referenceID": 40, "context": "(2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014) to learn how to play two-player pong.", "startOffset": 60, "endOffset": 123}, {"referenceID": 6, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 16, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 7, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 25, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al.", "startOffset": 67, "endOffset": 315}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014) to learn how to play two-player pong. More recently the same method has been used by Leibo et al. (2017) to study the emergence of collaboration and defection in sequential social dilemmas.", "startOffset": 67, "endOffset": 537}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014) to learn how to play two-player pong. More recently the same method has been used by Leibo et al. (2017) to study the emergence of collaboration and defection in sequential social dilemmas. Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016). In this line of work, passing gradients between agents during training and sharing parameters are two common ways to take advantage of centralised training. However, these methods do not allow for extra state information to be used during learning and do not address the multi-agent credit assignment problem. Gupta et al. (2017) investigate actor-critic methods for decentralised execution with centralised training.", "startOffset": 67, "endOffset": 1168}, {"referenceID": 28, "context": "Usunier et al. (2016) use a greedy MDP, which at each timestep sequentially chooses actions for agents given all previous actions, in combination with zero-order optimisation, while Peng et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "(2016) use a greedy MDP, which at each timestep sequentially chooses actions for agents given all previous actions, in combination with zero-order optimisation, while Peng et al. (2017) use an actor-critic method that relies on RNNs to exchange information between the agents.", "startOffset": 167, "endOffset": 186}, {"referenceID": 7, "context": "The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies.", "startOffset": 46, "endOffset": 69}, {"referenceID": 7, "context": "The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies. However, they focus on stabilising experience replay while using DQN and do not make full use of the centralised training regime. As they do not report on absolute win-rates we do not compare performance directly. However, Usunier et al. (2016) address similar scenarios to our experiments and implement a DQN baseline in a fully observable setting.", "startOffset": 46, "endOffset": 385}, {"referenceID": 7, "context": "The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies. However, they focus on stabilising experience replay while using DQN and do not make full use of the centralised training regime. As they do not report on absolute win-rates we do not compare performance directly. However, Usunier et al. (2016) address similar scenarios to our experiments and implement a DQN baseline in a fully observable setting. In section 6 we therefore report our competitive performance against these state-of-the-art baselines, while maintaining decentralised control. Omidshafiei et al. (2017) also address the stability of experience replay in multi-agent settings, but assume a fully decentralised training regime.", "startOffset": 46, "endOffset": 660}, {"referenceID": 20, "context": "Following previous work (Oliehoek et al., 2008; Kraemer & Banerjee, 2016; Foerster et al., 2016; Jorge et al., 2016), our problem setting allows centralised training but requires decentralised execution.", "startOffset": 24, "endOffset": 116}, {"referenceID": 7, "context": "Following previous work (Oliehoek et al., 2008; Kraemer & Banerjee, 2016; Foerster et al., 2016; Jorge et al., 2016), our problem setting allows centralised training but requires decentralised execution.", "startOffset": 24, "endOffset": 116}, {"referenceID": 13, "context": "Following previous work (Oliehoek et al., 2008; Kraemer & Banerjee, 2016; Foerster et al., 2016; Jorge et al., 2016), our problem setting allows centralised training but requires decentralised execution.", "startOffset": 24, "endOffset": 116}, {"referenceID": 4, "context": "To condition on this full history, a deep RL agent may make use of a recurrent neural network (Hausknecht & Stone, 2015), typically making use of a gated model such as LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Chung et al., 2014).", "startOffset": 213, "endOffset": 233}, {"referenceID": 27, "context": "In the remainder of this section, we provide some background on single-agent policy gradient methods (Sutton et al., 1999).", "startOffset": 101, "endOffset": 122}, {"referenceID": 14, "context": "In actor-critic approaches (Kimura et al., 2000; Schulman et al., 2015; Wang et al., 2016; Hafner & Riedmiller, 2011), the actor, i.", "startOffset": 27, "endOffset": 117}, {"referenceID": 23, "context": "In actor-critic approaches (Kimura et al., 2000; Schulman et al., 2015; Wang et al., 2016; Hafner & Riedmiller, 2011), the actor, i.", "startOffset": 27, "endOffset": 117}, {"referenceID": 33, "context": "In actor-critic approaches (Kimura et al., 2000; Schulman et al., 2015; Wang et al., 2016; Hafner & Riedmiller, 2011), the actor, i.", "startOffset": 27, "endOffset": 117}, {"referenceID": 18, "context": "where y = (1 \u2212 \u03bb) \u2211\u221e n=1 \u03bb n\u22121G (n) t , and the n-step returns G (n) t are calculated using a target network (Mnih et al., 2015) with parameters copied periodically from \u03b8.", "startOffset": 109, "endOffset": 128}, {"referenceID": 35, "context": "Many simpler multi-agent settings, such as Predator-Prey (Tan, 1993) or Packet World (Weyns et al., 2005), by contrast, have full simulators with controlled randomness that can be freely set to any state in order to perfectly replay experiences.", "startOffset": 85, "endOffset": 105}, {"referenceID": 31, "context": "This damage-based reward signal is comparable to that used by Usunier et al. (2016). Unlike Peng et al.", "startOffset": 62, "endOffset": 84}, {"referenceID": 22, "context": "Unlike Peng et al. (2017), our approach does not require estimating local rewards.", "startOffset": 7, "endOffset": 26}, {"referenceID": 3, "context": "The actor consists of 128-bit gated recurrent units (GRUs) (Cho et al., 2014) that use fully connected layers both to process the input and to produce the output values from the hidden state, ht .", "startOffset": 59, "endOffset": 77}, {"referenceID": 28, "context": "Our implementation uses TorchCraft (Synnaeve et al., 2016) and Torch 7 (Collobert et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 5, "context": ", 2016) and Torch 7 (Collobert et al., 2011).", "startOffset": 20, "endOffset": 44}, {"referenceID": 32, "context": "Usunier et al. (2016) report the performance of their best agents trained with their state-of-the-art centralised controller labelled GMEZO (greedy-MDP with episodic zero-order optimisation), and for a centralised DQN controller, both given a full field of view and access to attack-move macroactions.", "startOffset": 0, "endOffset": 22}], "year": 2017, "abstractText": "Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing or the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents\u2019 policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent\u2019s action, while keeping the other agents\u2019 actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.", "creator": "LaTeX with hyperref package"}}}