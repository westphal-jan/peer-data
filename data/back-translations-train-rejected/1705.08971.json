{"id": "1705.08971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Optimal Cooperative Inference", "abstract": "Cooperative transmission of data fosters rapid accumulation of knowledge by efficiently combining experience across learners. Although well studied in human learning, there has been less attention to cooperative transmission of data in machine learning, and we consequently lack strong formal frameworks through which we may reason about the benefits and limitations of cooperative inference. We present such a framework. We introduce a novel index for measuring the effectiveness of probabilistic information transmission, and cooperative information transmission specifically. We relate our cooperative index to previous measures of teaching in deterministic settings. We prove conditions under which optimal cooperative inference can be achieved, including a representation theorem which constrains the form of inductive biases for learners optimized for cooperative inference. We conclude by demonstrating how these principles may inform the design of machine learning algorithms and discuss implications for human learning, machine learning, and human-machine learning systems.", "histories": [["v1", "Wed, 24 May 2017 21:42:00 GMT  (356kb,D)", "http://arxiv.org/abs/1705.08971v1", "14 pages (5 pages of Supplementary Material), 1 figure"]], "COMMENTS": "14 pages (5 pages of Supplementary Material), 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["scott cheng-hsin yang", "yue yu", "arash givchi", "pei wang", "wai keen vong", "patrick shafto"], "accepted": false, "id": "1705.08971"}, "pdf": {"name": "1705.08971.pdf", "metadata": {"source": "CRF", "title": "Optimal Cooperative Inference", "authors": ["Scott Cheng-Hsin Yang", "Yue Yu", "Arash Givchi", "Pei Wang", "Wai Keen Vong"], "emails": ["scott.cheng.hsin.yang@gmail.com", "patrick.shafto@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In addition to direct observations and actions in their own environment, humans are also engaged in the targeted selection of data, the goal of which is to transmit knowledge about the world to less knowledgeable actors. Moreover, less knowledgeable actors assume a targeted, cooperative selection and use of cooperation to expand learning. Cooperative data selection and learning from such data play a central role in the theories of cognition [1], cognitive development [11] and cultural evolution [22]. Indeed, this cooperative inference is the trait that drives the accumulation of knowledge over generations. [23, 6] Such communication through cooperative inference has received relatively limited attention in the field of machine learning. The principles that seem to drive human cultural knowledge accumulation can also be used in machines to achieve similar goals."}, {"heading": "2 The Transmission Index", "text": "In this section, we define the transmission index to quantify the communication effectiveness. Communication takes place between two agents whom we call a teacher and a learner. Here, the teacher represents the process of data selection to convey a particular concept, and the learner represents the follow-up process of interpreting the received data. In a probable context, the effectiveness of communication is related to the probability that the interpretation of the learner coincides with the intended concept of the teacher. Let us leave h a concept in a limited concept space H. A dataset space, D, is a collection of subsets of a specified limited set of data points. D, D is referred to as a dataset that defines the PT (D, h) as a teacher the probability of selecting a dataset D for the communication of a specified concept h, and PL (h) is a collection of subsets of a specified limited set of data points. D, is referred to as a dataset. We do not respect the size of H, and fully respect D."}, {"heading": "3 Connection to Average Teaching Dimension", "text": "In this section we establish the link between the Transmission Index and the Average Teaching Dimension. The Average Teaching Dimension is a variant of the Teaching Dimension, a classic measure for quantifying the effectiveness of teaching. However, the Teaching Dimension and these analyses are based on a deterministic learning model and focus on efficiency rather than effectiveness. In order to establish the link to the analysis of the Teaching Dimension, we first extend the Transmission Index, a measure of effectiveness, to the measure of the effectiveness of teaching. Then we show that the expected Teaching Dimension, which is well defined for the likely transfer of knowledge, is the same as the Average Teaching Dimension as knowledge transfer. The analyses of the Teaching Dimension are typically anchored in the concept of the learning framework."}, {"heading": "4 Optimal cooperative inference", "text": "The transmission index introduced in Section 2 assumes that the learner and teacher, or more abstractly speaking, the process of concluding and the data selection process are independent of each other. However, communication for the transmission of knowledge is often cooperative (e.g. in pedagogy [9, 10] and the conversations [13]. Here, cooperation implies that the selection of the teacher's data depends on what the learner derives from it and vice versa. In this section, we formalize the cooperative inference that captures this interdependence between the two processes of inference and selection [15, 17]. It can be seen as a way of mapping a common convention to another that is more effective in the transmission of knowledge without a prior agreement on the coding of data concept pairs. We define the cooperative index as a measure of communication effectiveness in the cooperative environment by applying the transmission index to cooperative inference."}, {"heading": "5 Discussion", "text": "We have presented a transmission index to quantify the effectiveness of knowledge transfer across data, coupled with previous efficiency-focused approaches, a cooperative index to quantify the effectiveness of cooperative knowledge transfer, proven bidirectional conditions that attribute inductive distortions to optimal cooperative inference, and a simulation that illustrates how classic learning models can be modified to improve cooperative knowledge transfer across generations. Our results inform theory across fields. For fields interested in human learning, our cooperative inference representation theorem (Theorem 4.9) offers the first predictions of how cognition would be structured if it were optimized to facilitate the transmission of knowledge across generations. For fields interested in machine learning, there is great interest in whether machine learning can be interpreted or explained (e.g. banking [20], medicine, [18] a self-driving car framework, 3)."}, {"heading": "6 Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Details of Remark 2.6", "text": "The case 1 | D | \u221e and | H | is finite. TI (L, T) is clearly defined, laterally m \u2192 | D | m \u2211 i = 1 Li, jTi, j \u2264 | D | \u2211 i = 1 Ti, j = 1implies that lim m m m m \u2192 | D | m \u2211 i = 1 Li, jTi, j = 1 Li, jTi, j \u2264 | \u2211 j = 1 Li, j = 1. Then0 \u2264 TI (L, T) = lim n \u2192 | H | 1n \u0445 i = 1 n \u2211 j = 1 Li, jTi, j \u2264 lim n \u2192 H | \u2211 j = 1 Li, j = 1. Then0 \u2264 TI (L, T) = lim n \u2192."}, {"heading": "6.2 Proof of Theorem 4.5", "text": "For convenience, we first write the fixed titeration of (2) k = k = k (1) k (1) k = k (1) k = k (1) k (1) k (1) c (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x) x (1) x (1) x) x (2) x (1) x (x) x (x) x (1) x (x) x (x) x (1) x (1) x (1) x) x (1) x (1) x (1) x) x (1) x (1) x (1) x) x (x) x (x) x, x (x) x (x) x (x) x (2) x (1) x (x) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (x (1) x (1) x (1) x (1) x (1) x (x (1) x (1) x (1) x (1) x (1) x (x (1) x (1) x (1) x (x (1) x (1) x (1) x (1) x (1) x (x (1) x (1) x (1) x (x (1) x (1) x (x (1) x (1) x (1) x (1) x (1) x (x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x ("}, {"heading": "6.3 Proof of Theorem 4.9", "text": "The proof (1) (1) (2) (b): We first prove that (a) CI (M) = 1, and (b) M has exactly one positive diagonal are equivalent. Since M is a n \u00b7 n non-negative matrix with at least one positive diagonal, Theorem 4.5 guarantees that the iteration of the equation (3) converges exactly at a double stochastic matrix, M (3). According to Birkhoff-von Neumann Theorem [2, 24] there is only a slightly positive diagonal 1,. We assume the inner product notation between matrices: A \u00b7 B = i, j Ai, jBi, jBi, j, for two n \u00d7 n square matrices A and B. Then the following. We assume the inner product notation between matrices."}, {"heading": "6.4 Details to Example 4.10", "text": "To construct M, we must first state that M 1.1 = M 1.2 under all settings of \u2206, a and q. The reason for this is that a first and second order polynomial have the same fit to D1. For M 2.1, we know from symmetry arguments that the maximum fit of a first order polynomial to D2 is a horizontal line (f (x) = b. We can find this value of b by looking for a grid. In view of this b: M 2.1 = Nq (a; b) 2Nq (\u2212 a; b) 2Nq (\u2212 b) Nq (\u2206 + a; b), where Nq (z; b) = \u221a \u03b2 Cq eq (\u2212 xi \u2212 \u00b5) 2). Here is \u03b2 = 15 \u2212 3q (\u2212 b) 2Nq (\u2212 b) 2Nq (\u2212 b) the variance 1. eq (x) is the q-exponential function defined by [1 + & \u2212 q \u2212 x] 1 \u2212 qq \u2212 qq ()."}], "references": [{"title": "Evolution of the social brain", "author": ["Robert A Barton", "Robin IM Dunbar"], "venue": "Machiavellian intelligence II: Extensions and evaluations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Three observations on linear algebra", "author": ["Garrett Birkhoff"], "venue": "Univ. Nac. Tucuma\u0301n. Revista A,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1946}, {"title": "End to end learning for self-driving cars", "author": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The double-edged sword of pedagogy: Modeling the effect of pedagogical contexts on preschoolers\u2019 exploratory play", "author": ["Elizabeth Bonawitz", "Patrick Shafto", "Hyowon Gweon", "Isabel Chang", "Sydney Katz", "Laura Schulz"], "venue": "In Proceedings of the 31th annual conference of the Cognitive Science Society,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery", "author": ["Elizabeth Bonawitz", "Patrick Shafto", "Hyowon Gweon", "Noah D Goodman", "Elizabeth Spelke", "Laura Schulz"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The cultural niche: Why social learning is essential for human adaptation", "author": ["Robert Boyd", "Peter J Richerson", "Joseph Henrich"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Cooperating with machines", "author": ["Jacob W Crandall", "Mayada Oudah", "Fatimah Ishowo-Oloko", "Sherief Abdallah", "Jean-Fran\u00e7ois Bonnefon", "Manuel Cebrian", "Azim Shariff", "Michael A Goodrich", "Iyad Rahwan"], "venue": "arXiv preprint arXiv:1703.06207,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Recursive teaching dimension, VC-dimension and sample compression", "author": ["Thorsten Doliwa", "Gaojian Fan", "Hans Ulrich Simon", "Sandra Zilles"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Epistemic trust and education: Effects of informant reliability on student learning of decimal concepts", "author": ["Kelley Durkin", "Patrick Shafto"], "venue": "Child Development,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Parameterizing developmental changes in epistemic trust", "author": ["Baxter S Eaves", "Patrick Shafto"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Prosocial development", "author": ["Nancy Eisenberg", "Richard A Fabes", "Tracy L Spinrad"], "venue": "Wiley Online Library,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "On the complexity of teaching", "author": ["Sally A Goldman", "Michael J Kearns"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Nonliteral understanding of number words", "author": ["Justine T Kao", "Jean Y Wu", "Leon Bergen", "Noah D Goodman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The teaching dimension of linear learners", "author": ["Ji Liu", "Xiaojin Zhu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Teaching games: Statistical sampling assumptions for learning in pedagogical situations", "author": ["Patrick Shafto", "Noah Goodman"], "venue": "In Proceedings of the 30th annual conference of the Cognitive Science Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Learning from others: The consequences of psychological reasoning for human learning", "author": ["Patrick Shafto", "Noah D Goodman", "Michael C Frank"], "venue": "Perspectives on Psychological Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A rational account of pedagogical reasoning: Teaching by, and learning from, examples", "author": ["Patrick Shafto", "Noah D Goodman", "Thomas L Griffiths"], "venue": "Cognitive Psychology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deep learning in medical image analysis", "author": ["Dinggang Shen", "Guorong Wu", "Heung-Il Suk"], "venue": "Annual Review of Biomedical Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Concerning nonnegative matrices and doubly stochastic matrices", "author": ["Richard Sinkhorn", "Paul Knopp"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1967}, {"title": "Deep learning for mortgage risk", "author": ["Justin Sirignano", "Apaar Sadhwani", "Kay Giesecke"], "venue": "SSRN: https://ssrn.com/abstract=2799443,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "The cultural origins of human cognition", "author": ["M. Tomasello"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Why we cooperate", "author": ["Michael Tomasello"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Origins of human communication", "author": ["Michael Tomasello"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A certain zero-sum two-person game equivalent to the optimal assignment problem", "author": ["John Von Neumann"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1953}, {"title": "Machine teaching: An inverse problem to machine learning and an approach toward optimal education", "author": ["Xiaojin Zhu"], "venue": "In AAAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Human learning is characterized by the cooperative transmission of data [21].", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "The cooperative selection of data, and learning from such data, plays a central role in theories of cognition [1], cognitive development [11], and cultural evolution [22].", "startOffset": 110, "endOffset": 113}, {"referenceID": 10, "context": "The cooperative selection of data, and learning from such data, plays a central role in theories of cognition [1], cognitive development [11], and cultural evolution [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "The cooperative selection of data, and learning from such data, plays a central role in theories of cognition [1], cognitive development [11], and cultural evolution [22].", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "Indeed, this cooperative inference is argued to be the feature that drives accumulation of knowledge over generations [23, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 5, "context": "Indeed, this cooperative inference is argued to be the feature that drives accumulation of knowledge over generations [23, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 24, "context": "A recent effort in this direction is machine teaching [25, 10], which formalizes how to translate and communicate model inferences using data.", "startOffset": 54, "endOffset": 62}, {"referenceID": 9, "context": "A recent effort in this direction is machine teaching [25, 10], which formalizes how to translate and communicate model inferences using data.", "startOffset": 54, "endOffset": 62}, {"referenceID": 11, "context": "We also use the measure to extend the teaching dimension [12, 26]\u2014a classical measure of communication efficiency\u2014from deterministic to probabilistic settings.", "startOffset": 57, "endOffset": 65}, {"referenceID": 14, "context": "In Sections 4, we introduce cooperative inference based on previous research in human social learning [15, 17], present a Cooperative Index by extend the Transmission index to the cooperative setting, and identify the condition that must be satisfied to achieve optimal communication.", "startOffset": 102, "endOffset": 110}, {"referenceID": 16, "context": "In Sections 4, we introduce cooperative inference based on previous research in human social learning [15, 17], present a Cooperative Index by extend the Transmission index to the cooperative setting, and identify the condition that must be satisfied to achieve optimal communication.", "startOffset": 102, "endOffset": 110}, {"referenceID": 0, "context": "Because H and D are both discrete, in matrix notation, we can form the row-stochastic learner\u2019s inference matrix, L \u2208 [0, 1]|D|\u00d7|H|, having elements PL(h|D), and the column-stochastic teacher\u2019s selection matrix, T \u2208 [0, 1]|D|\u00d7|H|, having elements PT(D|h).", "startOffset": 118, "endOffset": 124}, {"referenceID": 0, "context": "Because H and D are both discrete, in matrix notation, we can form the row-stochastic learner\u2019s inference matrix, L \u2208 [0, 1]|D|\u00d7|H|, having elements PL(h|D), and the column-stochastic teacher\u2019s selection matrix, T \u2208 [0, 1]|D|\u00d7|H|, having elements PT(D|h).", "startOffset": 216, "endOffset": 222}, {"referenceID": 0, "context": "In particular, it is possible to construct a pair of L and T that causes the limit of TI to bounce in the interval [0, 1].", "startOffset": 115, "endOffset": 121}, {"referenceID": 11, "context": "The Teaching Dimension is well-studied; it has formal connections with the VC Dimension [12] and has been analyzed for certain models in continuous concept space [14] and in cooperative settings [26, 8].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "The Teaching Dimension is well-studied; it has formal connections with the VC Dimension [12] and has been analyzed for certain models in continuous concept space [14] and in cooperative settings [26, 8].", "startOffset": 162, "endOffset": 166}, {"referenceID": 7, "context": "The Teaching Dimension is well-studied; it has formal connections with the VC Dimension [12] and has been analyzed for certain models in continuous concept space [14] and in cooperative settings [26, 8].", "startOffset": 195, "endOffset": 202}, {"referenceID": 7, "context": "The classical version of Average Teaching Dimension [8] is defined as follows: First, for any h \u2208 H, let", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "Let M \u2208 [0, 1]|D|\u00d7|H| be a matrix, where the element M i,j represents the probability that hi is consistent with Dj .", "startOffset": 8, "endOffset": 14}, {"referenceID": 8, "context": ", in pedagogy [9, 10] and conversations [13]).", "startOffset": 14, "endOffset": 21}, {"referenceID": 9, "context": ", in pedagogy [9, 10] and conversations [13]).", "startOffset": 14, "endOffset": 21}, {"referenceID": 12, "context": ", in pedagogy [9, 10] and conversations [13]).", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "In this section, we formalize cooperative inference, which captures this inter-dependency between the two processes of inference and selection [15, 17].", "startOffset": 143, "endOffset": 151}, {"referenceID": 16, "context": "In this section, we formalize cooperative inference, which captures this inter-dependency between the two processes of inference and selection [15, 17].", "startOffset": 143, "endOffset": 151}, {"referenceID": 14, "context": "The cooperative inference equations in (2) can be solved using fixed-point iteration [15, 17]: Define an initial likelihood, or common convention, PT(D|h) = P0(D|h), for the first evaluation of (2a).", "startOffset": 85, "endOffset": 93}, {"referenceID": 16, "context": "The cooperative inference equations in (2) can be solved using fixed-point iteration [15, 17]: Define an initial likelihood, or common convention, PT(D|h) = P0(D|h), for the first evaluation of (2a).", "startOffset": 85, "endOffset": 93}, {"referenceID": 0, "context": "This symmetry implies that the initial likelihood matrix, M \u2208 [0, 1]|D|\u00d7|H| with elements P0(D|h), can be an arbitrary non-negative matrix because it always gets appropriately normalized in the first iteration.", "startOffset": 62, "endOffset": 68}, {"referenceID": 18, "context": "Then, the iteration of (2) becomes the well-known Sinkhorn-Knopp algorithm, which provably converges under certain conditions by Sinkhorn\u2019s theorem [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "5 (A simpler version of Sinkhorn\u2019s theorem [19]).", "startOffset": 43, "endOffset": 47}, {"referenceID": 24, "context": "Now, we give two simple examples: The first demonstrates the fixed-point iteration of (2); the second compares cooperative inference with machine teaching [25].", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "We apply cooperative index to examples previously used in human teaching [4] and machine teaching [14], and compare the effectiveness of cooperative inference and other communication protocols.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "We apply cooperative index to examples previously used in human teaching [4] and machine teaching [14], and compare the effectiveness of cooperative inference and other communication protocols.", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "The first example [4, 5, 16] tested children\u2019s learning of hidden functions of a novel toy after observing data presented by an experimenter.", "startOffset": 18, "endOffset": 28}, {"referenceID": 4, "context": "The first example [4, 5, 16] tested children\u2019s learning of hidden functions of a novel toy after observing data presented by an experimenter.", "startOffset": 18, "endOffset": 28}, {"referenceID": 15, "context": "The first example [4, 5, 16] tested children\u2019s learning of hidden functions of a novel toy after observing data presented by an experimenter.", "startOffset": 18, "endOffset": 28}, {"referenceID": 3, "context": "Empirical results [4, 5] have shown that when the experimenter activated one function accidentally (accidental demonstration), children inferred the toy to afford at least one function, and they also explored possible additional functions.", "startOffset": 18, "endOffset": 24}, {"referenceID": 4, "context": "Empirical results [4, 5] have shown that when the experimenter activated one function accidentally (accidental demonstration), children inferred the toy to afford at least one function, and they also explored possible additional functions.", "startOffset": 18, "endOffset": 24}, {"referenceID": 15, "context": "In contrary, under pedagogical demonstration (P), both parties apply cooperative inference on the selection and interpretation of data [16], which leads to a perfectly effective communication protocol, TI(L ,T ) = CI(M = L) = 1.", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "The second example [14] considers a version-space learner who is trying to learn a threshold classifier h\u03b8, \u03b8 \u2208 {1 .", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Following [14], machine teaching chooses data that maximize the likelihood for the learner to infer the correct hypothesis:", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "Since elements of M that lie in a positive diagonal do not tend to zero during cooperative inference [19] (i.", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Cooperative inference is central to accounts of human learning, language, and cultural evolution, but has not been deeply investigated in machine learning (but see [7]).", "startOffset": 164, "endOffset": 167}, {"referenceID": 19, "context": ", banking [20], medicine [18], self-driving cars [3]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": ", banking [20], medicine [18], self-driving cars [3]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": ", banking [20], medicine [18], self-driving cars [3]).", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "Cooperative transmission of data fosters rapid accumulation of knowledge by efficiently combining experience across learners. Although well studied in human learning, there has been less attention to cooperative transmission of data in machine learning, and we consequently lack strong formal frameworks through which we may reason about the benefits and limitations of cooperative inference. We present such a framework. We introduce a novel index for measuring the effectiveness of probabilistic information transmission, and cooperative information transmission specifically. We relate our cooperative index to previous measures of teaching in deterministic settings. We prove conditions under which optimal cooperative inference can be achieved, including a representation theorem which constrains the form of inductive biases for learners optimized for cooperative inference. We conclude by demonstrating how these principles may inform the design of machine learning algorithms and discuss implications for human learning, machine learning, and human-machine learning systems.", "creator": "LaTeX with hyperref package"}}}