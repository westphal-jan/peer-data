{"id": "1705.09011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Principled Hybrids of Generative and Discriminative Domain Adaptation", "abstract": "We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from both source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model, DAuto. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.", "histories": [["v1", "Thu, 25 May 2017 01:02:16 GMT  (1121kb,D)", "http://arxiv.org/abs/1705.09011v1", null], ["v2", "Fri, 27 Oct 2017 19:29:31 GMT  (1122kb,D)", "http://arxiv.org/abs/1705.09011v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["han zhao", "zhenyao zhu", "junjie hu", "adam coates", "geoff gordon"], "accepted": false, "id": "1705.09011"}, "pdf": {"name": "1705.09011.pdf", "metadata": {"source": "CRF", "title": "Principled Hybrids of Generative and Discriminative Domain Adaptation", "authors": ["Han Zhao", "Zhenyao Zhu", "Junjie Hu", "Adam Coates", "Geoff Gordon", "HAN ZHAO", "ZHENYAO ZHU", "JUNJIE HU", "ADAM COATES", "GEOFF GORDON"], "emails": ["HAN.ZHAO@CS.CMU.EDU", "ZHENYAOZHU@BAIDU.COM", "JUNJIEH@CMU.EDU", "ADAMCOATES@BAIDU.COM", "GGORDON@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it will be able to find a solution that adapts to the needs of the people."}, {"heading": "2. Related Work", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to live than in another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "3. A Principled Hybrid Model for Domain Adaptation", "text": "Generative models provide a principal way to use both labeled data from the source domain and unlabeled data from both domains. In this section, we begin with a general probabilistic framework for domain adaptation using a principal hybrid of generative and discriminatory models. We then provide a concrete instantiation of our framework and show that it leads to popular reconstruction-based domain adaptation models. Our derivative can also be used to explain the prevalence and success of autoencoders in both domain adaptation and semi-supervised learning (Rasmus et al., 2015; Bousmalis et al., 2016)."}, {"heading": "3.1 A Probabilistic Framework for Domain Adaptation", "text": "The central point, which makes the above-mentioned framework still valid, lies in the possible richness of the generational process."}, {"heading": "3.2 An Instantiation using Kernel Density Estimation", "text": "Now we use our probabilistic framework and instantiate it with appropriate decisions of both boundary distributions and conditional distributions. On the one hand, we want to make as few assumptions as possible about the generation process of x; on the other hand, the model should be so rich that, although instances of source and target domains may have different distributions in Rd, the model contains flexible transformations under which the induced distributions in both domains are similar enough. Taking both considerations into account, we propose to apply non-parametric kernel density estimators (KDE) to the model p (x; \u03c6). Specifically, K (\u00b7) should be the chosen kernel and {xi} ni = 1 be an unlabeled set of instances. Our KDE for p (x; \u03c6) for the original kernel density estimator (KDE) is given by: p; x; x nw n = 1 K (x \u2212 g (f; xi; z \u00b2) for the common instances."}, {"heading": "3.3 Learning by Maximizing Joint Likelihood", "text": "One of the advantages that a generative model gives us is a principled way to use blank data from both domains. Let's (xi, yi) label the data from the source domain and (xj) nj = 1 as blank data from both domains. Instead of just maximizing the conditional probability using designated data, together we can maximize both the conditional probability and the marginal probability. (2) It is clear that the negative conditional log probability function is precisely the cross entropy loss between the true label yi and its predicted counterpart. For the negative marginal probability, when we choose a Gaussian kernel and plug in our KDE estimator, we have: a real problem between the true label yi and its predicted counterpart that we have."}, {"heading": "3.4 A Concrete Model (DAuto)", "text": "We use neural networks as flexible function approximation factors for our desired transformations f and g. Specifically, we use fully connected neural networks for parameterization f and g and softmax function for parameterization h. If we can simply change the softmax function to be an affine function as output, we assume that we use only a single-layer, fully connected network to represent f and g: f (x) = \u03c3 (W1x) and g (z) = \u03c3 (W1), where the W2z function is an affine function as a starting point for the discussion. W2 is an elementary non-linear activation function, e.g. the correction of linear unit. To facilitate notation, we also disregard the translation terms from the affine transformations mentioned above. Let us leave out h (z) = softmax layer (Whz) the softmax layer for calculating the conditional probability of class assignment."}, {"heading": "4. Experiments", "text": "We evaluate DAuto using synthetic experiments with MNIST and then compare it with state-of-the-art models, including mSDA, the Ladder Network and THEN. We report experimental results on the Amazon benchmark dataset and an extensive time series dataset for speech recognition."}, {"heading": "4.1 Datasets and Experimental Setup", "text": "We choose 4 digits for these tasks: 3, 7, 8 and 9, because 9 and 7 are similar in many handwritten images, while 3 and 9 (7 and 8) are completely different. It is clear that the adjustment of the domain is not always possible. We would like to verify that DAuto succeeds when two domains are close to each other, and also indicate a failure when domains are sufficiently different. To do this, we must recognize for each task the digital recognition of the digital world {3, 7, 9}, we try 1000 images from the training, of which 500 digit i and the other digits are not in {3, 7, 8}; we sample 1500 images from the original test group, of which 750 images digit i and the other digits are not in {3, 7, 9}. There are 16 pairs of experiments for each possible pair."}, {"heading": "4.2 Results and Analysis", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5. Conclusion", "text": "We propose a probabilistic framework that allows us to use blank instances from both domains in principle. We also show that the empirical success of autoencoders in semi-supervised learning and domain adaptation can be explained as maximizing the marginal log probabilities of functional data within our framework, using kernel density estimators to model boundary distributions, providing the first probabilistic justification for joint training with autoencoders in practice. The instantiated model, DAuto, can be successfully applied to domain adaptation problems and also has a natural extension to time series. Detailed experiments show that DAuto consistently outperforms its competitors when domain adaptation is possible."}, {"heading": "Acknowledgement", "text": "HZ would like to thank the speech recognition team of SVAIL, Baidu Research, in particular Vinay Rao, Jesse Engel and Sanjeev Satheesh, for their helpful discussions on the speech recognition experiment. HZ would also like to thank Zhuoyuan Chen for his valuable suggestions and encouragement during the internship. JH is partially supported by the joint participation in the Robotics Consortium sponsored by the US Army Research Laboratory under the Collaborative Technology Alliance Program, Cooperative Agreement W911NF-10-2-0016. The views and conclusions contained in this document are those of the authors and should not be interpreted to represent the official guidelines of the US Government's Army Research Laboratory, either explicitly or implicitly."}], "references": [{"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["M. Baktashmotlagh", "M.T. Harandi", "B.C. Lovell", "M. Salzmann"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Baktashmotlagh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baktashmotlagh et al\\.", "year": 2013}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 conference on empirical methods in natural language processing,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In ACL,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Domain separation networks", "author": ["K. Bousmalis", "G. Trigeorgis", "N. Silberman", "D. Krishnan", "D. Erhan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bousmalis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "arXiv preprint arXiv:1206.4683,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Transferring naive bayes classifiers for text classification", "author": ["W. Dai", "G.-R. Xue", "Q. Yang", "Y. Yu"], "venue": "In AAAI,", "citeRegEx": "Dai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2007}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Ganin and Lempitsky.,? \\Q2015\\E", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2015}, {"title": "Domain-adversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ganin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Detecting change in data streams", "author": ["D. Kifer", "S. Ben-David", "J. Gehrke"], "venue": "In Proceedings of the Thirtieth international conference on Very large data bases-Volume", "citeRegEx": "Kifer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kifer et al\\.", "year": 2004}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "In ICML,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": ", denoising autoencoders (Vincent et al., 2008) and contractive autoencoders (Rifai et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 16, "context": ", 2008) and contractive autoencoders (Rifai et al., 2011).", "startOffset": 37, "endOffset": 57}, {"referenceID": 15, "context": "Our interpretation may also be used to explain the recent success of autoencoders in semi-supervised learning (Rasmus et al., 2015).", "startOffset": 110, "endOffset": 131}, {"referenceID": 10, "context": "Recently, due to the availability of rich data and powerful computational resources, non-linear representations and hypothesis classes for domain adaptation have been increasingly explored (Glorot et al., 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 0, "context": "Recently, due to the availability of rich data and powerful computational resources, non-linear representations and hypothesis classes for domain adaptation have been increasingly explored (Glorot et al., 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 6, "context": "Recently, due to the availability of rich data and powerful computational resources, non-linear representations and hypothesis classes for domain adaptation have been increasingly explored (Glorot et al., 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 9, "context": "Recently, due to the availability of rich data and powerful computational resources, non-linear representations and hypothesis classes for domain adaptation have been increasingly explored (Glorot et al., 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 10, "context": "This line of works focuses on building common and robust feature representations among multiple domains using either supervised neural networks (Glorot et al., 2011), or unsupervised pretraining using denoising auto-encoders (Vincent et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 9, "context": "Other works focus on learning feature transformations such that the feature distributions in the source and target domains are close to each other (Ben-David et al., 2007, 2010; Ajakan et al., 2014; Ganin et al., 2016).", "startOffset": 147, "endOffset": 218}, {"referenceID": 18, "context": "In practice it was observed that unsupervised pretraining using stacked denoising auto-encoders (mSDA) (Vincent et al., 2008; Chen et al., 2012) often improves the generalization accuracy (Ganin et al.", "startOffset": 103, "endOffset": 144}, {"referenceID": 6, "context": "In practice it was observed that unsupervised pretraining using stacked denoising auto-encoders (mSDA) (Vincent et al., 2008; Chen et al., 2012) often improves the generalization accuracy (Ganin et al.", "startOffset": 103, "endOffset": 144}, {"referenceID": 9, "context": ", 2012) often improves the generalization accuracy (Ganin et al., 2016).", "startOffset": 51, "endOffset": 71}, {"referenceID": 9, "context": "Domain adversarial neural networks (DANN) is a discriminative model to learn domain-invariant features (Ganin et al., 2016).", "startOffset": 103, "endOffset": 123}, {"referenceID": 1, "context": "DANN also enjoys a nice theoretical justification to learn a feature map to decrease the A-distance measure (Ben-David et al., 2007) between source and target domains.", "startOffset": 108, "endOffset": 132}, {"referenceID": 12, "context": "(2015) propose similar models where the maximum mean discrepancy (MMD) (Gretton et al., 2012) between two domains are minimized.", "startOffset": 71, "endOffset": 93}, {"referenceID": 7, "context": "Finally, domain adaptation can also be viewed as a semi-supervised learning problem by ignoring the domain shift, where source instances are treated as labeled data and target instances are unlabeled data (Dai et al., 2007; Rasmus et al., 2015).", "startOffset": 205, "endOffset": 244}, {"referenceID": 15, "context": "Finally, domain adaptation can also be viewed as a semi-supervised learning problem by ignoring the domain shift, where source instances are treated as labeled data and target instances are unlabeled data (Dai et al., 2007; Rasmus et al., 2015).", "startOffset": 205, "endOffset": 244}, {"referenceID": 0, "context": ", 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016). This line of works focuses on building common and robust feature representations among multiple domains using either supervised neural networks (Glorot et al., 2011), or unsupervised pretraining using denoising auto-encoders (Vincent et al., 2008, 2010). Other works focus on learning feature transformations such that the feature distributions in the source and target domains are close to each other (Ben-David et al., 2007, 2010; Ajakan et al., 2014; Ganin et al., 2016). In practice it was observed that unsupervised pretraining using stacked denoising auto-encoders (mSDA) (Vincent et al., 2008; Chen et al., 2012) often improves the generalization accuracy (Ganin et al., 2016). One of the limitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings. On the other hand, it is also not clear how to extend mSDA so that it can also be applied for time-series modeling. Domain adversarial neural networks (DANN) is a discriminative model to learn domain-invariant features (Ganin et al., 2016). It can be formulated as a minimax problem where the feature transformation component tries to learn a representation to confuse a following domain classification component. DANN also enjoys a nice theoretical justification to learn a feature map to decrease the A-distance measure (Ben-David et al., 2007) between source and target domains. Other distance measures between distributions can also be applied. Tzeng et al. (2014) and Long et al.", "startOffset": 8, "endOffset": 1661}, {"referenceID": 0, "context": ", 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016). This line of works focuses on building common and robust feature representations among multiple domains using either supervised neural networks (Glorot et al., 2011), or unsupervised pretraining using denoising auto-encoders (Vincent et al., 2008, 2010). Other works focus on learning feature transformations such that the feature distributions in the source and target domains are close to each other (Ben-David et al., 2007, 2010; Ajakan et al., 2014; Ganin et al., 2016). In practice it was observed that unsupervised pretraining using stacked denoising auto-encoders (mSDA) (Vincent et al., 2008; Chen et al., 2012) often improves the generalization accuracy (Ganin et al., 2016). One of the limitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings. On the other hand, it is also not clear how to extend mSDA so that it can also be applied for time-series modeling. Domain adversarial neural networks (DANN) is a discriminative model to learn domain-invariant features (Ganin et al., 2016). It can be formulated as a minimax problem where the feature transformation component tries to learn a representation to confuse a following domain classification component. DANN also enjoys a nice theoretical justification to learn a feature map to decrease the A-distance measure (Ben-David et al., 2007) between source and target domains. Other distance measures between distributions can also be applied. Tzeng et al. (2014) and Long et al. (2015) propose similar models where the maximum mean discrepancy (MMD) (Gretton et al.", "startOffset": 8, "endOffset": 1684}, {"referenceID": 0, "context": ", 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016). This line of works focuses on building common and robust feature representations among multiple domains using either supervised neural networks (Glorot et al., 2011), or unsupervised pretraining using denoising auto-encoders (Vincent et al., 2008, 2010). Other works focus on learning feature transformations such that the feature distributions in the source and target domains are close to each other (Ben-David et al., 2007, 2010; Ajakan et al., 2014; Ganin et al., 2016). In practice it was observed that unsupervised pretraining using stacked denoising auto-encoders (mSDA) (Vincent et al., 2008; Chen et al., 2012) often improves the generalization accuracy (Ganin et al., 2016). One of the limitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings. On the other hand, it is also not clear how to extend mSDA so that it can also be applied for time-series modeling. Domain adversarial neural networks (DANN) is a discriminative model to learn domain-invariant features (Ganin et al., 2016). It can be formulated as a minimax problem where the feature transformation component tries to learn a representation to confuse a following domain classification component. DANN also enjoys a nice theoretical justification to learn a feature map to decrease the A-distance measure (Ben-David et al., 2007) between source and target domains. Other distance measures between distributions can also be applied. Tzeng et al. (2014) and Long et al. (2015) propose similar models where the maximum mean discrepancy (MMD) (Gretton et al., 2012) between two domains are minimized. Very recently, Bousmalis et al. (2016) propose a model where orthogonal representations that are shared between domains and unique to each domain are learned simultaneously.", "startOffset": 8, "endOffset": 1845}, {"referenceID": 6, "context": "DAuto improves over mSDA (Chen et al., 2012) when the dimension of feature vectors is high and as a result the covariance matrix cannot be explicitly formed.", "startOffset": 25, "endOffset": 44}, {"referenceID": 15, "context": "Our derivation can also be used to explain the prevalence and success of autoencoders in both domain adaptation and semi-supervised learning (Rasmus et al., 2015; Bousmalis et al., 2016).", "startOffset": 141, "endOffset": 186}, {"referenceID": 5, "context": "Our derivation can also be used to explain the prevalence and success of autoencoders in both domain adaptation and semi-supervised learning (Rasmus et al., 2015; Bousmalis et al., 2016).", "startOffset": 141, "endOffset": 186}, {"referenceID": 17, "context": "In fact, this is a necessary and implicit assumption that underlies the success of recent regularization based discriminative models (Tzeng et al., 2014; Long et al., 2015; Ganin et al., 2016).", "startOffset": 133, "endOffset": 192}, {"referenceID": 14, "context": "In fact, this is a necessary and implicit assumption that underlies the success of recent regularization based discriminative models (Tzeng et al., 2014; Long et al., 2015; Ganin et al., 2016).", "startOffset": 133, "endOffset": 192}, {"referenceID": 9, "context": "In fact, this is a necessary and implicit assumption that underlies the success of recent regularization based discriminative models (Tzeng et al., 2014; Long et al., 2015; Ganin et al., 2016).", "startOffset": 133, "endOffset": 192}, {"referenceID": 15, "context": "This interpretation may also be used to explain the practical success of autoencoders in semi-supervised learning (Rasmus et al., 2015).", "startOffset": 114, "endOffset": 135}, {"referenceID": 13, "context": "One popular and effective choice is the A-distance introduced by (Kifer et al., 2004; Ben-David et al., 2007, 2010).", "startOffset": 65, "endOffset": 115}, {"referenceID": 1, "context": "It can be shown that the A-distance can be approximated by the binary classification error of the domain classifier that discriminates instances from the source or the target domain (Ben-David et al., 2007).", "startOffset": 182, "endOffset": 206}, {"referenceID": 4, "context": "Sentiment Analysis The Amazon dataset consists of reviews of products on Amazon (Blitzer et al., 2007).", "startOffset": 80, "endOffset": 102}, {"referenceID": 6, "context": "purpose of sentiment analysis (Blitzer et al., 2006, 2007; Chen et al., 2012; Ajakan et al., 2014; Ganin and Lempitsky, 2015).", "startOffset": 30, "endOffset": 125}, {"referenceID": 8, "context": "purpose of sentiment analysis (Blitzer et al., 2006, 2007; Chen et al., 2012; Ajakan et al., 2014; Ganin and Lempitsky, 2015).", "startOffset": 30, "endOffset": 125}, {"referenceID": 11, "context": "The network is trained end-to-end using the connectionist temporal classification (CTC) loss (Graves et al., 2006), which is the negative log-likelihood of training utterances.", "startOffset": 93, "endOffset": 114}, {"referenceID": 6, "context": "The constructed representations from mSDA are used to train a SVM classifier as suggestd in the original paper (Chen et al., 2012).", "startOffset": 111, "endOffset": 130}, {"referenceID": 15, "context": "The Ladder network (Rasmus et al., 2015) is a novel structure aiming for semi-supervised learning.", "startOffset": 19, "endOffset": 40}], "year": 2017, "abstractText": "We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from both source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model, DAuto. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.", "creator": "LaTeX with hyperref package"}}}