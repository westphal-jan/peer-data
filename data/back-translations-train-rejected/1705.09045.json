{"id": "1705.09045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Cross-Domain Perceptual Reward Functions", "abstract": "In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agents environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agents. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agents state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning.", "histories": [["v1", "Thu, 25 May 2017 04:54:36 GMT  (1768kb,D)", "https://arxiv.org/abs/1705.09045v1", "A shorter version of this paper was accepted to RLDM (this http URL)"], ["v2", "Wed, 7 Jun 2017 15:44:37 GMT  (1732kb,D)", "http://arxiv.org/abs/1705.09045v2", "A shorter version of this paper was accepted to RLDM (this http URL)"], ["v3", "Tue, 25 Jul 2017 15:40:28 GMT  (1732kb,D)", "http://arxiv.org/abs/1705.09045v3", "A shorter version of this paper was accepted to RLDM (this http URL)"]], "COMMENTS": "A shorter version of this paper was accepted to RLDM (this http URL)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ashley d edwards", "srijan sood", "charles l isbell jr"], "accepted": false, "id": "1705.09045"}, "pdf": {"name": "1705.09045.pdf", "metadata": {"source": "CRF", "title": "Cross-Domain Perceptual Reward Functions", "authors": ["Ashley D. Edwards", "Srijan Sood", "Charles L. Isbell"], "emails": ["srijansood}@gatech.edu,", "isbell@cc.gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that these two are very complex matters, and that this is a very complex and complex matter."}, {"heading": "2 Background", "text": "Reinforcement Learning (RL) problems are traditionally defined by a Markov decision-making process < S, A, P, R > [35]. The sentence S consists of states that correspond to the surroundings of the agent. An agent takes actions that correspond to a) A and receives rewards that depend on the current state. Generally, the task of an agent is defined solely by the rewards set forth in the MDP. Each time the task changes, the reward function needs to be redefined. The transition function P (s, a, s \") represents the likelihood that the agent will end up in the states after taking measures a. An (s, a) policy represents the likelihood that action will be taken in the states. We are typically interested in strategies that maximize the long-term expected reward over time. In our problem, we do not receive rewards as inputs for the MDP. Rather, we aim to learn a reward function that corresponds to the image of an agent that has applied to the image of one of the other, the image of an agent, and the similarity between the image of one of the environment and the other. < S, A, P, P, P, P, R > [35] The sentence S consists of states that correspond to the environment of the agent's surroundings."}, {"heading": "3 Related work", "text": "Throughout the literature, we have found that goals are often defined in relation to the agent's environment. Our own approach introduces a general mechanism for setting goals on our own terms. We discuss two general techniques for representing goals by explicitly instantiating them either through engineering or direct feedback, or by expressing them implicitly through demonstrations."}, {"heading": "3.1 Explicit goal instantiations", "text": "There is a wealth of literature describing such task-specific rewards, from those that indicate that an agent has reached a desired location [34] to more complex ones that can be used as feedback for training neural networks. Rewards are largely where we instill knowledge, and are therefore inherently specialized, and even when a reward is used for multiple problems in the same domain, it often remains a function of the configurations of each individual agent in the environment."}, {"heading": "3.2 Implicit goal instantiations", "text": "Learning from the demonstration (LfD) is often used when setting a goal, or when a problem is too difficult to solve on its own. Our approach differs from LfD's in that we do not aim to solve a task only what the task is. Nevertheless, we share some motivations with the work we are discussing now. A similar approach aims to derive a reward function from expert demonstrations [1]. This approach typically requires multiple demonstrations for a single task. We also want to learn a reward function in our approach, but we use a single sample to represent the goal. A similar approach aims to learn a policy directly from demonstrations."}, {"heading": "4 Approach", "text": "We focus on solving RL problems consisting of MDPs with visual states and unspecified reward functions. We are interested in problems where an agent has a task with multiple potential target configurations. An agent's general task may be to build furniture, but his goal might be to build a chair or a table or a bank. Each goal is a specific instance of a task. Typically, an agent's reward function is defined by a reward function. This remains true for our approach, but now we need the reward to be both a function of the state and a goal. Therefore, this general reward function remains fixed when the goal changes - only the inputs vary. We will now describe how we develop such rewards or CDPRs. We aim to specify goals using images from alternative environments."}, {"heading": "5 Experiments and results", "text": "This reward is represented by the negative Euclidean distance between the state of the agent and the domain's internal objective representation. We will now describe the tasks with which we evaluate our approach. We will now describe a metric for measuring the accuracy of the reward functions. While the evaluation with the RL may be sufficient, it may be impossible to iterate over many target instances. Therefore, we will introduce the tasks with which we evaluate our approach. We will now describe a metric for measuring the accuracy of the reward functions. While the evaluation with the RL may be sufficient, it may be impossible to iterate over many target instances. Therefore, we will introduce the Goal Retrieval Accuracy (GRA) metric motivated by image evaluation approaches. Given a number of states that were not seen during the training, we can use PRE metrics to measure multiple target accuracy."}, {"heading": "5.1 Maze task", "text": "The first domain on which we evaluate our approach is a labyrinth, as shown in Figure 2a. We randomly generated labyrinths consisting of 1-3 green, blue, or yellow spaces. To determine the desired space using standard RL approaches, we would need to know its coordinates, and its task is to navigate through the labyrinth. Targets determine the specific space the agent needs to reach. To determine the desired space using standard RL approaches, we would need to know its coordinates. Instead, we use two cross-domain target representations: a hand shape of sign language that indicates the first letter of the desired room color, and a spectrogram of a spoken command specifying \"Go into the [color] space.\" We use a dataset of sign language gestures from [4] for the hand shape specification. For the language specification, we drew a sample of the spoken command for each colored space and then a spectrogram of a command that is maximized to produce multiple commands."}, {"heading": "5.2 Music playing task", "text": "The agent can play 7 keys on a single scale of a-g, and each key can be a whole or half note. If the agent selects a note, an e.wav file will be generated, which will be converted into a spectrogram representing its state. The task is for the agent to play a song, and the targets will determine which song to play. To get the samples of both specifications, we would need to know how to play the notes on the piano. Instead, we will again use two cross-domain objectives: a spectrogram of the desired song to be played on a synthetic guitar, and the score sheet of the song. To get the samples of both specifications, we randomly generated notes and automatically created the notes and spectrograms for the guitar. We set the reward for the standard approach as the total percentage of notes that the agent played correctly."}, {"heading": "5.3 Results", "text": "We will now discuss the results for training the CDPRs for each task and carrying out RL with the rewards learned."}, {"heading": "5.4 Maze results", "text": "We first give qualitative results for the Maze task. Figure 3 shows a thermal camera for the rewards that could be generated from any location by the agent. We do not show results for the standard reward, as this would only give rewards of 1 in the right room and 0 otherwise. It is clear that these rewards were not learned directly to get the right rewards, as the reward is given when the agent is in the right room. We report on the rewards for the coding, as these rewards are not learned directly. It is clear that the right room receives higher rewards than the rest, as the reward will be 0 if the state and the goal are the same."}, {"heading": "5.5 Music results", "text": "We do not have any qualitative results for this task as the rewards are more difficult to visualize. The first result we report on is the GRA shown in Table 1. The specifications for guitar and sheet music have also reached a high level of accuracy, but the results are clearly not as good as the labyrinth results. Just as with classification problems, we need to determine how many errors we are willing to tolerate to avoid solutions to each problem. Nevertheless, future work will aim to develop more sophisticated architectures that produce higher GRAs. We will use the RL results to further investigate the accuracy of CDPRs, as shown in Figure 5. The desired objectives were for the agent to play the songs shown in Figure 2b: {(\"a,\" W), (\"b,\" W), (\"b,\" H), (\"b,\" H), (\"c, H), (\" H), (\"c,\" H), \"(\" c, \"both,\" (\"c),\" (both), \"W,\" (both)."}, {"heading": "6 Discussion and conclusion", "text": "Our results suggest that we can accurately learn the general reward functions specified by CDPRs. As discussed in the corresponding working section, there is clear evidence of dynamics in this area of research. Further research will include improving the accuracy of CDPRs and their performance in RL tasks. In addition, our initial work focused on determining that CDPRs can effectively be used for cross-domain objectives. Future work will aim to use real patterns of goals that may be achieved through crowdsourcing. Therefore, it will be necessary to improve the sample complexity required to learn the CDPRs that we did not examine in this study. In summary, we have shown how objectives can be defined in alternative environments than the agents."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, page 1. ACM,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment", "author": ["H.B. Ammar", "E. Eaton", "P. Ruvolo", "M.E. Taylor"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, 57(5):469\u2013483,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A new 2d static hand gesture colour image dataset for asl gestures", "author": ["A.L.C. Barczak", "N.H. Reyes", "M. Abastillas", "A. Piccio", "T. Susnjak"], "venue": "Research Letters in the Information and Mathematical Sciences, 15:12\u201320,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain separation networks", "author": ["K. Bousmalis", "G. Trigeorgis", "N. Silberman", "D. Krishnan", "D. Erhan"], "venue": "CoRR, abs/1608.06019,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 539\u2013546. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["C. Devin", "A. Gupta", "T. Darrell", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1609.07088,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Perceptual reward functions", "author": ["A. Edwards", "C. Isbell", "A. Takanishi"], "venue": "arXiv preprint arXiv:1608.03824,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["C. Finn", "X.Y. Tan", "Y. Duan", "T. Darrell", "S. Levine", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512\u2013519. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalizing skills with semi-supervised reinforcement learning", "author": ["C. Finn", "T. Yu", "J. Fu", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1612.00429,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Policy shaping: Integrating human feedback with reinforcement learning", "author": ["S. Griffith", "K. Subramanian", "J. Scholz", "C. Isbell", "A.L. Thomaz"], "venue": "Advances in Neural Information Processing Systems, pages 2625\u20132633,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["D. Harwath", "A. Torralba", "J. Glass"], "venue": "Advances in Neural Information Processing Systems, pages 1858\u20131866,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A tutorial on visual servo control", "author": ["S. Hutchinson", "G.D. Hager", "P. Corke"], "venue": "Robotics and Automation, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A social reinforcement learning agent", "author": ["C. Isbell", "C.R. Shelton", "M. Kearns", "S. Singh", "P. Stone"], "venue": "Proceedings of the fifth international conference on Autonomous agents, pages 377\u2013384. ACM,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["P. Isola", "J. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CoRR, abs/1611.07004,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Beating atari with natural language guided reinforcement learning", "author": ["R. Kaplan", "C. Sauer", "A. Sosa"], "venue": "arXiv preprint arXiv:1704.05539,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning task goals interactively with visual demonstrations", "author": ["J. Kirk", "A. Mininger", "J. Laird"], "venue": "Biologically Inspired Cognitive Architectures, 18:1\u20138,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance", "author": ["W.B. Knox", "P. Stone"], "venue": "Artificial Intelligence, 225:24\u201350,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "CoRR, abs/1512.09300,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Robot gains social intelligence through multimodal deep reinforcement learning", "author": ["A.H. Qureshi", "Y. Nakamura", "Y. Yoshikawa", "H. Ishiguro"], "venue": "Humanoid Robots (Humanoids), 2016 IEEE-RAS 16th International Conference on, pages 745\u2013751. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "cad)2rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "arXiv preprint arXiv:1611.04201,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Is imitation learning the route to humanoid robots", "author": ["S. Schaal"], "venue": "Trends in cognitive sciences,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Universal value function approximators", "author": ["T. Schaul", "D. Horgan", "K. Gregor", "D. Silver"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1312\u20131320,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Where do rewards come from", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto"], "venue": "Proceedings of the annual conference of the cognitive science society, pages 2601\u20132606,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting navigation states from a hand-drawn map", "author": ["M. Skubic", "P. Matsakis", "B. Forrester", "G. Chronis"], "venue": "Robotics and Automation, 2001. Proceedings 2001 ICRA. IEEE International Conference on, volume 1, pages 259\u2013264. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Third-person imitation learning", "author": ["B.C. Stadie", "P. Abbeel", "I. Sutskever"], "venue": "arXiv preprint arXiv:1703.01703,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards the grounding of abstract words: a neural network model for cognitive robots", "author": ["F. Stramandinoli", "A. Cangelosi", "D. Marocco"], "venue": "Neural Networks (IJCNN), The 2011 International Joint Conference on, pages 467\u2013474. IEEE,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "Proceedings of the seventh international conference on machine learning, pages 216\u2013224,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1990}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge Univ Press,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research, 10(Jul):1633\u20131685,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "Artificial Intelligence, 172(6-7):716\u2013737,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.06907,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "P. Abbeel", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "Workshop on the Algorithmic Foundations of Robotics (WAFR),", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Face valuing: Training user interfaces with facial expressions and reinforcement learning", "author": ["V. Veeriah", "P.M. Pilarski", "R.S. Sutton"], "venue": "arXiv preprint arXiv:1606.02807,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "arXiv preprint arXiv:1511.03791,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["B. Zoph", "Q.V. Le"], "venue": "arXiv preprint arXiv:1611.01578,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Learning from demonstration approaches, such as inverse reinforcement learning and imitation learning [3], aim to alleviate the engineering effort by utilizing examples of how a task should be solved.", "startOffset": 102, "endOffset": 105}, {"referenceID": 33, "context": "Reinforcement Learning (RL) problems are traditionally specified by a Markov Decision Process \u3008S,A, P,R\u3009 [35].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 32, "context": "There is a wealth of literature that describes such task-specific rewards, from those that indicate when an agent has reached a desired location [34], to more complex ones that can be used as feedback for training neural networks [43].", "startOffset": 145, "endOffset": 149}, {"referenceID": 41, "context": "There is a wealth of literature that describes such task-specific rewards, from those that indicate when an agent has reached a desired location [34], to more complex ones that can be used as feedback for training neural networks [43].", "startOffset": 230, "endOffset": 234}, {"referenceID": 13, "context": "This approach is similar to visuo-servoing, which uses target images to guide robots to some goal [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "A more recent RL approach learns the relevant features from the agent\u2019s state space [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "dissimilar motions of humans and a simulated robot into a similar structure, but this correspondence is hand-specified [10].", "startOffset": 119, "endOffset": 123}, {"referenceID": 28, "context": "Early work aimed to find rewards that generalized across multiple environments [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "[12] described an approach that allowed generalizing skills learned from \u201clabeled\u201d MDPs with known reward functions to similar \u201cunlabeled\u201d MDPs with unspecified reward functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Finally, we have seen approaches that aim learn a value function that generalizes over states and goals, but these approaches also assume that the two representations share a similar structure [29, 42].", "startOffset": 193, "endOffset": 201}, {"referenceID": 40, "context": "Finally, we have seen approaches that aim learn a value function that generalizes over states and goals, but these approaches also assume that the two representations share a similar structure [29, 42].", "startOffset": 193, "endOffset": 201}, {"referenceID": 24, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 36, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 37, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 39, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "For example, some approaches allow humans to provide rewards to agents in real-time [17, 22, 37].", "startOffset": 84, "endOffset": 96}, {"referenceID": 20, "context": "For example, some approaches allow humans to provide rewards to agents in real-time [17, 22, 37].", "startOffset": 84, "endOffset": 96}, {"referenceID": 35, "context": "For example, some approaches allow humans to provide rewards to agents in real-time [17, 22, 37].", "startOffset": 84, "endOffset": 96}, {"referenceID": 11, "context": "Alternatively, policy shaping takes a more hands-off approach, and provides policy \u201cadvice\u201d, as opposed to explicit rewards [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "Another approach aims to interactively teach an agent goals [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "[11] introduced an approach that allowed humans to select the pixel locations for where a robot should move an object.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Inverse RL aims to infer a reward function from expert demonstrations [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 26, "context": "A similar approach is imitation learning, which aims to learn a policy directly from demonstrations [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "When the observations are dissimilar, a correspondence problem must be solved [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 30, "context": "For example, one approach used deep learning to train an agent that had a first-person perspective through third-person samples [32].", "startOffset": 128, "endOffset": 132}, {"referenceID": 34, "context": "Another somewhat related RL approach is transfer learning, which aims to transfer learned behaviors from one domain to another [36], for example by initializing the parameters of policies in unsolved tasks [2], or by transferring skills across untrained robots [9].", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "Another somewhat related RL approach is transfer learning, which aims to transfer learned behaviors from one domain to another [36], for example by initializing the parameters of policies in unsolved tasks [2], or by transferring skills across untrained robots [9].", "startOffset": 206, "endOffset": 209}, {"referenceID": 7, "context": "Another somewhat related RL approach is transfer learning, which aims to transfer learned behaviors from one domain to another [36], for example by initializing the parameters of policies in unsolved tasks [2], or by transferring skills across untrained robots [9].", "startOffset": 261, "endOffset": 264}, {"referenceID": 29, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 108, "endOffset": 115}, {"referenceID": 31, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 175, "endOffset": 179}, {"referenceID": 23, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 223, "endOffset": 227}, {"referenceID": 38, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 281, "endOffset": 285}, {"referenceID": 17, "context": "Recently, an approach learned correspondences between written instructions and visual states in Atari games [19].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "Our work is inspired by an approach that uses deep learning to find cross-domain similarities between images and their corresponding spoken captions, which are represented through spectrograms [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "We used Deep RL to solve the tasks with the architecture described in the paper [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 3, "context": "\u201d We use a dataset of sign language gestures from [4] for the handshape specification.", "startOffset": 50, "endOffset": 53}, {"referenceID": 14, "context": "Conv1, Conv2, and FC1 are each followed by batch normalization [16] and then ELU [8].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Conv1, Conv2, and FC1 are each followed by batch normalization [16] and then ELU [8].", "startOffset": 81, "endOffset": 84}, {"referenceID": 18, "context": "In order to train the CDPRs for this task, we used an Adam optimizer [20] with an initial learning rate of 10\u22124.", "startOffset": 69, "endOffset": 73}], "year": 2017, "abstractText": "In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agent\u2019s environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agent\u2019s. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agent\u2019s state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning.", "creator": "LaTeX with hyperref package"}}}