{"id": "1705.09054", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment", "abstract": "Recognizing textual entailment is a fundamental task in a variety of text mining or natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones.", "histories": [["v1", "Thu, 25 May 2017 05:45:42 GMT  (14kb)", "http://arxiv.org/abs/1705.09054v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhipeng xie", "junfeng hu"], "accepted": false, "id": "1705.09054"}, "pdf": {"name": "1705.09054.pdf", "metadata": {"source": "CRF", "title": "Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment", "authors": ["Zhipeng Xie", "Junfeng Hu"], "emails": ["15210240075}@fudan.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.09 054v 1 [cs.C L] 25 May 201 7Keywords: Textual Entailment, Recursive Neural Networks, LSTM"}, {"heading": "1 Introduction", "text": "In fact, it is the case that you are able to live in a country in which you are able to govern a country, in which you are able to govern a country, and in which you are able to govern a country, in which you are able to govern a country, and in which you are able to govern a country, in which you are able to govern a country, in which you are able to govern a country."}, {"heading": "2 Related Work", "text": "In this section, some neural models responsible for the recognition of textual relationships are presented; the first and simplest neural model for RTE was proposed by Bowman et al. [4] in 2015, which uses separate LSTMs [15] to encrypt the premise and hypothesis of fixed vectors and then mold their concatenation into a multi-layered form (MLP) or classify other classifications; it learns the set of premise and hypothesis independently and does not retract its interaction; this first neural model suffers from the fact that the hypothesis and premise itself are modeled, and the information cannot flow back and forth between them."}, {"heading": "3 Base Method", "text": "Recognition of textual sequences is about the relationship between two sequences - premise X and hypothesis Y. The commonly used encoder decoder architecture processes the second sequence, which is conditioned by the first. In this thesis, we connect the hypothesis with the premise at word level, where each word of the hypothesis corresponds to and is paired with its most similar word in the premise. It leads to a simple basic method called MaxCosineLSTM, which consists of three steps, as shown in Figure 1: - Step 1: (Word Matching). Such a match strategy can be regarded as a conditional representation of hypothesis Y (referred to as \u03b3 (yt) in X, where the similarity between two words is measured as a cosmic similarity between their embeddings."}, {"heading": "3.1 Word Matching with Cosine Similarity", "text": "To assess whether a hypothesis Y can be derived from a given premise X, it is important to know whether each word in Y expresses a similar meaning to a word in premise X. The distribution hypothesis proposed by Harris [12] provides a guiding principle that says that words in similar contexts have similar meanings. (This principle has led to a variety of distribution models (DSM) that use multidimensional vectors as word-sense representations. (17) Latent semantic analysis [17] is a representative method of this kind that can apply truncated singular value decomposition to a matrix of word-context-co-occurrence matrix."}, {"heading": "3.2 Sequence Modeling with LSTM", "text": "Next, we want to convert the sequence Z into a vector, since the representation of the hypothesis depends on the premise. Of course, recurring neural networks are suitable for modelling sequences of variable length, which can recursively recompose any (2d) -dimensional vector zt with its previous memory. Traditional recurring neural networks often suffer from the problem of disappearing and exploding gradients [14] [3], which makes it difficult to train models. In this paper, we adopt the model of long short-term memory (LSTM) [15], which partially solves the problem by activating the gated activation function. LSTM maintains a memory state c through all the time steps in order to store the information over long periods of time. Specifically, the concatenation of two word embedding, zt, z, should be stored in the LSTM as input."}, {"heading": "3.3 Decision Making with Softmax Layer", "text": "The final output, hm, generated by the LSTM on the extended representation Z of the hypothesis Y under premise X, is fed into a softmax layer, which performs the following two steps. In the first step, hm undergoes a linear transformation to obtain a three-dimensional vector p: p = Wshm + b s (10), with the weight matrix Ws-R3 \u00b7 k and the bias vector bs-R3 being the parameters of the softmax layer. The p = (p1, p2, p3 is then transformed by a nonlinear softmax function, resulting in a probability prediction (t-1, t-2, t-3) of the three possible designations (Entailment = 1, contradiction = 2 or neutral = 3): t-i = exp pi-3 = exp-pj (11) During the training phase, the cross-entropy function is used as a defect test."}, {"heading": "4 Improvements over the Base Method", "text": "To achieve better prediction, three optional techniques are applied to the basic method described above: - Multiple word-embedding libraries that improve the vector representation of words; - Biway-LSTM integration that improves the representation of relationships between text pairs; - Ensemble based on averaging models that provide more accurate predictions."}, {"heading": "4.1 Mutliple Embeddings", "text": "Word2vec and Glove are two popular programs for learning word embedding from text corpus. We use Dw2v and Dglove to mark their induced word embedding libraries. For each word x, we can represent it as a vector D (x) as a concatenation of its embedding from Dw2v or from Dglove, i.e. D (x) = [Dw2v (x) Dglove (x)]. Or equivalent, a new embedding matrix D (2d) \u00d7 | V | is constructed by concatenating Dw2v with Dglove. The semantic similarity between words from the hypothesis and the premise is calculated in relation to this new embedding matrix. The aim of this technique is to integrate the potentially complementary information provided by different word embedding libraries."}, {"heading": "4.2 Biway-LSTM Integration", "text": "To justify this statement, let us consider the following simple example: Let us consider X (1) = \"John failed the test,\" X (2) = \"John passed the test,\" and Y = \"John passed the test.\" It is clear that X (2) contains Y, but X (1) contradicts Y. However, the extended representation of Y conditioned by X (1) is the same as that of Y conditioned by X (2). Therefore, we cannot distinguish these two situations only on the basis of the improved representation of Y. To make a tool, we extend our base model to the dual-track architecture shown in Figure 2. Two LSTMs are used to represent the hypothesis separately and to fulfill the final hypothesis."}, {"heading": "4.3 Ensemble by Model Averaging", "text": "The combination of several models usually results in better performance. It is expected that the component models are varied and precise to produce a high quality interaction. In this essay, all components are homogeneous, i.e. they are all induced by our basic method (optionally extended to bi-embedding integration and / or biway integration).The variety of these components results from the random initialization of the model parameters with different random seeds. The predictions from these component models are averaged to make the final decision."}, {"heading": "5 Experiments", "text": "In the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "6 Conclusion and Outlook", "text": "We proposed a simple neural method to determine the relationship between a hypothesis and a premise, based on a semantic agreement from the hypothesis to the premise (or vice versa), then used the LSTM to perform sequence modelling, and finally fed the final output of the LSTM into a softmax layer to make the classification decision. After we were equipped with three techniques to improve its performance, experimental results showed that our method achieved better accuracies than state-of-the-art systems. Furthermore, it is shown that all three techniques make their own contribution to the accuracy achieved."}, {"heading": "Acknowledgments", "text": "This work is supported by China's National High-Tech R & D Program (863 Program) (No. 2015AA015404) and Shanghai City Science and Technology Commission (No. 14511106802) and we thank the anonymous reviewers for their valuable comments."}], "references": [{"title": "Generating entailment rules from framenet", "author": ["R.B. Aharon", "I. Szpektor", "I. Dagan"], "venue": "Proceedings of the ACL 2010 Conference Short Papers. pp. 241\u2013246. Association for Computational Linguistics", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "Proceedings of the 36 Annual Meeting of the Association for Computational Linguistics and 17 International Conference on Computational Linguistics-Volume 1. pp. 86\u2013 90. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks 5(2), 157\u2013166", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "A large annotated corpus for learning natural language inference", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632\u2013642", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["S.R. Bowman", "J. Gauthier", "A. Rastogi", "R. Gupta", "C.D. Manning", "C. Potts"], "venue": "arXiv preprint arXiv:1603.06021", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["J. Cheng", "L. Dong", "M. Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches", "author": ["I. Dagan", "B. Dolan", "B. Magnini", "D. Roth"], "venue": "Natural Language Engineering 15(4), i\u2013xvii", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic textual entailment: Generic applied modeling of language variability", "author": ["I. Dagan", "O. Glickman"], "venue": "Learning Methods for Text Understanding and Mining. pp. 26\u201329", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Wordnet: An electronic lexical database", "author": ["Fellbaum", "C. (ed."], "venue": "MIT Press Cambridge", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "The third pascal recognizing textual entailment challenge", "author": ["D. Giampiccolo", "B. Magnini", "I. Dagan", "B. Dolan"], "venue": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. pp. 1\u20139. Association for Computational Linguistics", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Acquiring lexical paraphrases from a single corpus", "author": ["O. Glickman", "I. Dagan"], "venue": "Recent Advances in Natural Language Processing III. John Benjamins Publishing, Amsterdam, Netherlands pp. 81\u201390", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word 10(2-3), 146\u2013162", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1954}, {"title": "Methods for using textual entailment in open-domain question answering", "author": ["S. Harabagiu", "A. Hickl"], "venue": "Proceedings of the 21 International Conference on Computational Linguistics and 44 Annual Meeting of the ACL. pp. 905\u2013912", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["S. Hochreiter"], "venue": "Diploma, Technische Universit\u00e4t M\u00fcnchen", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8), 1735\u20131780", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review 104(2), 211\u2013240", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Dirt - discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": "Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 323\u2013328", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "A Text Summarization Approach under the Influence of Textual Entailment", "author": ["E. Lloret", "O. Ferr\u00e1ndez", "R. Munoz", "M. Palomar"], "venue": "NLPCS. pp. 22\u201331", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems (NIPS). pp. 3111\u20133119", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["L. Mou", "R. Men", "G. Li", "Y. Xu", "L. Zhang", "R. Yan", "Z. Jin"], "venue": "arXiv preprint arXiv:1512.08422", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Acquisition of verb entailment from text", "author": ["V. Pekar"], "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. pp. 49\u201356. Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Conference on Empirical Methods on Natural Language Processing (EMNLP). pp. 1532\u20131543", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u00fd", "P. Blunsom"], "venue": "International Conference on Learning Representations", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic paraphrase discovery based on context and keywords between NE pairs", "author": ["S. Sekine"], "venue": "Proceedings of IWP. vol. 2005, pp. 4\u20136", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Improving neural networks with dropout", "author": ["N. Srivastava"], "venue": "Ph.D. thesis, University of Toronto", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Augmenting wordnet-based inference with argument mapping", "author": ["I. Szpektor", "I. Dagan"], "venue": "Proceedings of the 2009 Workshop on Applied Textual Inference. pp. 27\u201335. Association for Computational Linguistics", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised acquisition of entailment relations from the web", "author": ["I. Szpektor", "H. Tanev", "I. Dagan", "B. Coppola", "M. Kouylekov"], "venue": "Natural Language Engineering 21(01), 3\u201347", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning natural language inference with LSTM", "author": ["S. Wang", "J. Jiang"], "venue": "Proceedings of the 15 Annual Conference of the North American Chapter of the Association for Computational Linguistics. pp. 700\u2013704", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "As formulated by Dagan and Glickman [8], the task of Recognizing Textual Entailment is to decide whether the meaning of a text fragment Y (called the Hypothesis) can be inferred (is inferred) from another text fragment X (called the Premise).", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "[10] extended the task to include the additional requirement that systems identify when the Hypothesis contradicts the Premise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The semantic inference needs are pervasive in a variety of NLP or text mining applications [7], inclusive of but not limited to, questionanswering [13], text summarization [19], and information extraction.", "startOffset": 91, "endOffset": 94}, {"referenceID": 12, "context": "The semantic inference needs are pervasive in a variety of NLP or text mining applications [7], inclusive of but not limited to, questionanswering [13], text summarization [19], and information extraction.", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "The semantic inference needs are pervasive in a variety of NLP or text mining applications [7], inclusive of but not limited to, questionanswering [13], text summarization [19], and information extraction.", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "WordNet [9] is the most prominent resource to extract entailment rules from.", "startOffset": 8, "endOffset": 11}, {"referenceID": 26, "context": "To make use of the other non-substitutable relations (such as entailment and cause relations), Szpektor and Dagan [27] populated these non-substitutable relations with argument mapping which are extracted various resource, and thus extended WordNet\u2019s inferential relations at the syntactic representation level.", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "FrameNet [2] is another manually constructed lexical knowledge base for entailment rule extraction.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "[1] detected the entailment relations implied in FrameNet, and utilized FrameNet\u2019s annotated sentences and relations between frames to extract both the entailment relations and their argument mappings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "The DIRT algorithm proposed by Lin and Pantel [18] was based on the so-called Extended Distributional Hypothesis which states that", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "Different from the Extended Distributional Hypothesis adopted by DIRT, Glickman and Dagan [11] proposed an instance-based approach, which uses linguistic filters to identify paraphrase instances that describe the same fact and then rank the candidate paraphrases based on a probabilistically motivated paraphrase likelihood measure.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "Sekine [25] extracted the phrase between two named entities as candidate linear pattern, then identified a keyword in each phrase and joined phrases with the same keyword into sets, and finally linked sets which involve the same pairs of individual named entities.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Besides paraphrase rules (which can be thought of as a specific case of entailment rules), a more general notion needed for RTE is that of entailment rules [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 21, "context": "Pekar [22] proposed a three-step method: it first identifies pairs of discourse-related clauses, and then creates patterns by extracting pairs of verbs along with relevant information as to their syntactic behaviour, and finally scores each verb pair in terms of plausibility of entailment.", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "[28] presented a fully unsupervised learning algorithm for Web-based extraction of entailment rules.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4], there comes an upsurge of end-to-end neural models for RTE, where the fundamental problem is how to model a sentence pair (X,Y ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], encodes the premise and the hypothesis with two separate LSTMs, and then feeds the concatenation of their final outputs into a MLP for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Several follow-ups have been proposed to solve this problem by modeling their interaction with a variety of attentive mechanisms [24] [29] [6].", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "Several follow-ups have been proposed to solve this problem by modeling their interaction with a variety of attentive mechanisms [24] [29] [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 5, "context": "Several follow-ups have been proposed to solve this problem by modeling their interaction with a variety of attentive mechanisms [24] [29] [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "Such kind of work includes the Stack-augmented Parser-Interpreter Neural Network (SPINN) [5] and Tree-based Convolutional Neural Network (TBCNN) [21].", "startOffset": 89, "endOffset": 92}, {"referenceID": 20, "context": "Such kind of work includes the Stack-augmented Parser-Interpreter Neural Network (SPINN) [5] and Tree-based Convolutional Neural Network (TBCNN) [21].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "[4] in 2015, which uses separate LSTMs [15] to encode the premise and the hypothesis as dense fixed-length vectors and then feeds their concatenation into a multi-layer perceptron (MLP) or other classifiers for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[4] in 2015, which uses separate LSTMs [15] to encode the premise and the hypothesis as dense fixed-length vectors and then feeds their concatenation into a multi-layer perceptron (MLP) or other classifiers for classification.", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "To solve this problem, a sequential LSTM model is proposed in [24].", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "[24] applied a neural attention model which can achieve better performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Wang and Jiang [29] used an LSTM to perform word-by-word matching of the hypothesis with the premise.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "The attentive mechanisms used in [24] and [29] are both between the hypothesis and the premise.", "startOffset": 33, "endOffset": 37}, {"referenceID": 28, "context": "The attentive mechanisms used in [24] and [29] are both between the hypothesis and the premise.", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "[6] induces undirected relations among tokens as an intermediate step of learning representations, which can be thought of as an intra-attention mechanism.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] introduced the Stack-augmented Parser-Interpreter Neural Network (or SPINN in short) to combine parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21] proposed a tree-based convolutional neural network (TBCNN) to capture sentence-level semantics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Distributional Hypothesis proposed by Harris [12] has provided a guiding principle, which states that words appearing", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "Latent semantic analysis [17] is a representative method of this kind, which applies truncated Singular Value Decomposition to a matrix of word-context co-occurrence matrix.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "Recently, neural network-based methods, such as Skip-Gram [20] and Glove [23], have been proposed to represent words as low-dimensional vectors called word embeddings.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Recently, neural network-based methods, such as Skip-Gram [20] and Glove [23], have been proposed to represent words as low-dimensional vectors called word embeddings.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Traditional recurrent neural networks often suffer from the problem of vanishing and exploding gradients [14] [3], making it hard to train models.", "startOffset": 105, "endOffset": 109}, {"referenceID": 2, "context": "Traditional recurrent neural networks often suffer from the problem of vanishing and exploding gradients [14] [3], making it hard to train models.", "startOffset": 110, "endOffset": 113}, {"referenceID": 14, "context": "In this paper, we adopt the Long Short-Term Memory (LSTM) model [15] which partially solves the problem by using gated activation function.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Dropout has shown a great success when working with feed-forward networks [26].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "As indicated in [30], our method drops the input and the output of the LSTM layer, with the same dropout rate.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "In the experimental part, we evaluate our method on the Stanford Natural Language Inference (SNLI) dataset [4] which consists of about 570K sentence pairs.", "startOffset": 107, "endOffset": 110}, {"referenceID": 15, "context": "We use stochastic mini-batch gradient descent with the ADAM optimizer [16], we set ADAM\u2019s hyperparameters \u03b21 = 0.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "We use both the pre-trained Glove [23] model glove.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "300d and Word2vec [20] model GoogleNews-vectorsnegative300 to initialize word embeddings.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "We don\u2019t tune the word embeddings when train, OOV words\u2019 vectors are set to be the average of their window words\u2019 vectors, follow same setting in the Match-LSTM paper [29].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "\u2013 Separate-LSTM: the first neural method proposed in [4], which encodes the premise and the hypothesis with two separate LSTMs independently.", "startOffset": 53, "endOffset": 56}, {"referenceID": 23, "context": "\u2013 Sequential-LSTM method: this method [24] makes use of two LSTMs, where an LSTM reads the premise, and a second LSTM initialized with the final cell state of the first LSTM reads a delimiter and the hypothesis.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "\u2013 Attention-LSTM: the method in [24] that attends over output vectors of the premise only for the final output of the hypothesis.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "\u2013 Word-by-Word Attention-LSTM: the method in [24] that attends over output vectors of the premise for every word in the hypothesis.", "startOffset": 45, "endOffset": 49}, {"referenceID": 28, "context": "\u2013 matchLSTM with word embedding: the method in [29] that performs wordby-word matching of the hypothesis with the premise.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "Recognizing textual entailment is a fundamental task in a variety of text mining or natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones.", "creator": "LaTeX with hyperref package"}}}