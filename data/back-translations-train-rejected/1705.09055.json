{"id": "1705.09055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "The cost of fairness in classification", "abstract": "We study the problem of learning classifiers with a fairness constraint, with three main contributions towards the goal of quantifying the problem's inherent tradeoffs. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for cost-sensitive classification and fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we show how the tradeoff between accuracy and fairness is determined by the alignment between the class-probabilities for the target and sensitive features. Underpinning our analysis is a general framework that casts the problem of learning with a fairness requirement as one of minimising the difference of two statistical risks.", "histories": [["v1", "Thu, 25 May 2017 05:56:08 GMT  (377kb,D)", "http://arxiv.org/abs/1705.09055v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aditya krishna menon", "robert c williamson"], "accepted": false, "id": "1705.09055"}, "pdf": {"name": "1705.09055.pdf", "metadata": {"source": "CRF", "title": "The cost of fairness in classification", "authors": ["Aditya Krishna Menon", "Robert C. Williamson"], "emails": ["bob.williamson}@data61.csiro.au"], "sections": [{"heading": "1 Introduction", "text": "Suppose we want a classifier to find suitable candidates for a job. This classifier can accept as input various characteristics of a candidate, such as his interview performance, qualifications and experience. Suppose one of these characteristics would be as accurate as possible. This is the issue of fairness, and he has considerable attention in the machinery. Pedreshi et al., 2008, Kamiran and Publishers, 2010, Dwork et al., 2012, Fukuchi et al., 2013, Zafar et al., 2016, Zafar et al."}, {"heading": "2 Background and notation", "text": "We correct notation and review relevant background information. Table 1 summarizes some key concepts to which we frequently refer."}, {"heading": "2.1 Standard learning from binary labels", "text": "Let X Rd be a measurable instance space, e.g. properties of a candidate for a job. In the standard learning of binary labels, we have samples from a distribution D over X \u00b7 {0, 1}, with (X, Y) \u0445 D. Here, Y is a target attribute that we want to predict, e.g. whether we hire a candidate. Our goal is to output a measurable randomized classifier, parameterized by f: X \u2192 [0, 1], which distinguishes between positive (Y = 1) and negative (Y = 0) instances. A randomized classifier predicts that each x value X is positive with the probability f (x); the quality of such a classifier is determined by a statistical risk R (\u00b7; D): [0, 1] X \u2192 R + which, for some things, is negative: [0, 1] 3 \u2192 R +, is [Narasimhan et al]."}, {"heading": "2.2 Fairness-aware learning", "text": "The statistical structure is modified by the assumption that, in addition to the target attribute Y, there is a sensitive attribute Y, which we wish to treat in a special way, for example, the race of a candidate. (We construct a general formalism for the problem by assuming that we reward classifiers who are \"fair\" in the treatment of Y.) We focus on two simple concepts of perfect fairness, which are called Y, Y and the classifying prediction Y with respect to the random variables. (f) We construct a general formalism for the problem 1.) We focus on Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,"}, {"heading": "2.3 Existing work on fairness", "text": "Fairness has been extensively studied in philosophy and welfare economics [Rawls, 1971, Sen, 2009], but with few exceptions [Bimore, 1994, Binmore, 2005] there is little formal utilitarian literature dealing with fairness. See Appendix E for a more detailed overview. In the machine learning community, Dwork et al. [2012] proposed an approach to ensure fairness based on a metric across instances. Zemel et al. [2013], Louizos et al. [2016] proposed approaches to learn representations that guarantee fairness. Both methods depend directly on the specific instances x-X. In contrast, our approach never touches the instances, but only risks; this has the essential advantage of avoiding changes under repair metrization, and avoids the infinite regression of determining what is \"similar.\""}, {"heading": "3 Fairness measures as statistical risks", "text": "We present our general view of fairness measures as statistical risks where the sensitive feature is the target. Thus, we can analyze fairness measures by using risk assessment tools."}, {"heading": "3.1 General fairness measures", "text": "To formalize the notion of a fairness measure, we will first specify our statistical structure for fairness-conscious learning. Let Djnt be a common distribution across X \u00b7 {0, 1} \u00b7 {0, 1}, with random variables (X, Y, Y) \u0445 Djnt. Here, X represents the instance, Y the target feature, and Y the sensitive feature. We will be interested in three induced distributions: We will refer to P (X, Y) as D, P (X, Y) as D-DP, and P (X, Y = 1) as D-EO. We will use D to refer generally to D-DP or D-EO. In fairness-conscious learning, our goal is to output a randomized classification f: X \u2192 [0, 1] with low statistical risk for D, so that Y is well predicted; we will label this risk with Rperf (\u00b7; D) and refer to Djant as performance."}, {"heading": "3.2 Classification-type fairness measures", "text": "In problem 1, Rfair depends on Djnt, which is defined by the triplet (X, Y, Y) (X, Y, Y). An interesting subclass of Rfair are those that depend only on D, DP, which is defined by the tuple (X, Y). As with Rperf, such Rfair can be defined as a statistical risk for D, DP (F, D, DP), FNR (F, DP), F, DP (D, DP), P (Y, DP), etc. Intuitively, we test whether we can predict the sensitive attribute Y of X. If it is possible to do this according to Rfair, we have no fairness."}, {"heading": "3.3 Anti-classifiers and symmetrised fairness", "text": "The application of statistical risk to Rfair in Equation 6 limits false-positive and negative interest rates. However, these limitations may assume that our classifier on D-DP is not trivial; for example, such measures could be deceived by an anti-classifier, i.e. one that has high fairness but whose negation has low fairness. Intuitively, one would like to prevent such a trivial transformation from adversely affecting fairness. An easy way to do this is to consider the symmetrical unit of fairness R fair (f; Djnt) = fair (f; Djnt) that Rfair (1 \u2212 f; Djnt), (Fnt), (9), where it denotes minimum operation. \u2212 Maximizing Equation 9 requires that both the classifier and the anti-classifier are fair."}, {"heading": "3.4 Relation to existing work", "text": "The notion that statistical risk is suitable for D & DP as a metric of fairness implies prior surveys of such metrics, and the formalization of this term allows us to then use risk analysis tools to analyze a range of fairness metrics. Not much attention has been paid to the need for symmetrical fairness, with work using MD and DI values, such as Calders and Verwer [2010], Feldman et al. [2015] implicitly assuming that learned classifiers will perform better than random guesses for D & DP."}, {"heading": "4 A cost-sensitive view of fairness measures", "text": "In the previous section, the DI- and MD-fairness measures were presented as statistical risks at D-level. We now show how they can be further linked to cost-sensitive risks, implying that the analysis of cost-sensitive fairness measures is sufficient to analyze these two measures. First, we present a useful repair parameterization of the standard cost-sensitive risk."}, {"heading": "4.1 Balanced cost-sensitive risk", "text": "Remember that the standard cost-sensitive risk (Example 1) is parameterized using the following parameters: (u, v, p) 7 \u2192 p \u00b7 (1 \u2212 c) \u00b7 c \u00b7 v. Now define the balanced cost-sensitive risk to be parameterized by \u03a6bal: (u, v, p) 7 \u2192 2 \u00b7 \u03a6 (u, v, 1 / 2), i.e. CSbal (f; D, c). = (1 \u2212 c) \u00b7 FNR (f; D) + c \u00b7 FPR (f; D). (10) If c = 1 / 2, we get the balanced error (Eq. 2). Generally, it is simply a scaled and repaired version of the standard cost-sensitive risk: We have CS (f; D, c) = (\u03b1 + \u03b2) \u00b7 CSbal (f; D, c), where \u03b1 = (1 \u2212 c), \u03b2 c = (appropriate) will, however, put it into existing measures."}, {"heading": "4.2 Disparate impact and cost-sensitive risk", "text": "Our first result is that the unequal influence factor (Eq.7) can be associated with the balanced, cost-sensitive risk. This suggests that studying the latter helps to understand the former. Lemma 1. Select any distribution D and random classifier f. Then, for each case, c. [0, 1], if c. = 11 +. [1 2, 1], DI (f; D], [1 \u2212 c, c]. (12) We make two remarks. First, Lemma 1 does not imply that unequal effects equals a cost-sensitive risk, but rather that their superlevel sets are related to each other. Nevertheless, this means that an unequal impact restriction equals a cost-sensitive constraint, the latter being easier to analyze. Second, since Lemma 1 applies to each distribution D, we can create an unequal effect in D, i.e. an equality in EDI (EY)."}, {"heading": "4.3 Mean difference score and balanced error", "text": "Our next result is that the mean differential value (Eq.8) has a strong link to a balanced cost-sensitive risk. Lemma 2. Select any distribution D and the random classifier f. Then, for each case, if c = 1 + \u03c42 (1 2, 1), MD (f; D) = 1 \u2212 2 \u00b7 CSbal (f; D, 1 / 2) (13) MD (f; D, 0, 1). Thus, the MD value is a transformation of the balanced error, as the latter corresponds to c = 1 / 2. Note that Eq.13 implies an equivalence of risks and not just super-level rates."}, {"heading": "4.4 The cost-sensitive fairness problem", "text": "The above results establish the versatility of cost-sensitive fairness measures. In the following, therefore, we will focus on such measures for general cost parameters, relying on terms 1 and 2 to correlate statements about them with statements about the EDI and MD values. In terms of symmetry, we will also focus on cost-sensitive risks to the underlying problem, although with possibly different cost parameters. The above measures require optimization: according to \u00a7 3.3, it is desirable to work with symmetrized versions of each fairness measure. In terms of general cost-sensitive risks, these symmetrized versions have a simple form: It is a simple calculation that for each c predisposition [0, 1] and the classification f, CSbal (1 \u2212 f; D-CSbal) = 1 \u2212 CSbal (f; D-Problem, c)."}, {"heading": "4.5 Relation to existing work", "text": "Lemma 1 is a special case of a broader relationship between broken performance measures and \"level finder\" functions [Parambath et al., 2014, Theorem 1], [Narasimhan et al., 2015, Lemma 7]. Feldman et al. [2015] attributed the different impacts to the balanced error, but their limit depends on the distribution and classifier, while we use a cost-sensitive risk with constant size; see \u00a7 7 and Appendix F."}, {"heading": "5 Bayes-optimal fairness-aware classifiers", "text": "After formalizing the fairness-conscious learning problem and further linking existing fairness measures with cost-sensitive risks, we are able to examine the trade-offs imposed by the problem. We begin with the question: What impact does the fairness requirement have on the Bayes-optimal solutions? The structure of these solutions provides insight into the problem and also suggests a simple practical algorithm. In the following, we will use the following quantities: \u03b7 (x). = P (Y = 1 | X = x) \u03c0. = P (Y = 1) \u03b7DP (x). = P (Y = 1 | X = x) \u03c0. = P (Y = 1) \u03b7EO (x). = P (Y = 1 | X = x, Y = 1) (16)."}, {"heading": "5.1 Bayes-optimal cost-sensitive classifiers", "text": "We will examine the Bayes-optimal classifiers of problem 2 so that both our fairness and performance measures pose cost-sensitive risks. If we work with D-DP (i.e. the demographic parity setting), Equation 15 allows an interesting minimisation. Proposal 3. Choose any distribution Djnt, costs c, costs c, costs c, costs c, costs c, costs c, costs and costs R. Then Argmin f, (0.1) XRfull (f; D, D) DP, c, c, costs c, costs c, costs c, costs c, costs c, costs f, costs. (0, 1)."}, {"heading": "5.2 Special case: using the sensitive feature as input", "text": "The form of the optimal classifier simplifies if we allow the sensitive feature as input. We have the following analogy to sentence 3, if we use D-DP. Corollary 5. Select any distribution Djnt, in which D contains the sensitive feature, the costs c, c, c [0, 1], and the like. For each feature (x, y) = P (x, y = x, Y = y), Argmin f-instance [0,1] X- {0,1} Rfull (f; D, D-DP, c, c, c, \u03bb) = {f-threshold (x, y-value) s (x, y-value), 0 = f-value (x, y-value) = ns-value (x, y-value) > 0o-value (x-value), (x-value, 0)."}, {"heading": "5.3 A plugin approach to fairness-aware learning", "text": "Analogous to the Bayes optimal classifiers for standard statistical risk, this encourages a simple plugin estimation approach for a fairness-conscious learning problem: estimate \u03b7, und separately, e.g. by logistic regression, and then combine them according to equations 17, 18 to construct a classifier. If the sensitive feature is available, all we need is a single model for \u03b7 (x, y), which is evaluated separately for each of the sensitive features. We make three comments on the proposed approach: Firstly, of course, one must establish a desirable trade-off between accuracy and fairness. Fortunately, this does not require retraining of any model, as we can simply use the learned \u03b7, and appropriately modify how they are set as a threshold to form a classifier."}, {"heading": "5.4 Relation to existing work", "text": "The calculation of the Bayes optimal classifiers as described above is not without precedent: Hardt et al. [2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of the previous work to calculate the optimal classifiers for approximate fairness measures. While the results have a similar note to the exact fairness case, it is important to explicate them in order to understand the full trade-off between accuracy and fairness (\u00a7 6), and also suggests a simple algorithm. Hardt et al. [2016] proposed to construct a fairness-aware classifier for equal opportunities by explicitly showing the results of a classifier trained on the original problem."}, {"heading": "6 Quantifying the accuracy-fairness tradeoff", "text": "We are now examining the trade-off between performance based on our basic problem and fairness, and showing that it can be quantified by a measure of target alignment and sensitive variables."}, {"heading": "6.1 The fairness frontier", "text": "Our definition of fairness-conscious learning problems (Problem 1) was in relation to a linear conflict of objectives between performance and fairness measures. To quantify the conflict of objectives imposed by a restriction of fairness, we will investigate the following explicitly restricted problem: for the behavior of (0, 1), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), \"(0),\" (0), \"(0),\" (0), \"(0),\" (0), \"(0),\" (0), \"(0),\" (0), \"(0),\" (0), \"(0),\" (0), \"(0),\" (0, \"(0),\" (0), \"(0,\", \"(0),\" (0), \"(0),\" (0, \",\" (0), \",\" (0), \"(0,\"), \"(0,\" (), \"(0,\"), \"(0,\"), \"(),\" (0, \"),\" (0, \"(),\" (0, \",\" (), \"(0,\"), (0, \"),\" (), \"(0,\" (), \"(0,\"), \"(),\" (0, (), (0, \"), (), (0, (), (),\" (0, \"), (), (0, (),\" (0, (), (), (), (0, (), (), \"(), (0,\" (), (), (0, (), \"(), (0, (), (), (), (), (),"}, {"heading": "6.2 The frontier and class-probability alignment", "text": "To get our first distribution-dependent statement about the curve, we need to note that F (Celsius) = 0 for each other distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0) = 0 for each distribution condition (0). We can formalize this notion of the distribution condition."}, {"heading": "7 Experiments", "text": "We present an experiment inspired by Feldman et al. [2015], which aims to certify whether a data set allows for different impacts (i.e. you can achieve DI (f; D) \u2264 \u03c4 for fixed vehicles) by testing whether the minimum achievable balanced error is below a certain threshold (see Appendix F.) Instead of applying the balanced error, we follow Lemma 1 and evaluate the minimum achievable balanced cost-sensitive risk for c = (1 + \u03c4) \u2212 1. Specifically, we look at the German data set of BER with Y, which indicates whether or not a person is over 25 years old, and evaluate the minimum achievable balanced cost-sensitive risk for c = (1) \u2212 1. For a number of train test columns that need to be specified, we build models to minimize the cost-sensitive logistical loss with parameter c (equation 20), and evaluate the test that determines the frugal impact on the frugal impacts of fixed impacts, as well as M (f)."}, {"heading": "8 Conclusion and future work", "text": "We have examined the trade-offs inherent in the problem of learning with a fairness limitation, and have shown that the optimal classifier for cost-sensitive fairness measures is an instance-dependent threshold of the class probability function and the quantification of performance impairment by a measure of target alignment and sensitive variables. There are several interesting directions for future work. To name just a few, we consider it valuable to examine Bayes-optimal scorers for ranking measures such as AUC, to establish the consistency of plugin estimators of \u00a7 5, to quantify the effects of working with a finite sample, and to extend our analysis to the case of sensitive characteristics of multiple categories."}, {"heading": "A Proofs of results in main body", "text": "The proof for Lemma 1 is applicable from the outset to the above-mentioned anticlassification; the first result is derived from the definition for Lemma 2, which is applicable to the above-mentioned impactivity; the first result is derived from the definition for Lemma 2, which can be extended to a symmetrized version of Equation 9; the second result is derived from the definition for Lemma 2, which is applicable to the above-mentioned impactivity; and the first result is derived from the definition for Lemma 1, which can be extended to a symmetrized version of Equation 9."}, {"heading": "B Helper results", "text": "Lemma 10. Select any distribution D and random classification f. Then for each cost parameter follows c (0, 1), CS (f; D, c) = (1 \u2212 c), \u00b7 p (X), \u00b7 p (X), \u00b7 p (X), where f (Y = 1), p (x), p (Y = 1 | x), p (X = x).Proof of of Lemma 10. By definition, CS (f; D, c) = (1 \u2212 c), where f (X), p (X), p (X), p (X), p (1 \u2212 f), p (x), p (x), \u2212 p (X), p (1 \u2212 c), p (1 \u2212 c), p (X), p (X), p (X), p (f), p (f), p (f), p (f), p (p), p, p, p, p, p, p, p, p"}, {"heading": "D Relating the constrained and unconstrained objectives", "text": "Consider the restricted version of the justice problem, f \u00b2. \u00b7 Argmin f \u00b2. (0.1) X Rperf (f; D): R fair (f; D \u00b2). \u2212 By Lemon 6, for finite X, this can be expressed as a solution for a linear program f \u00b2 F \u00b2 T, where F = {f \u00b2 bT f \u00b2, 1 \u00b2 \u00b2, 0 \u00b2 f (x \u00b2) and (x \u00b2 x \u00b2 X). = m (x) \u00b7 (c \u00b2 x \u00b2 X) (x \u00b2 X \u00b2) b (x \u00b2). = m (x \u00b2) \u00b7 (c \u00b2 \u00b2). \u00b7 As a strong duality for linear programs \u2212 4, we have a f \u00b2 F \u00b2 aT f = max \u00b2 problem, \u00b2 \u00b2 \u00b2 \u00b2 (minf \u00b2)."}, {"heading": "E A survey of fairness in philosophy and welfare economics", "text": "In fact, it is the case that most people who are able to determine themselves, determine themselves and decide what they want and what not."}, {"heading": "F Relating disparate impact and balanced error", "text": "Intuitively, we expect that if the balanced error of a classifier is low - meaning that the classifier accurately predicts the sensitive variable - we will have different effects. Conversely, we might hope that the presence of different effects implies a low balanced error."}, {"heading": "G Additional experiments", "text": "We present another experiment on the synthetic dataset considered in Zafar et al. [2016], where P (Y = 1) = 0.5, each X | Y = y \u0445 N (\u00b5y, \u0442y) of which 1 = [2] 1 = [5 1 1 5] \u00b50 = [10 1 3] 0 = [\u2212 2 \u2212 2] and P (Y = 1 | X = x) = P (X = Rx | Y = 1) P (X = Rx | Y = 1) + P (X = Rx | Y = \u2212 1) for the rotation matrix R = [cos \u03c6 \u2212 sin? sin?? sin? cos?]. We select N = 104 samples from this distribution and follow the same structure as the body: We construct a 2: 1 strain test split and compare COV and our plugin approach (2LR) with respect to the balanced error of the prediction of Y compared to the MD score in the prediction of Y."}], "references": [{"title": "A discriminative model for semi-supervised learning", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "Journal of the ACM,", "citeRegEx": "Balcan and Blum.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2010}, {"title": "Game Theory and the Social Contract Volume 1: Playing Fair", "author": ["Ken Bimore"], "venue": null, "citeRegEx": "Bimore.,? \\Q1994\\E", "shortCiteRegEx": "Bimore.", "year": 1994}, {"title": "Natural Justice", "author": ["Ken Binmore"], "venue": null, "citeRegEx": "Binmore.,? \\Q2005\\E", "shortCiteRegEx": "Binmore.", "year": 2005}, {"title": "Sorting Things Out: Classification and its Consequences", "author": ["Geoffrey C. Bowker", "Susan Leigh Star"], "venue": "MIT press,", "citeRegEx": "Bowker and Star.,? \\Q1999\\E", "shortCiteRegEx": "Bowker and Star.", "year": 1999}, {"title": "Three Naive Bayes approaches for discrimination-free classification", "author": ["Toon Calders", "Sicco Verwer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Calders and Verwer.,? \\Q2010\\E", "shortCiteRegEx": "Calders and Verwer.", "year": 2010}, {"title": "A measure of bizarreness", "author": ["Christopher P. Chambers", "Alan D. Miller"], "venue": "Quarterly Journal of Political Science,", "citeRegEx": "Chambers and Miller.,? \\Q2010\\E", "shortCiteRegEx": "Chambers and Miller.", "year": 2010}, {"title": "Algorithmic decision making and the cost of fairness", "author": ["Sam Corbett-Davies", "Emma Pierson", "Avi Feller", "Sharad Goel", "Aziz Huq"], "venue": "CoRR, abs/1701.08230,", "citeRegEx": "Corbett.Davies et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Corbett.Davies et al\\.", "year": 2017}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "venue": "In Innovations in Theoretical Computer Science Conference (ITCS),", "citeRegEx": "Dwork et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2012}, {"title": "The foundations of cost-sensitive learning", "author": ["Charles Elkan"], "venue": "In International joint conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Elkan.,? \\Q2001\\E", "shortCiteRegEx": "Elkan.", "year": 2001}, {"title": "Certifying and removing disparate impact", "author": ["Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Feldman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2015}, {"title": "Prediction with model-based neutrality", "author": ["Kazuto Fukuchi", "Jun Sakuma", "Toshihiro Kamishima"], "venue": "In European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD),", "citeRegEx": "Fukuchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fukuchi et al\\.", "year": 2013}, {"title": "Linear semi-infinite optimization, volume 2 of Wiley Series in Mathematical Methods in Practice", "author": ["Miguel A. Goberna", "Marco A. Lop\u00e9z"], "venue": null, "citeRegEx": "Goberna and Lop\u00e9z.,? \\Q1998\\E", "shortCiteRegEx": "Goberna and Lop\u00e9z.", "year": 1998}, {"title": "Equality of opportunity in supervised learning", "author": ["Moritz Hardt", "Eric Price", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility", "author": ["John C. Harsanyi"], "venue": "The Journal of Political Economy,", "citeRegEx": "Harsanyi.,? \\Q1955\\E", "shortCiteRegEx": "Harsanyi.", "year": 1955}, {"title": "Impartial predictive modeling: Ensuring fairness in arbitrary models", "author": ["Kory D. Johnson", "Dean P. Foster", "Robert A. Stine"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Classification without discrimination", "author": ["Faisal Kamiran", "Toon Calders"], "venue": "In IEEE International Conference on Computer, Control and Communication", "citeRegEx": "Kamiran and Calders.,? \\Q2009\\E", "shortCiteRegEx": "Kamiran and Calders.", "year": 2009}, {"title": "Fairness-aware classifier with prejudice remover regularizer", "author": ["Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma"], "venue": "In European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD),", "citeRegEx": "Kamishima et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kamishima et al\\.", "year": 2012}, {"title": "The ecological fallacy revisited: Aggregate- versus individual-level findings on economics and elections, and sociotropic voting", "author": ["Gerald H. Kramer"], "venue": "The American Political Science Review,", "citeRegEx": "Kramer.,? \\Q1983\\E", "shortCiteRegEx": "Kramer.", "year": 1983}, {"title": "Women, Fire, and Dangerous Things: What Categories Reveal about the Mind", "author": ["George Lakoff"], "venue": null, "citeRegEx": "Lakoff.,? \\Q1987\\E", "shortCiteRegEx": "Lakoff.", "year": 1987}, {"title": "We are all different\u201d: Statistical discrimination and the right to be treated as an individual", "author": ["Kasper Lippert-Rasmussen"], "venue": "The Journal of ethics,", "citeRegEx": "Lippert.Rasmussen.,? \\Q2011\\E", "shortCiteRegEx": "Lippert.Rasmussen.", "year": 2011}, {"title": "The variational fair autoencoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard S. Zemel"], "venue": "In ICLR,", "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "On the statistical consistency of plug-in classifiers for non-decomposable performance measures", "author": ["Harikrishna Narasimhan", "Rohit Vaish", "Shivani Agarwal"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Narasimhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2014}, {"title": "Optimizing non-decomposable performance measures: A tale of two classes", "author": ["Harikrishna Narasimhan", "Purushottam Kar", "Prateek Jain"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Optimizing F-measures by cost-sensitive classification", "author": ["Shameem Puthiya Parambath", "Nicolas Usunier", "Yves Grandvalet"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Parambath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Parambath et al\\.", "year": 2014}, {"title": "Discrimination-aware data mining", "author": ["Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Pedreshi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pedreshi et al\\.", "year": 2008}, {"title": "A Theory of Justice", "author": ["John Rawls"], "venue": null, "citeRegEx": "Rawls.,? \\Q1971\\E", "shortCiteRegEx": "Rawls.", "year": 1971}, {"title": "Composite binary losses", "author": ["Mark D. Reid", "Robert C. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2010\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2010}, {"title": "Information, divergence and risk for binary experiments", "author": ["Mark D Reid", "Robert C Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2011\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2011}, {"title": "Calibrated asymmetric surrogate losses", "author": ["Clayton Scott"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Scott.,? \\Q2012\\E", "shortCiteRegEx": "Scott.", "year": 2012}, {"title": "The Idea of Justice", "author": ["Amartya K. Sen"], "venue": null, "citeRegEx": "Sen.,? \\Q2009\\E", "shortCiteRegEx": "Sen.", "year": 2009}, {"title": "On the prevention of gerrymandering", "author": ["William Vickrey"], "venue": "Political Science Quarterly,", "citeRegEx": "Vickrey.,? \\Q1961\\E", "shortCiteRegEx": "Vickrey.", "year": 1961}, {"title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment", "author": ["Bilal Zafar", "Isabel Valera", "Manuel Gomez-Rodriguez", "Krishna Gummadi"], "venue": "In International World Wide Web Conference (WWW),", "citeRegEx": "Zafar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zafar et al\\.", "year": 2017}, {"title": "Learning fair classifiers", "author": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez-Rodriguez", "Krishna Gummadi"], "venue": "arXiv preprint arXiv:1507.05259,", "citeRegEx": "Zafar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zafar et al\\.", "year": 2016}, {"title": "Learning fair representations", "author": ["Richard Zemel", "Yu Wu", "Kevin Swersky", "Toniann Pitassi", "Cynthia Dwork"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zemel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2013}, {"title": "The theory we present here follows the precept of Sen [2009, Chapter 18] that mere identification of \u201cfully just social arrangements is neither necessary nor sufficient.\u201d We embrace Sen\u2019s pragmatism by focussing on the quantifiable tradeoffs one might make to approach (certain notions of) fairness and hence justice", "author": ["Miller"], "venue": null, "citeRegEx": "Miller and 2010..,? \\Q2009\\E", "shortCiteRegEx": "Miller and 2010..", "year": 2009}, {"title": "Relating disparate impact and balanced error", "author": ["Feldman"], "venue": null, "citeRegEx": "Feldman,? \\Q2015\\E", "shortCiteRegEx": "Feldman", "year": 2015}, {"title": "FPR( f ; D\u0304) = (1 \u2212 FNR( f ; D\u0304)", "author": ["Feldman"], "venue": null, "citeRegEx": "Feldman,? \\Q2015\\E", "shortCiteRegEx": "Feldman", "year": 2015}, {"title": "says that even if we have a classifier with high balanced error, there is no guarantee it will not have disparate impact. This is a worst case analysis over all possible classifiers we might have obtained. However, if we happen to know the false positive and negative rates", "author": ["Feldman"], "venue": null, "citeRegEx": "Feldman,? \\Q2015\\E", "shortCiteRegEx": "Feldman", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "A randomised classifier predicts any x \u2208 X to be positive with probability f (x); the quality of any such classifier is assessed by a statistical risk R(\u00b7; D) : [0, 1] \u2192 R+ which, for some \u03a6 : [0, 1]3 \u2192 R+, is [Narasimhan et al., 2014] R( f ; D) .", "startOffset": 210, "endOffset": 235}, {"referenceID": 21, "context": "For a broad class of \u03a6, the optimal classifier is a (possibly distribution dependent) thresholding of the class-probability function, f \u2217(x) = n\u03b7(x) > t\u2217(D)o [Narasimhan et al., 2014], where \u03b7(x) .", "startOffset": 158, "endOffset": 183}, {"referenceID": 8, "context": "For the cost-sensitive error with parameter c, the Bayes-optimal classifier is f \u2217(x) = n\u03b7(x) > co [Elkan, 2001].", "startOffset": 99, "endOffset": 112}, {"referenceID": 21, "context": "These Bayes-optimal classifiers motivate a plugin estimator, where one thresholds an empirical estimate of \u03b7 [Narasimhan et al., 2014].", "startOffset": 109, "endOffset": 134}, {"referenceID": 4, "context": ") The first is demographic parity [Calders and Verwer, 2010], which requires the predictions to be independent of the sensitive feature: P(\u0176 = 1 | \u0232 = 0) = P(\u0176 = 1 | \u0232 = 1).", "startOffset": 34, "endOffset": 60}, {"referenceID": 12, "context": "(3) The second is equality of opportunity [Hardt et al., 2016], which requires the predictions to be independent of the sensitive feature, but only for the positive instances: P(\u0176 = 1 | Y = 1, \u0232 = 0) = P(\u0176 = 1 | Y = 1, \u0232 = 1).", "startOffset": 42, "endOffset": 62}, {"referenceID": 12, "context": "Other notions of perfect fairness include equalised odds [Hardt et al., 2016], and lack of disparate mistreatment [Zafar et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 31, "context": ", 2016], and lack of disparate mistreatment [Zafar et al., 2017].", "startOffset": 44, "endOffset": 64}, {"referenceID": 9, "context": "The first is the disparate impact (DI) factor [Feldman et al., 2015], which is the ratio of the probabilities appearing in the definition of demographic parity:", "startOffset": 46, "endOffset": 68}, {"referenceID": 4, "context": "The second is the mean difference (MD) score [Calders and Verwer, 2010], which replaces the ratio with a difference: MD( f ) .", "startOffset": 45, "endOffset": 71}, {"referenceID": 24, "context": "Avoiding the use of the sensitive feature by itself does not guard against discrimination [Pedreshi et al., 2008].", "startOffset": 90, "endOffset": 113}, {"referenceID": 4, "context": "The second is the mean difference (MD) score [Calders and Verwer, 2010], which replaces the ratio with a difference: MD( f ) . = P(\u0176 = 1 | \u0232 = 1) \u2212 P(\u0176 = 1 | \u0232 = 0). (5) We refer the reader to \u017dliobait\u0117 [2015] for a survey of other fairness measures, including variants of the above.", "startOffset": 46, "endOffset": 210}, {"referenceID": 1, "context": "Fairness has received considerable study in philosophy and welfare economics [Rawls, 1971, Sen, 2009]; however, with few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness. See Appendix E for a more detailed overview. In the machine learning community, Dwork et al. [2012] proposed an approach to guarantee fairness relying on a metric over instances.", "startOffset": 133, "endOffset": 338}, {"referenceID": 1, "context": "Fairness has received considerable study in philosophy and welfare economics [Rawls, 1971, Sen, 2009]; however, with few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness. See Appendix E for a more detailed overview. In the machine learning community, Dwork et al. [2012] proposed an approach to guarantee fairness relying on a metric over instances. Zemel et al. [2013], Louizos et al.", "startOffset": 133, "endOffset": 437}, {"referenceID": 1, "context": "Fairness has received considerable study in philosophy and welfare economics [Rawls, 1971, Sen, 2009]; however, with few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness. See Appendix E for a more detailed overview. In the machine learning community, Dwork et al. [2012] proposed an approach to guarantee fairness relying on a metric over instances. Zemel et al. [2013], Louizos et al. [2016] proposed approaches to learn feature representations that guarantee fairness.", "startOffset": 133, "endOffset": 460}, {"referenceID": 4, "context": "Calders and Verwer [2010], Feldman et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Calders and Verwer [2010], Feldman et al. [2015] implicitly assuming that learned classifiers will perform better than random guessing on D\u0304DP.", "startOffset": 0, "endOffset": 49}, {"referenceID": 9, "context": "Feldman et al. [2015] related the disparate impact to the balanced error, but their bound depends on the distribution and classifier, while ours uses a cost-sensitive risk with constant \u03c4; see \u00a77 and Appendix F.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Computing the Bayes-optimal classifiers as above is not without precedent: Hardt et al. [2016], Corbett-Davies et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 5, "context": "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures.", "startOffset": 8, "endOffset": 37}, {"referenceID": 5, "context": "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of prior work on computing the optimal classifiers for approximate fairness measures. While the results have a similar flavour to the exact fairness case, explicating them is important to understand the full tradeoff between accuracy and fairness (\u00a76), and also suggests a simple algorithm. Hardt et al. [2016] proposed to construct a fairness-aware classifier in the equality of opportunity setting by post-processing the results of a classifier trained on the original problem.", "startOffset": 8, "endOffset": 439}, {"referenceID": 5, "context": "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of prior work on computing the optimal classifiers for approximate fairness measures. While the results have a similar flavour to the exact fairness case, explicating them is important to understand the full tradeoff between accuracy and fairness (\u00a76), and also suggests a simple algorithm. Hardt et al. [2016] proposed to construct a fairness-aware classifier in the equality of opportunity setting by post-processing the results of a classifier trained on the original problem. They considered a slightly different constrained version of the problem, where one forces the solution to have perfect rather than approximate fairness. Our Propositions 3 and 4 provide an explicit form for the correction when approximate fairness is desired, as well as when the sensitive feature is available or not during training. Recently, Woodworth et al. Woodworth et al. [2017] established limits on the post-processing approach of Hardt et al.", "startOffset": 8, "endOffset": 994}, {"referenceID": 5, "context": "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of prior work on computing the optimal classifiers for approximate fairness measures. While the results have a similar flavour to the exact fairness case, explicating them is important to understand the full tradeoff between accuracy and fairness (\u00a76), and also suggests a simple algorithm. Hardt et al. [2016] proposed to construct a fairness-aware classifier in the equality of opportunity setting by post-processing the results of a classifier trained on the original problem. They considered a slightly different constrained version of the problem, where one forces the solution to have perfect rather than approximate fairness. Our Propositions 3 and 4 provide an explicit form for the correction when approximate fairness is desired, as well as when the sensitive feature is available or not during training. Recently, Woodworth et al. Woodworth et al. [2017] established limits on the post-processing approach of Hardt et al. [2016]; studying this in our context of approximate fairness measures would be of interest.", "startOffset": 8, "endOffset": 1068}, {"referenceID": 4, "context": "Calders and Verwer [2010] proposed to modify the output of na\u00efve Bayes so as to minimise the MD score.", "startOffset": 0, "endOffset": 26}, {"referenceID": 28, "context": "for the surrogate cost-sensitive risk [Scott, 2012], CS(s; D, c, `) .", "startOffset": 38, "endOffset": 51}, {"referenceID": 11, "context": "(For infinite X we obtain a semi-infinite linear program [Goberna and Lop\u00e9z, 1998], whose duality is subtler to analyse.", "startOffset": 57, "endOffset": 82}, {"referenceID": 0, "context": "roughly, analogous to the notion of compatibility functions in semi-supervised learning [Balcan and Blum, 2010], wherein one can guarantee that unlabelled data is useful when there is an alignment of the marginal data distribution with one\u2019s function class.", "startOffset": 88, "endOffset": 111}, {"referenceID": 9, "context": "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.", "startOffset": 37, "endOffset": 59}, {"referenceID": 9, "context": "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.e. one can achieve DI( f ; D\u0304) \u2264 \u03c4 for fixed \u03c4) by testing if the minimal achievable balanced error is below some threshold (see Appendix F). Rather than employ the balanced error, we follow Lemma 1 and assess the minimal achievable balanced cost-sensitive risk for c = (1 + \u03c4)\u22121. Specifically, following Feldman et al. [2015], we consider the UCI german dataset with \u0232 denoting whether or not the age of a person is above 25, and fix \u03c4 = 0.", "startOffset": 37, "endOffset": 453}, {"referenceID": 9, "context": "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.e. one can achieve DI( f ; D\u0304) \u2264 \u03c4 for fixed \u03c4) by testing if the minimal achievable balanced error is below some threshold (see Appendix F). Rather than employ the balanced error, we follow Lemma 1 and assess the minimal achievable balanced cost-sensitive risk for c = (1 + \u03c4)\u22121. Specifically, following Feldman et al. [2015], we consider the UCI german dataset with \u0232 denoting whether or not the age of a person is above 25, and fix \u03c4 = 0.8. For a number of train-test splits to be specified, we train models to minimise the cost-sensitive logistic loss with parameter c (Equation 20), and evaluate on the test set the disparate impact, as well as the gap \u2206( f ) . = CSbal( f ; D\u0304, c) \u2212 (1 \u2212 c). Our Lemma 1 indicates that we should find the latter to be positive only when the former is larger than \u03c4 = 0.8. To construct our training sets, we make an initial 2:1 train-test split of the full data, treating \u0232 as the label to predict. To obtain models with varying levels of accuracy in predicting \u0232, we inject symmetric label noise of varying rates into the training (but not the test) set. Figure 6.2 shows that for the resulting models, as per Lemma 1, there is perfect agreement of disparate impact at \u03c4 = 0.8 and sign(\u2206( f )). We next present an experiment analogous to Zafar et al. [2016], where on the same german dataset we learn a classifier that respects a symmetrised MD score constraint, while being accurate for predicting the target variable in the sense of balanced error (BER).", "startOffset": 37, "endOffset": 1423}, {"referenceID": 9, "context": "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.e. one can achieve DI( f ; D\u0304) \u2264 \u03c4 for fixed \u03c4) by testing if the minimal achievable balanced error is below some threshold (see Appendix F). Rather than employ the balanced error, we follow Lemma 1 and assess the minimal achievable balanced cost-sensitive risk for c = (1 + \u03c4)\u22121. Specifically, following Feldman et al. [2015], we consider the UCI german dataset with \u0232 denoting whether or not the age of a person is above 25, and fix \u03c4 = 0.8. For a number of train-test splits to be specified, we train models to minimise the cost-sensitive logistic loss with parameter c (Equation 20), and evaluate on the test set the disparate impact, as well as the gap \u2206( f ) . = CSbal( f ; D\u0304, c) \u2212 (1 \u2212 c). Our Lemma 1 indicates that we should find the latter to be positive only when the former is larger than \u03c4 = 0.8. To construct our training sets, we make an initial 2:1 train-test split of the full data, treating \u0232 as the label to predict. To obtain models with varying levels of accuracy in predicting \u0232, we inject symmetric label noise of varying rates into the training (but not the test) set. Figure 6.2 shows that for the resulting models, as per Lemma 1, there is perfect agreement of disparate impact at \u03c4 = 0.8 and sign(\u2206( f )). We next present an experiment analogous to Zafar et al. [2016], where on the same german dataset we learn a classifier that respects a symmetrised MD score constraint, while being accurate for predicting the target variable in the sense of balanced error (BER). We employ the plugin estimator proposed in \u00a75.3, training logistic regression models to predict the target and sensitive variable and then combining them via Equation 17 for some \u03bb \u2208 R. On the test set, we compute the BER for the target variable, and the symmetrised MD score for the sensitive variable. We then employ the COV method of Zafar et al. [2016], which uses a surrogate to the MD constraint as discussed in \u00a75.", "startOffset": 37, "endOffset": 1979}], "year": 2017, "abstractText": "We study the problem of learning classifiers with a fairness constraint, with three main contributions towards the goal of quantifying the problem\u2019s inherent tradeoffs. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for cost-sensitive classification and fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we show how the tradeoff between accuracy and fairness is determined by the alignment between the class-probabilities for the target and sensitive features. Underpinning our analysis is a general framework that casts the problem of learning with a fairness requirement as one of minimising the difference of two statistical risks.", "creator": "LaTeX with hyperref package"}}}