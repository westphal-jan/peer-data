{"id": "1705.09064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "MagNet: a Two-Pronged Defense against Adversarial Examples", "abstract": "Deep learning has shown promising results on hard perceptual problems in recent years. However, deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human. Such specially crafted perturbations cause deep learning systems to output incorrect decisions, with potentially disastrous consequences. These vulnerabilities hinder the deployment of deep learning systems where safety or security is important. Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective.", "histories": [["v1", "Thu, 25 May 2017 06:49:57 GMT  (310kb,D)", "http://arxiv.org/abs/1705.09064v1", "In submission as a conference paper"], ["v2", "Mon, 11 Sep 2017 02:41:15 GMT  (2417kb,D)", "http://arxiv.org/abs/1705.09064v2", "Accepted at the ACM Conference on Computer and Communications Security (CCS), 2017"]], "COMMENTS": "In submission as a conference paper", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["dongyu meng", "hao chen"], "accepted": false, "id": "1705.09064"}, "pdf": {"name": "1705.09064.pdf", "metadata": {"source": "META", "title": "MagNet: a Two-Pronged Defense against Adversarial Examples", "authors": ["Dongyu Meng", "Hao Chen"], "emails": ["mengdy@shanghaitech.edu.cn", "chen@ucdavis.edu"], "sections": [{"heading": null, "text": "In this article, we propose MagNet, a framework for defending neural network classifiers against enemy examples. MagNet does not modify the protected classifier, nor does it know the process of generating enemy examples. MagNet includes one or more separate detector networks and a reformer network. In contrast to previous work, MagNet learns to distinguish between normal and enemy examples by approximating the diversity of normal examples. Since it does not rely on a process of generating enemy examples, it has considerable generalization power. In addition, MagNet reconstructs enemy examples by shifting them toward diversity, which is effective when it comes to correctly classifying adversary examples with small interferences. We discuss the intrinsic difficulty of defending against whitebox attacks, and propose a mechanism for defending against graybox attacks. Inspired by the use of randomness in cryptography, we suggest using diversity to advance against white box attacks, and we suggest that MagNet is most effective at strengthening the black box against very positive examples."}, {"heading": "1 INTRODUCTION", "text": "In recent years, deep learning has shown impressive performance on many tasks, such as image classification [8] and processing natural language [15]. Furthermore, recent research has shown that an attacker could generate hostile examples of dumb classifiers [33, 5, 23, 18]. Their algorithms interfered with harmless examples that were correctly classified by a small amount that did not affect human recognition, but resulted in neural networks being misclassified. We call these neural networks target classifiers. We can divide defense against hostile examples into three approaches: (1) training the target classifier with hostile examples, called enemy training [33, 5]; (2) training a classifier to distinguish between normal and hostile examples [19]; and (3) making target classifiers hard to attack by blocking the gradient path, e.g. defensive distillation."}, {"heading": "1.1 Adversarial examples", "text": "A normal example x for a classification task is a naturally occurring example. In other words, the process of natural data generation for this classification task generates x with not negligible probability. If the task is, for example, to classify handwritten numbers, then the process of data generation rarely generates an image of a tiger. 1Imagine the multitude of normal examples as magnets and test examples as iron particles in a high-dimensional space. The magnet is able to attract and move nearby particles (which illustrates the action of our reformer) but refuses to move distant particles (which illustrates the action of our detectors).ar Xiv: 170 5.09 064v 1 [cs.C R] May 25, 201 7An adverse example y for a classifier is not a normal example and the classifier's decision for y contradicts the prevailing judgment of man. See Section 3.1 for a more detailed discussion researchers speculate that their overall sample size is much lower for a normal one of 22 tasks."}, {"heading": "1.2 Causes of mis-classification and solutions", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in"}, {"heading": "1.3 Contributions", "text": "We make the following contributions. \u2022 We formally define opposing examples and metrics to evaluate defense against opposing examples (Section 3.1). \u2022 We propose a defense against opposing examples. \u2022 The defense is independent of either the target classification or the method of generating opposing examples (Section 4.1, Section 4.2). \u2022 We argue that it would be very difficult to defend oneself against whitebox attacks. Therefore, we propose the gray box threat model and advocate defense against such attacks through diversity. We show two approaches using diversity (Section 4.3)."}, {"heading": "2 BACKGROUND AND RELATEDWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Deep learning systems in adversarial environments", "text": "Deep learning systems play an increasingly important role in the modern world. They are used in autonomous control of robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28] and human-computer interaction [10, 12]. These safety-critical areas require a better understanding of neural networks from a security perspective. Recent work has shown that it is possible to attack such systems with carefully crafted inputs [2, 27, 7] for real-world systems. More specifically, researchers showed that it was possible to generate adverse examples in order to deceive classifiers [33, 5, 23, 18]. Their algorithms disrupted normal examples by a small volume that did not impair human recognition but caused misclassifications by the learning system."}, {"heading": "2.2 Distance metrics", "text": "Since it is difficult to model human perception, the researchers proposed three popular metrics to approximate the perception of visual difference by humans, namely L0, L2 and L \u221e [2]. These metrics are special cases of the Lp standard. They answer the question of how many pixels are changed in the two images. L2 measures the Euclidean distance between the two images. L0 measures the maximum difference for all pixels at corresponding positions in the two images. There has been debate about which metric is the best, but we will not discuss this question. Instead, we evaluated our defense in successful attacks that include all of these metrics."}, {"heading": "2.3 Past attacks", "text": "Since the discovery of the Gradient Method (FGSM), which has a normal image, a fast drawing method and a similar approach, the researchers have found a similar problem, which refers to different network architectures, including different types of systematics, such as systematization and semantic segmentation. The researchers have developed several methods to generate conflicting examples, most of which are based on the optimization of normal examples. Moosavi et al. showed that it was even possible to find an effective universal perturbation, which, when applied, translates many images. To simplify the discussion, we focus on attacks on neural networkers. We evaluated our defense against four popular and probably most advanced attacks. We explain this method of attack (FGSM)."}, {"heading": "2.4 Past defense", "text": "In fact, most of them will be able to play by the rules that they need for their policies to achieve their goals."}, {"heading": "3 PROBLEM DEFINITION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Adversarial examples", "text": "We define the following sentences: \u2022 S: the set of all examples in the example space (e.g. all images). \u2022 Ct: the set of mutually exclusive classes for the classification task. For example, if t is a handwritten number classification, then C = {0, 1,.., 9}. \u2022 Nt = {x | x, S and x occurs naturally with respect to the classification task t. Each classification task t assumes a data generation process that generates each example x x x,. S with the probability p (x). x occurs naturally when p (x) is not negligible. Researchers believe that Nt represents a diversity that is of much lower dimension than S [22]. Since we do not know the data generation process, we approach Nt by combining all natural datasets for t, such as CIFAR and MNIST for image recognition."}, {"heading": "3.2 Defense and evaluation", "text": "Definition 3.4. A defense against adverse examples of a classifier ft is a function dft: S \u2192 Ct. The defense algorithm dft extends the classifier ft to make it robust. The defense algorithm in dft can use ft in three different ways: \u2022 The defense algorithm does not read data in ft or modify parameters in ft. \u2022 The defense algorithm reads data in ft but does not modify parameters in ft. \u2022 The defense algorithm modifies parameters in ft. When evaluating the effectiveness of a defense algorithm, we cannot simply judge whether it correctly classifies each example, i.e. whether its decision is in line with that of the basic truth classifier. Finally, the goal of defense is to improve the accuracy of the classifier in adverse examples rather than in normal examples, i.e. definition 3.5."}, {"heading": "3.3 Threat model", "text": "We assume that the attacker knows everything about the classifier ft he wants to attack, called the target classifier, such as its architecture, parameters, training procedures, etc. Depending on whether the attacker knows the defensive ft, there are two scenarios: \u2022 Blackbox attack: The attacker does not know the parameters of dft. \u2022 Whitebox attack: The attacker knows the parameters of dft. \u2022 Graybox attack: Apart from the parameters, the attacker knows everything, such as its architecture, hyperparameters, training set, training periods. If we train a neural network several times while fixing these variables, we get different model parameters each time due to a random initialization. We can see that we get a different network each time. To take this one step further, we can train these different networks at the same time and 2Kurakin et al. showed that many opposing images are artificially generated."}, {"heading": "4 DESIGN", "text": "MagNet is a framework for defense against adverse examples (Figure 2). In Section 1.2, we have given two reasons why a classifier misclassifies an adverse example: (1) The example is far from the boundary of the manifold of normal examples, but the classifier has no way of rejecting it; (2) The example is close to the boundary of the manifold, but the classifier generalizes poorly near the example. Motivated by these observations, MagNet consists of two components: (1) a detector that rejects examples that are far from the manifold boundary, and (2) a reformer that seeks to find an example x'on or near the manifold, where x comes as close as possible to x, and then x \u2032 the target classifier. Figure 3 illustrates the effect of the detector and the reformer in a 2-D sample room."}, {"heading": "4.1 Detector", "text": "In fact, most of us are able to play by the rules they have imposed on ourselves."}, {"heading": "4.2 Reformer", "text": "An ideal reformer: (1) should not change the classification results of normal examples. (2) He should adequately change the contradictory examples so that the reconstructed examples are close to normal examples. In other words, he should reform the contradictory examples. (4) A naive reformer is a function that inserts random noise into the input. If we use the following reformer example (x) = clip (x) \u00b7 y, where y ~ N (y; 0, I) is the normal distribution with zero mean and identity-related codification matrix."}, {"heading": "4.3 Use diversity to mitigate graybox attacks", "text": "In fact, it is not surprising that he can use the same method to find adversarial examples, which he transforms into a new classification method for the implementation of the target classifier, where the attacker also knows the parameters of the detector and reformer, our evaluation has shown that he can use the same method he uses to find adversarial examples of f. \"t, if such adversarial examples do not exist or were negligible, then it would mean that f\" t \"s knows all the parameters of f\" t, so that he can use the same method he uses to find adversarial examples of f. \"t.\" If such adversarial examples do not exist or were negligible, then it would mean that f \"t\" with the ground truth classifier on almost all examples of the normal example. Since there is no evidence that we can find these perfect classifiers, we will soon cease to be classifiable, unclassifiable. \""}, {"heading": "5 IMPLEMENTATION AND EVALUATION", "text": "We examined the accuracy and characteristics of our defense described in section 4 using two standard datasets: MNIST [17] and CIFAR-10 [14]."}, {"heading": "5.1 Setup", "text": "OnMNIST, we selected 55 000 examples for the training set, 5 000 for the validation set and 1 000 for the test set. We trained a victim using the setting in [2] and achieved an accuracy of 99.4%. On CIFAR-10, we selected 45 000 examples for the training set, 5 000 for the validation set and 10 000 for the test set. We used the architecture in [32] and achieved an accuracy of 90.6%. The accuracy of these two classifiers corresponds to the state of the art in terms of these datasets. Table 1 and Table 2 show the architecture and training parameters of these classifiers. We used a scaled range of [0, 1] instead of [0, 255] for simplicity. In the rest of this section, we first evaluate the robustness of MagNet in black box attacks where the attacker does not know the parameters used in MagNet."}, {"heading": "5.2 Overall performance against blackbox attacks", "text": "This year, it has come to the point where it will only be once before a decision is reached that is no longer about anything."}, {"heading": "5.3 Case study on Carlini attack, why does MagNet work?", "text": "In this context, it should be noted that the case is an accident."}, {"heading": "5.4 Defend against graybox attacks", "text": "We assume that the attacker cannot predict the parameters that the defender uses in classifying their enemy examples; and (2) the attacker cannot try all the parameters that he uses in generating his opponents. (3) In this example, we offer diversity by trying all the parameters in creating counter-examples, then we can defend ourselves against attackers by diversifying our defensive networks. (3) We show an example of defense against graybox attacks. We offer diversity by using different autocoders for the reformer in magnet. In our proof-of-concept implementations, we use the same architecture, a conventional autocoder with 3 \u00d7 8 hidden layers, and ReLU activation to obtain eight autocoders of different parameters. During the training, we used the same hyperparameters as in Section 5.2."}, {"heading": "6 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Criteria for evaluating attacks and defenses", "text": "Therefore, attackers must find adversarial examples by creating new datasets. There are two ways: (1) by grading the task. Instead, we present the basic truth solutions in datasets with manually labeled examples, and we use these datasets to evaluate classifiers. However, given that we cannot use these datasets to evaluate the robustness of classifiers against contrary examples, we must first define the criteria for distinguishing between contrary examples and misclassified normal examples. However, we argue that the criteria cannot depend on the intention of the provider of the test examples. Since our goal is to improve the accuracy of the classifier, we often do not care or know the intention of the tester. It is reasonable to assume that benevolent users do not provide adversarial examples. Therefore, we must exclude all examples in existing datasets from existing datasets."}, {"heading": "6.2 Limitations", "text": "The effectiveness of MagNet against hostile examples depends on the following assumptions: \u2022 There are detector functions that measure the distance between its input and the multitude of normal examples. \u2022 There are reformer functions that output an example x that is noticeably close to the input x, and x \u00b2 is closer to the multiplicity than x. We have chosen an autoencoder for both the reformer and the two detector types in MagNet. MagNet's high accuracy against state-of-the-art attacks provides empirical evidence that our assumptions are likely to be correct. However, until we find stronger justifications or evidence, we cannot deny the possibility that our good results are due to the fact that modern attacks are not powerful enough. We hope that our results would motivate further work to find more powerful attacks or more powerful detectors and reformers."}, {"heading": "7 CONCLUSION", "text": "We proposed MagNet, a framework for defense against enemy interference from examples of neural networks. MagNet processes untrustworthy input in two ways. It detects enemy examples with large interference using dedicated detector networks, and pushes examples with small interference toward the diversity of normal examples. These two methods work together to increase classification accuracy. In addition, by using auto-encoders as detector networks, MagNet learns to detect enemy examples without requiring either hostile examples or knowledge of the process of generating them, leading to a better generalization capability. Experiments show that MagNet effectively defended against state-of-the-art attacks. In the event that the attacker knows the training examples of MagNet, we described a new graybox threat model and used diversity to effectively defend against this attack. We argue that defense should be independent of enemy examples."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Professor Xuming He from ShanghaiTech University for his valuable feedback."}], "references": [{"title": "End to end learning for self-driving cars", "author": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "In IEEE Symposium on Security and Privacy,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Learning transferable policies for monocular reactive mav control", "author": ["Shreyansh Daftry", "J Andrew Bagnell", "Martial Hebert"], "venue": "arXiv preprint arXiv:1608.00627,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Deep visual foresight for planning robot motion", "author": ["Chelsea Finn", "Sergey Levine"], "venue": "arXiv preprint arXiv:1610.00696,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "On the (statistical) detection of adversarial examples", "author": ["Kathrin Grosse", "PraveenManoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1702.06280,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "PraveenManoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Poster: deep learning for zero-day flash malware detection", "author": ["Wookhyun Jung", "Sangwon Kim", "Sangyong Choi"], "venue": "IEEE Symposium on Security and Privacy,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Uncertainty-aware reinforcement learning for collision avoidance", "author": ["Gregory Kahn", "Adam Villaflor", "Vitchyr Pong", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1702.01182,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Adversarial examples for generative models", "author": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "venue": "arXiv preprint arXiv:1702.06832,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Ask me anything: dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Adversarial examples in the physical world", "author": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "venue": "CoRR, abs/1607.02533,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "On detecting adversarial perturbations", "author": ["Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Universal adversarial perturbations", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "arXiv preprint arXiv:1610.08401,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "CoRR, abs/1511.04599,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Sample complexity of testing the manifold hypothesis", "author": ["H. Narayanan", "S. Mitter"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "In IEEE European Symposium on Security and Privacy (EuroSP),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "In IEEE Symposium on Security and Privacy,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Cleverhans v1.0.0: an adversarial machine learning library", "author": ["Nicolas Papernot", "Ian Goodfellow", "Ryan Sheatsley", "Reuben Feinman", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Crafting adversarial input sequences for recurrent neural networks", "author": ["Nicolas Papernot", "Patrick D. McDaniel", "Ananthram Swami", "Richard E. Harang"], "venue": "CoRR, abs/1604.08275,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Practical blackbox attacks against machine learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Malware classification with recurrent networks. InAcoustics", "author": ["Razvan Pascanu", "Jack W Stokes", "Hermineh Sanossian", "Mady Marinescu", "Anil Thomas"], "venue": "Speech and Signal Processing  (ICASSP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Understanding adversarial training: increasing local stability of neural nets through robust optimization", "author": ["Uri Shaham", "Yutaro Yamada", "Sahand Negahban"], "venue": "arXiv preprint arXiv:1511.05432,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deep learning in medical image analysis", "author": ["Dinggang Shen", "GuorongWu", "Heung-Il Suk"], "venue": "Annual Review of Biomedical Engineering,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}, {"title": "Deep learning for mortgage risk, 2016", "author": ["Justin Sirignano", "Apaar Sadhwani", "Kay Giesecke"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Striving for simplicity: the all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian J. Goodfellow", "Rob Fergus"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Adversarial examples for semantic segmentation and object detection", "author": ["Cihang Xie", "Jianyu Wang", "Zhishuai Zhang", "Yuyin Zhou", "Lingxi Xie", "Alan Yuille"], "venue": "arXiv preprint arXiv:1703.08603,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "In recent years, deep learning demonstrated impressive performance on many tasks, such as image classification [8] and natural language processing [15].", "startOffset": 111, "endOffset": 114}, {"referenceID": 14, "context": "In recent years, deep learning demonstrated impressive performance on many tasks, such as image classification [8] and natural language processing [15].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "However, recent research showed that an attacker could generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 105, "endOffset": 120}, {"referenceID": 4, "context": "However, recent research showed that an attacker could generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 105, "endOffset": 120}, {"referenceID": 22, "context": "However, recent research showed that an attacker could generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 105, "endOffset": 120}, {"referenceID": 17, "context": "However, recent research showed that an attacker could generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 105, "endOffset": 120}, {"referenceID": 32, "context": "We can divide defenses against adversarial examples into three approaches: (1) Training the target classifier with adversarial examples, called adversarial training [33, 5]; (2) Training a classifier to distinguish between normal and adversarial examples [19]; and (3) Making target classifiers hard to attack by blocking gradient pathway, e.", "startOffset": 165, "endOffset": 172}, {"referenceID": 4, "context": "We can divide defenses against adversarial examples into three approaches: (1) Training the target classifier with adversarial examples, called adversarial training [33, 5]; (2) Training a classifier to distinguish between normal and adversarial examples [19]; and (3) Making target classifiers hard to attack by blocking gradient pathway, e.", "startOffset": 165, "endOffset": 172}, {"referenceID": 18, "context": "We can divide defenses against adversarial examples into three approaches: (1) Training the target classifier with adversarial examples, called adversarial training [33, 5]; (2) Training a classifier to distinguish between normal and adversarial examples [19]; and (3) Making target classifiers hard to attack by blocking gradient pathway, e.", "startOffset": 255, "endOffset": 259}, {"referenceID": 23, "context": ", defensive distillation [24].", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "showed that defensive distillation did not significantly increase the robustness of neural networks [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 21, "context": "Researchers speculate that for many AI tasks, their relevant data lie on a manifold that is of much lower dimension than the full sample space [22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "In this setting, we evaluated MagNet on popular attacks [25, 21, 2].", "startOffset": 56, "endOffset": 67}, {"referenceID": 20, "context": "In this setting, we evaluated MagNet on popular attacks [25, 21, 2].", "startOffset": 56, "endOffset": 67}, {"referenceID": 1, "context": "In this setting, we evaluated MagNet on popular attacks [25, 21, 2].", "startOffset": 56, "endOffset": 67}, {"referenceID": 0, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 60, "endOffset": 69}, {"referenceID": 2, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 60, "endOffset": 69}, {"referenceID": 3, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 60, "endOffset": 69}, {"referenceID": 30, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 29, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 141, "endOffset": 149}, {"referenceID": 27, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 141, "endOffset": 149}, {"referenceID": 9, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 182, "endOffset": 190}, {"referenceID": 11, "context": "They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [31], medical treatments [30], information security [11, 28], and human-computer interaction [10, 12].", "startOffset": 182, "endOffset": 190}, {"referenceID": 1, "context": "Recent work has demonstrated the feasibility of attacking such systems with carefully crafted input [2, 27, 7] for real-world systems.", "startOffset": 100, "endOffset": 110}, {"referenceID": 26, "context": "Recent work has demonstrated the feasibility of attacking such systems with carefully crafted input [2, 27, 7] for real-world systems.", "startOffset": 100, "endOffset": 110}, {"referenceID": 6, "context": "Recent work has demonstrated the feasibility of attacking such systems with carefully crafted input [2, 27, 7] for real-world systems.", "startOffset": 100, "endOffset": 110}, {"referenceID": 32, "context": "More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 112, "endOffset": 127}, {"referenceID": 4, "context": "More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 112, "endOffset": 127}, {"referenceID": 22, "context": "More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 112, "endOffset": 127}, {"referenceID": 17, "context": "More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers [33, 5, 23, 18].", "startOffset": 112, "endOffset": 127}, {"referenceID": 1, "context": "Since it is hard to model human perception, researchers proposed three popular metrics to approximate human\u2019s perception of visual difference, namely L0, L2, and L\u221e [2].", "startOffset": 165, "endOffset": 168}, {"referenceID": 32, "context": "Since the discovery of adversarial examples for neural networks in [33], researchers found adversarial examples on various network architectures including feedforward convolutional classification networks [2], generative networks [13], and recurrent networks [26].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "Since the discovery of adversarial examples for neural networks in [33], researchers found adversarial examples on various network architectures including feedforward convolutional classification networks [2], generative networks [13], and recurrent networks [26].", "startOffset": 205, "endOffset": 208}, {"referenceID": 12, "context": "Since the discovery of adversarial examples for neural networks in [33], researchers found adversarial examples on various network architectures including feedforward convolutional classification networks [2], generative networks [13], and recurrent networks [26].", "startOffset": 230, "endOffset": 234}, {"referenceID": 25, "context": "Since the discovery of adversarial examples for neural networks in [33], researchers found adversarial examples on various network architectures including feedforward convolutional classification networks [2], generative networks [13], and recurrent networks [26].", "startOffset": 259, "endOffset": 263}, {"referenceID": 20, "context": ", classification[21] and semantic segmentation [34].", "startOffset": 16, "endOffset": 20}, {"referenceID": 33, "context": ", classification[21] and semantic segmentation [34].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "Researchers developed several methods for generating adversarial examples, most of which leveraged gradient based optimization from normal examples [2, 33, 5].", "startOffset": 148, "endOffset": 158}, {"referenceID": 32, "context": "Researchers developed several methods for generating adversarial examples, most of which leveraged gradient based optimization from normal examples [2, 33, 5].", "startOffset": 148, "endOffset": 158}, {"referenceID": 4, "context": "Researchers developed several methods for generating adversarial examples, most of which leveraged gradient based optimization from normal examples [2, 33, 5].", "startOffset": 148, "endOffset": 158}, {"referenceID": 19, "context": "showed that it was even possible to find one effective universal adversarial perturbation that, when applied, turned many images adversarial [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 4, "context": "Given a normal image x , fast gradient sign method [5] looks for a similar image x \u2032 in the L\u221e neighborhood of x that fools the classifier.", "startOffset": 51, "endOffset": 54}, {"referenceID": 15, "context": "[16] proposed to improve FGSM by using a finer iterative optimization strategy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "DeepFool is also an iterative attack but formalizes the problem in a different way [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "Carlini recently introduced a powerful attack that generates adversarial exampleswith small perturbation [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "such that x + \u03b4 \u2208 [0, 1]n", "startOffset": 18, "endOffset": 24}, {"referenceID": 28, "context": "One idea of defending against adversarial examples is to train a better classifier [29].", "startOffset": 83, "endOffset": 87}, {"referenceID": 32, "context": "For example, one may use a mixture of normal and adversarial examples in the training set to do data augmentation [33, 21], or mix the adversarial objective with the classification objective as regularizer [5].", "startOffset": 114, "endOffset": 122}, {"referenceID": 20, "context": "For example, one may use a mixture of normal and adversarial examples in the training set to do data augmentation [33, 21], or mix the adversarial objective with the classification objective as regularizer [5].", "startOffset": 114, "endOffset": 122}, {"referenceID": 4, "context": "For example, one may use a mixture of normal and adversarial examples in the training set to do data augmentation [33, 21], or mix the adversarial objective with the classification objective as regularizer [5].", "startOffset": 206, "endOffset": 209}, {"referenceID": 23, "context": "Defensive distillation [24] trains the classifier in a certain way such that it is nearly impossible for gradient based attacks to generate adversarial examples directly on the network.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "Defensive distillation leverages distillation training techniques [9] and hides the gradient between the pre-softmax layer (logits) and softmax outputs.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "However, [2] showed that it is easy to bypass the defense by adopting one of the three following strategies: (1) choose a more proper loss function (2) calculate gradient directly from pre-softmax layer instead of from post-softmax layer (3) attack an easy-to-attack network first and then transfer to the distilled network.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "Another idea of defense is to detect adversarial examples with hand-crafted statistical features [6] or separate classification networks [19].", "startOffset": 97, "endOffset": 100}, {"referenceID": 18, "context": "Another idea of defense is to detect adversarial examples with hand-crafted statistical features [6] or separate classification networks [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "An representative work of this idea is [19].", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "Researchers believe that Nt constitute a manifold that is of much lower dimension than S [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "showed that many adversarial images generated artificially remain adversarial after being printed and then captured by a camera [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "As an example of this approach, a recent work trained a classifier to distinguish between normal and adversarial examples [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "For example, [19] used a basic iterative attack based on the L2 norm.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "One approach would be to create a robust classifier such that even if the attacker knows all the parameters of the classifier, it would be difficult for her to find adversarial example [24].", "startOffset": 185, "endOffset": 189}, {"referenceID": 1, "context": "However, [2] showed that it was actually easy to find adversarial examples for the classifier hardened in [24].", "startOffset": 9, "endOffset": 12}, {"referenceID": 23, "context": "However, [2] showed that it was actually easy to find adversarial examples for the classifier hardened in [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "We evaluated the accuracy and properties of our defense described in section 4 on two standard dataset: MNIST [17] and CIFAR-10 [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "We evaluated the accuracy and properties of our defense described in section 4 on two standard dataset: MNIST [17] and CIFAR-10 [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "We trained a classifier using the setting in [2] and got an accuracy of 99.", "startOffset": 45, "endOffset": 48}, {"referenceID": 31, "context": "We used the architecture in [32] and got an accuracy of 90.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "We used a scaled range of [0, 1] instead of [0, 255] for simplicity.", "startOffset": 26, "endOffset": 32}, {"referenceID": 17, "context": "Previous work showed that untargeted attack is easier to succeed, results in smaller perturbations, and transfers better to different models [18, 2].", "startOffset": 141, "endOffset": 148}, {"referenceID": 1, "context": "Previous work showed that untargeted attack is easier to succeed, results in smaller perturbations, and transfers better to different models [18, 2].", "startOffset": 141, "endOffset": 148}, {"referenceID": 24, "context": "we used the implementation of Cleverhans [25].", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "For DeepFool and Carlini\u2019s, we used their authors\u2019 open source implementations [21, 2].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "For DeepFool and Carlini\u2019s, we used their authors\u2019 open source implementations [21, 2].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "Carlini showed that it was viable to mount transfer attack with higher confidence on MNIST [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 23, "context": "Among the attacks that we evaluated, Carlini\u2019s attack is the most interesting because it is the most effective on the distillation defense [24] and there is no known effective defense prior to our work.", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "For MNIST, we used the same classifier as in Carlini\u2019s paper [2] for generating adversarial examples and as the target classifier in our evaluation.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "For CIFAR-10, [2] did not evaluate the impact of confidence level, but we picked confidence levels in the range of [0, 100].", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "Therefore, the attack as described in [2] cannot handle MagNet.", "startOffset": 38, "endOffset": 41}], "year": 2017, "abstractText": "Deep learning has shown promising results on hard perceptual problems in recent years. However, deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human. Such specially crafted perturbations cause deep learning systems to output incorrect decisions, with potentially disastrous consequences. These vulnerabilities hinder the deployment of deep learning systems where safety or security is important. Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective. In this paper, we propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet does not modify the protected classifier or know the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. Different from previous work, MagNet learns to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since it does not rely on any process for generating adversarial examples, it has substantial generalization power. Moreover, MagNet reconstructs adversarial examples by moving them towards the manifold, which is effective for helping classify adversarial examples with small perturbation correctly. We discuss the intrinsic difficulty in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography, we propose to use diversity to strengthen MagNet. We show empirically that MagNet is effective against most advanced state-of-the-art attacks in blackbox and graybox scenarios while keeping false positive rate on normal examples very low.", "creator": "LaTeX with hyperref package"}}}