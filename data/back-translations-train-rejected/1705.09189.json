{"id": "1705.09189", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs", "abstract": "We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task.", "histories": [["v1", "Thu, 25 May 2017 14:09:48 GMT  (113kb,D)", "http://arxiv.org/abs/1705.09189v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jean maillard", "stephen clark", "dani yogatama"], "accepted": false, "id": "1705.09189"}, "pdf": {"name": "1705.09189.pdf", "metadata": {"source": "CRF", "title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs", "authors": ["Jean Maillard", "Stephen Clark"], "emails": ["jean@maillard.it", "sc609@cam.ac.uk", "dyogatama@google.com"], "sections": [{"heading": "1 Introduction", "text": "Recurring neural networks, in particular the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) and some of its variants (Bahdanau et al., 2014, Graves and Schmidhuber, 2005) have been widely applied to problems in processing natural language, such as language modeling (J\u00f3zefowicz et al., 2016, Sundermeyer et al., 2012), textual analogy (Bowman et al., 2015, Sha et al., 2016), and machine translation (Bahdanau et al., 2014, Sutskever et al., 2014). The topology of an LSTM network is linear: words are read sequentially, usually in left order. However, language is known to have an underlying hierarchical, tree-like structure (Chomsky et al, 1957)."}, {"heading": "2 Related work", "text": "Our work is seen as part of a broader class of models that determine their composition by a tree structure, which can be divided into two groups: (1) models based on traditional syntactic structures are usually used as input, and (2) models that have a tree structure. (3) In the first group, they are used as inspiration for semantic treatment. (4) These models are used as vehicles. (4)"}, {"heading": "3 Models", "text": "All models take a sentence as input, represented as an ordered sequence of words. Each word wi-V in the vocabulary is encoded as a (learned) word embedding wi-Rd. The models then output a sentence representation h-RD, where the output space RD does not necessarily match the input space Rd."}, {"heading": "3.1 Bag of Words", "text": "Our simplest baseline is a sack-of-words (BoW) model. Due to its dependence on addition, which is commutative, any information about the original word sequence is lost. In the face of a sentence encoded by embedding w1,..., wn it computesh = n \u0445 i = 1 tanh (Wwi + b), where W is a learned input projection matrix."}, {"heading": "3.2 LSTM", "text": "An obvious choice for a baseline is the popular Long Short-Term Memory (LSTM) architecture by Hochreiter and Schmidhuber (1997). It is a recursive neural network that runs for T time steps t = 1... T and itf tutot = Wwt + Uht \u2212 1 + b, ct = ct \u2212 1 \u03c3 (f t) + tanh (ut) \u03c3 (it), ht = \u03c3 (ot) tanh (ct), where \u03c3 (x) = 11 + e \u2212 x is the standard logistic function. LSTM is parameterized by the matrices W-R4D \u00b7 d, U-R4D \u00b7 D and the bias vector b-R4D. The vectors \u043c (it), \u03c3 (ot) and RD are known as input, forgot and output gates, while we call the vector R4b-RD-RD and 4b-RD-Bitor."}, {"heading": "3.3 Tree-LSTM", "text": "Tree LSTMs are a family of extensions of the LSTM architecture to tree structures (Tai et al., 2015, Zhu et al., 2015). We implement the version designed for binary constituency trees. Faced with a node called L and R, its representation is calculated as i fL fR u = Ww + UhL + VhR + b, (1) c = cL \u03c3 (fL) + cR \u03c3 (fR) + tanh (u) \u03c3 (i), (2) h = \u03c3 (o) tanh (c), (3), where w in (1) is an embedded word, only nonequal to zero on the leaves of the parse tree; and hL, hR and cL, cR, cR, are the node states of the children h and c states, only nonequal to zero on the branches. These calculations are repeated recursively after the tree structure, and the whole sentence is forgotten by the preceding state of the M, we also enter the root node in the STM."}, {"heading": "3.4 Unsupervised Tree-LSTM", "text": "This year it is more than ever before."}, {"heading": "4 Experiments", "text": "All models are implemented in Python 3.5.2 with the DyNet library for neural networks (Neubig et al., 2017) at commit bffe22b.The code for all subsequent experiments can be found on the first author's website.1For training, we use stochastic gradient pedigree with a batch size of 16 that has proven to be more powerful than AdaGrad (Duchi et al., 2010) and similar methods on our development data.The performance from the development data is used to determine when to finish the training.The textual entenment model was trained on a 2.2GHz Intel Xeon E5-2660 CPU and took one and a half weeks to converge. The reverse dictionary model was trained on an NVIDIA GeForce GTX TITAN Black GPU and took five days to convergence.Above the baselines already described in \u00a7 3, we also train for the following experiments, two more tree-branch composition, one tree-tree composite, and one fully branched tree composition."}, {"heading": "4.1 Textual Entailment", "text": "We are testing our model and baselines on the Stanford Natural Language Inference Task (Bowman et al., 2015), which consists of 570 k manually annotated pairs of sentences. In two sentences, the goal is to predict whether the first will result in embedding, contradict, or be neutral in relation to the second. For example, if \"children smile and wave at the camera\" and \"children are present,\" the model is expected to predict the effects. In this experiment, we select 100 x 37 369 word embeddings that are initialized during the training with 100D GloVe vectors (Pennington et al., 2014) and with words from the vocabulary set to the average of all other vectors, resulting in a 100 x 37 369 word embeddings matrix that perform fine-tuning during the training.For the monitored Tree LSTM model, we used the parameter trees contained in the data sets."}, {"heading": "4.2 Reverse Dictionary", "text": "We test our model and our fundamentals on the reverse dictionary problem by Hill et al. (2016), which consists of 852 k word-definition pairs. The goal is to retrieve the name of a concept from a list of words, given its definition. For example, if using the phrase \"control consisting of a mechanical device for controlling the flow of fluid,\" a model would be expected to rank the word \"valve\" ahead of other founders in a list. We use three test sentences provided by the authors: two sets of word definitions, either seen or held during training; and a set of concept descriptions instead of formal definitions. Performance is measured using three statistics: the mean rank of the correct answer to a list of 66 k words; and the ratio of cases in which the correct answer appears in the top 10 and 100 ranked words (top 10 accuracy and top 100 accuracy)."}, {"heading": "5 Discussion", "text": "The results in Tables 2 and 3 show that the unattended tree LSTM is comparable to all tested baselines. For the textual dewatering task, our model compares positively with all baselines, including the monitored tree LSTM, and we make some concrete suggestions in this direction. In the reverse dictionary task, the very poor performance of the monitored tree LSTM can be plausibly explained by the unusual association with other models."}, {"heading": "6 Conclusions", "text": "We presented a fully differentiated model to jointly learn sentence embedding and syntax, based on the tree-LSTM composition function. We demonstrated its advantages over traditional tree-LSTM using a textual task and a reverse dictionary task. The model is conceptually simple and can be improved by back propagation and stochastic gradient descent with popular dynamic computational graphs such as DyNet (Neubig et al., 2017) and PyTorch.3The unattended tree-LSTM we presented is relatively simple, but could plausibly be improved by combining it with aspects of other models. In particular, it should be noted that (4) the allocation of energy to alternative ways of building components is extremely basic and does not rely on any global information about the set. The use of a more complex function, possibly based on a mechanism such as the tracking of LSTM in Bowman et al. (2016), could lead to improvements in performance."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "CoRR, abs/1508.05326,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Programming Languages and Their Compilers: Preliminary Notes", "author": ["John Cocke"], "venue": "Courant Institute of Mathematical Sciences,", "citeRegEx": "Cocke.,? \\Q1969\\E", "shortCiteRegEx": "Cocke.", "year": 1969}, {"title": "Mathematical foundations for a compositional distributed model of meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark"], "venue": "Linguistic Analysis,", "citeRegEx": "Coecke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24, EECS Department,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["Felix Hill", "KyungHyun Cho", "Anna Korhonen", "Yoshua Bengio"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "editors, ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "CoRR, abs/1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "An efficient recognition and syntax analysis algorithm for context-free languages", "author": ["T. Kasami"], "venue": "Technical Report AFCRL-65-758,", "citeRegEx": "Kasami.,? \\Q1965\\E", "shortCiteRegEx": "Kasami.", "year": 1965}, {"title": "Structured Attention Networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "venue": "In ICLR", "citeRegEx": "Kim et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Easy-first dependency parsing with hierarchical tree lstms", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "TACL, 4:445\u2013461,", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A. Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska", "Demis Hassabis", "Claudia Clopath", "Dharshan Kumaran", "Raia Hadsell"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kirkpatrick et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 2017}, {"title": "The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization", "author": ["Phong Le", "Willem Zuidema"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Le and Zuidema.,? \\Q2015\\E", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds and Machines,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "URL http://www. aclweb.org/anthology/P/P14/P14-5010", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural semantic encoders. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 397\u2013407", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": null, "citeRegEx": "Munkhdalai and Yu.,? \\Q2017\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2017}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni"], "venue": null, "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reading and thinking: Re-read LSTM unit for textual entailment recognition", "author": ["Lei Sha", "Baobao Chang", "Zhifang Sui", "Sujian Li"], "venue": "COLING", "citeRegEx": "Sha et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sha et al\\.", "year": 2016}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "The Syntactic Process", "author": ["Mark Steedman"], "venue": null, "citeRegEx": "Steedman.,? \\Q2000\\E", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "INTERSPEECH", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Learning to compose words into sentences with reinforcement learning. 2016", "author": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "venue": "URL http://arxiv.org/ abs/1611.09100", "citeRegEx": "Yogatama et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 1967}, {"title": "Long short-term memory over recursive structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Recurrent neural networks, in particular the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) and some of its variants (Bahdanau et al.", "startOffset": 88, "endOffset": 122}, {"referenceID": 32, "context": "These can be either provided by an automatic parser (Tai et al., 2015), or taken from a gold-standard resource such as the Penn Treebank (Kiperwasser and Goldberg, 2016).", "startOffset": 52, "endOffset": 70}, {"referenceID": 16, "context": ", 2015), or taken from a gold-standard resource such as the Penn Treebank (Kiperwasser and Goldberg, 2016).", "startOffset": 74, "endOffset": 106}, {"referenceID": 16, "context": ", 2015), or taken from a gold-standard resource such as the Penn Treebank (Kiperwasser and Goldberg, 2016). Yogatama et al. (2016) proposed to remove this requirement by including a shift-reduce parser in the model, to be optimised alongside the composition function based on a downstream task.", "startOffset": 75, "endOffset": 131}, {"referenceID": 33, "context": "Our model is also unsupervised with respect to the parse trees, similar to Yogatama et al. (2016). We show that the proposed method outperforms other Tree-LSTM architectures based on fully left-branching, right-branching, and supervised parse trees on a textual entailment task and a reverse dictionary task.", "startOffset": 75, "endOffset": 98}, {"referenceID": 4, "context": "They model nouns as vectors, and relational words that take arguments (such as adjectives, that combine with nouns) as tensors, with tensor contraction representing application (Coecke et al., 2011).", "startOffset": 177, "endOffset": 198}, {"referenceID": 23, "context": "In the first group, Paperno et al. (2014) take inspiration from the standard Montagovian semantic treatment of composition.", "startOffset": 20, "endOffset": 42}, {"referenceID": 4, "context": "They model nouns as vectors, and relational words that take arguments (such as adjectives, that combine with nouns) as tensors, with tensor contraction representing application (Coecke et al., 2011). These tensors are trained via linear regression based on a downstream task, but the tree that determines their order of application is expected to be provided as input. Socher et al. (2012) and Socher et al.", "startOffset": 178, "endOffset": 390}, {"referenceID": 4, "context": "They model nouns as vectors, and relational words that take arguments (such as adjectives, that combine with nouns) as tensors, with tensor contraction representing application (Coecke et al., 2011). These tensors are trained via linear regression based on a downstream task, but the tree that determines their order of application is expected to be provided as input. Socher et al. (2012) and Socher et al. (2013) also rely on external trees, but use recursive neural networks as the composition function.", "startOffset": 178, "endOffset": 415}, {"referenceID": 17, "context": "Instead of using a single parse tree, Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty.", "startOffset": 38, "endOffset": 60}, {"referenceID": 17, "context": "Instead of using a single parse tree, Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty. The authors use a convolutional neural network composition function and, like our model, rely on a mechanism similar to the one employed by the CYK parser to process the trees. Ma et al. (2015) propose a related model, also making use of syntactic information and convolutional networks to obtain a representation in a bottom-up manner.", "startOffset": 38, "endOffset": 365}, {"referenceID": 13, "context": "Convolutional neural networks can also be used to produce embeddings without the use of tree structures, such as in Kalchbrenner et al. (2014).", "startOffset": 116, "endOffset": 143}, {"referenceID": 30, "context": "In the second group, Yogatama et al. (2016) shows the most similarities to our proposed model.", "startOffset": 21, "endOffset": 44}, {"referenceID": 1, "context": "The authors use reinforcement learning to learn tree structures for a neural network model similar to Bowman et al. (2016), taking performance on a downstream task that uses the computed sentence representations as the reward signal.", "startOffset": 102, "endOffset": 123}, {"referenceID": 1, "context": "The authors use reinforcement learning to learn tree structures for a neural network model similar to Bowman et al. (2016), taking performance on a downstream task that uses the computed sentence representations as the reward signal. Kim et al. (2017) take a slightly different approach: they formalise a dependency parser as a graphical model, viewed as an extension to attention mechanisms, and hand-optimise the backpropagation step through the inference algorithm.", "startOffset": 102, "endOffset": 252}, {"referenceID": 9, "context": "An obvious choice for a baseline is the popular Long Short-Term Memory (LSTM) architecture of Hochreiter and Schmidhuber (1997). It is a recurrent neural network that, given a sentence encoded by embeddings w1, .", "startOffset": 94, "endOffset": 128}, {"referenceID": 11, "context": "Following the recommendation of Jozefowicz et al. (2015), we deviate slightly from the vanilla LSTM architecture described above by also adding a bias of 1 to the forget gate, which was found to improve performance.", "startOffset": 32, "endOffset": 57}, {"referenceID": 5, "context": "For training we use stochastic gradient descent with a batch size of 16, which was found to perform better than AdaGrad (Duchi et al., 2010) and similar methods on our development data.", "startOffset": 120, "endOffset": 140}, {"referenceID": 1, "context": "We test our model and baselines on the Stanford Natural Language Inference task (Bowman et al., 2015), consisting of 570 k manually annotated pairs of sentences.", "startOffset": 80, "endOffset": 101}, {"referenceID": 25, "context": "For this experiment, we choose 100D input embeddings, initialised with 100D GloVe vectors (Pennington et al., 2014) and with out-of-vocabulary words set to the average of all other vectors.", "startOffset": 90, "endOffset": 115}, {"referenceID": 23, "context": "For this experiment, we choose 100D input embeddings, initialised with 100D GloVe vectors (Pennington et al., 2014) and with out-of-vocabulary words set to the average of all other vectors. This results in a 100\u00d7 37 369 word embedding matrix, fine-tuned during training. For the supervised Tree-LSTM model, we used the parse trees included in the dataset. Given a pair of sentences, one of the models is used to produce the embeddings s1, s2 \u2208 R. Following Yogatama et al. (2016) and Bowman et al.", "startOffset": 91, "endOffset": 480}, {"referenceID": 1, "context": "(2016) and Bowman et al. (2016), we then compute u = (s1 \u2212 s2), v = s1 s2,", "startOffset": 11, "endOffset": 32}, {"referenceID": 8, "context": "We also test our model and baselines on the reverse dictionary task of Hill et al. (2016), which consists of 852 k word-definition pairs.", "startOffset": 71, "endOffset": 90}, {"referenceID": 22, "context": "As output embeddings, we use the 500D CBOW vectors (Mikolov et al., 2013) provided by the authors.", "startOffset": 51, "endOffset": 73}, {"referenceID": 21, "context": "For the supervised Tree-LSTM model, we additionally parsed the definitions with Stanford CoreNLP (Manning et al., 2014) to obtain parse trees.", "startOffset": 97, "endOffset": 119}, {"referenceID": 8, "context": "Table 3 shows the results for our model and baselines, as well as the models of Hill et al. (2016) which are based on the same cosine training objective.", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "In the reversed dictionary task, the very poor performance of the supervised Tree-LSTM can be explained by the unusual tokenisation algorithm used in the dataset of Hill et al. (2016): all punctuation is simply stripped, turning for instance \u201c(archaic) a section of a poem\u201d into \u201carchaic a section of a poem\u201d, or stripping away the semicolons in long lists of synonyms.", "startOffset": 165, "endOffset": 184}, {"referenceID": 29, "context": "Type-raising and composition in formalisms such as combinatory categorial grammar (Steedman, 2000) do however allow such constituents.", "startOffset": 82, "endOffset": 98}, {"referenceID": 32, "context": "Following Yogatama et al. (2016), we also manually inspect the learned trees to see how closely they match conventional syntax trees, as would typically be assigned by trained linguists.", "startOffset": 10, "endOffset": 33}, {"referenceID": 10, "context": "Techniques such as batch normalization (Ioffe and Szegedy, 2015) or layer normalization (Ba et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 1, "context": "Using a more complex function, perhaps relying on a mechanism such as the tracking LSTM in Bowman et al. (2016), might lead to improvements in performance.", "startOffset": 91, "endOffset": 112}, {"referenceID": 19, "context": "In future work, it might be possible to obtain trees closer to human intuition by training a model to perform well on multiple tasks instead of a single one, an important feature for intelligent agents to demonstrate (Legg and Hutter, 2007).", "startOffset": 217, "endOffset": 240}, {"referenceID": 17, "context": "Techniques such as elastic weight consolidation (Kirkpatrick et al., 2017) have been shown to help with multitask learning, and could be readily applied to our model.", "startOffset": 48, "endOffset": 74}], "year": 2017, "abstractText": "We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task.", "creator": "LaTeX with hyperref package"}}}