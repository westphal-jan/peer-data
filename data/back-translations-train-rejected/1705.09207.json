{"id": "1705.09207", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Learning Structured Text Representations", "abstract": "In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.", "histories": [["v1", "Thu, 25 May 2017 14:54:07 GMT  (162kb,D)", "http://arxiv.org/abs/1705.09207v1", null], ["v2", "Thu, 20 Jul 2017 23:14:58 GMT  (176kb,D)", "http://arxiv.org/abs/1705.09207v2", "Comments: 10 pages; typos corrected"], ["v3", "Thu, 14 Sep 2017 20:42:25 GMT  (208kb,D)", "http://arxiv.org/abs/1705.09207v3", "Accepted by TACL"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yang liu", "mirella lapata"], "accepted": false, "id": "1705.09207"}, "pdf": {"name": "1705.09207.pdf", "metadata": {"source": "CRF", "title": "Learning Structured Text Representations", "authors": ["Yang Liu"], "emails": ["Yang.Liu2@ed.ac.uk,", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to integrate themselves, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they are able to"}, {"heading": "2 Background", "text": "In this section, we describe how previous work used the attention mechanism to represent individual sentences, the key idea being to capture the interaction between symbols within a sentence and generate a context representation for each word with weak structural information. This type of intra-sentence attention encodes relationships between words within each sentence and differs from the inter-sentence attention widely applied to sequence transduction tasks such as machine translation (Bahdanau et al., 2015), learning the latent alignment between source and target sequence. Figure 2 provides a schematic view of the intra-sentence attention mechanism represented as a sequence of n-word vectors."}, {"heading": "3 Encoding Text Representations", "text": "In this section, we present our model of document representation, following previous work (Tang et al., 2015a; Yang et al., 2016) in the hierarchical modeling of documents by first obtaining representations for sentences and then assembling them into a document representation. Structural information is taken into account, whereas when learning representations for sentences and documents, an attention mechanism is applied to both words within a sentence and sentences within a document. The general idea is to force the paired attention between text units into a non-projective dependency tree and automatically induce this tree in a differentiated way for various tasks of natural language processing. Below, we will first describe how the attention mechanism is applied to sentences, and then proceed to the presentation of our document model."}, {"heading": "3.1 Sentence Model", "text": "Let's call T = [u0, u1, \u00b7 \u00b7, un] a sentence that contains a sequence of words, each represented by a u vector that can be pre-trained on a large corpus. Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015) to speech recognition (Graves et al., 2013) to image caption generation (Xu et al., 2015). In this paper, we use bidirectional LSTMs as a way to represent elements in a sequence (i.e., words or sentences) together with their contexts that capture the element and an \"infinite\" window around it. Specifically, we run a bidirectional LSTM over the T sentence, and take the output vectors [h0, h1, \u00b7 hn] as the conventional representation of the T where we draw attention."}, {"heading": "3.2 Structured Attention Mechanism", "text": "We use a variant of Kirchhoff's Matrix-Tree Theorem (Koo et al., 2007; Tutte, 1984) to calculate the marginal probability of each edge. (We use a variant of Kirchhoff's Matrix-Tree Theorem (Koo et al., 2007) to calculate the dependence on each edge. (We use a variant of Kirchhoff's Matrix-Tree Theorem (Koo et al., 2007) to calculate the dependence on each edge. (We use a variant of Kirchhoff's Matrix-Tree Theorem (Koo et al., 2007) to calculate the marginal probability of each edge. (We use a variant of Kirchhoff's Matrix-Tree Theorem. (Koo et al., 2007) to calculate the marginal probability of each edge. (We use a variant of Kirchhoff's Matrix-Tree Theorem.)"}, {"heading": "3.3 Document Model", "text": "Dependency-based representations have already been used for the development of discourse savers (Hayashi et al., 2016; Li et al., 2014; Muller et al., 2012) and in applications such as the abstract (Hirao et al., 2013). As shown in Figure 4, the input for each sentence si is a sequence of word embeddings [ui0, ui1, \u00b7 \u00b7, uim], where m is the number of characters in si. By inserting the embeddings into a sentence-level-bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri0, ri1, \u00b7 \u00b7 \u00b7, ripodings], a fixed sentence-vector-1-vector function."}, {"heading": "3.4 End-to-End Training", "text": "In contrast to Kim et al. (2017), the training can be carried out efficiently. The main complexity of our model lies in the calculation of the gradients of the inverse matrix. Let A denote a matrix in relation to a real parameter x; provided that all component functions in A are differentiable and A is invertable for all possible values, the gradient of A is in relation to x: dA \u2212 1dx = \u2212 A \u2212 1dA dx A \u2212 1 (18) Multiplication of the three matrices and matrix inversion can be efficiently calculated on modern parallel hardware architectures such as GPUs. In our experiments, the calculation of structured attention takes only 1 / 10 of the training time."}, {"heading": "4 Experiments", "text": "In this section, we present our experiments to evaluate the performance of our model. Since sentence representations are the basic building blocks of our document model, we first evaluate the performance of structured attention at a sentence level, i.e. natural language reasoning. Then, we evaluate the document-level representations obtained from our model in a variety of classification tasks representing documents of different length, subject matter, and language. Our code is available at https: / / github.com / xxx / xxx."}, {"heading": "4.1 Natural Language Inference", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.2 Document Classification", "text": "We have selected four datasets, which we describe below. Table 3 summarizes some statistics for each dataset.Yelp ratings were each obtained from the 2013 Yelp Dataset Challenge. This dataset contains restaurant ratings, each associated with human ratings on a scale from 1 (negative) to 5 (positive), which we used as gold marks for the sensation classification; we followed the preprocessing used in (Tang et al., 2015a) and reports on their education, development and test partitions (80 / 10 / 10).The dataset contains ratings from Diao et al. (2014), which were randomly crawled ratings for 50K films. Each rating is received with user ratings from 1 to 10.Czech ratings were received by Brychc\u0131n and Habernal (2013).The dataset contains ratings from Czech Movie Datase2, each as neutral, negative or negative."}, {"heading": "4.3 Analysis of Induced Structures", "text": "In this context, it should be noted that the measures in question are measures primarily designed to reduce the impact of climate change on the environment."}, {"heading": "5 Conclusions", "text": "In this paper, we proposed a new model for representing documents while automatically learning rich structural dependencies. Our model normalizes intra-attention scores with the marginal probabilities of a non-projective dependency tree based on a matrix inversion process. Each operation in this process is differentiable and the model can be efficiently trained end-to-end while structural information is generated. We applied this approach to model documents hierarchically, taking into account both sentence and document structures. Experiments on sentence and document modeling tasks show that the representations learned through our model are competitive compared to strong comparison systems. Analyses of induced tree structures showed that they are significant, though different from linguistic ones, without ever exposing the model to linguistic annotations or an external parser."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the ICLR Conference. San Diego, California.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Trainable grammars for speech recognition", "author": ["James K Baker."], "venue": "The Journal of the Acoustical Society of America 65(1):132\u2013132.", "citeRegEx": "Baker.,? 1979", "shortCiteRegEx": "Baker.", "year": 1979}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Better document-level sentiment analysis from RST discourse parsing", "author": ["Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Por-", "citeRegEx": "Bhatia et al\\.,? 2015", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of the 54th Annual Meeting of the As-", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Cross-lingual RST discourse parsing", "author": ["Chlo\u00e9 Braud", "Maximin Coavoux", "Anders S\u00f8gaard."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Vancouver, Canada. To appear.", "citeRegEx": "Braud et al\\.,? 2017", "shortCiteRegEx": "Braud et al\\.", "year": 2017}, {"title": "Unsupervised improving of sentiment analysis using global target context", "author": ["Tom\u00e1\u0161 Brychc\u0131n", "Ivan Habernal."], "venue": "Proceedings of the Recent Advances in Natural Language Processing Conference. Hissar, Bulgaria, pages 122\u2013128.", "citeRegEx": "Brychc\u0131n and Habernal.,? 2013", "shortCiteRegEx": "Brychc\u0131n and Habernal.", "year": 2013}, {"title": "Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory", "author": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski."], "venue": "Current and new directions in discourse and dialogue, Springer, pages 85\u2013112.", "citeRegEx": "Carlson et al\\.,? 2003", "shortCiteRegEx": "Carlson et al\\.", "year": 2003}, {"title": "Distraction-based neural networks for modeling documents", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang."], "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16). New York City,", "citeRegEx": "Chen et al\\.,? 2016a", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Enhancing and combining sequential and tree LSTM for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang."], "venue": "arXiv preprint arXiv:1609.06038 .", "citeRegEx": "Chen et al\\.,? 2016b", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 551\u2013561.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On shortest arborescence of a directed graph", "author": ["Yoeng-Jin Chu", "Tseng-Hong Liu."], "venue": "Scientia Sinica 14(10):1396.", "citeRegEx": "Chu and Liu.,? 1965", "shortCiteRegEx": "Chu and Liu.", "year": 1965}, {"title": "Frustratingly short attention spans in neural language modeling", "author": ["Micha\u0142 Daniluk", "Tim Rockt\u00e4schel", "Johannes Welbl", "Sebastian Riedel."], "venue": "In Proceedings of the ICLR Conference. Toulon, France.", "citeRegEx": "Daniluk et al\\.,? 2017", "shortCiteRegEx": "Daniluk et al\\.", "year": 2017}, {"title": "Jointly modeling aspects, ratings and sentiments for movie recommendation", "author": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J Smola", "Jing Jiang", "Chong Wang."], "venue": "Proceedings of the 20th ACM SIGKDD international con-", "citeRegEx": "Diao et al\\.,? 2014", "shortCiteRegEx": "Diao et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Journal of Research of the national Bureau of Standards B 71(4):233\u2013240.", "citeRegEx": "Edmonds.,? 1967", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Textlevel discourse parsing with rich linguistic features", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Jeju Island, Korea,", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "Proceedings of the", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Empirical comparison of dependency conversions for RST discourse trees", "author": ["Katsuhiko Hayashi", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dia-", "citeRegEx": "Hayashi et al\\.,? 2016", "shortCiteRegEx": "Hayashi et al\\.", "year": 2016}, {"title": "Single-document summarization as a tree knapsack problem", "author": ["Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural", "citeRegEx": "Hirao et al\\.,? 2013", "shortCiteRegEx": "Hirao et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland,", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Neural discourse structure for text categorization", "author": ["Yangfeng Ji", "Noah Smith."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Vancouver, Canada. To appear.", "citeRegEx": "Ji and Smith.,? 2017", "shortCiteRegEx": "Ji and Smith.", "year": 2017}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M Rush."], "venue": "Proceedings of the ICLR Conference. Toulon, France.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Structured prediction models via the matrix-tree theorem", "author": ["Terry Koo", "Amir Globerson", "Xavier Carreras P\u00e9rez", "Michael Collins."], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural", "citeRegEx": "Koo et al\\.,? 2007", "shortCiteRegEx": "Koo et al\\.", "year": 2007}, {"title": "Complexity of dependencies in discourse: Are dependencies in discourse more complex than in syntax", "author": ["Alan Lee", "Rashmi Prasad", "Aravind Joshi", "Nikhil Dinesh", "Bonnie Webber."], "venue": "Proceedings of the 5th International Workshop", "citeRegEx": "Lee et al\\.,? 2006", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Text-level discourse dependency parsing", "author": ["Sujian Li", "Liang Wang", "Ziqiang Cao", "Wenjie Li."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland,", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Automatically evaluating text coherence using discourse relations", "author": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Lin et al\\.,? 2011", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:521\u2013535.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C Mann", "Sandra A Thompson."], "venue": "Text-Interdisciplinary Journal for the Study of Discourse 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "On the complexity of non-projective data-driven dependency parsing", "author": ["Ryan McDonald", "Giorgio Satta."], "venue": "Proceedings of the 10th International Conference on Parsing Technologies. Prague, Czech Republic, pages 121\u2013132.", "citeRegEx": "McDonald and Satta.,? 2007", "shortCiteRegEx": "McDonald and Satta.", "year": 2007}, {"title": "Dependency Syntax: Theory and Practice", "author": ["Igor A. Melc\u0306uk"], "venue": null, "citeRegEx": "Melc\u0306uk.,? \\Q1988\\E", "shortCiteRegEx": "Melc\u0306uk.", "year": 1988}, {"title": "Graphbased coherence modeling for assessing readability", "author": ["Mohsen Mesgar", "Michael Strube."], "venue": "Proceedings of the 4th Joint Conference on Lexical and Computational Semantics. Denver, Colorado, pages 309\u2013318.", "citeRegEx": "Mesgar and Strube.,? 2015", "shortCiteRegEx": "Mesgar and Strube.", "year": 2015}, {"title": "Implicitation of discourse connectives in (machine) translation", "author": ["Thomas Meyer", "Bonnie Webber."], "venue": "Proceedings of the Workshop on Discourse in Machine Translation. Sofia, Bulgaria, pages 19\u201326.", "citeRegEx": "Meyer and Webber.,? 2013", "shortCiteRegEx": "Meyer and Webber.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositiona lity", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S a nd Dean Jeff Corrado."], "venue": "Advances in Neural Information Processing Systems 26, Curran Associates, Inc.,", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Nat-", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Constrained decoding for text-level discourse parsing", "author": ["Philippe Muller", "Stergos Afantenos", "Pascal Denis", "Nicholas Asher."], "venue": "Proceedings of COLING 2012. Mumbai, India, pages 1883\u2013 1900.", "citeRegEx": "Muller et al\\.,? 2012", "shortCiteRegEx": "Muller et al\\.", "year": 2012}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Parikh et al\\.,? 2016", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar,", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The penn discourse treebank 2.0", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind Joshi", "Bonnie Webber"], "venue": "In Proceedings of the 6th International Conference on Language Resources and Evalua-", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "Proceedings of the ICLR Conference. San Juan, Puerto Rico.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal,", "citeRegEx": "Tang et al\\.,? 2015a", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Learning semantic representations of users and products for document level sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and", "citeRegEx": "Tang et al\\.,? 2015b", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "\u00c9l\u00e9ments de Syntaxe Structurale", "author": ["Louis Tesni\u00e9re."], "venue": "Editions Klincksieck.", "citeRegEx": "Tesni\u00e9re.,? 1959", "shortCiteRegEx": "Tesni\u00e9re.", "year": 1959}, {"title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts", "author": ["Matt Thomas", "Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Syd-", "citeRegEx": "Thomas et al\\.,? 2006", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "Discourse-based answering of why-questions", "author": ["Suzan Verberne", "Lou Boves", "Nelleke Oostdijk", "Peter-Arno Coppen."], "venue": "Traitement Automatique des Language, Discours et Document: traitements automatics 47(2):21\u201341.", "citeRegEx": "Verberne et al\\.,? 2007", "shortCiteRegEx": "Verberne et al\\.", "year": 2007}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1512.08849 .", "citeRegEx": "Wang and Jiang.,? 2015", "shortCiteRegEx": "Wang and Jiang.", "year": 2015}, {"title": "Coherence in Natural Language: Data Structures and Applications", "author": ["Florian Wolf", "Edward Gibson."], "venue": "The MIT Press.", "citeRegEx": "Wolf and Gibson.,? 2006", "shortCiteRegEx": "Wolf and Gibson.", "year": 2006}, {"title": "Integrating document clustering and topic modeling", "author": ["Pengtao Xie", "Eric P. Xing."], "venue": "Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence. Bellevue, Washington, pages 694\u2013703.", "citeRegEx": "Xie and Xing.,? 2013", "shortCiteRegEx": "Xie and Xing.", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with vi sual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aa ron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of the 32nd", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Dani Yogatama", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland,", "citeRegEx": "Yogatama and Smith.,? 2014", "shortCiteRegEx": "Yogatama and Smith.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies.", "startOffset": 90, "endOffset": 128}, {"referenceID": 24, "context": "Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies.", "startOffset": 90, "endOffset": 128}, {"referenceID": 50, "context": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al.", "startOffset": 138, "endOffset": 158}, {"referenceID": 9, "context": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 49, "context": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 3, "context": ", 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 47, "context": ", 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013).", "startOffset": 28, "endOffset": 51}, {"referenceID": 35, "context": ", 2007), and machine translation (Meyer and Webber, 2013).", "startOffset": 33, "endOffset": 57}, {"referenceID": 3, "context": "Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016).", "startOffset": 128, "endOffset": 188}, {"referenceID": 23, "context": "Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016).", "startOffset": 128, "endOffset": 188}, {"referenceID": 52, "context": "Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016).", "startOffset": 128, "endOffset": 188}, {"referenceID": 30, "context": "Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al.", "startOffset": 187, "endOffset": 217}, {"referenceID": 28, "context": "Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al.", "startOffset": 226, "endOffset": 267}, {"referenceID": 49, "context": "Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al.", "startOffset": 226, "endOffset": 267}, {"referenceID": 2, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al.", "startOffset": 51, "endOffset": 78}, {"referenceID": 28, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015).", "startOffset": 104, "endOffset": 147}, {"referenceID": 34, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015).", "startOffset": 104, "endOffset": 147}, {"referenceID": 8, "context": "The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.", "startOffset": 48, "endOffset": 91}, {"referenceID": 41, "context": "The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.", "startOffset": 48, "endOffset": 91}, {"referenceID": 22, "context": ", 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Ji and Eisenstein, 2014), and the common use of trees as representations of document structure.", "startOffset": 70, "endOffset": 123}, {"referenceID": 2, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Ji and Eisenstein, 2014), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.", "startOffset": 52, "endOffset": 462}, {"referenceID": 2, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Ji and Eisenstein, 2014), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.", "startOffset": 52, "endOffset": 595}, {"referenceID": 6, "context": ", Spanish, German, Basque, Dutch, and Brazilian Portuguese), they tend to be smaller than their English equivalents and of limited value for modeling purposes (Braud et al., 2017).", "startOffset": 159, "endOffset": 179}, {"referenceID": 43, "context": "The main idea is to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechaar X iv :1 70 5.", "startOffset": 163, "endOffset": 205}, {"referenceID": 0, "context": "nism (Bahdanau et al., 2015).", "startOffset": 5, "endOffset": 28}, {"referenceID": 11, "context": "Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016).", "startOffset": 165, "endOffset": 185}, {"referenceID": 19, "context": "Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006).", "startOffset": 121, "endOffset": 161}, {"referenceID": 26, "context": "Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006).", "startOffset": 121, "endOffset": 161}, {"referenceID": 11, "context": "Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees.", "startOffset": 166, "endOffset": 205}, {"referenceID": 11, "context": "Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees. Specifically, they take into account the dependency structure of a sentence by viewing the attention mechanism as a graphical model over latent variables. They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the forward-backward algorithm to normalize the scores with the marginal probabilities of a dependency tree. Without recourse to an external parser, their model learns meaningful taskspecific dependency structures, achieving competitive results in several sentence-level tasks. For document modeling, this approach has two drawbacks. Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006). As illustrated in Figure 1, the tree structure of a document can be flexible and the dependency edges may cross. Secondly, the forwardbackward pass in Kim et al. (2017) has complexity O(n3) with n being the number of text units, and without an efficient parallelizable implementation it is impractical for modeling documents.", "startOffset": 166, "endOffset": 1327}, {"referenceID": 30, "context": "Figure 1: The document is analyzed in the style of Rhetorical Structure Theory (Mann and Thompson, 1988), and represented as dependency tree following the conversion algorithm of Hayashi et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 19, "context": "Figure 1: The document is analyzed in the style of Rhetorical Structure Theory (Mann and Thompson, 1988), and represented as dependency tree following the conversion algorithm of Hayashi et al. (2016).", "startOffset": 179, "endOffset": 201}, {"referenceID": 32, "context": "Advantageously, it can induce nonprojective structures which are required for representing languages with free or flexible word order (McDonald and Satta, 2007).", "startOffset": 134, "endOffset": 160}, {"referenceID": 0, "context": "This type of intra-sentence attention encodes relationships between words within each sentence and differs from inter-sentence attention which has been widely applied to sequence transduction tasks like machine translation (Bahdanau et al., 2015) and learns the latent alignment between source and target sequences.", "startOffset": 223, "endOffset": 246}, {"referenceID": 11, "context": "Despite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 39, "context": ", 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies.", "startOffset": 35, "endOffset": 56}, {"referenceID": 1, "context": "Specifically, they normalize fij with a projective dependency tree using the forward-backward pass in the inside-outside algorithm (Baker, 1979):", "startOffset": 131, "endOffset": 144}, {"referenceID": 10, "context": "Despite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies. Since attention is computed as a simple probability distribution, it cannot capture structural dependencies such as trees (or graphs). Kim et al. (2017) induce richer internal structure by imposing structural constraints on the probability distribution computed by the attention mechanism.", "startOffset": 87, "endOffset": 404}, {"referenceID": 43, "context": "We follow previous work (Tang et al., 2015a; Yang et al., 2016) in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation.", "startOffset": 24, "endOffset": 63}, {"referenceID": 52, "context": "We follow previous work (Tang et al., 2015a; Yang et al., 2016) in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation.", "startOffset": 24, "endOffset": 63}, {"referenceID": 21, "context": "Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al.", "startOffset": 39, "endOffset": 80}, {"referenceID": 0, "context": "Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition Graves et al.", "startOffset": 180, "endOffset": 203}, {"referenceID": 51, "context": "(2013), and image caption generation (Xu et al., 2015).", "startOffset": 37, "endOffset": 54}, {"referenceID": 0, "context": "Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition Graves et al. (2013), and image caption generation (Xu et al.", "startOffset": 181, "endOffset": 248}, {"referenceID": 13, "context": "Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSMT output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts:", "startOffset": 24, "endOffset": 67}, {"referenceID": 37, "context": "Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSMT output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts:", "startOffset": 24, "endOffset": 67}, {"referenceID": 33, "context": "Much work in descriptive linguistics (Melc\u0306uk, 1988; Tesni\u00e9re, 1959) has advocated their suitability for representing syntactic structure across languages.", "startOffset": 37, "endOffset": 68}, {"referenceID": 45, "context": "Much work in descriptive linguistics (Melc\u0306uk, 1988; Tesni\u00e9re, 1959) has advocated their suitability for representing syntactic structure across languages.", "startOffset": 37, "endOffset": 68}, {"referenceID": 25, "context": "We use a variant of Kirchhoff\u2019s Matrix-Tree Theorem (Koo et al., 2007; Tutte, 1984) to calculate the marginal probability of each dependency edge P (zij = 1) of a non-projective dependency tree, and this probability is used as the attention weight that decides how much information is collected from child unit j to the parent unit i.", "startOffset": 52, "endOffset": 83}, {"referenceID": 25, "context": "lowing Koo et al. (2007):", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "Details of the proof can be found in (Koo et al., 2007).", "startOffset": 37, "endOffset": 55}, {"referenceID": 20, "context": ", 2012) and in applications such as summarization (Hirao et al., 2013).", "startOffset": 50, "endOffset": 70}, {"referenceID": 24, "context": "In contrast to in Kim et al. (2017), training can be done efficiently.", "startOffset": 18, "endOffset": 36}, {"referenceID": 4, "context": "For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation.", "startOffset": 77, "endOffset": 98}, {"referenceID": 39, "context": "Sentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows.", "startOffset": 154, "endOffset": 175}, {"referenceID": 40, "context": "We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings.", "startOffset": 36, "endOffset": 61}, {"referenceID": 15, "context": "All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the learning rate was set to 0.", "startOffset": 69, "endOffset": 89}, {"referenceID": 5, "context": "It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2016b).", "startOffset": 109, "endOffset": 150}, {"referenceID": 10, "context": "It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2016b).", "startOffset": 109, "endOffset": 150}, {"referenceID": 11, "context": "Consistent with previous work (Cheng et al., 2016; Parikh et al., 2016), we observe that simple attention brings performance improvements over no attention.", "startOffset": 30, "endOffset": 71}, {"referenceID": 39, "context": "Consistent with previous work (Cheng et al., 2016; Parikh et al., 2016), we observe that simple attention brings performance improvements over no attention.", "startOffset": 30, "endOffset": 71}, {"referenceID": 7, "context": "Exceptions include Cheng et al. (2016) and Parikh et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 7, "context": "Exceptions include Cheng et al. (2016) and Parikh et al. (2016) whose models include intra-attention encoding relationships between words within each sentence (see Equation (2)).", "startOffset": 19, "endOffset": 64}, {"referenceID": 4, "context": "It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2016b). The second block of Table 1 presents a version of our model without an intra-sentential attention mechanism as well as three variants with attention, assuming the structure of word-to-word relations and dependency trees. In the latter case we compare our matrix inversion based model against Kim et al.\u2019s (2017) forward-backward attention model.", "startOffset": 110, "endOffset": 464}, {"referenceID": 4, "context": "Classifier with handcrafted features (Bowman et al., 2015) 78.", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": "2 \u2014 300D LSTM encoders (Bowman et al., 2015) 80.", "startOffset": 23, "endOffset": 44}, {"referenceID": 5, "context": "0M 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 83.", "startOffset": 54, "endOffset": 75}, {"referenceID": 42, "context": "7M 100D LSTM with inter-attention (Rockt\u00e4schel et al., 2016) 83.", "startOffset": 34, "endOffset": 60}, {"referenceID": 48, "context": "5 252K 200D Matching LSTMs (Wang and Jiang, 2015) 86.", "startOffset": 27, "endOffset": 49}, {"referenceID": 11, "context": "9M 450D LSTMN with deep attention fusion (Cheng et al., 2016) 86.", "startOffset": 41, "endOffset": 61}, {"referenceID": 39, "context": "4M 200D Decomposable Attention over word embeddings (Parikh et al., 2016) 86.", "startOffset": 52, "endOffset": 73}, {"referenceID": 10, "context": "8 582K Enhanced BiLSTM Inference Model (Chen et al., 2016b) 88.", "startOffset": 39, "endOffset": 59}, {"referenceID": 10, "context": "(2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2016b; Cheng et al., 2016; Parikh et al., 2016).", "startOffset": 86, "endOffset": 147}, {"referenceID": 11, "context": "(2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2016b; Cheng et al., 2016; Parikh et al., 2016).", "startOffset": 86, "endOffset": 147}, {"referenceID": 39, "context": "(2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2016b; Cheng et al., 2016; Parikh et al., 2016).", "startOffset": 86, "endOffset": 147}, {"referenceID": 21, "context": "own model with tree matrix inversion slightly outperforms the forward-backward model of Kim et al. (2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 43, "context": "This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in (Tang et al., 2015a) and report experiments on their training, development, and testing partitions (80/10/10).", "startOffset": 225, "endOffset": 245}, {"referenceID": 14, "context": "IMDB reviews were obtained from Diao et al. (2014), who randomly crawled reviews for 50K movies.", "startOffset": 32, "endOffset": 51}, {"referenceID": 7, "context": "Experiments on this dataset perform 10-fold cross-validation following previous work (Brychc\u0131n and Habernal, 2013).", "startOffset": 85, "endOffset": 114}, {"referenceID": 7, "context": "Czech reviews were obtained from Brychc\u0131n and Habernal (2013). The dataset contains reviews from the Czech Movie Database2 each labeled as positive, neutral, or negative.", "startOffset": 33, "endOffset": 62}, {"referenceID": 43, "context": "0 \u2014 Paragraph vector (Tang et al., 2015a) 57.", "startOffset": 21, "endOffset": 41}, {"referenceID": 43, "context": "1 \u2014 \u2014- \u2014 Convolutional neural network (Tang et al., 2015a) 59.", "startOffset": 38, "endOffset": 58}, {"referenceID": 43, "context": "7 \u2014 \u2014 \u2014 \u2014 Convolutional gated RNN (Tang et al., 2015a) 63.", "startOffset": 34, "endOffset": 54}, {"referenceID": 43, "context": "5 \u2014 \u2014 \u2014 LSTM gated RNN (Tang et al., 2015a) 65.", "startOffset": 23, "endOffset": 43}, {"referenceID": 23, "context": "3 \u2014 \u2014 \u2014 RST-based recursive neural network (Ji and Smith, 2017) \u2014 \u2014 \u2014 75.", "startOffset": 43, "endOffset": 63}, {"referenceID": 52, "context": "7 \u2014 75D Hierarchical attention networks (Yang et al., 2016) 68.", "startOffset": 40, "endOffset": 59}, {"referenceID": 42, "context": "Regarding feature-based classification methods, results on Yelp and IBDM are taken from Tang et al. (2015a), on CZ movies from Brychc\u0131n and Habernal (2013), and Debates from Yogatama and Smith (2014).", "startOffset": 88, "endOffset": 108}, {"referenceID": 7, "context": "(2015a), on CZ movies from Brychc\u0131n and Habernal (2013), and Debates from Yogatama and Smith (2014).", "startOffset": 27, "endOffset": 56}, {"referenceID": 7, "context": "(2015a), on CZ movies from Brychc\u0131n and Habernal (2013), and Debates from Yogatama and Smith (2014). Wherever available we also provide the size of the hidden unit.", "startOffset": 27, "endOffset": 100}, {"referenceID": 46, "context": "Congressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.", "startOffset": 78, "endOffset": 99}, {"referenceID": 46, "context": "Congressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.S. floor debates in the House of Representatives for the year 2005. Each debate consists of a series of speech segments, each labeled by the vote (\u201cyea\u201d or \u201cnay\u201d) cast for the proposed bill by the the speaker of each segment. We used the pre-processed corpus from Yogatama and Smith (2014).3", "startOffset": 78, "endOffset": 421}, {"referenceID": 52, "context": "Following previous work (Yang et al., 2016), we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special UNK token.", "startOffset": 24, "endOffset": 43}, {"referenceID": 36, "context": "Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on the training and validation splits of each dataset.", "startOffset": 54, "endOffset": 76}, {"referenceID": 15, "context": "Parameters were optimized with Adagrad (Duchi et al., 2011), the learning rate was set to 0.", "startOffset": 39, "endOffset": 59}, {"referenceID": 51, "context": "Previous state-of-the-art results on the three review datasets were achieved by the hierarchical attention network of Yang et al. (2016), which models the document hierarchically with two GRUs and uses an attention mechanism to weigh the importance of each word and sentence.", "startOffset": 118, "endOffset": 137}, {"referenceID": 23, "context": "On the debates corpus, Ji and Smith (2017) obtained best results with a recursive neural network model operating on the output of an RST parser.", "startOffset": 23, "endOffset": 43}, {"referenceID": 31, "context": "Table 5: Descriptive statistics for dependency trees produced by our model and the Stanford parser (Manning et al., 2014) on the SNLI test set.", "startOffset": 99, "endOffset": 121}, {"referenceID": 12, "context": "Specifically, we used the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores.", "startOffset": 52, "endOffset": 86}, {"referenceID": 16, "context": "Specifically, we used the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores.", "startOffset": 52, "endOffset": 86}, {"referenceID": 31, "context": "Table 5 presents various statistics on the depth of the trees produced by our model on the SNLI test set and the Stanford dependency parser (Manning et al., 2014).", "startOffset": 140, "endOffset": 162}, {"referenceID": 29, "context": "Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism.", "startOffset": 38, "endOffset": 59}], "year": 2017, "abstractText": "In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.", "creator": "LaTeX with hyperref package"}}}