{"id": "1705.09231", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Neural Attribute Machines for Program Generation", "abstract": "Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients. Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar.", "histories": [["v1", "Thu, 25 May 2017 15:35:16 GMT  (185kb,D)", "https://arxiv.org/abs/1705.09231v1", null], ["v2", "Wed, 31 May 2017 04:00:12 GMT  (185kb,D)", "http://arxiv.org/abs/1705.09231v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["matthew amodio", "swarat chaudhuri", "thomas reps"], "accepted": false, "id": "1705.09231"}, "pdf": {"name": "1705.09231.pdf", "metadata": {"source": "CRF", "title": "Neural Attribute Machines for Program Generation\u2217", "authors": ["Matthew Amodio", "Swarat Chaudhuri"], "emails": ["mamodio@cs.wisc.edu", "swarat@rice.edu", "reps@cs.wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it has to be noted that most of them are people who are not in a position, who are in a position, who are in a position, who are in a position, who are in a position, and who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a world, who are in a world, who are in a world, who are in a world, who are in a world, who are living in a world, who are living in a world, who are living in a world, who are living in a world, who are we in a world, who are living in a world, who are we in a world, who are living in a world, who are we in a world, who are living in a world, who are living in a world, who are we in a world, who are living in a world, who are living in a world, who are we are living in a world, who are living in a world, who are living in a world, who are living a world, who are living in a world, who are living a world, who are we are living in a world, who are living in a world, who are living a world, who are living in a world, who are living a world, who are living in a world, who are living a world, who are living in a world, who are living a world, who are we, who are living a world, who are living a world who are living in a world, who are living a world, who are living a world, who are living a world, who are living a world, who are living a world, who are living in a world, who are living a world, who are living a world, who are living a world, who are living a world, who are living a world, who are we, who are living a world, who are living a world, who are living a world, who are living a world, who are living a world, who are living a world, who are living a world, who"}, {"heading": "2 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Background on Attribute Grammars", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "2.2 From an AST to a sequence", "text": "While other learning models that can be used to generate trees include performing twists over nodes in binary trees [12] or stacking multiple RNNs in fixed directions [13], a natural paradigm that can be used to represent a tree in a neural model is a one-sided, top-down sequencing of the tree. An AST is represented by a deeper sequencing of pairs: each pair (ni, pi), for 1 \u2264 i \u2264 node (T) | consists of a non-terminal ni-N and a production pi-P that stands ni on the left."}, {"heading": "2.3 Challenges", "text": "Large and varying-sized AST sequences pose several problems for traditional neural sequencers. The first is the existence of distant non-local dependencies, such as the declaration of variable v at the beginning of a file that could be many hundreds of steps away before the RNN has to predict the use of v. Another is the existence of complex relationships between nodes that are difficult to express in linear sequencing, such as when distinguishing between a great-grandparent node and a great-uncle node is important. Third, because of the approximate sequence distribution, it is very likely that new contexts will emerge when generating large ASTs in a randomized sample. In such a case, the training set would not contain explicit information that can guide future generations after entering \"untapped territory.\" Other approaches rely on the learner's ability to generalize from the training set."}, {"heading": "2.4 NAM", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave; in fact, it is a way in which people are able to decide what they want and what they want."}, {"heading": "3 Experiments", "text": "The selected versionAlgorithm 2: Generation 1 tree: = root ([hole]); 2 curFocus: = tree.child [1]; 3 curNonterminal: = nonterminal of curFocus; 4 curContext: = the values of attributes in curFocus; 5 repeat 6 op: = ChooseOperator (curNonterminal, curContext); 7 Insert op ([hole1],. [holearity (op)] in tree at curFocus; 8 curFocus: = next hole in preorder after curFocus; 9 curNonterminal of curFocus; 10 curContext: = update curContext in the values at curFocus; 11 until there are full programs is the long term Short Term Memory (= cell 14), which is all curContext: = curContext curContext: = curContext in the values of the attributes at curFocus; 11 until there is the long Short Term Memory (cell 14), which is long term to curContext [NAM]."}, {"heading": "3.1 Evaluation criteria", "text": "Our experiments were designed to answer the following questions: A. What is the quality of the simulated samples? B. How well do the models represent the training data? C. At what rate are limitations violated in the sample? Methods for evaluating the quality of the learned model in generation tasks can be more subjective than in prediction tasks where performance on held test sets is relevant. In our case, however, we can make various measurements of error rates when creating internal nodes, as well as test whether a generated tree fulfills the limitation as a whole, providing a general measure of success.3 Measurements are applied consistently. 1 The ability to learn the corpus based on the average negative log probability of each model's training samples. 2. The number of predictions that violate the limitation while generating new samples. 3. The number of trees that are completely legal under the specific limitations considered. (Inexpected tree, a total violation of this restriction)."}, {"heading": "3.2 Declared-variable constraint", "text": "Our first experiment has the restriction that every variable used must be declared (see columns 2-5 of Fig. 4 and Fig. 5). As shown in Fig. 4, although the vanilla RNN requires the entire tree in front of the node, it still makes many errors. For comparison, a stochastic context-free grammar, which is shown to have the same augmented input, is shown in Fig. 5 Since it now includes the context vector, it is referred to as stochastic grammar with context. (SGWC) This model never becomes variable v if the element of the context vector vector is not set."}, {"heading": "3.3 Typesafe-variable constraint", "text": "The same relative performance is evident when we work with the second constraint: the SGWC generalizes particularly poorly to the test set in this case, because the context vector varies more, and therefore there are rare or entirely new situations that the SGWC has to contend with. Vanilla RNN is the same model as in Section 3.2, because it does not take the constraint into account in any way. Infringements are lower in this environment, because the tests for this constraint are rarer: not all variable applications include multiple types that must match. Nevertheless, the simpler models produce fewer legal trees than in the experiment in Section 3.2."}, {"heading": "4 Related Work", "text": "The problem of the corpus-driven program generation has been studied before [21, 6, 22, 23, 24]. Statistical models used in this task include n-gram theme models [22], probabilistic tree substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher grammars [24], and recurring neural networks [6]. The most closely related piece of work is by Maddison and Tarlow [21], who use log-bilinear tree transversal models, a class of probabilistic push-down automata for program generation. Their model also addresses the problems of explaining names and type consistency, and they use \"traverse variables\" to disseminate information from parts of the already produced tree to influence the production selected at a node."}, {"heading": "5 Discussion", "text": "As our experiments show, this can be difficult in practice, but in some cases aspects of the structure can be explicitly presented. NAMs provide a framework for incorporating knowledge of these limitations into RNNs. They significantly exceed RNNs when trained on the same data.We have shown the usefulness of NAMs for two L-attributed AG problems. The work here makes it possible to create a generator of NAM systems: from a specification of a desired limited language, one could generate the appropriate form for the training.Moreover, these are just two of the possible types of errors that would prevent a program from passing through the many tests of a C compiler. A topic for future work is to include enough C constraints so that generated programs would have a high probability of being compiled.Since other sequences have an underlying type of errors that would prevent a program from passing the many tests of a C compiler."}], "references": [{"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A neural representation of sketch drawings", "author": ["David Ha", "Douglas Eck"], "venue": "arXiv preprint arXiv:1704.03477,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Fuzzing with code fragments", "author": ["Christian Holler", "Kim Herzig", "Andreas Zeller"], "venue": "In USENIX Security Symposium,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On the naturalness of software", "author": ["Abram Hindle", "Earl T Barr", "Zhendong Su", "Mark Gabel", "Premkumar Devanbu"], "venue": "In Software Engineering (ICSE),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Code completion with statistical language models", "author": ["Veselin Raychev", "Martin Vechev", "Eran Yahav"], "venue": "In PLDI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Semantics of context-free languages", "author": ["D.E. Knuth"], "venue": "Mathematical Systems Theory, 2(2):127\u2013145, June", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1968}, {"title": "Semantics of context-free languages: Correction", "author": ["D.E. Knuth"], "venue": "Mathematical Systems Theory, 5(1):95\u201396, March", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1971}, {"title": "Attributed translations", "author": ["P.M. Lewis", "D.J. Rosenkrantz", "R.E. Stearns"], "venue": "J. Comput. Syst. Sci., 9(3):279\u2013307, December", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1974}, {"title": "Complexity characterizations of attribute grammar languages", "author": ["Sophocles Efremidis", "Christos H. Papadimitriou", "Martha Sideris"], "venue": "Inf. and Comp.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "The Synthesizer Generator: A System for Constructing Language- Based Editors", "author": ["T. Reps", "T. Teitelbaum"], "venue": "Springer-Verlag, NY,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1504.01106,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Dag-recurrent neural networks for scene labeling", "author": ["Bing Shuai", "Zhen Zuo", "Bing Wang", "Gang Wang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "A batch-normalized recurrent network for sentiment classification", "author": ["Horia Margarit", "Raghav Subramaniam"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation", "author": ["Marijn F. Stollenga", "Wonmin Byeon", "Marcus Liwicki", "Juergen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1929}, {"title": "CIL: Intermediate language and tools for analysis and transformation of C programs", "author": ["George C. Necula", "Scott McPeak", "Shree P. Rahul", "Westley Weimer"], "venue": "In International Conference on Compiler Construction,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Structured generative models of natural source code", "author": ["C.J. Maddison", "D. Tarlow"], "venue": "ICML,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A statistical semantic language model for source code", "author": ["Tung Thanh Nguyen", "Anh Tuan Nguyen", "Hoan Anh Nguyen", "Tien N. Nguyen"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Graph-based statistical language model for code", "author": ["Anh Tuan Nguyen", "Tien N. Nguyen"], "venue": "In Proceedings of the 37th International Conference on Software Engineering - Volume 1,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "PHOG: Probabilistic model for code", "author": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Mining idioms from source code", "author": ["Miltiadis Allamanis", "Charles Sutton"], "venue": "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Neuro-symbolic program synthesis", "author": ["Emilio Parisotto", "Abdel-rahman Mohamed", "Rishabh Singh", "Lihong Li", "Dengyong Zhou", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1611.01855,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Bayesian sketch learning for program synthesis", "author": ["Vijayaraghavan Murali", "Swarat Chaudhuri", "Chris Jermaine"], "venue": "arXiv preprint arXiv:1703.05698,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Deepcoder: Learning to write programs", "author": ["Matej Balog", "Alexander L. Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1611.01989,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Visibly pushdown transducers", "author": ["J.-F. Raskin", "F. Servais"], "venue": "ICALP,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Bayesian recurrent neural network for language modeling", "author": ["Jen-Tzung Chien", "Yuan-Chu Ku"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "SAE-RNN deep learning for RGB-D based object recognition", "author": ["Jing Bai", "Yan Wu"], "venue": "In International Conference on Intelligent Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks have been applied successfully to many generative modeling tasks, from images with pixel-level detail [1] to strokes corresponding to crude sketches [2] to natural language in automated responses to user questions [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "Neural networks have been applied successfully to many generative modeling tasks, from images with pixel-level detail [1] to strokes corresponding to crude sketches [2] to natural language in automated responses to user questions [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "Neural networks have been applied successfully to many generative modeling tasks, from images with pixel-level detail [1] to strokes corresponding to crude sketches [2] to natural language in automated responses to user questions [3].", "startOffset": 230, "endOffset": 233}, {"referenceID": 3, "context": "Program generation has many potential applications, including automatically testing programming tools [4] and assisting humans as they solve programming tasks [5, 6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Program generation has many potential applications, including automatically testing programming tools [4] and assisting humans as they solve programming tasks [5, 6].", "startOffset": 159, "endOffset": 165}, {"referenceID": 5, "context": "Program generation has many potential applications, including automatically testing programming tools [4] and assisting humans as they solve programming tasks [5, 6].", "startOffset": 159, "endOffset": 165}, {"referenceID": 6, "context": "We use the formalism of attribute grammars [7] as the language for expressing rich structural constraints over programs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "1 An attribute grammar (AG) is a context-free grammar extended by attaching attributes to the terminal and nonterminal symbols of the grammar, and by supplying attribute equations to define attribute values [7].", "startOffset": 207, "endOffset": 210}, {"referenceID": 7, "context": "Noncircularity is a decidable property of AGs [8], and hence we can assume that no derivation tree exists in which an attribute instance is defined transitively in terms of itself.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "An AG is L-attributed [9] if, in each production X0 \u2192 X1, .", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "With reasonable assumptions about the computational power of attribute equations and attribute constraints, L-attributed AGs capture the class NP [10] (modulo a technical \u201cpadding\u201d adjustment).", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "\u201d (The notation is adapted from the Synthesizer Generator [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "While other attempts at learning models that can be used to generate trees include performing convolutions over nodes in binary trees [12] or stacking multiple RNNs in fixed directions [13], a natural paradigm to adopt for presenting a tree to a neural model is a single-traversal, top-down, left-to-right sequentialization of the tree.", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "While other attempts at learning models that can be used to generate trees include performing convolutions over nodes in binary trees [12] or stacking multiple RNNs in fixed directions [13], a natural paradigm to adopt for presenting a tree to a neural model is a single-traversal, top-down, left-to-right sequentialization of the tree.", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "child[1]; 3 curNonterminal := nonterminal of curFocus; 4 curContext := the values of the attributes at curFocus; 5 repeat 6 op := ChooseOperator(curNonterminal, curContext); 7 Insert op([hole1], .", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 15, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 16, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 17, "context": "The Adam optimizer [18] is used with learning rate .", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "001, dropout [19] with a keep probability of .", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Programs in the corpus are translated to an abstract syntax tree (AST) in the C Intermediate Language (CIL) [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 5, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 21, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 22, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 23, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 21, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 206, "endOffset": 210}, {"referenceID": 5, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 242, "endOffset": 245}, {"referenceID": 20, "context": "The most closely related piece of work is by Maddison and Tarlow [21], who use log-bilinear tree-traversal models, a class of probabilistic pushdown automata, for program generation.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "A recent body of work uses neural techniques [26, 27, 28] to solve this problem.", "startOffset": 45, "endOffset": 57}, {"referenceID": 26, "context": "A recent body of work uses neural techniques [26, 27, 28] to solve this problem.", "startOffset": 45, "endOffset": 57}, {"referenceID": 27, "context": "A recent body of work uses neural techniques [26, 27, 28] to solve this problem.", "startOffset": 45, "endOffset": 57}, {"referenceID": 27, "context": "[28] and Murali et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] use combinatorial search, guided by a neural network, to generate programs that satisfy language-level constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "As an automaton that sees an input stream that contains occurrences of \u201cpop\u201d and produces an output, a NAM is a form of transducer, namely a visibly-pushdown transducer [29].", "startOffset": 169, "endOffset": 173}, {"referenceID": 29, "context": "Neural stack machines like those in [30] augment an RNN with a stack, which the RNN must learn how to operate through differentiable approximations.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "Many attempts at customized regularization have been demonstrated [31, 32].", "startOffset": 66, "endOffset": 74}, {"referenceID": 31, "context": "Many attempts at customized regularization have been demonstrated [31, 32].", "startOffset": 66, "endOffset": 74}], "year": 2017, "abstractText": "Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients. Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar. Neural Attribute Machines (NAMs) are equipped with a logical machine that represents the underlying grammar, which is used to teach the constraints to the neural machine by (i) augmenting the input sequence, and (ii) optimizing a custom loss function. Unlike traditional RNNs, NAMs are exposed to the grammar, as well as samples from the language of the grammar. During generation, NAMs make significantly fewer violations of the constraints of the underlying grammar than RNNs trained only on samples from the language of the grammar.", "creator": "LaTeX with hyperref package"}}}