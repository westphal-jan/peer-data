{"id": "1705.09269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Geometric Methods for Robust Data Analysis in High Dimension", "abstract": "Machine learning and data analysis now finds both scientific and industrial application in biology, chemistry, geology, medicine, and physics. These applications rely on large quantities of data gathered from automated sensors and user input. Furthermore, the dimensionality of many datasets is extreme: more details are being gathered about single user interactions or sensor readings. All of these applications encounter problems with a common theme: use observed data to make inferences about the world. Our work obtains the first provably efficient algorithms for Independent Component Analysis (ICA) in the presence of heavy-tailed data. The main tool in this result is the centroid body (a well-known topic in convex geometry), along with optimization and random walks for sampling from a convex body. This is the first algorithmic use of the centroid body and it is of independent theoretical interest, since it effectively replaces the estimation of covariance from samples, and is more generally accessible.", "histories": [["v1", "Thu, 25 May 2017 17:25:04 GMT  (406kb,D)", "http://arxiv.org/abs/1705.09269v1", "180 Pages, 7 Figures, PhD thesis, Ohio State (2017)"]], "COMMENTS": "180 Pages, 7 Figures, PhD thesis, Ohio State (2017)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joseph", "erson"], "accepted": false, "id": "1705.09269"}, "pdf": {"name": "1705.09269.pdf", "metadata": {"source": "CRF", "title": "Geometric Methods for Robust Data Analysis in High Dimension", "authors": [], "emails": [], "sections": [{"heading": null, "text": "In this context, it should be noted that the measures in question are measures taken in recent years to violate the principle of gender equality."}, {"heading": "Acknowledgments", "text": "Finally, I want to thank my wife for failing to inspect her love. \"2016."}, {"heading": "1.1 Organization of this Thesis", "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "2.1 Main result", "text": "Our main result is an efficient algorithm that can restore the mixing matrix A in model X = AS if each Si has 1 + \u03b3 moments for a constant \u03b3 > 0. The main reason why we use this algorithm is that finite sample guarantees for it have been proven; we could be stuck in any other algorithm with such a guarantee. The theorem below also refers to Gaussian Damping, which is an algorithmic technique we will present in this chapter and explain briefly. Theorem 2.1.1 (Heavy-tailed ICA) Let X = AS be an ICA model that the distribution of S is absolutely continuous for all we have E (| Si | 1 +)."}, {"heading": "2.2 Preliminaries", "text": "In this section we review technical definitions and results used in the following sections. For a random vector X-Rn, we refer to the distribution function of X as FXand, if it exists, the density is referred to as fX. However, for a real value random variable X, cumulants of X are polynomials in the moments of X. For j \u2265 1, the jth cumulative number of X + 2m31 is defined as coefficients in the logarithmic function of X: log (etX) = m2 \u2212 m21, \u03ba3 (X) = m3 \u2212 m2m31. The first two cumulants are the same as the expectation and the variance."}, {"heading": "2.2.1 Heavy-Tailed distributions", "text": "Formally it can be written that the distribution of X varies greatly if e\u03bbxP (X > x) deviates as x approaches infinity. 16For our applications, however, we need only worry about the presence of higher moments of distribution. The primary statistical assumption made in the following work assumes only that EX1 + \u03b3 < \u221e for some \u03b3 > 0. The addition of 1 in the above areas of application is primarily such that the means are well defined. Of course, it is possible to construct distributions with a strong tail from this assumption, e.g. with a density proportional to 1 / x2 + \u03b3 that will have an initial moment, up to the moment of 1 + \u03b3, but not a higher one."}, {"heading": "2.2.2 Stable Distributions", "text": "Stable distributions are an important class of probability distributions characterized by a key property: the family itself is closed by addition and scaler multiplication. That is, if you have two random variables X and Y with the same stable distribution, X + Y will also be a stable distribution with slightly different parameters. Two well-known members of this family are the Gaussian and the Cauchy distributions, both of which will be of importance during this work. Stable distributions are completely characterized by four parameters and can be written as stable (\u03b1, \u03b2, c, \u00b5), where \u03b1 (0, 2] is the stable parameter (\u03b1 = 2 for Gaussian and \u03b1 = 1 for Cauchy), \u03b2 [\u2212 1, 1] is a skew parameter, c (0, \u221e) is scale, and \u00b5 \u00b2 R is location. PDF and CDF distributions are generally not expressible analytically, but stable distributions are those that can be written for the characteristic function."}, {"heading": "2.2.3 Convex optimization", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "2.2.4 Algorithmic convexity", "text": "We note here a particular case of a standard result in algorithmic convexity: There is an efficient algorithm to estimate the covariance matrix of the uniform distribution in a centrally symmetrical convex body given by a weak convex body. The result results from the random, run-based algorithm to generate approximately uniform random points from a convex body [41, 60, 70]. Most works use access to a membership oracle for the given convex body. In this thesis, we only have access to a -weak membership oracle. As discussed in [41, section 6, note 2], essentially the same algorithm implements efficient sampling when applying a -weak membership oracle. The problem of estimating the covariance matrix of a convex body was introduced in [60, section 5.2]."}, {"heading": "2.2.5 The centroid body", "text": "The most important tool in our orthogonalization algorithm is the function of a compact body [81] which is used as the first moment analogy to the covariance matrix. < < < < < < < < K (81, 73, 44]) Convex bodies associated with (the uniform distribution on) a given convex body. < K (81) < K (81), the convex distribution on a convex body for more general probabilities. Let X (81, 73, 44) be a random vector with a finite first moment, that is, for all u-convex bodies we have a generalization of the definition of E (< X >) < K (81), let us consider the function h (u) = E (< u, X > |). Then it is easy to see that h (0) is subjective, h = positive, h = and is ogeneous."}, {"heading": "2.3 Membership oracle for the centroid body", "text": "In this section, we provide an efficient, weak membership oracle (subroutine 2) for the centric body [X] of the r.v. X. This is done by first providing a weak membership oracle (subroutine 1) for the polar body [X]. We start with a problem that shows that the centric body is \"well rounded\" under certain general conditions. This property will prove useful in the membership tests. Let's make S = (S1,.., Sn) \"Rn\" contain an absolutely symmetrically distributed random vector, so that E (Si |) = 1 for all i. Then Bn1 S [\u2212 1, 1] n. Furthermore, n \u2212 1 / 2Bn2 (S) will be an absolutely symmetrically distributed random vector, so that the support function of the \"S\" (Si |) = 1 for all ii."}, {"heading": "2.3.1 Mean estimation using 1 + \u03b3 moments", "text": "To this end, we need to estimate the first absolute moment of the projection in one direction. Our assumption that each component Si has a finite (1 + \u03b3) moment will allow us to do so with a reasonable low probability of error. This is done via the following Chebyshev type inequality.Let X be a real-evaluated symmetric random variable such that E | X | 1 + \u2264 M for some M > 1 and 0 < 1. Then we will prove that the empirical average of the expectation of X adapts to the expectation of X. Let E's N [X] is the empirical average obtained from N independent samples X (1)., X (N), X (N), i.e."}, {"heading": "2.3.2 Membership oracle for the polar of the centroid body", "text": "As already mentioned, our membership oracle is based on the fact that 1 / horizontal function (1 / horizontal function) (1 / horizontal function) (1 / horizontal function) (1 / horizontal function) (1 / horizontal function) (1 / horizontal function) (1 / horizontal function) (1 / horizontal function) (1 / horizontal effect) (1 / horizontal effect) (1 / horizontal effect) (1 / horizontal effect) (1 / horizontal effect) (1 / horizontal effect) (1 / horizontal effect)."}, {"heading": "2.3.3 Membership oracle for the centroid body", "text": "We describe how the weak membership oracle for the centered body \"X\" using the weak membership oracle (\"X\") \"X\" (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X,\" \"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (X \") (\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\"X\") (\") (\" X \") (\") (\"X\") (\") (\" X \") (\") (\"X (\") (X) (\") (X) (\") (X) (\"(\") \"(X\") \"(\") \"(X\") \"(X\" (\")\" (X \") (\") (X \"(\") \"(X\") (\") (X\" (\")\" (X \") (\") (\")\" (X \")\" (X \") (\") \"(X\" (\")\" (\")\" (X \")\" (\")\" (X \")\" (\"(\") \")\" (X \"(\") \"(\") \"(\") \"(X\" (\")\" (\")\" (\"(\") \")\" (X \"(\") \"(\") \"(X)\" (\"(\") \"(\" (\")\" (X) \"(\") \"(\" (\")\" (X) \"(X)\" (\"(\" (\")\" (\")\" (X) \"(X)\" (\"(X)\" (X) \"(\" (\"(\") \"(X)\" (X) \"(\" (\")\") \"(X)\" (\"(\" (\")\" (X) \"(\" (X) \"(\") \"(X)\" (\"(X)\" (\"(\") \"(X)\" ("}, {"heading": "2.4 Orthogonalization via the uniform distribution in the", "text": "The following question is: \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"\" What is this?, \"\""}, {"heading": "2.5 Gaussian damping", "text": "In this section, we will give an efficient algorithm for the low-waisted ICA problem if the ICA matrix is a uniform matrix; no assumptions about the existence of moments of Si are required. The basic idea behind our algorithm is simple and intuitive: Using X, we construct another ICA model XR = ASR, where the matrix A can be estimated by applying existing ICA algorithms. For a random variable Z, we will show its probability density function using XR (\u00b7), how to efficiently generate samples of XR using samples of XR. The density of XR matrix A can be estimated by applying existing ICA algorithms."}, {"heading": "2.5.1 The fourth cumulant of Gaussian damping of heavy-", "text": "It is clear that if we are able to find a symmetrical, really weighted variable with E (X4) = \"We will limit our discussion to symmetrical random variables for the simplicity of exposure; for the purpose of our application of the theorem this is w.l.o.g. by arguing in Sec. 2.2.2.Theorem 2.5.2. Let X find a symmetrical, really weighted variable with E (X4) =\" We will find a symmetrical, random variable with E (X4) =. \"Then let us suppose that we find a symmetrical variable with E (X4) =\" we. \""}, {"heading": "2.5.2 Symmetrization", "text": "As usual, we work with the ICA model X = AS. Let us assume that we have an ICA algorithm that works if each of the component random variables Si is symmetrical, i.e. its probability density function Si (y) = \u03c6i (\u2212 y) for all y fulfills, with a polynomial dependence on the upper limit M4 of the fourth moment of Si and an inverse polynomial dependence on the lower limit of Si. Then we show that we also have an algorithm without the assumption of symmetry and with a similar dependence on M4 and E. We show that without loss of the Generality41 we can limit our attention to symmetrical densities, i.e. we can assume that each of the Si has a density function that Si (y) = 4S. For this purpose, S shall have an independent copy of Si i i i i i i i i (i), i (i) i (i) i (i) i (i) i (i) i (i)."}, {"heading": "2.6 Putting things together", "text": "In this section we combine the orthogonalization approach to 1 / 2. In this section we combine the orthogonalization approach to 1 / 2. In this section we combine the orthogonalization approach to 1 / 2. In this section we have the possibility that we have the distribution of 2 / 3. Si when42c in the interval [\u2212 R, R] has the fourth model of 3 / 4. Si when42c in the interval [\u2212 R, R] has the fourth model of 4 / 4. Si when42c in the interval [\u2212 R, R] has the distribution of 4 / 4. Si, R > 0 is such that we have 2 / 4. Si where we can select 1 / 4. Si (n) < p (n) < p = 1, and for simplicity we will fix it."}, {"heading": "2.7 Improving orthogonalization", "text": "As mentioned above, the technique in [10], although demonstrably efficient and correct, suffers from practical implementation problems. Here, we discuss two alternatives: orthogonalization by centrifugal body scaling and orthogonalization by using empirical covariance. The former, orthogonalization by centrifugal body scaling, uses the samples already present in the algorithm rather than relying on a random path to draw samples that are roughly the same in the approach of the algorithm to the centrifugal body (as in [10]). This removes the dependence on random pathways and the ellipsoid algorithm; instead, we use samples that are distributed according to the original center of gravity distribution but are not scaled linearly to lie within the centrifugal body (as in [10]). We prove in Lemma 14 that the covariance of this subset of samples is sufficient to orthogonize the mixing matrix A."}, {"heading": "2.7.1 Orthogonalization via centroid body scaling", "text": "In [10], another orthogonalization method, namely orthogonalization over the actual distribution in the centric body, it is theoretically proven to work. (We have described the orthogonal distribution over the centric body in Section 1, except for the estimation of p (x), the Minkowski functionality of the centric body. The complete procedure will be explained in Subroutine 3.We explain how to use the Minkowski functionality in Section 1. Minkowski functionality was informally defined in Section 1. Minkowski functionality is formally defined by p (x): = inf {t > 0: x). Our estimate of p (x) is based on an explicitly linear program (LP) that describes the Minkowski functionality of p (x)."}, {"heading": "2.7.2 Orthogonalization via covariance", "text": "Here we show the somewhat surprising fact that the orthogonalization of low-waisted signals is sometimes not possible using the \"standard\" approach: reversing the empirical covariance matrix. The advantage of this is that it is mathematically very simple, in particular that with low-waisted data, very little computational effort is required for the process of orthogonalization alone. It is standard to use the covariance matrix when the second moments of all independent components exist [57]: Given the samples from the ICA model X = AS, we calculate the empirical covariance that tends to the true covariance matrix, how we take further samples and determine that BA is a rotation matrix, and therefore by pre-multiplying the data from B we obtain an ICA model Y = (BA) S, where the mixed matrix BA is a rotation matrix, and this model is then fixed to enable different components."}, {"heading": "2.7.3 New Membership oracle for the centroid body", "text": "We will now describe and theoretically justify a new and practically efficient membership problem - a weak membership oracle for \"X,\" which is a black box that can answer approximate membership queries in \"X.\" Specifically, a -weak membership oracle for \"X\" is an oracle that solves the weak membership problem for \"K.\" For \"K\" [0, 1], a (,) -weak membership oracle for \"K,\" a (,) -weak membership oracle for \"K,\" a (,) -weak membership oracle for \"K,\" a (,) -weak membership oracle for \"K,\" a (, 1) -weak membership oracle for \"K,\" and a (,) -weak membership oracle for \"K.\" We start with an informal description of the algorithm and its correctness. The algorithm that implements the oracle (Subroutine 4) is the following: Leave \"n.\""}, {"heading": "2.8 Empirical Study", "text": "In this section, we demonstrate experimentally that heavy tail data pose a significant challenge to current ICA algorithms, and compare it with HTICA in different environments. We observe some clear situations where heavy tail data seriously impair standard ICA algorithms, and that these problems are often avoided by using the heavy tail ICA framework. In some cases, HTICA does not help much, but retains the same performance of simple FastICA. To generate the synthetic data, we generate a simple heavy tail density function f\u03b7 (x) proportional to (| x | + 1.5) \u2212 \u03b7, which is symmetrical, and for \u03b7 > 1 f\u03b7 is the density of a distribution having a finite k < \u03b7 \u2212 1 moment. Signal S is generated with any Si distributed independently of f\u03b7i. The mixing matrix A, Rn \u00d7 n, is compared with each coordinate > 1, to normalize the unit length (N)."}, {"heading": "2.8.1 Heavy-tailed ICA when A is orthogonal: Gaussian damp-", "text": "As proposed in [10], Gaussian damping is a pre-processing technology that converts data from an ICA model X = AS where A is uniform (columns are orthogonal with unit l2-norm) to data from a related ICA model XR = ASR, where R > 0 is a parameter to be chosen. Independent components of SR have finite moments of all orders, and thus the existing algorithms A.Using samples of X we construct the damped random variable XR (x), where R > 0 is a parameter to be chosen. Independent components of SR have finite moments of all orders, and thus the existing algorithms A.Using samples of X we construct the damped random variable XR (x), with pdfXR (x), exp (\u2212 x) exp (\u2212 R)."}, {"heading": "2.8.2 Experiments on synthetic heavy-tailed data", "text": "We now present the results of the HTICA using various orthogonalization techniques: (1) orthogonalization via covariance (Section 2.7.2 (2) orthogonalization via centrifugal body (Section 2.7.1) (3) the basic truth, direct reversal of the mixing matrix (oracle) and (4) no orthogonalization or damping (for comparison with plain FastICA) (identity).69The \"mixed\" regime in the left and middle corner of Figure 2.3 (where some signals are not heavy-waisted) shows a very dramatic contrast between different orthogonalization methods, even if only two low-waisted signals are present. Experimenting with different methods of orthogonalization, it was observed that if all the exponents are equal or very close, orthogonalization via covariance works better than orthogonalization via centrifuges and the true mixing matrix as shown in Figure 2.3."}, {"heading": "2.8.3 ICA on speech data", "text": "While the above synthetic data study offers interesting situations in which heavy tails can cause problems for ICA, we provide some results that use real data, particularly human speech. To examine the performance of HTICA on speech data, we must first examine whether the data are heavily waisted. Motivation to use speech data comes from the observations of the signal processing community (e.g. [63]), which can model the speech data, to examine the physical distributions that people face (0, 2), only the moments of order will be limited. Here, we present some results on a data collection of human language according to the standard cocktail party model, which is of [40].71 The physical setup of the experiments is shown by speakers and microphones. To estimate whether the data is heavily waisted, we estimate the parameters of a stable-stable distribution."}, {"heading": "3.1 Preliminaries", "text": "An n-simplex is the convex hull of n + 1 points in Rn that are not on a (n \u2212 1) dimensional affinity hyperplane. It will be convenient to work with the standard nsimplex defined in Rn + 1 as the convex hull of n + 1 canonical unit e1,."}, {"heading": "3.2 Computing the moments of a simplex", "text": "The k-th moment mk (u) beyond that is the function 7 \u2192 EX (u \u00b7 X) k). In this section we present a formula for the moment over \u00b2 n. Similar more general formulas appear in [69]. We can easily derive from [51] the following result for the integration over \u00b2 n: (x\u03b100 \u00b7 \u00b7 x\u03b1nn dx = 1 \u00b7 \u03b10! \u00b7 \u00b7 \u03b1n! (n + 1 + \u2211 i)!.86 Now we can derive a formula for the integration over \u00b2: (x\u03b100 \u00b7 x\u03b1nn dx = 1 \u00b7 \u03b1nn dx = 1 \u00b7 \u03b1nn + 1 \u00b7 \u03b1n! (n + \u2211 i)!.86Now (x0u0 +. + xnun) symmous (xnun) kdx!"}, {"heading": "3.3 Subroutine for finding the vertices of a rotated standard", "text": "In this section, we solve the following simpler problem: Suppose we have a new method to apply this method. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p (1) p (1) p (1) p (1) p (1) p (2) p (1) p (2) p (1) p (2) p (2) p (2) p (2) p (p) p (2) p (2) p) p (1) p (2) p (2) p (2) p) p (2) p (2) p (2) p (2) p (2) p) p (\"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" (2) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\") p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"(2) p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\") p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" (2) p \"p\") p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"."}, {"heading": "3.4 Learning simplices", "text": "The learning algorithm uses an affinity map T: Rn \u2192 Rn + 1, which maps some isotropic simplicities. To calculate such a problem, one can start with the first column of Q (n + 1) -by- (n + 1) -matrix B, which has ones in the diagonal and first column, is anything but zero. Let QR = B be a QR decomposition of B. By definition, we have that the first column of Q is parallel to 1 and the rest of the columns is column 1. In view of this, let A form the matrix formed by all columns of Q except the first."}, {"heading": "3.5 The local and global maxima of the 3rd moment of the", "text": "This is not necessary for our algorithmic result, but it gives insights into the geometry of the third moment (the location of local maxima / minima and stationary points) and suggests that more direct optimization algorithms such as gradient and Newton's method will also work, although we will not prove that we will understand the geometry of the third moment (the location of local maxima / minima and stationary points). Let us let X happen in K. Let V = {xi} n the number of normalized wells of K. Then V is a complete set of local maxima and a complete set of global maxima of F: Sn \u2212 1 \u2192 R given by F (u \u00b7 X).3 Idea of wells of K. Then V is a complete set of local maxima and a complete set of global maxima of F."}, {"heading": "3.6 Probabilistic results used", "text": "In this section we show the random results underlying the distributions of learning simplifications, and \"np balls to ICA\" (1.1). The results are: Theorems 3.6.1 and 3.6.2. They each show a simple non-linear distribution of the respective uniform distributions, which allows a distribution with independent components (Definition 6). (Theorem 3.6.1 below there is a simple non-linear distribution with independent components. (103Definition 6) We say that a random vector X has independent components if it has a random transformation of a random vector with independent coordinates. (Theorem 3.6.1) We leave X a uniformly random vector in the (n \u2212 1) -dimensional standard simplex \u2212 1. Let T have a random scalar distribution as an independent coordination. (Theorem 3.1) We leave X a uniformly random vector in the (x) -dimensional standard \u2212 1."}, {"heading": "3.7 Reducing simplex learning to ICA", "text": "We show random reductions from the following two natural statistical estimation problems to ICA: Problem 1 (simplex). Given uniformed random points from an n-dimensional simplex, we estimate the mean of an affectively transformed distribution. That is, we assume that the \"np sphere to be learned has only been transformed linearly.\" Problem 2 (linear transformed examples). To simplify the presentation of the second problem, we ignore the estimate of the mean of a transformed distribution. These problems have no obvious independence structure. Nevertheless, known representations of the uniform measurement in an \"np sphere and the cone measurement (defined in Section 3.1) on the surface of an np sphere are easily extended to a sample from these distributed components into an independent component."}, {"heading": "4.1 Preliminaries", "text": "The singular values of a matrix A-Rm \u00b7 n are ordered in the decreasing order: \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u2265 \u03c3min (m, n). By \u03c3min (A) we mean \u03c3min (m, n). For a real random variable X, the cumulants of X are certain polynomials at the moments of X. For j-1, the jth cumulant is called \u03baj (X). Characteristic of mj: = EXj are e.g.: \u04451 (X) = m1, \u04452 (X) = m2 \u2212 m21 and \u03ba3 (X) = m3 \u2212 3m2m1 + 2m31. In general, cumulants can be defined as certain coefficients of a Taylor expansion of the logarithm of the moment, which generate the function of X: log (EX) (etX) = 1 \u0445j (X) tj!"}, {"heading": "4.1.1 Gaussian Mixture Model.", "text": "For i = 1, 2,.., m, define Gaussian random vectors \u03b7i-Rn with the distribution \u03b7i-N (\u00b5i-i-i), where \u00b5i-Rn and \u041di-Rn-n. Let h be an integer random variable, which assumes the value i-m with the probability wi > 0, henceforth called weights. (This results \u2211 mi = 1wi = 1.) Then the random vector drawn as Z-ig can be interpreted according to the weights as a Gaussian mixed model (GMM) w1N (\u00b51-1) + wmN (\u00b5m-m). The sample of Z can be interpreted in such a way that first one of the components i-m is selected according to the weights, and then a Gaussian vector from component i-i (GMM-1-i-i-i-i-i-1) with the probability (GMM-i-i-i-i-1) and then the ability (GMM-i-i-i-1)."}, {"heading": "4.1.2 Underdetermined ICA.", "text": "In the basic formulation of ICA, the observed random variant X-Rn, the model is X = AS, where S-Rm is a latently random vector whose components Si are independent of each other, and A-Rn \u00b7 m is an unknown mixture matrix. We cannot hope to completely regenerate the sign of Si, unless there is this column of A, or that there is this column of some nonzero factor, then the resulting mixture matrix will regenerate with an appropriately scaled Si."}, {"heading": "4.1.3 ICA Results.", "text": "Theorem 4.1.1 (Theorem 4.1.1, of [48]) allows us to restore A up to the necessary ambiguities in the noise ICA environment. Theorem provides guarantees for an algorithm of [48] for noise under-determined ICA, UnderdeterminedICA. This algorithm takes as input a tensor order parameter d, number of signals m, Access121to samples according to the noise under-determined ICA model with unknown noise, accuracy parameters, trust parameters, limits for moments and cumulants M and \u0445, a limit for the condition parameter \u03c3m and a limit for the cumulative order. It gives approximations to the columns from A to signs and permutation.ICA results. The following theorem (of [48]) allows us to restore A up to the necessary ambiguities in the noise ICA environment."}, {"heading": "4.2 Learning GMM means using underdetermined ICA: The", "text": "In this section we give an informal outline of our main result, namely the learning of the means of the components in GMMs on the reduction to the under-determined ICA problem. Our reduction will be discussed in two parts. The first part gives the main idea of the reduction and will show how the means can be restored to their norms and signs, i.e. we will discuss \u00b1 \u00b5i / \u0445\u0430\u0441\u0430\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441knknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknkn"}, {"heading": "4.3 Correctness of the Algorithm and Reduction", "text": "Subroutine 5 is used to capture the main process of reduction. (Let's select the covariance matrix of GMM (Let's choose an integer as input, and a threshold of 126Subroutine 5 single sample reduction of GMM to approximate ICA threshold). (Let's select the covariance parameters from a mixture of m > identical Gaussians in Rn with variance, Poisson parameters, output: Y (a sample from the model (4,6)). (Generate R according to Poisson (\u03bb). (2: if R > then a relapse occurs.) 3: End if 4: Let Y = 0. 5: do for j = 1 to R 6: Get a sample Zj from the GMM. 7: Let Z \u2032 j = (Zj, 1) to embed the sample in Rn + 1: Y + Zj."}, {"heading": "4.4 Smoothed Analysis", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "4.5 Recovery of Gaussian Weights", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4.6 Addendum", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.6.1 Properties of Cumulants", "text": "The following properties of multivariants are well known and are inherited from the definition of cumulant generating function: \u2022 (symmetry) Let a permutation of k-indices be given. \u2022 (Multilinearity of coordinate variables) In view of the constants \u03b11, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Yi1, \u00b7 \u00b7 Yi '= (n) (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n (n) n) n (n) n) n (n) n) n (n) n (n) n) n (n) n (n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n) n (n) n (n) n) n) n (n) n) n (n) (n) n) (n) n (n) (n) n) (n) (n) n) (n) n) n) n) n) (n) (n) n) (n) (n) n) (n) n) n) n) n) (n) (n) n) n) (n) n) (n) (n) n) (n) n) n) (n) n) (n) (n) n) (n) (n) (n) n) (n) (n) n) (n) n) (n) n) n) (n) n) n) (n) (n) n) n) n) (n) n) n) (n) (n) (n) (n) (n) (n) n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) n) (n) (n) (n) n) (n) (n) n) (n) n) n) n) (n) n) ("}, {"heading": "4.6.2 Rudelson-Vershynin subspace bound", "text": "Lemma 31 (Rudelson-Vershynin [90]): If A-Rn \u00b7 m has columns C1,..., Cm, then C-i = span (Cj: j 6 = i), we have 1 \u221a m min i-dist (Ci, C-i) \u2264 \u03c3min (A), where as usual \u03c3min (A) = \u03c3min (m, n) (A)."}, {"heading": "4.6.3 Carbery-Wright anticoncentration", "text": "s leave Q (x1,.., xn) a multilinear polynomial of degree d. Suppose that Var (Q) = 1, if xi-N (0, 1) is i for all. Then there is an absolute constant C, so that for t-R and > 0, Pr (x1,..., xn) \u0445 N (0, In) (| Q (x1,..., xn) \u2212 t | \u2264 Cd 1 / d.154"}, {"heading": "4.6.4 Lemmas on the Poisson Distribution", "text": "If X-p Poisson (\u03bb) and Y-p = x-p-Bin (x, p), then Y-p Poisson (p\u03bb).Proof.P (Y = y) = s-p x: x-p (Y = x-p) P (X = x-p) p: x-y (xy) py (1 \u2212 p) x-y (x \u2212 p) x-y (x \u2212 y) x-y (x \u2212 x! = p-p)."}, {"heading": "4.6.5 Bounds on Stirling Numbers of the Second Kind", "text": "The following limit results from [84] Theorem 3rd Lemma 35. If n \u2265 2 and 1 \u2264 r \u2264 n \u2212 1 are integers, then {n r} \u2264 1 2 (n r) rn \u2212 r. From this we can derive a somewhat looser connection to the Stirling numbers of the second type, which does not depend on r: Lemma 36. If n, r \u00b2 Z + such that r \u2264 n, then {n} \u2264 nn \u2212 1. Proof. The Stirling number {n} of the second type indicates the number of possibilities to divide a series of n labeled objects into k unlabeled subsets. In the case of wherer = n, then {n} = 1 As n \u2265 1, it is clear that for these decisions of n and r {n r} \u2264 nn \u2212 1.By restricting 1 \u2264 r n, if n = 1, then n = r indicate that {n \u2212 rn} = 1 must be taken into account as such remaining cases."}, {"heading": "4.6.6 Values of Higher Order Statistics", "text": "In this appendix, we collect some of the explicit values for higher order statistics on the Poisson and normal distributions required to analyze our reduction from a Gaussian blend model to learn an ICA model from samples. Lemma 37 (cumulants of the Poisson distribution). Allow X \"Poisson (\u03bb). Thus, the cumulative generation function is g (t) = log (M (t) = \u03bb (et \u2212 1). The\" th \"derivative (\" \u2265 1) is given by g (') (t) = Exp (\u03bb (et \u2212 1). Therefore, the cumulative generation function is \"g (t) = log (M (m (et \u2212 1))."}, {"heading": "4.6.7 Total Variation Distance", "text": "In words, the total variation between two metrics is the largest difference between the metrics on a single event."}], "references": [{"title": "On spectral learning of mixture of distributions", "author": ["D. Achlioptas", "F. McSherry"], "venue": "The 18th Annual Conference on Learning Theory,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Quantitative estimates of the convergence of the empirical covariance matrix in logconcave ensembles", "author": ["Radoslaw Adamczak", "Alexander Litvak", "Alain Pajor", "Nicole Tomczak- Jaegermann"], "venue": "J. Amer. Math. Soc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles", "author": ["Rados  law Adamczak", "Alexander E. Litvak", "Alain Pajor", "Nicole Tomczak- Jaegermann"], "venue": "J. Amer. Math. Soc.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Blind identification of overcomplete mixtures of sources (biome)", "author": ["Laurent Albera", "Anne Ferr\u00e9ol", "Pierre Comon", "Pascal Chevalier"], "venue": "Linear algebra and its applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "The probabilistic method", "author": ["Noga Alon", "Joel H Spencer"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "A new learning algorithm for blind signal separation", "author": ["Shun Amari", "Andrzej Cichocki", "Howard H Yang"], "venue": "Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "CoRR, abs/1210.7559,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures", "author": ["Joseph Anderson", "Mikhail Belkin", "Navin Goyal", "Luis Rademacher", "James Voss"], "venue": "In Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Analogues of the covariance matrix for ica", "author": ["Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher"], "venue": "Thirty-First AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Heavy-tailed independent component analysis", "author": ["Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher"], "venue": "56th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2015),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Efficient learning of simplices", "author": ["Joseph Anderson", "Navin Goyal", "Luis Rademacher"], "venue": "In Conference on Learning Theory, pages 1020\u20131045,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Learning Mixtures of Arbitrary Gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "33rd ACM Symposium on Theory of Computing,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Provable ICA with unknown Gaussian noise, and implications for Gaussian mixtures and autoencoders", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra", "Sushant Sachdeva"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Provable ICA with unknown gaussian noise, with implications for gaussian mixtures and autoencoders", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra", "Sushant Sachdeva"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Sampling convex bodies: a random matrix approach", "author": ["Guillaume Aubrun"], "venue": "Proc. Amer. Math. Soc.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "A probabilistic approach to the geometry of the  `p -ball", "author": ["F. Barthe", "O. Gu\u00e9don", "S. Mendelson", "A. Naor"], "venue": "The Annals of Probability, 33(2):480\u2013513,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Blind signal separation in the presence of Gaussian noise", "author": ["Mikhail Belkin", "Luis Rademacher", "James Voss"], "venue": "In JMLR W&CP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In FOCS, pages 103\u2013112,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Smoothed analysis of tensor decompositions", "author": ["Aditya Bhaskara", "Moses Charikar", "Ankur Moitra", "Aravindan Vijayaraghavan"], "venue": "CoRR, abs/1311.3651,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Probability and measure. Wiley Series in Probability and Mathematical Statistics", "author": ["Patrick Billingsley"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Random points in isotropic convex sets. In Convex geometric analysis", "author": ["Jean Bourgain"], "venue": "of Math. Sci. Res. Inst. Publ.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Distributional and L norm inequalities for polynomials over convex bodies in R", "author": ["Anthony Carbery", "James Wright"], "venue": "Mathematical Research Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Super-symmetric decomposition of the fourth-order cumulant tensor", "author": ["J-F Cardoso"], "venue": "blind identification of more sources than sensors. In Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference on, pages 3109\u20133112. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1991}, {"title": "Blind beamforming for non-gaussian signals", "author": ["J.-F. Cardoso", "A. Souloumiac"], "venue": "Radar and Signal Processing, IEE Proceedings F, volume 140, pages 362\u2013370,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Source separation using higher order moments", "author": ["J.F. Cardoso"], "venue": "International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1989}, {"title": "Beyond gaussians: Spectral methods for learning mixtures of heavy-tailed distributions", "author": ["Kamalika Chaudhuri", "Satish Rao"], "venue": "In 21st Annual Conference on Learning Theory - COLT", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Robustness of prewhitening against heavytailed sources. In Independent Component Analysis and Blind Signal Separation", "author": ["Aiyou Chen", "Peter J. Bickel"], "venue": "Fifth International Conference,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "Consistent independent component analysis and prewhitening", "author": ["Aiyou Chen", "Peter J Bickel"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Portfolio value at risk based on independent component analysis", "author": ["Ying Chen", "Wolfgang Hrdle", "Vladimir Spokoiny"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Independent Component Analysis, a new concept", "author": ["P. Comon"], "venue": "Signal Processing, Elsevier,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "Independent component analysis, a new concept", "author": ["Pierre Comon"], "venue": "Signal processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}, {"title": "Handbook of Blind Source Separation", "author": ["Pierre Comon", "Christian Jutten", "editors"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Handbook of Blind Source Separation", "author": ["Pierre Comon", "Christian Jutten", "editors"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Probability for Statistics and Machine Learning", "author": ["A. Dasgupta"], "venue": "Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "On learning mixtures of heavy-tailed distributions", "author": ["Anirban Dasgupta", "John E. Hopcroft", "Jon M. Kleinberg", "Mark Sandler"], "venue": "In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2005),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Learning Mixture of Gaussians", "author": ["S. Dasgupta"], "venue": "40th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1999}, {"title": "A Two Round Variant of EM for Gaussian Mixtures", "author": ["S. Dasgupta", "L. Schulman"], "venue": "16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2000}, {"title": "Adaptive blind separation of independent sources: A deflation approach", "author": ["Nathalie Delfosse", "Philippe Loubaton"], "venue": "Signal Processing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1995}, {"title": "A random polynomial time algorithm for approximating the volume of convex bodies", "author": ["Martin E. Dyer", "Alan M. Frieze", "Ravi Kannan"], "venue": "J. ACM,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1991}, {"title": "PAC Learning Axis Aligned Mixtures of Gaussians with No Separation Assumption", "author": ["J. Feldman", "R.A. Servedio", "R. O\u2019Donnell"], "venue": "In The 19th Annual Conference on Learning Theory,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Learning linear transformations", "author": ["Alan M. Frieze", "Mark Jerrum", "Ravi Kannan"], "venue": "In FOCS,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1996}, {"title": "Geometric tomography, volume 58", "author": ["Richard J Gardner"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1995}, {"title": "Random points in isotropic unconditional convex bodies", "author": ["A. Giannopoulos", "M. Hartzoulaki", "A. Tsolomitis"], "venue": "J. London Math. Soc. (2), 72(3):779\u2013798,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2005}, {"title": "Matrix computations. Johns Hopkins Studies in the Mathematical Sciences", "author": ["Gene H. Golub", "Charles F. Van Loan"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1996}, {"title": "Learning convex bodies is hard", "author": ["Navin Goyal", "Luis Rademacher"], "venue": "In COLT,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Fourier PCA and robust tensor decomposition", "author": ["Navin Goyal", "Santosh Vempala", "Ying Xiao"], "venue": "In Symposium on Theory of Computing,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "The inverse moment problem for convex polytopes", "author": ["Nick Gravin", "Jean Lasserre", "Dmitrii V. Pasechnik", "Sinai Robins"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Geometric Algorithms and Combinatorial Optimization", "author": ["M. Gr\u00f6tschel", "L. Lov\u00e1sz", "A. Schrijver"], "venue": "Springer,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1988}, {"title": "Invariant integration formulas for the nsimplex by combinatorial methods", "author": ["A. Grundmann", "H.M. Moeller"], "venue": "SIAM J. Numer. Anal., 15:282\u2013290,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1978}, {"title": "Lp-norm spherical distribution", "author": ["A.K. Gupta", "D. Song"], "venue": "J. Statist. Plann. Inference, 60(2):241\u2013260,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M. Kakade"], "venue": "In ITCS,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Fast and robust fixed-point algorithms for independent component analysis", "author": ["Aapo Hyv\u00e4rinen"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1999}, {"title": "Independent Component Analysis", "author": ["Aapo Hyvarinen", "Juha Karhunen", "Erkki Oja"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2001}, {"title": "Isoperimetric problems for convex bodies and a localization lemma", "author": ["R. Kannan", "L. Lov\u00e1sz", "M. Simonovits"], "venue": "Discrete Comput. Geom., 13(3-4):541\u2013559,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1995}, {"title": "Random walks and an O\u2217(n5) volume algorithm for convex bodies", "author": ["Ravi Kannan", "L\u00e1szl\u00f3 Lov\u00e1sz", "Mikl\u00f3s Simonovits"], "venue": "Random Structures Algorithms,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1997}, {"title": "Independent component analysis using the spectral measure for alpha-stable distributions", "author": ["Preben Kidmose"], "venue": "In Proceedings of IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2001}, {"title": "Alpha-stable distributions in signal processing of audio signals", "author": ["Preben Kidmose"], "venue": "In 41st Conference on Simulation and Modelling, Scandinavian Simulation Society,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2000}, {"title": "Blind Separation of Heavy Tail Signals", "author": ["Preben Kidmose"], "venue": "PhD thesis, Technical University of Denmark,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2001}, {"title": "Learning geometric concepts via Gaussian surface area", "author": ["Adam R. Klivans", "Ryan O\u2019Donnell", "Rocco A. Servedio"], "venue": "In FOCS,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2008}, {"title": "A lower bound for agnostically learning disjunctions", "author": ["Adam R. Klivans", "Alexander A. Sherstov"], "venue": "In COLT,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2007}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": ["Adam R. Klivans", "Alexander A. Sherstov"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2009}, {"title": "PAC learning intersections of halfspaces with membership", "author": ["Stephen Kwek", "Leonard Pitt"], "venue": "queries. Algorithmica,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1998}, {"title": "The multi-dimensional version of  \u222b axdx", "author": ["Jean B. Lasserre", "Konstantin E. Avrachenkov"], "venue": "American Math. Month.,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2001}, {"title": "Simulated annealing in convex bodies and an O*(n4) volume algorithm", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz", "Santosh Vempala"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2006}, {"title": "On zonotopes", "author": ["P. McMullen"], "venue": "Trans. Amer. Math. Soc., 159:91\u2013109,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1971}, {"title": "Isotropic position and inertia ellipsoids and zonoids of the unit ball of a normed n-dimensional space", "author": ["V.D. Milman", "A. Pajor"], "venue": "Geometric aspects of functional analysis (1987\u201388), volume 1376 of Lecture Notes in Math., pages 64\u2013104. Springer, Berlin,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1989}, {"title": "Settling the polynomial learnability of mixtures of Gaussians", "author": ["A. Moitra", "G. Valiant"], "venue": "51st Annual IEEE Symposium on Foundations of Computer Science (FOCS 2010),", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2010}, {"title": "Noise stability of functions with low influences: Invariance and optimality", "author": ["Elchanan Mossel", "Ryan O\u2019Donnell", "Krzysztof Oleszkiewicz"], "venue": "Annals of Math.,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2010}, {"title": "Learning a parallelepiped: Cryptanalysis of GGH and NTRU signatures", "author": ["Phong Q. Nguyen", "Oded Regev"], "venue": "J. Cryptology,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2009}, {"title": "An Introduction to Integration", "author": ["Ole A Nielsen"], "venue": "Theory and Measure Theory. Wiley,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1997}, {"title": "Stable Distributions - Models for Heavy Tailed Data", "author": ["J.P. Nolan"], "venue": "Birkhauser, Boston,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2015}, {"title": "NP-hardness of largest contained and smallest containing simplices for V- and H-polytopes", "author": ["Asa Packer"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2002}, {"title": "Concentration of mass on convex bodies", "author": ["G. Paouris"], "venue": "Geom. Funct. Anal., 16(5):1021\u20131049,", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2006}, {"title": "Centroid surfaces", "author": ["C.M. Petty"], "venue": "Pacific J. Math., 11(4):1535\u20131547,", "citeRegEx": "81", "shortCiteRegEx": null, "year": 1961}, {"title": "Handbook of Heavy Tailed Distributions in Finance, Volume 1: Handbooks in Finance, Book 1", "author": ["S.T. Rachev"], "venue": "Elsevier,", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2003}, {"title": "Approximate independence of distributions on spheres and their stability properties", "author": ["S.T. Rachev", "L. Ruschendorf"], "venue": "The Annals of Probability, 19(3):1311\u2013 1337,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 1991}, {"title": "On Stirling numbers of the second kind", "author": ["B.C. Rennie", "A.J. Dobson"], "venue": "Journal of Combinatorial Theory, 7(2):116 \u2013 121,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 1969}, {"title": "Sampling inequalities for infinitely smooth functions, with applications to interpolation and machine learning", "author": ["Christian Rieger", "Barbara Zwicknagl"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2010}, {"title": "Moment recurrence relations for binomial, poisson and hypergeometric frequency distributions", "author": ["John Riordan"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 1937}, {"title": "Optimization of conditional value-at-risk", "author": ["R.T. Rockafellar", "S.P. Uryasev"], "venue": "The Journal of Risk, (1):21\u201341,", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2000}, {"title": "Random vectors in the isotropic position", "author": ["M. Rudelson"], "venue": "J. Funct. Anal., 164(1):60\u201372,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 1999}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Comm. Pure Appl. Math.,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2009}, {"title": "Blind source separation of noisy mixtures using a semi-parametric approach with application to heavy-tailed signals", "author": ["M. Sahmoudi", "K. Abed-Meraim", "M. Lavielle", "E. Kuhn", "Ph. Ciblat"], "venue": "In Proc. of EUSIPCO", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2005}, {"title": "On the volume of the intersection of two l  p balls", "author": ["G. Schechtman", "J. Zinn"], "venue": "Proc. Amer. Math. Soc, volume 110, pages 217\u2013224,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1990}, {"title": "Convex bodies: the Brunn-Minkowski theory, volume 44 of Encyclopedia of Mathematics and its Applications", "author": ["Rolf Schneider"], "venue": null, "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1993}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["Bernhard Sch\u00f6lkopf", "John C. Platt", "John Shawe-Taylor", "Alex J. Smola", "Robert C. Williamson"], "venue": "Neural Computation,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2001}, {"title": "Super-efficiency in blind signal separation of symmetric heavy-tailed sources", "author": ["Yoav Shereshevski", "Arie Yeredor", "Hagit Messer"], "venue": "In Statistical Signal Processing,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2001}, {"title": "Lp-nested symmetric distributions", "author": ["Fabian Sinz", "Matthias Bethge"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2010}, {"title": "The conjoint effect of divisive normalization and orientation selectivity on redundancy reduction", "author": ["Fabian H. Sinz", "Matthias Bethge"], "venue": "In NIPS,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2008}, {"title": "Lp-norm uniform distribution", "author": ["D. Song", "A.K. Gupta"], "venue": "Proc. Amer. Math. Soc., 125(2):595\u2013601,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 1997}, {"title": "Applications of analytic centers for the numerical solution of semiinfinite, convex programs arising in control theory", "author": ["G. Sonnevend"], "venue": "DFG report Nr. 170/1989, Univ. W\u00fcrzburg, Inst. f. angew. Mathematik,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 1989}, {"title": "Covariance estimation for distributions with 2 + moments", "author": ["Nikhil Srivastava", "Roman Vershynin"], "venue": null, "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2011}, {"title": "Covariance estimation for distributions with 2 + \u03b5 moments", "author": ["Nikhil Srivastava", "Roman Vershynin"], "venue": "Ann. Probab.,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2013}, {"title": "Matrix perturbation theory", "author": ["Gilbert W Stewart", "Ji-guang Sun"], "venue": null, "citeRegEx": "102", "shortCiteRegEx": "102", "year": 1990}, {"title": "A Spectral Algorithm for Learning Mixtures of Distributions", "author": ["S. Vempala", "G. Wang"], "venue": "43rd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning convex concepts from Gaussian distributions with pca", "author": ["Santosh Vempala"], "venue": "In FOCS, pages 541\u2013550,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2010}, {"title": "Learning convex concepts from Gaussian distributions with PCA", "author": ["Santosh Vempala"], "venue": "In FOCS, pages 124\u2013130,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2010}, {"title": "Structure from local optima: Learning subspace juntas via higher order PCA", "author": ["Santosh S. Vempala", "Ying Xiao"], "venue": "CoRR, abs/1108.3329,", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2011}, {"title": "How close is the sample covariance matrix to the actual covariance matrix", "author": ["Roman Vershynin"], "venue": "J. Theoret. Probab.,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2012}, {"title": "Ica by maximizing nonstability", "author": ["Baijie Wang", "Ercan E Kuruoglu", "Junying Zhang"], "venue": "In Independent Component Analysis and Signal Separation,", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2009}, {"title": "Scattered data approximation, volume 17", "author": ["Holger Wendland"], "venue": null, "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2005}, {"title": "Moments and Absolute Moments of the Normal Distribution", "author": ["A. Winkelbauer"], "venue": "ArXiv e-prints, September", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2012}, {"title": "Blind source separation via the second characteristic function", "author": ["Arie Yeredor"], "venue": "Signal Processing,", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2000}], "referenceMentions": [{"referenceID": 41, "context": "The first such reduction is a solution to the open problem of efficiently learning the intersection of n + 1 halfspaces in R, posed in [43].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "This pre-processing step is a random non-linear scaling inspired by the study of `p balls [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "This chapter is based on [10] and [9].", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "This chapter is based on [10] and [9].", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "This chapter is based on work published in [11].", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "This chapter is based on [8].", "startOffset": 25, "endOffset": 28}, {"referenceID": 30, "context": ", [31, 57, 33].", "startOffset": 2, "endOffset": 14}, {"referenceID": 53, "context": ", [31, 57, 33].", "startOffset": 2, "endOffset": 14}, {"referenceID": 32, "context": ", [31, 57, 33].", "startOffset": 2, "endOffset": 14}, {"referenceID": 38, "context": "algorithms of [39, 43], explicitly require the fourth moment to be finite.", "startOffset": 14, "endOffset": 22}, {"referenceID": 41, "context": "algorithms of [39, 43], explicitly require the fourth moment to be finite.", "startOffset": 14, "endOffset": 22}, {"referenceID": 103, "context": "Algorithms in [111, 48], which make use of the characteristic function also seem to require at least the fourth moment to be finite: while the characteristic function exists for distributions without moments, the algorithms in these papers use the second or higher derivatives of the (second) characteristic function, and for this to be well-defined one needs the moments of that order to exist.", "startOffset": 14, "endOffset": 23}, {"referenceID": 46, "context": "Algorithms in [111, 48], which make use of the characteristic function also seem to require at least the fourth moment to be finite: while the characteristic function exists for distributions without moments, the algorithms in these papers use the second or higher derivatives of the (second) characteristic function, and for this to be well-defined one needs the moments of that order to exist.", "startOffset": 14, "endOffset": 23}, {"referenceID": 56, "context": "[62, 64, 95, 28, 29, 91, 108].", "startOffset": 0, "endOffset": 29}, {"referenceID": 58, "context": "[62, 64, 95, 28, 29, 91, 108].", "startOffset": 0, "endOffset": 29}, {"referenceID": 87, "context": "[62, 64, 95, 28, 29, 91, 108].", "startOffset": 0, "endOffset": 29}, {"referenceID": 27, "context": "[62, 64, 95, 28, 29, 91, 108].", "startOffset": 0, "endOffset": 29}, {"referenceID": 28, "context": "[62, 64, 95, 28, 29, 91, 108].", "startOffset": 0, "endOffset": 29}, {"referenceID": 83, "context": "[62, 64, 95, 28, 29, 91, 108].", "startOffset": 0, "endOffset": 29}, {"referenceID": 100, "context": "[62, 64, 95, 28, 29, 91, 108].", "startOffset": 0, "endOffset": 29}, {"referenceID": 71, "context": "Heavy-tailed distributions arise in a wide variety of contexts including signal processing and finance; see [78, 82] for an extensive bibliography.", "startOffset": 108, "endOffset": 116}, {"referenceID": 75, "context": "Heavy-tailed distributions arise in a wide variety of contexts including signal processing and finance; see [78, 82] for an extensive bibliography.", "startOffset": 108, "endOffset": 116}, {"referenceID": 71, "context": ", [78].", "startOffset": 2, "endOffset": 6}, {"referenceID": 35, "context": ", [36, 27].", "startOffset": 2, "endOffset": 10}, {"referenceID": 26, "context": ", [36, 27].", "startOffset": 2, "endOffset": 10}, {"referenceID": 29, "context": ", [30]), heavy tailed distributions are commonly used to model catastrophic but somewhat unlikely scenarios.", "startOffset": 2, "endOffset": 6}, {"referenceID": 80, "context": "A standard measures of risk in that literature, the so called conditional value at risk [87], is only finite when the first moment is finite.", "startOffset": 88, "endOffset": 92}, {"referenceID": 46, "context": "The theorem below refers to the algorithm Fourier PCA [48] which solves ICA under the fourth moment assumption.", "startOffset": 54, "endOffset": 58}, {"referenceID": 66, "context": "is a quadratic form in u and its square root is the support function of a convex body, Legendre\u2019s inertia ellipsoid, up to some scaling factor (see [73] for example).", "startOffset": 148, "endOffset": 152}, {"referenceID": 46, "context": "to use an existing ICA algorithm (from [48] in our case) to handle the resulting ICA instance.", "startOffset": 39, "endOffset": 43}, {"referenceID": 74, "context": "The centroid body of a compact set was first defined in [81].", "startOffset": 56, "endOffset": 60}, {"referenceID": 48, "context": "The implementation works by first implementing a membership oracle for the polar of the centroid body via sampling and then using it via the ellipsoid method (see [50]) to construct a membership oracle for the centroid body.", "startOffset": 163, "endOffset": 167}, {"referenceID": 74, "context": "A fundamental property of the centroid body, for our analysis, is that the centroid body is linearly equivariant, that is, if one applies an invertible linear transformation to a probability measure then the corresponding centroid body transforms in the same way (already observed in [81]).", "startOffset": 284, "endOffset": 288}, {"referenceID": 38, "context": ", [39, 43]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 41, "context": ", [39, 43]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 21, "context": "Another related work is [22], on isotropic PCA, affine invariant clustering, and learning mixtures of Gaussians.", "startOffset": 24, "endOffset": 28}, {"referenceID": 103, "context": "Finally, in [111, 48] a different reweighting, using a \u201cFourier weight\u201d e T x (here u \u2208 R is a fixed vector and x \u2208 R is a data point) is used in the computation of the covariance matrix.", "startOffset": 12, "endOffset": 21}, {"referenceID": 46, "context": "Finally, in [111, 48] a different reweighting, using a \u201cFourier weight\u201d e T x (here u \u2208 R is a fixed vector and x \u2208 R is a data point) is used in the computation of the covariance matrix.", "startOffset": 12, "endOffset": 21}, {"referenceID": 48, "context": "This follows from applications of the ellipsoid method from [50].", "startOffset": 60, "endOffset": 64}, {"referenceID": 48, "context": "The definitions and theorems in this section all come (occasionally with slight rephrasing) from [50] except for the notion of ( , \u03b4)-weak oracle.", "startOffset": 97, "endOffset": 101}, {"referenceID": 48, "context": "This is done in [50] as they work out in detail the important low level issues of how the numbers in the algorithm are represented as general real numbers cannot be directly handled by computers.", "startOffset": 16, "endOffset": 20}, {"referenceID": 48, "context": "Definition 1 ([50]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "For \u03b4 \u2208 [0, 1], an ( , \u03b4)-weak membership oracle for K acts as follows: Given a point y \u2208 Q, with probability at least 1 \u2212 \u03b4 it solves the -weak membership problem for y,K, and otherwise its output can be arbitrary.", "startOffset": 8, "endOffset": 14}, {"referenceID": 48, "context": "Definition 2 ([50]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 48, "context": "2 in [50]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 48, "context": "2 as stated in [50] is stronger than the above statement in that it constructs a weak violation oracle (not defined here) which gives a weak validity oracle which suffices for us.", "startOffset": 15, "endOffset": 19}, {"referenceID": 48, "context": "1 in [50]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "The result follows from the random walk-based algorithms to generate approximately uniformly random points from a convex body [41, 60, 70].", "startOffset": 126, "endOffset": 138}, {"referenceID": 55, "context": "The result follows from the random walk-based algorithms to generate approximately uniformly random points from a convex body [41, 60, 70].", "startOffset": 126, "endOffset": 138}, {"referenceID": 64, "context": "The result follows from the random walk-based algorithms to generate approximately uniformly random points from a convex body [41, 60, 70].", "startOffset": 126, "endOffset": 138}, {"referenceID": 20, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 81, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 43, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 73, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 14, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 99, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 1, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 93, "context": "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].", "startOffset": 83, "endOffset": 116}, {"referenceID": 74, "context": ", [81, 73, 44]) convex body associated to (the uniform distribution on) a given convex body.", "startOffset": 2, "endOffset": 14}, {"referenceID": 66, "context": ", [81, 73, 44]) convex body associated to (the uniform distribution on) a given convex body.", "startOffset": 2, "endOffset": 14}, {"referenceID": 42, "context": ", [81, 73, 44]) convex body associated to (the uniform distribution on) a given convex body.", "startOffset": 2, "endOffset": 14}, {"referenceID": 74, "context": "Following [81], consider the function h(u) = E(|\u3008u,X\u3009|).", "startOffset": 10, "endOffset": 14}, {"referenceID": 74, "context": "It is a slight generalization of statements in [81] and [44, Theorem 9.", "startOffset": 47, "endOffset": 51}, {"referenceID": 48, "context": "2 of [50] (stated as Theorem 2.", "startOffset": 5, "endOffset": 9}, {"referenceID": 48, "context": "1 of [50] (Lemma 2 here) gives an algorithm to construct an ( , \u03b4)weak membership oracle WMEM\u0393X( , \u03b4, 1/r, 1/R) from WVAL(\u0393X)\u25e6( 1, \u03b4, R, r).", "startOffset": 5, "endOffset": 9}, {"referenceID": 48, "context": "1 in [50] shows WMEM\u0393X( , \u03b4, 1/r, 1/R) calls WVAL(\u0393X)\u25e6( 1, \u03b4, R, r) once, with 1 \u2265 1/poly(1/ , \u2016y\u2016, 1/r) (where y is the query point).", "startOffset": 5, "endOffset": 9}, {"referenceID": 48, "context": "2 from [50] would apply and would give that WVAL(\u0393X)\u25e6 outputs an answer as expected.", "startOffset": 7, "endOffset": 11}, {"referenceID": 39, "context": "2 sampling algorithm such as the one in [41] with c = /(2(n+1) ), r = sm/ \u221a n, R = sM \u221a n, and same \u03b4.", "startOffset": 40, "endOffset": 44}, {"referenceID": 66, "context": "It is known that any n-dimensional isotropic convex body is contained in the ball of radius n + 1 [73, 99],[59, Theorem 4.", "startOffset": 98, "endOffset": 106}, {"referenceID": 91, "context": "It is known that any n-dimensional isotropic convex body is contained in the ball of radius n + 1 [73, 99],[59, Theorem 4.", "startOffset": 98, "endOffset": 106}, {"referenceID": 0, "context": "Generate z \u223c U [0, 1].", "startOffset": 15, "endOffset": 21}, {"referenceID": 46, "context": "2 from [48] in a special case by setting parameters k and ki in that theorem to 4 for i \u2208 [n].", "startOffset": 7, "endOffset": 11}, {"referenceID": 46, "context": "2 of [48] is called Fourier PCA.", "startOffset": 5, "endOffset": 9}, {"referenceID": 46, "context": "[48] Let X \u2208 R be given by an ICA model X = AS where A \u2208 Rn\u00d7n is unitary and the Si are mutually independent, E[S i ] \u2264M4 for some positive constant M4, and |\u03ba4(Si)| \u2265 \u2206.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": ", using the algorithm in [43]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 46, "context": "The algorithm in [48] estimates the second derivative of \u03c8X(u) and computes its eigendecomposition.", "startOffset": 17, "endOffset": 21}, {"referenceID": 103, "context": "(In [111] and [48], this second derivative is interpreted as a kind of covariance matrix of X but with the twist that a certain \u201cFourier\u201d weight is used in the expectation computation for the covariance matrix.", "startOffset": 4, "endOffset": 9}, {"referenceID": 46, "context": "(In [111] and [48], this second derivative is interpreted as a kind of covariance matrix of X but with the twist that a certain \u201cFourier\u201d weight is used in the expectation computation for the covariance matrix.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "1 \u2208 [0, 1].", "startOffset": 4, "endOffset": 10}, {"referenceID": 46, "context": "While this theorem is not stated in [48], it is easy to derive from their proof of Theorem 2.", "startOffset": 36, "endOffset": 40}, {"referenceID": 46, "context": "1 in [48].", "startOffset": 5, "endOffset": 9}, {"referenceID": 46, "context": "where we ensured that \u2016\u03a8\u0303X\u0302(u)\u2212\u03a8X\u0302(u)\u2016F < by taking sufficiently many samples of X\u0302 to get a good estimate with probability at least \u03b4; as in [48], a standard concentration argument shows that poly(n,M4, 1/\u2206, 1/ , 1/\u03b4) samples suffice for this purpose.", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "As noted above, the technique in [10], while being provably efficient and correct, suffers from practical implementation issues.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "The former, orthogonalization via centroid body scaling, uses the samples already present in the algorithm rather than relying on a random walk to draw samples which are approximately uniform in the algorithm\u2019s approximation of the centroid body (as is done in [10]).", "startOffset": 261, "endOffset": 265}, {"referenceID": 9, "context": "In [10], another orthogonalization procedure, namely orthogonalization via the uniform distribution in the centroid body is theoretically proven to work.", "startOffset": 3, "endOffset": 7}, {"referenceID": 53, "context": "It\u2019s standard to use covariance matrix for whitening when the second moments of all independent components exist [57]: Given samples from the ICA model X = AS, we compute the empirical covariance matrix \u03a3\u0303 which tends to the true covariance matrix as we take more samples and set B = \u03a3\u0303\u22121/2.", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": ", [28]), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [28] also provided some theoretical analysis to explain this surprising performance.", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [28]), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [28] also provided some theoretical analysis to explain this surprising performance.", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "The following lemma from [10] says that the empirical average of the absolute value of X converges to the expectation of |X|.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "For \u03b4 \u2208 [0, 1], an ( , \u03b4)-weak membership oracle for K acts as follows: Given a point y \u2208 Q, with probability at least 1\u2212 \u03b4 it solves the -weak membership problem for y,K, and otherwise its output can be arbitrary.", "startOffset": 8, "endOffset": 14}, {"referenceID": 65, "context": "This is proven in [72].", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "the Frobenius norm to measure the error, but all experiments were also performed using the well-known Amari index [6]; the results have similar behavior and are not presented here.", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "As proposed in [10], Gaussian damping is a preprocessing technique that converts data from an ICA model X = AS, where A is unitary (columns are orthogonal with unit l2-norm) to data from a related ICA model XR = ASR, where R > 0 is a parameter to be chosen.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "For more details about the technical requirements for choosing R, see [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": ", [26] .", "startOffset": 2, "endOffset": 6}, {"referenceID": 57, "context": "[63]) that speech data can be modeled by \u03b1-stable distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "To estimate whether the data is heavy-tailed, as in [63], we estimate parameter \u03b1 of a best-fit \u03b1-stable distribution.", "startOffset": 52, "endOffset": 56}, {"referenceID": 45, "context": "For general n, it is known to require 2 \u221a n) samples [47] (see also [65] for a similar lower bound in a different but related model of learning).", "startOffset": 53, "endOffset": 57}, {"referenceID": 59, "context": "For general n, it is known to require 2 \u221a n) samples [47] (see also [65] for a similar lower bound in a different but related model of learning).", "startOffset": 68, "endOffset": 72}, {"referenceID": 45, "context": "As mentioned in [47], it turns out that if the body has few facets (e.", "startOffset": 16, "endOffset": 20}, {"referenceID": 86, "context": "[94] for the problem of estimating the support of a probability distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": ") However, the problem of finding a minimum volume simplex is in general NP-hard [79].", "startOffset": 81, "endOffset": 85}, {"referenceID": 41, "context": "[43] gave an efficient algorithm for this problem (with some restrictions on the allowed distributions, but also with some weaker requirements than full independence) along with most of the details of a rigorous analysis (a complete analysis", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "of a special case can be found in [13]; see also [106] for a generalization of ICA to subspaces along with a rigorous analysis).", "startOffset": 34, "endOffset": 38}, {"referenceID": 98, "context": "of a special case can be found in [13]; see also [106] for a generalization of ICA to subspaces along with a rigorous analysis).", "startOffset": 49, "endOffset": 54}, {"referenceID": 41, "context": "[43] asked if one could learn other convex bodies, and in particular simplices, efficiently from uniformly random samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[76] gave a simpler and rigorous algorithm and analysis for the case of learning parallelepipeds with similarities to the popular FastICA algorithm of [56].", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[76] gave a simpler and rigorous algorithm and analysis for the case of learning parallelepipeds with similarities to the popular FastICA algorithm of [56].", "startOffset": 151, "endOffset": 155}, {"referenceID": 69, "context": "The algorithm in [76] is a first order algorithm unlike Frieze et al.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "The algorithms in both [43, 76] make use of the fourth moment function of the probability distribution.", "startOffset": 23, "endOffset": 31}, {"referenceID": 69, "context": "The algorithms in both [43, 76] make use of the fourth moment function of the probability distribution.", "startOffset": 23, "endOffset": 31}, {"referenceID": 33, "context": "More information on ICA including historical remarks can be found in [58, 34].", "startOffset": 69, "endOffset": 77}, {"referenceID": 41, "context": "Let us note that [43] allow certain kinds of dependencies among the components, however this does not appear to be useful for learning simplices.", "startOffset": 17, "endOffset": 21}, {"referenceID": 61, "context": "[67] prove that learning intersections of n half-spaces in R (for constant > 0) is hard under standard cryptographic assumptions (PAC-learning is possible, however, if one also has access to a membership oracle in addition to random samples [68]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[67] prove that learning intersections of n half-spaces in R (for constant > 0) is hard under standard cryptographic assumptions (PAC-learning is possible, however, if one also has access to a membership oracle in addition to random samples [68]).", "startOffset": 241, "endOffset": 245}, {"referenceID": 60, "context": "[66, 104, 105] and references therein.", "startOffset": 0, "endOffset": 14}, {"referenceID": 96, "context": "[66, 104, 105] and references therein.", "startOffset": 0, "endOffset": 14}, {"referenceID": 97, "context": "[66, 104, 105] and references therein.", "startOffset": 0, "endOffset": 14}, {"referenceID": 41, "context": "[43] and [47] consider the uniform distribution over the intersection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[43] and [47] consider the uniform distribution over the intersection.", "startOffset": 9, "endOffset": 13}, {"referenceID": 47, "context": "[49] show how to reconstruct a polytope with N vertices in R, given its first O(nN) moments in (n + 1) random directions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "In our setting, where we have access to only a polynomial number of random samples, it\u2019s not clear how to compute moments of such high orders to the accuracy required for the algorithm of [49] even for simplices.", "startOffset": 188, "endOffset": 192}, {"referenceID": 6, "context": "A recent and parallel work of [7] is closely related to ours.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.", "startOffset": 103, "endOffset": 107}, {"referenceID": 84, "context": "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.", "startOffset": 159, "endOffset": 171}, {"referenceID": 76, "context": "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.", "startOffset": 159, "endOffset": 171}, {"referenceID": 90, "context": "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.", "startOffset": 159, "endOffset": 171}, {"referenceID": 41, "context": ", [43]), this transformation can be determined up to a rotation from the mean and the covariance matrix of the uniform distribution on the given simplex.", "startOffset": 2, "endOffset": 6}, {"referenceID": 52, "context": "A fixed point-like iteration (inspired by the analysis of FastICA [56] and of gradient descent in [76]) starting from a random point in the unit sphere finds a local maximum efficiently with high probability.", "startOffset": 66, "endOffset": 70}, {"referenceID": 69, "context": "A fixed point-like iteration (inspired by the analysis of FastICA [56] and of gradient descent in [76]) starting from a random point in the unit sphere finds a local maximum efficiently with high probability.", "startOffset": 98, "endOffset": 102}, {"referenceID": 69, "context": "By definition of total variation distance, Subroutine 5 succeeds with almost as large probability when given a sample from S (an argument already used in [76]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "The cone measure on the surface \u2202K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by", "startOffset": 77, "endOffset": 93}, {"referenceID": 84, "context": "The cone measure on the surface \u2202K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by", "startOffset": 77, "endOffset": 93}, {"referenceID": 76, "context": "The cone measure on the surface \u2202K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by", "startOffset": 77, "endOffset": 93}, {"referenceID": 90, "context": "The cone measure on the surface \u2202K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by", "startOffset": 77, "endOffset": 93}, {"referenceID": 84, "context": "From [92] and [83] we have the following representation of the cone measure on \u2202B p : 85", "startOffset": 5, "endOffset": 9}, {"referenceID": 76, "context": "From [92] and [83] we have the following representation of the cone measure on \u2202B p : 85", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "From [16], we also have the following variation, a representation of the uniform distribution in B p :", "startOffset": 5, "endOffset": 9}, {"referenceID": 63, "context": "Similar more general formulas appear in [69].", "startOffset": 40, "endOffset": 44}, {"referenceID": 49, "context": "We will use the following result from [51] for \u03b1i \u2265 0: \u222b", "startOffset": 38, "endOffset": 42}, {"referenceID": 69, "context": "As we noted in the introduction, our algorithm is inspired by the algorithm of [76] for the related problem of learning hypercubes and also by the FastICA algorithm in [56].", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "As we noted in the introduction, our algorithm is inspired by the algorithm of [76] for the related problem of learning hypercubes and also by the FastICA algorithm in [56].", "startOffset": 168, "endOffset": 172}, {"referenceID": 69, "context": "With the right update rule in hand the analysis turns out to be quite similar to the one in [76].", "startOffset": 92, "endOffset": 96}, {"referenceID": 41, "context": "A natural approach to do this would be to use gradient descent or Newton\u2019s method (this was done in [43]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 69, "context": "Our analysis has the same outline as that of [76].", "startOffset": 45, "endOffset": 49}, {"referenceID": 69, "context": "This is because the iteration that we get is the same as that of [76] except that cubing is replaced by squaring (see below); however some details in our proof are different.", "startOffset": 65, "endOffset": 69}, {"referenceID": 69, "context": "(A similar argument is made in [76] with different parameters.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "The next lemma complements the main result in [16], Theorem 1 (Theorem 3.", "startOffset": 46, "endOffset": 50}, {"referenceID": 88, "context": "The use of a non-linear scaling step to turn a distribution into one having independent components has been done before [96, 97], but there it is applied after finding a transformation that makes the distribution axis-aligned.", "startOffset": 120, "endOffset": 128}, {"referenceID": 89, "context": "The use of a non-linear scaling step to turn a distribution into one having independent components has been done before [96, 97], but there it is applied after finding a transformation that makes the distribution axis-aligned.", "startOffset": 120, "endOffset": 128}, {"referenceID": 88, "context": "[96, 97], without independent components, and therefore the use of ICA is somewhat heuristic.", "startOffset": 0, "endOffset": 8}, {"referenceID": 89, "context": "[96, 97], without independent components, and therefore the use of ICA is somewhat heuristic.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "This line of work was started in [37] where the first algorithm to recover parameters using a number of samples polynomial in the dimension was presented.", "startOffset": 33, "endOffset": 37}, {"referenceID": 37, "context": ", [38, 12, 103, 1, 42]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 11, "context": ", [38, 12, 103, 1, 42]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 95, "context": ", [38, 12, 103, 1, 42]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": ", [38, 12, 103, 1, 42]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 40, "context": ", [38, 12, 103, 1, 42]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 17, "context": "A completion of the attempts to weaken the separation conditions was achieved in [18] and [74], where it was shown that arbitrarily small separation was sufficient for learning a general mixture with a fixed number of components in polynomial time.", "startOffset": 81, "endOffset": 85}, {"referenceID": 67, "context": "A completion of the attempts to weaken the separation conditions was achieved in [18] and [74], where it was shown that arbitrarily small separation was sufficient for learning a general mixture with a fixed number of components in polynomial time.", "startOffset": 90, "endOffset": 94}, {"referenceID": 67, "context": "Moreover, a one-dimensional example given in [74] showed that an exponential dependence on the number of components", "startOffset": 45, "endOffset": 49}, {"referenceID": 37, "context": "It is worth noting that while quite different in many aspects, all of these papers used a general scheme similar to that in the original work [38] by reducing high-dimensional inference to a small number of low-dimensional problems through appropriate projections.", "startOffset": 142, "endOffset": 146}, {"referenceID": 51, "context": "However, a surprising result was recently proved in [54].", "startOffset": 52, "endOffset": 56}, {"referenceID": 51, "context": "The result in [54] is inherently high-dimensional as that condition is never satisfied when the means belong to a lower-dimensional space.", "startOffset": 14, "endOffset": 18}, {"referenceID": 67, "context": "The one-dimensional example in [74] cannot answer this question as it is a specific worst-case scenario, which can be potentially ruled out by some genericity condition.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "However, we show that for k points uniformly sampled from [0, 1] there are (with high probability) two mixtures of unit Gaussians with means on non-intersecting subsets of these points, whose L distance is O\u2217(e\u2212k) and which are thus not polynomially identifiable.", "startOffset": 58, "endOffset": 64}, {"referenceID": 82, "context": "To do that we provide smoothed analysis of the condition number using certain results from [90] and anti-concentration inequalities.", "startOffset": 91, "endOffset": 95}, {"referenceID": 46, "context": "We combine this with the recent work on efficient algorithms for underdetermined ICA from [48] to obtain the necessary bounds.", "startOffset": 90, "endOffset": 94}, {"referenceID": 46, "context": "3 of [48]; it should not be difficult, however, to adapt the algorithm to use a method similar to that of [54] to handle the case where m < n.", "startOffset": 5, "endOffset": 9}, {"referenceID": 51, "context": "3 of [48]; it should not be difficult, however, to adapt the algorithm to use a method similar to that of [54] to handle the case where m < n.", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "We point out the simultaneous and independent work of [19], where the authors prove learnability results related to our Theorems 4.", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "The results in [19], which are based on tensor decompositions, are stronger in that they can learn mixtures of axis-aligned Gaussians (with non-identical covariance matrices) without requiring to know the covariance matrices in advance.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "Let X be a set of 4k points uniformly sampled from [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 51, "context": "This is in contrast to conjectured computational barriers that arise in related settings based on the noisy parity problem (see [54] for pointers).", "startOffset": 128, "endOffset": 132}, {"referenceID": 67, "context": "The only previous information-theoretic lower bound for learning GMMs we are aware of is due to [74] and holds for two specially designed onedimensional mixtures.", "startOffset": 96, "endOffset": 100}, {"referenceID": 46, "context": "The ICA algorithm from [48] to which we will be reducing learning a GMM relies on the shared tensor structure of the derivatives of the second characteristic function and the higher order multi-variate cumulants.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": "1, from [48]) allows us to recover A up to the necessary ambiguities in the noisy ICA setting.", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "The theorem establishes guarantees for an algorithm from [48] for noisy underdetermined ICA, UnderdeterminedICA.", "startOffset": 57, "endOffset": 61}, {"referenceID": 46, "context": "The following theorem (from [48]) allows us to recover A up to the necessary ambiguities in the noisy ICA setting.", "startOffset": 28, "endOffset": 32}, {"referenceID": 46, "context": "The theorem establishes guarantees for an algorithm from [48] for noisy underdetermined ICA, UnderdeterminedICA.", "startOffset": 57, "endOffset": 61}, {"referenceID": 46, "context": "1 ([48]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "The algorithm then calculates various internal paremeters: a bound on directional covariances, error parameters to be split between the \u201cPoissonization\u201d process and the call to UnderdeterminedICA, the threshold parameter \u03c4 and Poisson parameter \u03bb to be used in Subroutine 5, and values explicitly needed by the proof of UnderdeterminedICA in [48].", "startOffset": 342, "endOffset": 346}, {"referenceID": 46, "context": "We show that the reduction results in a model that allows one to use the ICA algorithm UnderdeterminedICA presented in [48] (see Section 2.", "startOffset": 119, "endOffset": 123}, {"referenceID": 79, "context": "It is known (see [86]) that EY ` = \u2211\u0300", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "15 in [5]),", "startOffset": 6, "endOffset": 9}, {"referenceID": 46, "context": "1 We now show that after the reduction is applied, we can use the ICA routine given in [48] to learn the GMM.", "startOffset": 87, "endOffset": 91}, {"referenceID": 46, "context": ") As in [48], it will be convenient to work with the multilinear part of the Khatri\u2013Rao product: For a column vector Ak \u2208 R define A 2 k \u2208 R( n 2), a subvector of A 2 k \u2208 R 2 , given by (A 2 k )ij := (Ak)i(Ak)j for 1 \u2264 i < j \u2264 n.", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "This can be proved along the lines of a similar result in [48].", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "We will apply the anticoncentration inequality of Carbery\u2013Wright [23] to this polynomial to conclude that the distance between the k\u2019th column of (M + N) 2 and the span of the rest of the columns is unlikely to be very small (see Appendix 4.", "startOffset": 65, "endOffset": 69}, {"referenceID": 67, "context": "This goes beyond the specific example of exponential closeness given in [74] as we demonstrate that such mixtures are ubiquitous as long as there is no lower bound on the separation between the components.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "Specifically, let S be a cube [0, 1] \u2282 R.", "startOffset": 30, "endOffset": 36}, {"referenceID": 0, "context": "Let X be any subset of k points in [0, 1] .", "startOffset": 35, "endOffset": 41}, {"referenceID": 0, "context": "Let g be any positive function with L2 norm 1 supported on [0, 1] n and let f = Kg.", "startOffset": 59, "endOffset": 65}, {"referenceID": 78, "context": "From [85], Corollary 5.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "1 (taking \u03bb = 0) we have that for some A > 0 and h sufficiently small \u2016f \u2212 fX,k\u2016L\u221e([0,1]n) < exp(A log h h )", "startOffset": 83, "endOffset": 88}, {"referenceID": 0, "context": "and \u2016f \u2212 fX,k\u2016L2([0,1]n) < exp(A log h h )", "startOffset": 17, "endOffset": 22}, {"referenceID": 0, "context": "Note that the norm is on [0, 1] while we need to control the norm on R.", "startOffset": 25, "endOffset": 31}, {"referenceID": 101, "context": "28 of [109]) \u2016f \u2212 fX,k\u2016H = \u3008f \u2212 fX,k, f \u2212 fX,k\u3009H = \u3008f \u2212 fX,k, f\u3009H = \u3008f \u2212 fX,k,Kg\u3009H = \u3008f \u2212 fX,k, g\u3009L2([0,1]n) \u2264 \u2016f \u2212 fX,k\u2016L2(X)\u2016g\u2016L2(X) < exp(A log h h )", "startOffset": 6, "endOffset": 11}, {"referenceID": 0, "context": "28 of [109]) \u2016f \u2212 fX,k\u2016H = \u3008f \u2212 fX,k, f \u2212 fX,k\u3009H = \u3008f \u2212 fX,k, f\u3009H = \u3008f \u2212 fX,k,Kg\u3009H = \u3008f \u2212 fX,k, g\u3009L2([0,1]n) \u2264 \u2016f \u2212 fX,k\u2016L2(X)\u2016g\u2016L2(X) < exp(A log h h )", "startOffset": 101, "endOffset": 106}, {"referenceID": 0, "context": "Let X and Y be any two subsets of [0, 1] with fill h.", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "By integrating over the interval [0, 1], and since f is strictly positive on the interval, it is easy to see that \u03b1 > C (and by the same token \u03b2 > C), where C is some universal constant.", "startOffset": 33, "endOffset": 39}, {"referenceID": 24, "context": "The most theoretically justified ICA algorithms have relied on the tensor structure of multivariate cumulants, including the early, popular practical algorithm JADE [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 13, "context": "In the fully determined ICA setting in which the number source signals does not exceed the ambient dimension, the papers [14] and [17] demonstrate that ICA with additive Gaussian noise can be solved in polynomial time and using polynomial samples.", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "In the fully determined ICA setting in which the number source signals does not exceed the ambient dimension, the papers [14] and [17] demonstrate that ICA with additive Gaussian noise can be solved in polynomial time and using polynomial samples.", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "The tensor structure of the cumulants was (to the best of our knowledge) first exploited in [24] and later in [4] to solve underdetermined ICA.", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "The tensor structure of the cumulants was (to the best of our knowledge) first exploited in [24] and later in [4] to solve underdetermined ICA.", "startOffset": 110, "endOffset": 113}, {"referenceID": 46, "context": "Finally, [48] provides", "startOffset": 9, "endOffset": 13}, {"referenceID": 82, "context": "2 Rudelson-Vershynin subspace bound Lemma 31 (Rudelson\u2013Vershynin [90]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 68, "context": "The version of the anticoncentration inequality we use is explicitly given in [75] which in turn follows immediately from [23]:", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The version of the anticoncentration inequality we use is explicitly given in [75] which in turn follows immediately from [23]:", "startOffset": 122, "endOffset": 126}, {"referenceID": 68, "context": "Lemma 32 ([75]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": ", [35].", "startOffset": 2, "endOffset": 6}, {"referenceID": 77, "context": "The following bound comes from [84] Theorem 3.", "startOffset": 31, "endOffset": 35}, {"referenceID": 102, "context": "For general `, it is known (see [110]) that", "startOffset": 32, "endOffset": 37}, {"referenceID": 70, "context": "3 in [77] and Sect.", "startOffset": 5, "endOffset": 9}], "year": 2017, "abstractText": "Data-driven applications are growing. Machine learning and data analysis now finds both scientific and industrial application in biology, chemistry, geology, medicine, and physics. These applications rely on large quantities of data gathered from automated sensors and user input. Furthermore, the dimensionality of many datasets is extreme: more details are being gathered about single user interactions or sensor readings. All of these applications encounter problems with a common theme: use observed data to make inferences about the world. Our work obtains the first provably efficient algorithms for Independent Component Analysis (ICA) in the presence of heavy-tailed data. The main tool in this result is the centroid body (a well-known topic in convex geometry), along with optimization and random walks for sampling from a convex body. This is the first algorithmic use of the centroid body and it is of independent theoretical interest, since it effectively replaces the estimation of covariance from samples, and is more generally accessible. We demonstrate that ICA is itself a powerful geometric primitive. That is, having access to an efficient algorithm for ICA enables us to efficiently solve other important problems in machine learning. The first such reduction is a solution to the open problem of efficiently learning the intersection of n + 1 halfspaces in R, posed in [43]. This reduction relies on a non-linear transformation of samples from such an intersection of halfspaces (i.e. a simplex ) to samples which are approximately from a", "creator": "LaTeX with hyperref package"}}}