{"id": "1705.09406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Multimodal Machine Learning: A Survey and Taxonomy", "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "histories": [["v1", "Fri, 26 May 2017 01:35:31 GMT  (2070kb,D)", "http://arxiv.org/abs/1705.09406v1", null], ["v2", "Tue, 1 Aug 2017 17:39:39 GMT  (2070kb,D)", "http://arxiv.org/abs/1705.09406v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tadas baltru\\v{s}aitis", "chaitanya ahuja", "louis-philippe morency"], "accepted": false, "id": "1705.09406"}, "pdf": {"name": "1705.09406.pdf", "metadata": {"source": "CRF", "title": "Multimodal Machine Learning: A Survey and Taxonomy", "authors": ["Tadas Baltru\u0161aitis", "Chaitanya Ahuja", "Louis-Philippe Morency"], "emails": ["morency@cs.cmu.edu"], "sections": [{"heading": null, "text": "Index Terms - Multimodal, Machine Learning, Introduction, Surveying"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 APPLICATIONS: A HISTORICAL PERSPECTIVE", "text": "In fact, most people are able to determine themselves what they want and what they don't want. (...) It's not that people are able to determine themselves. (...) It's that people are able to determine themselves. (...) It's that people are able to determine themselves. (...) It's not that people are able to determine themselves. (...) It's that people are able to determine themselves. (...) It's that people are able to determine themselves. (...) It's that people are able to determine themselves what they want. (...) It's that they are able to determine themselves. (...) It's that people are able to determine themselves. (...) It's that people are able to act as if they do it, as if they do it, as if they do it. (...)"}, {"heading": "3 MULTIMODAL REPRESENTATIONS", "text": "Echoing the work of Bengio et al. [17], we use the term features and representations that are interchangeable, each of which refers to a vector or tenor representing an entity, be it an image, an audio sample, a single word, or a sentence. Multimodal representation is a representation of data that uses information from several such entities. Representing multiple modalities presents many difficulties: how to combine data from heterogeneous sources; how to deal with different levels of noise; and how to deal with missing data. Ability to represent data in a meaningful way is critical to multimodal problems and forms the backbone of any kind of modeling. Good representations are important for the performance of machine learning models behind recent proposals for speech recognition [79] and visual object classification."}, {"heading": "3.1 Joint Representations", "text": "In fact, it is such that most people will be able to go to another world, in which they are able to go to another world, in which they are able to go to another world, in which they are able to go, in which they are able to go, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they are able to put themselves, in which they are able to put themselves, in fact, in fact, are able to put themselves."}, {"heading": "3.2 Coordinated Representations", "text": "In fact, it is in such a way that most of them will be able to survive themselves without it coming to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, to a process, in which it comes to a process, in which it comes to a process, to a process, in which it comes to a process, in which it comes to a process, to a process, in which it comes to a process, in which it comes to a process, to a process, to a process, to a process, in which it comes to a process, to a process, in which it comes to a process, in which it comes to a process, to a process, in which it comes to a process, to a process, in which it comes to a process, to a process, in which it comes to a process, it comes to a process, to a process, in which it comes to a process, in which"}, {"heading": "3.3 Discussion", "text": "In this section, we identified two major types of multimodal representations - common and coordinated. Common representations project multimodal data into a common space and are best suited for situations where all modalities are present during the inference, and have been widely used for AVSR, affects, and multimodal gesture recognition. Coordinated representations, on the other hand, project each modality into a separate but coordinated space, making them suitable for applications where only one modality is available at test time, such as: multimodal retrieval and translation (Section 4), grounding (Section 7.2), and zero-shot learning (Section 7.2). While common representations have been used in situations to construct representations of more than two modalities, coordinated spaces have so far been largely limited to two modalities."}, {"heading": "4 TRANSLATION", "text": "A large part of multimodal machine learning deals with translation (mapping) from one modality to another. Faced with an entity in one modality, the task is to generate the same entity in another modality. For example, we want to generate a sentence that describes it or gives it a textual description. Multimodal translation, however, is a long-explored process in which computer vision and natural language processing (NLP) form communities [137], and intermodal retrieval [169]. More recently, interest in multimodal translation has been renewed."}, {"heading": "4.1 Example-based", "text": "There are two types of such algorithms: retrieval-based and combination-based models that directly use the retrieved translation without modifying it, while combination-based models rely on more complex rules to create translations based on a number of retrieved instances. Retrieval-based models are arguably the simplest form of multimodal translation. They rely on finding the closest examples in the dictionary and using these sentences as the translated result. Retrieval can take place in an imimodal space or an intermedial semantic space. Given a modality that needs to be translated, unimodal retrieval finds the closest distance in the dictionary to the sentences as the translated result. The retrieval of images can take place in an unimodal space or an intermedial semantic space."}, {"heading": "4.2 Generative approaches", "text": "It is a difficult problem because it requires the ability to understand both the source method and the generation of the target sequence or signal. As described in the following section, it is also much more difficult to judge such sentences because there is a large space of correct answers. In this study, we focus on the generation of three models: language, vision and sound. Language has been researched for a long time, and a lot of attention is devoted to tasks such as image and video."}, {"heading": "4.3 Model evaluation and discussion", "text": "A major challenge is that they are very difficult to assess, while some tasks such as speech recognition have a single correct translation, tasks such as language synthesis and media description do not. Sometimes, as in language translation, multiple answers are correct and decide which translation is better than others. Fortunately, there are a number of approximate automatic metrics that are able to assess a subjective task, meaning there will be a group of people evaluating each translation."}, {"heading": "5 ALIGNMENT", "text": "Multimodal alignment is defined as finding relationships and similarities between sub-components of instances of two or more modalities. For example, we want to use an image and a caption to find the areas of the image that correspond to the words or phrases of the caption [98]. Another example is the alignment of a film with the script or the book chapters on which it is based [252]. We categorize multimodal alignment in two ways - implicit and explicit. In explicit alignment, we are explicitly interested in aligning sub-components between modalities, e.g. by aligning recipe steps with the corresponding instructional video [131]. Implicit alignment is used as an intermediate step (often latent) for another task, e.g. text-based image alignment can include an alignment step between words and image regions [99]. An overview of such approaches can be viewed in Table 4 and is presented in more detail in the following sections."}, {"heading": "5.1 Explicit alignment", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "5.2 Implicit alignment", "text": "This enables better performance in a number of areas such as speech recognition, machine translation, media description and visual response to questions. We identify two types of implicit alignment models: earlier work based on graphic models, and more modern neural networking methods that are not based on superior alignment examples. Graphical models used to better align words are able to equip languages for machine translation."}, {"heading": "5.3 Discussion", "text": "Multimodal alignment faces a number of difficulties: 1) There are few datasets with explicitly commented alignments; 2) it is difficult to design similarity metrics between modalities; 3) several possible alignments may exist and not all elements in one modality have similarities in another. Previous work on multimodal alignment focused on the unattended alignment of multimodal sequences using graphical models and dynamic programming techniques. They relied on hand-defined measures of similarity between modalities or learned them unsupervised. With the recent availability of marked training data, supervised learning of similarities between modalities has become possible."}, {"heading": "6 FUSION", "text": "Multimodal fusion is one of the original themes of multimodal machine learning, with previous surveys highlighting early, late and hybrid fusion approaches [49], [247]. Technically, multimodal fusion is the concept of integrating information from multiple modalities with the aim of predicting an outcome: a class (e.g. happy or sad) by classification or a continuous value (e.g. positivity of feeling) by regression. It is one of the most researched aspects of multimodal machine learning with work going back to 25 years ago [243]. Interest in multimodal fusion stems from three key benefits it can offer. Firstly, having access to multiple modalities that observe the same phenomenon can enable more robust predictions, which has been particularly explored and exploited by the AVSR community [163]."}, {"heading": "6.1 Model-agnostic approaches", "text": "Historically, the vast majority of multimodal fusion has been carried out using model agnostic approaches [49]. Such approaches can be divided into early (i.e. feature-based), late (i.e. decision-based) and hybrid fusion [11]. Early fusion integrates features immediately after their extraction (often by simply concatenating their representations), whereas late fusion performs integration after each of the modalities has made a decision (e.g. classification or regression). Finally, hybrid fusion combines the results of early fusion with individual unimodal predictors. An advantage of model agnostic approaches is that they can be implemented using almost any unimodal classifier or regressor. Early fusion could be regarded as the first attempt of multimodal researchers to perform multimodal representation learning results - since it can learn to exploit the correlation and interactions between the features of low-level of each modality."}, {"heading": "6.2 Model-based approaches", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "6.3 Discussion", "text": "Multimodal fusion is a widely researched topic, with a variety of approaches being proposed to address it, including model agnostic methods, graphical models, multimodal learning, and various types of neural networks. Each approach has its own strengths and weaknesses, some of which are better suited to smaller data sets and others perform better in noisy environments. Recently, neural networks have become a very popular method for tackling multimodal fusion, although graphical models and multiple learning are still used, especially in tasks with limited training data or where the interpretability of models is important. Despite these advances, multimodal fusion still faces the following challenges: 1) signals cannot be synchronized (possibly dense continuous signal and a sparse event); 2) it is difficult to build models that evaluate complementary rather than complementary information; 3) each modality may have different types and different levels of noise at different times."}, {"heading": "7 CO-LEARNING", "text": "The final multimodal challenge in our taxonomy is colearning - modeling a (resource-poor) modality by leveraging knowledge from another (resource-rich) modality. This is especially relevant when one of the modalities has limited resources - lack of annotated data, noisy input, and unreliable labels. We call this challenge coloring, since the helper modality is most commonly used only during model training and is not used during the test period. In other words, if the multimodal observations originate from the same cases, for example, in an audiovisual language dataset where video and speech samples originate from the same speaker, non-parallel datasets do not require direct linkages between observations from other modalities. In contrast, these approaches usually achieve co-learning by using overlapping categories in object categories to detect hybrid data, where video and speech samples originate from the same speaker."}, {"heading": "7.1 Parallel data", "text": "In the meantime, it has been shown that more than half of the people who are able, are able, are able, are able to use the data to better model the modalities: co-training and representation acquisition. Co-training is the process of creating more labeled training samples when we have only a few labeled samples in a multimodal problem. [20] The basic algorithm builds weak classifiers in each modality to equip each other with labels for the unlabeled data. It has been shown that more training samples for website classification are discovered based on the websites themselves and hyperlinks that result in the basic work of Blum and Mitchell. This task requires parallel data as it relies on the overlap of multimodal samplers. Co-training was used for statistical analysis [178] to build better visual detectors [120] and for audiovisual speech recognition [39]."}, {"heading": "7.2 Non-parallel data", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "7.3 Hybrid data", "text": "In hybrid data setting, two non-parallel modalities are bridged by a common modality or a data set (see Figure 3c). The most notable example is the Bridge Correlational Neural Network [167], which uses a pivot modality to learn coordinated multimodal representations in the presence of non-parallel data. In the case of multilingual caption, for example, the image modality would always be coupled with at least one caption in any language. Such methods are also used to bridge languages that may not have a parallel corpora, but have access to a common pivot language, such as machine translation [148], [167] and document transliteration [100]. Instead of a separate modality for bridging, some methods rely on the existence of large data sets from a similar or related task to lead to better performance in a task that contains only limited data, and a text file [189] is better available for using a segmented file [78]."}, {"heading": "7.4 Discussion", "text": "Multimodal co-learning enables one modality to influence the formation of another by using the complementary information across modalities. It is important to note that co-learning is task-independent and could be used to create better merger, translation and alignment models. This challenge is illustrated by algorithms such as co-training, multimodal representation learning, conceptual grounding and zero-shot learning (ZSL) and has many applications in the areas of visual classification, action recognition, audiovisual speech recognition and semantic similarity assessment."}, {"heading": "8 CONCLUSION", "text": "As part of this study, we introduced a taxonomy of multimodal machine learning: representation, translation, fusion, alignment and co-learning. Some of them, such as fusion, have long been studied, but recent interest in representation and translation has led to a large number of new multimodal algorithms and exciting multimodal applications. We believe that our taxonomy will help catalog future research and also better understand the remaining unresolved problems of multimodal machine learning."}], "references": [{"title": "Analyzing the Behavior of Visual Question Answering Models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "EMNLP, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011", "author": ["C.N. Anagnostopoulos", "T. Iliou", "I. Giannoukos"], "venue": "Artificial Intelligence Review, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Expressive visual text-to-speech using active appearance models", "author": ["R. Anderson", "B. Stenger", "V. Wan", "R. Cipolla"], "venue": "CVPR, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu"], "venue": "ICML, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio-to-text alignment for speech recognition with very limited resources.", "author": ["X. Anguera", "J. Luque", "C. Gracia"], "venue": "in INTER- SPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["R. Arora", "K. Livescu"], "venue": "ICASSP, pp. 7135\u20137139, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal fusion for multimedia analysis: A survey", "author": ["P.K. Atrey", "M.A. Hossain", "A. El Saddik", "M.S. Kankanhalli"], "venue": "2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural Machine Translation By Jointly Learning To Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Dimensional Affect Recognition using Continuous Conditional Random Fields", "author": ["T. Baltru\u0161aitis", "N. Banda", "P. Robinson"], "venue": "IEEE FG, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Video In Sentences Out", "author": ["A. Barbu", "A. Bridge", "Z. Burchill", "D. Coroian", "S. Dickinson", "S. Fidler", "A. Michaux", "S. Mussman", "S. Narayanaswamy", "D. Salvi", "L. Schmidt", "J. Shangguan", "J.M. Siskind", "J. Waggoner", "S. Wang", "J. Wei", "Y. Yin", "Z. Zhang"], "venue": "Proc. of the Conference on Uncertainty in Artificial Intelligence, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Matching Words and Pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "JMLR, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Grounded cognition", "author": ["L.W. Barsalou"], "venue": "Annual review of psychology, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "TPAMI, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler- Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "JAIR, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "VizWiz: Nearly Real-Time Answers to Vvisual Questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "UIST, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Computational learning theory, 1998.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "ECCV, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly-Supervised Alignment of Video With Text", "author": ["P. Bojanowski", "R. Lajugie", "E. Grave", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid"], "venue": "ICCV, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A mew ASR approach based on independent processing and recombination of partial frequency bands", "author": ["H. Bourlard", "S. Dupont"], "venue": "International Conference on Spoken Language, 1996.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Coupled hidden Markov models for complex action recognition", "author": ["M. Brand", "N. Oliver", "A. Pentland"], "venue": "CVPR, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Video rewrite: Driving visual speech with audio", "author": ["C. Bregler", "M. Covell", "M. Slaney"], "venue": "SIGGRAPH, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Data Fusion through Cross-modality Metric Learning using Similarity-Sensitive Hashing", "author": ["M.M. Bronstein", "A.M. Bronstein", "F. Michel", "N. Paragios"], "venue": "CVPR, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics, pp. 263\u2013311, 1993.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1993}, {"title": "Distributional Semantics in Technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N.-K. Tran"], "venue": "ACL, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Distributional Semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "JAIR, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple Kernel Learning for Visual Object Recognition: A Review", "author": ["S.S. Bucak", "R. Jin", "A.K. Jain"], "venue": "TPAMI, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Visual- Semantic Hashing for Cross-Modal Retrieval", "author": ["Y. Cao", "M. Long", "J. Wang", "Q. Yang", "P.S. Yu"], "venue": "KDD, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "The AMI Meeting Corpus: A Pre-Announcement", "author": ["J. Carletta", "S. Ashby", "S. Bourban", "M. Flynn", "M. Guillemot", "T. Hain", "J. Kadlec", "V. Karaiskos", "W. Kraaij", "M. Kronenthal", "G. Lathoud", "M. Lincoln", "A. Lisowska", "I. McCowan", "W. Post", "D. Reidsma", "P. Wellner"], "venue": "Int. Conf. on Methods and Techniques in Behavioral Research, 2005.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Emotion recognition through multiple modalities: Face, body gesture, speech", "author": ["G. Castellano", "L. Kessous", "G. Caridakis"], "venue": "LNCS, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Listen, Attend, and Spell: a Neural Network for Large Vocabulary Conversational Speech Recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "ICASSP, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Emotion Recognition in the Wild with Feature Fusion and Multiple Kernel Learning", "author": ["J. Chen", "Z. Chen", "Z. Chi", "H. Fu"], "venue": "ICMI, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-modal Dimensional Emotion Recognition Using Recurrent Neural Networks", "author": ["S. Chen", "Q. Jin"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, 2015.  17", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO Captions: Data Collection and Evaluation Server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "L. Zitnick"], "venue": "2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "NIPS, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Co-Adaptation of audio-visual speech and gesture classifiers", "author": ["C.M. Christoudias", "K. Saenko", "L.-P. Morency", "T. Darrell"], "venue": "ICMI, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-view learning in the presence of view disagreement", "author": ["C.M. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "UAI, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Wav2Letter: an Endto-End ConvNet-based Speech Recognition System", "author": ["R. Collobert", "C. Puhrsch", "G. Synnaeve"], "venue": "2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Bimodal recognition experiments with recurrent neural networks", "author": ["P. Cosi", "E. Caldognetto", "K. Vagges", "G. Mian", "M. Contolini", "C. per Le Ricerche", "C. di Fonetica"], "venue": "ICASSP, 1994.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1994}, {"title": "Movie / Script : Alignment and Parsing of Video and Text Transcription", "author": ["T. Cour", "C. Jordan", "E. Miltsakaki", "B. Taskar"], "venue": "ECCV, 2008, pp. 1\u201314.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "WordsEye: an automatic text-to-scene conversion system", "author": ["B. Coyne", "R. Sproat"], "venue": "SIGGRAPH, 2001.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Facial Expression Analysis", "author": ["F. De la Torre", "J.F. Cohn"], "venue": "Guide to Visual Analysis of Humans: Looking at People, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech-Driven Facial Animation Using a Shared Gaussian Process Latent Variable Model", "author": ["S. Deena", "A. Galata"], "venue": "Advances in Visual Computing, 2009.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "EACL, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Language Models for Image Captioning: The Quirks and What Works", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "ACL, 2015.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "A Review and Meta-Analysis of Multimodal Affect Detection Systems", "author": ["S.K. D\u2019mello", "J. Kory"], "venue": "ACM Computing Surveys, 2015.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Image Description using Visual Dependency Representations", "author": ["D. Elliott", "F. Keller"], "venue": "EMNLP, no. October, 2013.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparing Automatic Evaluation Measures for Image Description", "author": ["\u2014\u2014"], "venue": "ACL, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention", "author": ["G. Evangelopoulos", "A. Zlatintsi", "A. Potamianos", "P. Maragos", "K. Rapantzikos", "G. Skoumas", "Y. Avrithis"], "venue": "IEEE Trans. Multimedia, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based Recurrent Neural Networks", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F.K. Soong"], "venue": "IN- TERSPEECH, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR, 2009.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "LNCS, 2010.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep correspondence restricted Boltzmann machine for cross-modal retrieval", "author": ["F. Feng", "R. Li", "X. Wang"], "venue": "Neurocomputing, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal Retrieval with Correspondence Autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACMMM, 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual Information in Semantic Representation", "author": ["Y. Feng", "M. Lapata"], "venue": "NAACL, 2010.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}, {"title": "A Sentence is Worth a Thousand Pixels Holistic CRF model", "author": ["S. Fidler", "A. Sharma", "R. Urtasun"], "venue": "CVPR, 2013.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2013}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "NIPS, 2013.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "DeViSE: A deep visualsemantic embedding model", "author": ["A. Frome", "G. Corrado", "J. Shlens"], "venue": "NIPS, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "EMNLP, 2016.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2016}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, 2015.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Boosted learning in dynamic bayesian networks for multimodal speaker detection", "author": ["A. Garg", "V. Pavlovic", "J.M. Rehg"], "venue": "Proceedings of the IEEE, 2003.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2003}, {"title": "Audio-visual speaker diarization based on spatiotemporal bayesian fusion", "author": ["I.D. Gebru", "S. Ba", "X. Li", "R. Horaud"], "venue": "TPAMI, 2017.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2017}, {"title": "On Feature Combination for Multiclass Object Classification", "author": ["P. Gehler", "S. Nowozin"], "venue": "ICCV, 2009.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Factorial hidden Markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine Learning, 1997.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple classifier systems for the classification of audio-visual emotional states", "author": ["M. Glodek", "S. Tschechne", "G. Layher", "M. Schels", "T. Brosch", "S. Scherer", "M. K\u00e4chele", "M. Schmidt", "H. Neumann", "G. Palm", "F. Schwenker"], "venue": "LNCS, 2011.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiple Kernel Learning Algorithms", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "JMLR, 2011.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde- Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, 2014.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "ICASSP, 2013.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2013}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV, 2013.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2013}, {"title": "Choosing Linguistics over Vision to Describe Images", "author": ["A. Gupta", "Y. Verma", "C.V. Jawahar"], "venue": "AAAI, 2012.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic Modality Weighting for Multi-stream HMMs in Audio-Visual Speech Recognition", "author": ["M. Gurban", "J.-P. Thiran", "T. Drugman", "T. Dutoit"], "venue": "ICMI, 2008.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2008}, {"title": "Canonical correlation analysis; An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-taylor"], "venue": "Tech. Rep., 2003.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2003}, {"title": "Alignment of speech to highly imperfect text transcriptions", "author": ["A. Haubold", "J.R. Kender"], "venue": "ICME, 2007.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, 2012.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Autoencoders, minimum description length and Helmoltz free energy", "author": ["G. Hinton", "R.S. Zemel"], "venue": "NIPS, 1993.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 1993}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, 2006.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 1997.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "JAIR, 2013.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2013}, {"title": "Relations Between Two Sets of Variates", "author": ["H. Hotelling"], "venue": "Biometrika, 1936.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 1936}, {"title": "Natural Language Object Retrieval", "author": ["R. Hu", "H. Xu", "M. Rohrbach", "J. Feng", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2016.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio-Visual Deep Learning for Noise Robust Speech Recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "ICASSP, 2013.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2013}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["A. Hunt", "A.W. Black"], "venue": "ICASSP, 1996.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1996}, {"title": "Medical image fusion : A survey of the state of the art", "author": ["A.P. James", "B.V. Dasarathy"], "venue": "Information Fusion, vol. 19, 2014.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-task , Multi- Kernel Learning for Estimating Individual Wellbeing", "author": ["N. Jaques", "S. Taylor", "A. Sano", "R. Picard"], "venue": "Multimodal Machine Learning Workshop in conjunction with NIPS, 2015.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Guiding the Long-Short Term Memory Model for Image Caption Generation", "author": ["X. Jia", "E. Gavves", "B. Fernando", "T. Tuytelaars"], "venue": "ICCV, 2015.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Cross-Modal Hashing", "author": ["Q.-y. Jiang", "W.-j. Li"], "venue": "CVPR, 2017.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2017}, {"title": "The classification of multi-modal data with hidden conditional random field", "author": ["X. Jiang", "F. Wu", "Y. Zhang", "S. Tang", "W. Lu", "Y. Zhuang"], "venue": "Pattern Recognition Letters, 2015.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "Video Description Generation using Audio and Visual Cues", "author": ["Q. Jin", "J. Liang"], "venue": "ICMR, 2016.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2016}, {"title": "Hidden Markov Models for Speech Recognition", "author": ["B.H. Juang", "L.R. Rabiner"], "venue": "Technometrics, 1991.  18", "citeRegEx": "95", "shortCiteRegEx": null, "year": 1991}, {"title": "EmoNets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulchere", "V. Michalski", "K. Konda", "J. Sebastien", "P. Froumenty", "Y. Dauphin", "N. Boulanger- Lewandowski", "R.C. Ferrari", "M. Mirza", "D. Warde-Farley", "A. Courville", "P. Vincent", "R. Memisevic", "C. Pal", "Y. Bengio"], "venue": "Journal on Multimodal User Interfaces, 2015.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP, 2013.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "NIPS, 2014.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2014}, {"title": "Everybody loves a rich cousin: An empirical study of transliteration through bridge languages", "author": ["M.M. Khapra", "A. Kumaran", "P. Bhattacharyya"], "venue": "NAACL, 2010.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics", "author": ["D. Kiela", "L. Bottou"], "venue": "EMNLP, 2014.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounding Semantics in Olfactory Perception", "author": ["D. Kiela", "L. Bulat", "S. Clark"], "venue": "ACL, 2015.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception", "author": ["D. Kiela", "S. Clark"], "venue": "EMNLP, 2015.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning for Robust Feature Generation in Audiovisual Emotion Recognition", "author": ["Y. Kim", "H. Lee", "E.M. Provost"], "venue": "ICASSP, 2013.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2013}, {"title": "Unifying Visual- Semantic Embeddings with Multimodal Neural Language Models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "2014.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2014}, {"title": "Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation", "author": ["B. Klein", "G. Lev", "G. Sadeh", "L. Wolf"], "venue": "CVPR, 2015.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language description of human activities from video images based on concept hierarchy of actions", "author": ["A. Kojima", "T. Tamura", "K. Fukunaga"], "venue": "IJCV, 2002.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2002}, {"title": "What are you talking about? Text-to-Image Coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler"], "venue": "CVPR, 2014.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-relational learning, text mining, and semi-supervised learning for functional genomics", "author": ["M.A. Krogel", "T. Scheffer"], "venue": "Machine Learning, 2004.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2004}, {"title": "An Overview of Sequence Comparison: Time Warps, String Edits, and Macromolecules", "author": ["J.B. Kruskal"], "venue": "Society for Industrial and Applied Mathematics Review, vol. 25, no. 2, pp. 201\u2013237, 1983.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 1983}, {"title": "BabyTalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "TPAMI, 2013.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hash functions for cross-view similarity search", "author": ["S. Kumar", "R. Udupa"], "venue": "IJCAI, 2011.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "ACL, 2012.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2012}, {"title": "Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "ICML, 2001.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2001}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "International Journal of Neural Systems, 2000.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimedia classification and event detection using double fusion", "author": ["Z.Z. Lan", "L. Bao", "S.I. Yu", "W. Liu", "A.G. Hauptmann"], "venue": "Multimedia Tools and Applications, 2014.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2014}, {"title": "Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world", "author": ["A. Lazaridou", "E. Bruni", "M. Baroni"], "venue": "ACL, 2014.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2014}, {"title": "Phrase-based Image Captioning", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "ICML, 2015.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised improvement of visual detectors using cotraining", "author": ["A. Levin", "P. Viola", "Y. Freund"], "venue": "ICCV, 2003.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2003}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T. Berg", "A. Berg", "Y. Choi"], "venue": "CoNLL, 2011.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of recent advances in visual feature detection", "author": ["Y. Li", "S. Wang", "Q. Tian", "X. Ding"], "venue": "Neurocomputing, 2015.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparison of automatic shot boundary detection algorithms", "author": ["R.W. Lienhart"], "venue": "Proceedings of SPIE, 1998.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 1998}, {"title": "Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics", "author": ["C.-Y. Lin", "E. Hovy"], "venue": "NAACL, 2003.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple kernel learning in the primal for multimodal Alzheimer\u2019s disease classification", "author": ["F. Liu", "L. Zhou", "C. Shen", "J. Yin"], "venue": "IEEE Journal of Biomedical and Health Informatics, 2014.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2014}, {"title": "Symbol interdependency in symbolic and embodied cognition", "author": ["M.M. Louwerse"], "venue": "Topics in Cognitive Science, 2011.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2011}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "IJCV, 2004.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical Co-Attention for Visual Question Answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS, 2016.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2016}, {"title": "Regularizing Long Short Term Memory with 3D Human-Skeleton Sequences for Action Recognition", "author": ["B. Mahasseni", "S. Todorovic"], "venue": "CVPR, 2016.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV, 2015.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2015}, {"title": "What\u2019s cookin\u2019? interpreting cooking videos using text, speech and vision", "author": ["J. Malmaud", "J. Huang", "V. Rathod", "N. Johnston", "A. Rabinovich", "K. Murphy"], "venue": "NAACL, 2015.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating Images from Captions with Attention", "author": ["E. Mansimov", "E. Parisotto", "J.L. Ba", "R. Salakhutdinov"], "venue": "ICLR, 2016.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2016}, {"title": "Generation and Comprehension of Unambiguous Object Descriptions", "author": ["J. Mao", "J. Huang", "A. Toshev", "O. Camburu", "A. Yuille", "K. Murphy"], "venue": "CVPR, 2016.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonparametric Method for Datadriven Image Captioning", "author": ["R. Mason", "E. Charniak"], "venue": "ACL, 2014.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2014}, {"title": "Text-to-Visual Speech Synthesis Based on Parameter Generation from HMM", "author": ["T. Masuko", "T. Kobayashi", "M. Tamura", "J. Masubuchi", "K. Tokuda"], "venue": "ICASSP, 1998.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning Multi-modal Similarity", "author": ["B. McFee", "G.R.G. Lanckriet"], "venue": "JMLR, 2011.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2011}, {"title": "The SE- MAINE corpus of emotionally coloured character interactions", "author": ["G. McKeown", "M.F. Valstar", "R. Cowie", "M. Pantic"], "venue": "IEEE International Conference on Multimedia and Expo, 2010.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["H. Mei", "M. Bansal", "M.R. Walter"], "venue": "AAAI, 2016.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 2013.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2013}, {"title": "Midge: Generating Image Descriptions From Computer Vision Detections", "author": ["M. Mitchell", "J. Dodge", "A. Goyal", "K. Yamaguchi", "K. Stratos", "A. Mensch", "A. Berg", "X. Han", "T. Berg", "O. Health"], "venue": "EACL, 2012.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Transfer Deep Learning for Audio-Visual Recognition", "author": ["S. Moon", "S. Kim", "H. Wang"], "venue": "NIPS Workshops, 2015.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2015}, {"title": "Majority vote of diverse classifiers for late fusion", "author": ["E. Morvant", "A. Habrard", "S. Ayache"], "venue": "LNCS, 2014.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multimodal learning for Audio-Visual Speech Recognition", "author": ["Y. Mroueh", "E. Marcheret", "V. Goel"], "venue": "ICASSP, 2015.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminative unsupervised alignment of natural language instructions with corresponding video segments", "author": ["I. Naim", "Y. Song", "Q. Liu", "L. Huang", "H. Kautz", "J. Luo", "D. Gildea"], "venue": "NAACL, 2015.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised Alignment of Natural Language Instructions with Video Segments", "author": ["I. Naim", "Y.C. Song", "Q. Liu", "H. Kautz", "J. Luo", "D. Gildea"], "venue": "AAAI, 2014.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving statistical machine translation for a resource-poor language using related resource-rich languages", "author": ["P. Nakov", "H.T. Ng"], "venue": "JAIR, 2012.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2012}, {"title": "A coupled HMM for audio-visual speech recognition", "author": ["A.V. Nefian", "L. Liang", "X. Pi", "L. Xiaoxiang", "C. Mao", "K. Murphy"], "venue": "Interspeech, vol. 2, 2002.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2002}, {"title": "ModDrop: Adaptive multi-modal gesture recognition", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "IEEE TPAMI, 2016.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal Deep Learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, 2011.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2011}, {"title": "Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence \u2013 Arousal Space", "author": ["M.A. Nicolaou", "H. Gunes", "M. Pantic"], "venue": "IEEE TAC, 2011.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep multimodal fusion for persuasiveness prediction", "author": ["B. Nojavanasghari", "D. Gopinath", "J. Koushik", "T. Baltru\u0161aitis", "L.-P. Morency"], "venue": "ICMI, 2016.  19", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal Speaker diarization", "author": ["A. Noulas", "G. Englebienne", "B.J. Kr\u00f6se"], "venue": "IEEE TPAMI, 2012.", "citeRegEx": "154", "shortCiteRegEx": null, "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS, 2011.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-source Deep Learning for Human Pose Estimation", "author": ["W. Ouyang", "X. Chu", "X. Wang"], "venue": "CVPR, 2014.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2014}, {"title": "Visually Indicated Sounds", "author": ["A. Owens", "P. Isola", "J. McDermott", "A. Torralba", "E.H. Adelson", "W.T. Freeman"], "venue": "CVPR, 2016.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2016}, {"title": "Zero-Shot Learning with Semantic Output Codes", "author": ["M. Palatucci", "G.E. Hinton", "D. Pomerleau", "T.M. Mitchell"], "venue": "NIPS, 2009.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2009}, {"title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "CVPR, 2016.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2016}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-j. Zhu"], "venue": "ACL, 2002.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2002}, {"title": "Flickr30k Entities: Collecting Regionto-Phrase Correspondences for Richer Image-to-Sentence Models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "ICCV, 2015.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis", "author": ["S. Poria", "E. Cambria", "A. Gelbukh"], "venue": "EMNLP, 2015.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2015}, {"title": "Recent advances in the automatic recognition of audio-visual speech", "author": ["G. Potamianos", "C. Neti", "G. Gravier", "A. Garg", "A.W. Senior"], "venue": "Proceedings of the IEEE, 2003.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 2003}, {"title": "Global Ranking Using Continuous Conditional Random Fields", "author": ["T. Qin", "T.-y. Liu", "X.-d. Zhang", "D.-s. Wang", "H. Li"], "venue": "NIPS, 2008.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2008}, {"title": "Hidden conditional random fields.", "author": ["A. Quattoni", "S. Wang", "L.-P. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE TPAMI, vol", "citeRegEx": "165", "shortCiteRegEx": "165", "year": 2007}, {"title": "Extending Long Short-Term Memory for Multi-View Structured Learning", "author": ["S.S. Rajagopalan", "L.-P. Morency", "T. Baltru\u0161aitis", "R. Goecke"], "venue": "ECCV, 2016.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2016}, {"title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning", "author": ["J. Rajendran", "M.M. Khapra", "S. Chandar", "B. Ravindran"], "venue": "NAACL, 2015.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling Latent Discriminative Dynamic of Multi-Dimensional Affective Signals", "author": ["G.A. Ramirez", "T. Baltru\u0161aitis", "L.-P. Morency"], "venue": "ACII workshops, 2011.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2011}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACMMM, 2010.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2010}, {"title": "Trainable methods for surface natural language generation", "author": ["A. Ratnaparkhi"], "venue": "NAACL, 2000.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2000}, {"title": "Generative Adversarial Text to Image Synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "H. Lee", "B. Schiele"], "venue": "ICML, 2016.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2016}, {"title": "Grounding Action Descriptions in Videos", "author": ["M. Regneri", "M. Rohrbach", "D. Wetzel", "S. Thater", "B. Schiele", "M. Pinkal"], "venue": "TACL, 2013.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2013}, {"title": "Hidden Conditional Random Fields for Meeting Segmentation", "author": ["S. Reiter", "B. Schuller", "G. Rigoll"], "venue": "ICME, 2007.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2007}, {"title": "The long-short story of movie description", "author": ["A. Rohrbach", "M. Rohrbach", "B. Schiele"], "venue": "Pattern Recognition, 2015.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2015}, {"title": "Movie description", "author": ["A. Rohrbach", "A. Torabi", "M. Rohrbach", "N. Tandon", "C. Pal", "H. Larochelle", "A. Courville", "B. Schiele"], "venue": "International Journal of Computer Vision, 2017.", "citeRegEx": "175", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International conference on artificial intelligence and statistics, 2009.", "citeRegEx": "176", "shortCiteRegEx": null, "year": 2009}, {"title": "Audiovisual synchronization and fusion using canonical correlation analysis", "author": ["M.E. Sargin", "Y. Yemez", "E. Erzin", "A.M. Tekalp"], "venue": "IEEE Trans. Multimedia, 2007.", "citeRegEx": "177", "shortCiteRegEx": null, "year": 2007}, {"title": "Applying Co-Training methods to statistical parsing", "author": ["A. Sarkar"], "venue": "ACL, 2001.", "citeRegEx": "178", "shortCiteRegEx": null, "year": 2001}, {"title": "AVEC 2011 \u2013 The First International Audio / Visual Emotion Challenge", "author": ["B. Schuller", "M.F. Valstar", "F. Eyben", "G. McKeown", "R. Cowie", "M. Pantic"], "venue": "ACII, 2011.", "citeRegEx": "179", "shortCiteRegEx": null, "year": 2011}, {"title": "Isotonic CCA for sequence alignment and activity recognition", "author": ["S. Shariat", "V. Pavlovic"], "venue": "ICCV, 2011.", "citeRegEx": "180", "shortCiteRegEx": null, "year": 2011}, {"title": "Black Holes and White Rabbits : Metaphor Identification with Visual Features", "author": ["E. Shutova", "D. Kelia", "J. Maillard"], "venue": "NAACL, 2016.", "citeRegEx": "181", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiple Kernel Learning for Emotion Recognition in the Wild", "author": ["K. Sikka", "K. Dykstra", "S. Sathyanarayana", "G. Littlewort", "M. Bartlett"], "venue": "ICMI, 2013.", "citeRegEx": "182", "shortCiteRegEx": null, "year": 2013}, {"title": "Grounded Models of Semantic Representation", "author": ["C. Silberer", "M. Lapata"], "venue": "EMNLP, 2012.", "citeRegEx": "183", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Grounded Meaning Representations with Autoencoders", "author": ["\u2014\u2014"], "venue": "ACL, 2014.", "citeRegEx": "184", "shortCiteRegEx": null, "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "185", "shortCiteRegEx": null, "year": 2015}, {"title": "An HMM-based system for automatic segmentation and alignment of speech", "author": ["K. Sj\u00f6lander"], "venue": "Proceedings of Fonetik, 2003.", "citeRegEx": "186", "shortCiteRegEx": null, "year": 2003}, {"title": "FaceSync: A linear operator for measuring synchronization of video facial images and audio tracks", "author": ["M. Slaney", "M. Covell"], "venue": "NIPS, 2000.", "citeRegEx": "187", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal video indexing: A review of the state-of-the-art", "author": ["C.G.M. Snoek", "M. Worring"], "venue": "Multimedia Tools and Applications, 2005.", "citeRegEx": "188", "shortCiteRegEx": null, "year": 2005}, {"title": "Connecting modalities: Semisupervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "CVPR, 2010.", "citeRegEx": "189", "shortCiteRegEx": null, "year": 2010}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS, 2013.", "citeRegEx": "190", "shortCiteRegEx": null, "year": 2013}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL, 2014.", "citeRegEx": "191", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal emotion recognition in response to videos", "author": ["M. Soleymani", "M. Pantic", "T. Pun"], "venue": "TAC, 2012.", "citeRegEx": "192", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-view latent variable discriminative models for action recognition", "author": ["Y. Song", "L.-P. Morency", "R. Davis"], "venue": "CVPR, 2012.", "citeRegEx": "193", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Human Behavior Analysis: Learning Correlation and Interaction Across Modalities", "author": ["\u2014\u2014"], "venue": "ICMI, 2012.", "citeRegEx": "194", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Alignment of Actions in Video with Text Descriptions", "author": ["Y.C. Song", "I. Naim", "A.A. Mamun", "K. Kulkarni", "P. Singla", "J. Luo", "D. Gildea", "H. Kautz"], "venue": "IJCAI, 2016.", "citeRegEx": "195", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout : A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR, 2014.", "citeRegEx": "196", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Representations for Multimodal Data with Deep Belief Nets", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "ICML, 2012.", "citeRegEx": "197", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "NIPS, 2012.", "citeRegEx": "198", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis", "author": ["H.I. Suk", "S.-W. Lee", "D. Shen"], "venue": "NeuroImage, 2014.", "citeRegEx": "199", "shortCiteRegEx": null, "year": 2014}, {"title": "Introduction to Conditional Random Fields for Relational Learning", "author": ["C. Sutton", "A. McCallum"], "venue": "Introduction to Statistical Relational Learning. MIT Press, 2006.", "citeRegEx": "200", "shortCiteRegEx": null, "year": 2006}, {"title": "Aligning plot synopses to videos for story-based retrieval", "author": ["M. Tapaswi", "M. B\u00e4uml", "R. Stiefelhagen"], "venue": "IJMIR, 2015.", "citeRegEx": "201", "shortCiteRegEx": null, "year": 2015}, {"title": "Book2Movie: Aligning video scenes with book chapters", "author": ["\u2014\u2014"], "venue": "CVPR, 2015.", "citeRegEx": "202", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic units of visual speech", "author": ["S.L. Taylor", "M. Mahler", "B.-j. Theobald", "I. Matthews"], "venue": "SIGGRAPH, 2012.", "citeRegEx": "203", "shortCiteRegEx": null, "year": 2012}, {"title": "Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R. Mooney"], "venue": "COLING, 2014.", "citeRegEx": "204", "shortCiteRegEx": null, "year": 2014}, {"title": "Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research", "author": ["A. Torabi", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "2015.", "citeRegEx": "205", "shortCiteRegEx": null, "year": 2015}, {"title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "B. Schuller", "S. Zafeiriou"], "venue": "ICASSP, 2016.", "citeRegEx": "207", "shortCiteRegEx": null, "year": 2016}, {"title": "AVEC 2013 \u2013 The Continuous Audio / Visual Emotion and Depression Recognition Challenge", "author": ["M. Valstar", "B. Schuller", "K. Smith", "F. Eyben", "B. Jiang", "S. Bilakhia", "S. Schnieder", "R. Cowie", "M. Pantic"], "venue": "ACM International Workshop on Audio/Visual Emotion Challenge, 2013.", "citeRegEx": "208", "shortCiteRegEx": null, "year": 2013}, {"title": "WaveNet: A Generative Model for Raw Audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "2016.", "citeRegEx": "209", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixel Recurrent Neural Networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "ICML, 2016.", "citeRegEx": "210", "shortCiteRegEx": null, "year": 2016}, {"title": "CIDEr: Consensusbased Image Description Evaluation Ramakrishna Vedantam", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "CVPR, 2015.  20", "citeRegEx": "211", "shortCiteRegEx": null, "year": 2015}, {"title": "Order- Embeddings of Images and Language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "ICLR, 2016.", "citeRegEx": "212", "shortCiteRegEx": null, "year": 2016}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "NAACL, 2015.", "citeRegEx": "213", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "ICML, 2014.", "citeRegEx": "214", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["\u2014\u2014"], "venue": "CVPR, 2015.", "citeRegEx": "215", "shortCiteRegEx": null, "year": 2015}, {"title": "HMM-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "Computational Linguistics, 1996.", "citeRegEx": "216", "shortCiteRegEx": null, "year": 1996}, {"title": "Deep Multimodal Hashing with Orthogonal Regularization", "author": ["D. Wang", "P. Cui", "M. Ou", "W. Zhu"], "venue": "IJCAI, 2015.", "citeRegEx": "217", "shortCiteRegEx": null, "year": 2015}, {"title": "Hashing for Similarity Search: A Survey", "author": ["J. Wang", "H.T. Shen", "J. Song", "J. Ji"], "venue": "2014.", "citeRegEx": "218", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Deep Structure- Preserving Image-Text Embeddings", "author": ["L. Wang", "Y. Li", "S. Lazebnik"], "venue": "CVPR, 2016.", "citeRegEx": "219", "shortCiteRegEx": null, "year": 2016}, {"title": "On deep multiview representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "ICML, 2015.", "citeRegEx": "220", "shortCiteRegEx": null, "year": 2015}, {"title": "Web Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings Image Annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "ECML, 2010.", "citeRegEx": "221", "shortCiteRegEx": null, "year": 2010}, {"title": "WSABIE: Scaling up to large vocabulary image annotation", "author": ["\u2014\u2014"], "venue": "IJCAI, 2011.", "citeRegEx": "222", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework", "author": ["M. W\u00f6llmer", "M. Kaiser", "F. Eyben", "B. Schuller", "G. Rigoll"], "venue": "IMAVIS, 2013.", "citeRegEx": "223", "shortCiteRegEx": null, "year": 2013}, {"title": "Context-Sensitive Multimodal Emotion Recognition from Speech and Facial Expression using Bidirectional LSTM Modeling", "author": ["M. W\u00f6llmer", "A. Metallinou", "F. Eyben", "B. Schuller", "S. Narayanan"], "venue": "INTERSPEECH, 2010.", "citeRegEx": "224", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal Dynamic Networks for Gesture Recognition", "author": ["D. Wu", "L. Shao"], "venue": "ACMMM, 2014.", "citeRegEx": "225", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-level Fusion of Audio and Visual Features for Speaker Identification", "author": ["Z. Wu", "L. Cai", "H. Meng"], "venue": "Advances in Biometrics, 2005.", "citeRegEx": "226", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploring Interfeature and Inter-class Relationships with Deep Neural Networks for Video Classification", "author": ["Z. Wu", "Y.-G. Jiang", "J. Wang", "J. Pu", "X. Xue"], "venue": "ACMMM, 2014.", "citeRegEx": "227", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML, 2016.", "citeRegEx": "228", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "ECCV, 2016.", "citeRegEx": "229", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, 2015.", "citeRegEx": "230", "shortCiteRegEx": null, "year": 2015}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso"], "venue": "AAAI, 2015.", "citeRegEx": "231", "shortCiteRegEx": null, "year": 2015}, {"title": "A Distributed Representation Based Query Expansion Approach for Image Captioning", "author": ["S. Yagcioglu", "E. Erdem", "A. Erdem", "R. Cakici"], "venue": "ACL, 2015.", "citeRegEx": "232", "shortCiteRegEx": null, "year": 2015}, {"title": "Corpus-Guided Sentence Generation of Natural Images", "author": ["Y. Yang", "C.L. Teo", "H. Daume", "Y. Aloimonos"], "venue": "EMNLP, 2011.", "citeRegEx": "233", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR, 2016.", "citeRegEx": "234", "shortCiteRegEx": null, "year": 2016}, {"title": "I2T: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proceedings of the IEEE, 2010.", "citeRegEx": "235", "shortCiteRegEx": null, "year": 2010}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "CVPR, 2015.", "citeRegEx": "236", "shortCiteRegEx": null, "year": 2015}, {"title": "A Novel Multiple Kernel Learning Framework for Heterogeneous Feature Fusion and Variable Selection", "author": ["Y.-r. Yeh", "T.-c. Lin", "Y.-y. Chung", "Y.-c. F. Wang"], "venue": "IEEE Trans. Multimedia, 2012.", "citeRegEx": "237", "shortCiteRegEx": null, "year": 2012}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["M.H.P. Young", "A. Lai", "J. Hockenmaier"], "venue": "TACL, 2014.", "citeRegEx": "238", "shortCiteRegEx": null, "year": 2014}, {"title": "On the Integration of Grounding Language and Lear ning Objects", "author": ["C. Yu", "D. Ballard"], "venue": "AAAI, 2004.", "citeRegEx": "239", "shortCiteRegEx": null, "year": 2004}, {"title": "Grounded Language Learning from Video Described with Sentences", "author": ["H. Yu", "J.M. Siskind"], "venue": "ACL, 2013.", "citeRegEx": "240", "shortCiteRegEx": null, "year": 2013}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CVPR, 2016.", "citeRegEx": "241", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling Context in Referring Expressions", "author": ["L. Yu", "P. Poirson", "S. Yang", "A.C. Berg", "T.L. Berg"], "venue": "ECCV, 2016.", "citeRegEx": "242", "shortCiteRegEx": null, "year": 2016}, {"title": "Integration of Acoustic and Visual Speech Signals Using Neural Networks", "author": ["B.P. Yuhas", "M.H. Goldstein", "T.J. Sejnowski"], "venue": "IEEE Communications Magazine, 1989.", "citeRegEx": "243", "shortCiteRegEx": null, "year": 1989}, {"title": "Statistical Parametric Speech Synthesis Based on Speaker and Language Factorization", "author": ["H. Zen", "N. Braunschweiler", "S. Buchholz", "M.J.F. Gales", "S. Krstulovi", "J. Latorre"], "venue": "IEEE Transactions on Audio, Speech & Language Processing, 2012.", "citeRegEx": "244", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication, vol. 51, 2009.", "citeRegEx": "245", "shortCiteRegEx": null, "year": 2009}, {"title": "Leveraging Video Descriptions to Learn Video Question Answering", "author": ["K.-H. Zeng", "T.-H. Chen", "C.-Y. Chuang", "Y.-H. Liao", "J.C. Niebles", "M. Sun"], "venue": "AAAI, 2017.", "citeRegEx": "246", "shortCiteRegEx": null, "year": 2017}, {"title": "A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE TPAMI, 2009.", "citeRegEx": "247", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization", "author": ["D. Zhang", "W.-J. Li"], "venue": "AAAI, 2014.", "citeRegEx": "248", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Concept Taxonomies from Multi-modal Data", "author": ["H. Zhang", "Z. Hu", "Y. Deng", "M. Sachan", "Z. Yan", "E.P. Xing"], "venue": "ACL, 2016.", "citeRegEx": "249", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalized time warping for multimodal alignment of human motion", "author": ["F. Zhou", "F. De la Torre"], "venue": "CVPR, 2012.", "citeRegEx": "250", "shortCiteRegEx": null, "year": 2012}, {"title": "Canonical time warping for alignment of human behavior", "author": ["F. Zhou", "F. Torre"], "venue": "NIPS, 2009.", "citeRegEx": "251", "shortCiteRegEx": null, "year": 2009}, {"title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "ICCV, 2015.", "citeRegEx": "252", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 235, "context": "One of the earliest examples of multimodal research is audio-visual speech recognition (AVSR) [243].", "startOffset": 94, "endOffset": 99}, {"referenceID": 89, "context": "Given the prominence of hidden Markov models (HMMs) in the speech community at the time [95], it is", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "without surprise that many of the early models for AVSR were based on various HMM extensions [23], [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "without surprise that many of the early models for AVSR were based on various HMM extensions [23], [24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 144, "context": "renewed interest from the deep learning community [151].", "startOffset": 50, "endOffset": 55}, {"referenceID": 71, "context": ", low signal-to-noise ratio) [75], [151], [243].", "startOffset": 29, "endOffset": 33}, {"referenceID": 144, "context": ", low signal-to-noise ratio) [75], [151], [243].", "startOffset": 35, "endOffset": 40}, {"referenceID": 235, "context": ", low signal-to-noise ratio) [75], [151], [243].", "startOffset": 42, "endOffset": 47}, {"referenceID": 7, "context": "A second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [11], [188].", "startOffset": 121, "endOffset": 125}, {"referenceID": 181, "context": "A second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [11], [188].", "startOffset": 127, "endOffset": 132}, {"referenceID": 181, "context": "While earlier approaches for indexing and searching these multimedia videos were keyword-based [188], new research problems emerged when trying to search the visual and multimodal content directly.", "startOffset": 95, "endOffset": 100}, {"referenceID": 117, "context": "This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [123] and video summarization [52].", "startOffset": 105, "endOffset": 110}, {"referenceID": 48, "context": "This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [123] and video summarization [52].", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "recordings of meetings, all fully transcribed and annotated [32].", "startOffset": 60, "endOffset": 64}, {"referenceID": 132, "context": "speakers and listeners [139].", "startOffset": 23, "endOffset": 28}, {"referenceID": 172, "context": "This dataset formed the basis of the first audio-visual emotion challenge (AVEC) organized in 2011 [179].", "startOffset": 99, "endOffset": 104}, {"referenceID": 41, "context": "landmark detection, and facial expression recognition [45].", "startOffset": 54, "endOffset": 58}, {"referenceID": 200, "context": "later instantiation including healthcare applications such as automatic assessment of depression and anxiety [208].", "startOffset": 109, "endOffset": 114}, {"referenceID": 45, "context": "[49].", "startOffset": 0, "endOffset": 4}, {"referenceID": 78, "context": "One of the most representative applications is image captioning where the task is to generate a text description of the input image [83].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "This is motivated by the ability of such systems to help the visually impaired in their daily tasks [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "The task of visual question-answering (VQA) was recently proposed to address some of the evaluation challenges [9], where the goal is to answer a specific question about the image.", "startOffset": 111, "endOffset": 114}, {"referenceID": 13, "context": "[17] we use the term feature and representation interchangeably, with each referring to a vector or tensor representation of an entity, be it an image, audio sample, individual word, or a sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 74, "context": "Good representations are important for the performance of machine learning models, as evidenced behind the recent leaps in performance of speech recognition [79] and visual", "startOffset": 157, "endOffset": 161}, {"referenceID": 103, "context": "object classification [109] systems.", "startOffset": 22, "endOffset": 27}, {"referenceID": 13, "context": "[17] identify a number of properties for good representations: smoothness, temporal and spatial coherence, sparsity, and natural", "startOffset": 0, "endOffset": 4}, {"referenceID": 191, "context": "Srivastava and Salakhutdinov [198] identify additional desirable properties for multimodal representations: similarity in the representation space should reflect the similarity of the corresponding concepts, the representation should be easy to obtain even in the", "startOffset": 29, "endOffset": 34}, {"referenceID": 1, "context": "extensively studied [5], [17], [122].", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "extensively studied [5], [17], [122].", "startOffset": 25, "endOffset": 29}, {"referenceID": 116, "context": "extensively studied [5], [17], [122].", "startOffset": 31, "endOffset": 36}, {"referenceID": 121, "context": "descriptors in the early 2000s, the scale invariant feature transform (SIFT) was hand designed [127], but currently", "startOffset": 95, "endOffset": 100}, {"referenceID": 103, "context": "most visual descriptions are learned from data using neural architectures such as convolutional neural networks (CNN) [109].", "startOffset": 118, "endOffset": 123}, {"referenceID": 74, "context": "Similarly, in the audio domain, acoustic features such as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [79] and recurrent neural networks for para-linguistic analysis [207].", "startOffset": 180, "endOffset": 184}, {"referenceID": 199, "context": "Similarly, in the audio domain, acoustic features such as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [79] and recurrent neural networks for para-linguistic analysis [207].", "startOffset": 244, "endOffset": 249}, {"referenceID": 134, "context": "In natural language processing, the textual features initially relied on counting word occurrences in documents, but have been replaced datadriven word embeddings that exploit the word context [141].", "startOffset": 193, "endOffset": 198}, {"referenceID": 45, "context": "While there has been a huge amount of work on unimodal representation, up until recently most multimodal representations involved simple concatenation of unimodal ones [49], but this has been rapidly changing.", "startOffset": 168, "endOffset": 172}, {"referenceID": 57, "context": "Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces.", "startOffset": 94, "endOffset": 97}, {"referenceID": 204, "context": "Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces.", "startOffset": 129, "endOffset": 134}, {"referenceID": 45, "context": "plest example of a joint representation is a concatenation of individual modality features (also referred to as early fusion [49]).", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "Neural networks have become a very popular method for unimodal data representation [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 144, "context": "sent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217].", "startOffset": 92, "endOffset": 97}, {"referenceID": 149, "context": "sent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217].", "startOffset": 99, "endOffset": 104}, {"referenceID": 209, "context": "sent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217].", "startOffset": 106, "endOffset": 111}, {"referenceID": 13, "context": "Due to the multilayer nature of deep neural networks each successive layer is hypothesized to represent the data in a more abstract way [17], hence it is common to use the final or penultimate neural layers as a form of data representation.", "startOffset": 136, "endOffset": 140}, {"referenceID": 5, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 197, "endOffset": 200}, {"referenceID": 138, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 202, "endOffset": 207}, {"referenceID": 149, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 209, "endOffset": 214}, {"referenceID": 219, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 216, "endOffset": 221}, {"referenceID": 75, "context": "As neural networks require a lot of labeled training data, it is common to pre-train such representations using an autoencoder on unsupervised data [80].", "startOffset": 148, "endOffset": 152}, {"referenceID": 144, "context": "[151] extended the idea of using autoencoders to the multimodal domain.", "startOffset": 0, "endOffset": 5}, {"referenceID": 177, "context": "Similarly, Silberer and Lapata [184] proposed to use a multimodal autoencoder", "startOffset": 31, "endOffset": 36}, {"referenceID": 209, "context": "It is also common to fine-tune the resulting representation on a particular task at hand as the representation constructed using an autoencoder is generic and not necessarily optimal for a specific task [217].", "startOffset": 203, "endOffset": 208}, {"referenceID": 144, "context": "the disadvantages comes from the model not being able to handle missing data naturally \u2014 although there are ways to alleviate this issue [151], [217].", "startOffset": 137, "endOffset": 142}, {"referenceID": 209, "context": "the disadvantages comes from the model not being able to handle missing data naturally \u2014 although there are ways to alleviate this issue [151], [217].", "startOffset": 144, "endOffset": 149}, {"referenceID": 65, "context": "often difficult to train [69], but the field is making progress in better training techniques [196].", "startOffset": 25, "endOffset": 29}, {"referenceID": 189, "context": "often difficult to train [69], but the field is making progress in better training techniques [196].", "startOffset": 94, "endOffset": 99}, {"referenceID": 13, "context": "Probabilistic graphical models are another popular way to construct representations through the use of latent random variables [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 169, "context": "The most popular approaches for graphical-model based representation are deep Boltzmann machines (DBM) [176], that stack restricted Boltzmann machines (RBM) [81] as building blocks.", "startOffset": 103, "endOffset": 108}, {"referenceID": 76, "context": "The most popular approaches for graphical-model based representation are deep Boltzmann machines (DBM) [176], that stack restricted Boltzmann machines (RBM) [81] as building blocks.", "startOffset": 157, "endOffset": 161}, {"referenceID": 169, "context": "The appeal of DBMs comes from the fact that they do not need supervised data for training [176].", "startOffset": 90, "endOffset": 95}, {"referenceID": 169, "context": "As they are graphical models the representation of data is probabilistic, however it is possible to convert them to a deterministic neural network \u2014 but this loses the generative aspect of the model [176].", "startOffset": 199, "endOffset": 204}, {"referenceID": 190, "context": "Work by Srivastava and Salakhutdinov [197] introduced multimodal deep belief networks as a multimodal representation.", "startOffset": 37, "endOffset": 42}, {"referenceID": 98, "context": "[104] used a deep belief network for each modality and then combined them into joint representation for audiovisual emotion recognition.", "startOffset": 0, "endOffset": 5}, {"referenceID": 81, "context": "Huang and Kingsbury [86] used a similar model for AVSR, and Wu et al.", "startOffset": 20, "endOffset": 24}, {"referenceID": 217, "context": "[225] for audio and skeleton joint based gesture recognition.", "startOffset": 0, "endOffset": 5}, {"referenceID": 191, "context": "Multimodal deep belief networks have been extended to multimodal DBMs by Srivastava and Salakhutdinov [198].", "startOffset": 102, "endOffset": 107}, {"referenceID": 149, "context": "[156] explore the use of multimodal DBMs for the task of human pose estimation from multi-view data.", "startOffset": 0, "endOffset": 5}, {"referenceID": 192, "context": "[199] use multimodal DBM representation to perform Alzheimer\u2019s", "startOffset": 0, "endOffset": 5}, {"referenceID": 138, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 37, "endOffset": 42}, {"referenceID": 144, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 44, "endOffset": 49}, {"referenceID": 219, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 51, "endOffset": 56}, {"referenceID": 177, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 71, "endOffset": 76}, {"referenceID": 191, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 108, "endOffset": 113}, {"referenceID": 98, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 129, "endOffset": 134}, {"referenceID": 90, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 160, "endOffset": 164}, {"referenceID": 145, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 166, "endOffset": 171}, {"referenceID": 159, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 186, "endOffset": 191}, {"referenceID": 56, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 37, "endOffset": 41}, {"referenceID": 99, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 43, "endOffset": 48}, {"referenceID": 152, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 62, "endOffset": 67}, {"referenceID": 223, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 69, "endOffset": 74}, {"referenceID": 27, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 100, "endOffset": 104}, {"referenceID": 204, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 106, "endOffset": 111}, {"referenceID": 240, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 113, "endOffset": 118}, {"referenceID": 212, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 140, "endOffset": 145}, {"referenceID": 191, "context": "The major disadvantage of DBMs is the difficulty of training them \u2014 high computational cost, and the need to use approximate variational training methods [198].", "startOffset": 154, "endOffset": 159}, {"referenceID": 77, "context": "Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213].", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213].", "startOffset": 202, "endOffset": 206}, {"referenceID": 205, "context": "Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213].", "startOffset": 208, "endOffset": 213}, {"referenceID": 8, "context": "a way that a decoder could reconstruct it [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": "[42] on AVSR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166].", "startOffset": 54, "endOffset": 58}, {"referenceID": 145, "context": "representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166].", "startOffset": 60, "endOffset": 65}, {"referenceID": 159, "context": "representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166].", "startOffset": 157, "endOffset": 162}, {"referenceID": 57, "context": "For example such models encourage the representation of the word dog and an image of a dog to have a smaller distance between them than distance between the word dog and an image of a car [61].", "startOffset": 188, "endOffset": 192}, {"referenceID": 213, "context": "[221], [222] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations.", "startOffset": 0, "endOffset": 5}, {"referenceID": 214, "context": "[221], [222] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations.", "startOffset": 7, "endOffset": 12}, {"referenceID": 56, "context": "An example of such coordinated representation is DeViSE \u2014 a deep visual-semantic embedding [60].", "startOffset": 91, "endOffset": 95}, {"referenceID": 99, "context": "[105] extended this to sentence and image coordinated representation by using an LSTM model and a pairwise ranking loss to coordinate the feature space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 184, "context": "[191] tackle the same task, but extend the language model to a dependency tree RNN to incorporate compositional semantics.", "startOffset": 0, "endOffset": 5}, {"referenceID": 152, "context": "[159], but using videos instead of images.", "startOffset": 0, "endOffset": 5}, {"referenceID": 223, "context": "[231] also constructed a coordinated space between videos and sentences using a \u3008subject, verb, object\u3009 compositional language model and a deep video model.", "startOffset": 0, "endOffset": 5}, {"referenceID": 210, "context": "for similar objects [218].", "startOffset": 20, "endOffset": 25}, {"referenceID": 22, "context": "The idea of cross-modal hashing is to create such codes for cross-modal retrieval [26], [93], [113].", "startOffset": 82, "endOffset": 86}, {"referenceID": 87, "context": "The idea of cross-modal hashing is to create such codes for cross-modal retrieval [26], [93], [113].", "startOffset": 88, "endOffset": 92}, {"referenceID": 107, "context": "The idea of cross-modal hashing is to create such codes for cross-modal retrieval [26], [93], [113].", "startOffset": 94, "endOffset": 99}, {"referenceID": 22, "context": "as a hash function attempts to enforce all of these three requirements [26], [113].", "startOffset": 71, "endOffset": 75}, {"referenceID": 107, "context": "as a hash function attempts to enforce all of these three requirements [26], [113].", "startOffset": 77, "endOffset": 82}, {"referenceID": 86, "context": "For example, Jiang and Li [92] introduced a method to learn such common binary space between sentence descriptions and corresponding images", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "[31] extended the approach with a more complex", "startOffset": 0, "endOffset": 4}, {"referenceID": 211, "context": "[219] constructed a coordinated space in which images (and", "startOffset": 0, "endOffset": 5}, {"referenceID": 204, "context": "Another example of a structured coordinated representation comes from order-embeddings of images and language [212], [249].", "startOffset": 110, "endOffset": 115}, {"referenceID": 241, "context": "Another example of a structured coordinated representation comes from order-embeddings of images and language [212], [249].", "startOffset": 117, "endOffset": 122}, {"referenceID": 204, "context": "[212] enforces a dissimilarity metric that is asymmetric and implements the notion of partial order in the multimodal space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 230, "context": "[238] where denotation graphs are used to induce a partial ordering.", "startOffset": 0, "endOffset": 5}, {"referenceID": 241, "context": "present how exploiting structured representations of text and images can create concept taxonomies in an unsupervised manner [249].", "startOffset": 125, "endOffset": 130}, {"referenceID": 79, "context": "A special case of a structured coordinated space is one based on canonical correlation analysis (CCA) [84].", "startOffset": 102, "endOffset": 106}, {"referenceID": 72, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 64, "endOffset": 68}, {"referenceID": 100, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 70, "endOffset": 75}, {"referenceID": 162, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 77, "endOffset": 82}, {"referenceID": 170, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 115, "endOffset": 120}, {"referenceID": 180, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 122, "endOffset": 127}, {"referenceID": 3, "context": "Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [7], [116].", "startOffset": 85, "endOffset": 88}, {"referenceID": 110, "context": "Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [7], [116].", "startOffset": 90, "endOffset": 95}, {"referenceID": 110, "context": "Kernel canonical correlation analysis (KCCA) [116] uses reproducing kernel Hilbert spaces for projection.", "startOffset": 45, "endOffset": 50}, {"referenceID": 3, "context": "Deep canonical correlation analysis (DCCA) [7] was introduced as an alternative to KCCA and addresses the scalability issue, it was also shown to lead to better correlated representation space.", "startOffset": 43, "endOffset": 46}, {"referenceID": 53, "context": "Similar correspondence autoencoder [57] and deep correspondence RBMs [56] have also been proposed for cross-modal retrieval.", "startOffset": 35, "endOffset": 39}, {"referenceID": 52, "context": "Similar correspondence autoencoder [57] and deep correspondence RBMs [56] have also been proposed for cross-modal retrieval.", "startOffset": 69, "endOffset": 73}, {"referenceID": 212, "context": "Deep canonically correlated autoencoders [220] also include an autoencoder based data reconstruction term.", "startOffset": 41, "endOffset": 46}, {"referenceID": 240, "context": "method [248] also encourages semantic relevance, while retaining correlation maximization and orthogonality of the resulting space \u2014 this leads to a combination of CCA and cross-modal hashing techniques.", "startOffset": 7, "endOffset": 12}], "year": 2017, "abstractText": "Our experience of the world is multimodal we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "creator": "LaTeX with hyperref package"}}}