{"id": "1705.09436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Human Trajectory Prediction using Spatially aware Deep Attention Models", "abstract": "Trajectory Prediction of dynamic objects is a widely studied topic in the field of artificial intelligence. Thanks to a large number of applications like predicting abnormal events, navigation system for the blind, etc. there have been many approaches to attempt learning patterns of motion directly from data using a wide variety of techniques ranging from hand-crafted features to sophisticated deep learning models for unsupervised feature learning. All these approaches have been limited by problems like inefficient features in the case of hand crafted features, large error propagation across the predicted trajectory and no information of static artefacts around the dynamic moving objects. We propose an end to end deep learning model to learn the motion patterns of humans using different navigational modes directly from data using the much popular sequence to sequence model coupled with a soft attention mechanism. We also propose a novel approach to model the static artefacts in a scene and using these to predict the dynamic trajectories. The proposed method, tested on trajectories of pedestrians, consistently outperforms previously proposed state of the art approaches on a variety of large scale data sets. We also show how our architecture can be naturally extended to handle multiple modes of movement (say pedestrians, skaters, bikers and buses) simultaneously.", "histories": [["v1", "Fri, 26 May 2017 05:37:36 GMT  (2530kb,D)", "http://arxiv.org/abs/1705.09436v1", "10 pages, 5 figures, Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017)"]], "COMMENTS": "10 pages, 5 figures, Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daksh varshneya", "g srinivasaraghavan"], "accepted": false, "id": "1705.09436"}, "pdf": {"name": "1705.09436.pdf", "metadata": {"source": "CRF", "title": "Human Trajectory Prediction using Spatially aware Deep Attention Models", "authors": ["Daksh Varshneya", "G. Srinivasaraghavan"], "emails": ["daksh.varshneya@iiitb.org", "gsr@iiitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Learning and drawing conclusions from visual data have recently gained enormous prominence in artificial intelligence research, much of it due to groundbreaking advances in AI and deep learning that have enabled vision and imaging systems to achieve near-human precision for many complex visual recognition tasks. In this paper, we present a method for learning and predicting the dynamic spatial behavior of people who move in crowded scenes using multiple navigation modes. As humans, we have the ability to easily navigate densely populated areas while walking or driving. In this paper, we propose a consistent deep learning system that can learn such limited navigation behavior by taking into account multiple influencing factors such as the adjacent dynamic subjects and also the spatial context in which the subject is located. We also show how our architecture can be naturally expanded to include multiple modes of navigation (for example, skaters, buses and cyclists)."}, {"heading": "2 Problem Statement", "text": "The question we are trying to answer is: \"Have we observed the trajectories [x (1) i,.., x (T) i], 1 \u2264 i \u2264 N of the moving subjects for T-time units?,\" \"Have we observed the trajectories [x (1) i,.., x (T) i], 1 \u2264 i \u2264 N of the moving subjects for T-time units?,\" \"Are these subjects likely at times T1,..., Tn after the initial observation time T?,\" where x (t) i denotes the two-dimensional spatial coordinate vector of the ith target object at time t?. \"In other words: We must predict values of [x (T1) i,.., x (Tn) i]. We assume that we have annotations of the spatial coordinate tracks of each unique subject participating in the respective scene."}, {"heading": "3 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 RNNs for Sequence to Sequence modeling", "text": "Encoder decoder models [1, 2, 3, 4] were originally introduced for machine translation tasks in [5], followed by automated answers to questions in [16]. These architectures encode the incoming sequential data into a fixed-size hidden representation using a Recurrent Neural Network (RNN) and then decrypt this hidden representation with another RNN to generate a sequential time output. These networks have also been modified to introduce an attention mechanism into them. These networks are inspired by the attention mechanism that humans visually possess. We, as humans, adjust our focus over time to focus more on a particular region of our vision, higher resolution and the environment to a lower resolution. Attention networks have been used very successfully for automatic fine-grained image descriptions / annotations [7]."}, {"heading": "3.2 Object-Object Interaction modeling", "text": "Helbing and Molnar's social power model [17] was the first to learn interaction patterns between different objects, such as attractive and repellent forces. Since then, various variants have been studied, such as (i) agent-based modeling [18] to use human attributes as models for learning behavioral patterns, and (ii) technical approaches such as those by Alahi et.al. [19] that extract social affinity traits to learn such patterns. Other approaches include identifying the most common object behavior through clusters [14, 15]. Giannotti et.al. [14] analyzed GPS traces of several bus fleets and extracted patterns in trajectories as concise descriptions of frequent behavior, both temporally and spatially.Recently, Alahi et.al. [22] recorded interactions between pedestrians using multiple Long Short Term Memory Networks (LSTMs) [23] and a social interaction mechanism for human pooling."}, {"heading": "3.3 Spatial Context modeling", "text": "Previous work has included matching-based approaches [8, 9] based on keypoint feature matching techniques, which are slow to calculate because they have to match each test image with an image database; and because it is a direct matching approach, there is no semantic understanding of the scene; most conventional approaches tend to be brittle because they rely heavily on handmade features; and a deep learning approach has also been used for spatial context modeling in [10], which assumes that dynamic objects and static objects can be semantically matched to the interaction they have with each other; and that a random area of the scene contains enough evidence to make the distinction between probable and unlikely patches for a particular object."}, {"heading": "4 Proposed System", "text": "This section describes our solution in detail and is structured as follows: First, we describe our proposed Spatially Static Context Network (SSCN) to model the static spatial context around the topic of interest; second, we define the pooling mechanism that captures the influence that eighteen boring subjects and nearby static artifacts have on a target object; and second, we describe our complete model, which uses an attention mechanism with LSTMs to learn patterns of spatial coordinates of subjects while preserving the spatial and dynamic context around the subjects of interest. Although LSTMs are traditionally used to model (typically short-term) time dependencies in sequential data, their use, together with an appropriate attention mechanism, allows us to use them for tasks such as flight path planning that require long-term dependency modeling."}, {"heading": "4.1 Spatial Context Matching", "text": "It is also very important that the model encompasses a large number of complex scenes and allows conclusions to be drawn about the interactions between people and space. Our proposed architecture clearly differs from this approach in two respects. First, it is redundant to build an input branch that accommodates different types of image fields, because an object belonging to the same semantic class (car, pedestrian) should ideally have the same spatial correspondence with a certain random patch of the image. For example, each car should have the same matching score with a patch of the road and hence. Second, it might be difficult to find a certain random patch of the image."}, {"heading": "4.2 Pooling Mechanism", "text": "We describe how to summarize the static and dynamic contexts for a given frame. (We use the F subscript to indicate the fact that the pooling that is done is specific to the frame. Dynamic Context PoolingHumans moving in a crowded area adapt their movement based on the behavior of people around them. For example, pedestrians often completely change their paths when they see someone else or a group of people approaching them. Such behavior cannot be predicted by observing a pedestrian zone in isolation without taking into account the surrounding dynamic and static context. This behavior motivated the pooling mechanism of the Social LSTM model [22]. We borrow the same pooling mechanism to capture such influences from neighboring subjects for our model. We use LSTMs to learn an efficient hidden representation of the temporal behavior of subjects as part of the observed subject."}, {"heading": "4.3 Spatio - Temporal Attention Model", "text": "The motivation for applying an attention mechanism is manageable; subjects often change their predicted trajectories as the \"context\" changes. Imagine a pedestrian in an airport walking toward safety and suddenly realizing that he / she has to pick up the piece of luggage and as a result make a sharp course correction to move toward the check-in counter. As this by Alahi et al. [22] only takes the final step toward the hidden representation of the pedestrian zone of interest, the model will respond to immediate instincts such as collision avoidance, but will not be very useful when the model begins its predictions, even a small error in the prediction could mean that the erroneous hidden representations will be extended to future time.The complete architecture for our spatio-temporary attention model will be shown in Figure 2.We emed the predictions in the prediction mean that the erroneous representations will be extended to future time."}, {"heading": "5 Experiments", "text": "The three large multi-object tracking datasets - ETH [11], UCY [12] and the Stanford Drone Dataset [13]. The ETH and UCY datasets consist of 5 scenes with 1,536 unique pedestrians entering and leaving the scenes. It includes challenging scenarios such as groups of people crossing each other and also the behavior of a pedestrian who completely deviates from his path. On the other hand, the Stanford Drone Dataset [13] consists of several aerial images consisting of 8 different locations around the Stanford campus and objects belonging to 6 different classes. We only use pedestrian trajectories to train and test our models. SetupWe set the hyperparameters of our model with a cross-validation strategy that takes an approach."}, {"heading": "5.1 Quantitative Results", "text": "We build two separate models for the complete problem - one with only dynamic context pooling coupled with the attention mechanism we call the D-ATT model - and the second with static context pooling added to the D-ATT model we call the - SD-ATT. We cannot test the SD-ATT model for ETH and UCY data sets because the resolution of the videos in these data sets is too low, making it impossible to use the SSCN model for static context pooling. Nevertheless, we consistently perform better than the S-LSTM [22] and O-LSTM [22] models in both evaluation metrics for all three data sets, as shown in Table 1. We also show that the results of the SD-ATT model on the Stanford Drone data set are much better than the Social LSTM [22] model."}, {"heading": "5.2 Qualitative Results", "text": "We show scenarios in which our models perform better than the S-LSTM [22] model. First, we show the results for the SSCN model in Figure 3. Second column of the figure shows an example of the designed highlighting map with the blue shade indicating the probability value of the corresponding spot. We can see that the probability values near the roundabout, the car, and the trees are low and high for areas such as road and pavement. In the rest of the figures, each unique pedestrian is represented by a unique color. In Figure 4, the first column shows that the SD-ATT model has learned to predict nonlinear trajectories. The next two columns show the collision avoidance property that the model has learned. In both examples, the model either delays one of the pedestrians or deflects it to avoid a collision. Figure 5 (a) compares the predictions of the SD-ATT model with the Social LM [22] model."}, {"heading": "6 Conclusion", "text": "We propose a novel approach to predicting human trajectories. Our model successfully extracts motion patterns in an unattended manner. Compared to previous state-of-the-art work, our approach models both the dynamic and spatial context around each type of subject, resulting in a better prediction of trajectories. Our proposed method exceeds the current state of the art in three large datasets. In addition, we propose a novel CNN-based SSCN architecture that contributes to a better semantic understanding of the scene."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning recursive distributed representations for holistic computation", "author": ["Lonnie Chrisman"], "venue": "Connection Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}, {"title": "Recursive hetero-associative memories for translation", "author": ["Mikel L Forcada", "Ram \u0301on P Neco"], "venue": "In International Work-Conference on Artificial Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR. Retrieved May", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Sift flow: Dense correspondence across different scenes", "author": ["C. Liu", "J. Yuen", "A. Torralba", "J. Sivic", "W.T. Free-man"], "venue": "Computer Vision\u2013ECCV 2008, pp. 28\u201342, Springer, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A data-driven approach for event prediction", "author": ["J. Yuen", "A. Torralba"], "venue": "Computer Vision\u2013ECCV 2010, pp. 707\u2013720, Springer, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep Learning Driven Visual Path Prediction From a Single Image", "author": ["S. Huang", "X. Li", "Z. Zhang", "Z. He", "F. Wu", "W. Liu", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "You\u2019ll never walk alone: Modeling social behavior for multi-target tracking", "author": ["S. Pellegrini", "A. Ess", "K. Schindler", "L. Van Gool"], "venue": "In Computer Vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Crowds by example", "author": ["A. Lerner", "Y. Chrysanthou", "D. Lischinski"], "venue": "In Computer Graphics Forum,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Trajectory pattern mining", "author": ["F. Giannotti", "M. Nanni", "F. Pinelli", "D. Pedreschi"], "venue": "in: ACM SIGKDD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Learning trajectory patterns by clustering: Experi- mental studies and comparative evaluation", "author": ["B. Morris", "M.M. Trivedi"], "venue": "in: CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Attention-based encoder-decoder model for answer selection in question answering", "author": ["Y. Nie", "Y. Han", "J. Huang", "B. Jiao", "A. Li"], "venue": "Frontiers of Information Technology & Electronic Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Social force model for pedestrian dynamics", "author": ["D. Helbing", "P. Molnar"], "venue": "Physical review E,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Agent-based modeling: Methods and tech- niques for simulating human systems", "author": ["E. Bonabeau"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Socially-aware large-scale crowd forecasting", "author": ["A. Alahi", "V. Ramanathan", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Robust object tracking by hierarchical association of detection responses", "author": ["C. Huang", "B. Wu", "R. Nevatia"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Multi-hypothesis motion planning for visual object tracking", "author": ["H. Gong", "J. Sim", "M. Likhachev", "J. Shi"], "venue": "In Proceedings of the 2011 International Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Social LSTM: Human Trajectory Prediction in Crowded Spaces", "author": ["A. Alahi", "K. Goel", "V. Ramanathan", "A. Robicquet", "L. Fei-Fei", "S. Savarese"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Long Short Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Gradient-based learning applied to document recognition.", "author": ["LeCun", "Yann"], "venue": "Proceedings of the IEEE", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Describing Videos by Exploiting Temporal Structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 1, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 2, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 3, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 4, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 143, "endOffset": 147}, {"referenceID": 6, "context": "Attention Networks have been used very successfully for automatic fine-grained image description/annotation [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 15, "context": "Helbing and Molnar\u2019s social force model[17] was the first to learn interaction patterns between different objects such as attractive and repulsive forces.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Since then, several variants such as (i) agent based modeling[18] to use human attributes as model priors to learn behavioral patterns, and (ii) feature engineered approaches like that of Alahi et.", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "[19] which extract social affinity features to learn such patterns, have been explored.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Other approaches include finding the most common object behaviour by means of clustering[14, 15].", "startOffset": 88, "endOffset": 96}, {"referenceID": 13, "context": "Other approaches include finding the most common object behaviour by means of clustering[14, 15].", "startOffset": 88, "endOffset": 96}, {"referenceID": 12, "context": "[14] analyzed GPS traces of several fleets of buses and extracted patterns in trajectories as concise descriptions of frequent behaviour, both temporally and spatially.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] captured the interactions between pedestrians using multiple Long Short Term Memory Networks(LSTMs)[23] and a social pooling mechanism to capture humanhuman interactions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] captured the interactions between pedestrians using multiple Long Short Term Memory Networks(LSTMs)[23] and a social pooling mechanism to capture humanhuman interactions.", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Such spatial modeling exists in [20, 21] but these do not include the dynamic modeling of the crowd.", "startOffset": 32, "endOffset": 40}, {"referenceID": 19, "context": "Such spatial modeling exists in [20, 21] but these do not include the dynamic modeling of the crowd.", "startOffset": 32, "endOffset": 40}, {"referenceID": 7, "context": "Earlier works include matching-based approaches[8, 9] which rely on keypoint feature matching techniques.", "startOffset": 47, "endOffset": 53}, {"referenceID": 8, "context": "Earlier works include matching-based approaches[8, 9] which rely on keypoint feature matching techniques.", "startOffset": 47, "endOffset": 53}, {"referenceID": 9, "context": "A deep learning approach was also used for spatial context modeling in [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Our proposed architecture is composed of Convolutional Neural Networks(CNNs)[24] and is inspired by the Spatial Matching Network introduced in [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "Our proposed architecture is composed of Convolutional Neural Networks(CNNs)[24] and is inspired by the Spatial Matching Network introduced in [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "This behaviour motivated the pooling mechanism of the Social LSTM model[22].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "We construct social tensors[22] S iF each of size (gs \u00d7 gs \u00d7 dH \u00d7 C) that capture the social context in a structured way", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "[22] only takes the last time step hidden representation of the pedestrian of interest, the model will be responsive to immediate instincts like collision avoidance but not be very useful in long term path planning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] to compute the attention weights \u03b1 i .", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "We use three large scale multi-object tracking datasets - ETH [11] , UCY [12] and the Stanford Drone Dataset [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "We use three large scale multi-object tracking datasets - ETH [11] , UCY [12] and the Stanford Drone Dataset [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "To compare our results with that of previous state of the art model - S-LSTM[22], we limit the subject type to only pedestrians.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "[22] - (i) Average Displacement error The euclidean distance between the predicted trajectory and the actual trajectory averaged over all time-steps for all pedestrians and (ii) Final Displacement error - The average euclidean distance between the predicted trajectory point and the actual trajectory point at the end of n time steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Table 1: Comparison across datasets Metric Dataset O-LSTM[22] S-LSTM[22] D-ATT SD-ATT Avg.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "Table 1: Comparison across datasets Metric Dataset O-LSTM[22] S-LSTM[22] D-ATT SD-ATT Avg.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "Error ETH[11] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "47 HOTEL[11] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "12 ZARA1[12] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "Error ETH[11] 1.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "85 HOTEL[11] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "19 ZARA1[12] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "Still, we consistently outperform the S-LSTM[22] and O-LSTM[22] models on both the evaluation metrics on all three datasets as shown in table 1.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "Still, we consistently outperform the S-LSTM[22] and O-LSTM[22] models on both the evaluation metrics on all three datasets as shown in table 1.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "We also show that the results of the SD-ATT model on the Stanford Drone Dataset are much better than the Social LSTM[22] model.", "startOffset": 116, "endOffset": 120}, {"referenceID": 20, "context": "We demonstrate scenarios where our models perform better than the S-LSTM[22] model.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "Figure 5(a) compares the predictions of SD-ATT model against the Social LSTM[22] model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "In both the examples shown, the pedestrian walks straight for T time steps because of which S-LSTM[22] and D-ATT", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "(a) Advantage of Attention Mechanism (b) Advantage of Static Context Pooling Figure 5: Comparison of our models against S-LSTM[22] model", "startOffset": 126, "endOffset": 130}], "year": 2017, "abstractText": "Trajectory Prediction of dynamic objects is a widely studied topic in the field of artificial intelligence. Thanks to a large number of applications like predicting abnormal events, navigation system for the blind, etc. there have been many approaches to attempt learning patterns of motion directly from data using a wide variety of techniques ranging from hand-crafted features to sophisticated deep learning models for unsupervised feature learning. All these approaches have been limited by problems like inefficient features in the case of hand crafted features, large error propagation across the predicted trajectory and no information of static artefacts around the dynamic moving objects. We propose an end to end deep learning model to learn the motion patterns of humans using different navigational modes directly from data using the much popular sequence to sequence model coupled with a soft attention mechanism. We also propose a novel approach to model the static artefacts in a scene and using these to predict the dynamic trajectories. The proposed method, tested on trajectories of pedestrians, consistently outperforms previously proposed state of the art approaches on a variety of large scale data sets. We also show how our architecture can be naturally extended to handle multiple modes of movement (say pedestrians, skaters, bikers and buses) simultaneously.", "creator": "LaTeX with hyperref package"}}}