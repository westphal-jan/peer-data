{"id": "1705.09516", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models", "abstract": "Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex hand-crafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.", "histories": [["v1", "Fri, 26 May 2017 10:36:12 GMT  (589kb,D)", "http://arxiv.org/abs/1705.09516v1", "The work has been accepted in BioNLP at ACL-2017"]], "COMMENTS": "The work has been accepted in BioNLP at ACL-2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["patchigolla v s s rahul", "sunil kumar sahu", "ashish anand"], "accepted": false, "id": "1705.09516"}, "pdf": {"name": "1705.09516.pdf", "metadata": {"source": "CRF", "title": "Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models", "authors": ["Patchigolla V S S Rahul", "Sunil Kumar Sahu", "Ashish Anand"], "emails": ["rahul.rahul.pvss@gmail.com", "anand.ashish}@iitg.ernet.in"], "sections": [{"heading": null, "text": "Biomedical events describe complex interactions between different biomedical units. Event trigger is a word or phrase that typically describes the occurrence of an event. Event trigger identification is an important first step in all methods of event extraction. However, many of the current approaches are either based on complex handcrafted features or consider features only within a window. In this paper, we propose a method that uses the advantage of recursive neural networks (RNN) to extract features present across the sentence at higher levels. Therefore, hidden state representations of RNN along with word and entity type embedding as features avoid relying on complex handcrafted features generated using various NLP toolkits. Our experiments have shown that a state-of-the-art F1 score on Multi-Level Event Extraction (MLEE) corpus is also discussed in the analysis of the significance and outcome of the result."}, {"heading": "1 Introduction", "text": "Biomedical events play an important role in improving biomedical research in many ways. Some of their applications include path curation (Ohta et al., 2013) and the development of a domain-specific semantic search engine (Ananiadou et al., 2015). In order to gain attraction among researchers, many new methods have been developed, such as BioNLP '09 (Kim et al., 2009), BioNLP' 11 (Kim et al., 2011), BioNLP '13 (Ne \u0301 dellec et al., 2013) and many new methods have also been proposed to address these tasks. An event can be defined as a combination of a trigger word and an arbitrary number of arguments. Figure 1 shows two events with trigger words such as \"inhibition\" and \"angiogenesis\" of trigger types \"negative regulation\" and \"blood vessel development.\""}, {"heading": "2 Related Work", "text": "Pyysalo et al. (2012) proposed a model in which various handcrafted features are extracted from the processed data and fed into a support vector machine (SVM) to perform a final classification. Zhou et al. (2014) proposed a novel framework for trigger identification, in which features of the word are fed into SVM in combination with handcrafted features for final classification using multiple kernel learning. Wei et al. (2015) proposed a pipeline method for BioNLP '13 corpus based on Conditional Random Field (CRF) and Support Vector Machine (SVM), in which CRF is used to mark valid triggers and SVM is ultimately used to identify the trigger type. The above methods rely on various NLP toolkits to extract the handcrafted features of FFN (F), which are not intersected by the traditional features of the network, resulting in differing performance and therefore having to be attributed to different methods."}, {"heading": "3 Model Architecture", "text": "Our model is based on bidirectional RNN, as shown in Figure 2 for the task of trigger identification. The proposed model recognizes both trigger words and their type. Our model uses embedding characteristics of words in the input level and learns higher representations in the subsequent levels and uses both the input level and higher characteristics to perform the final classification. We will now briefly explain each component of our model."}, {"heading": "3.1 Input Feature Layer", "text": "For each word in the sentence, we extract two characteristics, the exact word w-W and the entity type e-E. Here, W refers to the dictionary of words, and E to the dictionary of entities. Apart from all entities, E also contains the entity type None, which indicates the absence of an entity. In some cases, the entity may span several words, in which case we assign the same entity type to each word spanned by that entity."}, {"heading": "3.2 Embedding or Lookup Layer", "text": "Let us assume that Ew and Ee are the embedding matrices of W and E. The characteristics derived from these embedding matrices are concatenated and treated as the last word-level attribute (l) of the model. Ew embedding matrix is initialized with pre-trained word embeddings and the Ee-Rne-de embeddings matrix is initialized with random values. Here, nw, ne refer to the length of the dictionary or dictionary of entities and dw, de refer to the dimension of the word or entity embeddings."}, {"heading": "3.3 Bidirectional RNN Layer", "text": "We use both LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) variants of RNN in our experiments because they better handle the problem of the dwindling and exploding gradient (Pascanu et al., 2012). We use the bidirectional version of RNN (Graves, 2013), where for each word forward RNN captures features from the past and backward RNN features from the future, with each word inherently containing information about the whole sentence."}, {"heading": "3.4 Feed Forward Neural Network", "text": "The hidden state of the bi-directional RNN layer acts as sentence plane (g), the word and entity type embedding (l) acts as word plane, are both concatenated (1) and pass through a series of hidden layers (2), (3) with dropouts (Srivastava et al., 2014) and an output plane. In the output layer, the number of neurons is equal to the number of trigger markers. Finally, we use Softmax function (4) to obtain probability values for each classe.f = gk-lk (1) h0 = tanh (W0f + b0) (2) hi = tanh (Wihi \u2212 1 + bi) (3) p (y | x) = Softmax (Wohi + bo) (4) Here k refers to the kest word of the sentence, i refers to the ith-hidden layer in the network, i refers to the network operation, the parameter, the orbo, and where respectively."}, {"heading": "3.5 Training and Hyperparameters", "text": "The implementation 1 of the model is done in Python language using the library Theano (Bergstra et al., 2010). We use word embedding received from Moen et al. (2013) with word2vec. We use training and development sets for hyperparameter selection. We use word embedding of the 200 dimension, embedding of the entity type of the 50 dimension, RNN hidden state dimension of 250 and 2 hidden layers with dimensions 150 and 100. In both hidden layers we use dropout of the 0.2.1implementation is available at https: / / github.com / rahulpatchigolla / EventTriggerDetection."}, {"heading": "4 Experiments and discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset Description", "text": "We use MLEE (Pyysalo et al., 2012) corpus to perform our trigger identification experiments. Unlike other corpus for event extraction, it covers events at different levels from the molecular to the organism level. Events in this corpus are roughly divided into 4 categories, namely \"Anatomical,\" \"Molecular,\" \"General,\" \"Planned,\" which are further subdivided into 19 sub-categories, as in Table 1. Here, our task is to identify the correct sub-category of an event. Entity types associated with the data set are summarized in Table 2."}, {"heading": "4.2 Experimental Design", "text": "The data is provided in three parts as training, development and test kits. Hyperparameters are tuned using the development kit and then the final model is trained using the combined training and development kits based on the selected hyperparameters. The final results reported here are the best results over 5 laps. We used the micro-averaged F1 score as a benchmark and evaluated the performance of the model by ignoring the trigger classes with the number \u2264 10 in the test kit during training and viewing them as false-negative directly during the test."}, {"heading": "4.3 Performance comparison with Baseline Models", "text": "We compare our results with base models presented in Table 3. Pyysalo et al. (2012) defined an SVM-based classifier with handmade features. Wang et al. (2016a) defined a window-based CNN classifier. In addition to the proposed models, we also compare our results with two other base methods FFNN and CNN2004, which are our implementations. FFNN is a window-based forward-looking neural network in which word embedding within the window is used to predict trigger labeling (Collobert et al., 2011). We chose the window size as 3 (one word from the left and one word from the right) after setting it in the validation set. CNN2004 is our implementation of a window-based CNN classifier proposed by Wang et al. (2016a), due to the unavailability of its code in the public area of the model, which has shown a slight improvement in our model."}, {"heading": "4.4 Category Wise Performance Analysis", "text": "It can be observed that the performance of the model in anatomical and molecular categories is better than general and planned categories. From the confusion matrix shown in Figure 3, we can also conclude that positive regulation, negative regulation and regulation between general categories and planned categories cause many false positives and false negatives, thereby worsening the performance of the model."}, {"heading": "4.5 Further Analysis", "text": "In this section, we will examine the meaning of various features and model variants as in Table 5. Here, Ew + Ee and Ee refer to the use of word and entity embedding as a feature in the model, l and g to the use of word and sentence embedding for the final prediction. Examples in Table 6 illustrate the meaning of features used in the most powerful models. In Sentence 1, the word \"Knockdown\" is part of a unit, namely \"round about knockdown endothelial cells\" type \"Cell,\" and in Sentence 2, it is the trigger word type \"Planned Process,\" Methods 1 and 2 have failed to distinguish the two because they have no knowledge of the entity type. In Sentence 3, \"impaired\" is a trigger word type \"Negative Regulation\" Methods of the type \"Planned Process,\" Methods 1 and Methods 2 cannot help to identify the model, but successfully improve the feature \"l\" in the variation."}, {"heading": "5 Conclusion and Future Work", "text": "In this work, we have proposed a novel approach to trigger identification by learning higher quality features using RNN. Our experiments have shown that we can achieve state-of-the-art results on the MLEE corpus. In the future, we would like to perform a complete event extraction using deep learning techniques."}], "references": [{"title": "Event-based text mining for biology and functional genomics", "author": ["Sophia Ananiadou", "Paul Thompson", "Raheel Nawaz", "John McNaught", "Douglas B Kell."], "venue": "Briefings in functional genomics 14(3):213\u2013230.", "citeRegEx": "Ananiadou et al\\.,? 2015", "shortCiteRegEx": "Ananiadou et al\\.", "year": 2015}, {"title": "Theano: A cpu and gpu math compiler in python", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proc. 9th Python in Science", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "J. Mach. Learn. Res. 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "CoRR abs/1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. pages 1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Overview of bionlp\u201909 shared task on event extraction", "author": ["Jin-Dong Kim", "Tomoko Ohta", "Sampo Pyysalo", "Yoshinobu Kano", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the Workshop on Current Trends", "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Overview of bionlp shared task", "author": ["Jin-Dong Kim", "Sampo Pyysalo", "Tomoko Ohta", "Robert Bossy", "Ngan Nguyen", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the BioNLP Shared Task 2011 Workshop. Association for Computational Linguistics,", "citeRegEx": "Kim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2011}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "ACL 2014. pages 302\u2013 308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributional semantics resources for biomedical text processing", "author": ["Hans Moen", "Sampo Pyysalo", "Filip Ginter", "Tapio Salakoski", "Sophia Ananiadou"], "venue": null, "citeRegEx": "Moen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moen et al\\.", "year": 2013}, {"title": "Overview of bionlp shared task 2013", "author": ["Claire N\u00e9dellec", "Robert Bossy", "Jin-Dong Kim", "JungJae Kim", "Tomoko Ohta", "Sampo Pyysalo", "Pierre Zweigenbaum."], "venue": "Proceedings of the BioNLP Shared Task 2013 Workshop. Association for Computational", "citeRegEx": "N\u00e9dellec et al\\.,? 2013", "shortCiteRegEx": "N\u00e9dellec et al\\.", "year": 2013}, {"title": "Overview of the pathway curation (pc) task of bionlp shared task", "author": ["Tomoko Ohta", "Sampo Pyysalo", "Rafal Rak", "Andrew Rowley", "Hong-Woo Chun", "Sung-Jae Jung", "Changhoo Jeong", "Sung-pil Choi", "Sophia Ananiadou"], "venue": null, "citeRegEx": "Ohta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ohta et al\\.", "year": 2013}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "CoRR, abs/1211.5063 .", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Event extraction across multiple levels of biological organization", "author": ["Sampo Pyysalo", "Tomoko Ohta", "Makoto Miwa", "HanCheol Cho", "Jun\u2019ichi Tsujii", "Sophia Ananiadou"], "venue": null, "citeRegEx": "Pyysalo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pyysalo et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Biomedical event extraction without training data", "author": ["Andreas Vlachos", "Paula Buttery", "Diarmuid O S\u00e9aghdha", "Ted Briscoe."], "venue": "Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared", "citeRegEx": "Vlachos et al\\.,? 2009", "shortCiteRegEx": "Vlachos et al\\.", "year": 2009}, {"title": "Biomedical event trigger detection based on convolutional neural network", "author": ["Jian Wang", "Honglei Li", "Yuan An", "Hongfei Lin", "Zhihao Yang."], "venue": "International Journal of Data Mining and Bioinformatics 15(3):195\u2013213.", "citeRegEx": "Wang et al\\.,? 2016a", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Biomedical event trigger detection by dependencybased word embedding", "author": ["Jian Wang", "Jianhai Zhang", "Yuan An", "Hongfei Lin", "Zhihao Yang", "Yijia Zhang", "Yuanyuan Sun."], "venue": "BMC Medical Genomics 9(2):45.", "citeRegEx": "Wang et al\\.,? 2016b", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A hybrid method to extract triggers in biomedical events", "author": ["Xiaomei Wei", "Qin Zhu", "Chen Lyu", "Kai Ren", "Bo Chen."], "venue": "Journal of Digital Information Management 13(4):299.", "citeRegEx": "Wei et al\\.,? 2015", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Event trigger identification for biomedical events extraction using domain knowledge", "author": ["Deyu Zhou", "Dayou Zhong", "Yulan He."], "venue": "Bioinformatics 30(11):1587\u20131594.", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Some of its applications include pathway curation (Ohta et al., 2013) and development of domain specific semantic search engine (Ananiadou et al.", "startOffset": 50, "endOffset": 69}, {"referenceID": 0, "context": ", 2013) and development of domain specific semantic search engine (Ananiadou et al., 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 6, "context": "So as to gain attraction among researchers many challenges such as BioNLP\u201909 (Kim et al., 2009), BioNLP\u201911 (Kim et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 7, "context": ", 2009), BioNLP\u201911 (Kim et al., 2011), BioNLP\u201913 (N\u00e9dellec et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 11, "context": ", 2011), BioNLP\u201913 (N\u00e9dellec et al., 2013) have been organized and many novel methods have also been proposed addressing these tasks.", "startOffset": 19, "endOffset": 42}, {"referenceID": 18, "context": "Analysis in multiple studies (Wang et al., 2016b; Zhou et al., 2014) reveal that more than 60% of event extraction errors are caused due to incorrect trigger identification.", "startOffset": 29, "endOffset": 68}, {"referenceID": 20, "context": "Analysis in multiple studies (Wang et al., 2016b; Zhou et al., 2014) reveal that more than 60% of event extraction errors are caused due to incorrect trigger identification.", "startOffset": 29, "endOffset": 68}, {"referenceID": 16, "context": "Rule based approaches use various strategies including pattern matching and regular expression to define rules (Vlachos et al., 2009).", "startOffset": 111, "endOffset": 133}, {"referenceID": 14, "context": "Machine learning based approaches treat the trigger identification problem as a word level classification problem, where many features from the data are extracted using various NLP toolkits (Pyysalo et al., 2012; Zhou et al., 2014) or learned automatically (Wang et al.", "startOffset": 190, "endOffset": 231}, {"referenceID": 20, "context": "Machine learning based approaches treat the trigger identification problem as a word level classification problem, where many features from the data are extracted using various NLP toolkits (Pyysalo et al., 2012; Zhou et al., 2014) or learned automatically (Wang et al.", "startOffset": 190, "endOffset": 231}, {"referenceID": 8, "context": "(2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 3, "context": "(2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al., 2011) to perform final classification.", "startOffset": 187, "endOffset": 211}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification. Zhou et al. (2014) proposed a novel framework for trigger identification where embedding features of the word combined with hand-crafted features are fed to SVM for final classification using multiple kernel learning.", "startOffset": 0, "endOffset": 209}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification. Zhou et al. (2014) proposed a novel framework for trigger identification where embedding features of the word combined with hand-crafted features are fed to SVM for final classification using multiple kernel learning. Wei et al. (2015) proposed a pipeline method on BioNLP\u201913 corpus based on Conditional Random Field (CRF) and Support vector machine (SVM) where the CRF is used to tag valid triggers and SVM is finally used to identify the trigger type.", "startOffset": 0, "endOffset": 426}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification. Zhou et al. (2014) proposed a novel framework for trigger identification where embedding features of the word combined with hand-crafted features are fed to SVM for final classification using multiple kernel learning. Wei et al. (2015) proposed a pipeline method on BioNLP\u201913 corpus based on Conditional Random Field (CRF) and Support vector machine (SVM) where the CRF is used to tag valid triggers and SVM is finally used to identify the trigger type. The above methods rely on various NLP toolkits to extract the hand-crafted features which leads to error propagation thus affecting the classifier\u2019s performance. These methods often need to tailor different features for different tasks, thus not making them generalizable. Most of the hand-crafted features are also traditionally sparse one-hot features vector which fail to capture the semantic information. Wang et al. (2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al.", "startOffset": 0, "endOffset": 1073}, {"referenceID": 3, "context": "(2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al., 2011) to perform final classification. Wang et al. (2016a) proposed another model based on convolutional neural network (CNN) where word and entity mention features of words within a window around the word are fed to a CNN to perform final classification.", "startOffset": 188, "endOffset": 265}, {"referenceID": 5, "context": "We use both LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al.", "startOffset": 17, "endOffset": 51}, {"referenceID": 2, "context": "We use both LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) variants of RNN in our ex-", "startOffset": 60, "endOffset": 80}, {"referenceID": 13, "context": "periments as they handle the vanishing and exploding gradient problem (Pascanu et al., 2012) in a better way.", "startOffset": 70, "endOffset": 92}, {"referenceID": 4, "context": "We use bidirectional version of RNN (Graves, 2013) where for every word forward RNN captures features from the past and the backward RNN captures features from future, inherently each word has information about whole sentence.", "startOffset": 36, "endOffset": 50}, {"referenceID": 15, "context": "The hidden state of the bidirectional RNN layer acts as sentence-level feature (g), the word and entity type embeddings (l) act as a word-level features, are both concatenated (1) and passed through a series of hidden layers (2), (3) with dropout (Srivastava et al., 2014) and an output layer.", "startOffset": 247, "endOffset": 272}, {"referenceID": 1, "context": "The implementation1 of the model is done in python language using Theano (Bergstra et al., 2010) library.", "startOffset": 73, "endOffset": 96}, {"referenceID": 9, "context": "(2013) using word2vec tool (Mikolov et al., 2013).", "startOffset": 27, "endOffset": 49}, {"referenceID": 1, "context": "The implementation1 of the model is done in python language using Theano (Bergstra et al., 2010) library. We use pre-trained word embeddings obtained by Moen et al. (2013) using word2vec tool (Mikolov et al.", "startOffset": 74, "endOffset": 172}, {"referenceID": 14, "context": "We use MLEE (Pyysalo et al., 2012) corpus for performing our trigger identification experiments.", "startOffset": 12, "endOffset": 34}, {"referenceID": 3, "context": "Here FFNN is a window based feed forward neural network where embedding features of words within the window are used to predict the trigger label (Collobert et al., 2011).", "startOffset": 146, "endOffset": 170}, {"referenceID": 13, "context": "Pyysalo et al. (2012) defined a SVM based classifier with hand-crafted features.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Pyysalo et al. (2012) defined a SVM based classifier with hand-crafted features. Zhou et al. (2014) also defined a SVM based classifier with word embeddings and hand-crafted features.", "startOffset": 0, "endOffset": 100}, {"referenceID": 13, "context": "Pyysalo et al. (2012) defined a SVM based classifier with hand-crafted features. Zhou et al. (2014) also defined a SVM based classifier with word embeddings and hand-crafted features. Wang et al. (2016a) defined window based CNN classifier.", "startOffset": 0, "endOffset": 204}, {"referenceID": 3, "context": "Here FFNN is a window based feed forward neural network where embedding features of words within the window are used to predict the trigger label (Collobert et al., 2011). We chose window size as 3 (one word from left and one word from right) after tuning it in validation set. CNN\u03c8 is our implementation of window based CNN classifier proposed by Wang et al. (2016a) due to unavailability of their code in public domain.", "startOffset": 147, "endOffset": 368}, {"referenceID": 14, "context": "SVM (Pyysalo et al., 2012) 81.", "startOffset": 4, "endOffset": 26}, {"referenceID": 20, "context": "SVM+We (Zhou et al., 2014) 80.", "startOffset": 7, "endOffset": 26}, {"referenceID": 17, "context": "82 CNN (Wang et al., 2016a) 80.", "startOffset": 7, "endOffset": 27}], "year": 2017, "abstractText": "Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex handcrafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-ofart F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.", "creator": "LaTeX with hyperref package"}}}