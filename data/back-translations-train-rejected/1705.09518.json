{"id": "1705.09518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "A Sampling Theory Perspective of Graph-based Semi-supervised Learning", "abstract": "Graph-based methods have been quite successful in solving unsupervised and semi-supervised learning problems, as they provide a means to capture the underlying geometry of the dataset. It is often desirable for the constructed graph to satisfy two properties: first, data points that are similar in the feature space should be strongly connected on the graph, and second, the class label information should vary smoothly with respect to the graph, where smoothness is measured using the spectral properties of the graph Laplacian matrix. Recent works have justified some of these smoothness conditions by showing that they are strongly linked to the semi-supervised smoothness assumption and its variants. In this work, we reinforce this connection by viewing the problem from a graph sampling theoretic perspective, where class indicator functions are treated as bandlimited graph signals (in the eigenvector basis of the graph Laplacian) and label prediction as a bandlimited reconstruction problem. Our approach involves analyzing the bandwidth of class indicator signals generated from statistical data models with separable and nonseparable classes. These models are quite general and mimic the nature of most real-world datasets. Our results show that in the asymptotic limit, the bandwidth of any class indicator is also closely related to the geometry of the dataset. This allows one to theoretically justify the assumption of bandlimitedness of class indicator signals, thereby providing a sampling theoretic interpretation of graph-based semi-supervised classification.", "histories": [["v1", "Fri, 26 May 2017 10:39:08 GMT  (310kb,D)", "http://arxiv.org/abs/1705.09518v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aamir anis", "aly el gamal", "salman avestimehr", "antonio ortega"], "accepted": false, "id": "1705.09518"}, "pdf": {"name": "1705.09518.pdf", "metadata": {"source": "CRF", "title": "A Sampling Theory Perspective of Graph-based Semi-supervised Learning", "authors": ["Aamir Anis", "Aly El Gamal"], "emails": ["aanis@usc.edu,", "avestimehr@ee.usc.edu,", "ortega@sipi.usc.edu).", "elgamala@purdue.edu)."], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Data models", "text": "1) The separable model: In this model, we assume that the dataset consists of a pool of n random, d-dimensional characteristic vectors X = {X1, X2,.. Xn} regardless of a certain probability density function p (x) based on Rd (however, this is assumed for the sake of simplicity, the analysis can be extended to any variety, but it would be more technically involved). To simplify our analysis, we also assume that p (x) is limited from above, Lipschitz continuously and doubly differentiable. We assume that a smooth hypersurface model S, with radius of curvature lower limit by a constant Rd in two disjoint classes S and Sc, with indicator functions 1S: Rd \u2192 {0} and 1Sc (x): Rd \u2192 we {0} and 1Sc (x): Rd \u2192 0,}, {1} This is illustrated in Figure 1a."}, {"heading": "B. Graph construction", "text": "Using the n feature vectors, we construct an undirected distance-based similarity graph in which the data points and edge weights are proportional to their similarity given by the Gaussian core: wij = K\u03c32 (Xi, Xj) = 1 (2\u03c0\u03c32) d / 2 e \u2212 \u0441Xi \u2212 Xj \u0445 2 / 2\u03c32, (3), where \u03c3 is the variance (bandwidth) of the Gaussian core. Furthermore, we proceed from wii = 0, i.e. the graph has no self-loops. The adjacent matrix of the graph W is an n \u00b7 n symmetric matrix with elements wij, while the degree matrix is a diagonal with elements Dii = \u2211 j wij. We define the graph laplacically as L = 1n (D \u2212 W). The normalization by n ensures that the norm of L is stochastically bounded as n grows, because the graph is not equal \u00b7 L."}, {"heading": "C. Bandwidth of graph signals", "text": "The bandwidth \u03c9 (f) of any signal f on the graph is defined as the largest eigenvalue for which the projection of the signal onto the corresponding eigenvector is not zero, i.e., \u03c9 (f) = max i {\u043c | uTi f > 0}. (4) Ideally, determining the bandwidth \u03c9 (f) of a graph signal f requires the calculation of eigenvectors {ui} and the corresponding projections f \u00b2 i = uTi f. However, analyzing convergence 4 (a) (b) Figure 1: Statistical data models taken into account in this work: (a) The divisible model, (b) the inseparable model. Darker shadows indicate regions of higher density. These coefficients are technically sophisticated, so we resort to the following estimate of bandwidth [12]: 1 bandwidth f = (fTLmffT f) f = (5), where we call convergence."}, {"heading": "D. Bandlimited interpolation for classification", "text": "Bandwidth plays an important role in the scanning theory approach to semi-supervised learning. In this approach, we find a label mapping by minimizing the error over the known sentence and at the same time ensuring that the resulting class indicator vector is band-limited via the similarity graph, i.e., min f (L) \u2212 y (L) \u2212 2 is subject \u03c9 (f) < \u03c9L (7), where L denotes the set of known labels, y denotes the true class names, and f (L) and y (L) denotes the values of f and y on the sentence L. The constraint limits the hypotheses space by limiting it to a set of band-limited signals whose bandwidth is smaller than \u03c9L, which is equivalent to enforcing the smoothness of the labels over the chart. We can also use the sampling theory to limit the hypotheses space associated with the sample frequency by limiting it to a set of band-limited signals whose bandwidth is smaller than the bandwidth of the L, which is the enforcement of the bandwidth of the L."}, {"heading": "III. RELATED WORK AND CONNECTIONS", "text": "Existing graph-based, semi-supervised learning and spectral cluster methods were justified by analyzing the convergence of graph-based smoothing measurements (such as the graph section and laplac regularizer) for different graph construction programs in two different settings - classification and regression. The classification setting assumes that labels indicate class affiliations and are discrete, typically with 1 / 0 values. Note that both divisible and non-divisible data models taken into account in our work are in the classification setting. On the other hand, in the regression setting, one allows the class identification signal f to be smooth and continuous with soft values, i.e., f-Rn and later applies some threshold mechanisms to derive class affiliations. For example, in the two-class problem + 1 \u2212 1, one can assign the two classes and assign the quantification thresholds to these differences in the conformity of the conformity of these values."}, {"heading": "A. Classification setting", "text": "If m = 1, the bandwidth estimate \u03c9m (1S) for the divisible model in our 5work reduces (within a scaling factor) to the empirical graph intersection for the partitions S and Sc of the attribute space, i.e. intersection (S, Sc) for the partitioned side intersection (S, Sc) for the partitioned side intersection (S, Sc) for the partitioned side intersection. It was shown in [18] that the intersection formed by a hyperplane S in Rd converges with some scaling under rate conditions. The convergence of this quantity was previously investigated in the context of spectral clustering, where attempts were made to minimize it across the two partitions of the nodes."}, {"heading": "B. Regression setting", "text": "To predict the results of unknown samples in the regression setting, one generally minimizes the graph Laplacian Regularizer fTLf under the known label constraints [4]: min ffTLf so that f (L) = y (L), (10) A certain convergence result in this setting assumes that n data points in the asymptotic limit are recorded under the conditions of p (x) and np (n). [15] The graph fTLf can be shown that n data points in the asymptotic limit are recorded under the conditions of p (x)."}, {"heading": "IV. MAIN RESULTS AND DISCUSSION", "text": "A. The interpretation of bandwidth and bandwidth in the graph does not break. Finally, condition 3 leads to an improvement in the stated values \u2192 The bandwidth estimates of the class indicator signals, via the distance-based similarity curve previously described. < < / p > The bandwidth estimates of the class boundaries for both data models. This convergence is achieved under the following asymptotic regime: 1) The increasing convergence of the data sets is required for the stochastic convergence of the bandwidth estimates. 2) Condition 2 ensures that the limits are precise and have a simple interpretation of the data geometry. Intuitively, condition 2 causes the number of data points to increase, looking at a smaller neighborhood around each data point, as a result that the degree of amendment in the graph does not break."}, {"heading": "B. Label complexity of SSL", "text": "In the context of semi-supervised learning, we define label complexity as the minimum fraction of the marked examples required to accurately predict the labels of the unmarked data points. This quantity is essentially an indicator of how \"good\" the semi-supervised problem is, that is, how much help we get from the geometry as we learn. Low label complexity is indicative of a favorable situation in which one can learn from only a few known labels by exploiting data. In the following discussion, we first estimate the theoretical complexity of the data models we are looking at, and then show that the expected label complexity of the sampling theoretical approach to learning corresponds precisely to these values in asymptotic complexity: An easy way to calculate the label complexity of the data models we are looking at is to find the fraxity of the points belonging to a region."}, {"heading": "V. PROOFS", "text": "We now present the proofs of theorems 1 and 2 by a sequence of Lemmatas1. The main idea is to perform a variance biases decomposition of the bandwidth estimate and then prove the convergence of each term independently of each other. Specifically, one can consider the random variable for the indicator vector 1R-0, 1} n of any region R-Rn: (\u03c9m (1R) m = 1TRL m1R 1TR1R = 1 n1 T RL m1R 1 n1 T R1R. (25) We examine the convergence of this magnitude by looking at the counter and the denominator separately (it is easy to show that the fraction converges when both the counter and the denominator 1A partial sketch of the evidence for the divisible model is also provided in our parallel work [19]; here we provide the complete proof of convergence. Convergence) By the strong law of the large numbers, we conclude the following invergence for the invergence 1 and the invergence for the IR."}, {"heading": "A. Convergence of variance terms", "text": "Let us compile V = 1n1 T RL m1R, then we have the following concentration result: Lemma 1 (concentration) = 29. For each > 0 we have: Pr (| V \u2212 E {V} | >) \u2264 2 exp (\u2212 n / (m + 1)] \u03c3md 22CmE {V} + 23 | Cm \u2212 \u03c3mdE {V} |), (27) where C = 2 / (2\u03c0) d / 2.Proof. Referring to the fact that wi, j = K\u03c32 (Xi, Xj), we start with the explicit extension V = 1n1 T R (D \u2212 W) m1R into the following sum V = 1nm + 1, i2, in + 1 g (Xi1, Xi2,...., Xim + 1). (28) The above expansion has the form of a V statistic. Details on how to explicitly write the summation can be found in Appendix A., which is g, which is compile from a product we do not compose 2m."}, {"heading": "B. Convergence of the bias term for the separable model", "text": "To evaluate the convergence of the bias terms, we need the following properties of the d-dimensional Gaussian component: Lemma 2. If p (x) is doubly differentiable, then we have p (x, y) p (y) dy = p (x) + O (2). (33) Proof: With the substitution y = x + t follows a Taylor extension over x, then we have p (x, y) p (y) dy (x) dy = p (2) d / 2 p \u00b2) p (2) p \u00b2) p (2) p \u00b2) p (2) p \u00b2 2 (2) p) p (2) p (2) p (2) p (2) p (p) p (2) p (2) p (2) p (2) p (p) \u2212 d (2) p (p) p (2) p (2) p (p) p (2) p (2) p (p) \u2212 d (2) p (p) p (2) p (2) p (2) p (p) p (2) p (2) p \u2212 d."}, {"heading": "C. Convergence of bias term for the nonseparable model", "text": "For the inseparable model, we have to prove the convergence of E {1 n1 T AL m1A}. This is illustrated in the following example: Lemma 6. As n \u2192 \u221e, \u03c3 \u2192 0 so that m / n \u2192 0, m\u03c32 \u2192 0, we have E {1n 1TAL m1A} \u2192 \u03b1AcpA (x) pAc (x) p m \u2212 1 (x) dx. (43) Proof: E {1 n1 T AL m1A} term by extending Lm as (D \u2212 W) m \u2212 1 (D \u2212 W). Using (33) repeatedly we have for the first two terms of expansion: E {1n 1TAD..... D (D \u2212 W) 1A} = En (1A) AcpA (x) pAc (x) p \u2212 dm \u2212 1 (x) dproem (n) (44), we have."}, {"heading": "D. Proof of Theorem 4", "text": "We start by remembering the definition of the empirical spectral distribution (ESD) (then)."}, {"heading": "VI. NUMERICAL VALIDATION", "text": "We present simple numerical analyses to validate our results and demonstrate their usefulness in practice. To simulate the separable model, we first consider a data distribution based on a 2D Gaussian mixing model (GMM) with two Gaussians each: \u00b51 = [\u2212 1], \u04321 = 0.25I and \u00b52 = [1], \u04212 = 0.16I, and the mixing of proportions \u03b11 = 0.4 and \u03b12 = 0.6. The probability density function is shown in Figure 3. Next, we evaluate the claim of theorem 1 on five boundaries, described in Table II. These boundaries are shown in Figure 4 and are illustrative of typical assumptions of separation such as linear or non-linear or high density. For the simulation of the non-separable model, we first construct the following smooth (double differentiable) 2D probability density (x, y) = {3."}, {"heading": "VII. DISCUSSIONS AND FUTURE WORK", "text": "Our work analyzed the range of class indicator signals in relation to the Laplacian eigenvector base and revealed their connection with the underlying geometry of the dataset. In particular, our results showed that an estimate of the range of class indicators corresponds to the upper limit of probability density on the class boundaries for the divisible model and on the intersection regions for the inseparable model. This quantifies the link between the assumptions of smoothness (in relation to the range of class indicators) and the low density of the graph, since boundaries running through regions of low data density occur as an inseparable graph."}, {"heading": "APPENDIX A", "text": "Explanation of 1TSL m1S AND E {1 n1 T SL m1S} To expand 1TSL m1S with respect to the elements wij of W, we first write the expression for each product term. Since Lm = 1nm (D \u2212 W) m, there are 2m such terms. Let us first introduce the following notation: [D, W] m denotes a product term that contains the matrices D and W, so that there are m matrices in the product. Note that Lm is essentially a sum of all possible [D, W] m with corresponding characters. Now, the explicit expression for 1TS [D, W] m1S can be obtained using the following procedure: 1) All product terms are defined by the following template: 1TS [D, W] m1S = i1,..., with respect to the product i1S."}, {"heading": "APPENDIX B PROOF OF LEMMA 4", "text": "The key component required for the evaluation of integrals in Lemma 4 involves the selection of a radius R (< \u03c4) as a function of the two following properties: 0: 1) R \u2192 0, 2) R / 3) R / 3) R2 / 3 (0, 4) R / 3 (0, 4) R / 3 (0, 4) R / 3 (0, 4) R / 3 (0, 4) R / 3 (0, 4) R / 3 (0, 4) R / 3 (0, 4) R / 4 (0, 4) R / 4) 0, where R: 1) R2 (0, 2) R2 (64) R2 (1 / 4) R2) R2 (1) R2) R2 (1) R2)."}], "references": [{"title": "Supervised Learning (Adaptive Computation and Machine Learning)", "author": ["Olivier Chapelle", "Bernhard Sch\u00f6lkopf", "Alexander Zien. Semi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning with local and global consistency", "author": ["Denny Zhou", "Olivier Bousquet", "Thomas N. Lal", "Jason Weston", "Bernhard Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "On the relation between low density separation, spectral clustering and graph cuts", "author": ["Hariharan Narayanan", "Mikhail Belkin", "Partha Niyogi"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty"], "venue": "In IN ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Semi-supervised learning by higher order regularization", "author": ["Xueyuan Zhou", "Mikhail Belkin"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Using manifold stucture for partially labeled classification", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Semi-supervised learning on riemannian manifolds", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Towards a sampling theorem for signals on arbitrary graphs", "author": ["A Anis", "A Gadde", "A Ortega"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sampling large data on graphs", "author": ["H. Shomorony", "A.S. Avestimehr"], "venue": "In Signal and Information Processing (GlobalSIP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Discrete signal processing on graphs: Sampling theory", "author": ["S. Chen", "R. Varma", "A. Sandryhaila", "J. Kova\u010devi\u0107"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Efficient sampling set selection for bandlimited graph signals using graph spectral proxies", "author": ["A. Anis", "A. Gadde", "A. Ortega"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Active semisupervised learning using sampling theory for graph signals", "author": ["Akshay Gadde", "Aamir Anis", "Antonio Ortega"], "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Measure based regularization", "author": ["Olivier Bousquet", "Olivier Chapelle", "Matthias Hein"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Uniform Convergence of Adaptive Graph-Based Regularization, pages 50\u201364", "author": ["Matthias Hein"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Towards a theoretical foundation for laplacian-based manifold methods", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "An iterated graph laplacian approach for ranking on manifolds", "author": ["Xueyuan Zhou", "Mikhail Belkin", "Nathan Srebro"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "How the result of graph clustering methods depends on the construction of the graph", "author": ["Markus Maier", "Ulrike von Luxburg", "Matthias Hein"], "venue": "ESAIM: Probability and Statistics, 17:370\u2013418,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Asymptotic justification of bandlimited interpolation of graph signals for semi-supervised learning", "author": ["A Anis", "A El Gamal", "S Avestimehr", "A Ortega"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Signal processing techniques for interpolation in graph structured data", "author": ["S.K. Narang", "A Gadde", "A Ortega"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Localized iterative methods for interpolation in graph structured data", "author": ["S.K. Narang", "A Gadde", "E. Sanou", "A Ortega"], "venue": "In Global Conference on Signal and Information Processing (GlobalSIP),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Asymptotic behavior of `p-based laplacian regularization in semi-supervised learning", "author": ["Ahmed El Alaoui"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Geometrical aspects of statistical learning theory", "author": ["Matthias Hein"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1963}], "referenceMentions": [{"referenceID": 0, "context": "assumption [1], which states that the label function is smoother in regions of high data density.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "There also exist other similar variants of this assumption specialized for the classification setting, namely, the cluster assumption [2] (points in a cluster are likely to have the same class label) or the low density separation assumption [3] (decision boundaries pass through regions of low data density).", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "There also exist other similar variants of this assumption specialized for the classification setting, namely, the cluster assumption [2] (points in a cluster are likely to have the same class label) or the low density separation assumption [3] (decision boundaries pass through regions of low data density).", "startOffset": 241, "endOffset": 244}, {"referenceID": 1, "context": "For example, a commonly used measure of smoothness for a label function f is the graph Laplacian regularizer fLf (L being the graph Laplacian), and many algorithms involve minimizing this quadratic energy function while ensuring that f satisfies the known set of labels [2], [4].", "startOffset": 270, "endOffset": 273}, {"referenceID": 3, "context": "For example, a commonly used measure of smoothness for a label function f is the graph Laplacian regularizer fLf (L being the graph Laplacian), and many algorithms involve minimizing this quadratic energy function while ensuring that f satisfies the known set of labels [2], [4].", "startOffset": 275, "endOffset": 278}, {"referenceID": 4, "context": "There also exist higher-order variants of this measure known as iterated graph Laplacian regularizers fLf , that have been shown to make the problem more well-behaved [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "On the other hand, a spectral theory based classification algorithm restricts f to be spanned by the first few eigenvectors of the graph Laplacian [6], [7], that are known to form a representation basis for smooth functions on the graph.", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "On the other hand, a spectral theory based classification algorithm restricts f to be spanned by the first few eigenvectors of the graph Laplacian [6], [7], that are known to form a representation basis for smooth functions on the graph.", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "A more recent approach, derived from the emerging field of Graph Signal Processing (GSP) [8], considers the semisupervised learning problem from the perspective of sampling theory for graph signals [9]\u2013[12].", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "A more recent approach, derived from the emerging field of Graph Signal Processing (GSP) [8], considers the semisupervised learning problem from the perspective of sampling theory for graph signals [9]\u2013[12].", "startOffset": 198, "endOffset": 201}, {"referenceID": 11, "context": "A more recent approach, derived from the emerging field of Graph Signal Processing (GSP) [8], considers the semisupervised learning problem from the perspective of sampling theory for graph signals [9]\u2013[12].", "startOffset": 202, "endOffset": 206}, {"referenceID": 12, "context": "This can also be carried out without explicitly computing the eigenvectors of the Laplacian, and has been shown to be quite competitive in comparison to state-of-the-art graph-based semi-supervised learning methods [13].", "startOffset": 215, "endOffset": 219}, {"referenceID": 4, "context": ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]\u2013[17].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]\u2013[17].", "startOffset": 233, "endOffset": 236}, {"referenceID": 13, "context": ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]\u2013[17].", "startOffset": 238, "endOffset": 242}, {"referenceID": 16, "context": ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]\u2013[17].", "startOffset": 243, "endOffset": 247}, {"referenceID": 2, "context": "If points drawn from a smooth distrbution are separated by a smooth boundary into two classes, then the graph cut for the partition converges to a weighted volume of the boundary [3], [18].", "startOffset": 179, "endOffset": 182}, {"referenceID": 17, "context": "If points drawn from a smooth distrbution are separated by a smooth boundary into two classes, then the graph cut for the partition converges to a weighted volume of the boundary [3], [18].", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "enables us to theoretically assert that for the sampling theoretic approach to graph-based semi-supervised learning, the label complexity (minimum fraction of labeled points required) of learning classifiers matches the theoretical estimate and is indeed lower if the boundary lies in regions of low data density, as demonstrated empirically in earlier works [6], [7].", "startOffset": 359, "endOffset": 362}, {"referenceID": 6, "context": "enables us to theoretically assert that for the sampling theoretic approach to graph-based semi-supervised learning, the label complexity (minimum fraction of labeled points required) of learning classifiers matches the theoretical estimate and is indeed lower if the boundary lies in regions of low data density, as demonstrated empirically in earlier works [6], [7].", "startOffset": 364, "endOffset": 367}, {"referenceID": 18, "context": "It is worth noting that the bandwidth convergence result for the separable model and an interpretation of bandlimited reconstruction were given in our preliminary work [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "It is known that for a larger eigenvalue \u03bb, the corresponding eigenvector u exhibits greater variation when plotted over the nodes of the graph [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 11, "context": "Therefore, we resort to the following estimate of the bandwidth [12]:", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "One can also use sampling theory to set \u03c9L as the cutoff frequency associated with the sampling set [12], as considered in [13], [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "One can also use sampling theory to set \u03c9L as the cutoff frequency associated with the sampling set [12], as considered in [13], [20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 19, "context": "One can also use sampling theory to set \u03c9L as the cutoff frequency associated with the sampling set [12], as considered in [13], [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 5, "context": "Note that the bandwidth-based approach for semisupervised learning extends the Fourier eigenvector approach suggested in [6], [7] by allowing the estimation of the complexity of the bandlimited space via \u03c9L.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "Note that the bandwidth-based approach for semisupervised learning extends the Fourier eigenvector approach suggested in [6], [7] by allowing the estimation of the complexity of the bandlimited space via \u03c9L.", "startOffset": 126, "endOffset": 129}, {"referenceID": 20, "context": "It is based on iterative and alternate projections onto convex sets and can be implemented in an efficient manner via graph filtering operations [21].", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "[18] also studies convergence of graph cuts for weighted k-nearest neighbor and r-neighborhood graphs which we do not include for brevity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Narayanan et al [3] p(x) supported on manifold M \u2282 Rd, separated into S and Sc by smooth hypersurface \u2202S Normalized Gaussian weights w\u2032 ij = wij \u221a didj 1 n\u03c3 1SL1S n\u2192\u221e, \u03c3 \u2192 0 \u222b \u2202S p(s)ds", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "Maier et al [18] p(x) supported onM\u2282 Rd, separated into S and Sc by hyperplane \u2202S r-neighborhood, unweighted 1 nrd+1 1SL1S n\u2192\u221e, r \u2192 0 \u222b \u2202S p 2(s)ds", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "k-nn, unweighted, t = (k/n)1/d 1 ntd+1 1SL1S n\u2192\u221e, t\u2192 0 \u222b \u2202S p 1\u22121/d(s)ds fully-connected, Gaussian weights 1 n\u03c3 1SL1S n\u2192\u221e, \u03c3 \u2192 0 \u222b \u2202S p 2(s)ds Bousquet et al [14], Hein [15] p(x) and f(x) supported on Rd fully-connected, weights wij = 1 n\u03c3d K ( \u2016Xi\u2212Xj\u2016 \u03c32 ) , where K(.", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "k-nn, unweighted, t = (k/n)1/d 1 ntd+1 1SL1S n\u2192\u221e, t\u2192 0 \u222b \u2202S p 1\u22121/d(s)ds fully-connected, Gaussian weights 1 n\u03c3 1SL1S n\u2192\u221e, \u03c3 \u2192 0 \u222b \u2202S p 2(s)ds Bousquet et al [14], Hein [15] p(x) and f(x) supported on Rd fully-connected, weights wij = 1 n\u03c3d K ( \u2016Xi\u2212Xj\u2016 \u03c32 ) , where K(.", "startOffset": 169, "endOffset": 173}, {"referenceID": 4, "context": "Zhou et al [5] Uniformly distributed on d-dim.", "startOffset": 11, "endOffset": 14}, {"referenceID": 21, "context": "El Alaoui et al [22] p(x) supported on [0, 1]d fully-connected, weights wij = K ( \u2016Xi\u2212Xj\u2016 \u03c3 ) , where K(.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "El Alaoui et al [22] p(x) supported on [0, 1]d fully-connected, weights wij = K ( \u2016Xi\u2212Xj\u2016 \u03c3 ) , where K(.", "startOffset": 39, "endOffset": 45}, {"referenceID": 17, "context": "It has been shown in [18] that the cut formed by a hyperplane \u2202S in R converges with some scaling under the rate conditions \u03c3 \u2192 0 and n\u03c3 \u2192\u221e as", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "A similar result for a similarity graph constructed with normalized weights w\u2032 ij = wij/ \u221a didj was shown earlier for an arbitrary hypersurface \u2202S in [3], where di denotes the degree of node i.", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "The results in [3], [18] aim to provide an interpretation for spectral clustering \u2013 up to some scaling, the empirical cut value converges to a weighted volume of the boundary.", "startOffset": 15, "endOffset": 18}, {"referenceID": 17, "context": "The results in [3], [18] aim to provide an interpretation for spectral clustering \u2013 up to some scaling, the empirical cut value converges to a weighted volume of the boundary.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "Regression setting To predict the labels of unknown samples in the regression setting, one generally minimizes the graph Laplacian regularizer fLf subject to the known label constraints [4]:", "startOffset": 186, "endOffset": 189}, {"referenceID": 13, "context": "Laplacian regularizer fLf can be shown to converge in the asymptotic limit under the conditions \u03c3 \u2192 0 and n\u03c3 \u2192 \u221e as in [14], [15]: 1 n\u03c32 fLf p.", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "Laplacian regularizer fLf can be shown to converge in the asymptotic limit under the conditions \u03c3 \u2192 0 and n\u03c3 \u2192 \u221e as in [14], [15]: 1 n\u03c32 fLf p.", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "The work of [15], [23] also generalizes the result for arbitrary kernel functions used in defining graph weights, and data distributions defined over arbitrary manifolds in R.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "The work of [15], [23] also generalizes the result for arbitrary kernel functions used in defining graph weights, and data distributions defined over arbitrary manifolds in R.", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "Similar convergence results have also been derived for the higher-order Laplacian regularizer fLf obtained from uniformly distributed data [5].", "startOffset": 139, "endOffset": 142}, {"referenceID": 15, "context": "Extensions for non-uniform probability distributions p(x) over the manifold can be obtained using the weighted Laplace-Beltrami operator [16], [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Extensions for non-uniform probability distributions p(x) over the manifold can be obtained using the weighted Laplace-Beltrami operator [16], [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "More recently, an `p-based Laplacian regularization has been proposed for imposing smoothness constraints in semisupervised learning problems [22].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "It has been shown for a bounded density p(x) defined on [0, 1] that for every p \u2265 2, as n\u2192\u221e, \u03c3 \u2192 0, 1 n2\u03c3p+d Jp(f) p.", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "[0,1]d \u2016\u2207f(x)\u2016p(x)dx.", "startOffset": 0, "endOffset": 5}, {"referenceID": 11, "context": "As opposed to other smoothness measures considered earlier, analyzing the bandwidth allows us to interpret graph-based semi-supervised learning using the sampling theorem [12] and provide a quantitative insight into label complexity based on data geometry.", "startOffset": 171, "endOffset": 175}, {"referenceID": 17, "context": "As a special case of our analysis, we also get a convergence result for the graph cut in the nonseparable model analogous to the results of [18] for the separable model.", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "It is known that the fraction of samples required for perfectly reconstructing a bandlimited signal cannot be more than the fraction of eigenvalues of the Laplacian below the signal\u2019s bandwidth [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 18, "context": "1A partial sketch of the proof for the separable model is also provided in our parallel work [19]; here we provide the complete proof.", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "In order to apply a concentration inequality for V, we first re-write it in the form of a U-statistic by regrouping terms in the summation in order to remove repeated indices, as given in [24]:", "startOffset": 188, "endOffset": 192}, {"referenceID": 22, "context": "(32) Finally, plugging in the bound and variance of g\u2217 in Bernstein\u2019s inequality for U-statistics as stated in [23], [24], we arrive at the desired result of (27).", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "(32) Finally, plugging in the bound and variance of g\u2217 in Bernstein\u2019s inequality for U-statistics as stated in [23], [24], we arrive at the desired result of (27).", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "However, in this case, we encounter several terms of the form p(\u03b8x+(1\u2212\u03b8)y) for some \u03b8 \u2208 [0, 1].", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": "25I and \u03bc2 = [1 0],\u03a32 = 0.", "startOffset": 13, "endOffset": 18}, {"referenceID": 11, "context": "This method ensures that the obtained labeled set guarantees perfect recovery for signals spanned by the first B eigenvectors of L [12].", "startOffset": 131, "endOffset": 135}], "year": 2017, "abstractText": "Graph-based methods have been quite successful in solving unsupervised and semi-supervised learning problems, as they provide a means to capture the underlying geometry of the dataset. It is often desirable for the constructed graph to satisfy two properties: first, data points that are similar in the feature space should be strongly connected on the graph, and second, the class label information should vary smoothly with respect to the graph, where smoothness is measured using the spectral properties of the graph Laplacian matrix. Recent works have justified some of these smoothness conditions by showing that they are strongly linked to the semi-supervised smoothness assumption and its variants. In this work, we reinforce this connection by viewing the problem from a graph sampling theoretic perspective, where class indicator functions are treated as bandlimited graph signals (in the eigenvector basis of the graph Laplacian) and label prediction as a bandlimited reconstruction problem. Our approach involves analyzing the bandwidth of class indicator signals generated from statistical data models with separable and nonseparable classes. These models are quite general and mimic the nature of most real-world datasets. Our results show that in the asymptotic limit, the bandwidth of any class indicator is also closely related to the geometry of the dataset. This allows one to theoretically justify the assumption of bandlimitedness of class indicator signals, thereby providing a sampling theoretic interpretation of graph-based semi-supervised classification.", "creator": "LaTeX with hyperref package"}}}