{"id": "1705.09724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Semi-Supervised Model Training for Unbounded Conversational Speech Recognition", "abstract": "For conversational large-vocabulary continuous speech recognition (LVCSR) tasks, up to about two thousand hours of audio is commonly used to train state of the art models. Collection of labeled conversational audio however, is prohibitively expensive, laborious and error-prone. Furthermore, academic corpora like Fisher English (2004) or Switchboard (1992) are inadequate to train models with sufficient accuracy in the unbounded space of conversational speech. These corpora are also timeworn due to dated acoustic telephony features and the rapid advancement of colloquial vocabulary and idiomatic speech over the last decades. Utilizing the colossal scale of our unlabeled telephony dataset, we propose a technique to construct a modern, high quality conversational speech training corpus on the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and language model training. We describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test corpus of 7K manually transcribed utterances. We show relative word error rate (WER) reductions of {35%, 19%} on {agent, caller} utterances over our seed model and 5% absolute WER improvements over IBM Watson STT on this conversational speech task.", "histories": [["v1", "Fri, 26 May 2017 21:10:15 GMT  (1403kb,D)", "http://arxiv.org/abs/1705.09724v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shane walker", "morten pedersen", "iroro orife", "jason flaks"], "accepted": false, "id": "1705.09724"}, "pdf": {"name": "1705.09724.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Model Training for Unbounded Conversational Speech Recognition", "authors": ["Shane Walker", "Morten Pedersen", "Jason Flaks"], "emails": [], "sections": [{"heading": null, "text": "Based on the colossal size of our unlabeled telephone data set, we propose a technique to construct a modern, high-quality speech training corpus in the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and speech model training. We describe data collection, selection, and training, and evaluate the results of our updated speech recognition system using a test corpus of 7K manually transcribed utterances. We show relative word error rates (WHO) of {35%, 19%} for {agent, caller} utterances compared to our seed model, and 5% absolute WHO improvements over IBM Watson STT in this conversational task. Index terms: speech recognition, acoustic modeling, speech modeling, large unattended training sets, data selection, data augmentation."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2 Conversational ASR", "text": "Automatic Voice Recognition (ASR) of spontaneous conversations is a different and more complex task than ASR for Voice Command or Voice Search applications run by modern digital assistants. In addition to the usual challenges in LVCSR (e.g. speaker independence, coarticulation, variable speech rates, soundproofing, and LM capacity), other factors come into play in natural, unwritten conversations. These include disfluids such as hesitation in the middle of the sentence, stutterers, ungrammatic or full pauses (uh, uh, uh), back channels (yes, mhm, uhhuh), discourse markers (such as, you know), self-perpetuating terms (or rather, I mean), truncated phrases, repetitions, definitive lengthening of syllables, coughs, and laughs [14]. In an article dealing with the linguistic appropriations and interpretations of Chomsky's Colorless Green, Manfred's Ideational Communication principle is fully engaged."}, {"heading": "3 Building ASR Training Corpora", "text": "There are many approaches to building speech training sets, including acoustic data disruption and data synthesis. [11] Our literature survey is limited to unattended and semi-supervised approaches to corporate assembly.Google uses its broad bandwidth in constructing a training set for its Voice Search and Voice Input tasks for resource-poor languages such as Brazil, Italy, Russia, and France. Their unsupervised approach uses a slow but precise decoder, trust, transcript length, and transcripts of shallow heuristics to select the utterances for acoustic modeling. In conjunction with owner-uploaded transcripts, Youtube applies heuristics to generate additional semi-supervised training data for the deep neural network (DNN)."}, {"heading": "4 Speech Recognition System", "text": "The ASR seeds are based on an online decoder written with Kaldi. (The ASR seeds are based on a free, open source C + + toolkit for speech recognition research.) The seed decoders are able to identify with others. (The seed decoders use a \"prefabricated\" deep neural network and \"hidden\" model, which is then converted into \"scaled similarities\" for the states of an HMM.) In this hybrid model, a DNN is switched via minibatch asynchronous asynchronous stochastic descent to emit HMM. These are then converted into \"scaled similarities\" for the states of an HMM. Unlike the Gaussian Mixture Models (GMM), it is traditionally used in speech recognition, DNN models are superordinated, which are better classified."}, {"heading": "5 Training System Description", "text": "First, we introduce a brand new language corpus of North American English, and then describe in detail our semi-supervised training and data selection methods."}, {"heading": "5.1 The Marchex US English Corpus", "text": "Marchex's phone and voice analytics business safely records more than a million calls per business day or decades of encrypted audio recordings per week. These are conversations between consumers and businesses that are conducted via a modern mix of mobile phones on different phone networks or landlines and capture everyday North American life in all possible accents, fluent in English, under general environmental or noise conditions, and with a comprehensive, colloquial vocabulary. Speaker demographics range from teenagers to octogenarians. Exemplary conversations can be sales-related, such as calls to book a hotel, purchase a mobile phone, cable service, or renegotiate insurance rates. Other examples include service-related, such as making appointments for a dentist appointment, changing an oil, car repairs, or a moving service. The average call lasts four minutes. Both the caller and the answering channels are recorded. This non-scripted culture of phone calls and chats is topical."}, {"heading": "5.2 Data Collection and Processing", "text": "In fact, most people are able to decide for themselves what they want and what they don't want."}, {"heading": "5.3 Retraining The ASR Model", "text": "After successfully preparing the corrected, filtered automatic transcripts, it is time to retrain our ASR model.For retraining, we select the cleanest 5,000 hours, or 11.7 million utterances. This number has been selected to have minimal impact on the hyperparameters of the training recipe, paying attention to the maximum data capacity of the training model.For AM training, we insert 13K utterances from our manually transcribed set of 20K into the automatic, cleaned corpus.Since our manually transcribed utterances are the most accurate, these 13K are used for the initial monophonic and three-voiced steps of the training recipe. The remaining 7K utterances are excluded as a test set. Given the relative data increase compared to the seed model, we use a larger version of the online nnet2 recipe described in Section 4 with multiple splices, this recipe uses a total of six additional fully connected ones."}, {"heading": "6 Experimental Results", "text": "We calculate WHO values for the seed model, the IBM Watson Speech to Text Service [23] [24], and our updated production model. Our updated model performs strongly against IBM and demonstrates our ability to generalize well on an invisible dataset with a model trained on a mix of manually transcribed and automated transcriptions. While IBM has not trained its models in Marchex English, its results are valid benchmarks based on published results on the Hub5 2000 Evaluation Conversation Task, a corpus of 40 test phone calls from the SWBD and the CALLHOME Group [23]. We compare the shares of Hub5 2000 to the size of our test set of 7K utterances, or 4.5 hours without filler, manually transcribed, audio from the WHO 1 model that includes more than 3,000 WHO models."}, {"heading": "6.1 Comparing conversational corpora", "text": "In order to better understand our performance in relation to the IBM models trained on Fisher English (FE) or Switchboard (SWBD), we are now examining how the Marchex English (ME) differs. ME refers only to this first, post-seed iteration of 11.7M utterances or 5,000 hours used to train an updated model. During the capture of FE, topics were pre-assigned or worked out between contributors [4]. For SWBD, a conversation topic was proposed [7]. ME, on the other hand, captures real conversations with the full degree of naturalness. FE also excludes greetings and approvals that we consider essential to evaluate the naturalness of conversations on a 5-point scale."}, {"heading": "7 Conclusions", "text": "In this report, we have outlined the results of just one iteration of our semi-supervised approach. We are reviewing our expansion plan and promising next steps."}, {"heading": "7.1 Scaling Up", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves in the world, and we are able to assert ourselves in the world, we are able to assert ourselves, we are in the world."}, {"heading": "7.2 Future Work", "text": "Our semi-supervised training process allows us to create very large, high-quality conversational language records that are orders of magnitude larger than what is possible through manual transcription, with manual efforts heavily focused on specific tasks that have the greatest impact on WER reduction and enhance the compounding effect of flushing and repeating with a larger and better decoder, trained on cleaner and larger amounts of correctly labeled audio.Future work includes selectivity of VAD, improved speech recognition and voice signal conditioning, and opportunities to use RNN-LM or CNN text classification models to perform more powerful data selection [13]. In addition, we see great potential in the algorithmic superiority of bleeding ASR methods using attention-based models or sequence-trained neural networks with lattice-free MICTC or M2 targets [20]."}], "references": [{"title": "Deep speech 2: End-toend speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "In Proceedings of the 34th annual meeting on Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Resegmentation of switchboard", "author": ["N. Deshmukh", "A. Ganapathiraju", "A. Gleeson", "J. Hamaker", "J. Picone"], "venue": "In ICSLP. Syndey,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Studies on training text selection for conversational finnish language modeling", "author": ["S. Enarvi", "M. Kurimo"], "venue": "In Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Ivector-based speaker adaptation of deep neural networks for french broadcast audio transcription", "author": ["V. Gupta", "P. Kenny", "P. Ouellet", "T. Stafylakis"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "colorless green ideas sleep furiously: A linguistic test case and its appropriations. Literature and Linguistics: Approaches, Models and Applications: Studies in Honour of Jon Erickson", "author": ["M. Jahn"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "A big data approach to acoustic model training corpus selection", "author": ["O. Kapralova", "J. Alex", "E. Weinstein", "P.J. Moreno", "O. Siohan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Audio augmentation for speech recognition", "author": ["T. Ko", "V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "In INTER- SPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Modeling underresourced languages for speech recognition", "author": ["M. Kurimo", "S. Enarvi", "O. Tilk", "M. Varjokallio", "A. Mansikkaniemi", "T. Alum\u00e4e"], "venue": "lang. Res. Eval,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recognizing disfluencies in conversational speech", "author": ["M. Lease", "M. Johnson", "E. Charniak"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Semisupervised acoustic model training by discriminative data selection from multiple asr systems", "author": ["S. Li", "Y. Akita", "T. Kawahara"], "venue": "hypotheses. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Large scale deep neural network acoustic modeling with semisupervised training data for youtube video transcription", "author": ["H. Liao", "E. McDermott", "A. Senior"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Infinite number of sentences", "author": ["R. Mannell"], "venue": "Technical report, Macquarie University, Department of Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Speech recognition with weighted finite-state transducers", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "In Springer Handbook of Speech Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding, number EPFL-CONF-192584", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE Signal Processing Society,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["D. Povey", "V. Peddinti", "D. Galvez", "P. Ghahrmani", "V. Manohar", "X. Na", "Y. Wang", "S. Khudanpur"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Data augmentation for low resource languages", "author": ["A. Ragni", "K.M. Knill", "S.P. Rath", "M.J. Gales"], "venue": "In IN- TERSPEECH,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Improved feature processing for deep neural networks", "author": ["S.P. Rath", "D. Povey", "K. Vesel\u1ef3", "J. Cernock\u1ef3"], "venue": "In Interspeech,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "The IBM 2015 english conversational telephone speech recognition", "author": ["G. Saon", "H.J. Kuo", "S.J. Rennie", "M. Picheny"], "venue": "system. CoRR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The IBM 2016 english conversational telephone speech recognition", "author": ["G. Saon", "T. Sercu", "S.J. Rennie", "H.J. Kuo"], "venue": "system. CoRR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Deep neural network features and semisupervised training for low resource speech recognition", "author": ["S. Thomas", "M.L. Seltzer", "K. Church", "H. Hermansky"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "In Interspeech,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Unsupervised training for mandarin broadcast news and conversation transcription", "author": ["L. Wang", "M.J. Gales", "P.C. Woodland"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Minimum bayes risk decoding and system combination based on a recursion for edit distance", "author": ["H. Xu", "D. Povey", "L. Mangu", "J. Zhu"], "venue": "Computer Speech & Language,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Unsupervised training and directed manual transcription for lvcsr", "author": ["K. Yu", "M. Gales", "L. Wang", "P.C. Woodland"], "venue": "Speech Communication,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "We define dataset construction and training as semi-supervised because we employ a seed model to transcribe a vast quantity of unlabeled audio, perform data selection on the new transcripts, retrain the seed model and then repeat the process with the improved decoder [15].", "startOffset": 268, "endOffset": 272}, {"referenceID": 25, "context": "The transcribed text is filtered to select minimum-length utterances with the lowest MBR confidence [28] and the lowest language model (LM) perplexity.", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "Because the error rate of the confidence-filtered training data can limit the gains due to poor acoustic modeling alignments [26][27], we use various natural language processing (NLP) heuristics to algorithmically identify the highest prevalence, unique mistranscriptions for correction.", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "Because the error rate of the confidence-filtered training data can limit the gains due to poor acoustic modeling alignments [26][27], we use various natural language processing (NLP) heuristics to algorithmically identify the highest prevalence, unique mistranscriptions for correction.", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "These include disfluencies such as mid-sentence hesitations, stutters, ungrammatical or filled pauses (uh, um, ah, er), back-channels (yeah, mhm, uhhuh), discourse markers (like, so, you know), self-editing terms (or rather, I mean), cut-off phrases, restarts, repetitions, final lengthening of syllables, coughs and laughter [14][5].", "startOffset": 326, "endOffset": 330}, {"referenceID": 2, "context": "These include disfluencies such as mid-sentence hesitations, stutters, ungrammatical or filled pauses (uh, um, ah, er), back-channels (yeah, mhm, uhhuh), discourse markers (like, so, you know), self-editing terms (or rather, I mean), cut-off phrases, restarts, repetitions, final lengthening of syllables, coughs and laughter [14][5].", "startOffset": 330, "endOffset": 333}, {"referenceID": 6, "context": "In an article which tackles the linguistic appropriations and interpretations of Chomsky\u2019s \u201cColorless Green Ideas Sleep Furiously\u201d, Manfred discusses a co-operative principle in human communication which binds two speakers to conversational maxims [9].", "startOffset": 248, "endOffset": 251}, {"referenceID": 14, "context": "Now considering that the English language has approximately half a million words, excluding many colloquial forms, with Unabridged English dictionaries listings of between 300,000 to 600,000 words, we see the combinatorial complexity and valid correct transcription of an arbitrary spontaneous utterance though finite, is unbounded [17].", "startOffset": 332, "endOffset": 336}, {"referenceID": 8, "context": "There are many approaches to building speech training sets, including acoustic data perturbation and data synthesis [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "Google takes advantage of their large scale in constructing a training set for their Voice Search and Voice Input tasks for low-resource languages such as Brazilian Portuguese, Italian, Russian and French [10].", "startOffset": 205, "endOffset": 209}, {"referenceID": 13, "context": "In conjunction with owner-uploaded transcripts, Youtube apply \u201cisland of confidence\u201d filtering heuristics to generate additional semi-supervised training data for the deep neural network (DNN) based acoustic model (AM) driving their closed captions feature [16].", "startOffset": 257, "endOffset": 261}, {"referenceID": 7, "context": "[10][29] train acoustic models on a Mandarin language Broadcast News (BN) and Broadcast Conversation (BC) dataset created with semisupervised techniques.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[10][29] train acoustic models on a Mandarin language Broadcast News (BN) and Broadcast Conversation (BC) dataset created with semisupervised techniques.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "[21] use a semi-supervised system to build corpora for low-resource languages Zulu and Assamese task, using weighted word-confusion-network confidences for data selection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] employ semi-supervised methods to construct a Mandarin training corpus based on a Chinese television spoken lectures series, using conditional random fields (CRF) for confidence estimation instead of the raw ASR decoder confidence measure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[6][12] tackle a conversational Finnish language ASR task with a novel semi-supervised approach to training text selection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[6][12] tackle a conversational Finnish language ASR task with a novel semi-supervised approach to training text selection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[25] use a hybrid confidence score based on word-level ASR confidence as well as a posteriogram-based phoneme occurrence confidence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The seed ASR system is based on an online decoder written using Kaldi, a free, open-source C++ toolkit for speech recognition research [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "In contrast to Gaussian mixture models (GMM) traditionally used in speech recognition, DNN models are superior classifiers that generalize better with a smaller number of model parameters, even when the dimensionality of the input features is very high [26].", "startOffset": 253, "endOffset": 257}, {"referenceID": 19, "context": "The seed decoder\u2019s DNN is a 4 hidden layer neural network where the final layer is a softmax layer with a dimension corresponding to each of the 3500 context-dependent HMM states [22].", "startOffset": 179, "endOffset": 183}, {"referenceID": 16, "context": "Finally a speaker normalization transform is applied, called feature-space maximum likelihood linear regression (fMLLR) [19].", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "Augmenting a new speaker\u2019s input feature vector with a corresponding iVector projection before DNN processing, permits the DNN to discriminate better between phonetic events in an adaptive, speaker-independent fashion [8], with minimal impact to the DNN training cycle.", "startOffset": 218, "endOffset": 221}, {"referenceID": 15, "context": "For further details on ASR with weighted finitestate transducers refer to [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "Similarly to [10], we decode using a slower but more accurate, non-production decoder tuned to have a large-beam.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "For selecting utterances, we compute perplexity with a Kneser-Ney smoothed 5-gram model with a 125K vocabulary and 5M n-grams [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 20, "context": "We compute WER scores for the seed model, IBM Watson Speech to Text service [23][24] as well as our updated production model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "We compute WER scores for the seed model, IBM Watson Speech to Text service [23][24] as well as our updated production model.", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "While IBM have not trained their models on Marchex English, their results are valid benchmarks because of the their published results on the Hub5 2000 Evaluation conversational task, a corpus of 40 test telephone conversations from the SWBD and CALLHOME corpora [23][24].", "startOffset": 262, "endOffset": 266}, {"referenceID": 21, "context": "While IBM have not trained their models on Marchex English, their results are valid benchmarks because of the their published results on the Hub5 2000 Evaluation conversational task, a corpus of 40 test telephone conversations from the SWBD and CALLHOME corpora [23][24].", "startOffset": 266, "endOffset": 270}, {"referenceID": 4, "context": "For SWBD, a prompt suggested a topic of conversation [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "48 [7].", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In the table below, we draw further contrast between FE, SWBD and Marchex English [5][4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "There are also opportunities to use RNN-LM or CNN models for text classification to do more powerful data selection [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "Furthermore, we see a lot of potential in the algorithmic superiority of bleeding-edge ASR methods using attention-based models or sequence trained neural networks with lattice-free MMI or CTC objectives [2][20].", "startOffset": 204, "endOffset": 207}, {"referenceID": 17, "context": "Furthermore, we see a lot of potential in the algorithmic superiority of bleeding-edge ASR methods using attention-based models or sequence trained neural networks with lattice-free MMI or CTC objectives [2][20].", "startOffset": 207, "endOffset": 211}], "year": 2017, "abstractText": "For conversational large-vocabulary continuous speech recognition (LVCSR) tasks, up to about two thousand hours of audio is commonly used to train state of the art models. Collection of labeled conversational audio however, is prohibitively expensive, laborious and error-prone. Furthermore, academic corpora like Fisher English (2004) or Switchboard (1992) are inadequate to train models with sufficient accuracy in the unbounded space of conversational speech. These corpora are also timeworn due to dated acoustic telephony features and the rapid advancement of colloquial vocabulary and idiomatic speech over the last decades. Utilizing the colossal scale of our unlabeled telephony dataset, we propose a technique to construct a modern, high quality conversational speech training corpus on the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and language model training. We describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test corpus of 7K manually transcribed utterances. We show relative word error rate (WER) reductions of {35%, 19%} on {agent, caller} utterances over our seed model and 5% absolute WER improvements over IBM Watson STT on this conversational speech task.", "creator": "LaTeX with hyperref package"}}}