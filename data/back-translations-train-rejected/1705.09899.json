{"id": "1705.09899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks", "abstract": "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and we discuss its implications for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.", "histories": [["v1", "Sun, 28 May 2017 06:59:07 GMT  (35kb)", "https://arxiv.org/abs/1705.09899v1", "To appear in the proceedings of the 1st Workshop on Abusive Language Online. Please cite that version"], ["v2", "Tue, 30 May 2017 11:07:51 GMT  (35kb)", "http://arxiv.org/abs/1705.09899v2", "To appear in the proceedings of the 1st Workshop on Abusive Language Online. Please cite that version"]], "COMMENTS": "To appear in the proceedings of the 1st Workshop on Abusive Language Online. Please cite that version", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zeerak waseem", "thomas davidson", "dana warmsley", "ingmar weber"], "accepted": false, "id": "1705.09899"}, "pdf": {"name": "1705.09899.pdf", "metadata": {"source": "CRF", "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks", "authors": ["Zeerak Waseem", "Thomas Davidson", "Dana Warmsley"], "emails": ["z.w.butt@shef.ac.uk,", "trd54@cornell.edu,", "dw457@cornell.edu,", "iweber@hbku.edu.qa"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.09 899v 2 [cs.C L] 30 May 201 7While the number of research papers on the detection and analysis of abusive language is growing, there is a need for a critical examination of the relationships between various sub-tasks summarised under this label. Based on work on hate speech, cyberbullying and online abuse, we propose a typology that identifies key similarities and differences between sub-tasks and discusses their impact on the annotation of data and the construction of features. We highlight the practical measures that can be taken by researchers to best address their sub-task on the detection of abusive language."}, {"heading": "1 Introduction", "text": "In recent years, interest in identifying abusive language, hate speech, cyberbullying, and trolling has increased (Schmidt and Wiegand, 2017), and social media sites are also coming under increasing pressure to address these issues, and similarities between these subtasks have led scholars to group them under the umbrella terms \"offensive language,\" \"harmful language,\" and \"hate speech\" (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017), but little work has been done to investigate the relationship between them. Since each of these subtasks attempts to address a specific but sometimes overlapping phenomenon, we believe that there is much to be gained by studying how they relate to each other. The overlap between subtasks is illustrated by the variety of labels used in previous work."}, {"heading": "2 A typology of abusive language", "text": "Much of the work on abusive language subtasks can be implicitly identified in a dual typology that is directed at both racial groups (i) the abuse is directed at a particular target, and (ii) the degree to which it is explicit. Starting from the targets, abuse can either be directed at a particular individual or a particular ethnic entity, or it can be used toward a generalized other, for example, people with a particular ethnic or sexual orientation. This is an important sociological distinction, since the latter refers to an entire category of people rather than to a specific person, group, or organization (see Brubaker 2004, Wimmer 2013) and, as we discuss below, involves a linguistic distinction that can be used productively by researchers. To better illustrate this, the first set of Table 1 shows examples from the literature of directed abuse, where someone is either mentioned by name or by a username or by reference."}, {"heading": "3 Implications for future research", "text": "In the following section we will outline the implications of this typology and show where the available literature indicates how we can understand, measure and model each subtype of abuse."}, {"heading": "3.1 Implications for annotation", "text": "In the task of commenting on documents containing bullying, it appears that there is a common understanding of what cyberbullying entails: an intentionally harmful electronic attack by an individual or group on a victim, usually repetitive in nature (Dadvar et al., 2013).This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which merely ask annotators to determine whether a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014).High interannotatory agreement on cyberbullying tasks (93%) (Dadvar et al., 2013) further indicates a general consensus on the characteristics of cyberbullying (Van Hee et al., 2015b).After bullying, annotators were generally asked more detailed questions about the extremes of bullying, identifying phrases that indicate bullying, and suggesting bullying."}, {"heading": "3.2 Implications for modeling", "text": "It is important that researchers clarify which characteristics are most useful for which sub-tasks and which sub-tasks pose the greatest challenges. We do not attempt to review all the characteristics used (see Schmidt and Wiegand 2017 for a detailed review of abuse), but to suggest which characteristics might be most helpful for the various sub-tasks. For each aspect of the typology, we point out that characteristics that have proven successful predictors in previous work are used. Many characteristics occur in more than one form of abuse. Therefore, we do not suggest that certain characteristics are necessarily unique to each phenomenon, but that they provide different insights and should be used depending on what the researcher is trying to measure the abuse. Features that help identify the target of abuse are crucial for directed detection of abuse. Mentions, correct designations, named entities, and co-reference solutions can all be used in different contexts to identify targets."}, {"heading": "4 Discussion", "text": "This typology has a number of implications for future work in this area. First, we would like to encourage researchers working on these subtasks to learn from advances in other areas. Researchers working on supposedly different subtasks often work in parallel on the same problems. For example, the area of detection of hate speech can be strengthened by interacting with work on cyberbullying, and vice versa, since a large part of both subtasks is identifying targeted abuse. Second, we aim to highlight the important distinctions within subtasks that have been ignored so far. For example, many studies on hate speech have lumped different types of abuse together, forcing models to consider a large amount of variation within the class. We suggest that fine-grained distinctions along the axes allow for more focused abuse that can be more effective in identifying certain types of abuse. Third, we demand a closer look at how annotation guidelines relate to the phenomenon of interest."}, {"heading": "5 Conclusion", "text": "We have presented a typology that synthesizes the various subtasks in recognizing abusive language. Our goal is to bring together the results in these different areas and clarify the key aspects of recognizing abusive language. There are important analytical distinctions that have been largely overlooked in previous work, and by recognizing these and their implications, we hope to improve the systems for detecting abuses and our understanding of abusive language. Instead of trying to resolve the \"definitive quagmire\" (Faris et al., 2016) that is involved in finely delineating and defining each subtask, we encourage researchers to think carefully about the phenomena they want to measure and the appropriate research design. We intend that our typology will be used both at the stage of data collection and annotation, as well as at the stage of creating and modeling characteristics. We hope that future work will be more transparent in discussing the annotation and modeling tasks and examining the differences between these similarities and similarities."}], "references": [{"title": "Deep learning for hate speech detection in tweets", "author": ["Pinkesh Badjatiya", "Shashank Gupta", "Manish Gupta", "Vasudeva Varma."], "venue": "Proceedings of the 26th International Conference on World Wide Web Companion. pages 759\u2013760.", "citeRegEx": "Badjatiya et al\\.,? 2017", "shortCiteRegEx": "Badjatiya et al\\.", "year": 2017}, {"title": "Mythologies", "author": ["Roland Barthes."], "venue": "Seuil.", "citeRegEx": "Barthes.,? 1957", "shortCiteRegEx": "Barthes.", "year": 1957}, {"title": "Detecting offensive statements towards foreigners in social media", "author": ["Uwe Bretschneider", "Ralf Peters."], "venue": "Proceedings of the 50th Hawaii International Conference on System Sciences.", "citeRegEx": "Bretschneider and Peters.,? 2017", "shortCiteRegEx": "Bretschneider and Peters.", "year": 2017}, {"title": "Detecting online harassment in social networks", "author": ["Uwe Bretschneider", "Thomas Whner", "Ralf Peters."], "venue": "ICIS 2014 Proceedings: Conference Theme Track: Building a Better World through IS.", "citeRegEx": "Bretschneider et al\\.,? 2014", "shortCiteRegEx": "Bretschneider et al\\.", "year": 2014}, {"title": "Ethnicity without groups", "author": ["Rogers Brubaker."], "venue": "Harvard University Press.", "citeRegEx": "Brubaker.,? 2004", "shortCiteRegEx": "Brubaker.", "year": 2004}, {"title": "Cyber hate speech on twitter: An application of machine classification and statistical modeling for policy and decision making", "author": ["Pete Burnap", "Matthew L Williams."], "venue": "Policy & Internet 7(2):223\u2013242.", "citeRegEx": "Burnap and Williams.,? 2015", "shortCiteRegEx": "Burnap and Williams.", "year": 2015}, {"title": "Experts and machines against bullies: a hybrid approach to detect cyberbullies", "author": ["Maral Dadvar", "Dolf Trieschnigg", "Franciska de Jong."], "venue": "Conference on Artificial Intelligence. Springer International Publishing.", "citeRegEx": "Dadvar et al\\.,? 2014", "shortCiteRegEx": "Dadvar et al\\.", "year": 2014}, {"title": "Improving cyberbullying detection with user context", "author": ["Maral Dadvar", "Dolf Trieschnigg", "Roeland Ordelman", "Franciska de Jong."], "venue": "European Conference on Information Retrieval. Springer, pages 693\u2013696.", "citeRegEx": "Dadvar et al\\.,? 2013", "shortCiteRegEx": "Dadvar et al\\.", "year": 2013}, {"title": "Automated hate speech detection and the problem of offensive language", "author": ["Thomas Davidson", "Dana Warmsley", "Micheel Macy", "Ingmar Weber."], "venue": "Proceedings of the Eleventh International Conference on Web and Social Media. Montreal, Canada,", "citeRegEx": "Davidson et al\\.,? 2017", "shortCiteRegEx": "Davidson et al\\.", "year": 2017}, {"title": "Common sense reasoning for detection, prevention, and mitigation of cyberbullying", "author": ["Karthik Dinakar", "Birago Jones", "Catherine Havasi", "Henry Lieberman", "Rosalind Picard."], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS) 2(3):18.", "citeRegEx": "Dinakar et al\\.,? 2012", "shortCiteRegEx": "Dinakar et al\\.", "year": 2012}, {"title": "Modeling the detection of textual cyberbullying", "author": ["Karthik Dinakar", "Roi Reichart", "Henry Lieberman."], "venue": "The Social Mobile Web 11(02).", "citeRegEx": "Dinakar et al\\.,? 2011", "shortCiteRegEx": "Dinakar et al\\.", "year": 2011}, {"title": "Hate speech detection with comment embeddings", "author": ["Nemanja Djuric", "Jing Zhou", "Robin Morris", "Mihajlo Grbovic", "Vladan Radosavljevic", "Narayan Bhamidipati."], "venue": "Proceedings of the 24th International Conference on World Wide Web. ACM, pages", "citeRegEx": "Djuric et al\\.,? 2015", "shortCiteRegEx": "Djuric et al\\.", "year": 2015}, {"title": "Understanding harmful speech online", "author": ["Robert Faris", "Amar Ashar", "Urs Gasser", "Daisy Joo."], "venue": "Berkman Klein Center Research Publication 21.", "citeRegEx": "Faris et al\\.,? 2016", "shortCiteRegEx": "Faris et al\\.", "year": 2016}, {"title": "A lexicon-based approach for hate speech detection", "author": ["Njagi Dennis Gitari", "Zhang Zuping", "Hanyurwimfura Damien", "Jun Long."], "venue": "International Journal of Multimedia and Ubiquitous Engineering 10(4):215\u2013230.", "citeRegEx": "Gitari et al\\.,? 2015", "shortCiteRegEx": "Gitari et al\\.", "year": 2015}, {"title": "A longitudinal measurement study of 4chan\u2019s politically incorrect forum and its", "author": ["Gabriel Emile Hine", "Jeremiah Onaolapo", "Emiliano De Cristofaro", "Nicolas Kourtellis", "Ilias Leontiadis", "Riginos Samaras", "Gianluca Stringhini", "Jeremy Blackburn"], "venue": null, "citeRegEx": "Hine et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hine et al\\.", "year": 2017}, {"title": "Extracting relevant knowledge for the detection of sarcasm and nastiness in the social web", "author": ["Raquel Justo", "Thomas Corcoran", "Stephanie M. Lukin", "Marilyn Walker", "M. Ins Torres."], "venue": "Knowledge-Based Systems 69:124 \u2013 133.", "citeRegEx": "Justo et al\\.,? 2014", "shortCiteRegEx": "Justo et al\\.", "year": 2014}, {"title": "Computer-assisted keyword and document set discovery from unstructured text", "author": ["Gary King", "Patrick Lam", "Margaret E Roberts."], "venue": "American Journal of Political Science .", "citeRegEx": "King et al\\.,? 2017", "shortCiteRegEx": "King et al\\.", "year": 2017}, {"title": "Detecting cyberbullying: Query terms and techniques", "author": ["April Kontostathis", "Kelly Reynolds", "Andy Garron", "Lynne Edwards."], "venue": "Proceedings of the 5th Annual ACM Web Science Conference. ACM, New York, NY, USA, WebSci \u201913, pages 195\u2013204.", "citeRegEx": "Kontostathis et al\\.,? 2013", "shortCiteRegEx": "Kontostathis et al\\.", "year": 2013}, {"title": "Locate the hate: Detecting tweets against blacks", "author": ["Irene Kwok", "Yuzhou Wang."], "venue": "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence. AAAI Press, AAAI\u201913, pages 1621\u2013 1622.", "citeRegEx": "Kwok and Wang.,? 2013", "shortCiteRegEx": "Kwok and Wang.", "year": 2013}, {"title": "Detecting the hate code on social media", "author": ["Rijul Magu", "Kshitij Joshi", "Jiebo Luo."], "venue": "Proceedings of the Eleventh International Conference on Web and Social Media. Montreal, Canada, pages 608\u2013612.", "citeRegEx": "Magu et al\\.,? 2017", "shortCiteRegEx": "Magu et al\\.", "year": 2017}, {"title": "Abusive language detection in online user content", "author": ["Chikashi Nobata", "Joel Tetreault", "Achint Thomas", "Yashar Mehdad", "Yi Chang."], "venue": "Proceedings of the 25th International Conference on World Wide Web. pages 145\u2013153.", "citeRegEx": "Nobata et al\\.,? 2016", "shortCiteRegEx": "Nobata et al\\.", "year": 2016}, {"title": "Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "author": ["Bj\u00f6rn Ross", "Michael Rist", "Guillermo Carbonell", "Benjamin Cabrera", "Nils Kurowsky", "Michael Wojatzki."], "venue": "Proceedings of NLP4CMC III:", "citeRegEx": "Ross et al\\.,? 2016", "shortCiteRegEx": "Ross et al\\.", "year": 2016}, {"title": "A survey on hate speech detection using natural language processing", "author": ["Anna Schmidt", "Michael Wiegand."], "venue": "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media. Association for Computational Linguis-", "citeRegEx": "Schmidt and Wiegand.,? 2017", "shortCiteRegEx": "Schmidt and Wiegand.", "year": 2017}, {"title": "Analyzing the targets of hate in online social media", "author": ["Leandro Ara\u00fajo Silva", "Mainack Mondal", "Denzil Correa", "Fabr\u0131\u0301cio Benevenuto", "Ingmar Weber"], "venue": "In Proceedings of the Tenth International Conference on Web and Social Media. Cologne, Germany,", "citeRegEx": "Silva et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2016}, {"title": "Racial microaggressions in everyday life: implications for clinical practice", "author": ["Derald Wing Sue", "Christina M Capodilupo", "Gina C Torino", "Jennifer M Bucceri", "Aisha Holder", "Kevin L Nadal", "Marta Esquilin."], "venue": "American Psychologist 62(4):271\u2013286.", "citeRegEx": "Sue et al\\.,? 2007", "shortCiteRegEx": "Sue et al\\.", "year": 2007}, {"title": "The automated detection of racist discourse in dutch social media", "author": ["St\u00e9phan Tulkens", "Lisa Hilte", "Elise Lodewyckx", "Ben Verhoeven", "Walter Daelemans."], "venue": "CLIN Journal 6:3\u201320.", "citeRegEx": "Tulkens et al\\.,? 2016", "shortCiteRegEx": "Tulkens et al\\.", "year": 2016}, {"title": "Detection and fine-grained classification of cyberbullying events", "author": ["Cynthia Van Hee", "Els Lefever", "Ben Verhoeven", "Julie Mennes", "Bart Desmet", "Guy De Pauw", "Walter Daelemans", "Veronique Hoste."], "venue": "Proceedings of the International Conference Re-", "citeRegEx": "Hee et al\\.,? 2015a", "shortCiteRegEx": "Hee et al\\.", "year": 2015}, {"title": "Guidelines for the fine-grained analysis of cyberbullying", "author": ["Cynthia Van Hee", "Ben Verhoeven", "Els Lefever", "Guy De Pauw", "V\u00e9ronique Hoste", "Walter Daelemans."], "venue": "Technical report, LT3, Ghent University, Belgium.", "citeRegEx": "Hee et al\\.,? 2015b", "shortCiteRegEx": "Hee et al\\.", "year": 2015}, {"title": "Detecting hate speech on the world wide web", "author": ["William Warner", "Julia Hirschberg."], "venue": "Proceedings of the Second Workshop on Language in Social Media. Association for Computational Linguistics, LSM \u201912, pages 19\u201326.", "citeRegEx": "Warner and Hirschberg.,? 2012", "shortCiteRegEx": "Warner and Hirschberg.", "year": 2012}, {"title": "Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter", "author": ["ZeerakWaseem."], "venue": "Proceedings of the First Workshop on NLP andComputational Social Science. Association for Computational Linguistics, Austin, Texas, pages", "citeRegEx": "ZeerakWaseem.,? 2016a", "shortCiteRegEx": "ZeerakWaseem.", "year": 2016}, {"title": "Automatic Hate Speech Detection", "author": ["Zeerak Waseem."], "venue": "Master\u2019s thesis, University of Copenhagen.", "citeRegEx": "Waseem.,? 2016b", "shortCiteRegEx": "Waseem.", "year": 2016}, {"title": "Hateful symbols or hateful people? predictive features for hate speech detection on twitter", "author": ["Zeerak Waseem", "Dirk Hovy."], "venue": "Proceedings of the NAACL Student ResearchWorkshop. Association for Computational Linguistics, San Diego, California,", "citeRegEx": "Waseem and Hovy.,? 2016", "shortCiteRegEx": "Waseem and Hovy.", "year": 2016}, {"title": "Ethnic boundary making: Institutions, power, networks", "author": ["Andreas Wimmer."], "venue": "Oxford University Press.", "citeRegEx": "Wimmer.,? 2013", "shortCiteRegEx": "Wimmer.", "year": 2013}, {"title": "Ex machina: Personal attacks seen at scale", "author": ["Ellery Wulczyn", "Nithum Thain", "Lucas Dixon."], "venue": "Proceedings of the 26th International Conference on World Wide Web.", "citeRegEx": "Wulczyn et al\\.,? 2017", "shortCiteRegEx": "Wulczyn et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 22, "context": "There has been a surge in interest in the detection of abusive language, hate speech, cyberbullying, and trolling in the past several years (Schmidt and Wiegand, 2017).", "startOffset": 140, "endOffset": 167}, {"referenceID": 20, "context": "Similarities between these subtasks have led scholars to group them together under the umbrella terms of \u201cabusive language\u201d, \u201charmful speech\u201d, and \u201chate speech\u201d (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017) but little work has been done to examine the relationship between them.", "startOffset": 161, "endOffset": 229}, {"referenceID": 12, "context": "Similarities between these subtasks have led scholars to group them together under the umbrella terms of \u201cabusive language\u201d, \u201charmful speech\u201d, and \u201chate speech\u201d (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017) but little work has been done to examine the relationship between them.", "startOffset": 161, "endOffset": 229}, {"referenceID": 22, "context": "Similarities between these subtasks have led scholars to group them together under the umbrella terms of \u201cabusive language\u201d, \u201charmful speech\u201d, and \u201chate speech\u201d (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017) but little work has been done to examine the relationship between them.", "startOffset": 161, "endOffset": 229}, {"referenceID": 24, "context": "Van Hee et al. (2015b) identifies discriminative remarks (racist, sexist) as a subset of \u201cinsults\u201d, whereas Nobata et al.", "startOffset": 4, "endOffset": 23}, {"referenceID": 19, "context": "(2015b) identifies discriminative remarks (racist, sexist) as a subset of \u201cinsults\u201d, whereas Nobata et al. (2016) classifies similar remarks as \u201chate speech\u201d or \u201cderogatory language\u201d.", "startOffset": 93, "endOffset": 114}, {"referenceID": 19, "context": "(2015b) identifies discriminative remarks (racist, sexist) as a subset of \u201cinsults\u201d, whereas Nobata et al. (2016) classifies similar remarks as \u201chate speech\u201d or \u201cderogatory language\u201d. Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al.", "startOffset": 93, "endOffset": 207}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language.", "startOffset": 144, "endOffset": 167}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language.", "startOffset": 144, "endOffset": 248}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines - some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al.", "startOffset": 144, "endOffset": 511}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines - some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al.", "startOffset": 144, "endOffset": 580}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines - some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017). To help to bring together these literatures and to avoid these contradictions, we propose a typology that synthesizes these different subtasks.", "startOffset": 144, "endOffset": 607}, {"referenceID": 8, "context": "the border little n*gga\u201d (Davidson et al., 2017),", "startOffset": 25, "endOffset": 48}, {"referenceID": 17, "context": "(Kontostathis et al., 2013).", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "visit?\u201d (Dinakar et al., 2012),", "startOffset": 8, "endOffset": 30}, {"referenceID": 14, "context": "Google balls? #Dumbgoogles\u201d (Hine et al., 2017),", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": "\u201cyou\u2019re intelligence is so breathtaking!!!!!!\u201d (Dinakar et al., 2011)", "startOffset": 47, "endOffset": 69}, {"referenceID": 20, "context": "Kill all the g*ys there!\u201d (Nobata et al., 2016),", "startOffset": 26, "endOffset": 47}, {"referenceID": 18, "context": "another n*gger off the streets!!\u201d (Kwok and Wang, 2013).", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": "\u201d (Burnap and Williams, 2015),", "startOffset": 2, "endOffset": 29}, {"referenceID": 10, "context": "(Dinakar et al., 2011),", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "\u201cGas the skypes\u201d (Magu et al., 2017)", "startOffset": 17, "endOffset": 36}, {"referenceID": 5, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al.", "startOffset": 75, "endOffset": 148}, {"referenceID": 31, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al.", "startOffset": 75, "endOffset": 148}, {"referenceID": 8, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al.", "startOffset": 75, "endOffset": 148}, {"referenceID": 5, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al. (2016) come closest", "startOffset": 76, "endOffset": 180}, {"referenceID": 1, "context": "This is roughly analogous to the distinction in linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by Barthes (1957). Explicit abusive lan-", "startOffset": 208, "endOffset": 223}, {"referenceID": 28, "context": "Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012; Davidson et al., 2017), with abusive terms being used in a colloquial manner or by people who are victims of abuse.", "startOffset": 79, "endOffset": 131}, {"referenceID": 8, "context": "Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012; Davidson et al., 2017), with abusive terms being used in a colloquial manner or by people who are victims of abuse.", "startOffset": 79, "endOffset": 131}, {"referenceID": 10, "context": "Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014).", "startOffset": 229, "endOffset": 292}, {"referenceID": 7, "context": "Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014).", "startOffset": 229, "endOffset": 292}, {"referenceID": 15, "context": "Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014).", "startOffset": 229, "endOffset": 292}, {"referenceID": 24, "context": "Social scientists and activists have recently been paying more attention to implicit, and even unconscious, instances of abuse that have been termed \u201cmicroaggressions\u201d (Sue et al., 2007).", "startOffset": 168, "endOffset": 186}, {"referenceID": 7, "context": "In the task of annotating documents that contain bullying, it appears that there is a common understanding of what cyberbullying entails: an intentionally harmful electronic attack by an individual or group against a victim, usually repetitive in nature (Dadvar et al., 2013).", "startOffset": 254, "endOffset": 275}, {"referenceID": 6, "context": "This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which simply ask annotators to determine if a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014).", "startOffset": 186, "endOffset": 262}, {"referenceID": 17, "context": "This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which simply ask annotators to determine if a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014).", "startOffset": 186, "endOffset": 262}, {"referenceID": 3, "context": "This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which simply ask annotators to determine if a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014).", "startOffset": 186, "endOffset": 262}, {"referenceID": 7, "context": "High interannotator agreement on cyberbullying tasks (93%) (Dadvar et al., 2013) further indicates a general consensus around the features of cyberbullying (Van Hee et al.", "startOffset": 59, "endOffset": 80}, {"referenceID": 6, "context": "After bullying has been identified annotators are typically asked more detailed questions about the extremity of the bullying, the identification of phrases that indicate bullying, and the roles of users as bully/victim (Dadvar et al., 2014; Van Hee et al., 2015b; Kontostathis et al., 2013).", "startOffset": 220, "endOffset": 291}, {"referenceID": 17, "context": "After bullying has been identified annotators are typically asked more detailed questions about the extremity of the bullying, the identification of phrases that indicate bullying, and the roles of users as bully/victim (Dadvar et al., 2014; Van Hee et al., 2015b; Kontostathis et al., 2013).", "startOffset": 220, "endOffset": 291}, {"referenceID": 21, "context": "80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained.", "startOffset": 4, "endOffset": 92}, {"referenceID": 25, "context": "80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained.", "startOffset": 4, "endOffset": 92}, {"referenceID": 2, "context": "80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained.", "startOffset": 4, "endOffset": 92}, {"referenceID": 30, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al.", "startOffset": 161, "endOffset": 176}, {"referenceID": 7, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011).", "startOffset": 247, "endOffset": 310}, {"referenceID": 15, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011).", "startOffset": 247, "endOffset": 310}, {"referenceID": 10, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011).", "startOffset": 247, "endOffset": 310}, {"referenceID": 8, "context": "Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions (Davidson et al., 2017), others find that they do not improve the reliability of non-expert classifications (Ross et al.", "startOffset": 108, "endOffset": 131}, {"referenceID": 21, "context": ", 2017), others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016).", "startOffset": 92, "endOffset": 111}, {"referenceID": 8, "context": "Davidson et al. (2017), for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism.", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017).", "startOffset": 98, "endOffset": 171}, {"referenceID": 13, "context": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017).", "startOffset": 98, "endOffset": 171}, {"referenceID": 8, "context": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017).", "startOffset": 98, "endOffset": 171}, {"referenceID": 5, "context": "Typed dependencies offer a more sophisticated way to capture the relationship between terms (Burnap and Williams, 2015).", "startOffset": 92, "endOffset": 119}, {"referenceID": 2, "context": "Bretschneider and Peters (2017) use a multi-tiered system, first identifying offensive statements, then their severity, and finally the target.", "startOffset": 0, "endOffset": 32}, {"referenceID": 23, "context": "Generalized abuse online tends to target people belonging to a small set of categories, primarily racial, religious, and sexual minorities (Silva et al., 2016).", "startOffset": 139, "endOffset": 159}, {"referenceID": 28, "context": "Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al.", "startOffset": 85, "endOffset": 135}, {"referenceID": 20, "context": "Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al.", "startOffset": 85, "endOffset": 135}, {"referenceID": 18, "context": ", 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017).", "startOffset": 156, "endOffset": 200}, {"referenceID": 8, "context": ", 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017).", "startOffset": 156, "endOffset": 200}, {"referenceID": 13, "context": "Negative polarity and sentiment of the text are also likely indicators of explicit abuse that can be leveraged by researchers (Gitari et al., 2015).", "startOffset": 126, "endOffset": 147}, {"referenceID": 19, "context": "Building a specific lexicon may prove impractical, as in the case of the appropriation of the term \u201cskype\u201d in some forums (Magu et al., 2017).", "startOffset": 122, "endOffset": 141}, {"referenceID": 20, "context": "Additionally, character n-grams have been shown to be apt for abusive language tasks due to their ability to capture variation of words associated with abuse (Nobata et al., 2016; Waseem, 2016a).", "startOffset": 158, "endOffset": 194}, {"referenceID": 11, "context": "Word embeddings are also promising ways to capture terms associated with abuse (Djuric et al., 2015; Badjatiya et al., 2017), although they may still be insufficient for cases like 4Chan\u2019s connotation of \u201cskype\u201d where a word has a dominant meaning and a more subversive one.", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "Word embeddings are also promising ways to capture terms associated with abuse (Djuric et al., 2015; Badjatiya et al., 2017), although they may still be insufficient for cases like 4Chan\u2019s connotation of \u201cskype\u201d where a word has a dominant meaning and a more subversive one.", "startOffset": 79, "endOffset": 124}, {"referenceID": 7, "context": "To overcome these limitations researchers may find it prudent to incorporate features beyond just textual analysis, including the characteristics of the individuals involved (Dadvar et al., 2013) and other extra-textual features.", "startOffset": 174, "endOffset": 195}, {"referenceID": 5, "context": ", 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017). Negative polarity and sentiment of the text are also likely indicators of explicit abuse that can be leveraged by researchers (Gitari et al., 2015). Implicit abuse. Building a specific lexicon may prove impractical, as in the case of the appropriation of the term \u201cskype\u201d in some forums (Magu et al., 2017). Still, even partial lexicons may be used as seeds to inductively discover other keywords by use of a semi-supervised method proposed by King et al. (2017). Additionally, character n-grams have been shown to be apt for abusive language tasks due to their ability to capture variation of words associated with abuse (Nobata et al.", "startOffset": 178, "endOffset": 665}, {"referenceID": 12, "context": "Rather than attempting to resolve the \u201cdefinitional quagmire\u201d (Faris et al., 2016) involved in neatly bounding and defining each subtask we encourage researchers to think carefully about the phenomena they want to measure and the appro-", "startOffset": 62, "endOffset": 82}], "year": 2017, "abstractText": "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and we discuss its implications for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.", "creator": "LaTeX with hyperref package"}}}