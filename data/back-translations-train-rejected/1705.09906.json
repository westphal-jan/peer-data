{"id": "1705.09906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Listen, Interact and Talk: Learning to Speak via Interaction", "abstract": "One of the long-term goals of artificial intelligence is to build an agent that can communicate intelligently with human in natural language. Most existing work on natural language learning relies heavily on training over a pre-collected dataset with annotated labels, leading to an agent that essentially captures the statistics of the fixed external training data. As the training data is essentially a static snapshot representation of the knowledge from the annotator, the agent trained this way is limited in adaptiveness and generalization of its behavior. Moreover, this is very different from the language learning process of humans, where language is acquired during communication by taking speaking action and learning from the consequences of speaking action in an interactive manner. This paper presents an interactive setting for grounded natural language learning, where an agent learns natural language by interacting with a teacher and learning from feedback, thus learning and improving language skills while taking part in the conversation. To achieve this goal, we propose a model which incorporates both imitation and reinforcement by leveraging jointly sentence and reward feedbacks from the teacher. Experiments are conducted to validate the effectiveness of the proposed approach.", "histories": [["v1", "Sun, 28 May 2017 07:48:14 GMT  (614kb,D)", "http://arxiv.org/abs/1705.09906v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haichao zhang", "haonan yu", "wei xu"], "accepted": false, "id": "1705.09906"}, "pdf": {"name": "1705.09906.pdf", "metadata": {"source": "CRF", "title": "Listen, Interact and Talk: Learning to Speak via Interaction", "authors": ["Haichao Zhang", "Haonan Yu"], "emails": ["zhanghaichao@baidu.com", "haonanyu@baidu.com", "xuwei06@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there"}, {"heading": "2 Related Work", "text": "In recent years, it has been shown that the problem is a problem for which there is no solution. (...) In recent years, it has been shown that it is a problem. (...) In recent years, it has been shown that it is a problem. (...) In recent years, it has been shown that the problem cannot be solved. (...) In recent years, the number of unemployed has increased. (...) In recent years, the number of unemployed has increased. (...) In recent years, the number of unemployed has increased. (...) In the last ten years, the number of unemployed has increased. (...) In the last ten years, the number of unemployed has increased. (...) In the last ten years, the number of unemployed has increased. (...) In the last ten years, the number of unemployed has increased. (... In the last ten years, the number of unemployed has increased. (...) In the last ten years, the number of unemployed has increased. (... In the last ten years, the number of unemployed has increased."}, {"heading": "3 Interaction-based Language Learning", "text": "In this section, we will present the proposed interaction-based approach to learning natural language. The aim is to design a learning agent who can learn to talk through interaction with the teacher, who can be either a virtual teacher or a human being (see Figure 1-2). In due course, the teacher generates a sentence wt, which can be a question (e.g. \"what is in the east,\" \"where is apple\"), a statement (e.g. \"banana is in the north\") or an empty sentence (referred to as. \") according to a visual image v. The student takes the teacher's sentence wt + 1 and the visual content v and produces a sentence response to the teacher. The teacher will then give the learner feedback in the form of both sentences wt + 1 and reward rt + 1. The sentence wt + 1 represents verbal feedback from the teacher (e.g.\" Yes in the east is cherry, \"\" no apple is on the east \")."}, {"heading": "3.1 Problem Formulation", "text": "The agent's answer to the question of the value of learning can be used as an example of a probability distribution over possible output sequences."}, {"heading": "3.2 Approach", "text": "A hierarchical recursive neural network is used to capture the sequential structure both through sentences and within a sentence [Yu et al., 2016, Serban et al., 2016] as shown in Figure 2 (a). In time step t, an encoding RNN encodes the input sentence wt from the teacher as well as history information into a state vector htload passed by an action controller f (\u00b7) to generate a control vector kt as input to the action RNN to generate the response to the teacher's sentence. The teacher generates feedback F = {wt + 1, rt + 1} corresponding to both wt and at. In addition to its use as input to the action controller, the state vector is also passed on to the next time step and is used in the next step as the initial state of the encoding RNN (i.e. ht + 10, h t last) to learn from wt + 1, whereby a further layer of recurrence occurs."}, {"heading": "3.2.1 Imitation with Hierarchical-RNN-based Language Modeling", "text": "One way to learn from this source of information is predictive imitation. In particular, for a particular episode we can calculate the probability of the next sentence wt + 1 on the previous sentences w1: t and current picture v aspI\u03b8 (w + 1: t, v) = pI\u03b8 (wt + 1: t, v) = pI\u03b8 (wt + 1: t, v) = pI\u03b8 (wt + 1: t, v) = pImps (wt).iasD \"s is the last state of RNN at the time in which we summarize w 1: t, c.f. Figure 2: i indexes words within a sentence. It is natural to model the probability of the i-th word in the t + 1-th sentence with an RNN."}, {"heading": "3.2.2 Learning via Reinforcement for Sequence Actions", "text": "The reason for including a controller f (\u00b7) for modulation is that the basic language model only provides the ability to generate a sentence, but not necessarily the ability to react correctly."}, {"heading": "3.3 Training", "text": "(5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5)"}, {"heading": "4 Experimental Results", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to change the world."}, {"heading": "T: what is on the north", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "L: on the north is apple", "text": "It is as if men were able to survive themselves, if they were not able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (). (). (). (). (. (). (). (). (). (). (...). (). (). (). (...). (...). (). (). (). (...). (). (). (). (). (). (). (). (). ("}, {"heading": "5 Conclusion", "text": "We present an interactive environment for learning natural language and propose an approach that enables effective interactive learning of natural language by making full use of the feedback generated during interaction through shared imitation and amplification. Experimental results show that the proposed approach is an effective method for learning natural language in an interactive environment and enjoys a desirable generalization and transfer of skills under different scenarios. As for future work, we would like to explore the direction of explicit modelling of learned knowledge [Yang, 2003] and rapid learning about new concepts [Andrychowicz et al., 2016]. Another interesting direction is to combine the task presented in this paper on language acquisition with other heterogeneous tasks such as navigation."}, {"heading": "Acknowledgements", "text": "We thank Xiaochen Lian, Zhuoyuan Chen, Yi Yang and Qing Sun for their discussions and comments."}], "references": [{"title": "Early language acquisition: cracking the speech code", "author": ["P.K. Kuhl"], "venue": "Nat Rev Neurosci,", "citeRegEx": "Kuhl.,? \\Q2004\\E", "shortCiteRegEx": "Kuhl.", "year": 2004}, {"title": "Adversarial learning for neural dialogue generation", "author": ["2017a. J. Li", "W. Monroe", "T. Shi", "A. Ritter", "D. Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Reinforcement contingencies in language acquisition. Policy Insights", "author": ["A.I. Petursdottir", "J.R. Mellor"], "venue": null, "citeRegEx": "Petursdottir and Mellor.,? \\Q2017\\E", "shortCiteRegEx": "Petursdottir and Mellor.", "year": 2017}, {"title": "Building end-to-end dialogue systems", "author": ["ICLR", "2016. I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau"], "venue": null, "citeRegEx": "ICLR et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ICLR et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "2016", "shortCiteRegEx": "2016", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "One of the long-term goals of artificial intelligence is to build an agent that can communicate intelligently with human in natural language. Most existing work on natural language learning relies heavily on training over a pre-collected dataset with annotated labels, leading to an agent that essentially captures the statistics of the fixed external training data. As the training data is essentially a static snapshot representation of the knowledge from the annotator, the agent trained this way is limited in adaptiveness and generalization of its behavior. Moreover, this is very different from the language learning process of humans, where language is acquired during communication by taking speaking action and learning from the consequences of speaking action in an interactive manner. This paper presents an interactive setting for grounded natural language learning, where an agent learns natural language by interacting with a teacher and learning from feedback, thus learning and improving language skills while taking part in the conversation. To achieve this goal, we propose a model which incorporates both imitation and reinforcement by leveraging jointly sentence and reward feedbacks from the teacher. Experiments are conducted to validate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}