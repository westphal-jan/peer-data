{"id": "1705.09922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Bayesian Unification of Gradient and Bandit-based Learning for Accelerated Global Optimisation", "abstract": "Bandit based optimisation has a remarkable advantage over gradient based approaches due to their global perspective, which eliminates the danger of getting stuck at local optima. However, for continuous optimisation problems or problems with a large number of actions, bandit based approaches can be hindered by slow learning. Gradient based approaches, on the other hand, navigate quickly in high-dimensional continuous spaces through local optimisation, following the gradient in fine grained steps. Yet, apart from being susceptible to local optima, these schemes are less suited for online learning due to their reliance on extensive trial-and-error before the optimum can be identified. In this paper, we propose a Bayesian approach that unifies the above two paradigms in one single framework, with the aim of combining their advantages. At the heart of our approach we find a stochastic linear approximation of the function to be optimised, where both the gradient and values of the function are explicitly captured. This allows us to learn from both noisy function and gradient observations, and predict these properties across the action space to support optimisation. We further propose an accompanying bandit driven exploration scheme that uses Bayesian credible bounds to trade off exploration against exploitation. Our empirical results demonstrate that by unifying bandit and gradient based learning, one obtains consistently improved performance across a wide spectrum of problem environments. Furthermore, even when gradient feedback is unavailable, the flexibility of our model, including gradient prediction, still allows us outperform competing approaches, although with a smaller margin. Due to the pervasiveness of bandit based optimisation, our scheme opens up for improved performance both in meta-optimisation and in applications where gradient related information is readily available.", "histories": [["v1", "Sun, 28 May 2017 09:55:11 GMT  (90kb,D)", "http://arxiv.org/abs/1705.09922v1", "15th IEEE International Conference on Machine Learning and Applications (ICMLA 2016)"]], "COMMENTS": "15th IEEE International Conference on Machine Learning and Applications (ICMLA 2016)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["ole-christoffer granmo"], "accepted": false, "id": "1705.09922"}, "pdf": {"name": "1705.09922.pdf", "metadata": {"source": "CRF", "title": "Bayesian Unification of Gradient and Bandit-based Learning for Accelerated Global Optimisation", "authors": ["Ole-Christoffer Granmo"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Background and Motivation", "text": "The Multi-Arm Bandit Problem is a classic optimization problem that captures the trade between exploitation and exploration in the attachment of learning. The problem consists of an agent who sequentially pulls one of several arms attached to a slot machine, with each move leading to a scalar reward. Each reward is randomly drawn from an unknown distribution unique to each arm, the purpose of which is to identify the arm with the highest expected reward as quickly as possible, through targeted trials and errors. Bandit-based optimization programs have a huge advantage over gradient-based approaches (such as [1]) due to their global perspective, which eliminates the danger of sticking to local optimization problems or problems with a large number of weapons (actions), bandit-based optimization programs are hampered by their inability to generalize weapons as independent reward sources. This independence function leads to slow-down learning, because the expected reward function must be independent of each other."}, {"heading": "1.2 Paper Contributions and Outline", "text": "In this paper, we propose a radically new approach to stochastic optimization, combining the global perspective of multi-armed bandits with gradient-based local optimization, with the effect of significantly accelerating learning. In a nutshell, our approach offers a Bayesian unification of gradient and bandit-based learning (hereinafter referred to as BUG-B. Our contributions can be summarized as follows: \u2022 At the core of BUG-B, we find a novel Bayesian model that explicitly links the expected reward function to its gradient. \u2022 The model supports learning from both noisy functional values and gradient-related observations; furthermore, unobserved functional values and gradient information can be predicted throughout the action space to support targeted exploration and exploitation of the reward function. \u2022 In addition, we propose an accompanying bandit-driven exploration program that sets credible boundaries against exploration."}, {"heading": "2 Bayesian Unification of Gradient and Bandit-based Learning", "text": "(BUG-B)"}, {"heading": "2.1 The BUG-B Model", "text": "The BUG-B model is based on a linear approximation of the expected reward function f (xi) using a grid of input values xi (x0, x1,.., xN). This paper focuses on one-dimensional cases. The approximation then assumes the following recursive form: f (xi) = f (xi \u2212 1) + f (xi \u2212 1) \u00b7 (xi \u2212 1), for i (xi \u2212 1), N}. As shown in Fig. 2, for each input value xi, the function value f (xi \u2212 1) becomes a constant with respect to the gradient f (xi \u2212 1) and the function value f (xi \u2212 1), the preceding point xi \u2212 1. Note: For x0, f (x0), the function value f (x0) becomes a constant."}, {"heading": "2.2 Optimization Strategies with Thompson Sampling and Upper Confidence Intervals", "text": "From a relatively broad perspective, there are currently two competing strategies for finding the global optimum in the bandit setting: Thompson sampling (stochastic probability matching schemes) and those based on upper confidence (or credibility) bounds. Thompson sampling tends to perform better than UCB-based approaches in empirical studies, but is known to be too much to research. UCB-like approaches, on the other hand, offer a deterministic and purposeful path toward the global optimum, with the optimum likely to be found arbitrarily close to unity. Thompson sampling, on the other hand, always approaches the global optimum (with unity probability). [5] We proposed a Bayesian method for solving bandit-like problems, similar to the Thompson Sampling [8] principle, which leads to novel schemes for dealing with multi-armed and non-stationary (restless) bandit problems. Empirical results demonstrated the advantages of these techniques over established top performers."}, {"heading": "3 Empirical Results", "text": "In this section, we evaluate the BUG-B scheme by comparing it with the currently best performing approaches. By comparing it with these \"reference algorithms,\" it should be quite easy to relate the BUG-B performance results to the performance of other similar algorithms."}, {"heading": "3.1 Experimental Setup", "text": "We have performed numerous experiments using different functions and generated artificial data under different degrees of observation noise. The entire bandwidth of empirical results shows the same trend, but we report here on the performance of a representative subset of experimental configurations, which includes unimodal and multimodal functions with different degrees of noise and resolution. Performance is measured in repentance - the difference between the sum of the results reported according to N expected rewards and what would have been achieved by constant selection of the optimal point. For these experimental configurations, an interaction of 1000 independent replications with different random number flows was performed to minimize the variance of reported results. In order to investigate the performance of the schemes under a wide spectrum of environments, we tested the schemes with three different representative functions - an inclined, with a single maxima, and another with multiple local maxima, in particular similar to the global maxima. To study the performance of the schemes under a different degree of noise, we performed a 0.001, with a different degree of observation."}, {"heading": "3.2 Comparison of Regret", "text": "In this case, the sum of the amounts received is to be compared with the value of 0. This means that the amounts received are associated with the value of 0. It means that the amounts received are set off against the amounts received in the amounts received. It is the case that the amounts received are set off against the amounts received."}, {"heading": "4 Conclusions and Further Work", "text": "Our Bayesian model, BUG-B, combines the two paradigms in an integrated model. At the core of the model, we find a stochastic linear approach to the function to be optimized, where both gradient and function values are explicitly linked, allowing us to learn from both noisy function and gradient observations, as well as to predict these properties throughout the action space to support optimization. We also proposed an accompanying band-driven exploration scheme that uses Bayesian credibility limits to balance exploration against exploitation. Our empirical results showed that unifying bandit and gradient-based learning in a wide range of environments can consistently improve performance. Moreover, even if gradient feedback is not available, the flexibility of our model, including gradient prediction, can help to compete with both the availability of data and the availability of competitive approaches, even though based on a lower degree of availability."}], "references": [{"title": "Gradient-Based Optimization of Hyperparameters", "author": ["Y. Bengio"], "venue": "Neural Computation, vol. 12, pp. 1889 \u2013 1900, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1889}, {"title": "Global Multi-armed Bandits with Holder Continuity", "author": ["O. Atan", "C. Tekin", "M. v.d. Schaar"], "venue": "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), 2015, pp. 28 \u2013 36.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "X-Armed Bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesvari"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 1655 \u2013 1695, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M.W. Seeger"], "venue": "IEEE Transactions on Information Theory, vol. 58, pp. 3250 \u2013 3265, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian Learning Automaton", "author": ["O.-C. Granmo"], "venue": "International Journal of Intelligent Computing and Cybernetics, vol. 3, no. 2, pp. 207\u2013234, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "A modern Bayesian look at the multi-armed bandit", "author": ["S.L. Scott"], "venue": "Applied Stochastic Models in Business and Industry, no. 26, pp. 639\u2013658, 2010. 7", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimistic Bayesian sampling in contextual-bandit problems", "author": ["B.C. May", "N. Korda", "A. Lee", "D.S. Leslie"], "venue": "Submitted to the Annals of Applied Probability, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, vol. 25, pp. 285\u2013294, 1933.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1933}, {"title": "A Generic Solution to Multi-Armed Bernoulli Bandit Problems Based on Random Sampling from Sibling Conjugate Priors", "author": ["T. Norheim", "T. Br\u030aadland", "O.-C. Granmo", "B.J. Oommen"], "venue": "Proceedings of the Second International Conference on Agents and Artificial Intelligence (ICAART 2010). INSTICC, 2010, pp. 36\u201344.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Solving Non-Stationary Bandit Problems by Random Sampling from Sibling Kalman Filters", "author": ["O.-C. Granmo", "S. Berg"], "venue": "Proceedings of the Twenty Third International Conference on Industrial, Engineering, and Other Applications of Applied Intelligent Systems (IEA-AIE 2010). Springer, 2010, pp. 199\u2013208.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-armed bandit algorithms and empirical evaluation", "author": ["J. Vermorel", "M. Mohri"], "venue": "Proceedings of ECML 2005. Springer, 2005, pp. 437\u2013448.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning in embedded systems", "author": ["L.P. Kaelbling"], "venue": "Ph.D. dissertation, Stanford University, 1993.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, vol. 47, pp. 235\u2013256, 2002. 8", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Bandit based optimisation schemes have a tremendous advantage over gradient based approaches (such as [1]) due to their global perspective, which eliminates the danger of getting stuck at local optima.", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "One class of schemes, referred to as global multi-armed bandit schemes, models the expected rewards of the arms as (non-)linear functions of a global parameter \u03b3 [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "Another family of techniques attacks large action spaces through tree based searching, with X-Armed Bandits finding global maxima when the expected reward (objective) function is \u201dlocally Lipschitz\u201d [3].", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "Finally, Gaussian processes have been applied for smoothing and interpolation, forming the foundation for bandit based exploration and exploitation in continuous action spaces [4].", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "Note that BUG-B also lends itself towards so-called Thompson Sampling [5\u20138] due to its Bayesian nature.", "startOffset": 70, "endOffset": 75}, {"referenceID": 5, "context": "Note that BUG-B also lends itself towards so-called Thompson Sampling [5\u20138] due to its Bayesian nature.", "startOffset": 70, "endOffset": 75}, {"referenceID": 6, "context": "Note that BUG-B also lends itself towards so-called Thompson Sampling [5\u20138] due to its Bayesian nature.", "startOffset": 70, "endOffset": 75}, {"referenceID": 7, "context": "Note that BUG-B also lends itself towards so-called Thompson Sampling [5\u20138] due to its Bayesian nature.", "startOffset": 70, "endOffset": 75}, {"referenceID": 4, "context": "[5] In [5] we proposed a Bayesian technique for solving bandit like problems, akin to the Thompson Sampling [8] principle, leading to novel schemes for handling multi-armed and non-stationary (restless) bandit problems [9,10].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] In [5] we proposed a Bayesian technique for solving bandit like problems, akin to the Thompson Sampling [8] principle, leading to novel schemes for handling multi-armed and non-stationary (restless) bandit problems [9,10].", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "[5] In [5] we proposed a Bayesian technique for solving bandit like problems, akin to the Thompson Sampling [8] principle, leading to novel schemes for handling multi-armed and non-stationary (restless) bandit problems [9,10].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "[5] In [5] we proposed a Bayesian technique for solving bandit like problems, akin to the Thompson Sampling [8] principle, leading to novel schemes for handling multi-armed and non-stationary (restless) bandit problems [9,10].", "startOffset": 219, "endOffset": 225}, {"referenceID": 9, "context": "[5] In [5] we proposed a Bayesian technique for solving bandit like problems, akin to the Thompson Sampling [8] principle, leading to novel schemes for handling multi-armed and non-stationary (restless) bandit problems [9,10].", "startOffset": 219, "endOffset": 225}, {"referenceID": 5, "context": "Later on, as a further testimony to the renewed importance of the Thompson Sampling principle, a modern Bayesian look at the multi-armed bandit problem was also taken in [6, 7].", "startOffset": 170, "endOffset": 176}, {"referenceID": 6, "context": "Later on, as a further testimony to the renewed importance of the Thompson Sampling principle, a modern Bayesian look at the multi-armed bandit problem was also taken in [6, 7].", "startOffset": 170, "endOffset": 176}, {"referenceID": 10, "context": "The arm with the most optimistic reward probability estimate is then greedily selected [11,12].", "startOffset": 87, "endOffset": 94}, {"referenceID": 11, "context": "The arm with the most optimistic reward probability estimate is then greedily selected [11,12].", "startOffset": 87, "endOffset": 94}, {"referenceID": 12, "context": "In [13], the authors analysed several confidence interval based algorithms.", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "Bandit based optimisation schemes have a remarkable advantage over gradient based approaches due to their global perspective, which eliminates the danger of getting stuck at local optima. However, for continuous optimisation problems or problems with a large number of actions, bandit based approaches can be hindered by slow learning. Gradient based approaches, on the other hand, navigate quickly in high-dimensional continuous spaces through local optimisation, following the gradient in fine grained steps. However, apart from being susceptible to local optima, these schemes are also less suited for online learning due to their reliance on extensive trial-and-error before the optimum can be identified. In contrast, bandit algorithms seek to identify the optimal action (global optima) in as few steps as possible. In this paper, we propose a Bayesian approach that unifies the above two distinct paradigms in one single framework, with the aim of combining their advantages. At the heart of our approach we find a stochastic linear approximation of the function to be optimised, where both the gradient and values of the function are explicitly captured. This model allows us to learn from both noisy function and gradient observations, as well as predicting these properties across the action space to support optimisation. We further propose an accompanying bandit driven exploration scheme that uses Bayesian credible bounds to trade off exploration against exploitation. Our empirical results demonstrate that by unifying bandit and gradient based learning, one obtains consistently improved performance across a wide spectrum of problem environments. Furthermore, even when gradient feedback is unavailable, the flexibility of our model, including gradient prediction, still allows us outperform competing approaches, although with a smaller margin. Due to the pervasiveness of bandit based optimisation, our scheme opens up for improved performance both in meta-optimisation and in applications where gradient related information is readily available.", "creator": "LaTeX with hyperref package"}}}