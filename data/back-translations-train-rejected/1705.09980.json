{"id": "1705.09980", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations", "abstract": "We evaluate the character-level translation method for neural semantic parsing on a large corpus of sentences annotated with Abstract Meaning Representations (AMRs). Using a seq2seq model, and some trivial preprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1 (F-score on AMR-triples). We examine four different approaches to improve this baseline result: (i) reordering AMR branches to match the word order of the input sentence increases performance to 58.3; (ii) adding part-of-speech tags (automatically produced) to the input shows improvement as well (57.2); (iii) So does the introduction of super characters (conflating frequent sequences of characters to a single character), reaching 57.4; (iv) adding silver-standard training data obtained by an off-the-shelf parser yields the biggest improvement, resulting in an F-score of 64.0. Combining all four techniques leads to an F-score of 69.0, which is state-of-the-art in AMR parsing. This is remarkable because of the relatively simplicity of the approach: the only explicit linguistic knowledge that we use are part-of-speech tags.", "histories": [["v1", "Sun, 28 May 2017 19:41:09 GMT  (59kb,D)", "http://arxiv.org/abs/1705.09980v1", "In review for CLIN Journal"], ["v2", "Mon, 9 Oct 2017 08:30:33 GMT  (30kb)", "http://arxiv.org/abs/1705.09980v2", "Camera ready for CLIN 2017 journal"]], "COMMENTS": "In review for CLIN Journal", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rik van noord", "johan bos"], "accepted": false, "id": "1705.09980"}, "pdf": {"name": "1705.09980.pdf", "metadata": {"source": "CRF", "title": "Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations", "authors": ["Rik van Noord", "Johan Bos"], "emails": ["r.i.k.van.noord@rug.nl", "johan.bos@rug.nl"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that we are able to go in search of a solution that meets people's needs. (...) In fact, it is the case that we are able to find a solution that meets people's needs. (...) In fact, it is the case that we are able to find a solution that meets people's needs. (...) In fact, it is the case that we are able to find a solution that meets people's needs. (...)"}, {"heading": "2. Method and Data", "text": "We will first give some background information on AMRs. Then we will outline the basic ideas of the character-based translation model with English sentences as input and AMRs as output. We will then establish a base system with the aim of improving it in the next section."}, {"heading": "2.1 Abstract Meaning Representations", "text": "In our experiments with neural semantic analysis, we focus on parsing Abstract Meaning Representations (AMRs). AMRs were introduced by Banarescu et al. (2013) and are acyclic, directional graphs that represent the meaning of a sentence. In fact, there are three ways to display an AMR: as a graph, as a set of triples, or as a tree. An example of an AMR is shown in Figure 1, here is the format used in the commented enterprises. (a / effect-01An AMR consists of concepts associated with variable names, with a slash."}, {"heading": "2.2 The Basic Translation Model", "text": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017). Unlike Peng et al. (2017) and Konstas et al. (2017), which use word-level input, we use character-level input because we believe it is more generalized and therefore one of the most promising trends in machine translation (Chung et al. 2016). We train a model with bidirectional coding and general attention (Luong et al. 2015). Since training a full model takes two to three days on a GPU, we perform a heuristic parameter search instead of an exhaustive one. Parameter settings are optimized based on the development group and the final values are presented in Table 1. All models described in this paper are trained with these settings. Training is interrupted for three periods after there is no improvement in validation confusion."}, {"heading": "2.3 Postprocessing and Restoring Information", "text": "The output of the seq2seq model is, of course, an AMR with no variables, no wiki links, and no secondary variables. Furthermore, because of the character-based seq2seq model, it may well be that there are brackets in the output that do not match, or that some nodes representing concepts are incomplete, which obviously needs fixing. First, the variables in the AMRs are restored. We also try to repair invalid AMRs by applying a few heuristics, such as inserting brackets and quotations, or simply removing unfinished nodes, using a slightly modified version of the Barzdins and Gosko restoration script (2016), and then applying four methods to improve the quality of the AMRs. They are described below."}, {"heading": "2.3.1 Pruning", "text": "One problem with many in-depth learning approaches is that the decoder does not keep what it has already produced. As a result, we sometimes end up with duplicated, redundant material in our generation of AMRs, which is detrimental to precision. Therefore, we remove duplicated branches if they are related to the same kinship / concept pair. Duplicate material that occurs in different AMRs is removed the second time it is encountered. This process is a trade-off: Usually, duplicates are correctly recognized as redundant and can be removed, but sometimes duplicates that we erroneously remove should remain. Two examples of AMRs whose branches are trimmed are shown in Figure 3. (Material)"}, {"heading": "2.3.2 Sense Validation", "text": "This means that our generic seq2seq model must learn the corresponding sense per verb. As a result, the model sometimes outputs nodes that are impossible, either by generating a sense that is not within the PropBank framework used by AMRs, or by creating a combination of verbal sense and arguments that are not permitted within that framework. Thus, the verbal break-out can make sense 06, 07, and 10. When we encounter a different sense of break-out, we know that the model has created a false node and try to replace it with a possibly correct one. All of these senses can have different types of arguments with which they may occur (e.g. break-out-06 with: ARG0 and: ARG1, but break-out-07 only with: ARG1), making it a complicated matter. Our method of changing senses and arguments is illustrated in Figure 4."}, {"heading": "2.3.3 Wikification", "text": "We restore wiki links in the AMR output using a standard system (Daiber et al. 2013) that follows the method presented by Bjerva et al. (2016). You look at the: Name relationships in anAMR and try to find that name on Wikipedia. If it has a page, the corresponding link is added; otherwise, the AMR remains unchanged."}, {"heading": "2.3.4 Restoring co-referring nodes", "text": "When we output a double node (a node that has already been output for this AMR), it replaces the node with the variable name of the node first encountered. This can only be done once per unique node, as the third instance of such a node is already removed in the truncation phase."}, {"heading": "2.4 Baseline Results", "text": "Our first goal was to reproduce the results achieved by Barzdins and Gosko (2016), with an F score of 53.0 (see Table 2). Compared to the F score of Barzdins and Gosko (2016), our score is significantly higher, probably due to the higher amount of training data and the use of different seq2seq software. As shown in Table 2, concept circumcision, restoration of correlation variables, and wikification each increase the F score by about one percentage point. This small increase in performance is what one should expect, as each operation has little impact on the overall content of an AMR."}, {"heading": "3. Improving the Basic Translation Model", "text": "In the previous section, we outlined our basic method of generating AMRs using a seq2seq model based on characters. In this section, we will look at four different techniques to go beyond the F score that we achieve with our basic method, which we consider to be the baseline in this section."}, {"heading": "3.1 AMR Reshuffling", "text": "Although AMRs are by definition disordered, there is an order of branches in our textual representation of AMRs. However, these branches do not necessarily follow the sequence of words in the corresponding English sentence. It has been shown that the rearrangement of (statistical) machine translation improves the translation quality (Collins et al. 2005). We use the provided alignments to permutate the AMR in a way that best matches the sequence of words. We do this both at the subtree level and at the node level. An example of an AMR with a branch order that best matches the input sentence is shown in Figure 1 (Material). We are also able to use this approach to supplement the training data, as each rearrangement of the AMR provides us with a new AMR set pair. Due to the exponential increase, large AMRs often have thousands of possible orders. We conducted a series of experiments to find out through this data overload how we can best use them."}, {"heading": "3.2 Introducing Super Characters", "text": "We are not necessarily limited to using only characters as input. So, for example, we can consider the AMR relationships (e.g.: ARG0,: name) as atomic and not as a set of characters. In this way, we create a hybrid model that represents a combination of word and character input. An example of AMR input at the supercharacter level is in Figure 6. We also expand the set of supersigns outside the relationships by automatically finding AMR concepts that rarely match a word in the English sentence. This sentence includes chunks such as government organization, date unit, and time set. We are conducting two experiments, one with supercharacter relationships only and one with the extended supercharacter set. 21. Instead of ordering the AMR nodes that are reflected in the word order of the sentence, we have also tried two different experiments that are based on consistency. The first experiment simply ordered the nodes alphabetically, without other influences, we reduced the result of our 2.0 model to focus on two different ones."}, {"heading": "3.3 Adding Part-of-Speech Information", "text": "To demonstrate this, we analyze the sentences with the C & C Tools POS Tagger (Clark et al. 2003) using the Penn POS Tagset. Each tag is represented as a single character and placed after the last character representation of the word corresponding to the tag (see Figure 6). In other words, we create a new supercharacter for each single tag and add it to the input record, which both increases the size of the input and adds a lot of general, potentially useful information with just a single character."}, {"heading": "3.4 Adding Silver Standard Data", "text": "One problem with neural parsing approaches is data economy, as a lot of manual effort is required to generate the gold standard data. Peng et al. (2017) tries to overcome this by generalizing the training data comprehensively, but does not have nearly state-of-the-art results. Konstas et al. (2017) uses the GigaWord corpus to train their own system, using their own upstream parsers to analyze the previously invisible sentences and add them to the training data in a series of iterations. Ultimately, their system is trained on 20 million additional data AMR sentence pairs and receives an F score of 62.1. Without this additional data, they only achieve a score of 55.5, which is better than Peng et al. (2017), but not close to state-of-the-art performance. Our method differs from Konstas et al. (2017) in three ways: (i) to prepare the training data directly, rather than adding it to the training data."}, {"heading": "4. Results and Discussion", "text": "In fact, most of them will be able to abide by the rules they have established with regard to their country."}, {"heading": "5. Conclusion and Future Work", "text": "Reordering the AMR branches, introducing superfigures, and adding POS tags are therefore techniques that greatly improve neural AMR analysis using a character-based seq2seq model. However, the biggest increase in performance is triggered by the addition of a large amount of standard silver AMRs produced by existing (traditional) parsers, which is in line with the results of Konstas et al. (2017), who have used the Gigaword corpus to obtain additional training data, although their training method differs from ours. The results obtained are promising. Our best model, with an F score of 69.0, exceeds all previously published results on AMR analysis. This is noteworthy because traditional approaches are often based on extensive, manually crafted lexicon with linguistic knowledge. Of course, it should be noted that we use some linguistic knowledge in the form of POS tags in our best models."}, {"heading": "Acknowledgements", "text": "First of all, we would like to thank Antonio Toral and Lasha Abzianidze for their helpful discussions on neural AMR parsing and machine translation. We would also like to thank the Centre for Information Technology of the University of Groningen for their support and for providing access to the Peregrine High Performance Computing Cluster. Furthermore, we used a Tesla K40 GPU, kindly provided to us by the NVIDIA Corporation. This work was financed by the NWO-VICI scholarship Lost in Translation Found in Meaning (288-89-003)."}], "references": [{"title": "The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations", "author": ["Abzianidze", "Lasha", "Johannes Bjerva", "Kilian Evang", "Hessel Haagsma", "Rik van Noord", "Pierre Ludmann", "Duc-Duy Nguyen", "Johan Bos"], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume", "citeRegEx": "Abzianidze et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Abzianidze et al\\.", "year": 2017}, {"title": "Broad-coverage ccg semantic parsing with amr", "author": ["Artzi", "Yoav", "Kenton Lee", "Luke Zettlemoyer"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Artzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, Association for Computational Linguistics, Sofia, Bulgaria", "author": ["Banarescu", "Laura", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Developing a large semantically annotated corpus", "author": ["Basile", "Valerio", "Johan Bos", "Kilian Evang", "Noortje"], "venue": "Venhuizen", "citeRegEx": "Basile et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Basile et al\\.", "year": 2012}, {"title": "The meaning factory at semeval-2016 task 8: Producing amrs with boxer", "author": ["Bjerva", "Johannes", "Johan Bos", "Hessel Haagsma"], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),", "citeRegEx": "Bjerva et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bjerva et al\\.", "year": 2016}, {"title": "Open-domain semantic parsing with boxer", "author": ["Bos", "Johan"], "venue": "Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA", "citeRegEx": "Bos and Johan,? \\Q2015\\E", "shortCiteRegEx": "Bos and Johan", "year": 2015}, {"title": "Wide-coverage semantic representations from a ccg parser", "author": ["Bos", "Johan", "Stephen Clark", "Mark Steedman", "James R. Curran", "Julia Hockenmaier"], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING \u201904),", "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Smatch: an evaluation metric for semantic feature structures, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "author": ["Cai", "Shu", "Kevin Knight"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Nyu-mila neural machine translation systems for wmt16", "author": ["Chung", "Junyoung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Bootstrapping pos taggers using unlabelled data, Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, Association for Computational Linguistics", "author": ["Clark", "Stephen", "James R Curran", "Miles Osborne"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2003}, {"title": "Clause restructuring for statistical machine translation, Proceedings of the 43rd annual meeting on association for computational linguistics", "author": ["Collins", "Michael", "Philipp Koehn", "Ivona Ku\u010derov\u00e1"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Collins et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Daiber", "Joachim", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes"], "venue": "Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)", "citeRegEx": "Daiber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "An incremental parser for abstract meaning representation, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, Association for Computational Linguistics", "author": ["Damonte", "Marco", "Shay B. Cohen", "Giorgio Satta"], "venue": null, "citeRegEx": "Damonte et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Damonte et al\\.", "year": 2017}, {"title": "Language to logical form with neural attention, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin", "author": ["Dong", "Li", "Mirella Lapata"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2016}, {"title": "Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss", "author": ["Flanigan", "Jeffrey", "Chris Dyer", "Noah A Smith", "Jaime Carbonell"], "venue": "Proceedings of SemEval pp", "citeRegEx": "Flanigan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "A discriminative graph-based parser for the abstract meaning representation, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Flanigan", "Jeffrey", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Flanigan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Opennmt: Open-source toolkit for neural machine translation, arXiv preprint arXiv:1701.02810", "author": ["Klein", "Guillaume", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M Rush"], "venue": null, "citeRegEx": "Klein et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Neural amr: Sequence-to-sequence models for parsing and generation, arXiv preprint (accepted in ACL-2017) arXiv:1704.08381", "author": ["Konstas", "Ioannis", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer"], "venue": null, "citeRegEx": "Konstas et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Konstas et al\\.", "year": 2017}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong", "Thang", "Hieu Pham", "Christopher D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Semeval-2016 task 8: Meaning representation parsing", "author": ["May", "Jonathan"], "venue": "Proceedings of SemEval pp", "citeRegEx": "May and Jonathan,? \\Q2016\\E", "shortCiteRegEx": "May and Jonathan", "year": 2016}, {"title": "Addressing the data sparsity issue in neural amr parsing, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, Association for Computational Linguistics", "author": ["Peng", "Xiaochang", "Chuan Wang", "Daniel Gildea", "Nianwen Xue"], "venue": null, "citeRegEx": "Peng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "A transition-based algorithm for amr parsing, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association", "author": ["Wang", "Chuan", "Nianwen Xue", "Sameer Pradhan"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Research in this area comprises Bos et al. (2004), Bos (2015), Butler (2010), and Artzi et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 5, "context": "Research in this area comprises Bos et al. (2004), Bos (2015), Butler (2010), and Artzi et al.", "startOffset": 32, "endOffset": 62}, {"referenceID": 5, "context": "Research in this area comprises Bos et al. (2004), Bos (2015), Butler (2010), and Artzi et al.", "startOffset": 32, "endOffset": 77}, {"referenceID": 1, "context": "(2004), Bos (2015), Butler (2010), and Artzi et al. (2015). But recently there have been interesting attempts to view semantic parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method.", "startOffset": 39, "endOffset": 59}, {"referenceID": 1, "context": "(2004), Bos (2015), Butler (2010), and Artzi et al. (2015). But recently there have been interesting attempts to view semantic parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method. Dong and Lapata (2016) used sequence-to-sequence (seq2seq) and sequence-totree (seq2tree) neural translation models to produce logical forms from sentences for four different datasets (but not AMRs).", "startOffset": 39, "endOffset": 279}, {"referenceID": 1, "context": "(2004), Bos (2015), Butler (2010), and Artzi et al. (2015). But recently there have been interesting attempts to view semantic parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method. Dong and Lapata (2016) used sequence-to-sequence (seq2seq) and sequence-totree (seq2tree) neural translation models to produce logical forms from sentences for four different datasets (but not AMRs). Barzdins and Gosko (2016) used a similar method to produce Abstract Meaning Representations (AMRs) in the context of the shared task on semantic parsing at SemEval2016 (May 2016).", "startOffset": 39, "endOffset": 482}, {"referenceID": 2, "context": "AMRs were introduced by Banarescu et al. (2013) and are acyclic, directed graphs that represent the meaning of a sentence.", "startOffset": 24, "endOffset": 48}, {"referenceID": 16, "context": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017).", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "(2017), who use word-level input, we use character-level input, because we believe it generalizes better and it is therefore one of the promising trends in machine translation (Chung et al. 2016).", "startOffset": 176, "endOffset": 195}, {"referenceID": 18, "context": "We train a model with bidirectional encoding and general attention (Luong et al. 2015).", "startOffset": 67, "endOffset": 86}, {"referenceID": 15, "context": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017). In contrast to Peng et al. (2017) and Konstas et al.", "startOffset": 81, "endOffset": 135}, {"referenceID": 15, "context": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017). In contrast to Peng et al. (2017) and Konstas et al. (2017), who use word-level input, we use character-level input, because we believe it generalizes better and it is therefore one of the promising trends in machine translation (Chung et al.", "startOffset": 81, "endOffset": 161}, {"referenceID": 11, "context": "We restore wiki links in the output AMR by using an off-the-shelf system (Daiber et al. 2013), following the method presented by Bjerva et al.", "startOffset": 73, "endOffset": 93}, {"referenceID": 4, "context": "2013), following the method presented by Bjerva et al. (2016). They look at the :name relations in an", "startOffset": 41, "endOffset": 62}, {"referenceID": 10, "context": "It has been shown that for (statistical) machine translation reordering improves translation quality (Collins et al. 2005).", "startOffset": 101, "endOffset": 122}, {"referenceID": 9, "context": "To show this, we parse the sentences with the POS-tagger of the C&C tools (Clark et al. 2003), employing the Penn POS tagset.", "startOffset": 74, "endOffset": 93}, {"referenceID": 21, "context": "Instead of self-training our parser, we use the off-the-shelf AMR parsers CAMR (Wang et al. 2015) and JAMR (Flanigan et al.", "startOffset": 79, "endOffset": 97}, {"referenceID": 15, "context": "2015) and JAMR (Flanigan et al. 2014) to create silver standard data for our system.", "startOffset": 15, "endOffset": 37}, {"referenceID": 3, "context": "We parse 1,303,419 sentences from the Groningen Meaning Bank (Basile et al. 2012), which mainly consists of newswire text.", "startOffset": 61, "endOffset": 81}, {"referenceID": 16, "context": "Peng et al. (2017) tried to overcome this by extensive generalization of the training data, but did not get near state-of-the-art results.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Konstas et al. (2017) used the GigaWord corpus to self-train their system.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Konstas et al. (2017) used the GigaWord corpus to self-train their system. They use their own pre-trained parser to parse the previously unseen sentences and add those to the training data in a series of iterations. Ultimately, their system is trained on 20 million additional data AMR-sentence pairs and obtains an F-score of 62.1. Without this additional data, they only score 55.5, which is better than Peng et al. (2017), but not close to state-of-the-art performance.", "startOffset": 0, "endOffset": 425}, {"referenceID": 14, "context": "Konstas et al. (2017) used the GigaWord corpus to self-train their system. They use their own pre-trained parser to parse the previously unseen sentences and add those to the training data in a series of iterations. Ultimately, their system is trained on 20 million additional data AMR-sentence pairs and obtains an F-score of 62.1. Without this additional data, they only score 55.5, which is better than Peng et al. (2017), but not close to state-of-the-art performance. Our method differs from Konstas et al. (2017) in three ways: (i) we directly add new data to the training data instead of using it to pre-train the model; (ii) we use two off-the-shelf parsers to create the training data instead of self-training; (iii) we employ a method to exclude lower-quality AMRs instead of using all available data.", "startOffset": 0, "endOffset": 519}, {"referenceID": 17, "context": "5) of Konstas et al. (2017). Table 5 shows the results of the most notable previous AMR parsing systems.", "startOffset": 6, "endOffset": 28}, {"referenceID": 11, "context": "0 Damonte et al. (2017) AMR-eager LDC2015E86 64.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.", "startOffset": 2, "endOffset": 22}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.3 Wang et al. (2015) CAMR LDC2015E86 66.", "startOffset": 2, "endOffset": 69}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.3 Wang et al. (2015) CAMR LDC2015E86 66.5 Flanigan et al. (2016) JAMR-16 LDC2015E86 67.", "startOffset": 2, "endOffset": 113}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.3 Wang et al. (2015) CAMR LDC2015E86 66.5 Flanigan et al. (2016) JAMR-16 LDC2015E86 67.0 Pust et al. (2015) SBMT LDC2015E86 67.", "startOffset": 2, "endOffset": 156}, {"referenceID": 19, "context": "0 Peng et al. (2017) word-based seq2seq LDC2015E86 52.", "startOffset": 2, "endOffset": 21}, {"referenceID": 17, "context": "0 Konstas et al. (2017) word-based seq2seq LDC2015E86 55.", "startOffset": 2, "endOffset": 24}, {"referenceID": 17, "context": "0 Konstas et al. (2017) word-based seq2seq LDC2015E86 55.5 Konstas et al. (2017) word-based seq2seq + giga LDC2015E86 62.", "startOffset": 2, "endOffset": 81}, {"referenceID": 12, "context": "Table 6: Comparison with previous parsers using the evaluation script of Damonte et al. (2017). We also included precision and recall scores for our system.", "startOffset": 73, "endOffset": 95}, {"referenceID": 17, "context": "This is in line with the findings of Konstas et al. (2017), who used the Gigaword corpus to get extra training data, although their training method is different from ours.", "startOffset": 37, "endOffset": 59}, {"referenceID": 17, "context": "This is in line with the findings of Konstas et al. (2017), who used the Gigaword corpus to get extra training data, although their training method is different from ours. The obtained results are promising. Our best model, with an F-score of 69.0, outperformed any known previously published result on AMR parsing. This is remarkable, for traditional approaches are often based on extensive, manually crafted lexicons using linguistic knowledge. It should be noted, of course, that we use some linguistic knowledge in the form of POS-tags in our best models. This can only mean that we can expect further improvements in neural semantic parsing when adding further linguistic knowledge. One could consider the use of silver standard AMR data as a disadvantage, as there is still a need of an existing high-quality AMR parser to get the silver data in the first place. In our approach we rely even on two different off-the-shelf parsers. It would therefore be interesting to explore other opportunities, such as self-learning, as proposed by Konstas et al. (2017). We have the feeling that there are still a lot of techniques that one could try to increase the performance of neural AMR parsing.", "startOffset": 37, "endOffset": 1064}, {"referenceID": 3, "context": "It would be challenging to transfer the techniques of neural semantic parsing to scoped meaning representations, such as those used in the Groningen Meaning Bank (Basile et al. 2012) or the Parallel Meaning Bank (Abzianidze et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 0, "context": "2012) or the Parallel Meaning Bank (Abzianidze et al. 2017).", "startOffset": 35, "endOffset": 59}], "year": 2017, "abstractText": "We evaluate the character-level translation method for neural semantic parsing on a large corpus of sentences annotated with Abstract Meaning Representations (AMRs). Using a seq2seq model, and some trivial preprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1 (F-score on AMR-triples). We examine four different approaches to improve this baseline result: (i) reordering AMR branches to match the word order of the input sentence increases performance to 58.3; (ii) adding part-of-speech tags (automatically produced) to the input shows improvement as well (57.2); (iii) So does the introduction of super characters (conflating frequent sequences of characters to a single character), reaching 57.4; (iv) adding silver-standard training data obtained by an off-the-shelf parser yields the biggest improvement, resulting in an F-score of 64.0. Combining all four techniques leads to an F-score of 69.0, which is state-of-the-art in AMR parsing. This is remarkable because of the relatively simplicity of the approach: the only explicit linguistic knowledge that we use are part-of-speech tags.", "creator": "LaTeX with hyperref package"}}}