{"id": "1705.09993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Deep Learning for User Comment Moderation", "abstract": "Experimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of English Wikipedia comments, we show that an RNN outperforms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.", "histories": [["v1", "Sun, 28 May 2017 21:12:56 GMT  (1433kb,D)", "https://arxiv.org/abs/1705.09993v1", null], ["v2", "Mon, 17 Jul 2017 15:25:56 GMT  (1433kb,D)", "http://arxiv.org/abs/1705.09993v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["john pavlopoulos", "prodromos malakasiotis", "ion", "routsopoulos"], "accepted": false, "id": "1705.09993"}, "pdf": {"name": "1705.09993.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for User Comment Moderation", "authors": ["John Pavlopoulos", "Prodromos Malakasiotis"], "emails": ["ip@straintek.com", "mm@straintek.com", "ion@aueb.gr"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most of them will be able to put themselves in the world, in a world in which they blame themselves and others. (...) It is as if they are able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. \"(...) It is as if they are the world of the world, to change the world.\" (...) It is as if they were able to change the world. (...) It is as if they are able to change the world. (...), (...) It is as if they are the world of the world. \"(...) It is as if they are the world of the world. (...) It is as if they are the world of the world. (...)"}, {"heading": "2 Datasets", "text": "We will first discuss the data sets we use to familiarize the reader with the problem."}, {"heading": "2.1 Gazzetta dataset", "text": "The Gazzetta dataset contains approximately 1.45M training comments (for the period from January 1, 2015 to October 6, 2016), with the exception of 47% of the TEST-S; we call them G-TRAIN-L (Table 1). Some experiments use only the first 100K comments from G-TRAIN-L, called G-TRAIN-S. An additional set of 60,900 comments (October 7 to November 11, 2016) was divided into the development (G-DEV, 29,700 comments), a large test (G-TEST-L, 29,700) and a small test set (G-TEST-S, 1,500). Gazzetta's moderators (2 full-time, plus occasional helpful journalists) are occasionally instructed to be stricter (e.g. during violent events). In order to get a more precise overview of the performance in normal situations, we are manually moderated anew (referred to as \"accept\" or \"reject\") the comments from G-TEST-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-"}, {"heading": "2.2 Wikipedia datasets", "text": "Wulczyn et al. (2017) creates three sets of English Wikipedia comments on the discussion page.Attack record: This record contains approximately 115K comments that were marked as personal attacks (reject) or not (accept) using crowdsourcing. Each comment was labeled by at least 10 commenters, leading to binary labels (accept, reject). Alternatively, the gold label is the percentage of commenters who labeled the comment with crib endorffs (2004) alpha, 0.45. The gold label of each comment is determined by the majority of commenters, leading to binary labels (accept, reject). (Or the gold label is the percentage of commenters who label the comment as \"accept.\" 3We used CBOW, window size 5, minimum criterion freq. 5, negative sampling, obtaining a vocabulary of about 478K. (or \"reject\" apparent labels that lead to likely labels)."}, {"heading": "3 Methods", "text": "We experimented with an RNN that works on Word embedding, the same RNN that has been improved with our attention mechanism (a-RNN), several variants of an a-RNN, a Vanilla Convolutional Neural Network (CNN) that also works on Word embedding, the DETOX system by Wulczyn et al. (2017), and a baseline that uses word lists with accuracy values."}, {"heading": "3.1 DETOX", "text": "DETOX (Wulczyn et al., 2017) was the previous state of the art in comment reduction, in the sense that it had the best results on the Wikipedia records (Section 2.2), the largest publicly available records of moderated user comments. 5 DETOX represents each comment as a4We also construct probable gold labels (in addition to binary ones) for G-TEST-S-R. There are 5 annotators.5Two of the co-authors of Wulczyn et al. (2017) are with Jigsaw, who recently announced a system for detecting \"toxic\" comments. Perspective is not the same as DETOX (personal communication), but we could not get any scientific articles describing it. We applied for access to the word programs (n \u2264 2, each comment becomes a bag of 1-gram and 2-gram) or a bag of sign-n-grams."}, {"heading": "3.2 RNN-based methods", "text": "The RNN method is a chain reaction of the GRU cells (Cho et al, 2014) that the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1), the RNN (1)."}, {"heading": "3.3 CNN", "text": "We also compare CNN to a vanilla CNN that works on word embedding. We describe CNN only briefly because it is very similar to Kim's (2014); see also Goldberg (2016) for an introduction to CNN and Zhang and Wallace (2015).For Wikipedia comments, we use a \"narrow\" folding layer where the cores slide over (whole) embedding word n-grams of size n = 1,500.., 4. We use 300 cores for each n-value, a total of 1,200 cores. The results of each kernel obtained by applying the kernel to the various n-grams of a comment c are then max-pooled, resulting in a single output per kernel. The resulting feature vector (1,200 max-9We have also tried to use tf-idf scores in the hsum of da-CENT scores, instead of the attention scores, but preliminary results were poor."}, {"heading": "3.4 LIST baseline", "text": "A baseline called LIST captures any word that occurs in more than 10 (for W-ATT-TRAIN, WTOX-TRAIN, G-TRAIN-S) or 100 comments (for G-TRAIN-L) in the training set, along with the accuracy of w, i.e. the ratio of rejected training comments containing w divided by the total number of training comments containing w. the resulting lists contain 10,423, 11,360, 16,864 and 21,940 word types when W-ATT-TRAIN, W-TOXTRAIN, G-TRAIN-S, G-TRAIN-L. For a comment c, PLIST (reject | c) is the maximum accuracy of all words in c."}, {"heading": "3.5 Tuning thresholds", "text": "For semi-automatic moderation (fig. 1), a comment is rejected directly if its p is above a rejection threshold tr, it is accepted directly if p is below a rejection threshold ta, and it is shown to a moderator if ta \u2264 p \u2264 tr (gray area of fig. 3).In our experience, moderators (or their employers) can easily indicate the approximate percentage of comments they can afford to manually review (e.g. 20% daily) or, accordingly, the approximate percentage of comments that the system should automatically handle. We call the reporting the latter percentage; therefore, 1 \u2212 coverage is the approximate percentage 13We have implemented CNN directly in TensorFlow.Percentage of comments that need to be checked manually. Moderators, on the other hand, are perplexed when asked to vote tr and ta directly. As a result, we ask them to set the approximate desired coverage."}, {"heading": "4 Experimental results", "text": "According to Wulczyn et al. (2017), we report in Tables 2-3 AUC values (range below ROC curve), together with Spearman correlations between system-generated probabilities P (accept | c) and human probable gold labels (Section 2.2), when probable gold labels are available. 15A's first observation is that increasing the size of the Gazzetta training set (G-TRAIN-S to G-TRAINL, Table 2) significantly improves the performance of all methods; we do not report DETOX results for G-TRAIN-L because its implementation could not cope with the size of the Gazzetta training set. Tables 2-314 More precisely, if the calculation of F\u03b2 is reordered the development comments according to posted times and divided into batches of 100. For each ta (and tr) value, we calculate Fs per batch and macro average over batches."}, {"heading": "5 Related work", "text": "The system was used to label a dataset called YNACC, but it cannot be used for our purposes, but it may be possible to expand the annotation scheme for abusive comments to predict more fine-grained labels rather than \"accept\" or \"re-ject.\" Abusive comments have also been filtered out, so YNACC cannot be used for our purposes, but it may be possible to expand the annotation scheme for abusive comments to predict more fine-grained labels. Napoles et al. Also reported that up / down votes, a form of social filtering, are inappropriate."}, {"heading": "6 Conclusions", "text": "We experimented with a new publicly available dataset of 1.6 million moderated user comments from a Greek sports portal and two existing datasets of English Wikipedia comments. We showed that a GRU-RNN working on word embeddings exceeds the previous state of the art using an LR or MLP classifier with character or word-n-gram features. It also exceeds a vanilla CNN operation on word embeddings and a baseline using an automatically constructed word list with precision indicators. A novel, deep, classification-specific attention mechanism further improves the overall results of the RNN. The attention mechanism also improves the results of a simpler method that evaluates word embeddings on average. We looked at both fully automatic and semi-automatic moderation, along with sleeper tuning and evaluation measures for both. We plan to consider user-specific information (e.g. the ratio of comments to read-depth, the number of the past and the number of threads)."}, {"heading": "Acknowledgments", "text": "This work was funded by Google's Digital News Initiative (Project ML2P, Contract 362826).21 We thank Gazzetta for the data it provided and the moderators of Gazzetta for their feedback, insights and advice."}], "references": [{"title": "Internet Argument Corpus 2.0: An SQL schema for dialogic social media and the corpora to go", "author": ["R. Abbott", "B. Ecker", "P. Anand", "M.A. Walker"], "venue": null, "citeRegEx": "Abbott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abbott et al\\.", "year": 2016}, {"title": "Deep learning for hate speech detection in tweets", "author": ["P. Badjatiya", "S. Gupta", "M. Gupta", "V. Varma."], "venue": "WWW (Companion). Perth, Australia, pages 759\u2013 760.", "citeRegEx": "Badjatiya et al\\.,? 2017", "shortCiteRegEx": "Badjatiya et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "ICLR. San Diego, CA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan."], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Automatic Web rating: Filtering obscene content on the Web", "author": ["K.V. Chandrinos", "I. Androutsopoulos", "G. Paliouras", "C.D. Spyropoulos."], "venue": "Proc. of the 4th European Conference on Research and Advanced Technology for Digital Libraries. Lisbon,", "citeRegEx": "Chandrinos et al\\.,? 2000", "shortCiteRegEx": "Chandrinos et al\\.", "year": 2000}, {"title": "Antisocial behavior in online discussion communities", "author": ["J. Cheng", "C. Danescu-Niculescu-Mizil", "J. Leskovec."], "venue": "Proc. of the International AAAI Conference on Web and Social Media. Oxford University, England, pages 61\u201370.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio."], "venue": "EMNLP. Doha, Qatar, pages 1724\u20131734.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A coefficient of agreement for nominal scales", "author": ["J. Cohen."], "venue": "Educational and Psychological Measurement 20(1):37\u201346.", "citeRegEx": "Cohen.,? 1960", "shortCiteRegEx": "Cohen.", "year": 1960}, {"title": "Support-Vector Networks", "author": ["C. Cortes", "Vladimir Vapnik."], "venue": "Machine Learning 20(3):273\u2013297.", "citeRegEx": "Cortes and Vapnik.,? 1995", "shortCiteRegEx": "Cortes and Vapnik.", "year": 1995}, {"title": "Improving cyberbullying detection with user context", "author": ["M. Dadvar", "D. Trieschnigg", "R. Ordelman", "F. de Jong."], "venue": "ECIR. Moscow, Russia, pages 693\u2013696.", "citeRegEx": "Dadvar et al\\.,? 2013", "shortCiteRegEx": "Dadvar et al\\.", "year": 2013}, {"title": "Semisupervised recognition of sarcastic sentences in Twitter and Amazon", "author": ["D. Davidov", "O. Tsur", "A. Rappoport."], "venue": "CoNLL. Uppsala, Sweden, pages 107\u2013116. See https://digitalnewsinitiative.com/.", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Picking the NYT picks: Editorial criteria and automation in the curation of online news comments", "author": ["N. Diakopoulos."], "venue": "Journal of the International Symposium on Online Journalism 5:147\u2013166.", "citeRegEx": "Diakopoulos.,? 2015", "shortCiteRegEx": "Diakopoulos.", "year": 2015}, {"title": "Modeling the detection of textual cyberbullying", "author": ["K. Dinakar", "R. Reichart", "H. Lieberman."], "venue": "The Social Mobile Web. Barcelona, Spain, volume WS-11-02 of AAAI Workshops, pages 11\u201317.", "citeRegEx": "Dinakar et al\\.,? 2011", "shortCiteRegEx": "Dinakar et al\\.", "year": 2011}, {"title": "Hate speech detection with comment embeddings", "author": ["N. Djuric", "J. Zhou", "R. Morris", "M. Grbovic", "V. Radosavljevic", "N. Bhamidipati."], "venue": "WWW. Florence, Italy, pages 29\u201330.", "citeRegEx": "Djuric et al\\.,? 2015", "shortCiteRegEx": "Djuric et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["C.N. dos Santos", "B. Zadrozny."], "venue": "ICML. Beijing, China, pages 1818\u2013 1826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Stochastic gradient boosting", "author": ["J.H. Friedman."], "venue": "Computational Statistics and Data Analysis 38(4):367\u2013378.", "citeRegEx": "Friedman.,? 2002", "shortCiteRegEx": "Friedman.", "year": 2002}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio."], "venue": "Proc. of the International Conference on Artificial Intelligence and Statistics. Sardinia, Italy, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "A primer on neural network models for natural language processing", "author": ["Y. Goldberg."], "venue": "Journal of Artificial Intelligence Research 57:345\u2013420.", "citeRegEx": "Goldberg.,? 2016", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Identifying sarcasm in Twitter: A closer look", "author": ["R.I. Gonz\u00e1lez-Ib\u00e1\u00f1ez", "S. Muresan", "N. Wacholder."], "venue": "ACL. Portland, Oregon, pages 581\u2013586.", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.", "year": 2011}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville."], "venue": "MIT Press.", "citeRegEx": "Goodfellow et al\\.,? 2016", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "LSTM: A search space Odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "CoRR abs/1503.04069", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov."], "venue": "CoRR abs/1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Random Decision Forests", "author": ["T.K. Ho."], "venue": "Proc. of the 3rd International Conference on Document Analysis and Recognition. Montreal, Canada, volume 1, pages 278\u2013282.", "citeRegEx": "Ho.,? 1995", "shortCiteRegEx": "Ho.", "year": 1995}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["A. Joshi", "V. Sharma", "P. Bhattacharyya."], "venue": "ACL. Beijing, China, pages 757\u2013762.", "citeRegEx": "Joshi et al\\.,? 2015", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Bag of tricks for efficient text classification", "author": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov."], "venue": "EACL (short papers). Valencia, Spain, pages 427\u2013 431.", "citeRegEx": "Joulin et al\\.,? 2017", "shortCiteRegEx": "Joulin et al\\.", "year": 2017}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim."], "venue": "EMNLP. Doha, Qatar, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba."], "venue": "ICLR. San Diego, CA.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Content Analysis: An Introduction to Its Methodology (2nd edition)", "author": ["K. Krippendorff."], "venue": "Sage Publications.", "citeRegEx": "Krippendorff.,? 2004", "shortCiteRegEx": "Krippendorff.", "year": 2004}, {"title": "Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira."], "venue": "ICML. Williamstown, MA, pages 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov."], "venue": "ICML. Beijing, China, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Discovering high-quality threaded discussions in online forums", "author": ["J.-T. Lee", "M.-C. Yang", "H.-C. Rim."], "venue": "Journal of Computer Science and Technology 29(3):519\u2013531.", "citeRegEx": "Lee et al\\.,? 2014", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "C. Dyer", "A.W. Black", "I. Trancoso", "R. Fermandez", "S. Amir", "L. Marujo"], "venue": "Lu\u0131\u0301s", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Sentiment Analysis \u2013 Mining Opinions, Sentiments, and Emotions", "author": ["B. Liu."], "venue": "Cambridge University Press.", "citeRegEx": "Liu.,? 2015", "shortCiteRegEx": "Liu.", "year": 2015}, {"title": "Really? well", "author": ["S. Lukin", "M. Walker."], "venue": "apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue. In Proc. of the Workshop on Language in Social Media. Atlanta, Georgia, pages 30\u201340.", "citeRegEx": "Lukin and Walker.,? 2013", "shortCiteRegEx": "Lukin and Walker.", "year": 2013}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["T. Luong", "H. Pham", "C.D. Manning."], "venue": "EMNLP. Lisbon, Portugal, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Hunting for troll comments in news community forums", "author": ["T. Mihaylov", "P. Nakov."], "venue": "ACL. Berlin, Germany, pages 399\u2013405.", "citeRegEx": "Mihaylov and Nakov.,? 2016", "shortCiteRegEx": "Mihaylov and Nakov.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean."], "venue": "ICLR. Scottsdale, AZ.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig."], "venue": "NAACL-HLT . Atlanta, GA, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Blocking blog spam with language model disagreement", "author": ["G. Mishne", "D. Carmel", "R. Lempel."], "venue": "Proc. of the International Workshop on Adversarial Information Retrieval on the Web. Chiba, Japan.", "citeRegEx": "Mishne et al\\.,? 2005", "shortCiteRegEx": "Mishne et al\\.", "year": 2005}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu."], "venue": "NIPS. Montreal, Canada, pages 2204\u20132212.", "citeRegEx": "Mnih et al\\.,? 2014", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Automatically identifying good conversations online (yes, they do exist!)", "author": ["C. Napoles", "A. Pappu", "J. Tetreault."], "venue": "Proc. of the International AAAI Conference on Web and Social Media.", "citeRegEx": "Napoles et al\\.,? 2017a", "shortCiteRegEx": "Napoles et al\\.", "year": 2017}, {"title": "Finding good conversations online: The Yahoo News annotated comments corpus", "author": ["C. Napoles", "J. Tetreault", "E. Rosato", "B. Provenzale", "A. Pappu."], "venue": "Proc. of the Linguistic Annotation Workshop. Valencia, Spain, pages 13\u201323.", "citeRegEx": "Napoles et al\\.,? 2017b", "shortCiteRegEx": "Napoles et al\\.", "year": 2017}, {"title": "A quantitative study of forum spamming using context-based analysis", "author": ["Y. Niu", "Y.-M. Wang", "H. Chen", "M. Ma", "F. Hsu."], "venue": "Proc. of the Annual Network and Distributed System Security Symposium. San Diego, CA, pages 79\u201392.", "citeRegEx": "Niu et al\\.,? 2007", "shortCiteRegEx": "Niu et al\\.", "year": 2007}, {"title": "Abusive language detection in online user content", "author": ["C. Nobata", "J. Tetreault", "A. Thomas", "Y. Mehdad", "Y. Chang."], "venue": "WWW. Montreal, Canada, pages 145\u2013153.", "citeRegEx": "Nobata et al\\.,? 2016", "shortCiteRegEx": "Nobata et al\\.", "year": 2016}, {"title": "Creating and characterizing a diverse corpus of sarcasm in dialogue", "author": ["S. Oraby", "V. Harrison", "L. Reed", "E. Hernandez", "E. Riloff", "M.A. Walker."], "venue": "SIGDial. Los Angeles, CA, pages 31\u201341.", "citeRegEx": "Oraby et al\\.,? 2016", "shortCiteRegEx": "Oraby et al\\.", "year": 2016}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee."], "venue": "Foundations and Trends in Information Retrieval 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning."], "venue": "EMNLP. Doha, Qatar, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal."], "venue": "IEEE Transacions of Signal Processing 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Profanity use in online communities", "author": ["S. Sood", "J. Antin", "E.F. Churchill."], "venue": "SIGCHI. Austin, TX, pages 1481\u20131490.", "citeRegEx": "Sood et al\\.,? 2012a", "shortCiteRegEx": "Sood et al\\.", "year": 2012}, {"title": "Using crowdsourcing to improve profanity detection", "author": ["S. Sood", "J. Antin", "E.F. Churchill."], "venue": "AAAI Spring Symposium: Wisdom of the Crowd. Stanford, CA, pages 69\u201374.", "citeRegEx": "Sood et al\\.,? 2012b", "shortCiteRegEx": "Sood et al\\.", "year": 2012}, {"title": "Smokey: Automatic recognition of hostile messages", "author": ["E. Spertus."], "venue": "Proc. of the National Conference on Artificial Intelligence and the Innovative Applications of Artificial Intelligence Conference. Providence, Rhode Island, pages 1058\u20131065.", "citeRegEx": "Spertus.,? 1997", "shortCiteRegEx": "Spertus.", "year": 1997}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "NIPS. Montreal, Canada, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A corpus for research on deliberation and debate", "author": ["M.A. Walker", "J.E. Fox Tree", "P. Anand", "R. Abbott", "J. King."], "venue": "LREC. Istanbul, Turkey, pages 4445\u2013 4452.", "citeRegEx": "Walker et al\\.,? 2012", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Detecting hate speech on the World Wide Web", "author": ["W. Warner", "J. Hirschberg."], "venue": "Proc. of the 2nd Workshop on Language in Social Media. Montreal, Canada, pages 19\u201326.", "citeRegEx": "Warner and Hirschberg.,? 2012", "shortCiteRegEx": "Warner and Hirschberg.", "year": 2012}, {"title": "Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter", "author": ["Z. Waseem", "D. Hovy."], "venue": "Proc. of the NAACL Student Research Workshop. San Diego, CA, pages 88\u201393.", "citeRegEx": "Waseem and Hovy.,? 2016", "shortCiteRegEx": "Waseem and Hovy.", "year": 2016}, {"title": "Ex machina: Personal attacks seen at scale", "author": ["E. Wulczyn", "N. Thain", "L. Dixon."], "venue": "WWW. Perth, Australia, pages 1391\u20131399.", "citeRegEx": "Wulczyn et al\\.,? 2017", "shortCiteRegEx": "Wulczyn et al\\.", "year": 2017}, {"title": "Detecting offensive tweets via topical feature discovery over a large scale twitter corpus", "author": ["G. Xiang", "B. Fan", "L. Wang", "J. Hong", "C. Rose."], "venue": "CIKM. Maui, Hawaii, pages 1980\u20131984.", "citeRegEx": "Xiang et al\\.,? 2012", "shortCiteRegEx": "Xiang et al\\.", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "J.R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio."], "venue": "ICML. Lille, France, pages 2048\u20132057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Hierarchical attention networks for document classification", "author": ["Z. Yang", "D. Yang", "C. Dyer", "X. He", "A. Smola", "E. Hovy."], "venue": "NAACL-HLT . San Diego, CA, pages 1480\u20131489.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French", "author": ["D. Yarowsky."], "venue": "ACL. Las Cruces, NM, pages 88\u201395.", "citeRegEx": "Yarowsky.,? 1994", "shortCiteRegEx": "Yarowsky.", "year": 1994}, {"title": "Detection of harassment on Web 2.0", "author": ["D. Yin", "Z. Xue", "L. Hong", "B.D. Davison", "A. Kontostathis", "L. Edwards"], "venue": "In Proc. of the WWW workshop on Content Analysis in the Web", "citeRegEx": "Yin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2009}, {"title": "Characterlevel convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun."], "venue": "NIPS. Montreal, Canada, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Y. Zhang", "B.C. Wallace."], "venue": "CoRR abs/1510.03820.", "citeRegEx": "Zhang and Wallace.,? 2015", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "1 We examine how deep learning (Goodfellow et al., 2016; Goldberg, 2016) can be used to moderate user comments.", "startOffset": 31, "endOffset": 72}, {"referenceID": 17, "context": "1 We examine how deep learning (Goodfellow et al., 2016; Goldberg, 2016) can be used to moderate user comments.", "startOffset": 31, "endOffset": 72}, {"referenceID": 55, "context": "We also experiment on the datasets of Wulczyn et al. (2017),", "startOffset": 38, "endOffset": 60}, {"referenceID": 55, "context": "outperforms the system of Wulczyn et al. (2017), the previous state of the art for comment moderation, which employed logistic regression (LR) or a multi-layered Perceptron (MLP).", "startOffset": 26, "endOffset": 48}, {"referenceID": 2, "context": "from most previous ones (Bahdanau et al., 2015; Luong et al., 2015) in that it is used in text classification, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (Sutskever et al.", "startOffset": 24, "endOffset": 67}, {"referenceID": 34, "context": "from most previous ones (Bahdanau et al., 2015; Luong et al., 2015) in that it is used in text classification, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (Sutskever et al.", "startOffset": 24, "endOffset": 67}, {"referenceID": 7, "context": "Using Cohen\u2019s Kappa (Cohen, 1960), the mean pairwise", "startOffset": 20, "endOffset": 33}, {"referenceID": 26, "context": "Krippendorff\u2019s (2004) alpha was 0.", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "Krippendorff\u2019s (2004) alpha was 0.4762, close to the value (0.45) reported by Wulczyn et al. (2017) for Wikipedia comments.", "startOffset": 0, "endOffset": 100}, {"referenceID": 27, "context": "Inter-annotator agreement, measured on a random sample of 1K comments using Krippendorff\u2019s (2004) alpha, was 0.", "startOffset": 76, "endOffset": 98}, {"referenceID": 55, "context": "Wulczyn et al. (2017)", "startOffset": 0, "endOffset": 22}, {"referenceID": 55, "context": "a-RNN, a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of Wulczyn et al. (2017), and a baseline that uses word lists with precision scores.", "startOffset": 107, "endOffset": 129}, {"referenceID": 55, "context": "DETOX (Wulczyn et al., 2017) was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.", "startOffset": 6, "endOffset": 28}, {"referenceID": 22, "context": "Two of the co-authors of Wulczyn et al. (2017) are with Jigsaw, who recently announced Perspective, a system to detect \u2018toxic\u2019 comments.", "startOffset": 17, "endOffset": 47}, {"referenceID": 6, "context": "RNN: The RNN method is a chain of GRU cells (Cho et al., 2014) that transforms the tokens w1 .", "startOffset": 44, "endOffset": 62}, {"referenceID": 55, "context": "Wulczyn et al. (2017) report results only on W-ATT-DEV.", "startOffset": 0, "endOffset": 22}, {"referenceID": 39, "context": "Our attention mechanism differs from most previous ones (Mnih et al., 2014; Bahdanau et al., 2015; Xu et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence (e.", "startOffset": 56, "endOffset": 135}, {"referenceID": 2, "context": "Our attention mechanism differs from most previous ones (Mnih et al., 2014; Bahdanau et al., 2015; Xu et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence (e.", "startOffset": 56, "endOffset": 135}, {"referenceID": 57, "context": "Our attention mechanism differs from most previous ones (Mnih et al., 2014; Bahdanau et al., 2015; Xu et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence (e.", "startOffset": 56, "endOffset": 135}, {"referenceID": 34, "context": "Our attention mechanism differs from most previous ones (Mnih et al., 2014; Bahdanau et al., 2015; Xu et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence (e.", "startOffset": 56, "endOffset": 135}, {"referenceID": 51, "context": ", assign more weight to source words to translate next), unlike seq2seq models (Sutskever et al., 2014).", "startOffset": 79, "endOffset": 103}, {"referenceID": 2, "context": ", 2014; Bahdanau et al., 2015; Xu et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence (e.g., partly generated translation) to drive the attention (e.g., assign more weight to source words to translate next), unlike seq2seq models (Sutskever et al., 2014). It assigns larger weights at to hidden states ht corresponding to positions where there is more evidence that the comment should be accepted or rejected. Yang et al. (2016) use a similar attention mechanism, but ours is deeper.", "startOffset": 8, "endOffset": 517}, {"referenceID": 47, "context": "Their method uses two GRU RNNs, both bidirectional (Schuster and Paliwal, 1997), one turning the word embeddings of each sentence to a sentence embedding, and one turning the sentence embeddings to a document embedding, which is", "startOffset": 51, "endOffset": 79}, {"referenceID": 16, "context": "We use Glorot initialization (Glorot and Bengio, 2010), crossentropy loss, and Adam (Kingma and Ba, 2015).", "startOffset": 29, "endOffset": 54}, {"referenceID": 26, "context": "We use Glorot initialization (Glorot and Bengio, 2010), crossentropy loss, and Adam (Kingma and Ba, 2015).", "startOffset": 84, "endOffset": 105}, {"referenceID": 46, "context": "For the Wikipedia datasets, they are initialized to GLOVE embeddings (Pennington et al., 2014).", "startOffset": 69, "endOffset": 94}, {"referenceID": 24, "context": "We describe the CNN only briefly, because it is very similar to that of of Kim (2014); see also Goldberg (2016) for an introduction to CNNs, and Zhang and Wallace (2015).", "startOffset": 75, "endOffset": 86}, {"referenceID": 17, "context": "We describe the CNN only briefly, because it is very similar to that of of Kim (2014); see also Goldberg (2016) for an introduction to CNNs, and Zhang and Wallace (2015).", "startOffset": 96, "endOffset": 112}, {"referenceID": 17, "context": "We describe the CNN only briefly, because it is very similar to that of of Kim (2014); see also Goldberg (2016) for an introduction to CNNs, and Zhang and Wallace (2015).", "startOffset": 96, "endOffset": 170}, {"referenceID": 21, "context": "pooled outputs) goes through a dropout layer (Hinton et al., 2012) (p = 0.", "startOffset": 45, "endOffset": 66}, {"referenceID": 54, "context": "Following Wulczyn et al. (2017), we report in Tables 2\u20133 AUC scores (area under ROC curve), along with Spearman correlations between system-generated probabilities P (accept|c) and human probabilistic gold labels (Section 2.", "startOffset": 10, "endOffset": 32}, {"referenceID": 52, "context": "6K comments (1K threads) from the Internet Argument Corpus (Walker et al., 2012; Abbott et al., 2016).", "startOffset": 59, "endOffset": 101}, {"referenceID": 0, "context": "6K comments (1K threads) from the Internet Argument Corpus (Walker et al., 2012; Abbott et al., 2016).", "startOffset": 59, "endOffset": 101}, {"referenceID": 28, "context": "Lee et al. (2014) discuss social filtering in detail and propose features (e.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "Diakopoulos (2015) dis-", "startOffset": 0, "endOffset": 19}, {"referenceID": 39, "context": "In further work, Napoles et al. (2017a) aimed to identify high quality threads.", "startOffset": 17, "endOffset": 40}, {"referenceID": 29, "context": "using DOC2VEC (Le and Mikolov, 2014).", "startOffset": 14, "endOffset": 36}, {"referenceID": 28, "context": "An ensemble of Conditional Random Fields (CRFs) (Lafferty et al., 2001) assigns labels (from their annotation scheme, e.", "startOffset": 48, "endOffset": 71}, {"referenceID": 27, "context": "An ensemble of Conditional Random Fields (CRFs) (Lafferty et al., 2001) assigns labels (from their annotation scheme, e.g., for sentiment, off-topic) to the comments of each thread, viewing each thread as a sequence of DOC2VEC embeddings. The decisions of the CRFs are then used to convert each thread to a feature vector (total count and mean marginal probability of each label in the thread), which is passed on to an LR classifier. Further improvements were observed when additional features were added, BOW counts and POS n-grams being the most important ones. Napoles et al. (2017a) also experimented with a CNN, similar to that of Section 3.", "startOffset": 49, "endOffset": 588}, {"referenceID": 13, "context": "Character n-grams were the best, on their own outperforming Djuric et al. (2015). The best results, however, were obtained using all features.", "startOffset": 60, "endOffset": 81}, {"referenceID": 13, "context": "Character n-grams were the best, on their own outperforming Djuric et al. (2015). The best results, however, were obtained using all features. By contrast, we use no handcrafted features and parsers, making our methods easily portable to other domains and languages. Wulczyn et al. (2017) experimented with character and word n-grams, based on the findings of", "startOffset": 60, "endOffset": 289}, {"referenceID": 8, "context": "(2017) experimented with the same dataset using LR, SVMs (Cortes and Vapnik, 1995), Random Forests (Ho, 1995), Gradient Boosted Decision Trees (GBDT) (Friedman, 2002), CNN (similar to that of Section 3.", "startOffset": 57, "endOffset": 82}, {"referenceID": 22, "context": "(2017) experimented with the same dataset using LR, SVMs (Cortes and Vapnik, 1995), Random Forests (Ho, 1995), Gradient Boosted Decision Trees (GBDT) (Friedman, 2002), CNN (similar to that of Section 3.", "startOffset": 99, "endOffset": 109}, {"referenceID": 15, "context": "(2017) experimented with the same dataset using LR, SVMs (Cortes and Vapnik, 1995), Random Forests (Ho, 1995), Gradient Boosted Decision Trees (GBDT) (Friedman, 2002), CNN (similar to that of Section 3.", "startOffset": 150, "endOffset": 166}, {"referenceID": 20, "context": "3), LSTM (Greff et al., 2015), FastText (Joulin et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 24, "context": ", 2015), FastText (Joulin et al., 2017).", "startOffset": 18, "endOffset": 39}, {"referenceID": 1, "context": "Badjatiya et al. (2017) experimented with the same dataset using LR, SVMs (Cortes and Vapnik, 1995), Random Forests (Ho, 1995), Gradient Boosted Decision Trees (GBDT) (Friedman, 2002), CNN (similar to that of Section 3.", "startOffset": 0, "endOffset": 24}, {"referenceID": 59, "context": "lowing Yarowsky (1994).", "startOffset": 7, "endOffset": 23}, {"referenceID": 5, "context": "Cheng et al. (2015) predict which users would be banned from on-line communities.", "startOffset": 0, "endOffset": 20}, {"referenceID": 52, "context": "5K utterances from the Internet Argument Corpus (Walker et al., 2012; Abbott et al., 2016) annotated with nastiness scores, and 9.", "startOffset": 48, "endOffset": 90}, {"referenceID": 0, "context": "5K utterances from the Internet Argument Corpus (Walker et al., 2012; Abbott et al., 2016) annotated with nastiness scores, and 9.", "startOffset": 48, "endOffset": 90}, {"referenceID": 0, "context": ", 2012; Abbott et al., 2016) annotated with nastiness scores, and 9.9K utterances from the same corpus annotated for sarcasm.18 In a bootstrapping manner, they manually identified cue words and phrases (indicative of nastiness or sarcasm), used the cue words to obtain training comments, and extracted patterns from the training comments. Xiang et al. (2012) also employed bootstrapping to identify users whose tweets fre-", "startOffset": 8, "endOffset": 359}, {"referenceID": 3, "context": "The classifiers used topical features, obtained via LDA (Blei et al., 2003), and a feature indicating the presence of at least one of approx.", "startOffset": 56, "endOffset": 75}, {"referenceID": 38, "context": "point out that unlike other abusive content, spam in comments or discussion fora (Mishne et al., 2005; Niu et al., 2007) is off-topic and serves a commercial purpose.", "startOffset": 81, "endOffset": 120}, {"referenceID": 42, "context": "point out that unlike other abusive content, spam in comments or discussion fora (Mishne et al., 2005; Niu et al., 2007) is off-topic and serves a commercial purpose.", "startOffset": 81, "endOffset": 120}, {"referenceID": 45, "context": "Sentiment features have been used by several methods, but sentiment analysis (Pang and Lee, 2008; Liu, 2015) is typically not directly concerned with abusive content.", "startOffset": 77, "endOffset": 108}, {"referenceID": 32, "context": "Sentiment features have been used by several methods, but sentiment analysis (Pang and Lee, 2008; Liu, 2015) is typically not directly concerned with abusive content.", "startOffset": 77, "endOffset": 108}, {"referenceID": 10, "context": "For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 10, "context": "For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al. (2011), Joshi et al.", "startOffset": 17, "endOffset": 70}, {"referenceID": 10, "context": "For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al. (2011), Joshi et al. (2015), Oraby et al.", "startOffset": 17, "endOffset": 91}, {"referenceID": 10, "context": "For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al. (2011), Joshi et al. (2015), Oraby et al. (2016). Sentiment features have been used by several methods, but sentiment analysis (Pang and Lee, 2008; Liu, 2015) is typically not directly concerned with abusive content.", "startOffset": 17, "endOffset": 112}, {"referenceID": 8, "context": "(2011) and Dadvar et al. (2013) detect cyberbullying.", "startOffset": 11, "endOffset": 32}, {"referenceID": 4, "context": "Chandrinos et al. (2000) detect pornographic web pages, using a Naive Bayes classifier with text and image features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "Chandrinos et al. (2000) detect pornographic web pages, using a Naive Bayes classifier with text and image features. Spertus (1997) flag flame messages in Web feedback forms, using decision trees and hand-crafted features.", "startOffset": 0, "endOffset": 132}, {"referenceID": 61, "context": "We also plan to explore character-level RNNs or CNNs (Zhang et al., 2015), for example to produce embeddings of unknown or obfuscated words from characters (dos Santos and Zadrozny, 2014; Ling et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 31, "context": ", 2015), for example to produce embeddings of unknown or obfuscated words from characters (dos Santos and Zadrozny, 2014; Ling et al., 2015).", "startOffset": 90, "endOffset": 140}], "year": 2017, "abstractText": "Experimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of English Wikipedia comments, we show that an RNN outperforms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.", "creator": "LaTeX with hyperref package"}}}