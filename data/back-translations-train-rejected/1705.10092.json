{"id": "1705.10092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Role Playing Learning for Socially Concomitant Mobile Robot Navigation", "abstract": "In this paper, we present the Role Playing Learning (RPL) scheme for a mobile robot to navigate socially with its human companion in populated environments. Neural networks (NN) are constructed to parameterize a stochastic policy that directly maps sensory data collected by the robot to its velocity outputs, while respecting a set of social norms. An efficient simulative learning environment is built with maps and pedestrians trajectories collected from a number of real-world crowd data sets. In each learning iteration, a robot equipped with the NN policy is created virtually in the learning environment to play itself as a companied pedestrian and navigate towards a goal in a socially concomitant manner. Thus, we call this process Role Playing Learning, which is formulated under a reinforcement learning (RL) framework. The NN policy is optimized end-to-end using Trust Region Policy Optimization (TRPO), with consideration of the imperfectness of robot's sensor measurements. Simulative and experimental results are provided to demonstrate the efficacy and superiority of our method.", "histories": [["v1", "Mon, 29 May 2017 09:42:36 GMT  (2902kb)", "http://arxiv.org/abs/1705.10092v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["mingming li", "rui jiang", "shuzhi sam ge", "tong heng lee"], "accepted": false, "id": "1705.10092"}, "pdf": {"name": "1705.10092.pdf", "metadata": {"source": "CRF", "title": "Role Playing Learning for Socially Concomitant Mobile Robot Navigation", "authors": ["Mingming Li", "Rui Jiang", "Shuzhi Sam Ge"], "emails": ["mingming@u.nus.edu;", "jiang@u.nus.edu;", "samge@nus.edu.sg;", "eleleeth@nus.edu.sg)."], "sections": [{"heading": null, "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "II. RELATED WORK", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "B. Steering Models", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "III. PROBLEM FORMULATION", "text": "The robot should reach its destination as quickly as possible; 2) The robot should not collide with any of the pedestrians or its companions or run into any obstacle; 3) The robot should not run too far away from its destination defined by the tuple (S, F, O, p0, p 0). To give concrete definitions, we consider the navigation process as an endless-horizontal POMDP in discrete time, defined by the tuple (S, O, p0, p 0). S is a finite series of states reflecting the navigation status of the robot. A is a finite series of actions a. In this paper it is defined as a twosome of translational and rotational velocity of a synchro-drive mobile robot, i.e."}, {"heading": "IV. ROLE PLAYING LEARNING", "text": "The basic idea is to transform the data collected from the real world into a simulated and dynamic navigation environment in which the robot can play itself as a virtual pedestrian and iteratively improve the performance of P3 systems. Each environment Ej = (Tj, Mj) contains a series of strategies for pedestrian optimization (PO-TRPO). Let's consider a series of simulated navigation environments E = {E1, \u00b7 Ej, \u00b7 En, \u00b7 En, which improve the performance of P3 systems. Each environment Ej = (Tj, Mj) contains a series of pedestrian trajectories Tj = {0: Tk} and a binary map Mj, which approximates the 2-D Cartesian coordinates of6 obstacles / occupied space in the environment."}, {"heading": "A. Trusted Region Policy Optimization", "text": "The TRPO [40] algorithm is an effective on-policy optimization method for large nonlinear strategies and tends to bring about monotonous improvements during the iterative optimization process. To be precise, a fully observable MDP is taken into account by the TRPO and therefore the policy to be optimized is formulated as P * (a | s), where P * (a | s) is the parameter vector of the policy P *. Consider the following standard definitions of the state action value function QB (si, ai), the value function VE (si) and the advantage function AE (si, ai): QE (si, ai) = Esi + 1, ai + 1, \u00b7 \u00b7 \u00b7 VE (VE) l = 0 = 0E (si + l), (35) VE (si) = EE, si + 1, \u00b7 VE (ai) = 36 (VE) = VE (VE = 38)."}, {"heading": "B. Partially Observable TRPO", "text": "As already mentioned, our navigation problem is considered POMDP. Politics P\u03b8 (ai | si) is dependent on observing oi instead of the actual state. Therefore, we write the objective function (40) and the constraint (41) maximizing boy problem \u2212 si (a, o) q (a, o) q (a, o) q (a, o | s) A\u03b8 \u2212 (a, s)] (42) subject to the Es question \u2212 [DKL (P\u03b8 \u2212 (o) q (\u00b7 o) [o)] p. (43) For the PO-TRPO, the samples are collected by executing the old policy PTB \u2212 (a, o) to generate a series of procedures, such as s0, o0, s1, a1, aT \u2212 i, oT \u2212 1, oT \u2212 i."}, {"heading": "V. SIMULATION", "text": "In this section, we will describe how to build a simulative environment according to the proposed RPL scheme. In particular, the environments, the deep neural network policy and the PO-TRPO algorithms are developed from the ETH Going Algorithm (EWAP), which within the framework of RLLAB [11] the motion detection (MC) from the Zara and UCY video clips are collected from five different datasets. Note: The Zara and UCY datasets have several subsets: Zara01, Zara03, UCY01 and UCY03. Thus, there are completely different RPL environments, i.e.."}, {"heading": "A. Results", "text": "We trained our deep political network for 1200 iterations with the data from the RPL environments, except for the held ETH environments. The curve of the average obtained return is visualized in Fig. 5We compare the performance of our policy with that of the RVO [8], in which the robot, its companion and the surrounding pedestrians are treated. At each step, the positions and speeds of all agents are given to the planner. Note that the positions of the agents are subject to noise. (54) and (55) To observe the obstacles, we assume that the planner has full and perfect knowledge."}, {"heading": "VI. EXPERIMENTS", "text": "In experiments, we evaluate the performance of our developed navigation policy by comparing it to humans in the same scenarios. In particular, a robot and a human must repeat each specific navigation scenario ten times. Then, the following two indicators are calculated: 1) Average minimum distance to pedestrians (D-Com): the average of the minimum distance between the robot and other pedestrians during a trajectory. 2) Average maximum distance to the companion (D-Com): the average of the maximum distance between the therobot / its companion during a trajectory. We use the same mobile platform (a Turtlebot 2 with synchronous drive and a Kobuki base) and the same laser rangefinder (Hokuyo URG-04LX), which is simulated in the last section."}, {"heading": "A. Scenario 1: Traditional Social Navigation", "text": "In this subsection, we will examine the performance of our method in a traditional social navigation scenario. Although the robot must pass the corridor with two oncoming pedestrians and arrive at a destination 7 meters in front of it. Furthermore, in the same room, a control experiment is carried out with three people (one as compared to a human and the other two as a pedestrian), the metric sequence of motion is calculated. Exemplary trajectories of the robot and human control are shown in Fig. 7. In the robot experiments, the trajectories of pedestrians are determined from the robot's laser rangefinder, while the robot's trajectory is based on its own odometry sensor. On the other hand, all trajectories in the human control experiments are recorded using the UWB localization system. From Fig. 7, it is clear that the robot and its policies adequately understand the cooperative behavior of humans in collision avoidance and navigation."}, {"heading": "B. Scenario 2: Socially Concomitant Navigation", "text": "This subsection examines the scenario of SCN. A human companion who is first in front of the robot begins to walk through the same corridor while another pedestrian passes from the other end. As described in the previous sections, the robot should navigate closely with our policy and cooperatively avoid the oncoming pedestrian. An additional metric D-Com is used to evaluate the performance of our policy by comparing it with statistics from another 10 human control experiments. Examples of trajectories are shown in Figure 8 and the performance indicators D-Com and D-Com are summarized in Fig. IV. As shown in Fig. 8 and Fig. IV, the robot is able to achieve both goals of SCN. On the one hand, it is effectively involved in the common collision avoidance process. The resulting behavior is similar to the behavior observed in the last subsection, and the robot even has a slightly larger D-Com policy."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, the problem of social accompanying navigation (SCN) was investigated and formulated in the context of a POMDP experiment (a) The trajectories of the robot and its companion (from left to right) and a pedestrian (from right to left). (b) The experiment with human control in a similar SCN. The black (compared human) and orange (companion) trajectories run from left to right and the blue (pedestrian) trajectories run from right to left.Fig.8: The partially observable TRPO algorithm (PO-TRPO) was proposed to optimize navigation policy and the human control experiment in an SCN scenario, explicitly considering the limitation and imprecision of the onboard sensors of mobile robots."}], "references": [{"title": "The dynamic window approach to collision avoidance", "author": ["D.F.W.B.S. Thrun", "D. Fox", "W. Burgard"], "venue": "IEEE Transactions on Robotics and Automation, vol. 4, p. 1, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A potential field approach to path planning", "author": ["Y.K. Hwang", "N. Ahuja"], "venue": "IEEE Transactions on Robotics and Automation, vol. 8, no. 1, pp. 23\u201332, 1992.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "New potential functions for mobile robot path planning", "author": ["S.S. Ge", "Y.J. Cui"], "venue": "IEEE Transactions on robotics and automation, vol. 16, no. 5, pp. 615\u2013620, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Unfreezing the robot: Navigation in dense, interacting crowds", "author": ["P. Trautman", "A. Krause"], "venue": "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, 2010, pp. 797\u2013803.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Robot navigation in dense human crowds: Statistical models and experimental studies of human\u2013robot cooperation", "author": ["P. Trautman", "J. Ma", "R.M. Murray", "A. Krause"], "venue": "The International Journal of Robotics Research, vol. 34, no. 3, pp. 335\u2013356, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Social force model for pedestrian dynamics", "author": ["D. Helbing", "P. Molnar"], "venue": "Physical review E, vol. 51, no. 5, p. 4282, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Simulating dynamical features of escape panic", "author": ["D. Helbing", "I. Farkas", "T. Vicsek"], "venue": "Nature, vol. 407, no. 6803, pp. 487\u2013490, 2000.  11", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Reciprocal nbody collision avoidance", "author": ["J. Van Den Berg", "S.J. Guy", "M. Lin", "D. Manocha"], "venue": "Robotics research, 2011, pp. 3\u201319.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information", "author": ["J. Van Den Berg", "P. Abbeel", "K. Goldberg"], "venue": "The International Journal of Robotics Research, vol. 30, no. 7, pp. 895\u2013913, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Reciprocal velocity obstacles for real-time multi-agent navigation", "author": ["J. Van den Berg", "M. Lin", "D. Manocha"], "venue": "Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on, 2008, pp. 1928\u20131935.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "You\u2019ll never walk alone: Modeling social behavior for multi-target tracking", "author": ["S. Pellegrini", "A. Ess", "K. Schindler", "L. Van Gool"], "venue": "2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 261\u2013268.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Who are you with and where are you going?", "author": ["K. Yamaguchi", "A.C. Berg", "L.E. Ortiz", "T.L. Berg"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Feature-based prediction of trajectories for socially compliant navigation.", "author": ["M. Kuderer", "H. Kretzschmar", "C. Sprunk", "W. Burgard"], "venue": "in Robotics: science and systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Socially compliant mobile robot navigation via inverse reinforcement learning", "author": ["H. Kretzschmar", "M. Spies", "C. Sprunk", "W. Burgard"], "venue": "The International Journal of Robotics Research, p. 0278364915619772, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Socially adaptive path planning in human environments using inverse reinforcement learning", "author": ["B. Kim", "J. Pineau"], "venue": "International Journal of Social Robotics, vol. 8, no. 1, pp. 51\u201366, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards a society of robots", "author": ["A. Bicchi", "A. Fagiolini", "L. Pallottino"], "venue": "IEEE Robotics & Automation Magazine, vol. 17, no. 4, pp. 26\u201336, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Progress in developing a socially assistive mobile home robot companion for the elderly with mild cognitive impairment", "author": ["H.-M. Gross", "C. Schroeter", "S. Mueller", "M. Volkhardt", "E. Einhorn", "A. Bley", "C. Martin", "T. Langner", "M. Merten"], "venue": "Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on, 2011, pp. 2430\u20132437.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive shared control for a novel mobile assistive robot", "author": ["H. Wang", "X.P. Liu"], "venue": "IEEE/ASME Transactions on Mechatronics, vol. 19, no. 6, pp. 1725\u20131736, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, vol. 57, no. 5, pp. 469\u2013483, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, 2004, p. 1.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Maximum entropy inverse reinforcement learning.", "author": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "in AAAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 729\u2013736.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Planning-based prediction for pedestrians", "author": ["B.D. Ziebart", "N. Ratliff", "G. Gallagher", "C. Mertz", "K. Peterson", "J.A. Bagnell", "M. Hebert", "A.K. Dey", "S. Srinivasa"], "venue": "Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, 2009, pp. 3931\u2013 3936.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to navigate through crowded environments", "author": ["P. Henry", "C. Vollmer", "B. Ferris", "D. Fox"], "venue": "Robotics and Automation (ICRA), 2010 IEEE International Conference on, 2010, pp. 981\u2013986.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions", "author": ["P. Vernaza", "D. Bagnell"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 575\u2013583.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Activity forecasting", "author": ["K.M. Kitani", "B.D. Ziebart", "J.A. Bagnell", "M. Hebert"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 201\u2013214.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Map inference for bayesian inverse reinforcement learning", "author": ["J. Choi", "K.-E. Kim"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 1989\u20131997.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian process regression flow for analysis of motion trajectories", "author": ["K. Kim", "D. Lee", "I. Essa"], "venue": "Computer vision (ICCV), 2011 IEEE international conference on, 2011, pp. 1164\u20131171.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Social lstm: Human trajectory prediction in crowded spaces", "author": ["A. Alahi", "K. Goel", "V. Ramanathan", "A. Robicquet", "L. Fei-Fei", "S. Savarese"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 961\u2013971.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning social etiquette: Human trajectory understanding in crowded scenes", "author": ["A. Robicquet", "A. Sadeghian", "A. Alahi", "S. Savarese"], "venue": "European Conference on Computer Vision, 2016, pp. 549\u2013565.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Specification of the social force pedestrian model by evolutionary adjustment to video tracking  data", "author": ["A. Johansson", "D. Helbing", "P.K. Shukla"], "venue": "Advances in complex systems, vol. 10, no. supp02, pp. 271\u2013288, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Crowds by example", "author": ["A. Lerner", "Y. Chrysanthou", "D. Lischinski"], "venue": "Computer Graphics Forum, vol. 26, no. 3. Wiley Online Library, 2007, pp. 655\u2013664.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Pedestrian, crowd and evacuation dynamics", "author": ["D. Helbing", "A. Johansson"], "venue": "Encyclopedia of Complexity and Systems Science, 2009, pp. 6476\u20136495.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Socially inspired motion planning for mobile robots in populated environments", "author": ["J. M\u00fcller", "C. Stachniss", "K. Arras", "W. Burgard"], "venue": "Proc. of International Conference on Cognitive Systems, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Autonomous navigation in dynamic social environments using multi-policy decision making", "author": ["D. Mehta", "G. Ferrer", "E. Olson"], "venue": "Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, 2016, pp. 1190\u20131197.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic autonomous robot navigation in dynamic environments with human motion prediction", "author": ["A.F. Foka", "P.E. Trahanias"], "venue": "International Journal of Social Robotics, vol. 2, no. 1, pp. 79\u201394, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic window based approach to mobile robot motion control in the presence of moving obstacles", "author": ["M. Seder", "I. Petrovic"], "venue": "Robotics and Automation, 2007 IEEE International Conference on, 2007, pp. 1986\u20131991.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Motion planning in dynamic environments using velocity obstacles", "author": ["P. Fiorini", "Z. Shiller"], "venue": "The International Journal of Robotics Research, vol. 17, no. 7, pp. 760\u2013772, 1998.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "From perception to decision: A data-driven approach to end-toend motion planning for autonomous ground robots", "author": ["M. Pfeiffer", "M. Schaeuble", "J. Nieto", "R. Siegwart", "C. Cadena"], "venue": "arXiv preprint arXiv:1609.07910, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Decentralized noncommunicating multiagent collision avoidance with deep reinforcement learning", "author": ["Y.F. Chen", "M. Liu", "M. Everett", "J.P. How"], "venue": "arXiv preprint arXiv:1609.07845, 2016.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Extrinsic calibration of 2-d lidars using two orthogonal planes", "author": ["D.-G. Choi", "Y. Bok", "J.-S. Kim", "I.S. Kweon"], "venue": "IEEE Transactions on Robotics, vol. 32, no. 1, pp. 83\u201398, 2016.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimal planning for target localization and coverage using range sensing", "author": ["L.M. Miller", "T.D. Murphey"], "venue": "Automation Science and Engineering (CASE), 2015 IEEE International Conference on, 2015, pp. 501\u2013508.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "3-d mapping with an rgb-d camera", "author": ["F. Endres", "J. Hess", "J. Sturm", "D. Cremers", "W. Burgard"], "venue": "IEEE Transactions on Robotics, vol. 30, no. 1, pp. 177\u2013187, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Object modeling using a tof camera under an uncertainty reduction approach", "author": ["S. Foix", "G. Alenya", "J. Andrade-Cetto", "C. Torras"], "venue": "Robotics and Automation (ICRA), 2010 IEEE International Conference on, 2010, pp. 1306\u20131312.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Topological mapping and scene recognition with lightweight color descriptors for an omnidirectional camera", "author": ["M. Liu", "R. Siegwart"], "venue": "IEEE Transactions on Robotics, vol. 30, no. 2, pp. 310\u2013324, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Characterization of the compact hokuyo urg-04lx 2d laser range scanner", "author": ["L. Kneip", "F. T\u00e2che", "G. Caprari", "R. Siegwart"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on, 2009, pp. 1447\u20131454.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1997}, {"title": "Conjugate gradient methods", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Numerical optimization, pp. 101\u2013134, 2006.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Person tracking and following with 2d laser scanners", "author": ["A. Leigh", "J. Pineau", "N. Olmedo", "H. Zhang"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on, 2015, pp. 726\u2013733.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "For social navigation, the traditional approaches based on Dynamic Window Approach (DWA) [1] or potential fields [2], [3] are usually of limited efficacy as pedestrians are simply regarded as uncooperative obstacles.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "For social navigation, the traditional approaches based on Dynamic Window Approach (DWA) [1] or potential fields [2], [3] are usually of limited efficacy as pedestrians are simply regarded as uncooperative obstacles.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "For social navigation, the traditional approaches based on Dynamic Window Approach (DWA) [1] or potential fields [2], [3] are usually of limited efficacy as pedestrians are simply regarded as uncooperative obstacles.", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "An illustrative example is the freezing robot problem (FRP) [4], [5], where a mobile robot will be stuck in a narrow corridor when facing a crowd of people if it lacks the ability to predict the joint collision avoidance behaviors of human pedestrians.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "An illustrative example is the freezing robot problem (FRP) [4], [5], where a mobile robot will be stuck in a narrow corridor when facing a crowd of people if it lacks the ability to predict the joint collision avoidance behaviors of human pedestrians.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "end, researches have been done to understand the principles of humans\u2019 joint collision avoidance strategies and one of the pioneering works are the social force model (SFM) [6], [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 6, "context": "end, researches have been done to understand the principles of humans\u2019 joint collision avoidance strategies and one of the pioneering works are the social force model (SFM) [6], [7].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "Other joint collision avoidance model such as reciprocal velocity obstacles (RVO) have been proposed in [8], [9], [10], with an underlying assumption that all involved agents adopt the same collision avoidance strategies.", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "Other joint collision avoidance model such as reciprocal velocity obstacles (RVO) have been proposed in [8], [9], [10], with an underlying assumption that all involved agents adopt the same collision avoidance strategies.", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "Other joint collision avoidance model such as reciprocal velocity obstacles (RVO) have been proposed in [8], [9], [10], with an underlying assumption that all involved agents adopt the same collision avoidance strategies.", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "These ideas are also applied to visual tracking of pedestrians [11], [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "These ideas are also applied to visual tracking of pedestrians [11], [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 271, "endOffset": 275}, {"referenceID": 4, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 277, "endOffset": 280}, {"referenceID": 13, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 288, "endOffset": 292}, {"referenceID": 15, "context": "This capability is highly desirable for assistive mobile robots [16], [17], [18], which serve as assistants and companions and are expected to travel along with theirx human partners in not only home environment but also possibly crowded public areas.", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "This capability is highly desirable for assistive mobile robots [16], [17], [18], which serve as assistants and companions and are expected to travel along with theirx human partners in not only home environment but also possibly crowded public areas.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "This capability is highly desirable for assistive mobile robots [16], [17], [18], which serve as assistants and companions and are expected to travel along with theirx human partners in not only home environment but also possibly crowded public areas.", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 135, "endOffset": 138}, {"referenceID": 13, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 4, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 199, "endOffset": 202}, {"referenceID": 13, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 204, "endOffset": 208}, {"referenceID": 14, "context": "In addition, unlike [15], [14], the learned navigation policy operates without assess to the global map of the environment.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "In addition, unlike [15], [14], the learned navigation policy operates without assess to the global map of the environment.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 181, "endOffset": 184}, {"referenceID": 9, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "We evaluate the performance of our approach in both simulations and real-world experiments, by comparing it with a baseline planner based on RVO [8] and humans, repectively.", "startOffset": 145, "endOffset": 148}, {"referenceID": 18, "context": "Many researches have been proposed to describe the interactive navigation behaviors of humans by fitting a computational model to the observed pedestrians trajectories [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "In the field of robotics, a majority of work in this direction is done via inverse reinforcement learning (IRL) [20], which learns a cost function that explains the observed behaviors.", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "Instead, [15] adopts Maximum-A-Posteriori Bayesian IRL [27] to learn appropriate navigation behavior of a specific mobile robot from a set of demonstration trajectories.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "Instead, [15] adopts Maximum-A-Posteriori Bayesian IRL [27] to learn appropriate navigation behavior of a specific mobile robot from a set of demonstration trajectories.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Note that, the demonstration data in [15] is specific to configurations of the robot and its sensor and has to be collected via human operation, which could be time-consuming.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "On the other hand, [13], [14] learns probabilistic models of composite trajectories of pedestrians from video data by maximum entropy learning and IRL.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "On the other hand, [13], [14] learns probabilistic models of composite trajectories of pedestrians from video data by maximum entropy learning and IRL.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "Besides, interacting Gaussian process (IGP) is derived in [5] to model the joint trajectories of pedestrian while explicitly considering the effects of observation noise.", "startOffset": 58, "endOffset": 61}, {"referenceID": 10, "context": "In [11], Linear Trajectory Avoidance (LTA) is developed as a dynamic model for pedestrians in video space for shortterm trajectory prediction and it is integrated into visual tracking system.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "Gaussian process is adopted in [28] to learn the motion pattern of pedestrians.", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "Recently, Social LSTM is proposed in [29] for human trajectory prediction in crowd space.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "Similarly, the feature of social sensitivity is developed in [30] to analyze trajectories of pedestrians and bicyclists.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "One of the pioneering work in this direction is the social force model (SFM) [6], which uses energy/potential functions to encode the social status of pedestrian.", "startOffset": 77, "endOffset": 80}, {"referenceID": 30, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 43, "endOffset": 47}, {"referenceID": 32, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "In [34], the authors integrate a people tracker and an iterative A planner, with which the robot actively follows the pedestrian travelling in a similar direction to navigate through crowded environment.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "[35] follow the same idea and formulate the choice of a pedestrian to follow as a Multi-Policy Decision Making process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "On the other hands, [36] develops a hierarchical POMDP for predictive navigation in dynamic environment.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 138, "endOffset": 141}, {"referenceID": 36, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 37, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 213, "endOffset": 216}, {"referenceID": 8, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 218, "endOffset": 221}, {"referenceID": 9, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 36, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 219, "endOffset": 223}, {"referenceID": 7, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 293, "endOffset": 296}, {"referenceID": 8, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 298, "endOffset": 301}, {"referenceID": 9, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 303, "endOffset": 307}, {"referenceID": 38, "context": "During RPL, our policy is optimized by the PO-TRPO algorithm, which is derived based on the recent advances in deep reinforcement learning (DRL) [39], [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 39, "context": "During RPL, our policy is optimized by the PO-TRPO algorithm, which is derived based on the recent advances in deep reinforcement learning (DRL) [39], [40].", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "DRL exploits the massive representation power of deep neural networks (DNN) [41] to build a complex yet sophisticated decision model, with which an agent can directly learn from raw signals instead of carefully crafted feature and tends to act more intelligently.", "startOffset": 76, "endOffset": 80}, {"referenceID": 41, "context": "For example, an end-to-end motion planner is learned in [42] to map raw sensor data of a", "startOffset": 56, "endOffset": 60}, {"referenceID": 42, "context": "In [43], a decentralized multi-agent collision avoidance policy is learned via DRL, which can be thought as a DRL version of the original RVO approach [8].", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [43], a decentralized multi-agent collision avoidance policy is learned via DRL, which can be thought as a DRL version of the original RVO approach [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 43, "context": "Finally, a target-driven visual navigation policy for home environment is learned in [44] via DRL.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Robot motion dynamics: In this paper, synchro-drive mobile robots are considered, whose motion equation can be approximated by assuming the robot\u2019s velocities to be constant within a certain short time period [ti, ti+1] [1] with length \u2206t = ti+1 \u2212 ti.", "startOffset": 220, "endOffset": 223}, {"referenceID": 0, "context": "With the above formulations, our goal is optimizing a stochastic navigation policy P\u03b8 : O \u00d7 A \u2192 [0, 1] with parameters \u03b8 in order to maximize the expected discounted reward:", "startOffset": 96, "endOffset": 102}, {"referenceID": 44, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 145, "endOffset": 149}, {"referenceID": 45, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 151, "endOffset": 155}, {"referenceID": 46, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 163, "endOffset": 167}, {"referenceID": 47, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 189, "endOffset": 193}, {"referenceID": 48, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 222, "endOffset": 226}, {"referenceID": 49, "context": "These distances and offset angles can be easily obtained from the returned ranges array[50].", "startOffset": 87, "endOffset": 91}, {"referenceID": 39, "context": "The policy network P\u03b8 is to be trained with the Trust Region Policy Optimization (TRPO) [40] method.", "startOffset": 88, "endOffset": 92}, {"referenceID": 50, "context": "The output of the feature network is then fed to a LSTM network [51], a recurrent network for aggregation of the information collected through the navigation process.", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "The TRPO [40] algorithm is an effective on-policy optimization method for large nonlinear policies and tends to give monotonic improvement during the iterative optimization process.", "startOffset": 9, "endOffset": 13}, {"referenceID": 38, "context": "Next, for a trajectory s0:T , we use the generalized advantage estimation (GAE) [39] to construct an empirical estimation \u00c2 of the advantage function A\u03b8\u2212(ai|si) as the following:", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "By collecting a set of K trajectories {sk0:Tk , o k 0:Tk , ak0:Tk} K k=1, V\u0302\u03b6 is obtained by solving the following constrained regression problem [39]:", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "(51) which has the same form as the one obtained in [39], except that the policy P\u03b8(a|o) is conditioned on observation o instead.", "startOffset": 52, "endOffset": 56}, {"referenceID": 51, "context": "Finally, the constrained optimization problem described in (49) and (50) is solved by conjugate gradient algorithm [52] algorithm.", "startOffset": 115, "endOffset": 119}, {"referenceID": 52, "context": "Particularly, the environments, the deep neural network policy and the PO-TRPO algorithm (Algorithm 2) are developed under the framework of RLLAB [53].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "We make use of trajectories of interacting pedestrians collected from five different data sets, which includes the ETH and Hotel video clips from the ETH Walking Pedestrians (EWAP) [11], the motion capture (MC) data set from [14], as well as the Zara and UCY video clips from [32].", "startOffset": 181, "endOffset": 185}, {"referenceID": 13, "context": "We make use of trajectories of interacting pedestrians collected from five different data sets, which includes the ETH and Hotel video clips from the ETH Walking Pedestrians (EWAP) [11], the motion capture (MC) data set from [14], as well as the Zara and UCY video clips from [32].", "startOffset": 225, "endOffset": 229}, {"referenceID": 31, "context": "We make use of trajectories of interacting pedestrians collected from five different data sets, which includes the ETH and Hotel video clips from the ETH Walking Pedestrians (EWAP) [11], the motion capture (MC) data set from [14], as well as the Zara and UCY video clips from [32].", "startOffset": 276, "endOffset": 280}, {"referenceID": 49, "context": "Considering a Kobuki Turtlebot 2 with a Hokuyo URG04LX laser range finder [50] mounted on its top, we specify the sensor limitation of the robot in simulation as follows:", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "We compare the performance of our policy with a planner based on RVO [8], where the robot, its companion and the surrounding pedestrians are treated agents.", "startOffset": 69, "endOffset": 72}, {"referenceID": 53, "context": "For pedestrian detection and localization, we adopt the ROS-compatible leg tracker in [54].", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "In this paper, we present the Role Playing Learning (RPL) scheme for a mobile robot to navigate socially with its human companion in populated environments. Neural networks (NN) are constructed to parameterize a stochastic policy that directly maps sensory data collected by the robot to its velocity outputs, while respecting a set of social norms. An efficient simulative learning environment is built with maps and pedestrians trajectories collected from a number of real-world crowd data sets. In each learning iteration, a robot equipped with the NN policy is created virtually in the learning environment to play itself as a companied pedestrian and navigate towards a goal in a socially concomitant manner. Thus, we call this process Role Playing Learning, which is formulated under a reinforcement learning (RL) framework. The NN policy is optimized end-toend using Trust Region Policy Optimization (TRPO), with consideration of the imperfectness of robot\u2019s sensor measurements. Simulative and experimental results are provided to demonstrate the efficacy and superiority of our method. l", "creator": "LaTeX with hyperref package"}}}