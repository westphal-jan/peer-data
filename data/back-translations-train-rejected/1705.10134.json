{"id": "1705.10134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "On Residual CNN in text-dependent speaker verification task", "abstract": "Deep learning approaches are still not very common in the speaker verification field. We investigate the possibility of using deep residual convolutional neural network with spectrograms as an input features in the text-dependent speaker verification task. Despite the fact that we were not able to surpass the baseline system in quality, we achieved a quite good results for such a new approach getting an 5.23\\% ERR on the RSR2015 evaluation part. Fusion of the baseline and proposed systems outperformed the best individual system by 18\\% relatively.", "histories": [["v1", "Mon, 29 May 2017 11:50:57 GMT  (1699kb,D)", "https://arxiv.org/abs/1705.10134v1", "Accepted for Specom 2017"], ["v2", "Tue, 30 May 2017 13:17:47 GMT  (1708kb,D)", "http://arxiv.org/abs/1705.10134v2", "Accepted for Specom 2017"]], "COMMENTS": "Accepted for Specom 2017", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["egor malykh", "sergey novoselov", "oleg kudashev"], "accepted": false, "id": "1705.10134"}, "pdf": {"name": "1705.10134.pdf", "metadata": {"source": "CRF", "title": "On Residual CNN in text-dependent speaker verification task", "authors": ["Egor Malykh", "Sergey Novoselov", "Oleg Kudashev"], "emails": ["kudashev}@speechpro.com"], "sections": [{"heading": null, "text": "Tags: Speaker Verification, Remaining Learning Time, CNN, FFT"}, {"heading": "1 Introduction", "text": "I-vector systems are known to be state-of-the-art solutions to the text-dependent loudspeaker verification task [1-3, 21]. Recently, the solution of this task has increasingly been viewed from the perspective of deep learning approaches. For example, the ASR model of deep neural networks (DNN) [3,22] distinguishes acoustic space in senon classes and distinguishes the loudspeakers in this room using the classic model of total variability (TV). In such phonetic discriminatory DNN-based systems, two main approaches can be distinguished: the first is to use DNN background information to calculate tree-welch statistics, and the second is to use the bottleneck characteristics in combination with speaker-specific characteristics (MFCC) for the formation of the complete TV-UBM system. The second approach is considered the most robust for different conditions [4]. As recent publications show [6,8-10,23], the essential success of the highly discriminatory textual systems is due mainly to the modern supra-dependent supra-textual verification task."}, {"heading": "2 Baseline", "text": "The Ivector system models speech expression as a low-dimensional vector of channel and speaker dependent factors using the approach of total variability as follows: s = \u00b5 + Tw, where s is the mean supervector, \u00b5 is the mean supervector of a Universal Background Model (UBM), T is a low-grade matrix, and w is the i-vector estimated using the factor analysis method [1]. We used the implementation of the back-end from [16]. All i-vectors are length standardized and further regulated using the phrase-dependent covariance normalization (WCCN). Simple cosine-wise scoring is followed by the phrase-dependent s-norm score normalization [10]. 19 Mel frequency Cepstral coefficients (MFCC) + log energy is used as a basic feature. They are normalized by this system and through the vocal system."}, {"heading": "3 CNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Features", "text": "As acoustic input characteristics for this system, we use the normalized log power magnitude spectrum determined by Fast Fourier Transform (FFT). Spectrograms are extracted with the following parameters: Window size 256, Step size 64, and Sainsbury window function. An example of such a spectrogram is shown in Figure 1. The length of the spectrogram along the frequency axis is fixed, but the length along the timeline varies depending on the utterance. However, CNN needs a constant size image as input. To meet this requirement, we use the following technique: Images with a width of more than 800 pixels are cropped. Images with a width of less than 800 pixels are supplemented to the right by a separate copy."}, {"heading": "3.2 Residual architecture", "text": "Spectrograms as two-dimensional tensors can be viewed as images and can be processed by image processing methods. Currently, the best winding architecture for solving image processing tasks is a Residual CNN [15]. Residual architecture is described in [15, 20] as a stack of several residual units. Residual unit is a mappingxl + 1 = xl + F (xl, Wl), where xl and xl + 1 are the input and output of the unit. F consists of two 3 x 3 turns weighted Wl. Additional \"shortcut connection\" allows the network to fulfill the basic property: adding more layers does not result in degradation of the network. Thus, it is possible to train very deep networks with a size of 152 or more layers, as shown in [15]. For this study, a network of 18 layers of [15] with modifications from [20] was used."}, {"heading": "4 Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 RSR2015 corpus", "text": "In our experiments, we use the RSR2015 database [7]. RSR2015 provides data for three main use-case verification scenarios: - Unique passphrase: Each client speaks the same passphrase, - User-dependent passphrase: Each client pronounces its own passphrase, - Inspired text: Each client pronounces a sentence initiated by the system. In this work, we focus on the first use case where each speaker pronounces a specific sentence. RSR2015 database contains audio recordings of 300 speakers (143 female and 157 male). There are 9 sessions for each participant. Each session consists of 30 short sentences. The database is collected in the office environment with six different portable recording devices (four smartphones and two tablets). Each speaker was recorded with three random different devices from the sixty.The database is randomly divided into three non-overlapping groups of female speakers, one for each of the 47 evaluations, one for each of the 47 evaluations."}, {"heading": "4.2 Baseline", "text": "As described in [16], we use the following representation of the WCCN matrix: W = W + 12 E, where E is the unit matrix of appropriate dimensionality. This trick helps to prevent overmatching despite the small number of speakers in the background."}, {"heading": "4.3 CNN", "text": "CNN is implemented using the Keras framework [17] over the TensorFlow [18] backend. ADAM Optimizer [19] with a learning rate of 10 \u2212 4 is used for the training.Network is trained to distinguish all speakers in the training set by means of a softmax layer and categorical cross-entropy loss function. In the evaluation phase, an output from the 512-dimensional (like i-vector) penultimate layer is used as an embedding corresponding to the input anaesthetic."}, {"heading": "5 Results and discussion", "text": "The result of our research is presented in Table 2 with respect to the Equal Error Rate (EER) and the Minimum Detection Cost Function (minDCF) with Ptar = 10 \u2212 3. The base system showed a very good result with an EER of less than 1%, which is comparable to the result of [16]. The Deep CNN system achieved an EER of 6.02%. The merger of these two systems shows a relative improvement of 18% over the base system, which is proof that classical i-vector systems and deep learning systems lead to decorative embedding and can therefore be shared. The relatively poor performance of the investigated system can be explained by the small size of the training set (97 speakers). Such conditions lead to overequipment of the discriminatory model. The hypothesis is that the deep rest of CNN requires much more data for training and the extension of the training set will lead to a significant increase in accuracy. Experiments on the Extended Training Set (19ssER), which leads to a 5.23% viewing EER."}, {"heading": "6 Conclusion", "text": "In this paper, we presented studies on the deep CNN residual architecture in text-dependent verification, using raw normalized spectrograms of speech signals as input characteristics. Experiments conducted on Part 1 of the RSR2015 database showed that, despite the small amount of training data, it is possible to train a deep speaker embedding extractor, which makes it possible to separate the speaker classes fairly well. The best result achieved by the single system is a 5.23% EER. We also demonstrated that increasing the training data set leads to the expected strengthening of the extractor and improvement of results. Our future work will focus on improving the quality of deep CNN-based systems and raising them to the level of baseline i-vector systems. It is already evident that the merging of the deep CNN and i-vector extractors results in a good 18% increase in performance."}, {"heading": "7 Acknowledgements", "text": "This work was financially supported by the Ministry of Education and Science of the Russian Federation, Contract 14.578.21.0126 (ID RFMEFI57815X0126)."}], "references": [{"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A study of interspeaker variability in speaker verification", "author": ["P. Kenny", "P. Ouellet", "N. Dehak", "V. Gupta", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "McLaren", "May"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Advances in deep neural network approaches to speaker recognition", "author": ["M. McLaren", "Y. Lei", "Ferrer", "April"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Text-dependent speaker recognition using PLDA with uncertainty propagation", "author": ["T. Stafylakis", "P. Kenny", "P. Ouellet", "J. Perez", "M. Kockmann", "P. Dumouchel"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Database for Text-Dependent Speaker Verification using Multiple Pass-Phrases", "author": ["A. Larcher", "K.A. Lee", "B. Ma", "Li", "H. (2012", "September). RSR2015"], "venue": "In INTERSPEECH", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1583}, {"title": "Text-dependent speaker verification: Classifiers, databases and RSR2015", "author": ["A. Larcher", "K.A. Lee", "B. Ma", "H. Li"], "venue": "Speech Communication,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Text dependent speaker verification using a small development", "author": ["H. Aronowitz"], "venue": "set. In Odyssey 2012-The Speaker and Language Recognition Workshop", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Textdependent GMM-JFA system for password based speaker verification", "author": ["S. Novoselov", "T. Pekhovsky", "A. Shulipa", "Sholokhov", "May"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Analysis of DNN approaches to speaker identification", "author": ["P. Mat\u011bjka", "O. Glembek", "O. Novotn\u00fd", "O. Plchot", "F. Gr\u00e9zl", "L. Burget", "Cernock\u00fd", "J. H", "March"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["E. Variani", "X. Lei", "E. McDermott", "I.L. Moreno", "Gonzalez-Dominguez", "May"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "Shazeer", "March"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "2016, December). Endto-End attention based text-dependent speaker verification", "author": ["S.X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Deep Neural Networks and Hidden Markov Models in i-vector-based Text-Dependent Speaker Verification", "author": ["H. Zeinali", "L. Burget", "H. Sameti", "O. Glembek", "Plchot", "June"], "venue": "In Odyssey-The Speaker and Language Recognition Workshop", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "S. Ghemawat"], "venue": "arXiv preprint arXiv:1603.04467", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "Sun", "October"], "venue": "In European Conference on Computer Vision (pp", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Non-linear PLDA for i-vector speaker verification", "author": ["S. Novoselov", "T. Pekhovsky", "O. Kudashev", "V. Mendelev", "A. Prudnikov"], "venue": "Proc. of the Annual Conference of the International Speech Communication", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Usage of DNN in speaker recognition: advantages and problems", "author": ["O. Kudashev", "S. Novoselov", "T. Pekhovsky", "K. Simonchik", "Lavrentyeva", "July"], "venue": "In International Symposium on Neural Networks (pp", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Pldabased system for text-prompted password speaker verification", "author": ["S. Novoselov", "T. Pekhovsky", "A. Shulipa", "Kudashev", "August"], "venue": "In Advanced Video and Signal Based Surveillance (AVSS),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 1, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 2, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 18, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 2, "context": "For instance, ASR deep neural network (DNN) model [3,22] divides the acoustic space into senone classes and discriminates the speakers in this space using the classic total variability (TV) model [1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 19, "context": "For instance, ASR deep neural network (DNN) model [3,22] divides the acoustic space into senone classes and discriminates the speakers in this space using the classic total variability (TV) model [1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "For instance, ASR deep neural network (DNN) model [3,22] divides the acoustic space into senone classes and discriminates the speakers in this space using the classic total variability (TV) model [1].", "startOffset": 196, "endOffset": 199}, {"referenceID": 3, "context": "The second approach is considered the most robust to varying conditions [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 6, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 7, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 8, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 20, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 9, "context": "Thus, the success of the phonetic discriminative DNN in such a task leads to attempts to use similar approach in text-dependent systems [5, 11,16].", "startOffset": 136, "endOffset": 146}, {"referenceID": 14, "context": "Thus, the success of the phonetic discriminative DNN in such a task leads to attempts to use similar approach in text-dependent systems [5, 11,16].", "startOffset": 136, "endOffset": 146}, {"referenceID": 11, "context": "In parallel, there are several studies on the use of Deep-Learning approaches aiming to create an end-to-end solutions for discriminating speakers directly in a text-dependent task [13, 14].", "startOffset": 181, "endOffset": 189}, {"referenceID": 12, "context": "In parallel, there are several studies on the use of Deep-Learning approaches aiming to create an end-to-end solutions for discriminating speakers directly in a text-dependent task [13, 14].", "startOffset": 181, "endOffset": 189}, {"referenceID": 10, "context": "[12] describes a DNN for extracting a small speaker footprint which can be used to discriminate between speakers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "In this paper we investigate the deep residual CNN [15] for direct speaker discrimination.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Unlike [14] we focus on the use of spectrograms instead of MFCC as the input features and deep but light residual architecture instead of VGG-like network as the mapping.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "where s is the mean supervector, \u03bc is the mean supervector of an Universal Background Model (UBM), T is a low rank matrix and w is the i-vector estimated using the Factor Analysis method [1].", "startOffset": 187, "endOffset": 190}, {"referenceID": 14, "context": "We used implementation of the back-end from [16].", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "A simple cosine distance scoring is used followed by phrase-dependent s-norm score normalization [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Currently, the best convolutional architecture for solving image processing tasks is a Residual CNN [15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Residual architecture is described in [15, 20] as a stack of several residual units.", "startOffset": 38, "endOffset": 46}, {"referenceID": 17, "context": "Residual architecture is described in [15, 20] as a stack of several residual units.", "startOffset": 38, "endOffset": 46}, {"referenceID": 13, "context": "Thus, it becomes possible to train very deep networks with a size of 152 or more layers, as shown in the [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "For this study, a network with 18 layers from [15] with modifications from [20] was used.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "For this study, a network with 18 layers from [15] with modifications from [20] was used.", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "In our experiments we use the RSR2015 database [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 14, "context": "As described in [16], we use the following representation of the WCCN matrix:", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "CNN is implemented using the Keras framework [17] on top of the TensorFlow [18] backend.", "startOffset": 75, "endOffset": 79}, {"referenceID": 16, "context": "ADAM optimizer [19] with learning rate set at 10\u22124 is used for training Network is trained to discriminate between all speakers in training set using the softmax layer and categorical cross-entropy loss function.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "Baseline system demonstrated a very good result with an EER of less than 1% which is comparable with the result from [16].", "startOffset": 117, "endOffset": 121}], "year": 2017, "abstractText": "Deep learning approaches are still not very common in the speaker verification field. We investigate the possibility of using deep residual convolutional neural network with spectrograms as an input features in the text-dependent speaker verification task. Despite the fact that we were not able to surpass the baseline system in quality, we achieved a quite good results for such a new approach getting an 5.23% ERR on the RSR2015 evaluation part. Fusion of the baseline and proposed systems outperformed the best individual system by 18% relatively.", "creator": "LaTeX with hyperref package"}}}