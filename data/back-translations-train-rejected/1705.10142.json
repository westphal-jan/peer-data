{"id": "1705.10142", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Kronecker Recurrent Units", "abstract": "Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrence matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem.", "histories": [["v1", "Mon, 29 May 2017 12:14:45 GMT  (379kb,D)", "http://arxiv.org/abs/1705.10142v1", null], ["v2", "Tue, 30 May 2017 08:32:37 GMT  (379kb,D)", "http://arxiv.org/abs/1705.10142v2", null], ["v3", "Wed, 31 May 2017 08:55:38 GMT  (379kb,D)", "http://arxiv.org/abs/1705.10142v3", null], ["v4", "Wed, 26 Jul 2017 11:05:47 GMT  (380kb,D)", "http://arxiv.org/abs/1705.10142v4", null], ["v5", "Fri, 27 Oct 2017 09:33:54 GMT  (310kb,D)", "http://arxiv.org/abs/1705.10142v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cijo jose", "moustpaha cisse", "francois fleuret"], "accepted": false, "id": "1705.10142"}, "pdf": {"name": "1705.10142.pdf", "metadata": {"source": "CRF", "title": "Kronecker Recurrent Units", "authors": ["Cijo Jose", "Moustapha Ciss\u00e9"], "emails": ["cijo.jose@idiap.ch", "moustaphacisse@fb.com", "francois.fleuret@idiap.ch"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in fact, in which they, live, in which they, in which they, in which they, in which they, in fact, in fact, live, in which they, in which they, in which they, in which they, in fact, are able to move, in which they, are able to move, in which they, in which they, are able to move, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, are able to move, in which they, in which they, in which"}, {"heading": "2 Recurrent neural network formalism", "text": "Table 1 summarizes some of the notations we use in the work. We consider the field complex rather than real numbers. We will motivate the selection of complex numbers later in this section. Consider a standard recursive neural network Elman [1990]. Given a sequence of T input vectors: x0, x1,..., xT \u2212 1, t RNN performs in a time step the following: ht = \u03c3 (Wht \u2212 1 + Uxt + b) (1) y-t = Vht + c, (2) where y-t is the predicted value at the time of the step t."}, {"heading": "2.1 Over parameterization and computational efficiency", "text": "The total number of parameters in an RNN is c (dn + n2 + n + m + nm), where c stands for real parameters and 2 for complex parameters. As we can see, the number of parameters grows square with the hidden dimension, i.e. O (n2). We show in the experiments that this quadratic growth is an over-parameterization for many problems of the real world. Furthermore, it has a direct influence on the computing efficiency of the RNNs, since the evaluation of Wht \u2212 1O (n2) takes time and recursively depends on previous hidden states. However, other components Uxt and Vht can usually be efficiently calculated by a single matrix matrix multiplication for each of the components. That is, we can perform U [x0,... xT] and V [h0,... hT \u2212 1] to determine the efficiency of the matrix, if we are then able to determine the number of the matrix."}, {"heading": "2.2 Poor conditioning implies gradients explode or vanish", "text": "The problem of the disappearing and exploding gradient relates to the decay or growth of the partial derivative of the loss L (.) in relation to the hidden state ht, i.e. the number of time steps in which T grows Arjovsky et al. [2016]. Applying the chain rule, Arjovsky et al. [2016] can be shown that if the absolute value of the eigenvalues of W deviates from 1, L \u2202 ht can explode or disappear exponentially quickly in relation to T. Thus, a strategy for preventing the disappearing and exploding gradient is to control the spectrum of W."}, {"heading": "2.3 Why complex field?", "text": "Although Arjovsky et al. [2016] and Wisdom et al. [2016] use complex value networks with uniform constraints on the recurring matrix, the motivations for such models are not clear. We give a simple but compelling reason for complex evaluated recurring networks. The absolute value of the determinant of a uniform matrix is 1. Therefore, the set of all uniform (orthogonal) matrices in real space has a determinant of 1 or \u2212 1, i.e. the set of all rotations or reflections. Since the determinant is a continuous function, the uniform set is decoupled in real space. Consequently, with real value networks we cannot encompass the full uniform set with the usual continuous optimization procedures. On the contrary, the uniform set is connected in complex space, as its determinants are the points on the unit circle and we do not have this problem."}, {"heading": "3 Kronecker recurrent units (KRU)", "text": "We consider the parameterization of the recurring matrix W as a Kronecker product of k matrices W0,..., Wk \u2212 1, W = W1 \u00b7 \u00b7 Wk = ki = 1Wi. (4) Where each Wi-Cpi \u00b7 qi and each i = 0 pi = k i = 0 qi = n. To illustrate the Kronecker product of the matrices, we consider the simple case if we use i {pi = qi = 2}. This implies k = log 2n. And W is defined as follows repeatedly: W = log 2ni = 1 Wi = [w1 (1, 1) w1 (2) w1 (2, 1) w1 (2, 2) w1 (2) w1 (2, 2) log 2ni = 2 Wi, (5) = [w1) W2 w1 (1, 2) W2 w1 (2), Wi (2), Wi (3) if we hide the control between the two factors."}, {"heading": "3.1 Soft unitary constraint", "text": "Poor conditioning leads to disappearing or exploding gradients. Unfortunately, the standard solution, which consists of optimization based on the strict one-size-fits-all rule, suffers from the maintenance of noise over time. In fact, the small eigenvalues of the recurring matrix can be a really disappearing long-term influence on the problem in question, and in this sense, there may be good or bad disappearing gradients. Consequently, enforcing strict one-size-fits-all restrictions (which force the network to never forget) can be a bad strategy. A simple solution to get the best of both worlds is to enforce uniform limitations, for example through the following regulation: \"WHi Wi \u2212 I.\" 2, \"k.\" (7) Please note that these limitations are applied to every factor of the Kronecker product. This procedure is very efficient from a computational point of view, since the size of each factor is usually small. It is enough to do this if each of the Kronecker factors is approximately the same."}, {"heading": "4 Experiments", "text": "All models are implemented in C + +. Our library uses OpenBLAS and cuBLAS for fast matrix operations on CPU and GPU, respectively. We will publish our library to reproduce all the results we report in this article. We use tanh as an activation function for RNN and LSTM. While ueRNN, fuRNN and our model, KRU uses complex rectified linear units Arjovsky et al. [2016]."}, {"heading": "4.1 Copy memory problem", "text": "In this problem, each sequence of length T + 20 and each element in the sequence come from 10 classes {0,..., 9}. The first 10 elements are consistently sampled with substitutes from {1,.., 8} The aim of the model is to output a sequence of T + 10 blank categories, followed by the 10 element sequence from the beginning of the input sequence. The expected average cross-sectional entropy for a memorable strategy is 10 log 8 T + 20. Our experimental setup closely follows Wisdom et al. [2016] which in turn follows Arjovsky et al. [2016] but T expanded to 1000 and 2000. Our model, KRU uses a hidden dimension of Ja37x8 = 12x2 = 12x2 corresponding. We are not able to solve this problem."}, {"heading": "4.2 Adding problem", "text": "According to Arjovsky et al. [2016] we describe the addition problem Hochreiter and Schmidhuber [1997]. Each input vector is composed of two sequences of length T. The first sequence is sampled by U [0, 1]. In the second sequence, exactly two of the entries 1, the \"marker\" and the rest is 0 fail. The first sequence is sampled evenly in the first half of the sequence and the other 1 is again evenly in the other half of the sequence. The goal of the network is to predict the sum of the numbers from the first sequence corresponding to the marked locations in the second sequence. We evaluate four settings as in Arjovsky et al. [2016] with T = 100, T = 200, T = 400, T = 750. For all four settings, KRU uses a hidden dimension N of 512 with 2x2 Kronecker factors corresponding to the corresponding 3K parameters."}, {"heading": "4.3 Pixel by pixel MNIST", "text": "As outlined by Le et al. [2015], we evaluate the MNIST task pixel by pixel. We consider two tasks: (1) pixels are read from left to right from top or bottom, and (2) pixels are randomly permutated before they are shown to the network. The sequence length for these tasks is T = 28 \u00d7 28 = 784. The size of the MNIST training set is 60K, of which we choose 5K as the validation set. Models are trained at the remaining 55K points. The model that provides the best validation accuracy is selected to evaluate the test sets. All models are trained using RMSprop with a learning rate of 1e \u2212 3 and a decay of 0.9. Results are summarized in Table 2 and Figure 3."}, {"heading": "4.4 Character level language modelling on Penn TreeBank (PTB)", "text": "The vocabulary size has been limited to 10K most common words, and the rest of the words will be replaced by a special < UNK > character Mikolov [2012].The total number of unique characters in the data set is 50, including the special < UNK > character. All models have been trained for 20 epochs with a batch size of 50 and use ADAM Kingma and Ba [2014].We use a learning rate of 1e \u2212 3, found by cross-validation with the standard parameters Beta and Ba [2014].If we do not see an improvement in the validation bits per character, the learning rate is reduced by 0.30."}, {"heading": "4.5 Framewise phoneme classification on TIMIT", "text": "We evaluate the models for this task using the real TIMIT dataset Garofolo et al. [1993]. TIMIT contains a training set of 4620 statements, of which we use 184 as a validation set. The test set consists of 1680 statements. We extract 12 Mel Frequency Cepstrum Coefficients (MFCC) Marmelstein [1976] from 26 filter banks and also the log energy per frame. We also link the first derivative, which results in a characteristic descriptor of the dimension 26 per frame. The image size is selected to 10ms and the window size is 25ms. The number of time steps at which the back propagation through time (BPTT) is best corresponds to the length of each sequence. \u2212 Since each sequence is different in length, this means that for each sample BPTT rate has different values."}, {"heading": "5 Conclusion", "text": "We have presented a new recursive neural network model, which is essentially based on a Kronecker factored recurrent matrix. This factorization provides fine control over the model capacity and the calculation requirements by the size of the factors. In addition, it enables us to efficiently enforce a soft, uniform constraint. Experimental results show that our approach surpasses classical methods that use O (n2) parameters in the recursive matrix. Perhaps just as important, these experiments show that both toy problems (\u00a7 4.1 and 4.2) and real ones (\u00a7 4.3, 4.4 and \u00a7 4.5), while existing methods require tens of thousands of parameters in the recursive matrix, can be achieved competitively or better than state-of-the-art ones, with less than 100 parameters. These surprising results offer a new and counterintuitive perspective on desirable memory architectures: The state should remain high in order to allow the use of highly dynamic internal networks, but to allow the dynamic idea to flow into other, more dynamic ones."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Arjovsky et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2016}, {"title": "Do deep nets really need to be deep? In Advances in neural information processing", "author": ["Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "Ba and Caruana.,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James Wilson", "Stephen Tyree", "Kilian Weinberger", "Yixin Chen"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICML, pages 2067\u20132075,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Parseval networks: Improving robustness to adversarial examples", "author": ["Moustapha Cisse", "Piotr Bojanowski", "Edouard Grave", "Yann Dauphin", "Nicolas Usunier"], "venue": "arXiv preprint arXiv:1704.08847,", "citeRegEx": "Cisse et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cisse et al\\.", "year": 2017}, {"title": "Low precision storage for deep learning", "author": ["Matthieu Courbariaux", "Jean-Pierre David", "Yoshua Bengio"], "venue": "Arxiv: 1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon technical report n,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Orthogonal RNNs and long-memory tasks", "author": ["Mikael Henaff", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1602.06662,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, diploma thesis, institut f\u00fcr informatik, lehrstuhl prof. brauer, technische universit\u00e4t m\u00fcnchen", "author": ["Sepp Hochreiter"], "venue": null, "citeRegEx": "Hochreiter.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter.", "year": 1991}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks-with an erratum note", "author": ["Herbert Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,", "citeRegEx": "Jaeger.,? \\Q2001\\E", "shortCiteRegEx": "Jaeger.", "year": 2001}, {"title": "Scalable metric learning via weighted approximate rank component analysis", "author": ["Cijo Jose", "Fran\u00e7ois Fleuret"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Jose and Fleuret.,? \\Q2016\\E", "shortCiteRegEx": "Jose and Fleuret.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Quoc Le", "Tam\u00e1s Sarl\u00f3s", "Alex Smola"], "venue": "In Proceedings of the international conference on machine learning,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Distance measures for speech recognition, psychological and instrumental", "author": ["Paul Mermelstein"], "venue": "Pattern recognition and artificial intelligence,", "citeRegEx": "Mermelstein.,? \\Q1976\\E", "shortCiteRegEx": "Mermelstein.", "year": 1976}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": "PhD thesis,", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "The ubiquitous kronecker product", "author": ["Charles F Van Loan"], "venue": "Journal of computational and applied mathematics,", "citeRegEx": "Loan.,? \\Q2000\\E", "shortCiteRegEx": "Loan.", "year": 2000}, {"title": "Full-capacity unitary recurrent neural networks", "author": ["Scott Wisdom", "Thomas Powers", "John Hershey", "Jonathan Le Roux", "Les Atlas"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Wisdom et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wisdom et al\\.", "year": 2016}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Deep neural networks have defined the state-of-the-art in a wide range of problems in computer vision, speech analysis, and natural language processing Krizhevsky et al. [2012], Hinton et al.", "startOffset": 152, "endOffset": 177}, {"referenceID": 7, "context": "[2012], Hinton et al. [2012], Mikolov [2012].", "startOffset": 8, "endOffset": 29}, {"referenceID": 7, "context": "[2012], Hinton et al. [2012], Mikolov [2012]. However, these models suffer from two key issues.", "startOffset": 8, "endOffset": 45}, {"referenceID": 7, "context": "[2012], Hinton et al. [2012], Mikolov [2012]. However, these models suffer from two key issues. (1) They are over-parameterized; thus it takes a very long time for training and inference. (2) Learning deep models is difficult because of the poor conditioning of the matrices that parameterize the model. These difficulties are especially relevant to recurrent neural networks. Indeed, the number of distinct parameters in RNNs grows as the square of the size of the hidden state conversely to convolutional networks which enjoy weight sharing. Moreover, poor conditioning of the recurrent matrices results in the gradients to explode or vanish exponentially fast along the time horizon. This problem prevents RNN from capturing long-term dependencies Hochreiter [1991], Bengio et al.", "startOffset": 8, "endOffset": 769}, {"referenceID": 1, "context": "This problem prevents RNN from capturing long-term dependencies Hochreiter [1991], Bengio et al. [1994]. There exists an extensive body of literature addressing over-parameterization in neural networks.", "startOffset": 83, "endOffset": 104}, {"referenceID": 1, "context": "This problem prevents RNN from capturing long-term dependencies Hochreiter [1991], Bengio et al. [1994]. There exists an extensive body of literature addressing over-parameterization in neural networks. The problem was first studied by LeCun et al. [1990] and they proposed to remove unimportant weights in neural networks by exploiting the second order information.", "startOffset": 83, "endOffset": 256}, {"referenceID": 1, "context": "This problem prevents RNN from capturing long-term dependencies Hochreiter [1991], Bengio et al. [1994]. There exists an extensive body of literature addressing over-parameterization in neural networks. The problem was first studied by LeCun et al. [1990] and they proposed to remove unimportant weights in neural networks by exploiting the second order information. Several techniques which followed include low-rank decomposition Denil et al. [2013], training a small network on the soft-targets predicted by a big pre-trained network Ba and Caruana [2014], low bit precision training Courbariaux et al.", "startOffset": 83, "endOffset": 452}, {"referenceID": 1, "context": "[2013], training a small network on the soft-targets predicted by a big pre-trained network Ba and Caruana [2014], low bit precision training Courbariaux et al.", "startOffset": 92, "endOffset": 114}, {"referenceID": 1, "context": "[2013], training a small network on the soft-targets predicted by a big pre-trained network Ba and Caruana [2014], low bit precision training Courbariaux et al. [2014], hashing Chen et al.", "startOffset": 92, "endOffset": 168}, {"referenceID": 1, "context": "[2013], training a small network on the soft-targets predicted by a big pre-trained network Ba and Caruana [2014], low bit precision training Courbariaux et al. [2014], hashing Chen et al. [2015], etc.", "startOffset": 92, "endOffset": 196}, {"referenceID": 1, "context": "[2013], training a small network on the soft-targets predicted by a big pre-trained network Ba and Caruana [2014], low bit precision training Courbariaux et al. [2014], hashing Chen et al. [2015], etc. These techniques are primarily aimed at feed-forward fully connected networks. A notable exception is the deep fried convnets Yang et al. [2015] which explicitly parameterizes the fully connected layers in a convnet with a computationally cheap and ar X iv :1 70 5.", "startOffset": 92, "endOffset": 347}, {"referenceID": 14, "context": "parameter-efficient structured linear operator, the Fastfood transform Le et al. [2013]. Very few studies have focused on the particular case of recurrent networks Arjovsky et al.", "startOffset": 71, "endOffset": 88}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention.", "startOffset": 75, "endOffset": 98}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs.", "startOffset": 75, "endOffset": 223}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al.", "startOffset": 75, "endOffset": 404}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively.", "startOffset": 75, "endOffset": 454}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively. Other popular strategies include gradient clipping Pascanu et al. [2013], and orthogonal initialization of the recurrent weights Le et al.", "startOffset": 75, "endOffset": 588}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively. Other popular strategies include gradient clipping Pascanu et al. [2013], and orthogonal initialization of the recurrent weights Le et al. [2015]. More recently Arjovsky et al.", "startOffset": 75, "endOffset": 661}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively. Other popular strategies include gradient clipping Pascanu et al. [2013], and orthogonal initialization of the recurrent weights Le et al. [2015]. More recently Arjovsky et al. [2016] proposed to use a unitary recurrent weight matrix.", "startOffset": 75, "endOffset": 699}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively. Other popular strategies include gradient clipping Pascanu et al. [2013], and orthogonal initialization of the recurrent weights Le et al. [2015]. More recently Arjovsky et al. [2016] proposed to use a unitary recurrent weight matrix. The use of norm preserving unitary maps prevent the gradients from exploding or vanishing, and thus help to capture long-term dependencies. The resulting model called unitary evolution RNN (ueRNN) is computationally efficient since it only explores a small subset of general unitary matrices. Unfortunately, since ueRNNs can only span a reduced subset of unitary matrices their expressive power is limited Wisdom et al. [2016]. Full capacity unitary RNN (fuRNN) Wisdom et al.", "startOffset": 75, "endOffset": 1177}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively. Other popular strategies include gradient clipping Pascanu et al. [2013], and orthogonal initialization of the recurrent weights Le et al. [2015]. More recently Arjovsky et al. [2016] proposed to use a unitary recurrent weight matrix. The use of norm preserving unitary maps prevent the gradients from exploding or vanishing, and thus help to capture long-term dependencies. The resulting model called unitary evolution RNN (ueRNN) is computationally efficient since it only explores a small subset of general unitary matrices. Unfortunately, since ueRNNs can only span a reduced subset of unitary matrices their expressive power is limited Wisdom et al. [2016]. Full capacity unitary RNN (fuRNN) Wisdom et al. [2016] proposed to overcome this issue by parameterizing the recurrent matrix with a full dimensional unitary matrix, hence sacrificing computational efficiency.", "startOffset": 75, "endOffset": 1233}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively. Other popular strategies include gradient clipping Pascanu et al. [2013], and orthogonal initialization of the recurrent weights Le et al. [2015]. More recently Arjovsky et al. [2016] proposed to use a unitary recurrent weight matrix. The use of norm preserving unitary maps prevent the gradients from exploding or vanishing, and thus help to capture long-term dependencies. The resulting model called unitary evolution RNN (ueRNN) is computationally efficient since it only explores a small subset of general unitary matrices. Unfortunately, since ueRNNs can only span a reduced subset of unitary matrices their expressive power is limited Wisdom et al. [2016]. Full capacity unitary RNN (fuRNN) Wisdom et al. [2016] proposed to overcome this issue by parameterizing the recurrent matrix with a full dimensional unitary matrix, hence sacrificing computational efficiency. Indeed, fuRNN requires a computationally expensive projection step which takes O(n) time (n being the size of the hidden state) at each step of the stochastic optimization to maintain the unitary constraint on the recurrent matrix. Although the idea of parameterizing recurrent weight matrices with strict unitary linear operator is appealing, it suffers from several issues: (1) Strict unitary constraints severely restrict the search space of the model, thus making the learning process unstable. (2) Strict unitary constraints make forgetting irrelevant information difficult. While this may not be an issue for problems with non-vanishing long term influence, it causes failure when dealing with real world problems that have vanishing long term influence. Henaff et al. [2016] have previously pointed out that the good performance of strict unitary models on certain synthetic problems is because it exploits the biases in these data-sets which favours a unitary recurrent map and these models may not generalize well to real world data-sets.", "startOffset": 75, "endOffset": 2170}, {"referenceID": 0, "context": "Very few studies have focused on the particular case of recurrent networks Arjovsky et al. [2016]. The problem of vanishing and exploding gradients has also received significant attention. Hochreiter and Schmidhuber [1997] proposed an effective gating mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units (GRU) Chung et al. [2015] and the Highway networks Srivastava et al. [2015] for recurrent and feed-forward neural networks respectively. Other popular strategies include gradient clipping Pascanu et al. [2013], and orthogonal initialization of the recurrent weights Le et al. [2015]. More recently Arjovsky et al. [2016] proposed to use a unitary recurrent weight matrix. The use of norm preserving unitary maps prevent the gradients from exploding or vanishing, and thus help to capture long-term dependencies. The resulting model called unitary evolution RNN (ueRNN) is computationally efficient since it only explores a small subset of general unitary matrices. Unfortunately, since ueRNNs can only span a reduced subset of unitary matrices their expressive power is limited Wisdom et al. [2016]. Full capacity unitary RNN (fuRNN) Wisdom et al. [2016] proposed to overcome this issue by parameterizing the recurrent matrix with a full dimensional unitary matrix, hence sacrificing computational efficiency. Indeed, fuRNN requires a computationally expensive projection step which takes O(n) time (n being the size of the hidden state) at each step of the stochastic optimization to maintain the unitary constraint on the recurrent matrix. Although the idea of parameterizing recurrent weight matrices with strict unitary linear operator is appealing, it suffers from several issues: (1) Strict unitary constraints severely restrict the search space of the model, thus making the learning process unstable. (2) Strict unitary constraints make forgetting irrelevant information difficult. While this may not be an issue for problems with non-vanishing long term influence, it causes failure when dealing with real world problems that have vanishing long term influence. Henaff et al. [2016] have previously pointed out that the good performance of strict unitary models on certain synthetic problems is because it exploits the biases in these data-sets which favours a unitary recurrent map and these models may not generalize well to real world data-sets. Our motivation is to address the problems of existing recurrent networks mentioned above. We present a new model called Kronecker Recurrent Units (KRU). At the heart of KRU is the use of Kronecker factored recurrent matrix which provide an elegant way to adjust the number of parameters to the problem at hand. This factorization allows us to finely modulate the number of parameters required to encode n \u00d7 n matrices, from O(log(n)) when using factors of size 2 \u00d7 2, to O(n) parameters when using a single factor of the size of the matrix itself. We tackle the vanishing and exploding gradient problem through a soft unitary constraint. Thanks to the properties of Kronecker matrices Van Loan [2000], this constraint can be enforced efficiently.", "startOffset": 75, "endOffset": 3137}, {"referenceID": 8, "context": "Consider a standard recurrent neural network Elman [1990]. Given a sequence of T input vectors:", "startOffset": 45, "endOffset": 58}, {"referenceID": 0, "context": "\u2202L \u2202ht as the number of time steps T grows Arjovsky et al. [2016]. By the application of the chain rule, the following can be shown Arjovsky et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 0, "context": "\u2202L \u2202ht as the number of time steps T grows Arjovsky et al. [2016]. By the application of the chain rule, the following can be shown Arjovsky et al. [2016]: \u2225\u2225\u2225\u2225 \u2202L \u2202ht \u2225\u2225\u2225\u2225 \u2264 \u2016W\u2016 .", "startOffset": 43, "endOffset": 155}, {"referenceID": 0, "context": "Although Arjovsky et al. [2016] and Wisdom et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "Although Arjovsky et al. [2016] and Wisdom et al. [2016] use complex valued networks with unitary constraints on the recurrent matrix, the motivations for such models are not clear.", "startOffset": 9, "endOffset": 57}, {"referenceID": 24, "context": ",Wk} are unitary then the full matrix W is unitary Van Loan [2000] and if each of the factors are approximately unitary then the full matrix is approximately unitary.", "startOffset": 55, "endOffset": 67}, {"referenceID": 5, "context": "Cisse et al. [2017] showed that enforcing approximate orthogonality constraint on the weight matrices make the network robust to adversarial samples as well as improve the learning speed.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Cisse et al. [2017] showed that enforcing approximate orthogonality constraint on the weight matrices make the network robust to adversarial samples as well as improve the learning speed. In metric learning Jose and Fleuret [2016] have shown that it better conditions the projection matrix thereby improving the robustness of stochastic gradient over a wide range of step sizes as well as the generalization performance.", "startOffset": 0, "endOffset": 231}, {"referenceID": 0, "context": "Whereas ueRNN, fuRNN and our model, KRU uses complex rectified linear units Arjovsky et al. [2016].", "startOffset": 76, "endOffset": 99}, {"referenceID": 12, "context": "Copy memory problem Hochreiter and Schmidhuber [1997] tests the model\u2019s ability to recall a sequence after a long time gap.", "startOffset": 20, "endOffset": 54}, {"referenceID": 12, "context": "Copy memory problem Hochreiter and Schmidhuber [1997] tests the model\u2019s ability to recall a sequence after a long time gap. In this problem each sequence is of length T + 20 and each element in the sequence come from 10 classes {0, . . . , 9}. The first 10 elements are sampled uniformly with replacement from {1, . . . , 8}. The next T \u2212 1 elements are filled with 0, the \u2018blank\u2019 class followed by 9, the \u2018delimiter\u2019 and the remaining 10 elements are \u2018blank\u2019 category. The goal of the model is to output a sequence of T + 10 blank categories followed by the 10 element sequence from the beginning of the input sequence. The expected average cross entropy for a memoryless strategy is 10 log 8 T+20 . Our experimental setup closely follows Wisdom et al. [2016] which in turn follows Arjovsky et al.", "startOffset": 20, "endOffset": 761}, {"referenceID": 0, "context": "[2016] which in turn follows Arjovsky et al. [2016] but T extended to 1000 and 2000.", "startOffset": 29, "endOffset": 52}, {"referenceID": 0, "context": "[2016] which in turn follows Arjovsky et al. [2016] but T extended to 1000 and 2000. Our model, KRU uses a hidden dimension n of 128 with 2x2 Kronecker factors which corresponds to \u22485K parameters in total. We use a RNN of n = 128 (\u2248 19K parameters) , LSTM of n = 128 ( \u2248 72K parameters), ueRNN of n = 470 ( \u2248 21K parameters) , fuRNN of n = 128 ( \u2248 37K parameters). All the baseline models are deliberately chosen to have more parameters than KRU. Following Wisdom et al. [2016], Arjovsky et al.", "startOffset": 29, "endOffset": 478}, {"referenceID": 0, "context": "[2016] which in turn follows Arjovsky et al. [2016] but T extended to 1000 and 2000. Our model, KRU uses a hidden dimension n of 128 with 2x2 Kronecker factors which corresponds to \u22485K parameters in total. We use a RNN of n = 128 (\u2248 19K parameters) , LSTM of n = 128 ( \u2248 72K parameters), ueRNN of n = 470 ( \u2248 21K parameters) , fuRNN of n = 128 ( \u2248 37K parameters). All the baseline models are deliberately chosen to have more parameters than KRU. Following Wisdom et al. [2016], Arjovsky et al. [2016], we choose the training and test set size to be 100K and 10K respectively.", "startOffset": 29, "endOffset": 502}, {"referenceID": 15, "context": "ESNs are known to be able to learn long-term dependencies if they are properly initialized Jaeger [2001]. We argue that this data-set is not an ideal benchmark for evaluating RNNs in capturing long term dependencies.", "startOffset": 91, "endOffset": 105}, {"referenceID": 0, "context": "Following Arjovsky et al. [2016] we describe the adding problem Hochreiter and Schmidhuber [1997].", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "Following Arjovsky et al. [2016] we describe the adding problem Hochreiter and Schmidhuber [1997]. Each input vector is composed of two sequences of length T .", "startOffset": 10, "endOffset": 98}, {"referenceID": 0, "context": "Following Arjovsky et al. [2016] we describe the adding problem Hochreiter and Schmidhuber [1997]. Each input vector is composed of two sequences of length T . The first sequence is sampled from U [0, 1]. In the second sequence exactly two of the entries is 1, the \u2018marker\u2019 and the remaining is 0. The first 1 is located uniformly at random in the first half of the sequence and the other 1 is located again uniformly at random in the other half of the sequence. The network\u2019s goal is to predict the sum of the numbers from the first sequence corresponding to the marked locations in the second sequence. We evaluate four settings as in Arjovsky et al. [2016] with T=100,T=200,T=400,T=750.", "startOffset": 10, "endOffset": 660}, {"referenceID": 19, "context": "As outlined by Le et al. [2015], we evaluate the Pixel by pixel MNIST task.", "startOffset": 15, "endOffset": 32}, {"referenceID": 0, "context": "The models with strict unitary constraints, ueRNN Arjovsky et al. [2016] and fuRNN Wisdom et al.", "startOffset": 50, "endOffset": 73}, {"referenceID": 0, "context": "The models with strict unitary constraints, ueRNN Arjovsky et al. [2016] and fuRNN Wisdom et al. [2016] have problems in forgetting a truly vanishing long term influence and thus results in poor convergence as T increases.", "startOffset": 50, "endOffset": 104}, {"referenceID": 21, "context": "We now consider character level language modelling on Penn TreeBank data-set Marcus et al. [1993]. Penn TreeBank is composed of 5017K characters in the training set, 393K characters in the validation set and 442 characters in the test set.", "startOffset": 77, "endOffset": 98}, {"referenceID": 21, "context": "We now consider character level language modelling on Penn TreeBank data-set Marcus et al. [1993]. Penn TreeBank is composed of 5017K characters in the training set, 393K characters in the validation set and 442 characters in the test set. The size of the vocabulary was limited to 10K most frequently occurring words and the rest of the words are replaced by a special <UNK> character Mikolov [2012]. The total number of unique characters in the data-set is 50, including the special <UNK> character.", "startOffset": 77, "endOffset": 401}, {"referenceID": 17, "context": "All the models were trained for 20 epochs with a batch size of 50 and using ADAM Kingma and Ba [2014]. We use a learning rate of 1e\u22123 which was found through cross-validation with default beta parameters Kingma and Ba [2014].", "startOffset": 81, "endOffset": 102}, {"referenceID": 17, "context": "All the models were trained for 20 epochs with a batch size of 50 and using ADAM Kingma and Ba [2014]. We use a learning rate of 1e\u22123 which was found through cross-validation with default beta parameters Kingma and Ba [2014]. If we do not see an improvement in the validation bits per character (BPC) after each epoch then the learning rate is decreased by 0.", "startOffset": 81, "endOffset": 225}, {"referenceID": 9, "context": "Framewise phoneme classification Graves and Schmidhuber [2005] is the problem of classifying the phoneme corresponding to a sound frame.", "startOffset": 33, "endOffset": 63}, {"referenceID": 9, "context": "We evaluate the models for this task on the real world TIMIT data-set Garofolo et al. [1993]. TIMIT contains a training set of 4620 utterances among which we use 184 as the validation set.", "startOffset": 70, "endOffset": 93}, {"referenceID": 9, "context": "We evaluate the models for this task on the real world TIMIT data-set Garofolo et al. [1993]. TIMIT contains a training set of 4620 utterances among which we use 184 as the validation set. The test set is composed of 1680 utterances. We extract 12 Mel-Frequency Cepstrum Coefficients (MFCC) Mermelstein [1976] from 26 filter banks and also the log energy per frame.", "startOffset": 70, "endOffset": 310}, {"referenceID": 9, "context": "We evaluate the models for this task on the real world TIMIT data-set Garofolo et al. [1993]. TIMIT contains a training set of 4620 utterances among which we use 184 as the validation set. The test set is composed of 1680 utterances. We extract 12 Mel-Frequency Cepstrum Coefficients (MFCC) Mermelstein [1976] from 26 filter banks and also the log energy per frame. We also concatenate the first derivative, resulting in a feature descriptor of dimension 26 per frame. The frame size is chosen to be 10ms and the window size is 25ms. The number of time steps to which back-propagation through time (BPTT) is unrolled corresponds to the length of each sequence. Since each sequence is of different length this implies that for each sample BPTT steps are different. All the models are trained for 20 epochs with a batch size of 1 using ADAM with default beta parameters Kingma and Ba [2014]. The learning rate was cross-validated for each of the models from \u03b7 \u2208 {1e\u22122, 1e\u22123, 1e\u22124} and the best results are reported here.", "startOffset": 70, "endOffset": 889}, {"referenceID": 9, "context": "Figure 5: KRU performs better than baseline models with 3 order of magnitude less parameters in the recurrent weight matrix on the challenging TIMIT data-set Garofolo et al. [1993]. This significantly bring down the training and inference time of RNNs.", "startOffset": 158, "endOffset": 181}], "year": 2017, "abstractText": "Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrence matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on five standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.", "creator": "LaTeX with hyperref package"}}}