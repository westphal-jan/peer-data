{"id": "1705.10209", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "On Multilingual Training of Neural Dependency Parsers", "abstract": "We show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from the same family. The parser is implemented as a deep neural network whose only input is orthographic representations of words. In order to successfully parse, the network has to discover how linguistically relevant concepts can be inferred from word spellings. We analyze the representations of characters and words that are learned by the network to establish which properties of languages were accounted for. In particular we show that the parser has approximately learned to associate Latin characters with their Cyrillic counterparts and that it can group Polish and Russian words that have a similar grammatical function. Finally, we evaluate the parser on selected languages from the Universal Dependencies dataset and show that it is competitive with other recently proposed state-of-the art methods, while having a simple structure.", "histories": [["v1", "Mon, 29 May 2017 14:24:08 GMT  (317kb,D)", "http://arxiv.org/abs/1705.10209v1", "preprint accepted into the TSD2017"]], "COMMENTS": "preprint accepted into the TSD2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["micha{\\l} zapotoczny", "pawe{\\l} rychlikowski", "jan chorowski"], "accepted": false, "id": "1705.10209"}, "pdf": {"name": "1705.10209.pdf", "metadata": {"source": "CRF", "title": "On Multilingual Training of Neural Dependency Parsers", "authors": ["Micha\u0142 Zapotoczny", "Jan Chorowski"], "emails": ["mzapotoczny@gmail.com", "pawel.rychlikowski@cs.uni.wroc.pl", "jan.chorowski@cs.uni.wroc.pl"], "sections": [{"heading": null, "text": "Keywords: Dependency Parsing, Recurrent Neural Networks, Multitask Training"}, {"heading": "1 Introduction", "text": "Parsing text is an important component of many natural language processing applications. Recent state-of-the-art results have been achieved with parsers implemented using deep neural networks [3]. Neural networks are flexible learners who are able to express complicated input relationships. However, as more powerful machine learning techniques are used, the quality of results is not limited by the capacity of the model, but by the amount of training data available. In this article we explore the possibility of extending the training set by using trunks from similar languages. For example, in the forthcoming collection Universal Dependencies (UD) there are 2.0 trunks [28] 863 commented Ukrainian sentences, 333 Belarusian but almost 60k Russian sentences (divided into two sentences: a standard set of 4.4k sentences and SynTagRus with 55.4k sentences). Similarly, there are 7k Polish sentences and just over 100k Czech languages (1). As these languages belong to the same Slavic language family, the resources should be improved."}, {"heading": "2 Background and Related Work", "text": "Dependency savers represent sentences as trees in which each word is connected to its head by a directed edge (so-called dependency) labeled with the type of dependency. Parsers often contain parts that are learned on a corpus. In the past, learning algorithms were relatively simple, e.g., transition-based parsers used linear SVMs [27,26]. Recently, these simple learning models have been successfully replaced by deep neural networks [33,9,15,3]. This trend coincides with the successes of these models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35]. Neural networks have enough capacity to solve the parsing task directly."}, {"heading": "3 Model", "text": "We find low letters, which we merge into a series of letters, and then turn them into a vector. We can describe four basic parts: reader, tagger, labeler / scorer, and an optional POS tag predictor (Figure 1). The reader is assigned to transform the orthographic representation of a single word w into a vector Ew, also called the word REdim. We represent each word as a sequence of letters and endwords. We find the word REdim, also called the word w \"s embedding.\" We represent each word as a sequence of letters and endwords tokens.We find low letters to a vector EW REdim, also called the word w \"s embedding.\""}, {"heading": "4 Experiment Details and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Model Hyperparameters", "text": "We decided to use the same set of hyperparameters for all languages and multilingual parsers, which represented a compromise in the model capacity for languages that had small and large tree banks. The reported size of the recurring layers is a bit too large for monolingual parsers with low resources, but we found that it is optimal for languages with large tree banks and for multilingual training. The reader embeds each character in a size 15 vector and contains 1050 filters (50 x k filters of length k for k = 1,.., 6), whose outputs are transformed into 512-dimensional vectors transformed by a 3 equally large layers of feedforward neural networks with ReLU activation. Unlike 21.12, we decided to remove highway layers from the reader. Their use resulted in a marginal increase in accuracy, while almost doubling the computational load."}, {"heading": "4.2 Main Results", "text": "Our models achieve better values than the highly tuned transition-based SyntaxNet parser [3] and compete with the DRAGNN-based ParseySaurus, which also uses character-based input [1]. Multilingual training (Table 2) improves the performance of resource-poor languages. We observe that the optimal amount of parameter distribution depends on the similarity between languages and the body size - while it is advantageous to share all parameters of the PL-CZ and RU-CZ parsers, the PL-RU parser works best when the reader subnetworks are disconnected. We attribute this to the quality of the Czech pedestal, which has many more examples than Polish and Russian data sets put together."}, {"heading": "4.3 Analysis of Language Similarities Identified by the Network", "text": "We first analyzed whether a PL-RU parser could learn the correspondence between Latin and Cyrillic fonts2. For the analysis, we extracted the embeddings associated with all Polish and Russian characters, paired Polish and Russian letters that have similar pronunciations, and found that the pairing processed a word by having separate codes for Latin and Cyrillic characters. By matching the famous equation King-Man + Queen [24], we investigated the extent to which our network was able to derive Latin-Cyrillic correspondence for all the different pairs (p1 \u2212 r1, p2 \u2212 r2) of the letter correspondences that we compared with each other."}, {"heading": "4.4 Common Error Analysis", "text": "We examined two possible sources of error caused by the parser. First, we checked whether using a more advanced tree-building algorithm was better than using a greedy algorithm. We observed that the scorer generates very sharp probability distributions that can be converted into trees using a greedy algorithm that simply selects the highest-rated head for each word [12,13]. Counterintuitively, the Chu Liu Edmonds (CLE) algorithm that spans the tree [16] often leads to slightly worse decoding results. We found that the network is so sure in its predictions that non-top scorers do not reflect alternatives, but are just noise. So, when greedy decoding generates a cycle, the CLE usually breaks it in the wrong place, resulting in another pointer error. We used the POS predictor to determine which parts of the network are responsible for the error (tag / laboratory shooter) that the reader was almost always wrong to deliver."}, {"heading": "5 Conclusions and Future Works", "text": "We have shown that the proposed parser can easily be used in a multilingual setup where parsers for many languages that share parameters are trained together. We have found that the degree of sharing depends on language similarity and body size: the best PL-CZ parser and RU-CZ have all parameters in common (essentially creating a single parser for both languages), while the best PL-RU parser has separate morphological feature detectors (i.e. readers). We have also found that the network can extract meaningful relationships between languages, such as learning to match Latin to Cyrillic characters or associating Polish and Russian words that have a similar grammatical function. While this post focuses on improving performance on a resource-poor language, similar parameter-sharing techniques could be used to produce a better interpretation of words."}], "references": [{"title": "SyntaxNet Models for the CoNLL 2017 Shared Task", "author": ["C Alberti"], "venue": "arXiv:1703.04929", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Many Languages, One Parser", "author": ["W Ammar"], "venue": "Transactions of the Association for Computational Linguistics 4(0), 431\u2013444", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Globally Normalized Transition-Based Neural Networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "arXiv:1603.06042 [cs]", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv:1409.0473 [cs, stat]", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["M. Ballesteros", "C. Dyer", "N.A. Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "On achieving and evaluating language-independence in nlp", "author": ["E.M. Bender"], "venue": "Linguistic Issues in Language Technology 6(3), 1\u201326", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J Bergstra"], "venue": "Proc. SciPy", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning 28(1), 41\u201375", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["D. Chen", "C.D. Manning"], "venue": "EMNLP. pp. 740\u2013750", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K Cho"], "venue": "CoRR abs/1406.1078", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv:1412.1602 [cs, stat]", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Read, tag, and parse all at once, or fullyneural dependency parsing", "author": ["J. Chorowski", "M. Zapotoczny", "P. Rychlikowski"], "venue": "CoRR abs/1609.03441", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["T. Dozat", "C.D. Manning"], "venue": "CoRR abs/1611.01734", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "A neural network model for low-resource universal dependency parsing", "author": ["L. Duong", "T. Cohn", "S. Bird", "P. Cook"], "venue": "EMNLP. pp. 339\u2013348. Citeseer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["C. Dyer", "M. Ballesteros", "W. Ling", "A. Matthews", "N.A. Smith"], "venue": "arXiv preprint arXiv:1505.08075", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimim Branchings", "author": ["J. Edmonds"], "venue": "JOURNAL OF RESEARCH of the National Bureau of Standards - B. 71B(4), 233\u2013240", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1966}, {"title": "Maxout Networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML. pp. 1319\u20131327", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["J. Guo", "W. Che", "D. Yarowsky", "H. Wang", "T. Liu"], "venue": "ACL (1). pp. 1234\u20131244", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Paralell Distributed Processing: Explorations in the microstructure of cognition", "author": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart"], "venue": "Volume 1: Foundations. MIT Press/Bradford Books", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "Exploring the Limits of Language Modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv:1602.02410 [cs]", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "author": ["E. Kiperwasser", "Y. Goldberg"], "venue": "arXiv:1603.04351 [cs]", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["B van Merri\u00ebnboer"], "venue": "arXiv:1506.00619 [cs, stat]", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS. pp. 3111\u20133119", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "Makuhari, Chiba, Japan", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithms for Deterministic Incremental Dependency Parsing", "author": ["J. Nivre"], "venue": "Comput. Linguist. 34(4), 513\u2013553", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "MaltParser: A language-independent system for data-driven dependency parsing", "author": ["J Nivre"], "venue": "Natural Language Engineering p. 1", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal Dependencies 1.2. http://universaldependencies.github.io/docs/ (Nov 2015", "author": ["J Nivre"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing 45(11), 2673\u20132681", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR 15, 1929\u20131958", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway Networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv:1505.00387 [cs]", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "arXiv preprint arXiv:1409.3215", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "A latent variable model for generative dependency parsing", "author": ["I. Titov", "J. Henderson"], "venue": "In Proceedings of IWPT", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Grammar as a Foreign Language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "arXiv:1412.7449 [cs, stat]", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["Y Wu"], "venue": "arXiv: 1609.08144", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv:1212.5701", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Dependency parsing as head selection", "author": ["X. Zhang", "J. Cheng", "M. Lapata"], "venue": "CoRR abs/1606.01280", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Recent state-of-the-art results were obtained with parsers implemented using deep neural networks [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 27, "context": "0 treebank collection [28] there are 863 annotated Ukrainian sentences, 333 Belarusian, but nearly 60k Russian ones (divided into two sets: a default one of 4.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "Since these languages belong to the same Slavic language family, performance on the low resource languages should improve by joint training the model also on a better annotated language [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 26, "context": "transitionbased parsers used linear SVMs [27,26].", "startOffset": 41, "endOffset": 48}, {"referenceID": 25, "context": "transitionbased parsers used linear SVMs [27,26].", "startOffset": 41, "endOffset": 48}, {"referenceID": 32, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 8, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 14, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 2, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 24, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 98, "endOffset": 105}, {"referenceID": 19, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 98, "endOffset": 105}, {"referenceID": 3, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 122, "endOffset": 131}, {"referenceID": 31, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 122, "endOffset": 131}, {"referenceID": 34, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 122, "endOffset": 131}, {"referenceID": 33, "context": "For example a constituency parser can be implemented using a sequence-to-sequence network originally developed for translation [34].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 36, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 12, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 11, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 23, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 217, "endOffset": 226}, {"referenceID": 4, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 217, "endOffset": 226}, {"referenceID": 11, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 217, "endOffset": 226}, {"referenceID": 18, "context": "Another particularly nice property of neural models is that all internal computations use distributed representations of input data that are embedded in highly dimensional vector spaces [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 7, "context": "These internal representation can be easily shared between tasks [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 17, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 13, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 1, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 27, "context": "Creation of multilingual parsers is further facilitated by the introduction of standardized treebanks, such as the Universal Dependencies [28].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Each of the n parsers is a single neural network that directly reads a sequence of characters and finds dependency edges along with their labels [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "We use a multi-layer bidirectional GRU Recurrent Neural Network (BiRNN) [10,29].", "startOffset": 72, "endOffset": 79}, {"referenceID": 28, "context": "We use a multi-layer bidirectional GRU Recurrent Neural Network (BiRNN) [10,29].", "startOffset": 72, "endOffset": 79}, {"referenceID": 20, "context": "Unlike [21,12] we decided to remove Highway layers [31] from the reader.", "startOffset": 7, "endOffset": 14}, {"referenceID": 11, "context": "Unlike [21,12] we decided to remove Highway layers [31] from the reader.", "startOffset": 7, "endOffset": 14}, {"referenceID": 30, "context": "Unlike [21,12] we decided to remove Highway layers [31] from the reader.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "The tagger contains 2 BiRNN layers of GRU units with 548 hidden states for both forward and backward passes which are later aggregated using addition [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 16, "context": "The scorer uses a single layer of 384 tanh for head word scoring while the labeller uses 256 Maxout units (each using 2 pieces) to classify the relation label [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "We regularize the models using Dropout [30] applied to the reader output (20%), between the BiRNN layers of the tagger (70%) and to the labeller (50%).", "startOffset": 39, "endOffset": 43}, {"referenceID": 35, "context": "We have trained all models using the Adadelta [36] learning rule with epsilon annealed from 1e-8 to 1e-12 and adaptive gradient clipping [11].", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "We have trained all models using the Adadelta [36] learning rule with epsilon annealed from 1e-8 to 1e-12 and adaptive gradient clipping [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": "Our models reach better scores than the highly tuned SyntaxNet transition-based parser [3] and are competitive with the DRAGNN based ParseySaurus which also uses character-based input [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "Our models reach better scores than the highly tuned SyntaxNet transition-based parser [3] and are competitive with the DRAGNN based ParseySaurus which also uses character-based input [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 1, "context": "[2] uses version 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "SyntaxNet[3,1] works on predicted POS tags, while ParseySaurus[1] uses word spellings.", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "SyntaxNet[3,1] works on predicted POS tags, while ParseySaurus[1] uses word spellings.", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "SyntaxNet[3,1] works on predicted POS tags, while ParseySaurus[1] uses word spellings.", "startOffset": 62, "endOffset": 65}, {"referenceID": 23, "context": "Adapting the famous equation king \u2212 man + woman \u2248 queen [24] we inspected to what extent our network was able to deduce Latin-Cyrillic correspondences.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "We have observed that the scorer produces very sharp probability distributions that can be transformed into trees using a greedy algorithm that simply selects for each word the highest scored head [12,13].", "startOffset": 197, "endOffset": 204}, {"referenceID": 12, "context": "We have observed that the scorer produces very sharp probability distributions that can be transformed into trees using a greedy algorithm that simply selects for each word the highest scored head [12,13].", "startOffset": 197, "endOffset": 204}, {"referenceID": 15, "context": "Counterintuitively, the Chu-Liu-Edmonds (CLE) maximum spanning tree algorithm [16] often makes the decoding results slightly worse.", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "While this contribution focused on improving the performance on a low-resource language using data from another languages, similar parameter sharing techniques could be used to create one universal parser [2].", "startOffset": 205, "endOffset": 208}, {"referenceID": 6, "context": "The experiments used Theano [7], Blocks and Fuel [23] libraries.", "startOffset": 28, "endOffset": 31}, {"referenceID": 22, "context": "The experiments used Theano [7], Blocks and Fuel [23] libraries.", "startOffset": 49, "endOffset": 53}], "year": 2017, "abstractText": "We show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from the same family. The parser is implemented as a deep neural network whose only input is orthographic representations of words. In order to successfully parse, the network has to discover how linguistically relevant concepts can be inferred from word spellings. We analyze the representations of characters and words that are learned by the network to establish which properties of languages were accounted for. In particular we show that the parser has approximately learned to associate Latin characters with their Cyrillic counterparts and that it can group Polish and Russian words that have a similar grammatical function. Finally, we evaluate the parser on selected languages from the Universal Dependencies dataset and show that it is competitive with other recently proposed state-of-the art methods, while having a simple structure.", "creator": "LaTeX with hyperref package"}}}