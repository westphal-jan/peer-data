{"id": "1705.10272", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Who's to say what's funny? A computer using Language Models and Deep Learning, That's Who!", "abstract": "Humor is a defining characteristic of human beings. Our goal is to develop methods that automatically detect humorous statements and rank them on a continuous scale. In this paper we report on results using a Language Model approach, and outline our plans for using methods from Deep Learning.", "histories": [["v1", "Mon, 29 May 2017 16:20:21 GMT  (12kb)", "http://arxiv.org/abs/1705.10272v1", "3 pages, Appears in the Proceedings of the Women and Underrepresented Minorities in Natural Language Processing Workshop (WiNLP-2017), July 30, 2017, Vancouver, BC"]], "COMMENTS": "3 pages, Appears in the Proceedings of the Women and Underrepresented Minorities in Natural Language Processing Workshop (WiNLP-2017), July 30, 2017, Vancouver, BC", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinru yan", "ted pedersen"], "accepted": false, "id": "1705.10272"}, "pdf": {"name": "1705.10272.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yanxx418@d.umn.edu", "tpederse@d.umn.edu", "@midnight", "@midnight"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.10 272v 1 [cs.C L] 29 May 201 7Humor is a characteristic feature of human beings. Our goal is to develop methods that automatically recognize humorous statements and rank them on a continuous scale. In this essay, we report on results using a language model approach and outline our plans for using methods from deep learning."}, {"heading": "1 Introduction", "text": "Humour generation is the problem of automatically generating humorous statements (e.g. (Stock and Strapparava, 2003), (O'zbal and Strapparava, 2012). Humor detection attempts to identify humor in the text, and is sometimes presented as a binary classification problem that determines whether an input is humorous or not (e.g. (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015). However, our focus is on the continuous and subjective aspects of humor. We learn a particular sense of humor from a set of tweets aimed at a particular style of humor (Potash et al., 2016). This data consists of humorous tweets submitted in response to hashtag prompts, each of which was provided in the same way during the Chris Moep broadcast."}, {"heading": "2 Language Models", "text": "We used traditional Ngram language models as our first approach for two reasons: First, Ngram language models can learn a particular style of humor by using examples of them as training data for the model; second, they assign a probability to any input they receive, making it possible to evaluate tweets relative to each other. Thus, Ngram language models intend to rank humorous statements relative to a particular style of humor, taking into account the continuous and subjective nature of humor. We started this research by participating in SemEval-2017 Task 6 # HashtagWars: Learning a Sense of Humor (Potash et al., 2017), which included two subtasks: Pairwise Comparison (Subtask A) and Semi-Ranking (Subtask B). Pairwise Comparison requires a system to choose the funnier of two tweets. Semi-ranking requires that each of the tweets associated with a particular hashtag."}, {"heading": "3 Deep Learning", "text": "One limitation of our language modeling approach is the large number of words from the vocabulary we come across. This problem cannot be solved by increasing the amount of training data, because humor is based on creative language usage. Jokes often contain puns based on invented words, e.g. a singing cat makes beautiful meowsic. (Potash et al., 2016) suggests that character-based Convolutionary Neural Networks (CNNs) are an effective solution to these situations as they are not dependent on observing tokens in training data. Previous work has also shown that the CNNs are effective tools for speech modeling, even in the presence of complex morphology (Kim et al., 2015). Other recent work has shown that recurring neural networks (RNNNNs), especially Long Short-Term Memory Networks (LSTMs), are effective tools for speech modeling (e.g. Sundermeyer et al, 2012)."}, {"heading": "4 Future Work", "text": "Our current language modeling approach is effective, but does not take into account words from the vocabulary or dependencies over long distances. CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g. (Bertero and Fung, 2016)), which we will study and compare with our existing results. After evaluating CNNs and LSTMs, we will explore how to incorporate domain knowledge into these models. One possibility is to create word embedding from domain-specific materials and make these available to the CNNs along with more general text. Another is to investigate the use of tree-structured LSTMs (Tai et al., 2015), which have the potential advantage of preserving non-linear structures in the text, which can be helpful in recognizing some of the unusual variations of words and concepts that are characteristic of humor."}], "references": [{"title": "A long shortterm memory framework for predicting humor in dialogues", "author": ["Dario Bertero", "Pascale Fung."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Bertero and Fung.,? 2016", "shortCiteRegEx": "Bertero and Fung.", "year": 2016}, {"title": "Scalable modified Kneser-Ney languagemodel estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria,", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615 .", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Learning to laugh (automatically): Computational models for humor recognition", "author": ["Rada Mihalcea", "Carlo Strapparava."], "venue": "Computational Intelligence 22(2):126\u2013142.", "citeRegEx": "Mihalcea and Strapparava.,? 2006", "shortCiteRegEx": "Mihalcea and Strapparava.", "year": 2006}, {"title": "Automatic disambiguation of english puns", "author": ["Tristan Miller", "Iryna Gurevych."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "Miller and Gurevych.,? 2015", "shortCiteRegEx": "Miller and Gurevych.", "year": 2015}, {"title": "A computational approach to the automation of creative naming", "author": ["G\u00f6zde \u00d6zbal", "Carlo Strapparava."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Com-", "citeRegEx": "\u00d6zbal and Strapparava.,? 2012", "shortCiteRegEx": "\u00d6zbal and Strapparava.", "year": 2012}, {"title": "HashtagWars: Learning a sense of humor", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "arXiv preprint arXiv:1612.03216 .", "citeRegEx": "Potash et al\\.,? 2016", "shortCiteRegEx": "Potash et al\\.", "year": 2016}, {"title": "SemEval-2017 Task 6: #HashtagWars: learning a sense of humor", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Vancouver, BC.", "citeRegEx": "Potash et al\\.,? 2017", "shortCiteRegEx": "Potash et al\\.", "year": 2017}, {"title": "Inside jokes: Identifying humorous cartoon captions", "author": ["Dafna Shahaf", "Eric Horvitz", "Robert Mankoff."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, NewYork, NY, USA, KDD", "citeRegEx": "Shahaf et al\\.,? 2015", "shortCiteRegEx": "Shahaf et al\\.", "year": 2015}, {"title": "Getting serious about the development of computational humor", "author": ["Oliviero Stock", "Carlo Strapparava."], "venue": "Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence. Acapulco, pages 59\u201364.", "citeRegEx": "Stock and Strapparava.,? 2003", "shortCiteRegEx": "Stock and Strapparava.", "year": 2003}, {"title": "From feedforward to recurrent lstm neural", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schl\u00fcter"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Interspeech. pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Recognizing humor on twitter", "author": ["Renxian Zhang", "Naishi Liu."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and KnowledgeManagement. ACM, New York, NY, USA, CIKM \u201914, pages 889\u2013898.", "citeRegEx": "Zhang and Liu.,? 2014", "shortCiteRegEx": "Zhang and Liu.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": ", (Stock and Strapparava, 2003), (\u00d6zbal and Strapparava, 2012)).", "startOffset": 2, "endOffset": 31}, {"referenceID": 5, "context": ", (Stock and Strapparava, 2003), (\u00d6zbal and Strapparava, 2012)).", "startOffset": 33, "endOffset": 62}, {"referenceID": 3, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.", "startOffset": 2, "endOffset": 34}, {"referenceID": 13, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 8, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)).", "startOffset": 59, "endOffset": 80}, {"referenceID": 4, "context": ", 2015), (Miller and Gurevych, 2015)).", "startOffset": 9, "endOffset": 36}, {"referenceID": 6, "context": "We learn a particular sense of humor from a data set of tweets which are geared towards a certain style of humor (Potash et al., 2016).", "startOffset": 113, "endOffset": 134}, {"referenceID": 7, "context": "We began this research by participating in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor (Potash et al., 2017).", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": "We used KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.", "startOffset": 14, "endOffset": 37}, {"referenceID": 7, "context": "The accuracy and distance measures are defined by the task organizers (Potash et al., 2017).", "startOffset": 70, "endOffset": 91}, {"referenceID": 6, "context": "(Potash et al., 2016) suggests that character\u2013based Convolutional Neural Networks (CNNs) are an effective solution for these situations since they are not dependent on observing tokens in training data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Previous work has also shown the CNNs are effective tools for language modeling, even in the presence of complex morphology (Kim et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 11, "context": ", (Sundermeyer et al., 2012),(Sundermeyer et al.", "startOffset": 2, "endOffset": 28}, {"referenceID": 10, "context": ", 2012),(Sundermeyer et al., 2015)).", "startOffset": 8, "endOffset": 34}, {"referenceID": 6, "context": "(Potash et al., 2016) finds that external knowledge is necessary to detect humor in tweet based data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": ", (Bertero and Fung, 2016)) which we will explore and compare to our existing results.", "startOffset": 2, "endOffset": 26}, {"referenceID": 12, "context": "Another is to investigate the use of Tree\u2013 Structured LSTMs (Tai et al., 2015).", "startOffset": 60, "endOffset": 78}], "year": 2017, "abstractText": "Humor is a defining characteristic of human beings. Our goal is to develop methods that automatically detect humorous statements and rank them on a continuous scale. In this paper we report on results using a Language Model approach, and outline our plans for using methods from Deep Learning.", "creator": "LaTeX with hyperref package"}}}