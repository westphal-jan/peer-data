{"id": "1705.10279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Towards Visual Ego-motion Learning in Robots", "abstract": "Many model-based Visual Odometry (VO) algorithms have been proposed in the past decade, often restricted to the type of camera optics, or the underlying motion manifold observed. We envision robots to be able to learn and perform these tasks, in a minimally supervised setting, as they gain more experience. To this end, we propose a fully trainable solution to visual ego-motion estimation for varied camera optics. We propose a visual ego-motion learning architecture that maps observed optical flow vectors to an ego-motion density estimate via a Mixture Density Network (MDN). By modeling the architecture as a Conditional Variational Autoencoder (C-VAE), our model is able to provide introspective reasoning and prediction for ego-motion induced scene-flow. Additionally, our proposed model is especially amenable to bootstrapped ego-motion learning in robots where the supervision in ego-motion estimation for a particular camera sensor can be obtained from standard navigation-based sensor fusion strategies (GPS/INS and wheel-odometry fusion). Through experiments, we show the utility of our proposed approach in enabling the concept of self-supervised learning for visual ego-motion estimation in autonomous robots.", "histories": [["v1", "Mon, 29 May 2017 16:25:50 GMT  (3260kb,D)", "http://arxiv.org/abs/1705.10279v1", "Conference paper; Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017, Vancouver CA; 8 pages, 8 figures, 2 tables"]], "COMMENTS": "Conference paper; Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017, Vancouver CA; 8 pages, 8 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV", "authors": ["sudeep pillai", "john j leonard"], "accepted": false, "id": "1705.10279"}, "pdf": {"name": "1705.10279.pdf", "metadata": {"source": "CRF", "title": "Towards Visual Ego-motion Learning in Robots", "authors": ["Sudeep Pillai", "John J. Leonard"], "emails": ["spillai@csail.mit.edu", "jleonard@mit.edu"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. RELATED WORK", "text": "In fact, it is the case that most people who are able to realize themselves are also able to deny themselves. In fact, it is the case that they are able to deny themselves, that they are able to deny themselves. In fact, it is the case that people who are able to deny themselves, that they are able to deny themselves, that they are able to deny themselves, that they are in denial of themselves, that they are in denial of themselves, that they are in denial of themselves, that they are in denial of themselves, that they are in denial of themselves, that they are in denial of themselves, that they are in denial of themselves, that they are in denial of themselves, that they are in denial of themselves. \""}, {"heading": "III. EGO-MOTION REGRESSION", "text": "It is a question of the extent to which it is actually a question of the way in which people who are able to understand the world and what they do are able to understand the world. (...) It is a question of the extent to which people are able to understand the world. (...) It is a question of the extent to which people are able to understand the world. (...) It is a question of the extent to which people in the world understand themselves and people in the world understand the world. (...) It is a question of the extent to which they are able to understand the world. (...) It is a question of the extent to which people in the world are able to understand the world. (...) It is a question of the extent to which people in the world are able to understand the world. (...) It is a question of the extent to which they think in the world and in the world, what they think and what they think. (...) It is a question to what they think, what they think and what they think."}, {"heading": "IV. EXPERIMENTS", "text": "This year it is more than ever before."}, {"heading": "V. DISCUSSION", "text": "The initial results of bootstrapped learning for visual first-person motion have inspired new directions toward lifelong learning in autonomous robots. While our visual first-person motion model architecture is proving powerful enough to recover first-person motion for nonlinear camera optics such as fisheye and catadioptric lenses, we are exploring other improvements that are consistent with existing state-of-the-art models for these lens types. Our current model does not yet capture distortion effects, but that is much a3http: / / collab.cc.gatech.edu / borg / gtsam 4See http: / / people.csail.mit.edu / spillai / learning-egomotion and https: / github.com / spillai / learning-egomotionfuture direction that we would like to take. Another consideration is resource-limited setting, where the optimization goal will make us aware of additional parameters for the number of spilograms / negatives we want to use."}, {"heading": "VI. CONCLUSION", "text": "While many visual first-person motion algorithm variants have been proposed over the past decade, we envision that a fully traceable algorithm for generic camera ego motion estimates will have far-reaching implications in several areas, particularly in autonomous systems. In addition, we expect our method to work seamlessly in the near future under resource-constrained circumstances, using existing solutions for model reduction and dynamic adaptation of the model architecture. Given the availability of multiple sensors on these autonomous systems, we also anticipate our approach to bootstrapped task, which could allow robots to learn from experience and leverage the new models learned from these experiences to encode redundancy and error tolerance all within the same framework."}], "references": [{"title": "Visual odometry", "author": ["David Nist\u00e9r", "Oleg Naroditsky", "James Bergen"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Large-scale visual odometry for rough terrain", "author": ["Kurt Konolige", "Motilal Agrawal", "Joan Sola"], "venue": "In Robotics research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Real-time stereo visual odometry for autonomous ground vehicles", "author": ["Andrew Howard"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Visual odometry based on stereo image sequences with RANSAC-based outlier rejection scheme", "author": ["Bernd Kitt", "Andreas Geiger", "Henning Lategahn"], "venue": "In Intelligent Vehicles Symposium,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Motion estimation for self-driving cars with a generalized camera", "author": ["Gim Hee Lee", "Friedrich Faundorfer", "Marc Pollefeys"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Using multicamera systems in robotics: Efficient solutions to the n-PnP problem", "author": ["Laurent Kneip", "Paul Furgale", "Roland Siegwart"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "1-point-RANSAC structure from motion for vehicle-mounted cameras by exploiting non-holonomic constraints", "author": ["Davide Scaramuzza"], "venue": "Int\u2019l J. of Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Bundle adjustment A modern synthesis", "author": ["Bill Triggs", "Philip F McLauchlan", "Richard I Hartley", "Andrew W Fitzgibbon"], "venue": "In International workshop on vision algorithms,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Multiple view geometry in computer vision", "author": ["Richard Hartley", "Andrew Zisserman"], "venue": "Cambridge university press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Obstacle avoidance and navigation in the real world by a seeing robot rover", "author": ["Hans P Moravec"], "venue": "Technical report, DTIC Document,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1980}, {"title": "Dynamic stereo vision", "author": ["Larry Henry Matthies"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Robust stereo ego-motion for long distance navigation", "author": ["Clark F Olson", "Larry H Matthies", "H Schoppers", "Mark W Maimone"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["Martin A Fischler", "Robert C Bolles"], "venue": "Communications of the ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1981}, {"title": "Omnidirectional visual odometry for a planetary rover", "author": ["Peter Corke", "Dennis Strelow", "Sanjiv Singh"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Visual navigation using planar homographies", "author": ["Bojian Liang", "Nick Pears"], "venue": "In Robotics and Automation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Transforming camera geometry to a virtual downward-looking camera: Robust ego-motion estimation and groundlayer detection", "author": ["Qifa Ke", "Takeo Kanade"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Real-time monocular visual odometry for on-road vehicles with 1point RANSAC", "author": ["Davide Scaramuzza", "Friedrich Fraundorfer", "Roland Siegwart"], "venue": "In Robotics and Automation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Learning general optical flow subspaces for egomotion estimation and detection of motion anomalies", "author": ["Richard Roberts", "Christian Potthast", "Frank Dellaert"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Evaluation of non-geometric methods for visual odometry", "author": ["Thomas A Ciarfuglia", "Gabriele Costante", "Paolo Valigi", "Elisa Ricci"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Exploring Representation Learning With CNNs for Frameto-Frame Ego-Motion Estimation", "author": ["Gabriele Costante", "Michele Mancini", "Paolo Valigi", "Thomas A Ciarfuglia"], "venue": "IEEE Robotics and Automation Letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "author": ["Andreas Geiger", "Philip Lenz", "Raquel Urtasun"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "VINet: Visual-Inertial odometry as a sequence-to-sequence learning problem", "author": ["Ronald Clark", "Sen Wang", "Hongkai Wen", "Andrew Markham", "Niki Trigoni"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Visual odometry [tutorial", "author": ["Davide Scaramuzza", "Friedrich Fraundorfer"], "venue": "IEEE robotics & automation magazine,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Learning visual odometry with a convolutional network", "author": ["Kishore Konda", "Roland Memisevic"], "venue": "In International Conference on Computer Vision Theory and Applications,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "KLT: An implementation of the Kanade-Lucas-Tomasi feature tracker", "author": ["Stan Birchfield"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Benefit of large field-of-view cameras for visual odometry", "author": ["Zichao Zhang", "Henri Rebecq", "Christian Forster", "Davide Scaramuzza"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA). IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Omnidirectional 3d reconstruction in augmented manhattan worlds", "author": ["Miriam Sch\u00f6nbein", "Andreas Geiger"], "venue": "In Intelligent Robots and Systems (IROS 2014),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "iSAM2: Incremental smoothing and mapping using the bayes tree", "author": ["Michael Kaess", "Hordur Johannsson", "Richard Roberts", "Viorela Ila", "John J Leonard", "Frank Dellaert"], "venue": "Int\u2019l J. of Robotics Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Visual odometry (VO) [1], commonly referred to as egomotion estimation, is a fundamental capability that enables robots to reliably navigate its immediate environment.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 1, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 189, "endOffset": 192}, {"referenceID": 2, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 204, "endOffset": 207}, {"referenceID": 3, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 250, "endOffset": 253}, {"referenceID": 5, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 255, "endOffset": 258}, {"referenceID": 6, "context": "fisheye, catadioptric) and the range of motions observed by these cameras mounted on various platforms [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Recovering relative camera poses from a set of images is a well studied problem under the context of Structurefrom-Motion (SfM) [8], [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "Recovering relative camera poses from a set of images is a well studied problem under the context of Structurefrom-Motion (SfM) [8], [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "SfM is usually treated as a non-linear optimization problem, where the camera poses (extrinsics), camera model parameters (intrinsics), and the 3D scene structure are jointly optimized via non-linear leastsquares [8].", "startOffset": 213, "endOffset": 216}, {"referenceID": 9, "context": "Most of the early work in VO was done primarily to determine vehicle egomotion [10], [11], [12] in 6-DOF, especially in the Mars planetary rover.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Most of the early work in VO was done primarily to determine vehicle egomotion [10], [11], [12] in 6-DOF, especially in the Mars planetary rover.", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "Most of the early work in VO was done primarily to determine vehicle egomotion [10], [11], [12] in 6-DOF, especially in the Mars planetary rover.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "[1], where the authors proposed the first real-time and scalable VO algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "In their work, they developed a 5-point minimal solver coupled with a RANSAC-based outlier rejection scheme [13] that is still extensively used today.", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "Other researchers [14] have extended this work to various camera types including catadioptric and fisheye lenses.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "One particularly popular strategy for VO estimation in vehicles is to enforce planar homographies during matching features on the ground plane [15], [16], thereby being able to robustly recover both relative orientation and absolute scale.", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "One particularly popular strategy for VO estimation in vehicles is to enforce planar homographies during matching features on the ground plane [15], [16], thereby being able to robustly recover both relative orientation and absolute scale.", "startOffset": 149, "endOffset": 153}, {"referenceID": 6, "context": "[7], [17] introduced a novel 1-point solver by imposing the vehicle\u2019s non-holonomic motion constraints, thereby speeding up the VO estimation up to 400Hz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[7], [17] introduced a novel 1-point solver by imposing the vehicle\u2019s non-holonomic motion constraints, thereby speeding up the VO estimation up to 400Hz.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "Typical approaches have leveraged dimensionality reduction techniques by learning a reduced-dimensional subspace of the optical flow vectors induced by the egomotion [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 18, "context": "In [19], Ciarfuglia et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The authors further build upon their previous result by swapping out the SVR module with an end-to-end trainable convolutional neural network [20] while showing improvements in the overall performance on the KITTI odometry benchmark [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 20, "context": "The authors further build upon their previous result by swapping out the SVR module with an end-to-end trainable convolutional neural network [20] while showing improvements in the overall performance on the KITTI odometry benchmark [21].", "startOffset": 233, "endOffset": 237}, {"referenceID": 21, "context": "[22] introduced a visual-inertial odometry solution that takes advantage of a neural-network architecture to learn a mapping from raw inertial measurements and sequential imagery to 6-DOF pose estimates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Traditional VO [23] 7 7 3 7", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "End-to-end VO [20], [22] 7 3 3 7", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "End-to-end VO [20], [22] 7 3 3 7", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 115, "endOffset": 118}, {"referenceID": 14, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 15, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "While model-based approaches have shown tremendous progress in accuracy, robustness, and run-time performance, a few recent data-driven approaches have been shown to produce equally compelling results [20], [22], [24].", "startOffset": 201, "endOffset": 205}, {"referenceID": 21, "context": "While model-based approaches have shown tremendous progress in accuracy, robustness, and run-time performance, a few recent data-driven approaches have been shown to produce equally compelling results [20], [22], [24].", "startOffset": 207, "endOffset": 211}, {"referenceID": 23, "context": "While model-based approaches have shown tremendous progress in accuracy, robustness, and run-time performance, a few recent data-driven approaches have been shown to produce equally compelling results [20], [22], [24].", "startOffset": 213, "endOffset": 217}, {"referenceID": 6, "context": "Our approach is motivated by previous minimally parameterized models [7], [17] that are able to recover ego-motion from a single tracked feature.", "startOffset": 69, "endOffset": 72}, {"referenceID": 16, "context": "Our approach is motivated by previous minimally parameterized models [7], [17] that are able to recover ego-motion from a single tracked feature.", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "However, it has been previously shown that under non-holonomic vehicle motion, camera ego-motion may be fully recoverable up to a sufficient degree of accuracy using a single point [7], [17].", "startOffset": 181, "endOffset": 184}, {"referenceID": 16, "context": "However, it has been previously shown that under non-holonomic vehicle motion, camera ego-motion may be fully recoverable up to a sufficient degree of accuracy using a single point [7], [17].", "startOffset": 186, "endOffset": 190}, {"referenceID": 24, "context": "In typical associative mapping problems, the joint probability density p(x, z) is decomposed into the product of two terms: (i) p(z|x): the conditional density of the target pose z \u2208 SE(3) conditioned on the input feature correspondence x = (x,\u2206x) obtained from sparse optical flow (KLT) [25] (ii) p(x): the unconditional density of the input data x.", "startOffset": 288, "endOffset": 292}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this two-stage approach over a simulated dataset [27].", "startOffset": 131, "endOffset": 135}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For illustrative purposes only, we r fer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "We evaluate the performance of our proposed approach on various publicly-available datasets including the KITTI dataset [21], the Multi-FOV synthetic dataset [27] (pinhole, fisheye, and catadioptric lenses), an omnidirectionalcamera dataset [28], and on the Oxford Robotcar 1000km Dataset [29].", "startOffset": 120, "endOffset": 124}, {"referenceID": 25, "context": "We evaluate the performance of our proposed approach on various publicly-available datasets including the KITTI dataset [21], the Multi-FOV synthetic dataset [27] (pinhole, fisheye, and catadioptric lenses), an omnidirectionalcamera dataset [28], and on the Oxford Robotcar 1000km Dataset [29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 26, "context": "We evaluate the performance of our proposed approach on various publicly-available datasets including the KITTI dataset [21], the Multi-FOV synthetic dataset [27] (pinhole, fisheye, and catadioptric lenses), an omnidirectionalcamera dataset [28], and on the Oxford Robotcar 1000km Dataset [29].", "startOffset": 241, "endOffset": 245}, {"referenceID": 3, "context": "In this section, we evaluate our approach against a few state-of-the-art algorithms for monocular visual odometry [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 20, "context": "On the KITTI dataset [21], the pre-trained estimator is used to robustly and accurately predict ego-motion from KLT features tracked over the dataset image sequence.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "We test on a variety of publicly-available datasets including (a) Multi-FOV synthetic dataset [27] (pinhole shown above), (b) an omnidirectional-camera dataset [28], (c) Oxford Robotcar 1000km Dataset [29] (2015-11-13-10-28-08) (d-h) KITTI dataset [21].", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "We test on a variety of publicly-available datasets including (a) Multi-FOV synthetic dataset [27] (pinhole shown above), (b) an omnidirectional-camera dataset [28], (c) Oxford Robotcar 1000km Dataset [29] (2015-11-13-10-28-08) (d-h) KITTI dataset [21].", "startOffset": 160, "endOffset": 164}, {"referenceID": 20, "context": "We test on a variety of publicly-available datasets including (a) Multi-FOV synthetic dataset [27] (pinhole shown above), (b) an omnidirectional-camera dataset [28], (c) Oxford Robotcar 1000km Dataset [29] (2015-11-13-10-28-08) (d-h) KITTI dataset [21].", "startOffset": 248, "endOffset": 252}, {"referenceID": 25, "context": "30 m Multi-FOV [27] Pinhole 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "18 m Multi-FOV [27] Fisheye 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "48 m Multi-FOV [27] Catadioptric 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "36 m Omnidirectional [28] Catadioptric 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "7: Varied camera optics: An illustration of the performance of our general-purpose approach for varied camera optics (pinhole, fisheye, and catadioptric lenses) on the Multi-FOV synthetic dataset [27].", "startOffset": 196, "endOffset": 200}, {"referenceID": 27, "context": "The constraints are incrementally added and solved using iSAM2 [30] as the measurements are streamed in, with updates performed every 10 frames.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "Many model-based Visual Odometry (VO) algorithms have been proposed in the past decade, often restricted to the type of camera optics, or the underlying motion manifold observed. We envision robots to be able to learn and perform these tasks, in a minimally supervised setting, as they gain more experience. To this end, we propose a fully trainable solution to visual ego-motion estimation for varied camera optics. We propose a visual ego-motion learning architecture that maps observed optical flow vectors to an ego-motion density estimate via a Mixture Density Network (MDN). By modeling the architecture as a Conditional Variational Autoencoder (CVAE), our model is able to provide introspective reasoning and prediction for ego-motion induced scene-flow. Additionally, our proposed model is especially amenable to bootstrapped egomotion learning in robots where the supervision in ego-motion estimation for a particular camera sensor can be obtained from standard navigation-based sensor fusion strategies (GPS/INS and wheel-odometry fusion). Through experiments, we show the utility of our proposed approach in enabling the concept of self-supervised learning for visual ego-motion estimation in autonomous robots.", "creator": "LaTeX with hyperref package"}}}