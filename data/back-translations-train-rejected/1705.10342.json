{"id": "1705.10342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Deep Learning for Ontology Reasoning", "abstract": "In this work, we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning. To this end, we introduce a new model for statistical relational learning that is built upon deep recursive neural networks, and give experimental evidence that it can easily compete with, or even outperform, existing logic-based reasoners on the task of ontology reasoning. More precisely, we compared our implemented system with one of the best logic-based ontology reasoners at present, RDFox, on a number of large standard benchmark datasets, and found that our system attained high reasoning quality, while being up to two orders of magnitude faster.", "histories": [["v1", "Mon, 29 May 2017 18:17:52 GMT  (19kb)", "http://arxiv.org/abs/1705.10342v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["patrick hohenecker", "thomas lukasiewicz"], "accepted": false, "id": "1705.10342"}, "pdf": {"name": "1705.10342.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["thomas.lukasiewicz}@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.10 342v 1 [cs.A I] 2 9M ay"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to take the lead in the EU in order to win the Presidency of the Council of the European Union."}, {"heading": "2 Background", "text": "As already mentioned in the introduction, our work lies at the intersection of two traditionally quite separate fields, namely ML and KRR. Therefore, in this section we review the most important concepts from both areas that are required to follow the following explanations."}, {"heading": "2.1 Ontological Knowledge Bases (OKBs)", "text": "A central idea in the field of KRR is the use of so-called ontologies. In this context, an ontology is a formal description of a concept or domain, such as a part of the real world, and the word \"formal\" emphasizes that such a description must be specified by clearly defined semantics. This, in turn, allows us to use a formal reasoning to draw conclusions from such an ontology. An important aspect is that an ontology is situated at the meta-level, which means that it could indicate general concepts or relationships, but does not contain facts. However, we are only talking about a number of facts along with an ontology."}, {"heading": "2.2 Recursive Neural Tensor Networks (RNTNs)", "text": "Recursive NNs (Pollack, 1990) are a special type of network architecture introduced to handle training instances that serve as trees rather than more general feature vectors. Generally, they can handle any directed acyclic graph (DAG), since any of these graphs can be unrolled as a tree, and the only requirement is that the leaf nodes have vector representations attached to them. An example from the field of NLP is the parse tree of a sentence in which each node represents a word and is embedded either as a hot vector or a previously learned word."}, {"heading": "3 Relational Tensor Networks (RTNs)", "text": "In this section, we present a new model for SRL, which, for lack of a better name, we call the Relational Tensor Network (RTN). An RTN is essentially an RNTN that uses a modified bilinear tensor layer. However, the underlying intuition is quite different, and the term \"relational\" emphasizes the focus on relational data sets."}, {"heading": "3.1 The Basic Model", "text": "In this case, however, it is a reactionary situation, in which it is a reactionary situation, in which there is a reactionary downward spiral, in which there is a reactionary downward spiral. (...) In this case, it is a reactionary downward spiral. (...) The reactionary downward spiral in the reactionary downward spiral leads to a reactionary downward spiral in the reactionary downward spiral. (...) The reactionary downward spiral in the reactionary downward spiral is in the reactionary downward spiral. (...)"}, {"heading": "3.2 Training", "text": "As previously indicated, we usually use RTNs to calculate embedding for individuals used as input for a specific prediction task. Therefore, it makes sense to train an RTN along with the model used to calculate these predictions, and when we subsequently talk about an RTN, we assume that it will be used along with a predictor above it. If we only care about individual embedding, regardless of a specific subsequent task, then we can simply add a feed layer - or another differentiated learning model - on the RTN and train the model to reconstruct the provided feature vectors. In this way, an RTN can act as a kind of relational autoencoder. Training such a model is simple and alternates between calculating embedding and making predictions based on them. In each training step, we will start from the feature vectors of individuals as provided in the data."}, {"heading": "3.3 Related Models", "text": "In the field of SFD, there are a few other approaches that model the effects of relationships on individual embedding in the form of (higher-value) tensor products - cf. e.g. Nickel et al. (2011, 2012). However, these methods, which belong to the category of latent variable models, are based on the idea of factoring a tensor that describes the structure of a relational dataset into a product of an embedding matrix and another tensor that represents the relationships present in the data. On the basis of this formulation, the actual learning process is then cast as a regulated minimization problem. In contrast, an RTN calculates embedding both during training and during application by means of a random process, and thus differs fundamentally from this idea."}, {"heading": "4 Reasoning with RTNs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Applying RTNs to OKBs", "text": "As discussed in Section 2.1, OKBs can be considered DAGs, and therefore the application of an RTN to this type of data is straightforward. Therefore, we are left with only the indication of the predictive model we want to use over the RTN. However, in the context of an OKB, there are two types of predictions that interest us, namely, the affiliation of individuals to classes on the one hand, and the existence of relationships on the other. From an ML perspective, these are really two different goals, and we can formally describe them as follows: letK is an OKB that (accurately) includes the non-ary predictions P1,... Pk, and (accurately) the binary predictions Q1,. Qand T K are the part of the OKB that we have as a training set. Then t (1) and t (2) are two target functions defined as a unit (1): {Individuals (K) \u2192 1, 1} i \u2192 7 (i) (i)."}, {"heading": "4.2 Predicting Classes and Relations Simultaneously", "text": "While the targets t (1) and t (2) can be considered independent in terms of prediction, this is clearly not the case when calculating individual embedding. We need an embedding that reflects all the information we have about an individual as specified in the semantics of the OKB under consideration. Therefore, the tensor layers of an RTN need to learn how individual vectors can be adapted to both simple and binary predictors, i.e. classes and relationships. To take this into account, we train RTNs - with regard to the specific use case of ontological thinking - on mini-batches consisting of training samples for both prediction goals."}, {"heading": "5 Evaluation", "text": "To evaluate the proposed approach in a realistic scenario, we have implemented a novel triple storage called NeTS (Neural Triple Store), which achieves ontology reasoning solely through an RTN. NeTS provides a simple, SPARQL-like query interface that allows you to submit atomic queries as well as conjunctions of such queries (see Figure 1). When the system is booted, the first step is to load a set of learned weights from the disk - the actual learning process is not currently part of NeTS and can be included in future versions of it. Next, it monitors whether previously embedded individuals have been created on the disk, and loads them as well, if at all. If not, then NeTS creates such embeddings as described above, a step comparable to what is normally referred to as materialization in the context of database systems."}, {"heading": "5.1 Test Data", "text": "In order to maintain comparability, we have evaluated our approach using the same data sets that Motik et al. (2014) used for their experiments with RDFox (Nenov et al., 2015).2 As already mentioned, RDFox is indeed a great yardstick, as it has currently proven to be the most efficient triple memory. However, for comparison with other systems, we refer the interested reader to Motik et al. (2014).The test data consists of four semantic web KBs of different sizes and properties, including two data sets from the real world, a fraction of DBpedia (Bizer et al., 2009) and the Claros KB3, as well as two synthetic, LUBM (Guo et al., 2005) and UOBM (Ma et al., 2006).Their properties are summarized in Table 1. While all of these data are available in multiple formats, we have used the ontologies stated in OWL and the facts that we consider sufficient for our experiments to ensure that at least 5% of the RTN is sufficient for our experiments."}, {"heading": "5.2 Experimental Setup", "text": "All of our experiments were conducted on a server with 24 Intel Xeon E5-2620 (6 x 2.40GHz) CPUs, 64GB RAM, and an Nvidia GeForce GTX Titan X. The test system housed the Ubuntu server 14.04 LTS (64 bit) with CUDA 8.0 and cuDNN 5.1 for GPGPU. Note, however, that NeTS does not use multiprocessor or threading besides GPU, which means that the only way to parallelise is on the GPU. Therefore, NeTS had about half the resources available for CPU and RAM that RDFox used in the experiments by Motik et al. (2014). 2All of these data sets are available at http: / www.cs.ox.ac.uk / tools / RDFox / AI /."}, {"heading": "5.3 Results", "text": "In order to assess the quality of NeTS, we need to evaluate it on two accounts. First, we need to look at its predictive performance based on the embedding calculated by the underlying RTN model, and second, we need to determine the efficiency of the system in terms of time consumption. We start with the former. To this end, we look at Table 2, which includes the accuracy as well as the F1 values that NeTS has achieved on the test sets held, averaged across all classes and relationships. We see that the model consistently achieves great results on both measures. However, note that the F1 rating is the more critical criterion, as all predicates are highly unbalanced. Nevertheless, the RTN effectively learns embedding that allows a distinction between positive and negative instances. Table 3, in contrast, lists the times for NeTS to import and materialize the data, each of the data sets together with the respective measurements RDFox (Motik et)."}, {"heading": "6 Summary and Outlook", "text": "We have presented a new method for SRL based on deep learning, and used it to develop a highly efficient, learning-based system for ontological thinking. In addition, we have made an experimental comparison with one of the currently best logic-based ontological argumentators, RDFox, on several major standard benchmarks, and demonstrated that our approach achieves a high quality of reasoning while being up to two orders of magnitude faster. An interesting topic for future research is exploring ways to further improve our accuracy in ontological thinking, for example by incorporating additional synthetic data and / or minor refinements to the RTN architecture."}, {"heading": "Acknowledgments", "text": "This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) under grants EP / J008346 / 1, EP / L012138 / 1 and EP / M025268 / 1 and by the Alan Turing Institute under EPSRC grant EP / N510129 / 1. Patrick is also supported by the EPSRC under the grant OUCL / 2016 / PH and the Oxford-DeepMindGraduate Scholarship under the grant GAF1617 _ OGSMF-DMCS _ 1036172."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["cent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "The Description Logic Handbook: Theory, Implementation, and Applications", "author": ["Franz Baader", "Diego Calvanese", "Deborah L. McGuinness", "Daniele Nardi", "Peter F. PatelSchneider"], "venue": null, "citeRegEx": "Baader et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baader et al\\.", "year": 2007}, {"title": "DBpedia\u2014A crystallization point for the Web of Data", "author": ["Christian Bizer", "Jens Lehmann", "Georgi Kobilarov", "S\u00f6ren Auer", "Christian Becker", "Richard Cyganiak", "Sebastian Hellmann"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Bizer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bizer et al\\.", "year": 2009}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Samuel R. Bowman"], "venue": null, "citeRegEx": "Bowman.,? \\Q2013\\E", "shortCiteRegEx": "Bowman.", "year": 2013}, {"title": "Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches", "author": ["Evgeniy Gabrilovoch", "Ramanathan Guha", "Andrew McCallum", "Kevin Murphy", "editors"], "venue": "Palo Alto, California,", "citeRegEx": "Gabrilovoch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gabrilovoch et al\\.", "year": 2015}, {"title": "Introduction to Statistical Relational Learning. Adaptive Computation and Machine Learning", "author": ["Lise Getoor", "Ben Taskar"], "venue": null, "citeRegEx": "Getoor and Taskar.,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Taskar.", "year": 2007}, {"title": "Learning Task-Dependent Distributed Representations by Backpropagation Through Structure", "author": ["Christoph Goller", "Andreas K\u00fcchler"], "venue": "In IEEE International Conference on Neural Networks,", "citeRegEx": "Goller and K\u00fcchler.,? \\Q1996\\E", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "LUBM: A benchmark for OWL knowledge base systems", "author": ["Yuanbo Guo", "Zhengxiang Pan", "Jeff Heflin"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Guo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2005}, {"title": "PyCUDA and PyOpenCL: A Scripting-Based Approach to GPU Run-Time Code Generation", "author": ["Andreas Kl\u00f6ckner", "Nicolas Pinto", "Yunsup Lee", "B. Catanzaro", "Paul Ivanov", "Ahmed Fasih"], "venue": "Parallel Computing,", "citeRegEx": "Kl\u00f6ckner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kl\u00f6ckner et al\\.", "year": 2012}, {"title": "Towards a Complete OWL Ontology Benchmark", "author": ["Li Ma", "Yang Yang", "Zhaoming Qiu", "Guotong Xie", "Yue Pan", "Shengping Liu"], "venue": "In Proceedings of the 3rd European Semantic Web Conference (ESWC", "citeRegEx": "Ma et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2006}, {"title": "Parallel Materialisation of Datalog Programs in Centralised, Main-Memory RDF Systems", "author": ["Boris Motik", "Yavor Nenov", "Robert Piro", "Ian Horrocks", "Dan Olteanu"], "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Motik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Motik et al\\.", "year": 2014}, {"title": "RDFox: A Highly-Scalable RDF Store", "author": ["Yavor Nenov", "Robert Piro", "Boris Motik", "Ian Horrocks", "Zhe Wu", "Jay Banerjee"], "venue": "In Proceedings of the 14th International Semantic Web Conference (ISWC 2015),", "citeRegEx": "Nenov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nenov et al\\.", "year": 2015}, {"title": "A Three-WayModel for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing YAGO", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st International Conference on World Wide Web,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Recursive distributed representations", "author": ["Jordan B. Pollack"], "venue": "Artificial Intelligence,", "citeRegEx": "Pollack.,? \\Q1990\\E", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge", "author": ["Luciano Serafini", "Artur d\u2019Avila Garcez"], "venue": null, "citeRegEx": "Serafini and Garcez.,? \\Q2016\\E", "shortCiteRegEx": "Serafini and Garcez.", "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "However, many of these issues can be dealt with effectively by using methods of ML, which are in this context often subsumed under the notion of statistical relational learning (SRL; Getoor and Taskar, 2007)\u2014cf.", "startOffset": 177, "endOffset": 207}, {"referenceID": 4, "context": ", Gabrilovoch et al. (2015). The main motivation behind this is that most KRR formalisms used today are rooted in symbolic logic, which allows for answering queries accurately by employing formal reasoning, but also comes with a number of issues, like difficulties with handling incomplete, conflicting, or uncertain information and scalability problems.", "startOffset": 2, "endOffset": 28}, {"referenceID": 4, "context": ", Gabrilovoch et al. (2015). The main motivation behind this is that most KRR formalisms used today are rooted in symbolic logic, which allows for answering queries accurately by employing formal reasoning, but also comes with a number of issues, like difficulties with handling incomplete, conflicting, or uncertain information and scalability problems. However, many of these issues can be dealt with effectively by using methods of ML, which are in this context often subsumed under the notion of statistical relational learning (SRL; Getoor and Taskar, 2007)\u2014cf. Nickel et al. (2016) for a recent survey.", "startOffset": 2, "endOffset": 588}, {"referenceID": 3, "context": "However, these focus on rather restricted logics, like natural logic (Bowman, 2013) or real logic (Serafini and d\u2019Avila Garcez, 2016), and do not consider reasoning in its full generality.", "startOffset": 69, "endOffset": 83}, {"referenceID": 3, "context": "However, these focus on rather restricted logics, like natural logic (Bowman, 2013) or real logic (Serafini and d\u2019Avila Garcez, 2016), and do not consider reasoning in its full generality. Besides this, \u00bbreasoning\u00ab appears in connection with deep learning mostly in the context of NLP\u2014 e.g., Socher et al. (2013). The main contributions of this paper are briefly as follows:", "startOffset": 70, "endOffset": 313}, {"referenceID": 11, "context": "\u2022 Furthermore, we provide an experimental comparison of the suggested approach with one of the best logic-based ontology reasoners at present, RDFox (Nenov et al., 2015), on several large standard benchmarks.", "startOffset": 149, "endOffset": 169}, {"referenceID": 1, "context": "In practice, and in the context of description logics (Baader et al., 2007), ontologies are usually defined in terms of unary and binary predicates.", "startOffset": 54, "endOffset": 75}, {"referenceID": 15, "context": "Recursive NNs (Pollack, 1990) are a special kind of network architecture that was introduced in order to deal with training instances that are given as trees rather than, as more commonly, feature vectors.", "startOffset": 14, "endOffset": 29}, {"referenceID": 17, "context": "In this work, we make use of the following recursive layer, which defines what is referred to as recursive neural tensor network (RNTN; Socher et al., 2013):", "startOffset": 129, "endOffset": 156}, {"referenceID": 6, "context": "In general, recursive NNs are trained by means of stochastic gradient descent (SGD) together with a straightforward extension of standard backpropagation, called backpropagation through structure (BPTS; Goller and K\u00fcchler, 1996).", "startOffset": 196, "endOffset": 228}, {"referenceID": 17, "context": "The foregoing considerations also explain the differences between Equation 2 and the original tensor layer given in Equation 1 (Socher et al., 2013).", "startOffset": 127, "endOffset": 148}, {"referenceID": 17, "context": "For t, we first add an additional original tensor layer as given in Equation 1, like it was used by Socher et al. (2013), and use multinomial logistic regression on top of it as well.", "startOffset": 100, "endOffset": 121}, {"referenceID": 8, "context": "2 (Kl\u00f6ckner et al., 2012).", "startOffset": 2, "endOffset": 25}, {"referenceID": 11, "context": "(2014) used for their experiments with RDFox (Nenov et al., 2015).", "startOffset": 45, "endOffset": 65}, {"referenceID": 10, "context": "To maintain comparability, we evaluated our approach on the same datasets that Motik et al. (2014) used for their experiments with RDFox (Nenov et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 10, "context": "To maintain comparability, we evaluated our approach on the same datasets that Motik et al. (2014) used for their experiments with RDFox (Nenov et al., 2015). As mentioned earlier, RDFox is indeed a great benchmark, since it has been shown to be the most efficient triple store at present. For a comparison with other systems, however, we refer the interested reader to Motik et al. (2014).", "startOffset": 79, "endOffset": 390}, {"referenceID": 2, "context": "Among these are two real-world datasets, a fraction of DBpedia (Bizer et al., 2009) and the Claros KB, as well as two synthetic ones, LUBM (Guo et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 7, "context": ", 2009) and the Claros KB, as well as two synthetic ones, LUBM (Guo et al., 2005) and UOBM (Ma et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 9, "context": ", 2005) and UOBM (Ma et al., 2006).", "startOffset": 17, "endOffset": 34}, {"referenceID": 10, "context": "Therefore, in terms of CPU and RAM, NeTS had about half of the resources at its disposal that RDFox utilized in the experiments conducted by Motik et al. (2014).", "startOffset": 141, "endOffset": 161}, {"referenceID": 10, "context": "Table 3, in contrast, lists the times for NeTS to import and materialize each of the datasets along with the respective measurements for RDFox (Motik et al., 2014).", "startOffset": 143, "endOffset": 163}, {"referenceID": 10, "context": "For RDFox, these are the numbers reported by Motik et al. (2014) for computing a lower (left) and upper bound (right), respectively, on the possible inferences.", "startOffset": 45, "endOffset": 65}], "year": 2017, "abstractText": "In this work, we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning. To this end, we introduce a new model for statistical relational learning that is built upon deep recursive neural networks, and give experimental evidence that it can easily compete with, or even outperform, existing logic-based reasoners on the task of ontology reasoning. More precisely, we compared our implemented system with one of the best logic-based ontology reasoners at present, RDFox, on a number of large standard benchmark datasets, and found that our system attained high reasoning quality, while being up to two orders of magnitude faster.", "creator": "LaTeX with hyperref package"}}}