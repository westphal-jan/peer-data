{"id": "1705.10385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Collaborative Deep Learning for Speech Enhancement: A Run-Time Model Selection Method Using Autoencoders", "abstract": "We show that a Modular Neural Network (MNN) can combine various speech enhancement modules, each of which is a Deep Neural Network (DNN) specialized on a particular enhancement job. Differently from an ordinary ensemble technique that averages variations in models, the propose MNN selects the best module for the unseen test signal to produce a greedy ensemble. We see this as Collaborative Deep Learning (CDL), because it can reuse various already-trained DNN models without any further refining. In the proposed MNN selecting the best module during run time is challenging. To this end, we employ a speech AutoEncoder (AE) as an arbitrator, whose input and output are trained to be as similar as possible if its input is clean speech. Therefore, the AE can gauge the quality of the module-specific denoised result by seeing its AE reconstruction error, e.g. low error means that the module output is similar to clean speech. We propose an MNN structure with various modules that are specialized on dealing with a specific noise type, gender, and input Signal-to-Noise Ratio (SNR) value, and empirically prove that it almost always works better than an arbitrarily chosen DNN module and sometimes as good as an oracle result.", "histories": [["v1", "Mon, 29 May 2017 20:30:24 GMT  (287kb,D)", "http://arxiv.org/abs/1705.10385v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["minje kim"], "accepted": false, "id": "1705.10385"}, "pdf": {"name": "1705.10385.pdf", "metadata": {"source": "CRF", "title": "Collaborative Deep Learning for Speech Enhancement: A Run-Time Model Selection Method Using Autoencoders", "authors": ["Minje Kim"], "emails": ["minje@indiana.edu"], "sections": [{"heading": null, "text": "Keywords: Language Improvement, Source Separation, Deep Learning, Modular Neural Networks, Autoencoder"}, {"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city, in which it is a country."}, {"heading": "2 DNN for Supervised and Unsupervised Learning", "text": "In this section, we will discuss two basic DNN systems for supervised speech denocialization and unattended speech modeling, which will then be merged into a proposed system in Section 3."}, {"heading": "2.1 DNN for Supervised Speech Denoising", "text": "We start with a D-dimensional complex-weighted Fourier spectrum in the t-th timeframe = > Selection procedure for the immediate mixing of a pure speech and sound spectrum: xt = st + nt 2. Normally, the input x (sometimes together with its successive frames) goes through a selection procedure to construct the input feature vector x-RK (1). The goal of the training is now to learn the mapping function FDNN to generate an output vector y-RD, which is either an estimate for the original language characteristics or a mask that can be used later to restore the language. In the latter case, the feedback forward and masking procedures work as follows: y = FDNN (x), s = y x, (1) where stands for an element-by-element multiplication and s is an estimate of s. For the training we can calculate the magnitude-masking ratio."}, {"heading": "2.2 Autoencoders for Unsupervised Speech Modeling", "text": "It is another type of neural network whose target variables are identical to the input. (4) It is a simple AU that models a source, such as language, can be practiced by using pure speech spectra for both input and target. (4) It is a greedy layer learning process in which the input vector is characterized by random perturbations such as masking noise. (5) E is also a greedy layer-by-layer learning process. (6) It is about random perturbations such as masking noise. (5) It is a way in which one proves oneself incapable. (5) It is a way in which one proves oneself innocent. (6)"}, {"heading": "3 The Proposed Modular Neural Network for Collaborative Deep Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Proposed Architecture", "text": "FDNNj is one of the J participating DNN modules in the MNN, which has only been trained on a subset of all kinds of corruption, and the structure of the modules may also vary in terms of the number of layers and hidden units, the choice of activation functions, the use of relapses and folds, etc. Once each of them estimates a clean speech signal (DNNj), it is transmitted in the form of a magnitude spectrum to the FDAE model selector. FDAE is trained in advance to generate a clean speech spectrum for its input of the same kind, while we opt for the robustness of the various imperfections of the s (DNNj) to use a failure-based DAE as in (6) instead of an AE trained in clean speech. The key assumption is that a properly tracked NDNE keeps its clean speech spectrum intact, while its behavior is not guaranteed for an invisible non-language spectrum."}, {"heading": "3.2 Computational and Spatial Complexity", "text": "The computational and space complexity at runtime is clearly a problem with the proposed MNN-for-CDL method, as each participating DNN has to take a feedback step. There are some promising approaches to compressing DNNs, such as a slight approximation of weight matrices [22] and networks that work with bit logic and binary variables [23, 24]. Since the compressed networks demand their efficiency during runtime, they can replace the comprehensive networks for the purpose of model selection. Finally, after the AE selection is complete, we operate the best comprehensive DNN. We leave this network compression problem to future work."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 The Speech AE", "text": "Randomly selected 400 expressions from the TIMIT training set are used for the training (20 speakers \u00d7 2 genders \u00d7 10 expressions) using a short-term Fourier transformation with an image size of 1024 points and an overlap of 75% for time-frequency conversion. Loadable back propagation (rprop) [25] technique is used, and its parameters are found by validation with an additional four speakers: 0.5, 1.5, 10 \u2212 7 and 10 \u2212 1 for tracing, acceleration and minimum or maximum step sizes. We select a modified synchronous unit (ReLU) as proposed in [14] for the activation function. Failure parameters p (l) are all set to 0.8. The sum of the square error is minimized during the training. The stack size has been set at 1,000."}, {"heading": "4.2 Experiment 1: Variations in the Noise Types", "text": "For training, we prepare 60 clean expressions per sound type: (6 loudspeakers) \u00b7 (2 genders) \u00b7 (5 expressions) and then mix them up with one of the three sound types selected from {\"birds,\" \"typing,\" \"motorcycle\"] [11] at 0 dB SNR. We train one 512 \u00d7 2 DNN per one of the three noisy speech records as described in Section 4.1, except for some facts that (a) three input frames are always linked to form an input vector (b) the goal is a central frame masking vector. The logistical function ensures soft masking in the last layer. As for evaluation, we use both Signal-to-Distortion Ratio (SDR) [26] and Short-Time Objective Intelligibility (STOI) [27].Three test data sets are of 20 gender-balanced clean expressions (5 of 4 loudspeakers each mixed with different types of noise systems) of the three parts of the 81."}, {"heading": "4.3 Experiment 2: Variations in Gender", "text": "Next, we construct two sets of data, each from 12 male or 12 female speakers. This time, all ten sound types used in [11] are mixed with the 12 x 5 clean utterances, for a total of 600 per gender. From these data sets, two gender-specific 2048 x 2 DNNs are trained each. To test, we collect 10 x 5 utterances per gender and mix them with the same ten sound types. The module trained by the same sex performs better on the test set with the same sex than the wrong choice: 10.38 vs 8.15 and 11.00 vs 7.58 dB in SDR (Table 2), although its distance is smaller than Table 1. It could be because there is a male test speaker whose voice is more similar to a female training speaker, and vice versa. Similarly, the oracle case is better than the right choice of DNN. Consequently, the decision of the AEs in this experiment is not perfect, but approaches the oracle case and shows much better results than the chance."}, {"heading": "4.4 Experiment 3: Variations in the Input SNR", "text": "For the final experiment, we randomly select 12 gender-balanced loudspeakers and their five expressions for training, and then mix all ten types of noise to form a set of 600 noisy expressions. For each set of 600 signals, we determine the volume of the source of interference so that the mix has one of three SNR values, -5, 0 and + 5. We train three 2048 x 2 DNN modules on it. Table 3 shows that the difference between the DNN modules is minuscule. For example, for the test signals with -5 dB SNR, the DNN system trained for 0 dB mixtures performs better than the correct choice for SDRs (7.00 vs 6.89 dB), because the correct DNN has separated the noise too strongly, while introducing more artifacts, which in turn reduces the overall separation quality. Nevertheless, the DNN system trained on the -5 dB samples shows the best performance in STOI."}, {"heading": "5 Conclusion", "text": "The proposed MNN method performed better than the average of applicant results in general. One flat PCS was sufficient for most jobs, while the other deeper and larger PCS was better suited to confusing high quality cases. The system was tested with variations in the type of noise, sex and input SNR. As a future work, we plan to investigate further variations, e.g. Hall and LSTMs for both DNN modules and DAEs."}], "references": [{"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Reconstruction techniques for improving the perceptual quality of binary masked speech", "author": ["D.S. Williamson", "Y. Wang", "D.L. Wang"], "venue": "Journal of the Acoustical Society of America, vol. 136, pp. 892\u2013902, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Ideal ratio mask estimation using deep neural networks for robust speech recognition", "author": ["A. Narayanan", "D.L. Wang"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 2013, pp. 7092\u20137096.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint optimization of masks and deep recurrent neural networks for monaural source separation", "author": ["P. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 12, pp. 2136\u20132147, Dec 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards scaling up classification-based speech separation", "author": ["Y. Wang", "D.L. Wang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7, pp. 1381\u20131390, July 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["J.R. Hershey", "Z. Chen", "J. Le Roux", "S. Watanabe"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Mar. 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr", "author": ["F. Weninger", "H. Erdogan", "S. Watanabe", "E. Vincent", "J. Le Roux", "J.R. Hershey", "B. Schuller"], "venue": "Proceedings of the International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), Aug. 2015. 6", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep NMF for speech separation", "author": ["J. Le Roux", "J.R. Hershey", "F. Weninger"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Apr. 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised and semi-supervised separation of sounds from singlechannel mixtures", "author": ["P. Smaragdis", "B. Raj", "M. Shashanka"], "venue": "Proceedings of the International Conference on Independent Component Analysis and Blind Signal Separation (ICA), London, UK, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Stopping criteria for non-negative matrix factorization based supervised and semi-supervised source separation", "author": ["F.G. Germain", "G.J. Mysore"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 10, pp. 1284\u20131288, Oct 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Online PLCA for real-time semi-supervised source separation", "author": ["Z. Duan", "G.J. Mysore", "P. Smaragdis"], "venue": "Proceedings of the International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), 2012, pp. 34\u201341.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, pp. 788\u2013791, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems (NIPS). 2001, vol. 13, MIT Press.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Experiments on deep learning for speech denoising", "author": ["D. Liu", "P. Smaragdis", "M. Kim"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), Sep 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive denoising autoencoders: A fine-tuning scheme to learn from test mixtures", "author": ["M. Kim", "P. Smaragdis"], "venue": "Proceedings of the International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), August 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Unseen noise estimation using separable deep auto encoder for speech enhancement", "author": ["M. Sun", "X. Zhang", "H. Van hamme", "T.F. Zheng"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 1, pp. 93\u2013104, Jan 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech intelligibility potential of general and specialized deep neural network based speech enhancement systems", "author": ["M. Kolb\u00e6k", "Z.H. Tan", "J. Jensen"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 1, pp. 153\u2013167, Jan 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural Computation, vol. 3, no. 1, pp. 79\u201387, Mar. 1991.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1991}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), 2008, pp. 1096\u20131103.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, Dec. 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, Jan. 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1929}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2013, pp. 2365\u20132369.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Bitwise neural networks", "author": ["M. Kim", "P. Smaragdis"], "venue": "International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning, Jul 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2015, pp. 3105\u20133113.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A direct adaptive method for faster backpropagation learning: the rprop algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "IEEE International Conference on Neural Networks, 1993, pp. 586\u2013591 vol.1.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "C. Fevotte", "R. Gribonval"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 1462\u20131469, 2006. 7", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "A short-time objective intelligibility measure for timefrequency weighted noisy speech", "author": ["C.H. Taal", "R.C. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2010. 8", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "introduced a pre-training based Speech Denoising AutoEncoder (SDAE), which uses magnitudes of Fourier coefficients for both input and output [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "Ideal Binary Masks (IBM) [2] and Ideal Ratio Masks (IRM) [3] are another common target representations.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Ideal Binary Masks (IBM) [2] and Ideal Ratio Masks (IRM) [3] are another common target representations.", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "\u2019s Deep Recurrent Neural Networks (DRNN) added the recurrent structure to SDAE, along with a discriminative term, too [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 4, "context": "More specialized speech features showed state-of-the-art performances such as cochleagrams [5], Mel-Frequency Cepstrum Coefficients (MFCC) [3], and their combinations.", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "More specialized speech features showed state-of-the-art performances such as cochleagrams [5], Mel-Frequency Cepstrum Coefficients (MFCC) [3], and their combinations.", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "Structural variations have been also investigated in the literature: deep clustering based on the independence of speakers [6], Long Short-Term Memory (LSTM) to handle long-term dependency of time-structured speech signals [7], deep unfolding networks to substitute the iterations in some estimation algorithms with a number of hidden layers [8], etc.", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "Structural variations have been also investigated in the literature: deep clustering based on the independence of speakers [6], Long Short-Term Memory (LSTM) to handle long-term dependency of time-structured speech signals [7], deep unfolding networks to substitute the iterations in some estimation algorithms with a number of hidden layers [8], etc.", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "Structural variations have been also investigated in the literature: deep clustering based on the independence of speakers [6], Long Short-Term Memory (LSTM) to handle long-term dependency of time-structured speech signals [7], deep unfolding networks to substitute the iterations in some estimation algorithms with a number of hidden layers [8], etc.", "startOffset": 342, "endOffset": 345}, {"referenceID": 8, "context": "For example, in the semi-supervised source separation scenario, the system can learn the unseen noise dictionary from the noise source mixed in the test signal during run time along with the ordinary speech dictionary [9, 10, 11].", "startOffset": 218, "endOffset": 229}, {"referenceID": 9, "context": "For example, in the semi-supervised source separation scenario, the system can learn the unseen noise dictionary from the noise source mixed in the test signal during run time along with the ordinary speech dictionary [9, 10, 11].", "startOffset": 218, "endOffset": 229}, {"referenceID": 10, "context": "For example, in the semi-supervised source separation scenario, the system can learn the unseen noise dictionary from the noise source mixed in the test signal during run time along with the ordinary speech dictionary [9, 10, 11].", "startOffset": 218, "endOffset": 229}, {"referenceID": 9, "context": "As shown in [10], this semi-supervised technique is prone to overfitting due to the lack of the knowledge about the noise source.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "performed some experiments to see the generalization power of a SDAE [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Nonnegative Matrix Factorization (NMF) [12, 13] and its variations are a common choice for the dictionary learning algorithm.", "startOffset": 39, "endOffset": 47}, {"referenceID": 12, "context": "Nonnegative Matrix Factorization (NMF) [12, 13] and its variations are a common choice for the dictionary learning algorithm.", "startOffset": 39, "endOffset": 47}, {"referenceID": 14, "context": "AEs: the bottom SDAE trained from known mixtures of speech and noise (to denoise them) and the top AE trained only from pure speech [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "[2], where an NMF speech dictionary was used to further clean up the DNN results, although the use of NMF was limited to smoothing the results rather than fine-tune the main DNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In there, the speech AE and an embedded NMF dictionary for an additional speech modeling are trained in advance, while the noise AE is trained from the test signal [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "The proposed Modular Neural Network (MNN) assumes that it is easier to learn a smaller specialized DNN that work better for a particular enhancement job than a larger DNN for the general speech enhancement task as partially shown in [17].", "startOffset": 233, "endOffset": 237}, {"referenceID": 17, "context": "Similarly, an MNN consists of local experts that provide various outputs for a test sample and a gating network that chooses the best module [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "In the deep learning literature, a DAE has been also used to provide a greedy layer-wise feature learning [19, 20], where the input vector goes through random perturbations such as masking noise: s\u0303 = |s| \u03bd, \u03bdi \u223c Bernoulli(p), (5) E ( s\u0304 \u2225\u2225FDAE(s\u0303)), (6)", "startOffset": 106, "endOffset": 114}, {"referenceID": 19, "context": "In the deep learning literature, a DAE has been also used to provide a greedy layer-wise feature learning [19, 20], where the input vector goes through random perturbations such as masking noise: s\u0303 = |s| \u03bd, \u03bdi \u223c Bernoulli(p), (5) E ( s\u0304 \u2225\u2225FDAE(s\u0303)), (6)", "startOffset": 106, "endOffset": 114}, {"referenceID": 20, "context": "A similar concept can be found in the dropout technique, too [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "A clarification for SDAE: As we have reviewed in Section 1, SDAEs have been actively used to directly approximate the mapping from the contaminated speech to the clean ones in the context of supervised learning [1, 14].", "startOffset": 211, "endOffset": 218}, {"referenceID": 13, "context": "A clarification for SDAE: As we have reviewed in Section 1, SDAEs have been actively used to directly approximate the mapping from the contaminated speech to the clean ones in the context of supervised learning [1, 14].", "startOffset": 211, "endOffset": 218}, {"referenceID": 21, "context": "There are some promising approaches to compressing DNNs such as a low-rank approximation of the weight matrices [22] and networks that operate using bit logics and binary variables [23, 24].", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "There are some promising approaches to compressing DNNs such as a low-rank approximation of the weight matrices [22] and networks that operate using bit logics and binary variables [23, 24].", "startOffset": 181, "endOffset": 189}, {"referenceID": 23, "context": "There are some promising approaches to compressing DNNs such as a low-rank approximation of the weight matrices [22] and networks that operate using bit logics and binary variables [23, 24].", "startOffset": 181, "endOffset": 189}, {"referenceID": 24, "context": "Resilient backpropagation (Rprop) [25] technique is employed, and their parameters are found through a validation with additional four speakers: 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "We choose a modified Rectified Linear Unit (ReLU) as proposed in [14] for the activation function.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "They are then mixed up with one of three noise types chosen from {\u201cBirds\u201d, \u201cTyping\u201d, \u201cMotorcycle\u201d} [11] at 0 dB SNR.", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "As for evaluation, we use both Signal-to-Distortion Ratio (SDR) [26] and Short-Time Objective Intelligibility (STOI) [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 26, "context": "As for evaluation, we use both Signal-to-Distortion Ratio (SDR) [26] and Short-Time Objective Intelligibility (STOI) [27].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "This time all ten noise types used in [11] are mixed with the 12\u00d7 5 clean utterances, totalling 600 per gender.", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "We show that a Modular Neural Network (MNN) can combine various speech enhancement modules, each of which is a Deep Neural Network (DNN) specialized on a particular enhancement job. Differently from an ordinary ensemble technique that averages variations in models, the propose MNN selects the best module for the unseen test signal to produce a greedy ensemble. We see this as Collaborative Deep Learning (CDL), because it can reuse various already-trained DNN models without any further refining. In the proposed MNN selecting the best module during run time is challenging. To this end, we employ a speech AutoEncoder (AE) as an arbitrator, whose input and output are trained to be as similar as possible if its input is clean speech. Therefore, the AE can gauge the quality of the module-specific denoised result by seeing its AE reconstruction error, e.g. low error means that the module output is similar to clean speech. We propose an MNN structure with various modules that are specialized on dealing with a specific noise type, gender, and input Signal-to-Noise Ratio (SNR) value, and empirically prove that it almost always works better than an arbitrarily chosen DNN module and sometimes as good as an oracle result.", "creator": "LaTeX with hyperref package"}}}