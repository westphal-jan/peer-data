{"id": "1705.10422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation", "abstract": "Sensor fusion is indispensable to improve accuracy and robustness in an autonomous navigation setting. However, in the space of end-to-end sensorimotor control, this multimodal outlook has received limited attention. In this work, we propose a novel stochastic regularization technique, called Sensor Dropout, to robustify multimodal sensor policy learning outcomes. We also introduce an auxiliary loss on policy network along with the standard DRL loss that help reduce the action variations of the multimodal sensor policy. Through empirical testing we demonstrate that our proposed policy can 1) operate with minimal performance drop in noisy environments, 2) remain functional even in the face of a sensor subset failure. Finally, through the visualization of gradients, we show that the learned policies are conditioned on the same latent input distribution despite having multiple sensory observations spaces - a hallmark of true sensor-fusion. This efficacy of a multimodal policy is shown through simulations on TORCS, a popular open-source racing car game. A demo video can be seen here:", "histories": [["v1", "Tue, 30 May 2017 00:52:24 GMT  (4822kb,D)", "http://arxiv.org/abs/1705.10422v1", null], ["v2", "Wed, 1 Nov 2017 02:30:51 GMT  (1828kb,D)", "http://arxiv.org/abs/1705.10422v2", "to be published in Conference on Robot Learning (CoRL), 2017"]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["guan-horng liu", "avinash siravuru", "sai prabhakar", "manuela veloso", "george kantor"], "accepted": false, "id": "1705.10422"}, "pdf": {"name": "1705.10422.pdf", "metadata": {"source": "CRF", "title": "Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation", "authors": ["Guan-Horng Liu", "Avinash Siravuru", "Sai Prabhakar", "Manuela Veloso", "George Kantor"], "emails": ["guanhorl@andrew.cmu.edu", "avinashs@andrew.cmu.edu", "spandise@andrew.cmu.edu", "mmv@cs.cmu.edu,", "kantor@ri.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "One of the main challenges for the development of autonomous navigation systems is that they are efficient and take appropriate measures."}, {"heading": "2 Related Work", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3 Multimodal Deep Reinforcement Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Deep Reinforcement Learning (DRL) Brief Review", "text": "We consider a standard Reinforcement Learning (RL) setup in which an agent operates in an environment. At each discrete time step, the agent observes a state of st-S, selects an action at step T and receives a scalar reward r (st, at).R. The agent's goal is to learn a policy that ultimately maximizes the expected return. The policy learned can be formulated either as a stochastic formula (a-s) = P (a-s) or as a deterministic factor a = \u00b5 (s). The agent's goal is to learn a policy that ultimately maximizes the expected return. The policy learned can be formulated either as an auchastic formula (a-s) = P (a-s) or as a deterministic a = \u00b5 (s). Value Function V and Action-Value Function Q\u03c0 describe the expected return for each state and state-action pair in adhering to a policy. Finally, an advantage function is defined as an additional benefit (s) or reward."}, {"heading": "3.2 Multimodal Sensor Policy Architecture", "text": "We refer to a series of observations composed of M sensors as S = [S (1) S (2).. S (M)] T, where S (i) stands for sensor observation. In the multimodal network, each sensory signal is pre-processed along an independent path. Each path has a feature extraction module that can be either a pure identity function (Modality 1) or a fold-based layer (Modality 2 \u2192 M). Naturally, the modular feature extraction stage allows independent extraction of outstanding information that can be transferred (with a little tuning if necessary) to other applications. Finally, the results of the feature extraction modules are flattened and linked to a multimodal state."}, {"heading": "4 Augmenting MDRL", "text": "In this section, we propose two methods to improve the formation of a multi-sensor policy: First, we introduce a new stochastic regulation called Sensor Dropout and explain its advantages over the standard dropout for this problem. Later, we propose an additional, unattended auxiliary loss function to reduce policy deviation."}, {"heading": "4.1 Sensor Dropout (SD) for Robustness", "text": "The sender configuration is a variant of the sender configuration of the sender configuration [35] that maintains the sender configuration on each sensor module instead of on each individual neuron. Although both methods share a similar motivation for stochastic regularization, SD is better motivated to handle multimodal sensor conditions such as latency, noise, and even partial sensor failure. As shown in Fig.1, the multimodal state is S (1) S (2) S (M) T, where S (i) overrides the sender configuration and even partial sensor failure."}, {"heading": "4.2 Auxiliary Loss for Variance Reduction", "text": "An alternative interpretation of the SD-enhanced policy is that sub-policies induced by each sensor combination are jointly optimized during the training. Describe the ultimate SD-enhanced policy and sub-policies induced by each sensor combination as microp and microp. To encourage the political network to determine a geometric mean over N different measures, we would like to point out that p (i) is a pseudo-Bernoulli, as we limit our attention to cases where at least one sensor block is switched on at a certain point in the shift. This implies that the switching on of one sensor block S (i) is independent of the other, the switching off of the aux policies is no longer completely independent of each sensor.Characteristics of each sensor that can be embedded with similar representations on the layer imply that the switching on of one sensor block S (i) is independent of the others."}, {"heading": "5 Evaluation Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Platform Setup", "text": "TORCS Simulator The proposed approach will be verified on TORCS [41], a popular open source car racing simulator that is capable of simulating physically realistic vehicle dynamics and multiple sensor modalities [43] to build complex AI agents. (2) Sensor 2 consists of four consecutive laser scans (i.e., at a certain point in time we enter scans from the times t, t \u2212 1, t \u2212 2 & t \u2212 3) Finally, as Sensor 3, we provide four consecutive color images that capture the front view of the car. These three images are used separately to develop our uni-modal sensor strategies, whereas the multimodal state has access to all sensors at any point."}, {"heading": "5.2 Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6 Discussion", "text": "As shown in the first and third columns, the performance of the naive multimodal sensor policy (red) and the policy trained with standard dropout (blue) drops dramatically as politics loses access to the image, which accounts for 87.9% of the entire multimodal state. Although dropout increases the performance of politics in the test environment, the generalization is limited to using the full multimodel state as input. On the other hand, SD generalizes politics across sensor modules, successfully transferring the sub-policy to the test environment. It is worth noting that the policy trained with SD is able to work even when both laser and image sensor are blocked."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we introduce a new stochastic regulation technology called Sensor Dropout to promote effective fusion of information from multiple sensors, which can further reduce policy variance by introducing additional loss during training. We show that SD's help reduces policy sensitivity to a specific sensor subgroup and enables it to function even in the event of partial sensor failure. Furthermore, the policy network is able to automatically draw conclusions and identify weight points that provide important information. In future work, we would like to extend the framework to other environments such as real robotics systems and other algorithms such as GPS [16], TRPO [31] and Q-Prop [9], etc. Second, systematic studies of the problems involved in improving the reward function for other important driving tasks such as collision avoidance and lane change, and how to adjust SD distribution during training are interesting."}, {"heading": "A Continuous Action Space Algorithms", "text": "A.1 Normalized Advantage Function (NAF) Q-Learning [36] is an extraordinary model-free algorithm in which the agent learns an approximatedQ function and follows a greedy policy \u00b5 (s) = argmaxaQ (s, a) at each step. The objective function J = Esi, ri \u0445 E, ai \u0445 \u03c0 [R1] can be achieved by defining the square loss Bellman error L = 1N \u2211 N i (yi \u2212 Q (si, ai | \u03b8Q) 2, in which target yi is defined as r (si, ai) + \u03b3Q (si + 1, \u00b5) (si \u2212 1) by minimizing the square loss Bellman error L = 1N \u2211 N (i \u2212 Q (si, ai | Q) 2, with target yi defined as r (si, ai) and ministerial task."}, {"heading": "B Experiment Details", "text": "The choice of the reward function differs slightly from [20] and [24] as an additional punitive term to punish byways that drift along the track. In practice, this modification leads to more stable strategies during training [15].B.2 Network ArchitectureFor laser feature extraction module, we use two 1D convolution layer with 4 filters of size 4 \u00d7 1, while image feature extraction is composed of three 2D convolution layer: one layer of 16 filters of size 4 \u2212 and riding length 4, followed by two layer with 32 filters of size 2 \u00d7 2 and riding length 2. Batch normalization is follow after each convolution layer. All these extraction modules are merged together and are followed later with two fully connected layers of 200 hidden units each."}, {"heading": "C More Experimental Results", "text": "C.1 Effect of Auxiliary LossesThe PCA covariance and the actual action variance are summarized in Table 4 and 5, respectively."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Sensor fusion is indispensable to improve accuracy and robustness in an<lb>autonomous navigation setting. However, in the space of end-to-end sensorimotor<lb>control, this multimodal outlook has received limited attention. In this work,<lb>we propose a novel stochastic regularization technique, called Sensor Dropout,<lb>to robustify multimodal sensor policy learning outcomes. We also introduce<lb>an auxiliary loss on policy network along with the standard DRL loss that help<lb>reduce the action variations of the multimodal sensor policy. Through empirical<lb>testing we demonstrate that our proposed policy can 1) operate with minimal<lb>performance drop in noisy environments, 2) remain functional even in the face<lb>of a sensor subset failure. Finally, through the visualization of gradients, we<lb>show that the learned policies are conditioned on the same latent input distribution<lb>despite having multiple sensory observations spaces a hallmark of true sensor-<lb>fusion. This efficacy of a multimodal policy is shown through simulations on<lb>TORCS, a popular open-source racing car game. A demo video can be seen here:<lb>https://youtu.be/HC3TcJjXf3Q.", "creator": "LaTeX with hyperref package"}}}