{"id": "1705.10432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning", "abstract": "Recent advances in combining deep learning and Reinforcement Learning have shown a promising path for designing new control agents that can learn optimal policies for challenging control tasks. These new methods address the main limitations of conventional Reinforcement Learning methods such as customized feature engineering and small action/state space dimension requirements. In this paper, we leverage one of the state-of-the-art Reinforcement Learning methods, known as Trust Region Policy Optimization, to tackle intersection management for autonomous vehicles. We show that using this method, we can perform fine-grained acceleration control of autonomous vehicles in a grid street plan to achieve a global design objective.", "histories": [["v1", "Tue, 30 May 2017 02:04:29 GMT  (1010kb,D)", "http://arxiv.org/abs/1705.10432v1", "Accepted in IEEE Smart World Congress 2017"]], "COMMENTS": "Accepted in IEEE Smart World Congress 2017", "reviews": [], "SUBJECTS": "cs.AI cs.RO cs.SY", "authors": ["hamid mirzaei", "tony givargis"], "accepted": false, "id": "1705.10432"}, "pdf": {"name": "1705.10432.pdf", "metadata": {"source": "CRF", "title": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning", "authors": ["Hamid Mirzaei"], "emails": ["mirzaeib@uci.edu", "givargis@uci.edu"], "sections": [{"heading": null, "text": "This year, it is more than ever before in the history of the city."}, {"heading": "II. RELATED WORK", "text": "Advances in autonomous vehicles in recent years have provided a portrait of a near future in which all vehicles will be controlled by artificially intelligent agents. This emerging technology requires an intelligent transportation system by redesigning the current transportation system to be used by human drivers. One of the interesting topics emerging in intelligent transportation systems is AIM. Dresner et al. have proposed a multi-agent control scheme in which vehicles communicate with interface management agents to reserve a dedicated spatio-temporal path at the intersection. In [6] Authors have proposed a self-organizing control system in which a cooperative multi-agent control scheme is used in addition to the autonomy of each vehicle. Authors have proposed a priority system to determine the drive through intersections based on the characteristics of the vehicles or intersection restrictions."}, {"heading": "III. REINFORCEMENT LEARNING", "text": "In this section, we will briefly review RL and introduce the notations used in the rest of the work. Figure 1 shows the agent environment model of RL. The \"agent\" interacts with the \"environment\" by applying \"actions\" that affect the state of the environment in future time steps and observe the state and \"reward\" in the next step resulting from the actions taken. \"Return\" is defined as the sum of all rewards that occur from the current step to the end of the current \"episode\": Gt = T \u2211 i = t ri are future rewards and T is the total number of steps in the episode. An \"episode\" is defined as a sequence of agent-environment interactions. In the last step of an episode, the control task is \"completed.\" Episode termination is defined specifically for the control task of the application. For example, in the cart pole balancing task, the agent of the controller, the environment is the physical command, the action is the system that is applied to violence."}, {"heading": "IV. PROBLEM STATEMENT", "text": "For the sake of simplicity, we assume that all initial vehicle positions and desired targets are located at the nodes. There is a control agent for the entire area. The task of the agent is to calculate the acceleration command for the vehicles in real time (see fig. 2). We assume that there are no silent or moving obstacles other than vehicles or road boundaries. Input to the agent is the real collision state of the vehicles, which consists of their positions and speeds. We assume that the vehicles are points and their anecular dynamics are ignored. However, collision avoidance is ignored in the formulation of the problem, we define a safe radius for each vehicle and no objects (vehicles or road boundaries) should be closer than the safe radii to the vehicles.Algorithm 1: High-Level Description of the Trust Region Optimization Data: See Actual System or Simulation Model."}, {"heading": "A. Solving the AIM problem using TRPO", "text": "The simulation model can be implemented on the basis of the RL formulation described in Section IV. To use TRPO, in addition to the simulation model, we need a parameterized stochastic policy, \u03c0\u03b8 (at | st). The policy should specify the probability distribution for each element of the action vector defined in (10) as a function of the current state. We have used the sequential representation of the politics of the deep neural network (DNN) as described in [5]. The input layer preserves the state that contains the position and speed of the vehicles (defined in (9)). There are a number of hidden layers followed by each tanh activation function. Finally, the output layer generates the mean value of a Gaussian distribution for each element of the action vector. To execute the optimal policy that TRPO has learned in each sampling period, the agent calculates the advance of the DNN using the current state of acceleration. As the next calculator, it assumes that all the actions of the action are taken by the agent."}, {"heading": "V. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Baseline Method", "text": "Therefore, we will use conventional optimization methods to investigate how close the proposed solution is to the optimal solution. Furthermore, we will see that conventional optimization is capable of solving the AIM problem only for very small problems, confirming that the proposed RL-based solution is a promising alternative to conventional methods.Theoretically, we can get the best solution to the problem defined in Section IV by reforming it as a conventional optimization problem. The following equations and inequalities describe the AIM optimization problem: at xi = argmaxat T \u2212 1 x = 0 x \u2212 0 x \u2212 dix \u2212 dix \u2212 dix \u2212 dix \u2212 dix \u2212 dix \u2212 dix \u2212 dix \u2212 dix) s. (14) see t. x \u2264 xit \u2264 xit \u2264 x (1 \u2264 i) elements (1 \u2264 i)."}, {"heading": "B. Simulation results", "text": "The implementation of the TRPO in the rllab library [4] is used to simulate the RL formulation of the AIM problem described in Section IV. For this purpose, the AIM state transition and the reward calculation are implemented as OpenAI Gyms. To speed up the simulation, normalized units are used for the physical properties of the environment rather than for the real-world quantities."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have shown that Deep RL can be a promising solution to the problem of intelligent intersection management in local road environments where the number of vehicles is limited and fine-grained acceleration control and motion planning can lead to more efficient navigation of autonomous vehicles. We proposed an RL environment definition in which collisions are avoided using a safety mechanism. Instead of heavy penalties for collisions in the reward function, the agent can learn the optimal policy more quickly and the learned policy can be applied in practice where the safety mechanism is actually implemented. Experiments show that conventional optimization methods cannot solve the problem with the magnitudes that are solvable by the proposed method. Similar to the learning process of human beings, the main advantage of the RL approach is that explicit mathematical modeling of the system is not required and, more importantly, that the demanding control design is eliminated for a high level of automotive safety systems, however, as the first step of a training system is required to develop a safety solution in most cases."}, {"heading": "VII. ACKNOWLEDGEMENT", "text": "We would like to thank Nvidia for their generous hardware donation, which was partially supported by the National Science Foundation under NSF grant number 1563652."}], "references": [{"title": "Autonomous intersection management: Multi-intersection optimization", "author": ["M. Hausknecht", "T.-C. Au", "P. Stone"], "venue": "2011 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2011, pp. 4581\u20134586.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A multiagent approach to autonomous intersection management", "author": ["K. Dresner", "P. Stone"], "venue": "Journal of artificial intelligence research, vol. 31, pp. 591\u2013656, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "A comparison of motion planning algorithms for cooperative collision avoidance of multiple cognitive automobiles", "author": ["C. Frese", "J. Beyerer"], "venue": "Intelligent Vehicles Symposium (IV), 2011 IEEE. IEEE, 2011, pp. 1156\u20131162.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-organizing control framework for driverless vehicles", "author": ["M.N. Mladenovi\u0107", "M.M. Abbas"], "venue": "16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013). IEEE, 2013, pp. 2076\u20132081.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Intersection management for autonomous vehicles using icacc", "author": ["I.H. Zohdy", "R.K. Kamalanathsharma", "H. Rakha"], "venue": "2012 15th International IEEE Conference on Intelligent Transportation Systems. IEEE, 2012, pp. 1109\u20131114.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A decentralized optimal control framework for connected and automated vehicles at urban intersections", "author": ["A.A. Malikopoulos", "C.G. Cassandras", "Y.J. Zhang"], "venue": "arXiv preprint arXiv:1602.03786, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Performance analysis of various activation functions in generalized mlp architectures of neural networks", "author": ["B. Karlik", "A.V. Olgac"], "venue": "International Journal of Artificial Intelligence and Expert Systems, vol. 1, no. 4, pp. 111\u2013122, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "The aimms outer approximation algorithm for minlp", "author": ["M. Hunting"], "venue": "Paragon Decision Technology, Haarlem, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Mixed integer programming for multi-vehicle path planning", "author": ["T. Schouwenaars", "B. De Moor", "E. Feron", "J. How"], "venue": "Control Conference (ECC), 2001 European. IEEE, 2001, pp. 2603\u20132608.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Openai gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": "2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In these works [1][2], high-level control of the vehicles is implemented such that the vehicles are self-contained agents that only communicate with the intersection management agent to reserve space-time slots in the intersection.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "In these works [1][2], high-level control of the vehicles is implemented such that the vehicles are self-contained agents that only communicate with the intersection management agent to reserve space-time slots in the intersection.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "problem since conventional control methods are not applicable because of the non-convex collision avoidance constraints [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Emerging Deep RL methods [4] that leverage the deep neural networks to automatically extract features seem like promising solutions to shortcomings of the classical RL methods.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "(2) Employing TRPO proposed in [5] to solve the formulated AIM problem.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "have proposed a multi-agent AIM system in which vehicles communicate with intersection management agents to reserve a dedicated spatio-temporal trajectory at the intersection [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "In [6], authors have proposed a self-organizing control framework in which a cooperative multi-agent control scheme is employed in addition to each vehicle\u2019s autonomy.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "presented an approach in which the Cooperative Adaptive Cruise Control (CACC) systems are leveraged to minimize delays and prevent clashes [7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 7, "context": "A decentralized optimal control formulation is proposed in [8] in which the acceleration/deceleration of the vehicles are minimized subject to collision avoidance constraints.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "extended the approach proposed in [2] to multiintersection settings via dynamic traffic assignment and dynamic lane reversal [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "extended the approach proposed in [2] to multiintersection settings via dynamic traffic assignment and dynamic lane reversal [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 8, "context": "Therefore, we model the system as a Markov Decision Process (MDP) assuming that the environment has the Markov property [9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "In the second category, which is called policy optimization and has been successfully applied to large-scale and continuous control systems [4], the policy is parameterized directly as \u03c0(at|st) and the parameter vector of the optimal policy \u03b8 is estimated.", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "The Trust Region Policy Method (TRPO) [5] is an example of the second category of methods that guarantees monotonic policy improvement and is designed to be scalable to large-scale settings.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "We have used the sequential deep neural network (DNN) policy representation as described in [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "There are a number of hidden layers, each followed by tanh activation functions [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "AOA[11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "To overcome this issue, we should reformulate the optimization problem using 1-norm and introduce new integer variables for the distance between vehicles using the ideas proposed in [12].", "startOffset": 182, "endOffset": 186}, {"referenceID": 3, "context": "The implementation of the TRPO in rllab library [4] is used to simulate the RL formulation of the AIM problem described in Section IV.", "startOffset": 48, "endOffset": 51}, {"referenceID": 12, "context": "For this purpose, the AIM state transition and reward calculation are implemented as an OpenAI Gym [13] environment.", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "Recent advances in combining deep learning and Reinforcement Learning have shown a promising path for designing new control agents that can learn optimal policies for challenging control tasks. These new methods address the main limitations of conventional Reinforcement Learning methods such as customized feature engineering and small action/state space dimension requirements. In this paper, we leverage one of the state-of-the-art Reinforcement Learning methods, known as Trust Region Policy Optimization, to tackle intersection management for autonomous vehicles. We show that using this method, we can perform fine-grained acceleration control of autonomous vehicles in a grid street plan to achieve a global design objective.", "creator": "LaTeX with hyperref package"}}}