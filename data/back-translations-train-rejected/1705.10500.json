{"id": "1705.10500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Exploiting Restricted Boltzmann Machines and Deep Belief Networks in Compressed Sensing", "abstract": "This paper proposes a CS scheme that exploits the representational power of restricted Boltzmann machines and deep learning architectures to model the prior distribution of the sparsity pattern of signals belonging to the same class. The determined probability distribution is then used in a maximum a posteriori (MAP) approach for the reconstruction. The parameters of the prior distribution are learned from training data. The motivation behind this approach is to model the higher-order statistical dependencies between the coefficients of the sparse representation, with the final goal of improving the reconstruction. The performance of the proposed method is validated on the Berkeley Segmentation Dataset and the MNIST Database of handwritten digits.", "histories": [["v1", "Tue, 30 May 2017 08:11:05 GMT  (5060kb)", "http://arxiv.org/abs/1705.10500v1", "Accepted for publication at IEEE Transactions on Signal Processing"]], "COMMENTS": "Accepted for publication at IEEE Transactions on Signal Processing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luisa f polania", "kenneth e barner"], "accepted": false, "id": "1705.10500"}, "pdf": {"name": "1705.10500.pdf", "metadata": {"source": "CRF", "title": "Exploiting Restricted Boltzmann Machines and Deep Belief Networks in Compressed Sensing", "authors": ["Luisa F. Polan\u0131\u0301a"], "emails": ["lpolania@amfam.com).", "barner@udel.edu)."], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Compressed sensing", "text": "Thus, the signal x can be approximated by a linear combination of a small number of column vectors from D, i.e. x = Ds + r, where s is the sparse vector of the weighting coefficients, r is the representation error, andK \u00b2 n. The support of s is called \u03b8 and is related to the column pattern S, which is defined as Si = 1\u03b8 (i) for i = 1..., N, where 1\u03b8 (i) assumes the value of one if i economics and zero elsewhere. It is an M \u00d7 N sampling matrix, M < N. Compressed sampling [1], [2] addresses the restoration of x from linear measurements of the shape y = \u03a6ns matrix. Compressed sampling values show that the signal x can be reconstructed from the matrix if the matrix is small, [D] meets a condition, then the condition for SUBIS-Istometry ends with the constants."}, {"heading": "B. Deep learning architectures", "text": "The deeper learning objectives of an RBM form a bipartite diagram as represented in the composition of the subordinate units. (17) The deeper learning processes are defined by the composition of the subordinate units (17). (17) The deeper learning processes are inspired by biological structures in human brain mechanisms to process natural signals. (17) The deeper learning architecture, the DBN [30], is composed in this section of a single visible layer and a single hidden layer. The visible units, v = [v1v2]. vJ] T, represent the input data whose probability distribution must be modeled. The hidden units, h = [h2] T, are trained to capture high-order data correlations observed on the visible units. Symmetrical connections between the layers are represented by a weight matrixW."}, {"heading": "III. PROPOSED COMPRESSED SENSING SCHEME", "text": "The proposed CS reconstruction algorithm uses the previous one determined in an MAP approach. In this section, the CS and training phases are described in detail. The training phase varies depending on the frugal transformation used, either orthonormal basics or over-fully learned dictionaries. Block diagrams of the proposed CS schemes for over-complete dictionaries and orthonormal basics are shown in Fig. 2 and 3 respectively."}, {"heading": "A. Compressed sensing stage", "text": "In this paper, we look at the traditional synthesis-based CS approach, which aims to reconstruct the sparse representation of a signal x from studied and noisy measurements of the form y = x + n, which are the sampling matrix and n accounts for the additive Gaussian distribution. Vector y can also be written: Vector y can be written + n. (6) Let us leave the sampling matrix and n accounts of the additive Gaussian sampling distribution. (6) Let us leave the sampling matrix + n and n constellation of the formatting D, then Vector y takes the formatting. (7) As both r and n are the Gaussian distributions. (8) Let us leave the sampling matrix and n accounts. (6) Let us leave the sampling matrix and n constellation. (8) Let us leave the sampling matrix and n constellation."}, {"heading": "B. Training stage using Overcomplete Learned Dictionaries", "text": "In this section, the sparsifying transformation D of (6) is assumed to be an over-complete conceptual pattern. We learn D from a set of training data belonging to the same class, such as signal x. the resulting sparse metrics and the illustrative error are used to estimate the model parameters defining p-dimensional training, known as training dataset I. A methodology for constructing the over-complex dictatorial dictatorial system D = 1. D \u00b7 J \u00b7 J \u00b7 B) is the solution to the following optimization problem, which is called the training dataset. A method for constructing the over-complex dictatorial system D = 1. D \u00b7 J \u00b7 J \u00b7 J \u00b7 J) is to solve the following optimization problem. D, A \u00b7 DA \u00b7 2F s."}, {"heading": "C. Training stage using Orthonormal bases", "text": "As in section III-B1, G = [G \u00b7 1. G \u00b7 B], i.e. A \u00b7 x = U \u00b7 x = U \u00b7 x = U \u00b7 x = U \u00b7 x = U \u00b7 x = U \u00b7 x = U \u00b7 U \u00b7 U \u00b7 U \u00b7 U \u00b7 U \u00b7 U = U \u00b7 U \u00b7 U \u00b7 U = U \u00b7 U \u00b7 U \u00b7 U = U \u00b7 U \u00b7 U \u00b7 U = U \u00b7 U \u00b7 U \u00b7 U \u00b7 U = U \u00b7 U \u00b7 U \u00b7 U \u00b7 U = U \u00b7 U \u00b7 U \u00b7 U = U \u00b7 U \u00b7 U \u00b7 S = U \u00b7 S = U \u00b7 S = U \u00b7 J = U \u00b7 J. In this section, D does not stand for a complete dictionary, but for an orthonomic basis. Each vector G \u00b7 J can be expressed as G \u00b7 J = DA \u00b7 J \u00b7 J, with A \u00b7 J being the representation of signal G \u00b7 J in the D domain. Therefore, the signal G \u00b7 J can be modelled in the D domain. Unlike A \u00b7 J, A \u00b7 J indicates the best domain A \u00b7 J is the J-K coximation, with the J in the other domain, while the J is the J-K coefficient in D."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "To validate the proposed compressed sampling schemes, a series of experiments with synthetic and real signals is carried out, the results are presented for an average of 50 repetitions of each experiment, with the measurement matrix realized differently with each iteration. Reconstruction SNR (R-SNR) and peak SNR (PSNR) are used as performance measures for one-dimensional signals or images, respectively. Reconstruction SNR is defined as R-SNR (dB) = 10log10 R2MSE, (25) where R is the maximum possible pixel value of the image (255 for 8-bit images) and MSE is defined as N-dimensional original or reconstructed signals. PSNR is defined as PSNR (dB) = 10log10 R2MSE, (25) where R represents the best possible pixel value of the image (255 for 8-bit images) and MSE is defined as the mean quadratic error = M2SE-1F."}, {"heading": "A. Experiments with synthetic signals", "text": "The motivation of these experiments is to avoid errors in the parameter estimation that takes place during the training in order to multiply in the reconstruction algorithm and thus achieve a more accurate evaluation of the bias. In the first experiment, a set of 50,000 presetting patterns is generated from an RBM whose parameters are known by Gibbs sampling. From this set, 45000 and 5000 presetting patterns are randomly selected for training and testing. The number of hidden units is equal to the number of visible units from an RBM whose parameters are known, using the presetting patterns and the magnitude of each nonzero coefficient are drawn from a Gaussian distribution with zero mean and known deviations. Weights and hidden presets are drawn from a uniform distribution."}, {"heading": "B. Experiments with the MNIST Database", "text": "This year it is more than ever before."}, {"heading": "C. Experiments with the Berkeley segmentation dataset", "text": "It is not only a matter of time before there will be a solution, but also of how there will be a solution, how there will be a solution, \"he said.\" It is a matter of time before there will be a solution. \"Also, the question of whether there will be a solution has not yet been answered:\" It is a matter of time before there will be a solution. \"The question of whether there will be a solution has not yet been answered:\" It is a matter of time before there will be a solution. \"The question is only how there will be a solution:\" It is a matter of time before there will be a solution. \"The question of whether there will be a solution,\" how there will be a solution, \"is not yet clear.\""}, {"heading": "V. CONCLUSIONS", "text": "This paper demonstrated how the ability of a deep-learning architecture, the deep-faith network, and limited Boltzmann machines to capture the complex statistical structure of input data can be utilized by CS systems. Statistical dependencies are informative and lead to improvements in reconstruction performance. The proposed scheme works on signals belonging to a particular signal class. In this paper, the signal classes of natural images from the Berkeley segmentation data set and handwritten digits were selected from the MNIST database, but the scheme can also be applied to other signal classes, such as radar, EEG, EEG, medical imaging, speech signals, etc. Restricted Boltzmann machines and DBNs were used to model a prior distribution for the frugality pattern of the signal class. Such a scheme was used by an MAP estimator for reconstruction."}], "references": [{"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289\u20131306, Sept. 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to compressive sampling", "author": ["E.J. Cand\u00e8s", "M.B. Wakin"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21\u201330, Mar. 2008.  13", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Structured compressed sensing: From theory to applications", "author": ["M.F. Duarte", "Y.C. Eldar"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 9, pp. 4053\u20134085, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Block-sparse signals: Uncertainty relations and efficient recovery", "author": ["Y.C. Eldar", "P. Kuppinger", "H. Bolcskei"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 6, pp. 3042\u20133054, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian compressive sensing", "author": ["S. Ji", "Y. Xue", "L. Carin"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 6, pp. 2346\u20132356, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressed sensing and Bayesian experimental design", "author": ["M.W. Seeger", "H. Nickisch"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 912\u2013919.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative algorithms for compressed sensing with partially known support", "author": ["R.E. Carrillo", "L.F. Polania", "K.E. Barner"], "venue": "Proc., IEEE ICASSP, Dallas, TX, Mar. 2010, pp. 3654 \u20133657.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressed sensing and redundant dictionaries", "author": ["H. Rauhut", "K. Schnass", "P. Vandergheynst"], "venue": "IEEE Transactions on Information Theory, vol. 54, no. 5, pp. 2210\u20132219, May 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimized projections for compressed sensing", "author": ["M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 55, no. 12, pp. 5695\u20135702, Dec 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to sense sparse signals: Simultaneous sensing matrix and sparsifying dictionary optimization", "author": ["J.M. Duarte-Carvajalino", "G. Sapiro"], "venue": "IEEE Transactions on Image Processing, vol. 18, no. 7, pp. 1395\u20131408, July 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated diffusion spectrum imaging with compressed sensing using adaptive dictionaries", "author": ["B. Bilgic", "K. Setsompop", "J. Cohen-Adad", "A. Yendiki", "L.L. Wald", "E. Adalsteinsson"], "venue": "Magnetic Resonance in Medicine, vol. 68, no. 6, pp. 1747\u20131754, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 791\u2013804, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric Bayesian dictionary learning for analysis of noisy and incomplete images", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "L. Li", "Z. Xing", "D. Dunson", "G. Sapiro", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 130\u2013144, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting statistical dependencies in sparse representations for signal recovery", "author": ["T. Peleg", "Y.C. Eldar", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 5, pp. 2286\u20132303, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Representational power of restricted Boltzmann machines and deep belief networks", "author": ["N. Le Roux", "Y. Bengio"], "venue": "Neural Computation, vol. 20, no. 6, pp. 1631\u20131649, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep, narrow sigmoid belief networks are universal approximators", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "Neural Computation, vol. 20, no. 11, pp. 2629\u20132636, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., vol. 2, no. 1, pp. 1\u2013127, Jan. 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse signal recovery using Markov random fields", "author": ["V. Cevher", "M.F. Duarte", "C. Hegde", "R. Baraniuk"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 257\u2013264.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning horizontal connections in a sparse coding model of natural images", "author": ["P. Garrigues", "B.A. Olshausen"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 505\u2013512.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Boltzmann machine and mean-field approximation for structured sparse decompositions", "author": ["A. Dremeau", "C. Herzet", "L. Daudet"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 7, pp. 3425\u20133438, July 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "A statistical prediction model based on sparse representations for single image super-resolution", "author": ["T. Peleg", "M. Elad"], "venue": "IEEE transactions on image processing, vol. 23, no. 6, pp. 2569\u20132582, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximate message passing with restricted Boltzmann machine priors", "author": ["E.W. Tramel", "A. Dr\u00e9meau", "F. Krzakala"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 2016, no. 7, pp. 073401, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Approximate message passing algorithms for compressed sensing", "author": ["M.A. Maleki", "D. Donoho", "R. Gray", "A. Montanari"], "venue": "Stanford University,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Sparse approximation based on a random overcomplete basis", "author": ["Y. Nakanishi-Ohno", "T. Obuchi", "M. Okada", "Y. Kabashima"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 2016, no. 6, pp. 063302, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "An lp-based reconstruction algorithm for compressed sensing radar imaging", "author": ["L. Zheng", "A. Maleki", "Q. Liu", "X. Wang", "X. Yang"], "venue": "IEEE Radar Conference, 2016, pp. 1\u20135.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 643\u2013669, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Markov chain Monte Carlo maximum likelihood", "author": ["C.J. Geyer"], "venue": "1991.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning with blocks: Composite likelihood and contrastive divergence", "author": ["A.U. Asuncion", "Q. Liu", "A.T. Ihler", "P. Smyth"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 33\u201340.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Near-optimal signal recovery from random projections: Universal encoding strategies", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 12, pp. 5406\u20135425, Dec 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, July 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Comput., vol. 14, no. 8, pp. 1771\u20131800, Aug. 2002.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "AISTATS, 2009, vol. 1, p. 3.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Variational bayesian algorithm for quantized compressed sensing", "author": ["Z. Yang", "L. Xie", "C. Zhang"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 11, pp. 2815\u20132824, June 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Cram\u00e9r-Rao-type bounds for sparse Bayesian learning", "author": ["R. Prasad", "C.R. Murthy"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 3, pp. 622\u2013632, Feb 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning horizontal connections in a sparse coding model of natural images", "author": ["P. Garrigues", "B.A. Olshausen"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 505\u2013512.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A Bruckstein"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311\u20134322, Nov 2006.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "CS Technion, vol. 40, no. 8, pp. 1\u201315, 2008.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Joint segmentation of piecewise constant autoregressive processes by using a hierarchical model and a Bayesian sampling approach", "author": ["N. Dobigeon", "J. Tourneret", "M. Davy"], "venue": "IEEE Transactions on Signal Processing, vol. 55, no. 4, pp. 1251\u20131263, 2007.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised linear spectral unmixing using a hierarchical Bayesian model for hyperspectral imagery", "author": ["N. Dobigeon", "J. Tourneret", "C. Chang"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 2684\u20132695, 2008.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian fusion of multi-band images", "author": ["Q. Wei", "N. Dobigeon", "J. Tourneret"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 9, no. 6, pp. 1117\u20131127, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM Journal on Scientific Computing, vol. 20, no. 1, pp. 33\u201361, Dec. 1998.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "Eighth IEEE International Conference on Computer Vision. IEEE, 2001, vol. 2, pp. 416\u2013423.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "COMPRESSED sensing has become an extensive research area due to its potential to perfectly reconstruct sparse signals from a small set of nonadaptive linear measurements in the form of random projections [1], [2].", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "COMPRESSED sensing has become an extensive research area due to its potential to perfectly reconstruct sparse signals from a small set of nonadaptive linear measurements in the form of random projections [1], [2].", "startOffset": 209, "endOffset": 212}, {"referenceID": 2, "context": "In last decade, the area of CS has extended to new applications that require structured signal models that go beyond the simplistic sparsity model [3]\u2013[7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "In last decade, the area of CS has extended to new applications that require structured signal models that go beyond the simplistic sparsity model [3]\u2013[7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 2, "context": "Examples of deterministic models include the wavelet tree model, which assumes that the non-zero signal coefficients lie in a rooted and connected tree structure, and the block-sparsity model, which assumes that the non-zero signal coefficients form clusters [3], [4].", "startOffset": 259, "endOffset": 262}, {"referenceID": 3, "context": "Examples of deterministic models include the wavelet tree model, which assumes that the non-zero signal coefficients lie in a rooted and connected tree structure, and the block-sparsity model, which assumes that the non-zero signal coefficients form clusters [3], [4].", "startOffset": 264, "endOffset": 267}, {"referenceID": 4, "context": "Instead of imposing an explicit structure of the coefficients, statistical approaches usually impose a prior belief about the signal of interest in terms of a sparseness prior [5], [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 5, "context": "Instead of imposing an explicit structure of the coefficients, statistical approaches usually impose a prior belief about the signal of interest in terms of a sparseness prior [5], [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "Even though the bulk of CS theory has been developed for signals that have a sparse representation in an orthonormal basis, efforts have been made to extend CS theory to signals that are sparse with respect to an overcomplete dictionary [8]\u2013 [10].", "startOffset": 237, "endOffset": 240}, {"referenceID": 9, "context": "Even though the bulk of CS theory has been developed for signals that have a sparse representation in an orthonormal basis, efforts have been made to extend CS theory to signals that are sparse with respect to an overcomplete dictionary [8]\u2013 [10].", "startOffset": 242, "endOffset": 246}, {"referenceID": 0, "context": "The coherence between the columns of an overcomplete dictionary poses some limitations in extending the CS theory to sparse overcomplete representations [1], [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "The coherence between the columns of an overcomplete dictionary poses some limitations in extending the CS theory to sparse overcomplete representations [1], [8].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "[8] showed that CS is viable in the context of signals that are sparse in an overcomplete dictionary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Therefore, unlike most of the works that use overcomplete learned dictionaries in CS problems [11]\u2013[13], which only use the training stage to learn the dictionary and disregard the sparse codes associated with such a dictionary, the proposed approach exploits both dictionary and sparse codes from the training stage to improve CS reconstruction algorithm performance.", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "Therefore, unlike most of the works that use overcomplete learned dictionaries in CS problems [11]\u2013[13], which only use the training stage to learn the dictionary and disregard the sparse codes associated with such a dictionary, the proposed approach exploits both dictionary and sparse codes from the training stage to improve CS reconstruction algorithm performance.", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "To overcome this limitation, we propose a greedy approach realized by modifying the orthogonal matching pursuit\u2013based algorithm proposed in [14] to maximize the posterior distribution of the sparsity pattern.", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "First, they possess tremendous representational power; second, inference and parameter learning can be efficiently achieved using contrastive divergence and greedy layer\u2013wise training [15]\u2013 [17].", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "First, they possess tremendous representational power; second, inference and parameter learning can be efficiently achieved using contrastive divergence and greedy layer\u2013wise training [15]\u2013 [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "[15] showed that an RBM can model any discrete distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] showed that deep belief networks can approximate any distribution over binary vectors to an arbitrary level of accuracy, even when the width of each layer is limited to the dimensionality of the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "growing field in artificial intelligence [17].", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "Restricted Boltzmann machines have been employed to model the dependencies between low resolution and high resolution patches in the image super\u2013resolution problem [21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "[22] also uses RBMs to model the sparsity pattern of signals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "However, it has been shown that AMP algorithms are very sensitive to parameter tuning [23].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "Regardless of their simplicity and ease of implementation, OMP-based algorithms outperform AMP algorithms in some cases [24], [25], specially when the non-zero coefficients of sparse signals differ in magnitude [25].", "startOffset": 120, "endOffset": 124}, {"referenceID": 24, "context": "Regardless of their simplicity and ease of implementation, OMP-based algorithms outperform AMP algorithms in some cases [24], [25], specially when the non-zero coefficients of sparse signals differ in magnitude [25].", "startOffset": 126, "endOffset": 130}, {"referenceID": 24, "context": "Regardless of their simplicity and ease of implementation, OMP-based algorithms outperform AMP algorithms in some cases [24], [25], specially when the non-zero coefficients of sparse signals differ in magnitude [25].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In practice, pseudo\u2013likelihood learning has a high computational overhead compared to contrastive divergence [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "Pseudo\u2013likelihood learning does not approximate the maximum likelihood estimator well, except in the limit of zero dependence [27].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "It was shown that contrastive divergence is equivalent to pseudo\u2013likelihood for fully visible Boltzmann machines if single\u2013step Gibbs sampling is employed and outperforms pseudo\u2013likelihood when the number of sampling steps is larger than one [28].", "startOffset": 242, "endOffset": 246}, {"referenceID": 0, "context": "Compressed sensing [1], [2] addresses the recovery of x from linear measurements of the form y = \u03a6x \u2248 \u03a6Ds.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Compressed sensing [1], [2] addresses the recovery of x from linear measurements of the form y = \u03a6x \u2248 \u03a6Ds.", "startOffset": 24, "endOffset": 27}, {"referenceID": 28, "context": "Compressed sensing results show that the signal x can be reconstructed from y if the matrix \u039e = \u03a6D satisfies a condition, known as the restricted isometry property (RIP) [29], with a sufficiently small restricted isometry constant.", "startOffset": 170, "endOffset": 174}, {"referenceID": 7, "context": "However, if the matrix D is overcomplete, the coherence between its columns makes it difficult for the matrix \u039e to satisfy the RIP with a sufficiently small restricted isometry constant [8].", "startOffset": 186, "endOffset": 189}, {"referenceID": 8, "context": "Several works have addressed this limitation by designing the sampling matrix \u039e so as to minimize the mutual coherence of the effective dictionary \u039e [9], [10].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Several works have addressed this limitation by designing the sampling matrix \u039e so as to minimize the mutual coherence of the effective dictionary \u039e [9], [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "Deep learning aims at learning hierarchical feature representations with higher level features formed by the composition of lower level features [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "A deep learning architecture, the DBN [30], is presented in this section.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "(b) Schematic of a deep belief network of one visible and three hidden layers (adapted from [32]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "Given that computing the exact gradient of the log\u2013likelihood is intractable, the contrastive divergence approximation [31] is typically employed.", "startOffset": 119, "endOffset": 123}, {"referenceID": 29, "context": "(5) The log-probability of the training data can be improved by adding layers to the network, which, in turn, increases the true representational power of the network [30].", "startOffset": 167, "endOffset": 171}, {"referenceID": 29, "context": "[30] was a greedy, layer\u2013wise unsupervised learning algorithm that allows efficient training of DBNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We adopt the commonly used assumption that the sampling noise variance \u03c3 n is known [14], [33], [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 32, "context": "We adopt the commonly used assumption that the sampling noise variance \u03c3 n is known [14], [33], [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "We adopt the commonly used assumption that the sampling noise variance \u03c3 n is known [14], [33], [34].", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "The approach proposed in [14] is adopted in this paper, namely first calculating the MAP estimator of the sparsity pattern and then calculating the MAP estimator of the sparse vector.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "employed in [14], [35] for nonzero sparse coefficients.", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "employed in [14], [35] for nonzero sparse coefficients.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "Using RBMs to model the sparsity pattern probability distribution is justified by results that show that an RBM can model any discrete distribution and that adding hidden units yields strictly enhanced modeling performance, unless the RBM already perfectly models the data [15].", "startOffset": 273, "endOffset": 277}, {"referenceID": 15, "context": "Similarly, the use of deep belief networks is justified by their capabilities to approximate any distribution over binary vectors to an arbitrary level of accuracy, even when the width of each layer is limited to the dimensionality of the data [16].", "startOffset": 244, "endOffset": 248}, {"referenceID": 14, "context": "However, DBNs offer an additional advantage over RBMs: they yield more efficient and compact representations in terms of the number of parameters [15].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "[14] proposed a greedy pursuit algorithm based on Orthogonal Matching Pursuit (OMP) to approximate the MAP estimator of a sparse representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The same algorithm is used here for reconstructing the sparse representation s, although using a different posterior distribution than that in [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 14, "context": "5) Selection of the probabilistic generative model: Even though DBNs and RBMs have the same representational power [15], they differ in the number of parameters needed to model the probability distribution of the data.", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Indeed, in [17], it was shown that deep architectures can sometimes be exponentially more efficient than shallow ones in terms of number of parameters needed to represent a function.", "startOffset": 11, "endOffset": 15}, {"referenceID": 35, "context": "[36] is employed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] show that the dominant operations in K-SVD are sparse-coding, atom updates and coefficients updates, wich leads to a total computational complexity of O(B(K3 + 2KNJ + 4NK + 4KJ) + 5NJ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "2) Sampling matrix design: Even though an overcomplete dictionary offers more flexibility and leads to sparser representations compared to an orthonormal basis, the coherence between its columns limits the performance of CS recovery algorithms when using the traditional sub\u2013Gaussian sensing matrix [8]\u2013[10].", "startOffset": 299, "endOffset": 302}, {"referenceID": 9, "context": "2) Sampling matrix design: Even though an overcomplete dictionary offers more flexibility and leads to sparser representations compared to an orthonormal basis, the coherence between its columns limits the performance of CS recovery algorithms when using the traditional sub\u2013Gaussian sensing matrix [8]\u2013[10].", "startOffset": 303, "endOffset": 307}, {"referenceID": 8, "context": "For example, Elad [9] optimizes the sampling matrix to minimize the coherence of the effective dictionary.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "[10] also aims at minimizing the coherence of the effective dictionary, with their approach designed to make the columns of the effective dictionary as orthogonal as possible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "For more details on contrastive divergence, the reader is referred to [31].", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "Hinton presented a powerful greedy layer\u2013wise method to learn these parameters [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Additionally, the set of sparse codes A can be employed to estimate the variance \u03c3 si of each ith element of the sparse representation s of signal x (see [14]):", "startOffset": 154, "endOffset": 158}, {"referenceID": 37, "context": "In [38]\u2013[40], a number of samples equal to 500 leads to a good approximation of the empirical average using samplingbased methods.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [38]\u2013[40], a number of samples equal to 500 leads to a good approximation of the empirical average using samplingbased methods.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "Synthetic signals of dimension N = 256 are formed with the sparsity patterns and the magnitude of each nonzero coefficient is drawn from a Gaussian distribution with zero mean and known variance \u03c3 si \u2208 [10, 50].", "startOffset": 202, "endOffset": 210}, {"referenceID": 9, "context": "The sampling noise variance and the variance of the nonzero coefficients are kept the same as in the previous experiment, \u03c3 n = 1 and \u03c3 2 si \u2208 [10, 50], \u2200i, respectively.", "startOffset": 143, "endOffset": 151}, {"referenceID": 40, "context": "The MNIST dataset [41], which contains 70000 grayscale images of handwritten digits of size N = 28 \u00d7 28, is employed for the experiments.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "also evaluated their RBM-based reconstruction method using the MNIST database [22].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "However, this is not a fair comparison since the authors of [22] selected different values for the model and experiment parameters.", "startOffset": 60, "endOffset": 64}, {"referenceID": 41, "context": "Figures 8 compares the performance of the proposed algorithms on the testing dataset, using the model configurations chosen via cross-validation, with CS reconstruction algorithms, such as OMP and basis pursuit denoising (BPDN) [42].", "startOffset": 228, "endOffset": 232}, {"referenceID": 13, "context": "sented in [14] (henceforth referred to as FV\u2013OMP-like algorithm), which has the same structure as the RBM\u2013OMP\u2013 like algorithm, with the difference being that it employs a fully visible Boltzmann machine to model the probability distribution of the sparsity pattern.", "startOffset": 10, "endOffset": 14}, {"referenceID": 42, "context": "This section presents experimental validation of the proposed algorithms using real data from the Berkeley segmentation dataset [43].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "In the case of overcomplete learned dictionaries, the sensing matrix is optimized as described in [10] and the number of dictionary atoms is set to 2N .", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Such parameters are learned from the training dataset using the maximum likelihood approach described in [14].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "[15], which shows that, even though a DBN and an RBM can have the 0.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "This paper proposes a CS scheme that exploits the representational power of restricted Boltzmann machines and deep learning architectures to model the prior distribution of the sparsity pattern of signals belonging to the same class. The determined probability distribution is then used in a maximum a posteriori (MAP) approach for the reconstruction. The parameters of the prior distribution are learned from training data. The motivation behind this approach is to model the higher\u2013order statistical dependencies between the coefficients of the sparse representation, with the final goal of improving the reconstruction. The performance of the proposed method is validated on the Berkeley Segmentation Dataset and the MNIST Database of handwritten digits.", "creator": "LaTeX with hyperref package"}}}