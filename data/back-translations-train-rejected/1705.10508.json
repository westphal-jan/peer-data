{"id": "1705.10508", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Implications of Decentralized Q-learning Resource Allocation in Wireless Networks", "abstract": "Reinforcement Learning is gaining attention by the wireless networking community due to its potential to learn good-performing configurations only from the observed results. In this work we propose a stateless variation of Q-learning, which we apply to exploit spatial reuse in a wireless network. In particular, we allow networks to modify both their transmission power and the channel used solely based on the experienced throughput. We concentrate in a completely decentralized scenario in which no information about neighbouring nodes is available to the learners. Our results show that although the algorithm is able to find the best-performing actions to enhance aggregate throughput, there is high variability in the throughput experienced by the individual networks. We identify the cause of this variability as the adversarial setting of our setup, in which the most played actions provide intermittent good/poor performance depending on the neighbouring decisions. We also evaluate the effect of the intrinsic learning parameters of the algorithm on this variability.", "histories": [["v1", "Tue, 30 May 2017 08:41:53 GMT  (644kb)", "https://arxiv.org/abs/1705.10508v1", "Conference"], ["v2", "Tue, 29 Aug 2017 09:13:51 GMT  (645kb)", "http://arxiv.org/abs/1705.10508v2", "Conference"]], "COMMENTS": "Conference", "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["francesc wilhelmi", "boris bellalta", "cristina cano", "ers jonsson"], "accepted": false, "id": "1705.10508"}, "pdf": {"name": "1705.10508.pdf", "metadata": {"source": "CRF", "title": "Implications of Decentralized Q-learning Resource Allocation in Wireless Networks", "authors": ["Francesc Wilhelmi", "Boris Bellalta", "Cristina Cano", "Anders Jonsson"], "emails": [], "sections": [{"heading": null, "text": "In recent years it has been shown that the problem is not only a problem, but also a problem that we have to solve. (...) It is not that we can solve it. (...) It is not that we can solve it. (...) It is not that we can solve it. (...) It is not that we can solve it. (...) It is not that we can create it. (...) It is not that we can create it. (...) It is not that we can create it. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...).). (...).). (...).). (...).). (...).). (...). (...).).). (...). (...).).). (...). (...).).). (...).).). (...).). (...).).). (...). (...).). (...). (...).).). (...). (...). (...).). (...).). (...).).).)...)...................)....................................................................."}, {"heading": "II. SYSTEM MODEL", "text": "For the rest of this work, we consider a scenario in which multiple WNs are placed in a 3D map (with parameters described later in Section IV-A), each of which is formed by an access point (AP) that is downlinked to a single station (STA)."}, {"heading": "A. Channel modelling", "text": "The displacement loss between WN i and j is determined by PLi, j = Ptx, i \u2212 Prx, j = = PL0 + 10\u03b1PL log10 (di, j) + Gs + di, j dobs Go, where Ptx, i is the transmitting power in dBm by WN i, Prx, j is the power in dBm received in dB, PL0 is the displacement loss at one meter in dB, \u03b1PL is the displacement loss exponent, di, j is the distance between transmitter and receiver in meters, Gsis is the shadow loss in dB and Go the obstacle loss in dB. Note that we include the factor dobs, i.e. the distance between two obstacles in meters."}, {"heading": "B. Throughput calculation", "text": "By using the received power and interference, we calculate the maximum theoretical throughput of each WN i at a given time t, {1, 2...} using the Shannon capacitance.\u0435i, t = B log2 (1 + SINRi, t), where B is the channel bandwidth and the received signal to interference plus noise ratio (SINR) by: SINRi, t = Pi, tIi, t + N, where Pi, t and Ii, t is the received power and the sum of the interferences to WN i at a given time or N. For each STA in a WN, the interference is considered to be the total power received by all APs of the other coexisting WNs as if they were transmitted continuously."}, {"heading": "III. DECENTRALIZED STATELESS Q-LEARNING FOR ENHANCING SPATIAL REUSE IN WNS", "text": "Q-Learning [6, 7] is an RL technique that enables an agent to learn the optimal policy to follow in a particular environment. Q-Learning [6, 7] is a method that enables an agent to learn the optimal policy (Q-Q =). Q-Learning is a set of possible states that describe the environment and the actions defined in this model. In particular, an agent maintains an estimate of the expected long-term and discounted reward for each state action pair and selects measures with the aim of maximizing it. The expected cumulative reward is given by the following measures: V\u03c0 (s) = lim N \u2192 \u221e E (N). A discount factor (s) is used. The optimal policy is applied that maximizes the overall expected reward by giving the optimum of the equation [6]: Q-Learning Equation (s, a)."}, {"heading": "IV. PERFORMANCE EVALUATION", "text": "In this section we present the simulation parameters and describe the experiments. 2 Then we show the main results. 1We point out that local information such as the observed instant channel quality could be incorporated into the state definition. However, such a description of the system entails increased complexity. 2The code used for simulations can be found at https: / / github.com / wn-upf / Decentralized Qlearning resource allocation in WNs.git (Commit: eb4042a1830c8ea30b7eae3d72a51afe765a8d86). Algorithm 1: Stateless Q-Learning1 function Stateless Q-Learning (SINR, A); Input: SINR: Signal-to-Interference-plus-Noise Ratiosensed at at at at at the STAA: set of actions possible in {1..., K} Output: Mean throughput in the experience} Q-Learning (SINR, A); Input: Signal-to-Interference-plus-plus-Noise Ratiosensed at the STAA-Q-Q-to-Q-experienced (STAA-Q-to-Q-1)."}, {"heading": "A. Simulation Parameters", "text": "According to [8], a typical high density scenario for residential buildings contains 0.0033 APs / m3. Then we consider a map scenario measuring 10 x 5 x 10 m, which contains 4 WNs that form a grid topology in which STAs are placed at the greatest possible distance from the other networks. This toy scenario allows us to investigate the performance of stateless Q-Learning in a controlled environment, which is useful to verify the applicability of RL in WNs only using local information 3. We assume that the number of channels corresponds to half the number of coexisting WNs, so that we can investigate a challenging situation in terms of spatial reuse. Table I describes the parameters used."}, {"heading": "B. Optimal solution", "text": "We first identify the optimal solutions that maximize the rule of quantitative adjustment with respect to network throughput, and ii) proportional fairness, which is calculated as the logarithmic sum of the throughput experienced by each WN. Note that since the scenario under consideration is symmetrical, there are two equivalent solutions. Note also that in the order of maximizing the aggregate network throughput, two of the WNs sacrifice themselves by choosing a lower transmitting power. Input Parameters AnalysisWe first analyze the effects of the modifying \u03b1 rate (the discount factor) and the discount factor 0 (the initial exploration coefficient of quantitative updating) in relation to the network reached."}, {"heading": "V. CONCLUSIONS", "text": "Decentralized Q-Learning can be used to improve spatial reuse in dense wireless networks and increase performance by leveraging the most rewarding measures. In this article, we have shown, using a toy scenario, that stateless Qlearning enables in particular the discovery of powerful configurations that achieve near-optimal (in terms of throughput maximization and proportional fairness) approaches. However, the competitiveness of the fully decentralized environment depicted involves the non-existence of a Nash equilibrium. Thus, we have also found high variability in experienced individual throughput due to the constant changes in the actions played, motivated by the fact that the reward of each action changes according to the adversaries. We have evaluated the effects of intrinsic parameters on the learning algorithm, showing that this variability can be reduced by reducing the degree of exploration and the learning rate."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was partially supported by the Spanish Ministry of Economy and Competitiveness under the Maria de Maeztu Programme for Units of Excellence (MDM-2015-0502) and the European Regional Development Fund (ERDF) under the TEC2015-71303-R Programme (MINECO / FEDER)."}], "references": [{"title": "A distributed access point selection algorithm based on no-regret learning for wireless access networks", "author": ["Chen", "May"], "venue": "In Vehicular Technology Conference (VTC 2010-Spring),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Channel selection for networkassisted D2D communication via no-regret bandit learning with calibrated forecasting", "author": ["S. Maghsudi", "S. Staczak"], "venue": "IEEE Transactions on Wireless Communications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Joint channel selection and power control in infrastructureless wireless networks: A multiplayer multiarmed bandit framework", "author": ["S. Maghsudi", "S. Staczak"], "venue": "IEEE Transactions on Vehicular Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A Q-learning-based dynamic channel assignment technique for mobile communication systems", "author": ["J. Nie", "S. Haykin"], "venue": "IEEE Transactions on Vehicular Technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "December). A Q-learning based approach to interference avoidance in self-organized femtocell networks", "author": ["M. Bennis", "D. Niyato"], "venue": "In GLOBECOM Workshops (GC Wkshps),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Reinforcement learning: An introduction (Vol", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "IEEE 802.11 ax: High-efficiency WLANs.", "author": ["B. Bellalta"], "venue": "IEEE Wireless Communications", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Reinforcement Learning (RL) has recently spread use in the wireless communications field to solve many kinds of problems such as Access Point (AP) association [1], channel selection [2] or transmit power adjustment [3], as it allows learning good-performing configurations only from the observed results.", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "INTRODUCTION Reinforcement Learning (RL) has recently spread use in the wireless communications field to solve many kinds of problems such as Access Point (AP) association [1], channel selection [2] or transmit power adjustment [3], as it allows learning good-performing configurations only from the observed results.", "startOffset": 195, "endOffset": 198}, {"referenceID": 2, "context": "INTRODUCTION Reinforcement Learning (RL) has recently spread use in the wireless communications field to solve many kinds of problems such as Access Point (AP) association [1], channel selection [2] or transmit power adjustment [3], as it allows learning good-performing configurations only from the observed results.", "startOffset": 228, "endOffset": 231}, {"referenceID": 3, "context": "Among these, Q-learning has been applied to dynamic channel assignment in mobile networks in [4] and to automatic channel selection in Femto Cell networks in [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Among these, Q-learning has been applied to dynamic channel assignment in mobile networks in [4] and to automatic channel selection in Femto Cell networks in [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "DECENTRALIZED STATELESS Q-LEARNING FOR ENHANCING SPATIAL REUSE IN WNS Q-learning [6, 7] is an RL technique that enables an agent to learn the optimal policy to follow in a given environment.", "startOffset": 81, "endOffset": 87}, {"referenceID": 5, "context": "The optimal policy \u03c0\u2217 that maximizes the total expected reward is given by the Bellman\u2019s Optimality Equation [6]: Q\u2217(s, a) = E {", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Simulation Parameters According to [8], a typical high-density scenario for residential buildings contains 0.", "startOffset": 35, "endOffset": 38}], "year": 2017, "abstractText": "Reinforcement Learning is gaining attention by the wireless networking community due to its potential to learn goodperforming configurations only from the observed results. In this work we propose a stateless variation of Q-learning, which we apply to exploit spatial reuse in a wireless network. In particular, we allow networks to modify both their transmission power and the channel used solely based on the experienced throughput. We concentrate in a completely decentralized scenario in which no information about neighbouring nodes is available to the learners. Our results show that although the algorithm is able to find the best-performing actions to enhance aggregate throughput, there is high variability in the throughput experienced by the individual networks. We identify the cause of this variability as the adversarial setting of our setup, in which the most played actions provide intermittent good/poor performance depending on the neighbouring decisions. We also evaluate the effect of the intrinsic learning parameters of the algorithm on this variability.", "creator": "LaTeX with hyperref package"}}}