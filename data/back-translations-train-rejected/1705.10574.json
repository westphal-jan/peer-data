{"id": "1705.10574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Multi-Focus Image Fusion Via Coupled Sparse Representation and Dictionary Learning", "abstract": "We address the multi-focus image fusion problem, where multiple images captured with different focal settings are to be fused into an all-in-focus image of higher quality. Algorithms for this problem necessarily admit the source image characteristics along with focused and blurred feature. However, most sparsity-based approaches use a single dictionary in focused feature space to describe multi-focus images, and ignore the representations in blurred feature space. Here, we propose a multi-focus image fusion approach based on coupled sparse representation. The approach exploits the facts that (i) the patches in given training set can be sparsely represented by a couple of overcomplete dictionaries related to the focused and blurred categories of images; and (ii) merging such representations leads to a more flexible and therefore better fusion strategy than the one based on just selecting the sparsest representation in the original image estimate. By jointly learning the coupled dictionary, we enforce the similarity of sparse representations in the focused and blurred feature spaces, and then introduce a fusion approach to combine these representations for generating an all-in-focus image. We also discuss the advantages of the fusion approach based on coupled sparse representation and present an efficient algorithm for learning the coupled dictionary. Extensive experimental comparisons with state-of-the-art multi-focus image fusion algorithms validate the effectiveness of the proposed approach.", "histories": [["v1", "Tue, 30 May 2017 12:20:26 GMT  (2869kb,D)", "http://arxiv.org/abs/1705.10574v1", "27 pages, 8 figures, 1 table"]], "COMMENTS": "27 pages, 8 figures, 1 table", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rui gao", "sergiy a vorobyov"], "accepted": false, "id": "1705.10574"}, "pdf": {"name": "1705.10574.pdf", "metadata": {"source": "CRF", "title": "Multi-Focus Image Fusion Via Coupled Sparse Representation and Dictionary Learning", "authors": ["Rui Gao", "Sergiy A. Vorobyov"], "emails": ["rui.gao@aalto.fi", "svor@ieee.org"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "A. Contributions", "text": "In this paper, we propose to extend the coupled dictionary approach to the problem of merging multiple focus images based on coupled sparse and redundant representations. Coupled with over-complete dictionaries, they are expected to result in a more compact representation of focused and blurred image categories. The paper presents both algorithmic developments and simulation results for multiple image fusion. Compared to previous work, the main difference of the proposed approach is that the coupled dictionary is used for a more flexible and accurate representation of multiple focus images."}, {"heading": "B. Organization of the Paper and Notation", "text": "The rest of the work is organized as follows: Section II gives the problem description and summarizes some general assumptions; Section III gives a detailed explanation of our fusion procedure; Section IV focuses on the problem of the coupled dictionary; specifically, a K-SVD-based coupled dictionary learning algorithm is presented; simulation results are provided in Section V. Finally, we conclude the work in Section VI with a summary of this work. May 31, 2017 DRAFT5 We use bold uppercase letters for matrices; for example, we refer to k-te multifocus and all-infokus images as the following matrices of pixel Ik and IF, respectively. Similarly, the DF and DB matrices denote focus and blurred dictionaries; Bold lowercase letters stand for vectors and all-infokus images as the following matrices Ik and IF."}, {"heading": "II. PROBLEM DESCRIPTION", "text": "Consider the problem of constructing / reconstructing a high-quality all-in-focus IF image from a series of multi-focus source images (1) that can be mapped in the form of the following images (1). (2) It is a complete image drawn from the normal distribution N + V (2). (2) The goal of the fusion is to obtain a complete image of IK (1). (2) Some other assumptions such as capturing multiple images are the following. (2) Any multi-focus image IC is captured for the same scene and all multi-focus images are properly aligned. (2) Note: The latter assumption is typical in literature with a focus on image fusion, but the correct alignment of the images is also an important practical problem."}, {"heading": "III. FUSION VIA COUPLED SPARSE REPRESENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. The Problem of Finding Fusing Operator", "text": "The problem of the construction / reconstruction of a high-quality, clean all-in-focus image IF can generally be expressed as the following maximum A-posteriori probability (MAP): (IF) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall problem IF (IK) overall picture IF (IK) overall picture IF (IK) overall picture IF (IK) total picture IF (IK) total picture IF (IF) (IF) total picture IF (IF) IF (IF) (IF) total picture IF (IF) IF (IF) total picture IF (IF) IF (IF) total picture IF (IF) IF (IF) total picture IF (IF) IF (IF) total picture IF (IF) IF (IF) total picture IF (IF) IF (IF) total picture IF (IF) (IF) total picture IF (IF) IF (IF) total picture IF (IF) (IF) (IF) total picture IF (IF) (IF) (IF) total picture IF (IF) (IF) total picture IF (IF (IF) (IF) (IF) total picture IF (IF) IF (IF (IF) (IF (IF) (IF) total picture IF (IF (IF)"}, {"heading": "B. Local Optimal Fusion", "text": "The search for the local optimal all-in-focus vectors of the coefficients (1), which we must first find and consider in order to find the capture of each coefficient (1) (2) and take into account (2). Due to the fact that visible artifacts can occur at the borders of the market, the overlapping patches, which include the pixels of adjacent patches, can typically be used to suppress such artifacts. May 31, 2017 DRAFT10Fusing operators can be applied to the number of image patches. (3) Kk = 1 can be formulated as iF."}, {"heading": "C. Global Reconstruction", "text": "In order to remove possible artifacts and improve the spatial smoothness, the global reconstruction problem must be solved to ensure consistency between the original IF0 estimate and the final result. IF0 value previously used in the natural image analysis to the order of magnitude of the image gradients can be used to write the global reconstruction problem as IF = argmin IF1 2 and IF \u2212 IF0 [1] value (2) (12), with the global reconstruction problem taking the form of total optimization, [2] i, j denotes the discretion of the gradient for (i, j) -the element, defined as [3] value, [4] value, [4] value, [4] value, [5] value, [5] value, [5] value, [5] value, [5] value, [5] value, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, [5] value [5, 5] value, 5, 5, 5, [5] value, 5, 5, [5, [5] value, 5, [2] value, [2, [2] -1, [2, [2] -1, [2] -1, [2, [2] -1] -1, [2, [2] -1, [2, [2] -1] -1, [2, [2] -1, [2] -1, [2] -1, [2, [2, [2] -1] -1, [2, [2] -1, [2] -1, [2] -1, [2] -1, [2] -1, [2] -1] -1, [2] -1, [2, [2, [2] -1] -1, [2] -1, [2, [2] -1] -1] -1, [2, ["}, {"heading": "D. Summary of the Fusion Via Sparse Representation Algorithm", "text": "In summary: When the underlying dictionaries DF and DB are known, the optimal merging is first calculated using local sparse representations, then the global reconstruction is applied to improve the spatial smoothness of the reconstructed all-in-focus source image. General merging of multi-focus images using sparse representation algorithms is called an algorithm 1. Note that before reconstructing the all-in-focus image, we must ensure that all images {Ik} Kk = 1 are of the same intensity that can be achieved by removing the mean intensity for source images as reflected in step 1 of the algorithm 1. Finally, the mean intensity must be added to the reconstructed all-in-focus image {Ik} Kk = 1 of the same intensity that can be achieved by removing the mean intensity for source images as reflected in step 1 of the algorithm 1."}, {"heading": "IV. COUPLED DICTIONARY LEARNING", "text": "The description of the merging process in the previous section is based on the assumption that the coupled dictionary (DF, DB) is known. However, the dictionary must also be learned, as shown in Fig. 1. In this section, the problem of the coupled dictionary (DF, DB), which learns from a number of available blurred image fields, is addressed with the aim of developing a computationally efficient algorithm."}, {"heading": "A. Coupled Dictionary Learning Problem Formulation", "text": "This year, it is so far that it is not so far before it goes to the next round."}, {"heading": "B. Dictionary Update", "text": "In (17) and (18) we describe the products DFK and DBK as the sums of the vector outer products. (17) we describe the products DFK and DBK as the sums of the vector outer products. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK. (17) we describe the products DFK and DBK."}, {"heading": "C. Sparse Coding", "text": "q q q q c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "D. Summary of the Coupled Dictionary Learning Algorithm", "text": "Alternately between dictionary update and sparse coding steps, the overall algorithm for coupled dictionary learning can be summarized as in algorithm 2. Also, the rapid convergence for type (30) updates is well documented (see summary in [49]). Algorithm 2: coupled dictionary learning. Input: Training sets for vectorized focus and blurred image fields XF and XB, the initial dictionaries DF0 and D B 0, and the initial sparse coding matrix: 0.1: Initialization: Set DF: = DF0, D B: = DB0, Result: = 0. 2: for t = 1 \u00b7 \u00b7 \u00b7 N do 3: Calculation of the error matrices EFt and E B t for the atoms F t and d B, each 4: Gate for all atoms (Fine) and Fine: (Fine for all atoms)."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experimental Setup", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "C. Effects of Main Parameters", "text": "The following three main parameters affect fusion performance: patch size d2, tolerance error, and overlap of length between adjacent patches p. We compare the DRAFT24 methods performed on May 31, 2017, which are the SR-CM, SR-KSVD, and the proposed methods, on the grayscale multi-focus dataset, which contains 10 pairs of grayscale images and 30 pairs of artificial source images with 256 grayscales. Figures 4-6 show the averages of QMI, QAB / F, and runtime for different values of pitch size, tolerance error, and overlapping length, respectively. We also provide the corresponding parameter values for all methods in the caption. Figures 4 (a) and (b) show that both QMI and QAB / F are slightly better when the patch size is larger. As the patch size increases, runtime increases (see Figure 4 (c) for the proposed QMig."}, {"heading": "D. Robustness to Noise", "text": "To test how robust the proposed noise method is, we add Gaussian noise to the source images and perform fusion experiments for different noise levels \u03c3 = {0, 5, 10, 15, 20}. The results are shown in Fig. 7. It is evident that QMI and QAB / F gradually decrease as noise levels increase. For comparison, we also show the results of applying the SR-CM and SR-KSVD methods to noisy images with \u03c3 = 15. As can be seen in the fusion images shown in Fig. 8, there are significant differences in the pores of the human face. In particular, Fig. 8 (c) and (d) show an oversmoothing effect. In addition, some details also disappear. Our approach works best as it appears visually in Fig. 8 (e). Therefore, the proposed approach is able to perform fusion and restoration simultaneously.31 May 2017 DRAFT25"}, {"heading": "VI. CONCLUSION", "text": "We first formalized the physical process of capturing multi-focus images, and then developed a basic model based on the coupled sparse representation of all-in-focus images. We introduced a coupled dictionary learning algorithm based on K-SVD that enforces the sparse approximations for dual feature spaces (focused and blurred). Using the coupled dictionary of focused and blurred feature spaces, we developed an efficient and precise merging approach and demonstrated that the proposed approach preserves the edge and structural information of source images, drastically reduces blocking artifacts, circular blurring, and artificial distortion, and generally shows better results than existing fusion methods, including state-of-the-art methods."}], "references": [{"title": "Multifocus image fusion based on robust principal component analysis, \u201dPattern", "author": ["T. Wan", "C. Zhu", "Z. Qin"], "venue": "Recognit. Lett., vol. 34,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Multi-focus image fusion with dense SIFT", "author": ["Y. Liu", "S. Liu", "Z. Wang"], "venue": "Inf. Fusion, vol. 23, pp. 139\u2013155, May 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Generation of all-in-focus images by noise-robust selective fusion of limited depth-of-field images", "author": ["S. Pertuz", "D. Puig", "M.A. Garcia", "A. Fusiello"], "venue": "IEEE Trans. Image Process., vol. 22, no. 3, pp. 1242\u20131251, Mar. 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-focus image fusion using wavelet-domain statistics", "author": ["J. Tian", "L. Chen"], "venue": "Proc. IEEE Int. Conf. Image Process., Hong Kong, 2010, pp. 1205\u20131208.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-focus image fusion using a bilateral gradient-based sharpness criterion", "author": ["J. Tian", "L. Chen", "L. Ma", "W. Yu"], "venue": "Opt. Commun., vol. 284, no. 1, pp. 80\u201387, Jan. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Focusing techniques", "author": ["M. Subbarao", "T. Choi", "A. Nikzad"], "venue": "Opt. Eng., vol. 32, pp. 2824\u20132836, Mar. 1993.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "Multifocus image fusion using the nonsubsampled contourlet transform", "author": ["Q. Zhang", "B.L. Guo"], "venue": "Signal Process., vol. 89, pp. 1334\u20131346, Jul. 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Remote sensing image fusion using the curvelet transform", "author": ["F. Nencini", "A. Garzelli", "S. Baronti", "L. Alparone"], "venue": "Inf. Fusion, vol. 8, no. 2, pp. 143\u2013156, Apr. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A wavelet-based image fusion tutorial", "author": ["G. Pajares", "J. Cruz"], "venue": "Pattern Recognit., vol. 37, no. 9, pp. 1855\u20131872, Sep. 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1855}, {"title": "Image sequence fusion using a shift-invariant wavelet transform", "author": ["O. Rockinger"], "venue": "Proc. IEEE Int. Conf. Image Process., Santa Barbara, CA, 1997, pp. 288\u2013291.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Feature-based fusion of medical imaging data", "author": ["V.D. Calhoun", "T. Adali"], "venue": "IEEE Trans. Inf. Technol. Biomedicine, vol. 13, no. 5, pp. 711\u2013720, Sep. 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Fusion of multi-focus images using differential evolution algorithm", "author": ["V. Aslantas", "R. Kurban"], "venue": "Expert Syst. Appl., vol. 37, no. 12, pp. 8861\u20138870, Dec. 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The laplacian pyramid as a compact image code", "author": ["P. Burt", "E. Adelson"], "venue": "IEEE Trans. Commun., vol. 31, no. 4, pp. 532\u2013 540, Apr. 1983. May 31, 2017  DRAFT  26", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1983}, {"title": "Multifocus image fusion using region segmentation and spatial frequency", "author": ["S. Li", "B. Yang"], "venue": "Inf. Fusion, vol. 26, no. 7, pp. 971\u2013979, Jul. 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-scale weighted gradient-based fusion for multi-focus images", "author": ["Z. Zhou", "S. Li", "B. Wang"], "venue": "Image Vision Comput., vol. 20, pp. 60\u201372, Nov. 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive multi-focus image fusion using a wavelet-based statistical sharpness measure", "author": ["J. Tian", "L. Chen"], "venue": "Signal Process., vol. 92, no. 9, pp. 2137\u20132146, Sep. 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Multifocus image fusion using the nonsubsampled contourlet transform", "author": ["Q. Zhang", "B. Guo"], "venue": "Signal Process., vol. 89, no. 7, pp. 1334\u20131346, Jul. 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Compressive image fusion", "author": ["T. Wan", "N. Canagarajah", "A. Achim"], "venue": "Proc. IEEE Int. Conf. Image Process., San Diego, CA, 2008, pp. 1308\u20131311.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Fusion of algorithms for compressed sensing", "author": ["S. Ambat", "S. Chatterjee", "K. Hari"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 14, pp. 3699\u20133704, May. 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A robust fusion scheme for multifocus images using sparse features", "author": ["T. Wan", "Z. Qin", "C. Zhu", "R. Liao"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., British Columbia, Canada, 2013, pp. 1957\u20131961.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-focus image fusion based on sparse feature matrix decomposition and morphological filtering", "author": ["H. Li", "L. Li", "J. Zhang"], "venue": "Opt. Commun., vol. 342, pp. 1\u201311, May. 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Multifocus image fusion and restoration with sparse representation", "author": ["B. Yang", "S. Li"], "venue": "IEEE Trans. Instrum. Meas., vol. 59, no. 4, pp. 884\u2013892, Apr. 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-focus image fusion using dictionary-based sparse representation", "author": ["M. Nejati", "S. Samavi", "S. Hirani"], "venue": "Inf. Fusion, vol. 25, pp. 72\u201384, Sep. 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust multi-focus image fusion using multi-task sparse representation and spatial context", "author": ["Q. Zhang", "M.D. Levine"], "venue": "IEEE Trans. Image Process., vol. 25, no. 5, pp. 2045\u20132058, Mar. 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Image fusion with cosparse analysis operator", "author": ["R. Gao", "S.A. Vorobyov", "H. Zhao"], "venue": "IEEE Signal Process. Lett., vol. 24, no. 7, pp. 943\u2013947, July 2017.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-focus image fusion based on spatial frequency in discrete cosine transform domain", "author": ["L. Cao", "L. Jin", "H. Tao", "G. Li", "Z. Zhuang", "Y. Zhang"], "venue": "IEEE Signal Process. Lett., vol. 22, no. 2, pp. 220\u2013224, Sep. 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311\u20134322, Nov. 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S.O. Aase", "J.H. Husoy"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., Phoenix, AZ, USA, 1999, pp. 2443\u20132446.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Proc. ACM Int. Conf. Mach. Learn., Montreal, QC, Canada, 2009, pp. 689\u2013696.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Trans. Image Process., vol. 19, no. 11, pp. 2861\u20132873, May. 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Coupled dictionary training for image super-resolution", "author": ["J. Yang", "Z. Wang", "Z. Lin", "S. Cohen", "T. Huang"], "venue": "IEEE Trans. Image Process., vol. 21, no. 8, pp. 3467\u20133478, Aug. 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint dictionary training for bandwidth extension of speech signals", "author": ["J. Sadasivan", "S. Mukherjee", "C.S. Seelamantula"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., Shanghai, China, 2016, pp. 5925\u20135929.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis", "author": ["S. Wang", "L. Zhang", "Y. Liang", "Q. Pan"], "venue": "Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit., Rhode Island, USA, 2012, pp. 2216\u20132223.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "A statistical prediction model based on sparse representations for single image super-resolution", "author": ["T. Peleg", "M. Elad"], "venue": "IEEE Trans. Image Process., vol. 23, no. 6, pp. 2569\u20132582, Jun. 2014. May 31, 2017  DRAFT  27", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust multiframe super-resolution employing iteratively re-weighted minimization", "author": ["T. Kohler", "X. Huang", "F. Schebesch", "A. Aichert", "A. Maier", "J. Hornegger"], "venue": "IEEE Trans. Comput. Imag., vol. 2, no. 1, pp. 42\u201358, Mar. 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["A. Mesaros", "T. Heittola", "O. Dikmen", "T. Virtanen"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., South Brisbane, Australia, 2015, pp. 151\u2013155.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Trans. Signal Process., vol. 58, no. 3, pp. 1553\u20131564, Mar. 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-focus image fusion via coupled dictionary training", "author": ["R. Gao", "S.A. Vorobyov", "H. Zhao"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., Shanghai, China, 2016, pp. 1666\u20131670.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "CS Technion, vol. 40, no. 8, pp. 1\u201315, Apr. 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "A plurality of sparse representations is better than the sparsest one alone", "author": ["M. Elad", "I. Yavneh"], "venue": "IEEE Trans. Inf. Theory, vol. 55, no. 10, pp. 4701\u20134714, Oct. 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting statistical dependencies in sparse representations for signal recovery", "author": ["T. Peleg", "Y. Eldar", "M. Elad"], "venue": "IEEE Trans. Signal Process., vol. 60, no. 5, pp. 2286\u20132303, Feb. 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Convex image denoising via non-convex regularization", "author": ["A. Lanza", "S. Morigi", "F. Sgallari"], "venue": "Scale Sp. Var. Methods Comput. Vis., vol. 9087, Springer, pp. 666\u2013677, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for finding global minimizers of image segmentation and denoising models", "author": ["M. Nikolova", "S. Esedoglu", "T.F. Chan"], "venue": "SIAM J. Appl. Math., vol. 66, no. 5, pp. 1632\u20131648, Jun. 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex denoising using non-convex tight frame regularization,\u201dIEEE", "author": ["A. Parekh", "I.W. Selesnick"], "venue": "Signal Process. Lett., vol. 22,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D. Bertsekas"], "venue": "Math. Program., vol. 55, no. 3, pp. 293\u2013318, Nov. 1992.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1992}, {"title": "Alternating direction method for balanced image restoration", "author": ["S. Xie", "S. Rahardja"], "venue": "IEEE Trans. Image Process., vol. 21, no. 11, pp. 4557\u20134567, Nov. 2012.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving dictionary learning: multiple dictionary updates and coefficient reuse", "author": ["L.N. Smith", "M. Elad"], "venue": "IEEE Signal Process. Lett., vol. 20, no. 1, pp. 79\u201382, Jan. 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.Q. Luo"], "venue": "SIAM J. Optim., vol. 23, no. 2, pp. 1126\u20131153, Jan. 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Ideal spatial adaptation by wavelet shrinkage", "author": ["D. Donoho", "J. Johnstone"], "venue": "Biometrika, vol. 81, no. 3, pp. 425\u2013455, 1994.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1994}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on Pure and Applied Mathematics, vol. 57, no. 11, pp. 1413\u20131457, 2004.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "Comments on information measure for performance of image fusion", "author": ["M. Hossny", "S. Nahavandi", "D. Creighton"], "venue": "Electron. Lett., vol. 44, no. 18, pp. 1066\u20131067, Aug. 2008.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2008}, {"title": "Objective image fusion performance measure", "author": ["C. Xydeas", "V. Petrovi\u0107"], "venue": "Electron. Lett., vol. 36, no. 4, pp. 308\u2013309, Feb. 2000.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-focus image fusion using dictionary-based sparse representation", "author": ["M. Nejati", "S. Samavi", "S. Shirani"], "venue": "Inf. Fusion, vol. 25, pp. 72\u201384, Sep. 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "The USC-SIPI Image Database [Online]. Available: http://sipi.usc.edu/database", "author": ["A. Weber"], "venue": "May 31,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1981}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Over the last several decades, considerable attention has been given to the multi-focus image fusion problem [1]\u2013[5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "INTRODUCTION Over the last several decades, considerable attention has been given to the multi-focus image fusion problem [1]\u2013[5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Multi-focus image fusion is an effective post-processing technique for combining multiple images captured with different focal distances into an all-in-focus image, without sacrificing image quality, and at the same time without using specialized optic sensors [6]\u2013[8].", "startOffset": 261, "endOffset": 264}, {"referenceID": 6, "context": "Multi-focus image fusion is an effective post-processing technique for combining multiple images captured with different focal distances into an all-in-focus image, without sacrificing image quality, and at the same time without using specialized optic sensors [6]\u2013[8].", "startOffset": 265, "endOffset": 268}, {"referenceID": 7, "context": "The problem is of high importance in many fields, ranging from remote sensing to medical imaging [9]\u2013[12], especially for addressing the demand for cost minimization of optical sensors/cameras.", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "The problem is of high importance in many fields, ranging from remote sensing to medical imaging [9]\u2013[12], especially for addressing the demand for cost minimization of optical sensors/cameras.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "The majority of the existing literature on the topic can be categorized into two basic classes of approaches [13]: the spatial frequency-based and transform domainbased approaches.", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "Corresponding algorithms include discrete wavelet transform (DWT) [17], curvelet transform (CVT) [9], non-subsampled contourlet transform (NSCT) [18] among others.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "Corresponding algorithms include discrete wavelet transform (DWT) [17], curvelet transform (CVT) [9], non-subsampled contourlet transform (NSCT) [18] among others.", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "Corresponding algorithms include discrete wavelet transform (DWT) [17], curvelet transform (CVT) [9], non-subsampled contourlet transform (NSCT) [18] among others.", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "Looking at recent approaches, sparsity and overcompleteness have been successfully used for computational image fusion [19]\u2013[27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 25, "context": "Looking at recent approaches, sparsity and overcompleteness have been successfully used for computational image fusion [19]\u2013[27].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "Representative examples include the K-SVD method [28], the method of optimal directions (MOD) [29], the online dictionary learning (OLD) method [30], and their variants.", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "Representative examples include the K-SVD method [28], the method of optimal directions (MOD) [29], the online dictionary learning (OLD) method [30], and their variants.", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "Representative examples include the K-SVD method [28], the method of optimal directions (MOD) [29], the online dictionary learning (OLD) method [30], and their variants.", "startOffset": 144, "endOffset": 148}, {"referenceID": 29, "context": "While the single dictionary model has been extensively studied, there is an advanced coupled dictionary viewpoint to sparsity and overcompleteness that has tracked the double feature space representation problem [31]\u2013[34].", "startOffset": 212, "endOffset": 216}, {"referenceID": 32, "context": "While the single dictionary model has been extensively studied, there is an advanced coupled dictionary viewpoint to sparsity and overcompleteness that has tracked the double feature space representation problem [31]\u2013[34].", "startOffset": 217, "endOffset": 221}, {"referenceID": 33, "context": "The combination of learned coupled dictionary and sparse approximation is shown to be superior for representing double feature spaces [35]\u2013[38].", "startOffset": 134, "endOffset": 138}, {"referenceID": 36, "context": "The combination of learned coupled dictionary and sparse approximation is shown to be superior for representing double feature spaces [35]\u2013[38].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "The aforementioned image fusion methods [19]\u2013[25], [27] directly learn and exploit a single overcomplete dictionary in a single feature space in order to describe multiple images which contain both the focused and blurred categories of image features.", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "The aforementioned image fusion methods [19]\u2013[25], [27] directly learn and exploit a single overcomplete dictionary in a single feature space in order to describe multiple images which contain both the focused and blurred categories of image features.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "The aforementioned image fusion methods [19]\u2013[25], [27] directly learn and exploit a single overcomplete dictionary in a single feature space in order to describe multiple images which contain both the focused and blurred categories of image features.", "startOffset": 51, "endOffset": 55}, {"referenceID": 37, "context": "In our conference paper [39], some initial results for the first contribution have been presented.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "\u2022 We develop a K-SVD-based coupled dictionary learning algorithm by extending the wellknown K-SVD algorithm [28].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "The benefit of using a coupled dictionary {D, D} versus using a single focused dictionary D only, as in the existing fusion approaches [19]\u2013[25], [27], is the following.", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "The benefit of using a coupled dictionary {D, D} versus using a single focused dictionary D only, as in the existing fusion approaches [19]\u2013[25], [27], is the following.", "startOffset": 140, "endOffset": 144}, {"referenceID": 25, "context": "The benefit of using a coupled dictionary {D, D} versus using a single focused dictionary D only, as in the existing fusion approaches [19]\u2013[25], [27], is the following.", "startOffset": 146, "endOffset": 150}, {"referenceID": 39, "context": "blurred feature spaces gives a larger space and more degrees of freedom to the problem [41].", "startOffset": 87, "endOffset": 91}, {"referenceID": 38, "context": ", conventional orthogonal matching pursuit (OMP) algorithm [40].", "startOffset": 59, "endOffset": 63}, {"referenceID": 39, "context": "In particular, the two vectors of competitive representations \u03b1k and \u03b1 B k in (8) can be found by randomized orthogonal matching pursuit (RandOMP) method [41], [42].", "startOffset": 154, "endOffset": 158}, {"referenceID": 40, "context": "In particular, the two vectors of competitive representations \u03b1k and \u03b1 B k in (8) can be found by randomized orthogonal matching pursuit (RandOMP) method [41], [42].", "startOffset": 160, "endOffset": 164}, {"referenceID": 21, "context": "According to the general rule [23], the sparse vector of coefficients \u03b1 is the one from the set of vectors {\u03b1k}k=1 that has the largest l1-norm, that is, \u03b1 = R { {\u03b1k}k=1 } = argmax k 6=m {\u2016\u03b1k\u20161, \u2016\u03b1m\u20161}", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "with linear operators Dh and Dv representing finite difference approximations of the first-order horizontal and vertical partial derivatives [43].", "startOffset": 141, "endOffset": 145}, {"referenceID": 41, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 29, "endOffset": 33}, {"referenceID": 43, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 34, "endOffset": 38}, {"referenceID": 44, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 151, "endOffset": 155}, {"referenceID": 45, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 157, "endOffset": 161}, {"referenceID": 29, "context": "Many numerical algorithms [31]\u2013[34] have been developed to address problems of type (14) in alternating manner.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "Many numerical algorithms [31]\u2013[34] have been developed to address problems of type (14) in alternating manner.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "Although the algorithms in [31]\u2013 [34] can be used for addressing (14) after some necessary modifications, their computational efficiency is not satisfactory.", "startOffset": 27, "endOffset": 31}, {"referenceID": 32, "context": "Although the algorithms in [31]\u2013 [34] can be used for addressing (14) after some necessary modifications, their computational efficiency is not satisfactory.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "The K-SVD algorithm [28] is another well-known alternating approach to overcomplete dictionary learning.", "startOffset": 20, "endOffset": 24}, {"referenceID": 46, "context": "To avoid some redundant updates, we can introduce a so-called mask matrix [48] that consists of zeros and ones, and aims to keep all the non-zero column elements intact.", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "Problems (21) and (22) can be then easily solved by singular value decomposition (SVD) [28] of the corresponding error matrices.", "startOffset": 87, "endOffset": 91}, {"referenceID": 47, "context": "Then (27) can be efficiently addressed using block successive minimization method [49], which is a general framework for many large-scale optimization methods.", "startOffset": 82, "endOffset": 86}, {"referenceID": 48, "context": "where the operator S \u03bb T (dt) {\u00b7} is the soft thresholding operator with the threshold \u03bb/T (dt) applied on the entries of its argument [50], [51] and T (dt) = \u2016dt\u2016 to guarantee the convergence (see details in [49]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 49, "context": "where the operator S \u03bb T (dt) {\u00b7} is the soft thresholding operator with the threshold \u03bb/T (dt) applied on the entries of its argument [50], [51] and T (dt) = \u2016dt\u2016 to guarantee the convergence (see details in [49]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 47, "context": "where the operator S \u03bb T (dt) {\u00b7} is the soft thresholding operator with the threshold \u03bb/T (dt) applied on the entries of its argument [50], [51] and T (dt) = \u2016dt\u2016 to guarantee the convergence (see details in [49]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 47, "context": "Also the fast convergence for updates of type (30) is well documented fact (see the summary in [49]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 50, "context": "\u2022 The measure of how well the mutual information from the source images is preserved in the fused image, denoted as QMI [52];", "startOffset": 120, "endOffset": 124}, {"referenceID": 51, "context": "\u2022 The measure of how well the success of edge information transfers from the source images to the fused image, denored as, Q [53].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 199, "endOffset": 203}, {"referenceID": 16, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 277, "endOffset": 281}, {"referenceID": 0, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 355, "endOffset": 358}, {"referenceID": 21, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 432, "endOffset": 436}, {"referenceID": 53, "context": "For the coupled dictionary learning, the training sets of focused image patches of size 8 consisting of 50,000 patches taken from the USC-SIPI image database [55] of 40 natural images, and 50,000 blurred patches of the same size, which are created from focused patches using Gaussian blur function, are used.", "startOffset": 158, "endOffset": 162}, {"referenceID": 52, "context": "Visual Comparison We conduct our fusion experiments over the standard multi-focus dataset [54], and show some representative fusion results in Figs.", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "We address the multi-focus image fusion problem, where multiple images captured with different focal settings are to be fused into an all-in-focus image of higher quality. Algorithms for this problem necessarily admit the source image characteristics along with focused and blurred feature. However, most sparsity-based approaches use a single dictionary in focused feature space to describe multi-focus images, and ignore the representations in blurred feature space. Here, we propose a multi-focus image fusion approach based on coupled sparse representation. The approach exploits the facts that (i) the patches in given training set can be sparsely represented by a couple of overcomplete dictionaries related to the focused and blurred categories of images; and (ii) merging such representations leads to a more flexible and therefore better fusion strategy than the one based on just selecting the sparsest representation in the original image estimate. By jointly learning the coupled dictionary, we enforce the similarity of sparse representations in the focused and blurred feature spaces, and then introduce a fusion approach to combine these representations for generating an all-in-focus image. We also discuss the advantages of the fusion approach based on coupled sparse representation and present an efficient algorithm for learning the coupled dictionary. Extensive experimental comparisons with state-of-the-art multi-focus image fusion algorithms validate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}