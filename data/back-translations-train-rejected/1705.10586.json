{"id": "1705.10586", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Character-Based Text Classification using Top Down Semantic Model for Sentence Representation", "abstract": "Despite the success of deep learning on many fronts especially image and speech, its application in text classification often is still not as good as a simple linear SVM on n-gram TF-IDF representation especially for smaller datasets. Deep learning tends to emphasize on sentence level semantics when learning a representation with models like recurrent neural network or recursive neural network, however from the success of TF-IDF representation, it seems a bag-of-words type of representation has its strength. Taking advantage of both representions, we present a model known as TDSM (Top Down Semantic Model) for extracting a sentence representation that considers both the word-level semantics by linearly combining the words with attention weights and the sentence-level semantics with BiLSTM and use it on text classification. We apply the model on characters and our results show that our model is better than all the other character-based and word-based convolutional neural network models by \\cite{zhang15} across seven different datasets with only 1\\% of their parameters. We also demonstrate that this model beats traditional linear models on TF-IDF vectors on small and polished datasets like news article in which typically deep learning models surrender.", "histories": [["v1", "Mon, 29 May 2017 15:53:00 GMT  (1514kb,D)", "http://arxiv.org/abs/1705.10586v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhenzhou wu", "xin zheng", "daniel dahlmeier"], "accepted": false, "id": "1705.10586"}, "pdf": {"name": "1705.10586.pdf", "metadata": {"source": "CRF", "title": "Character-Based Text Classification using Top Down Semantic Model for Sentence Representation", "authors": ["Zhenzhou Wu", "Xin Zheng", "Daniel Dahlmeier"], "emails": ["hyciswu@gmail.com", "xin.zheng@sap.com", "d.dahlmeier@sap.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}, {"heading": "1.1 TDSM on Characters", "text": "TDSM is a framework that can be applied to both word-level and character-level input. In this essay, we opt for character-level input over word-level input for practical reasons in the industry.1. In industrial applications, continuous learning is often required on records that change over time, which means that vocabulary can change over time, so character-by-character storage of the record dispels the need to rebuild a new vocabulary every time there are new words.2 Industrial records are usually very complex and loud with a large vocabulary, so the footprint when storing word embeddings is much larger than character embeddings. Therefore, improving the performance of a character-based model has much greater practical value compared to a word-based model."}, {"heading": "2 Related Work", "text": "There are a variety of methods of learning for text classification, and most of them may well achieve good results for the formal text datasets, but there are also a number of methods that deal with the text classification that have been used in practice in recent years. For example, it is the way in which the individual terms relate to the individual terms, and the way in which the individual terms are tailored to the individual terms. There are a variety of studies that relate to the problem of text classification. There is a model that is similar to Collobert and that relates to the individual terms. There are two types of language that encompass both terms. There are two channels of word vectors that are interconnected. One is statically consistent in education and the other is finely different. There is a variety of research that relates to the problem of text classification."}, {"heading": "3 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Characters to Topic-Vector", "text": "In contrast to word embedding (Mikolov et al., 2013), topic vector attempts to learn a distributed theme representation in each dimension of the representation vector, whereby the simple addition of topic vectors at word level can form a sentence representation. Figure 1 illustrates how the topic vector is extracted from letters in words using FCN (Fully Convolutional Network). To force representation at word level with topic meanings, we apply a sigmoid function via the output of FCN. We limit the values in each dimension to 0 to 1, forcing the model to learn a distributed theme representation of the word."}, {"heading": "3.2 Sentence Representation Vector Construction", "text": "Formulating a sentence of words can be done simply by summarizing the word descriptions that produce catastrophic results (Johnson & Zhang, 2015) due to the cancellation effect of embedding vectors (negative plus positive equals zero) or, in our model, summarizing word-topic vectors at the word level is equivalent to the BoW vectors in word counting, treating the previous contribution of each word to the final sentence vector equally. Traditionally, a better sentence representation via BoW TF-IDF will be one that gives each word in a document a weight in relation to IDF (inverse document vectors in word counting). If we draw inspiration from the TF-IDF representations, we can have a recursive neural network that pays attention (Yang et al, 2016)."}, {"heading": "3.2.1 Bi-Directional LSTM", "text": "BiLSTM consists of a forward-looking LSTM (FLSTM) and a backward-looking LSTM (BLSTM), both of which are LSTM of the same design, except that FLSTM reads the sentence forward and BLSTM reads the sentence backwards. \u2212 A recurring step in LSTMof Equation 2 consists of the following steps f (s), v (s), v (s), v (t), c (t), c (t), f (5) i (t), c (t), c (t), c (t), c (t), c (t), c (t), c (t), c (t), c (t), c (t), c (t), c (t), c (t), c, c, c, c, c (t), c, c, c (t), c (t, t, t, t, t, t, t, t, t, t, t, t, t, t, c), c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, t, t, t, t, t, t, t, t (t), t, t, t, t, t, t, t, t, t), t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t), t, t, t, t, t, t, t (t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t,"}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Model", "text": "The entire model has only 780,000 parameters, which is only 1% of the parameters of the large CNN model (Zhang et al., 2015). We used BiLSTM (Graves et al., 2013a) with 100 units both in forward and backward LSTM cell. The output of the BiLSTM is 200 dimensions after we have concatenated the outputs from the forward and backward cells. We then use the attention on the words by transforming 200 output dimensions linearly to 1, followed by a softmax over the 1-dimensional outputs of all words. Following the signs of theme vector transformation by FCN, each theme vector will have 180 dimensions. The theme vectors are then linearly combined with attention weights to form a 180-dimensional BoW-like set vector. This vector is further linked to 200 dimensions of BiLSTM outputs. The 380 dimensions of the 10 blocks of the model are not completely connected (REAL) after a few functions."}, {"heading": "4.2 Datasets", "text": "We use the standard benchmark datasets created by (Zhang et al., 2015), which have different numbers of training samples and test samples ranging from 28,000 to 3,600,000 training samples - 1https: / / github.com / hycis / TensorGraphples and varying text lengths, ranging from an average of 38 words for Ag News to 566 words for Sogou News, as shown in Table 1. The datasets are a good mix of polished (AG) and noisy (Yelp and Amazon Reviews), long (Sogou) and short (DBP and AG), large (Amazon Reviews) and small (AG) datasets, so the results from these datasets serve as a good evaluation of the quality of the model."}, {"heading": "4.3 Word Setting", "text": "In this thesis, we use 128 ASCII characters as a character set, which is how most test documents are composed. We define the word length as 20 and the embedding length as 100 characters. If a word is written with less than 20 characters, we fill it with z-ros. If the length is greater than 20 characters, we simply take the first 20 characters. We set the maximum word length as the average word number of documents in the record plus two standard deviations, which is enough to cover more than 97.5% of the documents. For documents with more words than the default maximum word number, we discard the excess words."}, {"heading": "4.4 Baseline Models", "text": "We select both traditional models and the convolutionary models (Zhang et al., 2015), the recursive models (Yang et al., 2016; Tang et al., 2016), as baselines. Also, to ensure a fair comparison of the models, so that any variation in the result is purely due to the model difference, we only compare TDSM with models trained in the same way of data preparation, i.e. the words are lowered and there are no additional data changes or augmentations with thesaurus. Unfortunately (Yang et al., 2016; Tang et al., 2016) recursive models are trained in full text instead of lowered text, so that their models cannot objectively be compared with our models, as it is well known from (Uysal & Gunal, 2014) that a deviating text pre-processing will have a significant impact on the final results, Zhang's result shows that a lowering of a simple fold can still lead to a deviation from the classification of 4% to a good understanding of the accuracy of the models."}, {"heading": "4.4.1 Traditional Models", "text": "BoW is the standard method of word counting, where the attribute vector represents the term frequency of the words in a sentence. TF-IDF is similar to BoW except that it is derived by counting the words in the sentence, weighted by the term frequency of each word and the frequency of the reverse document (Joachims, 1998). This is a very competitive model, especially on clean and small datasets."}, {"heading": "4.4.2 Word-Based Models", "text": "Bag-of-Means is derived from the accumulation of words with k-means in 5000 clusters, followed by a BoW representation of words in 5000 clusters. Lg. w2v Conv and Sm w2v Conv are CNN models for the following word embeddings (Zhang et al., 2015), to ensure fair comparison with character-based models, the CNN architecture is the same as Lg. Conv and Sm. Conv with the same number of parameters. LSTM-GRNN from (Tang et al., 2016) is essentially a recurring neural network based on LSTM and GRU (Chung et al., 2014) across the words in a sentence and across the sentences in a document. It attempts to learn a hierarchical representation of text from multiple levels of recurring layers. HN-ATT from (Yang et al., 2016) is essentially LSTM-GRNN similar, except that it learns the phrases directly during the summary of the words during the learning process."}, {"heading": "4.4.3 Character-Based Models", "text": "Lg. Conv and Sm. Conv are proposed in (Zhang et al., 2015), a CNN character encoding model that is the primary character-based base model with which we compare."}, {"heading": "5 Results", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "6 Conclusion", "text": "Based on the results, we see a strong potential for TDSM as a competitive model for text classification due to its hybrid architecture, which looks at the sentence from both the traditional TF-IDF perspective and the recent perspective of deep learning. Results show that this type of view can derive rich text representation for both small and large datasets."}], "references": [{"title": "Learning mid-level features for recognition", "author": ["Y-Lan Boureau", "Francis R. Bach", "Yann LeCun", "Jean Ponce"], "venue": "In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Semantic segmentation with modified deep residual networks", "author": ["Xinze Chen", "Guangliang Cheng", "Yinghao Cai", "Dayong Wen", "Heping Li"], "venue": "In Pattern Recognition - 7th Chinese Conference, CCPR,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning character-level representations for part-ofspeech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Computer Vision - ECCV - 14th European Conference,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": "In European conference on machine learning,", "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Semi-supervised convolutional neural networks for text categorization via region embedding", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, A meeting of SIGDAT, a Special Interest Group of the ACL,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "CoRR, abs/1405.4053,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Feed-forward networks with attention can solve some long-term memory problems", "author": ["Colin Raffel", "Daniel P.W. Ellis"], "venue": "CoRR, abs/1512.08756,", "citeRegEx": "Raffel and Ellis.,? \\Q2015\\E", "shortCiteRegEx": "Raffel and Ellis.", "year": 2015}, {"title": "Pruning subsequence search with attention-based embedding", "author": ["Colin Raffel", "Daniel P.W. Ellis"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Raffel and Ellis.,? \\Q2016\\E", "shortCiteRegEx": "Raffel and Ellis.", "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom"], "venue": "CoRR, abs/1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei-Fei Li"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "The impact of preprocessing on text classification", "author": ["Alper Kursat Uysal", "Serkan Gunal"], "venue": "Information Processing & Management,", "citeRegEx": "Uysal and Gunal.,? \\Q2014\\E", "shortCiteRegEx": "Uysal and Gunal.", "year": 2014}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "We apply the model on characters and our results show that our model is better than all the other characterbased and word-based convolutional neural network models by (Zhang et al., 2015) across seven different datasets with only 1% of their parameters.", "startOffset": 167, "endOffset": 187}, {"referenceID": 23, "context": "Recently, deep learning has been particularly successful in speech and image as an automatic feature extractor (Graves et al., 2013b; Russakovsky et al., 2014; He et al., 2015), however deep learning\u2019s application to text as an automatic feature extractor has not been always successful (Zhang et al.", "startOffset": 111, "endOffset": 176}, {"referenceID": 7, "context": "Recently, deep learning has been particularly successful in speech and image as an automatic feature extractor (Graves et al., 2013b; Russakovsky et al., 2014; He et al., 2015), however deep learning\u2019s application to text as an automatic feature extractor has not been always successful (Zhang et al.", "startOffset": 111, "endOffset": 176}, {"referenceID": 28, "context": ", 2015), however deep learning\u2019s application to text as an automatic feature extractor has not been always successful (Zhang et al., 2015) even compared to simple linear models with BoW or TF-IDF feature representation.", "startOffset": 118, "endOffset": 138}, {"referenceID": 28, "context": "In many experiments when the text is polished like news articles or when the dataset is small, BoW or TF-IDF is still the state-of-art representation compared to sent2vec or paragraph2vec (Le & Mikolov, 2014) representation using deep learning models like RNN (Recurrent Neural Network) or CNN (Convolution Neural Network) (Zhang et al., 2015).", "startOffset": 323, "endOffset": 343}, {"referenceID": 19, "context": "The weights wi for each word in the sentence summation is learnt by recurrent neural network (RNN) (Pascanu et al., 2013) with attention over the words (Yang et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 27, "context": ", 2013) with attention over the words (Yang et al., 2016).", "startOffset": 38, "endOffset": 57}, {"referenceID": 28, "context": "Recently, many deep learning methods have been proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).", "startOffset": 94, "endOffset": 151}, {"referenceID": 12, "context": "Recently, many deep learning methods have been proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).", "startOffset": 94, "endOffset": 151}, {"referenceID": 24, "context": "Deep convolutional neural network has been extremely successful for image classification (Krizhevsky et al., 2012; Sermanet et al., 2013).", "startOffset": 89, "endOffset": 137}, {"referenceID": 0, "context": "For both convolution and max-pooling layers, they employ 1-D filters (Boureau et al., 2010).", "startOffset": 69, "endOffset": 91}, {"referenceID": 15, "context": "The transformation is done with fully convolutional network (FCN) similar to (Long et al., 2015), each hierarchical level of the FCN will extract an n-gram character feature of the word until the word-level topic-vector.", "startOffset": 77, "endOffset": 96}, {"referenceID": 25, "context": "(Tang et al., 2016; Yang et al., 2016) uses two hierarchies of recurrent neural network to extract the document representation.", "startOffset": 0, "endOffset": 38}, {"referenceID": 27, "context": "(Tang et al., 2016; Yang et al., 2016) uses two hierarchies of recurrent neural network to extract the document representation.", "startOffset": 0, "endOffset": 38}, {"referenceID": 25, "context": "The major difference between (Tang et al., 2016) and (Yang et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 27, "context": ", 2016) and (Yang et al., 2016) is that Yang applies attention over outputs from the recurrent when learning a summarizing representation.", "startOffset": 12, "endOffset": 31}, {"referenceID": 22, "context": "Usually, attention is used in sequential model (Rockt\u00e4schel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).", "startOffset": 47, "endOffset": 136}, {"referenceID": 17, "context": "Usually, attention is used in sequential model (Rockt\u00e4schel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).", "startOffset": 47, "endOffset": 136}, {"referenceID": 11, "context": "Usually, attention is used in sequential model (Rockt\u00e4schel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).", "startOffset": 47, "endOffset": 136}, {"referenceID": 1, "context": "Residual network (He et al., 2015, 2016; Chen et al., 2016) is known to be able to make very deep neural networks by having skip-connections that allows gradient to back-propagate through the skip-connections.", "startOffset": 17, "endOffset": 59}, {"referenceID": 7, "context": "Residual network in (He et al., 2015) outperforms the state-of-the-art models on image recognition.", "startOffset": 20, "endOffset": 37}, {"referenceID": 16, "context": "Unlike word-embedding (Mikolov et al., 2013), topic-vector tries to learn a distributed topic representation at each dimension of the representation vector, which thus allows the simple addition of the word-level topic-vectors to form a sentence representation.", "startOffset": 22, "endOffset": 44}, {"referenceID": 27, "context": "Drawing inspiration from TF-IDF representation, we can have a recurrent neural network that outputs the attention (Yang et al., 2016) over the words.", "startOffset": 114, "endOffset": 133}, {"referenceID": 28, "context": "The entire model has only 780,000 parameters which is only 1% of the parameters in (Zhang et al., 2015) large CNN model.", "startOffset": 83, "endOffset": 103}, {"referenceID": 1, "context": "The 380 dimensions undergo 10 blocks of ResNet (Chen et al., 2016) plus one fully connected layer.", "startOffset": 47, "endOffset": 66}, {"referenceID": 28, "context": "We use the standard benchmark datasets prepare by (Zhang et al., 2015).", "startOffset": 50, "endOffset": 70}, {"referenceID": 28, "context": "Unfortunately, these two RNN models did not use the same text preprocessing technique as other models, so their models may not be objectively comparable to Zhang\u2019s or our model, because it is well known that (Zhang et al., 2015; Uysal & Gunal, 2014), the difference in text preprocessing will have a significant impact on the final accuracy.", "startOffset": 208, "endOffset": 249}, {"referenceID": 25, "context": "00 LSTM-GRNN (Tang et al., 2016) - - - 67.", "startOffset": 13, "endOffset": 32}, {"referenceID": 27, "context": "6 HN-ATT (Yang et al., 2016) - - - 71.", "startOffset": 9, "endOffset": 28}, {"referenceID": 28, "context": "Conv (Zhang et al., 2015) 87.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": "Conv (Zhang et al., 2015) 84.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": "We select both traditional models and the convolutional models from (Zhang et al., 2015), the recurrent models from (Yang et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 27, "context": ", 2015), the recurrent models from (Yang et al., 2016; Tang et al., 2016) as baselines.", "startOffset": 35, "endOffset": 73}, {"referenceID": 25, "context": ", 2015), the recurrent models from (Yang et al., 2016; Tang et al., 2016) as baselines.", "startOffset": 35, "endOffset": 73}, {"referenceID": 27, "context": "Unfortunately, (Yang et al., 2016; Tang et al., 2016) recurrent models are trained on full text instead of lowered text, so their models may not be objectively compared to our models, since it is well known from (Uysal & Gunal, 2014) that differ-", "startOffset": 15, "endOffset": 53}, {"referenceID": 25, "context": "Unfortunately, (Yang et al., 2016; Tang et al., 2016) recurrent models are trained on full text instead of lowered text, so their models may not be objectively compared to our models, since it is well known from (Uysal & Gunal, 2014) that differ-", "startOffset": 15, "endOffset": 53}, {"referenceID": 9, "context": "TF-IDF is similar to BoW, except that it is derived by the counting of the words in the sentence weighted by individual word\u2019s term-frequency and inverse-document-frequency (Joachims, 1998).", "startOffset": 173, "endOffset": 189}, {"referenceID": 28, "context": "w2v Conv and Sm w2v Conv is CNN model on word embeddings following (Zhang et al., 2015), to ensure fair comparison with character-based models, the CNN architecture is the same as Lg.", "startOffset": 67, "endOffset": 87}, {"referenceID": 25, "context": "LSTM-GRNN from (Tang et al., 2016) is basically a recurrent neural network based on LSTM and GRU (Chung et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 2, "context": ", 2016) is basically a recurrent neural network based on LSTM and GRU (Chung et al., 2014) over the words in a sentence, and over the sentences in a document.", "startOffset": 70, "endOffset": 90}, {"referenceID": 27, "context": "HN-ATT from (Yang et al., 2016) is basically similar to LSTM-GRNN except that instead of just learning the hierarchical representation of the text directly with RNN, it also learns attention weights over the words during the summarization of the words and over the sentences during the summarization of the sentences.", "startOffset": 12, "endOffset": 31}, {"referenceID": 28, "context": "Conv are proposed in (Zhang et al., 2015), which is a CNN model on character encoding and is the primary characterbased baseline model that we are comparing with.", "startOffset": 21, "endOffset": 41}, {"referenceID": 13, "context": "Character-based models are the most significant and practical model for real large scale industry deployment because of its smaller memory footprint, agnostic to changes in vocabulary and robust to misspellings (Kim et al., 2015).", "startOffset": 211, "endOffset": 229}], "year": 2017, "abstractText": "Despite the success of deep learning on many fronts especially image and speech, its application in text classification often is still not as good as a simple linear SVM on n-gram TF-IDF representation especially for smaller datasets. Deep learning tends to emphasize on sentence level semantics when learning a representation with models like recurrent neural network or recursive neural network, however from the success of TF-IDF representation, it seems a bag-of-words type of representation has its strength. Taking advantage of both representions, we present a model known as TDSM (Top Down Semantic Model) for extracting a sentence representation that considers both the word-level semantics by linearly combining the words with attention weights and the sentence-level semantics with BiLSTM and use it on text classification. We apply the model on characters and our results show that our model is better than all the other characterbased and word-based convolutional neural network models by (Zhang et al., 2015) across seven different datasets with only 1% of their parameters. We also demonstrate that this model beats traditional linear models on TF-IDF vectors on small and polished datasets like news article in which typically deep learning models surrender.", "creator": "LaTeX with hyperref package"}}}