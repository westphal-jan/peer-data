{"id": "1705.10610", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "The Importance of Automatic Syntactic Features in Vietnamese Named Entity Recognition", "abstract": "This paper presents a state-of-the-art system for Vietnamese Named Entity Recognition (NER). By incorporating automatic syntactic features with word embeddings as input for bidirectional Long Short-Term Memory (Bi-LSTM), our system, although simpler than some deep learning architectures, achieves a much better result for Vietnamese NER. The proposed method achieves an overall F1 score of 92.05% on the test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community. Our named entity recognition system outperforms the best previous systems for Vietnamese NER by a large margin.", "histories": [["v1", "Mon, 29 May 2017 08:10:15 GMT  (153kb)", "http://arxiv.org/abs/1705.10610v1", "7 pages, 3 figures. arXiv admin note: substantial text overlap witharXiv:1705.04044"], ["v2", "Wed, 31 May 2017 16:23:40 GMT  (153kb)", "http://arxiv.org/abs/1705.10610v2", "7 pages, 3 figures, resolve text overlapping. arXiv admin note: text overlap witharXiv:1705.04044"], ["v3", "Thu, 8 Jun 2017 16:17:49 GMT  (152kb)", "http://arxiv.org/abs/1705.10610v3", "7 pages, 3 figures, submitted to PACLIC 2017. arXiv admin note: text overlap witharXiv:1705.04044"], ["v4", "Mon, 28 Aug 2017 02:32:16 GMT  (153kb)", "http://arxiv.org/abs/1705.10610v4", "7 pages, 9 tables, 3 figures, accepted to PACLIC 2017"]], "COMMENTS": "7 pages, 3 figures. arXiv admin note: substantial text overlap witharXiv:1705.04044", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thai-hoang pham", "phuong le-hong"], "accepted": false, "id": "1705.10610"}, "pdf": {"name": "1705.10610.pdf", "metadata": {"source": "CRF", "title": "The Importance of Automatic Syntactic Features in Vietnamese Named Entity Recognition", "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 5.10 610v 1 [cs.C L] 29 MThis paper introduces a state-of-the-art Vietnamese Named Entity Recognition (NER) system. By integrating automatic syntactic features with word embedding as input for the bi-directional Long Short-Term Memory (BiLSTM), our system, while simpler than some deep learning architectures, achieves a much better result for Vietnamese NER. The proposed method achieves an overall score of 92.05% in a test series organized by the Vietnamese Language and Language Processing Community (VLSP) at the end of 2016. Our designated Entity Recognition System by far surpasses the best previous systems for Vietnamese NER."}, {"heading": "1 Introduction", "text": "The purpose of this task is to identify noun phrases and classify each of them into a predefined class. NER is a crucial step in pre-processing, which is used in some NLP applications such as information extraction, question answering, automatic translation, language processing and biomedical science. These systems, which are considered a group of four, require specialized translation, language processing and biomedical science. In two common tasks, CoNLL 20021 and CoNLL 20032, language-independent nouvel systems for English, German, Spanish and Dutch are evaluated. These systems are referred to as quadrilingual: http: / / www.cnts.ac.be / conll2002 / ner / ner / ner _ art, which identifies with other languages that are not related."}, {"heading": "2 Related Works", "text": "Within the large body of research on NER Le published over the past two decades, we identify two main approaches. The first approach is characterized by the use of traditional sequence labeling models such as CRF, hidden markov model, support vector machine, maximum entropy, which depend heavily on handmade features (Florian et al., 2003; Lin and Wu, 2009; Durrett and Klein, 2014; Luo and Xiaojiang Huang, 2015), which have attempted to use information other than the available training data such as gazetteers and uncommented data. In recent years, several neural architectures have been proposed for the task of NER. These methods have a long history, but have only recently been focused by advances in computational power and high-quality word embedding. First, (Collobert et al, 2011) a CNN model of conditional random word embedding at the top level has been proposed."}, {"heading": "3 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Feature Engineering", "text": "To create Vietnamese word skip gram vectors, we use a data set consisting of 7.3GB of text from 2 million articles collected via a Vietnamese news portal.4 The text is first normalized to lowercase letters and all special characters are removed. Common symbols such as comma, semicolon, colon, period and percent signs are replaced by a special token dot. All number strings are replaced by a special token number, so that correlations between certain words and numbers are correctly recognized by the neural network or the log-bilinear regression model.Each word in the Vietnamese language can consist of more than one syllable with spaces, which the unmonitored models can view as multiple words.Therefore, it is necessary to replace the spaces within each word with underscores to create complete word markers.The tokenization process follows the method described in (Le-Hong et al, 2008)."}, {"heading": "3.2 Long Short-Term Memory", "text": "Long-term short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of the Recurrent Neural Network (RNN), which is designed to solve problems associated with dwindling and exploding gradients (Bengio et al., 1994; Pascanu et al., 2013) when learning with long-range sequences. LSTM networks are the same as RNN, 4 http: / / www.baomoi.com, except that the hidden layer updates are replaced by memory cells. Basically, a memory cell unit consists of three multiplicative gates that control the portions of information to forget and pass on to the next time step. As a result, it is better for using long-range dependency data. The memory cell is composed as follows: it (Wiht \u2212 1 + Uixt + bi) ft = proportions to forget and pass on to the next time step."}, {"heading": "3.3 Bidirectional Long Short-Term Memory", "text": "The original LSTM uses only past functions. For many sequence marking tasks, it is advantageous to access past and future contexts. Therefore, we use the bi-directional LSTM (Bi-LSTM) (Graves and Schmidhuber, 2005; Graves et al., 2013) for the NER task. The basic idea is to move forward as well as backward to capture past and future information and link two hidden states to form a final representation. Figure 2 illustrates the pros and cons of Bi-LSTM."}, {"heading": "3.4 Our Deep Learning Model", "text": "For Vietnamese entity recognition, we use a two-layer Bi-LSTM with a Softmax layer on top to recognize named entities in a sequence of sentences. Inputs are a combination of word and syntactic features, and the results are the probability distributions over named entity tags. Figure 3 describes the details of our deep learning model. In the next sections, we present our experimental results."}, {"heading": "4 Results And Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 VLSP Corpus", "text": "There are four named entity types in this corpus, names of the person, place, organization, and other named entities. Four types of named entities are compatible with their descriptions in the CoNLL Shared Task 2003. Automatic POS and chunking tags are available in this dataset. The format of this corpus is the same as that of the CoNLL 2003 Shared Task. It consists of five columns. The order of these columns is word, POS tag, chunking tag, named entity label, and nested entity label. Our system focuses on only named entities without nesting, so we do not use the fifth column. Named entity labels are commented with IOB notation, as in the CoNLL Shared Tasks. There are 9 labels: B-PER and PER-PER are used for people."}, {"heading": "4.2 Evaluation Method", "text": "Performance is measured by the F1 score: F1 = 2 \u043c Accuracy \u043a Recallprecision + RecallPrecision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities present in the corpus and found by the system. A named entity is correct only if it matches the corresponding entity in the data file. Performance of our system is evaluated by the automatic evaluation script of the CoNLL 2003 joint task."}, {"heading": "4.3 Results", "text": "In this last section we analyze the efficiency of word embedding, bidirectional learning, model configuration and especially automatic syntactical improvements. [http: / / www.cnts.be / conll2003 / ner / embeddings] To evaluate the effectiveness of word embedding, we compare the systems on three types of input factors: on the one hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other, on the one hand, on the other, on the other, on the other, on the other, on the other, on the one hand, on the other, on the other, on the other hand, on the other, on the other, on the other, on the other, on the other, on the other, on the other, on the other, on the last, on the third, on the third, on the last, on the last, on the last, on the last, on the last, on the last, on the last on the last, last on the last, last on the last, last on the last on the last, last on the last, last on the last on the last, last on the last, last on the last on the last, last on the last, last on the last, last on the last, last on the last, last on the last, last on the last, last on the last on the last, last, last on the last on the last, last on the last, on the last, last on the last on the last, on the last, on the last, on the last, on the last, on the last, on the last, on the last, on the last, on the last, on the last, on the last, on the last, on the last of the last, on the last, on the last of the last of the last, on the last, on the last, on the last of the last, on the last, on the last, on the last of the last of the last, on the last, on the last, on the last, on the last, on the last, on the last of the last of the last of the last of the last, on the last, on the last, on the last of the last, on the last of the last, on the last, on the"}, {"heading": "5 Conclusion", "text": "In this thesis, we presented a state-of-the-art Vietnamese language Entity Recognition System that achieves an F1 value of 92.05% on the standard corpus recently released by the Vietnamese Language and Speech Community. Our system outperforms the primary shared task system of NER by a large margin, especially 3.2%. We also demonstrated the effectiveness of using automatic syntactical features for the BiLSTM model that exceed the combination of BiLSTM-CNN-CRF models, although they require less time to compute."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2016] Jason P.C. Chiu", "Eric Nichols"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Chiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A joint model for entity analysis: Coreference", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": "typing, and linking. Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "Proceedings of CoNLL-2003,", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Framewise phoneme classification with bidirectional lstm networks", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of 2005 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alex Graves", "Abdel rahmand Mohamed", "Geoffrey Hinton"], "venue": "In Proceedings of 2013 IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint arXiv:1603.01360", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "A hybrid approach to word segmentation of Vietnamese texts", "author": ["Thi Minh Huyen Nguyen", "AzimRoussanaly", "TuongVinh Ho"], "venue": "In Language and Automata Theory and Applications,", "citeRegEx": "Le.Hong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Le.Hong et al\\.", "year": 2008}, {"title": "Vietnamese named entity recognition using token regular expressions and bidirectional inference", "author": ["Phuong Le-Hong"], "venue": "In Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing,", "citeRegEx": "Le.Hong.,? \\Q2016\\E", "shortCiteRegEx": "Le.Hong.", "year": 2016}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Joint entity recognition and disambiguation", "author": ["Luo", "Xiaojiang Huang2015] Gang Luo", "Zaiqing Nie Xiaojiang Huang", "Chin-Yew Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Vietnamese named entity recognition at vlsp 2016 evaluation campaign", "author": ["Le Minh Nguyen", "Xuan Chien Tran"], "venue": "In Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Pro-", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "End-to-end recurrent neural network models for vietnamese named entity recognition: Word-level vs. character-level", "author": ["Pham", "Le-Hong2017] Thai-Hoang Pham", "Phuong Le-Hong"], "venue": "arXiv preprint arXiv:1705.04044", "citeRegEx": "Pham et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 4, "context": "The first approach is characterized by the use of traditional sequence labelling models such as CRF, hidden markov model, support vector machine, maximum entropy that are heavily dependent on hand-crafted features (Florian et al., 2003; Lin and Wu, 2009; Durrett and Klein, 2014; Luo and Xiaojiang Huang, 2015).", "startOffset": 214, "endOffset": 310}, {"referenceID": 2, "context": "Firstly, (Collobert et al., 2011) used a CNN over a sequence of word embeddings with a conditional random field on the top.", "startOffset": 9, "endOffset": 33}, {"referenceID": 8, "context": "(Huang et al., 2015) used Bi-LSTM with CRF layer for joint decoding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Instead of using hand-crafted feature as (Huang et al., 2015), (Chiu and Nichols, 2016) proposed a hybrid model that combined BiLSTM with CNN to learn both characterlevel and word-level representations.", "startOffset": 41, "endOffset": 61}, {"referenceID": 9, "context": "Unlike (Chiu and Nichols, 2016), (Lample et al., 2016) used BI-LSTM to model both character and word-level information.", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": "78% used MEMM with many hand-crafted features (Le-Hong, 2016).", "startOffset": 46, "endOffset": 61}, {"referenceID": 14, "context": "Meanwhile, (Nguyen et al., 2016) applied deep learning approach for this task.", "startOffset": 11, "endOffset": 32}, {"referenceID": 9, "context": "They used the implementation provided by (Lample et al., 2016).", "startOffset": 41, "endOffset": 62}, {"referenceID": 11, "context": "59% that is competitive with the accuracy of (Le-Hong, 2016).", "startOffset": 45, "endOffset": 60}, {"referenceID": 10, "context": "The tokenization process follows the method described in (Le-Hong et al., 2008).", "startOffset": 57, "endOffset": 79}, {"referenceID": 11, "context": "These regular expressions over tokens described particularly in (Le-Hong, 2016) are shown to provide helpful features for classifying candidate named entities, as shown in the experiments.", "startOffset": 64, "endOffset": 79}, {"referenceID": 0, "context": "Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of Recurrent Neural Network (RNN) which is designed to deal with gradient vanishing and exploding problems (Bengio et al., 1994; Pascanu et al., 2013) when learning with long-range sequences.", "startOffset": 185, "endOffset": 228}, {"referenceID": 15, "context": "Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of Recurrent Neural Network (RNN) which is designed to deal with gradient vanishing and exploding problems (Bengio et al., 1994; Pascanu et al., 2013) when learning with long-range sequences.", "startOffset": 185, "endOffset": 228}, {"referenceID": 6, "context": "For this reason, we utilize the bidirectional LSTM (Bi-LSTM) (Graves and Schmidhuber, 2005; Graves et al., 2013) for NER task.", "startOffset": 61, "endOffset": 112}, {"referenceID": 11, "context": "02%, which is higher than the best participating system (Le-Hong, 2016) in that shared task about 3.", "startOffset": 56, "endOffset": 71}, {"referenceID": 11, "context": "Moreover, (Pham and Le-Hong, 2017) used a combination of Bi-LSTM, CNN, and CRF that achieved the same performance with (Le-Hong, 2016).", "startOffset": 119, "endOffset": 134}, {"referenceID": 11, "context": "This system is end-to-end architecture that required only word embeddings while (Le-Hong, 2016) used many syntactic and hand-crafted features with MEMM.", "startOffset": 80, "endOffset": 95}, {"referenceID": 11, "context": "Models Types F1 (Le-Hong, 2016) ME 88.", "startOffset": 16, "endOffset": 31}], "year": 2017, "abstractText": "This paper presents a state-of-the-art system for Vietnamese Named Entity Recognition (NER). By incorporating automatic syntactic features with word embeddings as input for bidirectional Long Short-Term Memory (BiLSTM), our system, although simpler than some deep learning architectures, achieves a much better result for Vietnamese NER. The proposed method achieves an overall F1 score of 92.05% on the test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community. Our named entity recognition system outperforms the best previous systems for Vietnamese NER by a large margin.", "creator": "LaTeX with hyperref package"}}}