{"id": "1705.10686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial Examples", "abstract": "Feature squeezing is a recently-introduced framework for mitigating and detecting adversarial examples. In previous work, we showed that it is effective against several earlier methods for generating adversarial examples. In this short note, we report on recent results showing that simple feature squeezing techniques also make deep learning models significantly more robust against the Carlini/Wagner attacks, which are the best known adversarial methods discovered to date.", "histories": [["v1", "Tue, 30 May 2017 15:00:55 GMT  (360kb,D)", "http://arxiv.org/abs/1705.10686v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["weilin xu", "david evans", "yanjun qi"], "accepted": false, "id": "1705.10686"}, "pdf": {"name": "1705.10686.pdf", "metadata": {"source": "CRF", "title": "Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial Examples", "authors": ["Weilin Xu", "David Evans", "Yanjun Qi"], "emails": [], "sections": [{"heading": null, "text": "Key concepts Machine learning, evasive attack, feature squeezing"}, {"heading": "1 INTRODUCTION", "text": "Many techniques for producing opposing examples have been proposed, and almost all of them perform a kind of algorithmic search, starting from a seed sample, with the aim of finding a misclassified example within a distance (according to a given distance metric) limited by a parameter of opposing strength [3, 6, 7, 9]. Feature squeezing is a general method for mitigating and detecting adversarial examples against machine learning models [10]. It reduces the search space available to an opponent by merging samples that match many different feature squeezing vectors in the original space in a single sample. Feature squeezing is a cost-effective pre-processing technology that can be used with any classification model so that it is orthogonal to other methods of model hardening."}, {"heading": "2 BACKGROUND", "text": "Carlini and Wagner have recently introduced three new gradient-based attack algorithms (L2, L \u221e and L0) that are more effective in terms of the adversarial success rates achieved with minimal interference than all previously known methods [2]. Their L2 attack uses a logic-based objective function that differs from all existing L2 attacks and avoids box constraints by modifying variables. Their L \u221e and L0 attacks are based on the L2 attack and are tailored to different distance metrics."}, {"heading": "2.1 Threat Model", "text": "s goal is to generate opposing examples that are misclassified (an untargeted attack) or that are assigned to a specific class (targeted attack). We use the Carlini and Wagner attack algorithms to generate opposing examples against the target model. In our evaluation, these opposing examples are squeezed out before they are fed into the target model. We do not consider an opponent that adapts to the characteristic squeezing defense. Such an adjustment is an area for further research, but does not seem trivial, at least when feature squeezing is used in the proposed detection framework."}, {"heading": "2.2 Feature Squeezer", "text": "There are many possible feature squeezers in the image space, and both color depth reduction and median smoothing have been used in previous work [10]. This short essay focuses on median smoothing with a 2 \u00d7 2 window, since our preliminary results suggest that it is almost always the best squeezer to mitigate opposing Carlini / Wagner examples. There are several possible approaches to achieving media smoothing with a window size of even numbers. We use the SciPy implementation [8], where the middle pixel is always in the lower right corner of the 2 \u00d7 2 sliding window. If there are two equal median values (two of the pixels have a value of x1 and the other two have a value of x2), the resulting median value is the larger (max (x1, x2))."}, {"heading": "3 ROBUSTNESS EVALUATION", "text": "We evaluate all three attack methods, L2, L \u221e and L0, proposed by Carlini and Wagner [2], in both non-targeted and targeted variations.ar Xiv: 170 5.10 686v 1 [cs.C R] 30 May 201 7There are the most effective enemy methods found so far. Our defense does not depend on the details of these methods. However, we find that part of their effectiveness, measured by the goal, changes as little as possible in the input to achieve the enemy's goal of changing the class, which makes the defense against them so effective."}, {"heading": "3.1 Experimental Design", "text": "We used two sets of data, MNIST [5] (28 x 28 grayscale handwritten numerical images) and CIFAR-10 [4] (32 x 32 color images of objects) for the experiment. These are the same sets of data used in Carlini et al. \"s evaluation [2].Two target models for the two sets were trained with Carlini's code [1]. We used all standard parameters for the three attack methods, except that we changed the maximum iterations of the L2 attack from 10,000 to 1,000 to ensure consistency with other two methods. We used only the first 1,000 test images in each set to generate counter-examples, as there were three different attack methods (about 2 minutes per sample for L1 and L0)."}, {"heading": "3.2 Results", "text": "The target class is selected according to (l + 1) mod # class, with l being the basic truth class. Success rate of a targeted attack is defined as the percentage of enemy examples that are misclassified as target classes.Table 1 summarizes the results. Feature squeezing using 2 \u00d7 2 median smoothing does little to affect the accuracy of legitimate examples (99.5% to 99.4%) for MNIST and preserves 93.2% of the original accuracy for CIFAR-10 (73.0% versus 78.3%), but it dramatically improves the robustness of the two models, reducing the enemy's success rate for targeted attacks from almost 100% to less than 6% for all three attack methods on both datasets. Our defense increases the target accuracy of opposing examples from at least 81% to 81% and the accuracy of all IST models to over 81%."}, {"heading": "4 DETECTING ADVERSARIAL EXAMPLES", "text": "Outside this framework (i.e. only using the output of the classifier on the squeezed input) there is a risk that opponents will be confronted with new uses. The detection framework compares the difference in predictions between an original input and the squeezed version. The score of each input x is calculated in the L1 standard: score = (x) \u2212 (squeeze (x)) | 1 Our intuition is that legitimate examples should have low values, because feature squeezing methods should not alter normal inputs in a way that affects the classification. On the other hand, an opposing example may look dramatically different from a target model with feature squeezing, which results in a high score."}, {"heading": "4.1 Experimental Design", "text": "The detection experiment uses the same design and data sets as the robustness experiments. We used the contrasting examples from the previous experiment (Section 3.1), where each of the first 1,000 legitimate examples was associated with six different contrasting examples. To get a balanced data set to evaluate detection performance, we used the first 6,000 test images from each data set as legitimate samples for a total of 12,000 images. We split the images into two parts: one half as a training set and the rest as a validation set. We selected a threshold that maximized detection accuracy on the training set and reported the detection performance measured on the validation set."}, {"heading": "4.2 Results", "text": "Table 2 summarizes the results: The ROC-AUC score of 0.9950 indicates that we get a near-perfect detector on the MNIST dataset. If we choose 0.1147 as the threshold, we get a detection accuracy of 98.80%, while the true positive rate is 99.30% and the false positive rate is 1.73%. The actual threshold used by a model operator may vary between the false negatives and the false positives depending on the desired trade-off. Detection performance on CIFAR-10 is worse than on MNIST, where the ROC-AUC score on the validation rate is 0.8711 (compared to 0.9950 on the MNIST). We believe that this is due to the fact that the target CIFAR-10 model does not have a state-of-the-art classification accuracy. Given the poor accuracy of 78.3% on the first 1000 legitimate examples, it may be difficult to distinguish a large feature from the LNIST."}, {"heading": "5 CONCLUSIONS", "text": "The experimental success of very simple feature-squeezing defenses against complicated adversarial methods does not mean that there are no effective adversarial methods. Instead, it means that we need to rethink the formal definition of an adversarial example and develop a stronger theoretical understanding of the behavior of machine learning models and potential adversaries."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was funded by a grant from the National Science Foundation and donations from Amazon and Google. We thank Nicolas Carlini and the other authors for providing the source code to the research community."}], "references": [{"title": "Towards Evaluating the Robustness of Neural Networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "In IEEE Symposium on Security and Privacy (Oakland)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky"], "venue": "Master\u2019s Thesis, University of Toronto", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The MNIST Database of Handwritten Digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": "http://yann.lecun.com/exdb/mnist", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "DeepFool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The Limitations of Deep Learning in Adversarial Settings", "author": ["Nicolas Papernot", "PatrickMcDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In IEEE European Symposium on Security and Privacy", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Intriguing Properties of Neural Networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks", "author": ["Weilin Xu", "David Evans", "Yanjun Qi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "Many techniques for generating adversarial examples have been proposed, and nearly all of these perform some kind of algorithmic search starting from a seed sample with the aim of finding a misclassified example within a distance (according to a given distance metric) limited by an adversarial strength parameter [3, 6, 7, 9].", "startOffset": 314, "endOffset": 326}, {"referenceID": 4, "context": "Many techniques for generating adversarial examples have been proposed, and nearly all of these perform some kind of algorithmic search starting from a seed sample with the aim of finding a misclassified example within a distance (according to a given distance metric) limited by an adversarial strength parameter [3, 6, 7, 9].", "startOffset": 314, "endOffset": 326}, {"referenceID": 5, "context": "Many techniques for generating adversarial examples have been proposed, and nearly all of these perform some kind of algorithmic search starting from a seed sample with the aim of finding a misclassified example within a distance (according to a given distance metric) limited by an adversarial strength parameter [3, 6, 7, 9].", "startOffset": 314, "endOffset": 326}, {"referenceID": 6, "context": "Many techniques for generating adversarial examples have been proposed, and nearly all of these perform some kind of algorithmic search starting from a seed sample with the aim of finding a misclassified example within a distance (according to a given distance metric) limited by an adversarial strength parameter [3, 6, 7, 9].", "startOffset": 314, "endOffset": 326}, {"referenceID": 7, "context": "Feature squeezing is a general method for mitigating and detecting adversarial examples against machine learning models [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "[10] found two simple feature squeezing methods (bit depth reduction and median smoothing) to be extremely effective in mitigating and detecting the adversarial examples generated by Fast Gradient Sign Method (FGSM) [3] and Jacobian-based Saliency Map Approach (JSMA) [7].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[10] found two simple feature squeezing methods (bit depth reduction and median smoothing) to be extremely effective in mitigating and detecting the adversarial examples generated by Fast Gradient Sign Method (FGSM) [3] and Jacobian-based Saliency Map Approach (JSMA) [7].", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "[10] found two simple feature squeezing methods (bit depth reduction and median smoothing) to be extremely effective in mitigating and detecting the adversarial examples generated by Fast Gradient Sign Method (FGSM) [3] and Jacobian-based Saliency Map Approach (JSMA) [7].", "startOffset": 268, "endOffset": 271}, {"referenceID": 0, "context": "In this short note, we evaluate a simple feature squeezing defense against three state-of-the-art adversarial techniques proposed recently by Carlini and Wagner [2].", "startOffset": 161, "endOffset": 164}, {"referenceID": 0, "context": "Carlini and Wagner recently introduced three new gradient-based attack algorithms (L2, L\u221e and L0) that are more effective than all previously known methods in terms of the adversarial success rates achieved with minimal perturbation amounts [2].", "startOffset": 241, "endOffset": 244}, {"referenceID": 7, "context": "There exists many possible feature squeezers in the image space, and both color depth reduction and median smoothing were used in previous work [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 0, "context": "We evaluate all three attack methods, L2, L\u221e and L0 proposed by Carlini and Wagner [2], in both untargeted and targeted variations.", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "We use two datasets, MNIST [5] (28\u00d728 gray scale images of handwritten digits) and CIFAR-10 [4] (32\u00d732 color photos of objects) for the experiment.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "We use two datasets, MNIST [5] (28\u00d728 gray scale images of handwritten digits) and CIFAR-10 [4] (32\u00d732 color photos of objects) for the experiment.", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "\u2019s evaluation [2].", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Feature squeezing is a recently-introduced framework for mitigating and detecting adversarial examples. In previous work, we showed that it is effective against several earlier methods for generating adversarial examples. In this short note, we report on recent results showing that simple feature squeezing techniques also make deep learning models significantly more robust against the Carlini/Wagner attacks, which are the best known adversarial methods discovered to date.", "creator": "LaTeX with hyperref package"}}}