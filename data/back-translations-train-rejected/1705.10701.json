{"id": "1705.10701", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Multi-Labelled Value Networks for Computer Go", "abstract": "This paper proposes a new approach to a novel value network architecture for the game Go, called a multi-labelled (ML) value network. In the ML value network, different values (win rates) are trained simultaneously for different settings of komi, a compensation given to balance the initiative of playing first. The ML value network has three advantages, (a) it outputs values for different komi, (b) it supports dynamic komi, and (c) it lowers the mean squared error (MSE). This paper also proposes a new dynamic komi method to improve game-playing strength. This paper also performs experiments to demonstrate the merits of the architecture. First, the MSE of the ML value network is generally lower than the value network alone. Second, the program based on the ML value network wins by a rate of 67.6% against the program based on the value network alone. Third, the program with the proposed dynamic komi method significantly improves the playing strength over the baseline that does not use dynamic komi, especially for handicap games. To our knowledge, up to date, no handicap games have been played openly by programs using value networks. This paper provides these programs with a useful approach to playing handicap games.", "histories": [["v1", "Tue, 30 May 2017 15:23:32 GMT  (1175kb)", "http://arxiv.org/abs/1705.10701v1", "This version was also submitted to IEEE TCIAIG on May 30, 2017"]], "COMMENTS": "This version was also submitted to IEEE TCIAIG on May 30, 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["ti-rong wu", "i-chen wu", "guan-wun chen", "ting-han wei", "tung-yi lai", "hung-chun wu", "li-cheng lan"], "accepted": false, "id": "1705.10701"}, "pdf": {"name": "1705.10701.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "(It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is.) (It is. (It is.) (It is.) (It is. (It is.) (It is.) (It is.) (It is.) (It is.) (It is. (It is.) (It is.) (It is.) (It is.) (It is. (It is.) (It is.) (It is. (It is.) (It is. (it.) (It is.) (It is. (it.) (It is. (it.) (It is. (it.) (It is. (it.) (It is. (it.) (it.) (It is. (it.) (It is. (it.) (it is.) (It is. (it.) (it.) (It is. (it.) (it. (it is.) (it. (it is.) (it.) (it is. (it.) (it is. (it.) (it is. (it.) (it is. (it.) (it. (it is.) (it.) (it is. (it.) (it is. (it.) (it is. (it is. (it.) (it.) (it is. (it is. (it.) (it is.) (it is.) (it is. (it is. (it.) (it is. (it.) (it.) (it is. (it is.) (it is. (it.) (it is. (it. (it. (it.)) (it is. (it is. (it.)) (it is. (it. (it is. (it is.). (it."}, {"heading": "II. BACKGROUND", "text": "This section covers the SL / RL Policy Network and Value Network in Subsection II.A, describes the combination of DCNNs and MCTS in Subsection II.B, introduces the Board Evaluation Network in Subsection II.C and discusses the dynamic Komi technique for MCTS [3] in Subsection II.D."}, {"heading": "A. SL and RL Policy Network and Value Network", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "B. MCTS and DCNNs", "text": "Monte Carlo Tree Search (MCTS) [6] is an initial search algorithm based on a search tree [23] [33], in which Monte Carlo rollouts are used to estimate state values. A tree policy, traditionally the upper trust function (UCB) [2], is used to select positions (states) from the root to one of the leaves. If a leaf position is selected, one or more children are extended from the selected leaf position. AlphaGo used the PUCT algorithm [31] (which serves as tree policy) to select states in a tree, and applied the SL policy network and the value network to MCTS by asynchronous updates. The SL policy network is used in the MCTS expansion, in which MCTS takes into account the probability results from the network of extended child states and uses them as initial values of the PUCT algorithm. The Monte Carlo score consists of two parts, the Monte and the network output."}, {"heading": "C. Board Evaluation Network", "text": "In addition to the value network, another neural network with territory was previously proposed, the Board Evaluation (BV) network. [19] The number of outputs of the BV network corresponds to the number of board points. Each output value corresponds to a board point, which indicates the probability that the point to the endgame belongs to black (the probability for white is simply the addition of this value). The output layer of the BV network uses a sigmoid activation, while the goal of the network is to minimize the MSE between predicted probabilities and the actual end game ownership of each board point (in the form of one of two possible values). [19] The BV network uses 8 input channels, 5 layers of 64, 64, 48, 48 filters in each layer, using the Go4Go dataset as training data. To date, no one has continued to use this idea except a few mentioned in [30] discussions."}, {"heading": "D. Dynamic Komi", "text": "In fact, it is not as if this is a purely mental game, but a mental game aimed at finding a solution that does justice to the interests of the individual."}, {"heading": "III. OUR DESIGN", "text": "We present our network design in Section III.A and dynamic Komi methods in Section III.B."}, {"heading": "A. Network Design", "text": "This year is the highest in the history of the country."}, {"heading": "B. Dynamic Komi", "text": "This subchapter first modifies the win rate of rollouts and VS models (described in Subsection II.D) to support dynamic combi, and then proposes a new dynamic combi method. \u2212 For the SS method, the dynamic combi is adjusted based on the three types of territorial generations that can be generated from rollouts, board ratings of the BV network, or a mixture of the two types of value creation categories. \u2212 For the SS-M method, the weighting parameters are used, such as the way the value is mixed from the rollouts and value networks. \u2212 The formula of the combi rate is the same as [3]. For the VS method, when the current dynamic combi rate is adjusted on the basis, the next dynamic combi is adjusted based on the winning class. \u2212 The next dynamic combi rate is adjusted based on the winning class."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we will conduct all experiments on computers running four GTX 980Ti GPUs, two Intel Xeon E5-2690 (28 cores in total), 2.6 GHz, 128 GB of memory, and running on Linux. In Section IV.A, we will present four different network architectures and measure their mean square errors (MSEs) and game strengths. In Section IV.B, we will analyze the MSEs of these networks for games with different Komi values to verify the advantage of multi-level (ML) value networks. The accuracy of the number of win points in the BV-ML value network will be presented and analyzed in Section IV.D. In Section IV.D, various dynamic Komi techniques in MCTS with different handicap games will be discussed. Section IV.E will analyze the performance difference when using 7.5 and 6.5 to play 6.5-Komi games. Finally, we will analyze the correlation of BV and ML value networks in Section 0."}, {"heading": "A. Different Value Networks", "text": "In recent years, the number of those who live in the USA has increased significantly. (...) In recent years, the number of those who live in the USA has increased significantly. (...) In recent years, the number of those who live in the USA has increased significantly. (...) The number of those who live in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "B. Multi-labelled Value Network in Different Komi Games", "text": "In order to measure the network quality in various Komi games, we analyze two other sets of data, the 6.5-Komi and 0.5-Komi-VSE are usually lower than 6.5-Komi games. Since the MSE of VN and BV-VN are almost identical and the MSE of both ML-VN and BV-ML-VN are almost identical (also from the previous subsection), we show only the MSE of BV-VN and BV-ML-VN for simple games. First, we analyze the MSE of value networks for 6.5-Komi games. For BV-VN, since the edition contains only one label (trained by 7.5-Komi games), it is used to estimate the winning odds for these games. For BV-ML-VN, we use the value 6.5 to estimate the winning odds of these games, instead. Figure 5 (below) shows the MSE of BV-VN and BV-ML-VN. \""}, {"heading": "C. The Accuracy of Winning Points in BV-ML-VN", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "D. Dynamic Komi and Handicap Games", "text": "In fact, the number of those who are able to sit at the top of the leaderboard is even greater than the number of those who are in the first row. (...) The number of those who are in the second row continues to increase. (...) The number of those who are in the second row continues to increase. (...) The number of those who are in the second row is increasing. (...) The number of those who are in the third row is increasing. (...) The number of those who are in the first row is increasing. (...) The number of those who are in the second row is increasing. (...) The number of those who are in the third row is increasing. (...) The number of those who are in the third row is increasing. (...) The number of those who are in the first row is increasing. (...) The number of those who are in the first row is increasing."}, {"heading": "F. The Correlation between BV and VN", "text": "In this subsection, the correlation between BV and VN in the BV-ML-VN is presented in a scatter chart as Figure 9 (below). In this figure, each point represents a position, and for convenience only 2,000 positions from KGS 7.5 Komi games are randomly selected. For each point, the y axis is the value 7.5 of the corresponding position, and the x axis indicates territory based on BV, namely the sum of the ownership probabilities of all board points from BV outputs. The figure shows that both are generally correlated, as indicated by a retrograde line (black) centered around (7.5, 50%). Most points on the right of the vertical line (red) have values (winning probabilities) that are higher than 50%, while most points on the left have values that are lower than 50%. These points indicate a positive correlation between board ownership (from BV) and the probability of winning (from VN)."}, {"heading": "V. CONCLUSIONS", "text": "In fact, it is the case that most of them will be able to hold their own, and that they will be able to achieve their goals, \"he said.\" But it is not the case that they will be able to achieve the goals that they have set themselves. \"He added,\" I do not believe that they will be able to achieve the goals that they have set themselves, and that they will be able to achieve the goals that they have set themselves. \""}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank the Ministry of Science and Technology of the Republic of China (Taiwan) for their financial support of this research under contract numbers MOST 104-2221-E-009-127-MY2, 104-2221-E-009-074-MY2 and 105-2218-E-259-001."}], "references": [{"title": "Searching for Solutions in Games and Artificial Intelligence", "author": ["L.V. Allis"], "venue": "Ph.D. Thesis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2-3),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Balancing MCTS by dynamically adjusting the komi value", "author": ["P. Baudi\u0161"], "venue": "ICGA Journal, 34(3),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Pachi: State of the art open source Go program", "author": ["P. Baudi\u0161", "J.L. Gailly"], "venue": "In Advances in computer games,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A survey of monte carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and AI in games,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Residual Networks for Computer Go", "author": ["T. Cazenave"], "venue": "Accepted by IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "The move decision process of Go Intellect", "author": ["K. Chen"], "venue": "Computer Go, (14),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Training deep convolutional neural networks to play go", "author": ["C. Clark", "A. Storkey"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Computing elo ratings of move patterns in the game of go", "author": ["R. Coulom"], "venue": "In Computer games workshop,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Efficient selectivity and backup operators in Monte Carlo tree search.", "author": ["Coulom", "R\u00e9mi"], "venue": "International Conference on Computers and Games. Springer Berlin Heidelberg,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Fuego\u2014an opensource framework for board games and Go engine based on Monte Carlo tree search", "author": ["M. Enzenberger", "M. Muller", "B. Arneson", "R. Segal"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, 2(4),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Knowledge Representation in The Many Faces of Go, available at http://www.smart-games.com/knowpap.txt", "author": ["D. Fotland"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Combining online and offline knowledge in UCT", "author": ["S. Gelly", "D. Silver"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "The grand challenge of computer Go: Monte Carlo tree search and extensions", "author": ["S. Gelly", "L. Kocsis", "M. Schoenauer", "M. Sebag", "D. Silver", "C. Szepesv\u00e1ri", "O. Teytaud"], "venue": "Communications of the ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Using Deep Convolutional Neural Networks in Monte Carlo Tree Search", "author": ["T. Graf", "M. Platzner"], "venue": "In International Conference on Computers and Games, Springer International Publishing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Message to the computer-go mailing list, Different Rules", "author": ["H. Kato"], "venue": "Available: http://computer-go.org/pipermail/computer-go/2017- March/009995.html,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Message to the computer-go mailing list, Zen lost to Mi Tu Ting", "author": ["H. Kato"], "venue": "Available: http://computer-go.org/pipermail/computer-go/2017- March/009979.html,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In European conference on machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Move evaluation in go using deep convolutional neural networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "arXiv preprint arXiv:1412.6564,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen"], "venue": "Nature, 518(7540),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Message to the computer-go mailing list, Training the value network (a possibly more efficient approach)", "author": ["B. Peng"], "venue": "Available: http://computergo.org/pipermail/computer-go/2017-January/009820.html,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}, {"title": "Multi-armed bandits with episode context", "author": ["C.D. Rosin"], "venue": "Annals of Mathematics and Artificial Intelligence, 61(3),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Reinforcement learning and simulation based search in the game of Go", "author": ["D. Silver"], "venue": "Ph.D. dissertation, Dept. Comput. Sci., Univ. Alberta,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman"], "venue": "Nature, 529(7587),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1999}, {"title": "On-line policy improvement using Monte- Carlo search", "author": ["G. Tesauro", "G.R. Galperin"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1996}, {"title": "Better computer go player with neural network and long-term prediction", "author": ["Y. Tian", "Y. Zhu"], "venue": "arXiv preprint arXiv:1511.06410,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "The authors are with the Department of Computer Science, National Chiao Although the rules of Go are simple, its game tree complexity is extremely high, estimated to be 10 in [1][40].", "startOffset": 175, "endOffset": 178}, {"referenceID": 13, "context": "In the past, computer Go was listed as one of the AI grand challenges [16][28].", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "By 2006, the strengths of computer Go programs were generally below 6 kyu [5][8][14], far away from amateur dan players.", "startOffset": 77, "endOffset": 80}, {"referenceID": 11, "context": "By 2006, the strengths of computer Go programs were generally below 6 kyu [5][8][14], far away from amateur dan players.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 51, "endOffset": 55}, {"referenceID": 26, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 55, "endOffset": 59}, {"referenceID": 3, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 134, "endOffset": 137}, {"referenceID": 8, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "In 2006, Monte Carlo tree search (MCTS) [6][11][15][23][37] was invented and computer Go programs started making significant progress [4][10][13], roughly up to 6 dan in 2015.", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "In 2016, this grand challenge was achieved by the program AlphaGo [34] when it defeated (4:1) Lee Sedol, a 9 dan grandmaster who had won the most world Go champion titles in the past decade.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "Up to date, DeepMind, the team behind AlphaGo, had published the techniques and methods of AlphaGo in Nature [34].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "AlphaGo was able to surpass experts\u2019 expectations by proposing a new method that uses three deep convolutional neural networks (DCNNs) [24][25]: a supervised learning (SL) policy network [7][9][18][26][38] learning to predict experts\u2019 moves from human expert game records, a reinforcement learning (RL) policy network [27] improving the SL policy network via self-play, and a value network that performs state evaluation based on self-play game simulations.", "startOffset": 135, "endOffset": 139}, {"referenceID": 5, "context": "AlphaGo was able to surpass experts\u2019 expectations by proposing a new method that uses three deep convolutional neural networks (DCNNs) [24][25]: a supervised learning (SL) policy network [7][9][18][26][38] learning to predict experts\u2019 moves from human expert game records, a reinforcement learning (RL) policy network [27] improving the SL policy network via self-play, and a value network that performs state evaluation based on self-play game simulations.", "startOffset": 187, "endOffset": 190}, {"referenceID": 7, "context": "AlphaGo was able to surpass experts\u2019 expectations by proposing a new method that uses three deep convolutional neural networks (DCNNs) [24][25]: a supervised learning (SL) policy network [7][9][18][26][38] learning to predict experts\u2019 moves from human expert game records, a reinforcement learning (RL) policy network [27] improving the SL policy network via self-play, and a value network that performs state evaluation based on self-play game simulations.", "startOffset": 190, "endOffset": 193}, {"referenceID": 14, "context": "AlphaGo was able to surpass experts\u2019 expectations by proposing a new method that uses three deep convolutional neural networks (DCNNs) [24][25]: a supervised learning (SL) policy network [7][9][18][26][38] learning to predict experts\u2019 moves from human expert game records, a reinforcement learning (RL) policy network [27] improving the SL policy network via self-play, and a value network that performs state evaluation based on self-play game simulations.", "startOffset": 193, "endOffset": 197}, {"referenceID": 19, "context": "AlphaGo was able to surpass experts\u2019 expectations by proposing a new method that uses three deep convolutional neural networks (DCNNs) [24][25]: a supervised learning (SL) policy network [7][9][18][26][38] learning to predict experts\u2019 moves from human expert game records, a reinforcement learning (RL) policy network [27] improving the SL policy network via self-play, and a value network that performs state evaluation based on self-play game simulations.", "startOffset": 197, "endOffset": 201}, {"referenceID": 27, "context": "AlphaGo was able to surpass experts\u2019 expectations by proposing a new method that uses three deep convolutional neural networks (DCNNs) [24][25]: a supervised learning (SL) policy network [7][9][18][26][38] learning to predict experts\u2019 moves from human expert game records, a reinforcement learning (RL) policy network [27] improving the SL policy network via self-play, and a value network that performs state evaluation based on self-play game simulations.", "startOffset": 201, "endOffset": 205}, {"referenceID": 20, "context": "AlphaGo was able to surpass experts\u2019 expectations by proposing a new method that uses three deep convolutional neural networks (DCNNs) [24][25]: a supervised learning (SL) policy network [7][9][18][26][38] learning to predict experts\u2019 moves from human expert game records, a reinforcement learning (RL) policy network [27] improving the SL policy network via self-play, and a value network that performs state evaluation based on self-play game simulations.", "startOffset": 318, "endOffset": 322}, {"referenceID": 16, "context": ", the program DeepZenGo\u2019s losing match [21] during the World Go Championship [29] against Mi Yuting, one of the top players in the world ranking [12], who, to date, had a highest ranking of being the third strongest player.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "At the time, one of the authors of DeepZenGo proposed a trick [20] to offset the expected inaccuracies by setting komi to 5.", "startOffset": 62, "endOffset": 66}, {"referenceID": 24, "context": "With these new approaches, the ML value network can not only offer games with different komi, but also support dynamic komi (which was not supported in [34]) to improve the game playing strength.", "startOffset": 152, "endOffset": 156}, {"referenceID": 2, "context": "C, and discusses the dynamic komi technique for MCTS [3] in Subsection II.", "startOffset": 53, "endOffset": 56}, {"referenceID": 25, "context": "To further improve the policy network, AlphaGo trained a policy network through self-play via reinforcement learning (RL) [35][36].", "startOffset": 126, "endOffset": 130}, {"referenceID": 28, "context": "The above described process followed the REINFORCE algorithm [41], which is summarized below:", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "With the RL training complete, AlphaGo was able to win 85% of the games played against Pachi [4], an opensource computer Go program.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Monte Carlo Tree Search (MCTS) [6] is a best-first search algorithm on top of a search tree [23][33], using Monte Carlo rollouts to estimate state values.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "Monte Carlo Tree Search (MCTS) [6] is a best-first search algorithm on top of a search tree [23][33], using Monte Carlo rollouts to estimate state values.", "startOffset": 92, "endOffset": 96}, {"referenceID": 23, "context": "Monte Carlo Tree Search (MCTS) [6] is a best-first search algorithm on top of a search tree [23][33], using Monte Carlo rollouts to estimate state values.", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "A tree policy, traditionally the upper confidence bounds (UCB) function [2], is used to select positions (states) from the root to one of the leaves.", "startOffset": 72, "endOffset": 75}, {"referenceID": 22, "context": "AlphaGo used the PUCT algorithm [31] (serving as the tree policy) to select states in a tree, and applied the SL policy network and the value network to MCTS by asynchronous updates.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "To date, none have exploited this idea further except a few discussions given in [30].", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "In traditional computer Go programs with MCTS, dynamic komi [3] is a technique that is widely used to make the program play more aggressively, especially for handicap games.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "There are two methods in [3] that adjust the komi dynamically, providing this incentive to play better in extreme conditions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "(45%, 50%) [3]), or at least as close to the interval as possible.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "In [3], some criterion is also used to avoid oscillation of komi values, especially in the endgame.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "1000 [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "Previous research reported to work fine with the latter [3], while Aya, a notable Go program, chose to use the former [42].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "In [3], the author also mentioned the possibility of increasing or decreasing by more than one when using the VS method.", "startOffset": 3, "endOffset": 6}, {"referenceID": 27, "context": "As a side note, we trained the SL policy network with 3-step prediction, which was proposed in [38].", "startOffset": 95, "endOffset": 99}, {"referenceID": 3, "context": "resulting strength of our policy network won 70% of games against Pachi [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 24, "context": "5 is used, like the way the value is mixed from the rollouts and the value network in [34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "The formula of komi rate is the same as [3].", "startOffset": 40, "endOffset": 43}, {"referenceID": 24, "context": "To measure the quality of these value networks, we analyze their MSEs like AlphaGo [34] did.", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "5 only and one-stone handicap was valued at about 7 points difference according to the experiments in [3], we only analyze 1-stone to 4-stone handicaps, in addition to even games.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Second, while the VS method performed the best in [3], in our experiments, the VS-M method did not perform well.", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "In [3], since only the rollout was used, it performed relatively well.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "This paper proposes a new approach to a novel value network architecture for the game Go, called a multi-labelled (ML) value network. In the ML value network, different values (win rates) are trained simultaneously for different settings of komi, a compensation given to balance the initiative of playing first. The ML value network has three advantages, (a) it outputs values for different komi, (b) it supports dynamic komi, and (c) it lowers the mean squared error (MSE). This paper also proposes a new dynamic komi method to improve game-playing strength. This paper also performs experiments to demonstrate the merits of the architecture. First, the MSE of the ML value network is generally lower than the value network alone. Second, the program based on the ML value network wins by a rate of 67.6% against the program based on the value network alone. Third, the program with the proposed dynamic komi method significantly improves the playing strength over the baseline that does not use dynamic komi, especially for handicap games. To our knowledge, up to date, no handicap games have been played openly by programs using value networks. This paper provides these programs with a useful approach to playing handicap games. Keywords\u2014Value Network, Policy Network, Supervised Learning, Reinforcement Learning, Dynamic Komi, Computer Go,", "creator": "Microsoft\u00ae Word 2013"}}}