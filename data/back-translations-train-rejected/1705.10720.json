{"id": "1705.10720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Low Impact Artificial Intelligences", "abstract": "There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.", "histories": [["v1", "Tue, 30 May 2017 16:15:16 GMT  (1174kb,D)", "http://arxiv.org/abs/1705.10720v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["stuart armstrong", "benjamin levinstein"], "accepted": false, "id": "1705.10720"}, "pdf": {"name": "1705.10720.pdf", "metadata": {"source": "CRF", "title": "Low Impact Artificial Intelligences", "authors": ["Stuart Armstrong", "Benjamin Levinstein"], "emails": ["stuart.armstrong@philosophy.ox.ac.uk;", "balevinstein@gmail.com"], "sections": [{"heading": null, "text": "Keywords: low impact, AI, motivation, value, control"}, {"heading": "1 Introduction", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2 The General Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The penalty function", "text": "The basic idea is that AI has an active goal, such as curing cancer or filtering spam, but wants to pursue that goal without changing the world in any way. We can then describe its usefulness function as follows: U = u \u2212 \u00b5R. (1) Function u is a standard usefulness function that gives the AI its active goal. Function R is the streamlining function that punishes the AI for having a great influence. \u00b5 is a scaling factor that determines the importance of low impacts in relation to the active goal of AI. To prevent the AI from accepting a high R penalty in return for a large U gain, we will want to define a limited u so that performance close to the maximum limit is not too difficult to achieve."}, {"heading": "2.2 Defining the alternatives", "text": "Most natural alternatives, and the ones we use, are the world in which the AI has never been successfully turned on to have very little impact; the behavior of the active AI will depend on this baseline. For such a distribution, which makes sense, we assume that the turn on of the AI is not accompanied by the likelihood that it will. For example, we can make it dependent on a signal going through a particular environment - say, an unstable gas - that has a tiny chance of interrupting the signal."}, {"heading": "2.3 Boxing the AI", "text": "Unlike other boxing proposals [ASB12, Bos14, Yam12], the aim here is not to prevent AI from exercising influence by limiting it, but rather to minimise the difference between worlds with X and \u00ac X. The AI is not to be prevented from acting (the definition of low influence will ensure this), but to prevent us from seeing that AI is active. Therefore, all visible effects of AI will only arise from AI decisions and not from its very existence."}, {"heading": "3 Different measures of low impact", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The fundamental challenge", "text": "The fundamental challenge in developing the low-impact approach is that, from a physical point of view, everything that is done or not done has an impact on all possible future scenarios (at least one probable one). Therefore, the world must be fundamentally different from the world in the face of X, in the face of X, simply because of the difference between X and X. Particles are arranged differently in these different scenarios, and this effect will exacerbate the further progression of the event.What really interests us is \"no major impact.\" It matters little if the atoms defining the US president are a little to the left of where they would otherwise be. It is of great importance that the AI does not get out of its box, eliminates all people, and optimizes the universe in the service of the goal. A second component of this challenge is that the AI may end up creating a significantly different world simply because some processes, such as the weather, and X versus X change the starting conditions."}, {"heading": "3.2 Coarse graining: Twenty billion questions", "text": "The question that arises is whether this is a way in which people put themselves and themselves at the centre, a way in which they put themselves at the centre, a way in which they put themselves at the centre, a way in which they put themselves at the centre, a way in which they put themselves at the centre, a way in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre and in which they put themselves at the centre."}, {"heading": "3.3 The importance of knowing X", "text": "The intuitive idea is that if the AI has a high impact, knowing whether the AI has been turned on or not says much more about the future of the world than any other generic information. We can then set the penalty R to measure the importance of X / \u00ac X in relation to other evidence. If the AI has a really low impact, then when it is presented with a large number of facts about the world, X will not stand out as more informative than other facts.How would we measure the meaning? One approach is to look at a large U of different utility functions and check whether the knowledge of X makes little expected difference to one of them. \u2212 Note that this is in contrast to more traditional AI approaches, where the terms must be correctly defined [Yud08] or based [Har90]. The purpose is not to have the meaning of each variable precisely defined, but to grasp so many variables that they limit the AI's ability to have great capabilities."}, {"heading": "3.4 Undetectable means unimpactful?", "text": "These sections look at a different way of defining low impact: undetectability. If there was a resident of London in the 19th century, and there is no record of any kind of existence, it is likely that they had a very small impact. Let's suppose that we are given a partial description of their impact, and ask ourselves, from the point of view of today and all the evidence we have, whether they existed or not. If we really can't tell how any real agent can look like this or that, this seems to be a well-defined example of low impact: their potential impact is literally lost in the noise of the world. How could this be formalized? Suppose that the AI uses a probability estimator, the P. \"This estimator is limited by practical concerns, since a real-world agent can have only limited rationality. Its limitless equivalent is P. And these estimators have a certain degree of reflection [CYHB13], so that P can\" make some estimates about the outcome of a flexible agent (P) and (P) agent expression."}, {"heading": "3.5 Comparing distributions abstractly", "text": "In order to avoid some of the problems and decisions required for the options discussed so far, we can try to compare the two conditional probability distributions P (\u2212 | X) and P (\u2212 | \u00ac X) abstractly. The intuition behind this is that if we have little or no artificial intelligence, the probability distribution over future worlds is probably spread far and wide over many different options, but if there is a powerful, unrestricted artificial intelligence, the future will most likely be highly optimized, giving P (\u2212 | X) a different form. Therefore, we can compare conditional distributions by looking at different descriptors that they might have - means, variances, peaks, etc. Such descriptors depend on the underlying metric on the amount of worlds W, and therefore ideally these descriptors will be very similar for a wide variety of natural metrics that we could choose. The weakness of this approach is that we formally know an extremely reduced distribution, so that we do not have a strong understanding of what is going on in front of W relative terms, but whether W is a relatively strong distribution."}, {"heading": "3.5.1 Generalised Cross-Entropy", "text": "A natural abstract method of measuring the expected effects of X is to compare the divergence between P (W | X) in relation to P (W | \u00ac X). If the two distributions are relatively close to each other, then X probably does not have a particularly large impact. Unfortunately, it is not obvious what specific measure of divergence we should apply. Kullback-Leibler divergence - the default size - will not work in this case. Let us allow PX = P (W | X) and P \u00ac X = P (W | \u00ac X). PX (X) = 1 and P \u00ac X (X) = 0, i.e. DKL (P'X | PX) = \u221e. However, there are other measures of generalized entropy and divergence that are limited and can do their job. Bound Bregman divergences, for example, are often used to quantify the amount of generalized information needed to move from one probability function to another. 11 Whether such an approach remains to work remains to be seen."}, {"heading": "4 High impact from low impact", "text": "Of course, we don't want low impacts overall - we want low negative impacts. The problem is that we can't successfully define these negative impacts in advance, so how can we ensure that we actually achieve some degree of positive impact through the use of such AIs?"}, {"heading": "4.1 Calibrating the penalty function", "text": "The most obvious option is to put \"the dial\" into equation (1) by changing the value of \"\u00b5.\" We can start with a very large \"\u00b5\" that has no effect at all - the AI will do nothing. We can then gradually decrease \"\u00b5\" until we get an action that actually increases, but that doesn't seem very safe. The first problem is that we have little understanding of the correct value of \"\u00b5,\" so little understanding of the correct rate at which we reduce \"\u00b5.\" It is conceivable that we spend a million steps to reduce \"\u00b5\" through the realm of \"doing nothing,\" and that the next step goes beyond the \"safe increase of\" directly into the realm of \"dangerous effects.\" In other words, there could be a hasty jump from the level at which \"\u00b5R\" u dominates, \"to the level at which u is sufficiently constrained by\" nothing, \"and that the next step moves\" directly beyond the safe range of \"u to the\" increase. \""}, {"heading": "4.2 Unsafe output channel", "text": "To ensure that P (O | \u00ac X) is 6 = 0, we set up an alternative system that generates a random message. Then, we exclude the content of O from consideration of minor effects. We can, for example, modify the coarse-grained approach by replacing the above equation (2) with R = maxw | P (Vw | X, O) \u2212 P (Vw | \u00ac X, O) |. Alternatively, we can abstractly compare P (W | X, O) with P (W | \u00ac X, O) or consider the equation (3) to be: R = min \u00b2 E \u00b2 (P (G | X, B, O) P (G | \u00ac X, O) as proof of such a miscalculation."}, {"heading": "4.3 WBE, chatbots, and conditioning on results", "text": "Indeed, unless we have something else in our hands other than being able to find a solution."}, {"heading": "4.4 High impact from multiple conditional low impact", "text": "Let's assume that an asteroid is heading toward Earth and that we are aiming for a laser to deflect it. If we ask a single precipitating artificial intelligence to do so, a small impact will prevent it from doing so. But let's now imagine that there are two artificial intelligence, Alice and Bob, and that they are activated by events X and Y. Each is indifferent or almost indifferent to how the world ends since the other one is turned on. However, under the condition that Bob is turned off, Alice is a low impact, but he wishes to give the correct x-coordinate for the laser's targeting (this defines uAlice). Likewise, due to Alice being turned off, Bob is a low impact, but wishes to give the correct y-coordinate. So, because Alice only cares about worlds where \u00ac Y, even if she is highly confident that Y will output the correct x-coordinates, she will still lack the correct x-coordinates for both of them, in addition to the euro, we will have to output the correct x-coordinates."}, {"heading": "4.5 Extension of category concepts", "text": "The previous example of \"targeted laser\" does not allow artificial intelligence to have a (desired and targeted) high effect in general situations. For example, it does not allow artificial intelligence to walk around and notice that the other artificial intelligence is actually on - it only works because they do not know that the other artificial intelligence has been activated. How could we apply this approach more broadly? One potential idea is that artificial intelligence derives general low-impact concepts that depend on the X situation - concepts such as \"do not fill the universe with self-replicating probes\" or \"do not take over the world\" provided that the other artificial intelligence is not activated. Then, we could hope that these concepts can be generalized to cover the X situation as well. Specifically, we could imagine that artificial intelligence creates the code for a subagent by using these concepts, and that the subagents may interact with the world and possibly the subagents of the world, and do the things with the others."}, {"heading": "5 Known issues", "text": "In fact, it is a pure fabric that is able to hide itself in a position."}], "references": [{"title": "Motivated value selection for artificial agents", "author": ["Stuart Armstrong"], "venue": "presented at the 1st International Workshop on AI and Ethics,", "citeRegEx": "Armstrong.,? \\Q2015\\E", "shortCiteRegEx": "Armstrong.", "year": 2015}, {"title": "Thinking inside the box: Controlling and using an oracle ai", "author": ["Stuart Armstrong", "Anders Sandberg", "Nick Bostrom"], "venue": "Minds and Machines,", "citeRegEx": "Armstrong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Armstrong et al\\.", "year": 2012}, {"title": "Superintelligence: Paths, dangers, strategies", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2014\\E", "shortCiteRegEx": "Bostrom.", "year": 2014}, {"title": "Definability of truth in probabilistic logic", "author": ["Paul Christiano", "Eliezer Yudkowsky", "Marcello Herreshoff", "Mihaly Barasz"], "venue": "MIRI Early Draft,", "citeRegEx": "Christiano et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Christiano et al\\.", "year": 2013}, {"title": "Learning what to value. In Artificial General Intelligence, pages 309\u2013314", "author": ["Daniel Dewey"], "venue": null, "citeRegEx": "Dewey.,? \\Q2011\\E", "shortCiteRegEx": "Dewey.", "year": 2011}, {"title": "Game theory, maximum entropy, minimum discrepancy, and robust bayesian decision theory", "author": ["P.D. Gr\u00fcnwald", "A.P. Dawid"], "venue": "Annals of Statistics,", "citeRegEx": "Gr\u00fcnwald and Dawid.,? \\Q2004\\E", "shortCiteRegEx": "Gr\u00fcnwald and Dawid.", "year": 2004}, {"title": "Fact, fiction, and forecast", "author": ["Nelson Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q1983\\E", "shortCiteRegEx": "Goodman.", "year": 1983}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Tilmann Gneiting", "Adrian E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting and Raftery.,? \\Q2007\\E", "shortCiteRegEx": "Gneiting and Raftery.", "year": 2007}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Basic ai drives", "author": ["S. Omohundro"], "venue": "In Proceedings of the First AGI Conference,", "citeRegEx": "Omohundro.,? \\Q2008\\E", "shortCiteRegEx": "Omohundro.", "year": 2008}, {"title": "Whole brain emulation. a roadmap", "author": ["Anders Sandberg", "Nick Bostrom"], "venue": "Future of Humanity Institute. Technical Report,", "citeRegEx": "Sandberg and Bostrom.,? \\Q2008\\E", "shortCiteRegEx": "Sandberg and Bostrom.", "year": 2008}, {"title": "Leakproofing the singularity: artificial intelligence confinement problem", "author": ["Roman V. Yampolskiy"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Yampolskiy.,? \\Q2012\\E", "shortCiteRegEx": "Yampolskiy.", "year": 2012}, {"title": "Artificial intelligence as a positive and negative factor in global risk", "author": ["Eliezer Yudkowsky"], "venue": "Global catastrophic risks,", "citeRegEx": "Yudkowsky.,? \\Q2008\\E", "shortCiteRegEx": "Yudkowsky.", "year": 2008}], "referenceMentions": [], "year": 2017, "abstractText": "There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of \u2018low impact\u2019. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.", "creator": "LaTeX with hyperref package"}}}