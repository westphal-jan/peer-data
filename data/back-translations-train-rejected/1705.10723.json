{"id": "1705.10723", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Fast Regression with an $\\ell_\\infty$ Guarantee", "abstract": "Sketching has emerged as a powerful technique for speeding up problems in numerical linear algebra, such as regression. In the overconstrained regression problem, one is given an $n \\times d$ matrix $A$, with $n \\gg d$, as well as an $n \\times 1$ vector $b$, and one wants to find a vector $\\hat{x}$ so as to minimize the residual error $\\|Ax-b\\|_2$. Using the sketch and solve paradigm, one first computes $S \\cdot A$ and $S \\cdot b$ for a randomly chosen matrix $S$, then outputs $x' = (SA)^{\\dagger} Sb$ so as to minimize $\\|SAx' - Sb\\|_2$.", "histories": [["v1", "Tue, 30 May 2017 16:20:34 GMT  (129kb,D)", "http://arxiv.org/abs/1705.10723v1", "ICALP 2017"]], "COMMENTS": "ICALP 2017", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["eric price", "zhao song", "david p woodruff"], "accepted": false, "id": "1705.10723"}, "pdf": {"name": "1705.10723.pdf", "metadata": {"source": "CRF", "title": "Fast Regression with an `\u221e Guarantee\u2217", "authors": ["Eric Price", "Zhao Song", "David P. Woodruff"], "emails": ["zhaos@utexas.edu", "zhaos@utexas.edu", "dpwoodru@us.ibm.com"], "sections": [{"heading": null, "text": "The sketch and solution paradigm gives a limit to the problem of \"x\" \u2212 x \"2 if A is\" well conditioned. \"Our main result is that if S is the subsampled randomized Fourier / Hadamard transformation, the error\" x \"\u2212 x\" behaves as if it were in a \"random\" direction within that limit: For each fixed direction we have an \"x\" -c probability that < a, x \"\u2212 x\" >. There is also a better limit for generalizing \"x\" and \"2d,\" (1) where \"c\" > 0 \"are arbitrary constants. This means that\" x \"\u2212 x\" -x \"is a factor smaller than\" x. \"There is also a better limit for generalizing\" x \"to new examples: If rows of\" A correspond to the examples and columns, then our boundary for the sketch and solution paradigm introduced gives a better result."}, {"heading": "1 Introduction", "text": "It is a generalization of the matrices S-Rm \u00b7 n with such a problem that for each d-dimensional sub-space U-Rn, we maintain with \"high\" probability the norm of each vector in sub-space. (1) It is a generalization of the classic JohnsonLindenstrauss problem of vectors in sub-spaces. (1) It is a generalization of vectors in sub-spaces. (1) It is a generalization of vectors in sub-spaces. (1 + 2) It is a great application of OSEs to regression. (2) The regression problem is, given the b-Rn and A-Rn \u00b7 d for formatting. (2) It is a great application of OSEs to regression. (2) The regression is there b-Rn and A-Rn \u00b7 d for formatting."}, {"heading": "1.1 Our Contributions", "text": "Despite the success of using subspace descriptions to accelerate regression, what is often of interest to practitioners is not the maintenance of the cost of the regression problem, but rather the generalization or prediction error caused by the x vector. \"Ideally, for each future (unseen) classification, we would aim for one (RLSC) [RYP03], which has been found in cases that can be performed just as well as support vector machines, but is much simpler [ZP04]. In this application, a series of examples is identified with multiple (non-binary) labels identified with the rows of an n \u00d7 d matrix A."}, {"heading": "1.1.1 Comparison to Gradient Descent", "text": "While this work is primarily about sketching methods, one could instead use iterative methods such as gradient descent after preconditioning the matrix accordingly, see e.g. [AMT10, ZF13, CW13]. This means that one can use an OSE with constant \u03b5 to construct a preconditioner for A and then execute conjugate gradients with the preconditioner, resulting in a general dependence on Log (1 /).The main drawback of this approach is that one loses the ability to save memory or number of runs when A appears in a stream, or to save communication or rounds when A is distributed. Given the increasing number of datasets, such scenarios are now quite common, see e.g. [CW09] for regression algorithms in the data stream model. In situations where the inputs of A appear sequential, for example, one does not need to compress a series at a time while one cannot store the full matrix \u00d7 A, but only needs to store the matrix \u00d7 A."}, {"heading": "1.1.2 Proof Techniques", "text": "This indicates that our analysis is non-standard and cannot use generic properties of OSEs as a black box. Therefore, in our analysis, we must consider matrix products of the form S > S (UU > S) k, which make analysis considerably more difficult and do not allow us to appeal to standard results on approximate matrix products (see, for example, Woo14) for a survey. The key idea is that it is the same matrix S (UU > S) k with a property of S. We use properties that are only suitable for OSEs S specifications: first, that each column of S is a vector; and second, that for all pairs (i > S > S) and i (i > S > S) a property of S. We use properties that are only suitable for OSEs S specifications."}, {"heading": "1.2 Notation", "text": "For a positive integer, leave [n] = {1, 2,.., n}. For a vector x-Rn, define \"A-Rm-n\" = (\u2211 n-i = 1 x-2 i) 1-2 and \"A-Rm-n.\" For a matrix A-Rm-n, define \"A-Rm-n\" = supx-Ax-2 / x-2 as the spectral norm of \"A\" and \"A-Rm-F\" = (\"I, j-A-2 i, j) 1 / 2 as the Frobenius norm of\" A. We use \"A\" to denote the Moore-Penrose pseudo-inverse of \"m x-n-matrix A,\" which, if A-Rm-V-F > is their SVD (where U-Rm-n-n-Rn-Rn-Rn-Rn-Rn-Rn-Rn-Rn-Rn-Rn-Rn-n-n-n-n-n-n-r-n-r-r-r-form is."}, {"heading": "2 Warmup: Gaussians OSEs", "text": "First, we show that if S is a Gaussian random matrix, it fulfills the generalization warranty.This results from the rotational invariance of the Gaussian distribution.Theorem 7. Suppose A-Rn \u00b7 d has the full column rank. If the entries of S-Rm \u00b7 n are i.i.d. N (0, 1 / m), m = O (d / \u03b52), then we have for arbitrary vectors a, b and x \u043c = A \u2020 b, with the probability 1 \u2212 1 / poly (d), | a > (SA) \u2020 Sb \u2212 a > x \u0445 |."}, {"heading": "3 SRHT Matrices", "text": "We first provide the definition of the subsampled Randomized Hadamard transform (SRHT = > SRHT): let S = > p = > P = > P = > P = > P = > P = > S = > S = > S = > S = > S = > S = > S = > S = > S = > P = [1]. The matrix Hn is the hadamard matrix of size n \u2212 n, and we assume that n \u2212 S has a power of 2. Here, Hn = [Hn / 2, \u2212 Hn / 2] and H1 = [1]. The r \u00b7 n matrix P patterns of a n dimensional vector uniformity at random.For other sub-space settings, we no longer have SU and Sb being independent; to analyze them, we start with an assertion that allows us to make the inverse of a matrix into a serial claim."}, {"heading": "4 Proof of Lemma 9", "text": "There is also < Si > = 1 for all i and | < Si, Sj > |. \u00b7 Log (1 / 3). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4). \u00b7 Log (1 / 4)."}, {"heading": "5 Lower bound for `2 and `\u221e guarantee", "text": "D (D). D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "A Proof for Gaussian case", "text": "Lemma 19 \u2212 \u2212 \u2212 Rm \u00b7 n is i. \u2212 > > > Actual number N (0, 1 / m), m = O (d / \u03b52) and U > b = 0, then \u2212 a > (SA) \u2020 Sb |. Matrix SA has linear independent columns, and so is (SA) \u2020 for all vectors a, b with probability 1 \u2212 1 / poly (d).Proof. With probability 1 is Matrix SA with linear independent columns, and so is (SA) \u2020 = (A > S > SA) \u2212 1A > S > = (V) > S > S > (S > SUV > probability) \u2212 1V > S > S > = V. Proof. \u2212 1 (U > S > V > SU) \u2212 1V > V > V > V > V > V > U > (U > S > S), the most likely."}, {"heading": "B Combining Different Matrices", "text": "In some cases, it may be useful to combine different matrices that meet the generalization limit. (Theorem 11) Let us A-B-D, and let R-Rm-r-r and S-Rr-n be derived from distributions of matrices that are located depending on the generalization limit and meet the generalization limit (6). Then, RS satisfies the generalization, which is based on a constant factor loss in the probability of failure and approximation factory.Proof B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-"}, {"heading": "C Count-Sketch does not obey the `\u221e guarantee", "text": "Here we show that such a matrix is an OSE with probability, in which there is a constant that is not met with constant probability, so that such matrices cannot meet the generalization guarantee (6) with high probability. Theorem 20. Let us leave the \"true\" solution x = A \u2020 b and the approximation x \u00b2 per column. There is a matrix A \u00b2 Rn \u00b7 d and b \u00b2 Rn such that if s2d. m = d3s, then the \"true\" solution x = A \u00b2 and the approximation x \u00b2 = (SA) \u2020 Sb \u00b2 s great distance with constant probability: x \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2."}, {"heading": "D Leverage score sampling does not obey the `\u221e guarantee", "text": "Leverage Scores Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) \"Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Definition 22 (Leverage Scores) Leverage 22 (Leverage) Leverage 22 (Leverage) (Leverage) 22 (Leverage) (Leverage) 22 (Leverage) 22 (Leverage) (Leverage) 22 (Leverage) 22 (Leverage) (Leverage) Definitions 22 (Leverage) Definitions 22 (Leverage) Definitions 22 (Leverage) Definitions (Leverage) Definitions 22 (Leverage) Definitions (Leverage) Definitions 22 (Leverage) Definitions (Leverage) Definitions 22 (Leverage) Definitions (Leverage) Definitions 22 (Leverage) Definitions (Leverage) Definitions (Leverage) Definitions 22 (Leverage) Definitions (Leverage) Definitions 22 (Leverage) Definitions (Leverage (Leverage) Definitions (Leverage) Definitions 22 (Leverage) Definitions (Leverage (Leverage) Definitions (Leverage) Definitions 22 (Leverage (Leverage) Definitions (Leverage) Definitions (Leverage (Leverage) Definitions (Leverage (Leverage Scores) Definitions (Leverage) Definitions (Leverage (Leverage) Definitions (Leverage (Leverage) Definitions (Leverage (Leverage Scores) Definitions (Leverage (Leverage) Definitions (Leverage) Definitions (Leverage (Leverage) Definitions (Leverage) Definitions ("}, {"heading": "E Bounding E[\u2016Z\u20162F ]", "text": "Before taking evidence, we define the key property of S = = = < K = = > Definition 24 (All Inner Product Small (AIPS) Property). \u2212 K = > Definition 24 (All Inner Product Small (AIPS) Property). \u2212 K \u2212 K \u2212 K \u00b7 S \u00b7 1 \u00b7 \u00b7 \u00b7 S \u00b7 1 \u00b7 0 \u00b7 S \u00b7 1 \u00b7 0 \u00b7 S \u00b7 0 \u00b7 0 \u00b7 S \u00b7 0 \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 0 \u00b7 S \u00b7 2 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 1 \u00b7 S \u00b7 1 \u00b7 0 \u00b7 S \u00b7 0 \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 0 \u00b7 S \u00b7 2 \u00b7 S \u00b7 0 \u00b7 S \u00b7 S \u00b7 0 \u00b7 S \u00b7 S \u00b7 0 \u00b7 S \u00b7 0, SJ \u00b7 S \u00b7 n \u00b7 n)."}], "references": [{"title": "On computing inverse entries of a sparse matrix in an out-ofcore environment", "author": ["Patrick Amestoy", "Iain S. Duff", "Jean-Yves L\u2019Excellent", "Yves Robert", "Fran\u00e7ois-Henry Rouet", "Bora U\u00e7ar"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Amestoy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Amestoy et al\\.", "year": 2012}, {"title": "Blendenpik: Supercharging lapack\u2019s least-squares solver", "author": ["Haim Avron", "Petar Maymounkov", "Sivan Toledo"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Avron et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2010}, {"title": "Toward a unified theory of sparse dimensionality reduction in euclidean space", "author": ["Jean Bourgain", "Sjoerd Dirksen", "Jelani Nelson"], "venue": "In STOC,", "citeRegEx": "Bourgain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bourgain et al\\.", "year": 2015}, {"title": "Ridge leverage scores for low-rank approximation", "author": ["Michael B Cohen", "Cameron Musco", "Christopher Musco"], "venue": "arXiv preprint arXiv:1511.07263,", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Optimal approximate matrix product in terms of stable rank", "author": ["Michael B Cohen", "Jelani Nelson", "David P. Woodruff"], "venue": "In Proceedings of the 43rd International Colloquium on Automata, Languages and Programming (ICALP 2016),", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Nearly tight oblivious subspace embeddings by trace inequalities", "author": ["Michael B. Cohen"], "venue": "In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Cohen.,? \\Q2016\\E", "shortCiteRegEx": "Cohen.", "year": 2016}, {"title": "Matrix multiplication via arithmetic progressions", "author": ["Don Coppersmith", "Shmuel Winograd"], "venue": "J. Symb. Comput.,", "citeRegEx": "Coppersmith and Winograd.,? \\Q1990\\E", "shortCiteRegEx": "Coppersmith and Winograd.", "year": 1990}, {"title": "Numerical linear algebra in the streaming model", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2009\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2012}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["Fran\u00e7ois Le Gall"], "venue": "In International Symposium on Symbolic and Algebraic Computation, ISSAC \u201914,", "citeRegEx": "Gall.,? \\Q2014\\E", "shortCiteRegEx": "Gall.", "year": 2014}, {"title": "How many entries of a typical orthogonal matrix can be approximated by independent normals", "author": ["Tiefeng Jiang"], "venue": "The Annals of Probability,", "citeRegEx": "Jiang,? \\Q2006\\E", "shortCiteRegEx": "Jiang", "year": 2006}, {"title": "Computing entries of the inverse of a sparse matrix using the FIND algorithm", "author": ["Song Li", "Shaikh S. Ahmed", "Gerhard Klimeck", "Eric Darve"], "venue": "J. Comput. Physics,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Faster ridge regression via the subsampled randomized hadamard transform", "author": ["Yichao Lu", "Paramveer Dhillon", "Dean Foster", "Lyle Ungar"], "venue": "In Proceedings of the Neural Information Processing Systems (NIPS) Conference,", "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "Annals of Statistics,", "citeRegEx": "Laurent and Massart.,? \\Q2000\\E", "shortCiteRegEx": "Laurent and Massart.", "year": 2000}, {"title": "Iterative row sampling", "author": ["Mu Li", "Gary L Miller", "Richard Peng"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Meng and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Meng and Mahoney.", "year": 2013}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L Nguy\u00ean"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "Lower bounds for oblivious subspace embeddings. In Automata, Languages, and Programming ", "author": ["Jelani Nelson", "Huy L. Nguy\u00ean"], "venue": "41st International Colloquium,", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2014\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2014}, {"title": "Provable deterministic leverage score sampling", "author": ["Dimitris Papailiopoulos", "Anastasios Kyrillidis", "Christos Boutsidis"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Papailiopoulos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Papailiopoulos et al\\.", "year": 2014}, {"title": "Regularized least-squares classification", "author": ["Ryan Rifkin", "Gene Yeo", "Tomaso Poggio"], "venue": "Nato Science Series Sub Series III Computer and Systems Sciences,", "citeRegEx": "Rifkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2003}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tam\u00e1s Sarl\u00f3s"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Sarl\u00f3s.,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s.", "year": 2006}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Multiplying matrices faster than coppersmithwinograd", "author": ["Virginia Vassilevska Williams"], "venue": "In Proceedings of the 44th Symposium on Theory of Computing Conference,", "citeRegEx": "Williams.,? \\Q2012\\E", "shortCiteRegEx": "Williams.", "year": 2012}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}, {"title": "Randomized extended kaczmarz for solving least squares", "author": ["Anastasios Zouzias", "Nikolaos M. Freris"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Zouzias and Freris.,? \\Q2013\\E", "shortCiteRegEx": "Zouzias and Freris.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "Sketching has emerged as a powerful technique for speeding up problems in numerical linear algebra, such as regression. In the overconstrained regression problem, one is given an n \u00d7 d matrix A, with n d, as well as an n \u00d7 1 vector b, and one wants to find a vector x\u0302 so as to minimize the residual error \u2016Ax\u2212 b\u20162. Using the sketch and solve paradigm, one first computes S \u00b7 A and S \u00b7 b for a randomly chosen matrix S, then outputs x\u2032 = (SA)\u2020Sb so as to minimize \u2016SAx\u2032 \u2212 Sb\u20162. The sketch-and-solve paradigm gives a bound on \u2016x\u2032 \u2212 x\u2217\u20162 when A is well-conditioned. Our main result is that, when S is the subsampled randomized Fourier/Hadamard transform, the error x\u2032 \u2212 x\u2217 behaves as if it lies in a \u201crandom\u201d direction within this bound: for any fixed direction a \u2208 R, we have with 1\u2212 d\u2212c probability that \u3008a, x\u2032 \u2212 x\u2217\u3009 . \u2016a\u20162\u2016x \u2032 \u2212 x\u2217\u20162 d 1 2\u2212\u03b3 , (1) where c, \u03b3 > 0 are arbitrary constants. This implies \u2016x\u2032 \u2212 x\u2217\u2016\u221e is a factor d 1 2\u2212\u03b3 smaller than \u2016x\u2032 \u2212 x\u2217\u20162. It also gives a better bound on the generalization of x\u2032 to new examples: if rows of A correspond to examples and columns to features, then our result gives a better bound for the error introduced by sketch-and-solve when classifying fresh examples. We show that not all oblivious subspace embeddings S satisfy these properties. In particular, we give counterexamples showing that matrices based on Count-Sketch or leverage score sampling do not satisfy these properties. We also provide lower bounds, both on how small \u2016x\u2032 \u2212 x\u2217\u20162 can be, and for our new guarantee (1), showing that the subsampled randomized Fourier/Hadamard transform is nearly optimal. Our lower bound on \u2016x\u2032 \u2212 x\u2217\u20162 shows that there is an O(1/\u03b5) separation in the dimension of the optimal oblivious subspace embedding required for outputting an x\u2032 for which \u2016x\u2032\u2212 x\u2217\u20162 \u2264 \u2016Ax\u2217\u2212 b\u20162 \u00b7 \u2016A\u2020\u20162, compared to the dimension of the optimal oblivious subspace embedding required for outputting an x\u2032 for which \u2016Ax\u2032 \u2212 b\u20162 \u2264 (1 + )\u2016Ax\u2217 \u2212 b\u20162, that is, the former problem requires dimension \u03a9(d/ ) while the latter problem can be solved with dimension O(d/ ). This explains the reason known upper bounds on the dimensions of these two variants of regression have differed in prior work. \u2217A preliminary version of this paper appears in Proceedings of the 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017). ar X iv :1 70 5. 10 72 3v 1 [ cs .D S] 3 0 M ay 2 01 7", "creator": "LaTeX with hyperref package"}}}