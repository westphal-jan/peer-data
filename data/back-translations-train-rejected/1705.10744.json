{"id": "1705.10744", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Knowledge Base Completion: Baselines Strike Back", "abstract": "Many papers have been published on the knowledge base completion task in the past few years. Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18. This paper shows that the accuracy of almost all models published on the FB15k can be outperformed by an appropriately tuned baseline - our reimplementation of the DistMult model. Our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyper-parameter tuning or different training objectives. This should prompt future research to re-consider how the performance of models is evaluated and reported.", "histories": [["v1", "Tue, 30 May 2017 16:54:19 GMT  (43kb,D)", "http://arxiv.org/abs/1705.10744v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["rudolf kadlec", "ondrej bajgar", "jan kleindienst"], "accepted": false, "id": "1705.10744"}, "pdf": {"name": "1705.10744.pdf", "metadata": {"source": "CRF", "title": "Knowledge Base Completion: Baselines Strike Back", "authors": ["Rudolf Kadlec", "Ondrej Bajgar", "Jan Kleindienst"], "emails": ["jankle}@cz.ibm.com", "Hits@10"], "sections": [{"heading": "1 Introduction", "text": "Projects such as Wikidata1 or earlier Freebase (Bollacker et al., 2008) have successfully amassed an enormous amount of knowledge in the form of < entity1 - Relationship - Entity 2 > Triplets. Given this vast knowledge, it would be extremely useful to teach machines to think about such knowledge bases. One possible way to test such considerations is to complete the Knowledge Base (KBC).The goal of the KBC task is to translate the missing piece of information into an incomplete threefold. For example, in the face of a query < Donald Trump, President of,? > one should predict that the target unit is an entity that can be evaluated by a series of entities E and a series of binary relationships R about these entities, a knowledge base (sometimes referred to as Knowl-1https: / www.wikidata.org / edge graph) that the target unit is an entity that can be evaluated by a series of entities; a series of entities and an array of entities."}, {"heading": "2 The Model", "text": "Inspired by the success of word embedding in the processing of natural language, distribution models for KBC have recently been extensively studied. Distribution models represent the units and sometimes even the relationships as N-dimensional real vectors 3, we will use these vectors in bold, h, r, t RN.2We could even say too simple, as they presuppose symmetry of all relationships, which is clearly unrealistic.3Some models present relationships as matrices instead.ar Xiv: 170 5.10 744v 1 [cs.L G] 30 May 201 7The DistMult model was introduced by Yang et al. (2015). Subsequently, Toutanova and Chen (2015) achieved better empirical results with the same model, changing hyperparameters of the training process and negative logging probability of the softmax model instead of L1-based ranking losses."}, {"heading": "3 Experiments", "text": "It is not only a question of reason, but also a question of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason and of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason and of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason, of reason and of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason, of reason and of reason, of reason, of reason, of reason and of reason, of reason, of reason, of reason and of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason and of reason, of reason, of reason, of reason, of reason and of reason, of reason, of reason and of reason, of reason, of reason and of reason and of reason, of reason, of reason and of reason, of reason, of reason, of reason, of reason and of reason and of reason"}, {"heading": "3.1 Hyper-parameter influence on FB15k", "text": "In our experiments with FB15k, we found that increasing the number of negative examples M has a positive effect on performance. Another interesting observation is that the batch size has a strong influence on final performance. Larger batch size always leads to better results, e.g. Hits @ 10 improved by 14.2% in absolute terms when the batch size was increased from 16 to 2048. See Figure 1. Compared to previous work that trained DistMult on these data sets (results see Table 2 below), we use different training targets than Yang et al. (2015) and Trouillon et al. (2017), which optimized the maximum margin objective and NLL of the Softplus activation function (Softplus (x) = ln (1 + ex)). Similar to Toutanova and Chen (2015), we use NLL of the Softmax function, but ADAM optimizers instead of RProp (Riedmiller and Braun, 1993)."}, {"heading": "4 Conclusion", "text": "Simple conclusions from our work are: 1) Increasing lot size dramatically improves DistMult's performance, raising the question of whether other models would benefit significantly from a similar hyperparameter setting or other training objectives; 2) In the future, it may be better to focus more on metrics that are less frequently used in this area, such as Hits @ 1 (accuracy) and MRR, as, for example, many models on WN18 achieve similar, very high hits @ 10, but even models that are competitive in Hits @ 10 perform worse in Hits @ 1, which is the case of our DistMult implementation. Lately, many research focuses have focused on the filtered scenario, which is why we have decided to use it in this study. One advantage is that it is easy to evaluate. However, the scenario trains the model to expect that there will be only one correct response among the candidates, which is unrealistic in the context of the knowledge base. Continuing the research could focus on the raw scenario, however, on the more raw scenario that requires other research."}], "references": [{"title": "TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems", "author": ["Zheng"], "venue": null, "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD International", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio."], "venue": "Machine Learning 94(2):233\u2013259.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Translating embeddings for modeling multirelational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger,", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Emerging trends: I did it, I did it, I did it, but.", "author": ["Kenneth Ward Church"], "venue": "Natural Language Engineering", "citeRegEx": "Church.,? \\Q2017\\E", "shortCiteRegEx": "Church.", "year": 2017}, {"title": "Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks", "author": ["Rajarshi Das", "Arvind Neelakantan", "David Belanger", "Andrew Mccallum."], "venue": "EACL .", "citeRegEx": "Das et al\\.,? 2017", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "WordNet", "author": ["Christiane Fellbaum."], "venue": "Wiley Online Library.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "GAKE: Graph Aware Knowledge Embedding", "author": ["Jun Feng", "Minlie Huang", "Yang Yang", "Xiaoyan Zhu."], "venue": "Proceedings of the 27th International Conference on Computational Linguistics (COLING\u201916). pages 641\u2013651.", "citeRegEx": "Feng et al\\.,? 2015", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Composing Relationships with Translations", "author": ["Alberto Garc\u0131\u0301a-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases", "author": ["Alberto Garcia-Duran", "Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet."], "venue": "Journal of Artificial Intelligence Research 55:715\u2014-742.", "citeRegEx": "Garcia.Duran et al\\.,? 2016", "shortCiteRegEx": "Garcia.Duran et al\\.", "year": 2016}, {"title": "On the Equivalence of Holographic and Complex Embeddings for Link Prediction pages 1\u20138", "author": ["Katsuhiko Hayashi", "Masashi Shimbo."], "venue": "http://arxiv.org/abs/1702.05563.", "citeRegEx": "Hayashi and Shimbo.,? 2017", "shortCiteRegEx": "Hayashi and Shimbo.", "year": 2017}, {"title": "Learning to Represent Knowledge Graphs with Gaussian Embedding", "author": ["Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao."], "venue": "CIKM \u201915 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management pages 623\u2013", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Knowledge Graph Completion with Adaptive Sparse Transfer Matrix", "author": ["Guoliang Ji", "Kang Liu", "Shizhu He", "Jun Zhao."], "venue": "Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016) pages 985\u2013991.", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "International Conference on Learning Representations pages 1\u2013", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225. https://doi.org/10.1186/1472-6947-15-", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun."], "venue": "CoRR abs/1506.00379. http://arxiv.org/abs/1506.00379.", "citeRegEx": "Lin et al\\.,? 2015a", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learning pages 2181\u2013", "citeRegEx": "Lin et al\\.,? 2015b", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Hierarchical random walk inference in knowledge graphs", "author": ["Qiao Liu", "Liuyi Jiang", "Minghao Han", "Yao Liu", "Zhiguang Qin."], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "An overview of embedding models of entities and relationships for knowledge base completion https://arxiv.org/pdf/1703.08098.pdf", "author": ["Dat Quoc Nguyen"], "venue": null, "citeRegEx": "Nguyen.,? \\Q2017\\E", "shortCiteRegEx": "Nguyen.", "year": 2017}, {"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "author": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "A Review of Relational Machine Learning for Knowledge Graph", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich."], "venue": "Proceedings of the IEEE (28):1\u201323. https://doi.org/10.1109/JPROC.2015.2483592.", "citeRegEx": "Nickel et al\\.,? 2015", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "Holographic Embeddings of Knowledge Graphs", "author": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio."], "venue": "AAAI pages 1955\u20131961. http://arxiv.org/abs/1510.04935.", "citeRegEx": "Nickel et al\\.,? 2016", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Discriminative gaifman models", "author": ["Mathias Niepert."], "venue": "Advances in Neural Information Processing Systems. pages 3405\u20133413.", "citeRegEx": "Niepert.,? 2016", "shortCiteRegEx": "Niepert.", "year": 2016}, {"title": "A direct adaptive method for faster backpropagation learning: The rprop algorithm", "author": ["Martin Riedmiller", "Heinrich Braun."], "venue": "Neural Networks, 1993., IEEE International Conference on. IEEE, pages 586\u2013591.", "citeRegEx": "Riedmiller and Braun.,? 1993", "shortCiteRegEx": "Riedmiller and Braun.", "year": 1993}, {"title": "Modeling Relational Data with Graph Convolutional Networks http://arxiv.org/abs/1703.06103", "author": ["Michael Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling"], "venue": null, "citeRegEx": "Schlichtkrull et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Schlichtkrull et al\\.", "year": 2017}, {"title": "Implicit reasonet: Modeling large-scale structured relationships with shared memory", "author": ["Yelong Shen", "Po-Sen Huang", "Ming-Wei Chang", "Jianfeng Gao."], "venue": "arXiv preprint arXiv:1611.04642 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "ProjE : Embedding Projection for Knowledge Graph Completion", "author": ["Baoxu Shi", "Tim Weniger."], "venue": "AAAI .", "citeRegEx": "Shi and Weniger.,? 2017", "shortCiteRegEx": "Shi and Weniger.", "year": 2017}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the Advances in Neural Information Processing Systems 26 (NIPS 2013) .", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Observed versus latent features for knowledge base and text inference", "author": ["Kristina Toutanova", "Danqi Chen."], "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality pages 57\u201366.", "citeRegEx": "Toutanova and Chen.,? 2015", "shortCiteRegEx": "Toutanova and Chen.", "year": 2015}, {"title": "Knowledge Graph Completion via Complex Tensor Factorization http://arxiv.org/abs/1702.06879", "author": ["Th\u00e9o Trouillon", "Christopher R. Dance", "Johannes Welbl", "Sebastian Riedel", "\u00c9ric Gaussier", "Guillaume Bouchard"], "venue": null, "citeRegEx": "Trouillon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Trouillon et al\\.", "year": 2017}, {"title": "Complex Embeddings for Simple Link Prediction", "author": ["Th\u00e9o Trouillon", "Johannes Welbl", "Sebastian Riedel", "Eric Gaussier", "Guillaume Bouchard."], "venue": "Proceedings of ICML 48:2071\u20132080. http://arxiv.org/pdf/1606.06357v1.pdf.", "citeRegEx": "Trouillon et al\\.,? 2016", "shortCiteRegEx": "Trouillon et al\\.", "year": 2016}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."], "venue": "AAAI Conference on Artificial Intelligence pages 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Text-enhanced representation learning for knowledge graph", "author": ["Zhigang Wang", "Juanzi Li."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. AAAI Press, pages 1293\u20131299.", "citeRegEx": "Wang and Li.,? 2016", "shortCiteRegEx": "Wang and Li.", "year": 2016}, {"title": "Ssp: Semantic space projection for knowledge graph embedding with text descriptions", "author": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu."], "venue": "Pro- ceedings of the 31st AAAI Conference on Artificial In- telligence.", "citeRegEx": "Xiao et al\\.,? 2017", "shortCiteRegEx": "Xiao et al\\.", "year": 2017}, {"title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "ICLR page 12. http://arxiv.org/abs/1412.6575.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations", "author": ["Hee-geun Yoon", "Hyun-je Song", "Seong-bae Park", "Se-young Park."], "venue": "Naacl pages 1\u20139.", "citeRegEx": "Yoon et al\\.,? 2016", "shortCiteRegEx": "Yoon et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Projects such as Wikidata1 or earlier Freebase (Bollacker et al., 2008) have successfully accumulated a formidable amount of knowledge in the form of \u3008entity1 - relation - entity2\u3009 triplets.", "startOffset": 47, "endOffset": 71}, {"referenceID": 21, "context": "An extensive amount of work has been published on this task (for a review see (Nickel et al., 2015; Nguyen, 2017), for a plain list of citations see Table 2).", "startOffset": 78, "endOffset": 113}, {"referenceID": 19, "context": "An extensive amount of work has been published on this task (for a review see (Nickel et al., 2015; Nguyen, 2017), for a plain list of citations see Table 2).", "startOffset": 78, "endOffset": 113}, {"referenceID": 35, "context": "Among those DistMult (Yang et al., 2015) is one of the simplest.", "startOffset": 21, "endOffset": 40}, {"referenceID": 3, "context": "standard KBC datasets, WN18 and FB15k (Bordes et al., 2013).", "startOffset": 38, "endOffset": 59}, {"referenceID": 31, "context": "The DistMult model was introduced by Yang et al. (2015). Subsequently Toutanova and Chen (2015) achieved better empirical results with the same model by changing hyper-parameters of the training procedure and by using negative-log likelihood of softmax instead of L1-based max-margin ranking loss.", "startOffset": 37, "endOffset": 56}, {"referenceID": 0, "context": "(2015). Subsequently Toutanova and Chen (2015) achieved better empirical results with the same model by changing hyper-parameters of the training procedure and by using negative-log likelihood of softmax instead of L1-based max-margin ranking loss.", "startOffset": 1, "endOffset": 47}, {"referenceID": 0, "context": "(2015). Subsequently Toutanova and Chen (2015) achieved better empirical results with the same model by changing hyper-parameters of the training procedure and by using negative-log likelihood of softmax instead of L1-based max-margin ranking loss. Trouillon et al. (2016) obtained even better empirical result on the FB15k dataset just by changing DistMult\u2019s hyper-parameters.", "startOffset": 1, "endOffset": 273}, {"referenceID": 6, "context": "dard datasets WN18 derived from WordNet (Fellbaum, 1998) and FB15k derived from the Freebase knowledge graph (Bollacker et al.", "startOffset": 40, "endOffset": 56}, {"referenceID": 1, "context": "dard datasets WN18 derived from WordNet (Fellbaum, 1998) and FB15k derived from the Freebase knowledge graph (Bollacker et al., 2008).", "startOffset": 109, "endOffset": 133}, {"referenceID": 14, "context": "We train the final models using Adam (Kingma and Ba, 2015) optimizer (lr = 0.", "startOffset": 37, "endOffset": 58}, {"referenceID": 26, "context": "On FB15k only the IRN model (Shen et al., 2016) shows better Hits@10 and the ProjE (Shi and Weniger, 2017) has better MR.", "startOffset": 28, "endOffset": 47}, {"referenceID": 27, "context": ", 2016) shows better Hits@10 and the ProjE (Shi and Weniger, 2017) has better MR.", "startOffset": 43, "endOffset": 66}, {"referenceID": 10, "context": "ComplEx models (that are equivalent as shown by Hayashi and Shimbo (2017)).", "startOffset": 48, "endOffset": 74}, {"referenceID": 31, "context": "Compared to previous works that trained DistMult on these datasets (for results see bottom of Table 2) we use different training objective than Yang et al. (2015) and Trouillon et al.", "startOffset": 144, "endOffset": 163}, {"referenceID": 0, "context": "(2015) and Trouillon et al. (2017) that optimized max margin objective and NLL of softplus activation function (softplus(x) = ln(1 + ex)), respectively.", "startOffset": 1, "endOffset": 35}, {"referenceID": 0, "context": "(2015) and Trouillon et al. (2017) that optimized max margin objective and NLL of softplus activation function (softplus(x) = ln(1 + ex)), respectively. Similarly to Toutanova and Chen (2015) we use NLL of softmax function, however we use ADAM optimizer instead of", "startOffset": 1, "endOffset": 192}, {"referenceID": 24, "context": "RProp (Riedmiller and Braun, 1993).", "startOffset": 6, "endOffset": 34}, {"referenceID": 22, "context": "Results marked by \u2020, \u2021 and ] are from (Nickel et al., 2016), (Trouillon et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 30, "context": ", 2016), (Trouillon et al., 2017) and (Schlichtkrull et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 25, "context": ", 2017) and (Schlichtkrull et al., 2017), respectively.", "startOffset": 12, "endOffset": 40}, {"referenceID": 2, "context": "N on e Unstructured (Bordes et al., 2014) 304 38.", "startOffset": 20, "endOffset": 41}, {"referenceID": 3, "context": "3 TransE (Bordes et al., 2013) 251 89.", "startOffset": 9, "endOffset": 30}, {"referenceID": 32, "context": "TransH (Wang et al., 2014) 303 86.", "startOffset": 7, "endOffset": 26}, {"referenceID": 17, "context": "4 TransR (Lin et al., 2015b) 225 92.", "startOffset": 9, "endOffset": 28}, {"referenceID": 17, "context": "7 CTransR (Lin et al., 2015b) 218 92.", "startOffset": 10, "endOffset": 29}, {"referenceID": 11, "context": "2 KG2E (He et al., 2015) 331 92.", "startOffset": 7, "endOffset": 24}, {"referenceID": 12, "context": "0 TransD (Ji et al., 2015) 212 92.", "startOffset": 9, "endOffset": 26}, {"referenceID": 36, "context": "3 lppTransD (Yoon et al., 2016) 270 94.", "startOffset": 12, "endOffset": 31}, {"referenceID": 13, "context": "7 TranSparse (Ji et al., 2016) 211 93.", "startOffset": 13, "endOffset": 30}, {"referenceID": 9, "context": "5 TATEC (Garcia-Duran et al., 2016) - - - 58 76.", "startOffset": 8, "endOffset": 35}, {"referenceID": 28, "context": "7 NTN (Socher et al., 2013) - 66.", "startOffset": 6, "endOffset": 27}, {"referenceID": 22, "context": "25 HolE (Nickel et al., 2016) - 94.", "startOffset": 8, "endOffset": 29}, {"referenceID": 20, "context": "524 STransE (Nguyen et al., 2016) 206 93.", "startOffset": 12, "endOffset": 33}, {"referenceID": 30, "context": "ComplEx (Trouillon et al., 2017) - 94.", "startOffset": 8, "endOffset": 32}, {"referenceID": 27, "context": "692 ProjE wlistwise (Shi and Weniger, 2017) - - - 34 88.", "startOffset": 20, "endOffset": 43}, {"referenceID": 26, "context": "4 IRN (Shen et al., 2016) 249 95.", "startOffset": 6, "endOffset": 25}, {"referenceID": 8, "context": "RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015) - - - 50 76.", "startOffset": 8, "endOffset": 36}, {"referenceID": 16, "context": "2 PTransE (Lin et al., 2015a) - - - 58 84.", "startOffset": 10, "endOffset": 29}, {"referenceID": 7, "context": "Pa th GAKE (Feng et al., 2015) - - - 119 64.", "startOffset": 11, "endOffset": 30}, {"referenceID": 23, "context": "8 Gaifman (Niepert, 2016) 352 93.", "startOffset": 10, "endOffset": 25}, {"referenceID": 18, "context": "2 Hiri (Liu et al., 2016) - 90.", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "603 R-GCN+ (Schlichtkrull et al., 2017) - 96.", "startOffset": 11, "endOffset": 39}, {"referenceID": 29, "context": "NLFeat (Toutanova and Chen, 2015) - 94.", "startOffset": 7, "endOffset": 33}, {"referenceID": 33, "context": "Te xt TEKE H (Wang and Li, 2016) 114 92.", "startOffset": 13, "endOffset": 32}, {"referenceID": 34, "context": "SSP (Xiao et al., 2017) 156 93.", "startOffset": 4, "endOffset": 23}, {"referenceID": 35, "context": "DistMult (orig) (Yang et al., 2015) - 94.", "startOffset": 16, "endOffset": 35}, {"referenceID": 29, "context": "N on e DistMult (Toutanova and Chen, 2015) - - - - 79.", "startOffset": 16, "endOffset": 42}, {"referenceID": 30, "context": "555 DistMult (Trouillon et al., 2017) - 93.", "startOffset": 13, "endOffset": 37}, {"referenceID": 29, "context": "\u201cNLFeat\u201d abbreviates Node+LinkFeat model from (Toutanova and Chen, 2015).", "startOffset": 46, "endOffset": 72}, {"referenceID": 28, "context": "The results for NTN (Socher et al., 2013) listed in this table are taken from Yang et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 19, "context": "This table was adapted from (Nguyen, 2017).", "startOffset": 28, "endOffset": 42}, {"referenceID": 0, "context": "\u201cNLFeat\u201d abbreviates Node+LinkFeat model from (Toutanova and Chen, 2015). The results for NTN (Socher et al., 2013) listed in this table are taken from Yang et al. (2015). This table was adapted from (Nguyen, 2017).", "startOffset": 67, "endOffset": 171}, {"referenceID": 4, "context": "For broader discussion see (Church, 2017).", "startOffset": 27, "endOffset": 41}, {"referenceID": 4, "context": "future research could focus more on the raw scenario which however requires using other information retrieval metrics such as mean average precision (MAP), previously used in KBC for instance by Das et al. (2017). We see this preliminary work as a small contribution to the ongoing discussion in the machine learning community about the current strong focus on state-of-the-art empirical results when it might be sometimes questionable whether they were achieved due to a better model/algorithm or just by more extensive hyper-parameter search.", "startOffset": 195, "endOffset": 213}, {"referenceID": 15, "context": "els (Levy et al., 2015).", "startOffset": 4, "endOffset": 23}], "year": 2017, "abstractText": "Many papers have been published on the knowledge base completion task in the past few years. Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18. This paper shows that the accuracy of almost all models published on the FB15k can be outperformed by an appropriately tuned baseline \u2014 our reimplementation of the DistMult model. Our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyperparameter tuning or different training objectives. This should prompt future research to re-consider how the performance of models is evaluated and reported.", "creator": "LaTeX with hyperref package"}}}