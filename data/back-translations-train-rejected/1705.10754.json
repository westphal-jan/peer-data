{"id": "1705.10754", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "A Low Dimensionality Representation for Language Variety Identification", "abstract": "Language variety identification aims at labelling texts in a native language (e.g. Spanish, Portuguese, English) with its specific variation (e.g. Argentina, Chile, Mexico, Peru, Spain; Brazil, Portugal; UK, US). In this work we propose a low dimensionality representation (LDR) to address this task with five different varieties of Spanish: Argentina, Chile, Mexico, Peru and Spain. We compare our LDR method with common state-of-the-art representations and show an increase in accuracy of ~35%. Furthermore, we compare LDR with two reference distributed representation models. Experimental results show competitive performance while dramatically reducing the dimensionality --and increasing the big data suitability-- to only 6 features per variety. Additionally, we analyse the behaviour of the employed machine learning algorithms and the most discriminating features. Finally, we employ an alternative dataset to test the robustness of our low dimensionality representation with another set of similar languages.", "histories": [["v1", "Tue, 30 May 2017 17:07:45 GMT  (731kb,D)", "http://arxiv.org/abs/1705.10754v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["francisco rangel", "marc franco-salvador", "paolo rosso"], "accepted": false, "id": "1705.10754"}, "pdf": {"name": "1705.10754.pdf", "metadata": {"source": "CRF", "title": "A Low Dimensionality Representation for Language Variety Identification", "authors": ["Francisco Rangel", "Marc Franco-Salvador", "Paolo Rosso"], "emails": ["francisco.rangel@autoritas.es,", "mfranco@prhlt.upv.es,", "prosso@dsic.upv.es"], "sections": [{"heading": null, "text": "Keywords: representation of low dimensionality; recognition of linguistic diversity; discrimination of similar languages; author profiling; big data; social media"}, {"heading": "1 Introduction", "text": "In fact, most of them see themselves as being able to trump themselves, and that they are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them have trumped themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...)"}, {"heading": "2 Low Dimensionality Representation", "text": "The key aspect of the representation of low dimensionality (LDR) is the use of weights to represent the probability that each term belongs to each of the different language varieties. We assume that the distribution of weights for a particular document should be closer to the weights of the corresponding language variety. Formally, the LDR is estimated as follows: Term frequency - inverse document frequency (tf-idf) matrix generation. First, we apply the tf-idf [11] weighting to the terms of the documents of the training set D. As a result, we obtain the following matrix: \u0445 = w11 w12... w1m \u00b2 (d1) Vocabulary frequency of the terms used (d2)... wn1 wn2... wnm \u00b2 (dn), (1) where each line in the matrix \u00b2 weighting represents a document, each column c = weighting of the term, each column represents a Vocabulary term (D)."}, {"heading": "3 Evaluation Framework", "text": "In this section, we describe the corpus and alternative representations we use in this work."}, {"heading": "3.1 HispaBlogs Corpus", "text": "We created HispaBlogs Dataset7 by collecting contributions from Spanish blogs from five different countries: Argentina, Chile, Mexico, Peru and Spain. For each country, there are 450 and 200 blogs for training and testing, ensuring that each author appears in only one sentence. Each blog contains at least 10 posts. The total number of blogs is 2,250 and 1,000 respectively. Statistics on the number of words are shown in Table 3."}, {"heading": "3.2 Alternative representations", "text": "We are interested in investigating the impact of the proposed representation and comparing its performance with state-of-the-art representations based on n-grams and two approaches based on the most recent and most popular distributed representations of words. [1] The use of the LDR document is represented by a set of features multiplied by the number of categories (the 5 language variants), in our case this is a significant dimensionality reduction that may be helpful to deal with large data environments. [7] The HispaBlogs dataset was collected by experts from the Autoritas Consulting Company (http: / / www.autoritas.net). Autoritas experts in the various countries selected popular words related to politics, online marketing, technology or trends. The HispaBlogs dataset is publicly available at: https: / github.com / autoritas / RD-Lab / master data / HispaBlogs dataset of the 5000 representations."}, {"heading": "4 Experimental Results", "text": "In this section, we show experimental results obtained with the machine learning algorithms that best solve the problem with the proposed representation, the impact of pre-processing on performance, the results achieved compared to those achieved with modern and distributed representations, the error analysis that provides useful insights to better understand differences between languages, a in-depth analysis of the contribution of the different features and a cost analysis that highlights the suitability of LDR for a big data scenario."}, {"heading": "4.1 Machine learning algorithms comparison", "text": "We tested several machine learning algorithms 9 with the aim of selecting the one that best solves the task. As shown in Table 4, Multiclass Classifier10 achieves the best result (the results in the rest of the paper refer to Multiclass Classifier).8 We used 300-dimensional vectors, size 10 context windows, and 20 negative words for each sample. We edited the text with lowercase letters, tokenization, removal of one-length words, and with phrase recognition using word2vec tools: https: / / code.google.com / p / word2vec / 9 http: / / www.cs.waikato.nz / ml / we used a full translation problem with a multiple-level SVM for a full translation."}, {"heading": "4.2 Preprocessing impact", "text": "The proposed representation aims to use the entire vocabulary to obtain the weights of its terms. Social media texts may have noise and poorly written words. In addition, some of these words can only be used by a few authors. To investigate their effect on the classification, we performed a pre-processing step to remove words that occur less than n times in the corpus and iterate n between 1 and 100. Figure 1 shows the corresponding accuracies. In the left part of the figure (a), the results for n between 1 and 10 are displayed on a continuous scale. In the right part (b), values from 10 to 100 are displayed on a non-continuous scale. As can be seen, the best result was obtained with n equal to 5, with an accuracy of 71.1%. As expected, the proposed representation utilizes the entire vocabulary, although it is advisable to remove words with very few conveniences that can alter the results."}, {"heading": "4.3 Language variety identification results", "text": "In Table 6, we show the results obtained by the representations described with the Multiclass Classifier. BOW achieves slightly better results than 4-gram characters, and both significantly improve the results achieved with tf-idf 2-gram. Instead of selecting the most common n-gram representations, our approach uses the entire vocabulary and assigns higher weights to the most diverse words for the various language variants, as in Equation 2. We emphasize that our LDR achieves competitive results compared to the use of distributed representations. Specifically, there is no significant difference between them (skip program z0.05 = 0, 5457 < 1, 960 and SenVecz0.05 = 0, 7095 < 1, 960). Furthermore, our proposal considerably reduces the dimensionality of an order of magnitude as shown in Table 6."}, {"heading": "4.4 Error analysis", "text": "As can be seen in Table 7, the Spanish variety is the easiest to distinguish. However, one of the highest confusion occurs from Argentine to Spanish. Mexicans and Spaniards have also been significantly confused with Argentines. Finally, the greatest confusion occurs from Peruvians to Chileans, although the least average confusion occurs with Peruvians. Generally, Latin American varieties are closer to each other and it is more difficult to distinguish them from each other. The language evolves over time. It is logical that language varieties from nearby countries - such as Latin America - developed in a more similar way than the Spanish variety. It is also logical that more language variety similarities are shared in neighboring countries, e.g. Chileans compared to Peruvian and Argentine. Figure 3. F1 values for identification as the corresponding language variety compared to others. In Figure 3, we show the accuracy and memory values for the varieties that are most likely to be identified in Chile, so that these varieties are most likely to remember others."}, {"heading": "4.5 Most discriminating features", "text": "In Table 8 we show the most differentiated characteristics. The characteristics are sorted according to their information gain (IG). As can be seen, the highest gain is achieved by average, maximum and minimum deviation and standard deviation. On the other hand, probability and proportionality characteristics show a low gain in information.We experimented with different sets of characteristics and show the results in Figure 4. As is to be expected, characteristics get high accuracies on an average basis (67.0%).Although characteristics based on standard deviation do not show the highest gain in information, they achieve the highest results individually (69.2%) and in combination with average characteristics (70.8%).Characteristics based on minimum and maximum values achieve low results (48.3% and 54.7%, respectively), but in combination they achieve a significant increase (61.1%).The combination of the previous characteristics achieves almost the highest accuracy (71.0%), which corresponds with probability and proportionality characteristics (71.1%)."}, {"heading": "4.6 Cost analysis", "text": "We analyze the costs from two perspectives: i) the complexity of the features; and ii) the number of features required to represent a document. If we define l as the number of different language variants and n as the number of terms of the document to be classified, the cost of obtaining the features of Table 2 (average, minimum, maximum, probability, and proportionality) is O (l \u00b7 n. If we define l as the number of terms in the document that match a term in the vocabulary, the cost of obtaining the standard deviation is O (l \u00b7 n). Since the average was previously required for the standard deviation calculation, the total cost is O (l \u00b7 n) + O (l \u00b7 m), the O (max (l \u00b7 n, l \u00b7 m))) = O (l \u00b7 n). Since the number of terms in the vocabulary will always be equal or greater than the number of random terms (l \u00b7 n) (< < the number of terms < < the < the number of < the < the number of terms needed in the < < the < the number of the < the number of the <"}, {"heading": "4.7 Robustness", "text": "To analyze the robustness of the representation of low dimensions in different languages, we experimented with the development set of the DSLCC corpus11 from the task of discrimination between similar languages [12]. The corpus consists of 2,000 sentences per language or variety, with 20 to 100 characters per sentence, obtained from message headings. In Table 9, we show the results obtained with the proposed representation and the two distributed representations, Skip-gram and SenVec. It is important to note that in general, when one particular representation for one language improves, this comes at the expense of the other. We can conclude that the three representations achieved comparative results and support the robustness of the representation of low dimensions."}, {"heading": "5 Conclusions", "text": "The experimental results exceeded conventional state-of-the-art and achieved competitive results compared to two distributed representation-based approaches using the popular continuous Skip-gram model. Dimensionality reduction achieved using LDR is from thousands to only 6 features per language variant, enabling large collections to be handled in big data environments such as social media. Recently, we applied LDR to the task of age and gender identification to achieve competitive results with the most powerful teams in author profiling in the PAN12 Lab at CLEF.13. As future work, we plan to apply LDR to other author profiling tasks such as personality identification. 11 http: / / ttg.uni-saarland.de / lt4vardial2015 / dsl.html 12 http: / / pan.webis.de 13 http: / / www.clef-innitiative.org"}], "references": [{"title": "Classes for fast maximum entropy training", "author": ["Goodman", "Joshua"], "venue": "Proceedings of the Acoustics, Speech, and Signal Processing (ICASSP\u201901) vol. 1 pp. 561\u2013564", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Michael U.", "Hyv\u00e4rinen", "Aapo"], "venue": "The Journal of Machine Learning Research vol. 13 pp. 307\u2013361", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations, Parallel distributed processing: explorations in the microstructure of cognition, vol", "author": ["Hinton", "Geoffrey E.", "Mcclelland", "James L.", "Rumelhart", "David E."], "venue": "1: foundations. In: MIT Press, Cambridge, MA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1986}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V.", "Mikolov", "Tomas"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML\u201914) vol. 32", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Language variety identification in Spanish tweets", "author": ["Maier", "Wolfgang", "G\u00f3mez-Rodr\u0131\u0301guez", "Carlos"], "venue": "Workshop on Language Technology for Closely Related Languages and Language Variants (EMNLP\u201914) pp. 25\u201335", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "Proceedings of Workshop at International Conference on Learning Representations (ICLR\u201913)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S.", "Dean", "Jeff"], "venue": "Advances in Neural Information Processing Systems pp. 3111\u20133119", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML\u201912) pp. 1751\u20131758", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic identification of Arabic language varieties and dialects in social media", "author": ["Sadat", "Fatiha", "Kazemi", "Farmazeh", "Farzindar", "Atefeh"], "venue": "1st. International Workshop on Social Media Retrieval and Analysis (SoMeRa\u201914)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Gerard", "Buckley", "Christopher"], "venue": "Information processing & management, vol. 24(5), pp. 513\u2013523", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Merging comparable data sources for the discrimination of similar languages: The DSL corpus collection", "author": ["Tan", "Liling", "Zampieri", "Marcos", "Ljube\u0161ic", "Nicola", "Tiedemann", "J\u00f6rg"], "venue": "7th Workshop on Building and Using Comparable Corpora Building Resources for Machine Translation Research (BUCC\u201914) pp. 6\u201310", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic identification of language varieties: The case of portuguese", "author": ["Zampieri", "Marcos", "Gebrekidan-Gebre", "Binyam"], "venue": "Proceedings of the 11th Conference on Natural Language Processing (KONVENS\u201912) pp. 233\u2013237", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "A report on the DSL shared task 2014", "author": ["Zampieri", "Marcos", "Tan", "Liling", "Ljube\u0161i", "Nicola", "Tiedemann", "J\u00f6rg"], "venue": "Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial\u201914) pp. 58\u201367", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "In the last years, several tasks and workshops have been organized: the Workshop on Language Technology for Closely Related Languages and Language Variants @ EMNLP 20141; the VarDial Workshop @ COLING 2014 - Applying NLP Tools to Similar Languages, Varieties and Dialects2; and the LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialect3 @ RANLP [14][12].", "startOffset": 395, "endOffset": 399}, {"referenceID": 10, "context": "In the last years, several tasks and workshops have been organized: the Workshop on Language Technology for Closely Related Languages and Language Variants @ EMNLP 20141; the VarDial Workshop @ COLING 2014 - Applying NLP Tools to Similar Languages, Varieties and Dialects2; and the LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialect3 @ RANLP [14][12].", "startOffset": 399, "endOffset": 403}, {"referenceID": 8, "context": "In [10] the authors addressed the problem of identifying Arabic varieties in blogs and social fora.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Similarly, [13] collected 1,000 news articles of two varieties of Portuguese.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "With respect to the Spanish language, [6] focused on varieties from Argentina, Chile, Colombia, Mexico and Spain in Twitter.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "We differentiate from the previous works as follows: i) instead of n-gram based representations, we propose a low dimensionality representation that is helpful when dealing with big data in social media; ii) in order to reduce the possible over-fitting, our training and test partitions do not share any author of instance between them4; and iii) in contrast to the Twitter dataset of [6], we will make available our dataset to the research community.", "startOffset": 385, "endOffset": 388}, {"referenceID": 9, "context": "First, we apply the tf-idf [11] weighting for the terms of the documents of the training set D.", "startOffset": 27, "endOffset": 31}, {"referenceID": 2, "context": "Distributed representations Due to the increasing popularity of the distributed representations [4], we used the continuous Skip-gram model to generate distributed representations of words (e.", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "The continuous Skip-gram model [7,8] is an iterative algorithm which attempts to maximize the classification of the context surrounding a word.", "startOffset": 31, "endOffset": 36}, {"referenceID": 6, "context": "The continuous Skip-gram model [7,8] is an iterative algorithm which attempts to maximize the classification of the context surrounding a word.", "startOffset": 31, "endOffset": 36}, {"referenceID": 6, "context": "To estimate p(wt+j |wt) we used negative sampling [8] that is a simplified version of the Noise Contrastive Estimation (NCE) [3,9] which is only concerned with preserving vector quality in the context of Skip-gram learning.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "To estimate p(wt+j |wt) we used negative sampling [8] that is a simplified version of the Noise Contrastive Estimation (NCE) [3,9] which is only concerned with preserving vector quality in the context of Skip-gram learning.", "startOffset": 125, "endOffset": 130}, {"referenceID": 7, "context": "To estimate p(wt+j |wt) we used negative sampling [8] that is a simplified version of the Noise Contrastive Estimation (NCE) [3,9] which is only concerned with preserving vector quality in the context of Skip-gram learning.", "startOffset": 125, "endOffset": 130}, {"referenceID": 6, "context": "The experimental results in [8] show that this function obtains better results at the semantic level than hierarchical softmax [2] and NCE.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "The experimental results in [8] show that this function obtains better results at the semantic level than hierarchical softmax [2] and NCE.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "In addition, we used Sentence vectors (SenVec) [5], a variant that follows Skip-gram architecture to train a special vector sv representing the sentence.", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "Following state-of-the-art approach [5], in the evaluation we used a logistic classifier for both SenVec and Skip-gram approaches.", "startOffset": 36, "endOffset": 39}, {"referenceID": 10, "context": "In order to analyse the robustness of the low dimensionality representation to different languages, we experimented with the development set of the DSLCC corpus11 from the Discriminating between Similar Languages task [12].", "startOffset": 218, "endOffset": 222}], "year": 2017, "abstractText": "Language variety identification aims at labelling texts in a native language (e.g. Spanish, Portuguese, English) with its specific variation (e.g. Argentina, Chile, Mexico, Peru, Spain; Brazil, Portugal; UK, US). In this work we propose a low dimensionality representation (LDR) to address this task with five different varieties of Spanish: Argentina, Chile, Mexico, Peru and Spain. We compare our LDR method with common state-of-the-art representations and show an increase in accuracy of \u223c35%. Furthermore, we compare LDR with two reference distributed representation models. Experimental results show competitive performance while dramatically reducing the dimensionality \u2014 and increasing the big data suitability \u2014 to only 6 features per variety. Additionally, we analyse the behaviour of the employed machine learning algorithms and the most discriminating features. Finally, we employ an alternative dataset to test the robustness of our low dimensionality representation with another set of similar languages.", "creator": "LaTeX with hyperref package"}}}