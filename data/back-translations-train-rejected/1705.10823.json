{"id": "1705.10823", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Practical Neural Network Performance Prediction for Early Stopping", "abstract": "In the neural network domain, methods for hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of neural network configurations. In this paper, we show that a simple regression model, based on support vector machines, can predict the final performance of partially trained neural network configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We use this regression model to develop an early stopping strategy for neural network configurations. With this early stopping strategy, we obtain significant speedups in both hyperparameter optimization and meta-modeling. Particularly in the context of meta-modeling, our method can learn to predict the performance of drastically different architectures and is seamlessly incorporated into reinforcement learning-based architecture selection algorithms. Finally, we show that our method is simpler, faster, and more accurate than Bayesian methods for learning curve prediction.", "histories": [["v1", "Tue, 30 May 2017 19:00:53 GMT  (326kb,D)", "http://arxiv.org/abs/1705.10823v1", "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA"]], "COMMENTS": "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["bowen baker", "otkrist gupta", "ramesh raskar", "nikhil naik"], "accepted": false, "id": "1705.10823"}, "pdf": {"name": "1705.10823.pdf", "metadata": {"source": "CRF", "title": "Practical Neural Network Performance Prediction for Early Stopping", "authors": ["Bowen Baker", "Otkrist Gupta", "Ramesh Raskar", "Nikhil Naik"], "emails": ["naik}@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, the neural networks have achieved significant gains in machine learning performance."}, {"heading": "2 Related Work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, and to move."}, {"heading": "3 Background", "text": "We experiment empirically with our early-stop algorithm by combining it with a MetaQNN [1] and Hyperband [11] algorithm for metamodeling and hyperparameter optimization. In the following we give a brief background on MetaQNN and Hyperband."}, {"heading": "3.1 MetaQNN", "text": "Baker et al. [1] train a Q-Learning [24] agent to design Convolutionary Neural Networks. Using this method, the agent tries out architectures from a large, finite space by traversing a path from the input to the final layer through nodes with different layer types. Over time, the neural network model defined by the agent's trajectory is trained on the task and its validation accuracy is presented to the agent as a reward. However, with the help of a greedy exploration strategy [23] and experience reproduction [12], the agent becomes better at discovering designs that achieve high performance in the learning task. However, the MetaQNN method uses 100 GPU days to train 2700 neural architectures, and the similar experiment by Zoph and Le [25] used 10,000 GPU days to create 12,800 models on CIFAR-10. The amount of computational resources needed for these approaches (e.g. large data spaces) makes them unmanageable."}, {"heading": "3.2 Hyperband", "text": "Recently, Li et al. [11] introduced Hyperband, a random search method based on multi-armed bandits that performs state-of-the-art hyperparameter optimization in a variety of settings, and the hyperband algorithm trains a population of models with different hyperparameter configurations and iteratively discards models below a certain percentile in the population until the computing budget is exhausted or satisfactory results are achieved. Hyperband uses a last seen value heuristically, which essentially means that it uses the last-seen validation power to assess which models should be ejected. Although this heuristics already works very well, we show that the use of a predictor trained on the partially observed learning curve can be used to make even more aggressive early stops and accelerate overall performance compared to the original hyperband algorithm."}, {"heading": "4 Method", "text": "We first describe our model for predicting the performance of neural networks, followed by a method for early termination of underdeveloped network architectures."}, {"heading": "4.1 Modeling Learning Curves", "text": "Our goal is to model the validation accuracy v (x, t) of a neural network configuration. For each T-epoch trained configuration, we record a time series y (t) = y1, y2,.., yT of validation accuracy. Note that this problem formulation Klein et al. [7] is very similar. We propose to use a set of features ux (x1, y1 (t)), (x2, y2 (t)), (xn, yn (t)))). We propose to use a set of features ux, derived from the neural network configuration x, together with a subset of time series accuracies y (t) 1-Africa = (yt)."}, {"heading": "4.2 Early Stopping", "text": "To accelerate hyperparameter optimization and metamodeling methods, we develop an algorithm to determine whether we should continue a partially trained model configuration using our \u03bdSVR predictor. If we want to sample N entire neural network configurations and train n N configurations to create a training set, we then train a model f (xf) with \u03bd-SVR to predict yT. Well, given the best observed yBEST configuration at the moment, we would like to terminate a new configuration x to add its partially observed learning curve y (t) 1-\u03c4 if f (xf) = y-T \u2264 yBEST so as not to waste computational resources that form a suboptimal configuration. However, if f (xf) has a poor sound generalization, we may erroneously conclude the optimal configuration y (T). So, if we assume that our estimation of durability can be based on GI-T, we can use hyperparameter optimization and metmodeling methods."}, {"heading": "5 Experiments and Results", "text": "We now evaluate the performance of our algorithm in three different settings. First, we analyze the ability of a \u03bd-SVR model to predict the final validation accuracy of a trained neural network. Second, we integrate the \u03bd-SVR-based early-stop model in MetaQNN [1] to show that it can accelerate the process of meta-modelling without disrupting the reward function to the point where the agent learns suboptimal strategies. Finally, we show that our early-stop model can also accelerate the hyperband algorithm [11]. First, we describe the process by which we train our performance prediction model.For all experiments, we train the \u03bd-SVR model with a random search of over 1000 hyperparameter configurations from the C-Log Uniform (10 \u2212 5, 10), as well as the process by which we use our performance prediction models.For all experiments, we train the \u03bd-SVR model with random search over 1000 hyperparameter configurations from the C-Log Uniform (10 \u2212 5), the Log Uniform (not the Log Uniform \u2212 10), the Log Uniform (not the Log Uniform \u2212 10), and the Log Uniform (not the Log Uniform \u2212 5)."}, {"heading": "5.1 Datasets and Training Procedures", "text": "We describe the data sets used in our experiments."}, {"heading": "5.2 Prediction Performance", "text": "We are now evaluating the ability of \u03bd-SVR, formed with linear and RBF cores, to predict the final performance of partially formed neural networks. We are comparing it against the Bayesian Neural Network (BNN), represented by Klein et al. [7] using a Hamilton SVR sampler. In the formation of SVN, we are comparing it not only with the subset of fully observed learning curves, but also with all other partially observed learning curves from the training set. While we are not comparing the partially observed curves with the \u03bd-SVR model for training, we felt it was a fair comparison of how \u03bd-SVR uses the entire partially observed learning curve during inference. Furthermore, we are comparing the learning curve extrapolation (LCE) method introduced by Domhan et al. [4] and the last seen value (LastSeenValue) heuristic [11], which do not include both previous learning curves during training."}, {"heading": "5.3 Early Stopping for Meta-modeling", "text": "We now have the power of 1 to 3 minutes to speed up architecture selection. First, we take 1000 random models from the MetaQNN [1] search space. We simulate the MetaQNN algorithm by taking 10 random orders from the first 100 fully observed curves, while the LCE model exits from each individual subcurve and begins early termination."}, {"heading": "5.4 Early Stopping for Hyperparameter Optimization", "text": "In this section, we present the results of the hyperband algorithm [11]. So, in order to use our early stop algorithm, we need to have 20-100 fully observed learning curves (see supplement, section 2). So, while integrating our algorithm into the hyperband framework, we allow an initial burn-in period during which we train 100 models to maximum iterations and use this to train constant SVR performance predictors. Then, we use these predictors to perform an early stop within the hyperband halving, resulting in a more aggressive termination criterion. Figure 6 shows that our early stop algorithm evaluates the same number of unique configurations as hyperband search parameters within half the computation time, while achieving the same final accuracy within standard errors."}], "references": [{"title": "Designing neural network architectures using reinforcement learning", "author": ["Bowen Baker", "Otkrist Gupta", "Nikhil Naik", "Ramesh Raskar"], "venue": "International Conference on Learning Representations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "JMLR, 13(Feb):281\u2013305,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures", "author": ["James Bergstra", "Daniel Yamins", "David D Cox"], "venue": "ICML (1),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves", "author": ["Tobias Domhan", "Jost Tobias Springenberg", "Frank Hutter"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": "In International Conference on Learning and Intelligent Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Learning curve prediction with bayesian neural networks", "author": ["Aaron Klein", "Stefan Falkner", "Jost Tobias Springenberg", "Frank Hutter"], "venue": "International Conference on Learning Representations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hyperband: A novel bandit-based approach to hyperparameter optimization", "author": ["Lisha Li", "Kevin Jamieson", "Giulia DeSalvo", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "International Conference on Learning Representations,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Long-Ji Lin"], "venue": "Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1992}, {"title": "Convolutional neural fabrics", "author": ["Shreyas Saxena", "Jakob Verbeek"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Combinations of genetic algorithms and neural networks: A survey of the state of the art", "author": ["J David Schaffer", "Darrell Whitley", "Larry J Eshelman"], "venue": "International Workshop on Combinations of Genetic Algorithms and Neural Networks,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "New support vector algorithms", "author": ["Bernhard Sch\u00f6lkopf", "Alex J Smola", "Robert C Williamson", "Peter L Bartlett"], "venue": "Neural computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Taking the human out of the loop: A review of bayesian optimization", "author": ["Bobak Shahriari", "Kevin Swersky", "Ziyu Wang", "Ryan P Adams", "Nando de Freitas"], "venue": "Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Mostofa Patwary", "Mr Prabhat", "Ryan Adams"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Evolving neural networks through augmenting topologies", "author": ["Kenneth O Stanley", "Risto Miikkulainen"], "venue": "Evolutionary Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "A genetic programming approach to designing convolutional neural network architectures", "author": ["Masanori Suganuma", "Shinichi Shirakawa", "Tomoharu Nagao"], "venue": "arXiv preprint arXiv:1704.00764,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Freeze-thaw bayesian optimization", "author": ["Kevin Swersky", "Jasper Snoek", "Ryan Prescott Adams"], "venue": "arXiv preprint arXiv:1406.3896,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Generative neuroevolution for deep learning", "author": ["Phillip Verbancsics", "Josh Harguess"], "venue": "arXiv preprint arXiv:1312.5355,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Multi-armed bandit algorithms and empirical evaluation", "author": ["Joannes Vermorel", "Mehryar Mohri"], "venue": "European Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1989}], "referenceMentions": [{"referenceID": 1, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 5, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 8, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 14, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 15, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 0, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 201, "endOffset": 215}, {"referenceID": 2, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 201, "endOffset": 215}, {"referenceID": 19, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 201, "endOffset": 215}, {"referenceID": 0, "context": "Figure 1: Early Stopping Example: (Left) 1000 learning curves sampled from the MetaQNN [1] search space.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "While there is some prior work on neural network performance prediction [4, 7], our work is the first to show that these methods are viable for deep convolutional neural network architecture searches.", "startOffset": 72, "endOffset": 78}, {"referenceID": 6, "context": "While there is some prior work on neural network performance prediction [4, 7], our work is the first to show that these methods are viable for deep convolutional neural network architecture searches.", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "2 Related Work Neural Network Performance Prediction: There has been limited work on predicting neural network performance during the training process [4, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "2 Related Work Neural Network Performance Prediction: There has been limited work on predicting neural network performance during the training process [4, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 3, "context": "[4] introduce a weighted probabilistic model for learning curves and utilize this model for speeding up hyperparameter search in small convolutional neural networks (CNNs) and fully-connected networks (FCNs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[21] develop a Gaussian Process-based method for predicting individual learning curves for logistic regression models (among others), but not for neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4], Klein et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] train Bayesian neural networks for predicting unobserved learning curves using a training set of fully and partially observed learning curves.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "The earliest meta-modeling approaches for neural net design were based on genetic algorithms [14, 19, 22].", "startOffset": 93, "endOffset": 105}, {"referenceID": 16, "context": "The earliest meta-modeling approaches for neural net design were based on genetic algorithms [14, 19, 22].", "startOffset": 93, "endOffset": 105}, {"referenceID": 19, "context": "The earliest meta-modeling approaches for neural net design were based on genetic algorithms [14, 19, 22].", "startOffset": 93, "endOffset": 105}, {"referenceID": 17, "context": "[20] use Cartesian genetic programming to obtain competitive results on image classification tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Saxena and Verbeek [13] use densely connected networks of layers to search for well-performing networks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Another popular tool for meta-modeling is Bayesian optimization [16].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "[3] utilize Tree of Parzen Estimators (TPE) to design feed-forward networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] show that a Q-learning agent can design competitive CNNs for image classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "methods have been proposed for hyperparameter optimization, including methods based on sequential model-based optimization (SMAC) [6], Gaussian processes (GP) [17], and TPE [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 14, "context": "methods have been proposed for hyperparameter optimization, including methods based on sequential model-based optimization (SMAC) [6], Gaussian processes (GP) [17], and TPE [3].", "startOffset": 159, "endOffset": 163}, {"referenceID": 2, "context": "methods have been proposed for hyperparameter optimization, including methods based on sequential model-based optimization (SMAC) [6], Gaussian processes (GP) [17], and TPE [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "[18] utilize neural networks to efficiently model distributions over functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "However, random search or grid search [2] is most commonly used in practical settings.", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "[11] introduced Hyperband, a multi-armed bandit-based efficient random search technique that outperforms state-of-the-art Bayesian optimization methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We empirically experiment with our early stopping algorithm by integrating it with both a metamodeling and hyperparameter optimization algorithm, namely MetaQNN [1] and Hyperband [11].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "We empirically experiment with our early stopping algorithm by integrating it with both a metamodeling and hyperparameter optimization algorithm, namely MetaQNN [1] and Hyperband [11].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "[1] train a Q-learning [24] agent to design convolutional neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[1] train a Q-learning [24] agent to design convolutional neural networks.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Using an -greedy exploration strategy [23] and experience replay [12], the agent becomes better with time at discovering designs that obtain high performance on the learning task.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Using an -greedy exploration strategy [23] and experience replay [12], the agent becomes better with time at discovering designs that obtain high performance on the learning task.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "[11] introduced Hyperband, a random search technique based on multi-armed bandits that obtains state-of-the-art performance in hyperparameter optimization in a variety of settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "We utilize \u03bd-Support Vector Regression (\u03bd-SVR) [15] for training a model for yT .", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "[15] for further details on \u03bd-SVR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Second, we integrate the \u03bd-SVR-based early stopping model into MetaQNN [1] to show that it can speed up the meta-modeling process without perturbing the reward function to the point that the agent learns suboptimal policies.", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "Finally, we show that our early stopping model can also speed up the Hyperband algorithm [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "We compare our method against BNN [7], LCE [4], and a \u201clast seen value\u201d heuristic [11].", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "We compare our method against BNN [7], LCE [4], and a \u201clast seen value\u201d heuristic [11].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "We compare our method against BNN [7], LCE [4], and a \u201clast seen value\u201d heuristic [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "[1], which allows for varying the numbers and orderings of convolution, pooling, and fully connected layers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] are reproduced in the Supplementary Materials.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "AlexNet: We train the AlexNet [9] model on the ILSVRC12 dataset.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Deep Resnet: We sample 500 ResNet [5] architectures from a search space similar to Zoph et al.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "[7] using a Hamiltonian Monte Carlo sampler.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] and the last seen value (LastSeenValue) heuristic [11], both of which don\u2019t incorporate prior learning curves during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[4] and the last seen value (LastSeenValue) heuristic [11], both of which don\u2019t incorporate prior learning curves during training.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "First, we take 1,000 random models from the MetaQNN [1] search space.", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "[4] as a baseline, which has a similar probability threshold termination criterion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Thus, for an experiment on the order of that of [1, 25], many days of computation time would be added as overhead to fit the LCE models when compared to our simple model.", "startOffset": 48, "endOffset": 55}, {"referenceID": 8, "context": "In this section we present results on the Hyperband algorithm [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Figure 5: MetaQNN on CIFAR-10 with Early Stopping: A full run of the MetaQNN algorithm [1] on the CIFAR-10 dataset with early stopping.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "With the advent of large scale automated architecture search [1, 25], methods such as ours will be vital in exploring even larger and more complex search spaces.", "startOffset": 61, "endOffset": 68}], "year": 2017, "abstractText": "In the neural network domain, methods for hyperparameter optimization and metamodeling are computationally expensive due to the need to train a large number of neural network configurations. In this paper, we show that a simple regression model, based on support vector machines, can predict the final performance of partially trained neural network configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We use this regression model to develop an early stopping strategy for neural network configurations. With this early stopping strategy, we obtain significant speedups in both hyperparameter optimization and meta-modeling. Particularly in the context of meta-modeling, our method can learn to predict the performance of drastically different architectures and is seamlessly incorporated into reinforcement learningbased architecture selection algorithms. Finally, we show that our method is simpler, faster, and more accurate than Bayesian methods for learning curve prediction.", "creator": "LaTeX with hyperref package"}}}