{"id": "1705.10834", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Experience Replay Using Transition Sequences", "abstract": "Experience replay is one of the most commonly used approaches to improve the sample efficiency of reinforcement learning algorithms. In this work, we propose an approach to select and replay sequences of transitions in order to accelerate the learning of a reinforcement learning agent in an off-policy setting. In addition to selecting appropriate sequences, we also artificially construct transition sequences using information gathered from previous agent-environment interactions. These sequences, when replayed, allow value function information to trickle down to larger sections of the state/state-action space, thereby making the most of the agent's experience. We demonstrate our approach on modified versions of standard reinforcement learning tasks such as the mountain car and puddle world problems and empirically show that it enables better learning of value functions as compared to other forms of experience replay. Further, we briefly discuss some of the possible extensions to this work, as well as applications and situations where this approach could be particularly useful.", "histories": [["v1", "Tue, 30 May 2017 19:24:09 GMT  (904kb)", "http://arxiv.org/abs/1705.10834v1", "23 pages, 6 figures, Submitted to the journal Artificial Intelligence"]], "COMMENTS": "23 pages, 6 figures, Submitted to the journal Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["thommen george karimpanal", "roland bouffanais"], "accepted": false, "id": "1705.10834"}, "pdf": {"name": "1705.10834.pdf", "metadata": {"source": "CRF", "title": "Experience Replay Using Transition Sequences", "authors": ["Thommen George Karimpanal", "Roland Bouffanais"], "emails": ["thommen_george@mymail.sutd.edu.sg,", "bouffanais@sutd.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.10 834v 1 [cs.A I] 3 0M ayExperience replay is one of the most widely used approaches to improving the sample efficiency of reinforcement learning algorithms. In this work, we propose an approach to selecting and reproducing transition sequences to accelerate the learning of reinforcement learning in a non-political environment. In addition to selecting suitable sequences, we also construct artificially transition sequences based on information obtained from previous interactions with the agent. These sequences, when repeated, allow value information to seep into larger areas of the state-state sphere of action, making the most of the agent's experience. We demonstrate our approach to modified versions of standard reinforcement learning tasks such as the problems of the mining and puddle world, and demonstrate empirically that it enables a better learning of value functions compared to other forms of experiential replay. Furthermore, we briefly discuss some possible extensions of this work, as well as possible applications in these situations and in particular situations."}, {"heading": "1. Introduction", "text": "In fact, we will be able to find ourselves in a situation where we are able to change the world, where we are able to change the world, \"he said in an interview."}, {"heading": "2. Background", "text": "The problem of learning from limited experiences is not new in the field of the RL [11, 12]. In general, learning speed and example efficiency are critical factors determining the feasibility of using learning algorithms in the real world. [13, 14] It is therefore important that a learner is able to gather as much relevant knowledge as possible, whatever exploratory actions are being talked about. [15] Algorithms are well suited to this need, as they enable multiple value functions to be learned together. [16] When behavior and goal policies differ in different ways, the importance of sampling is often used to obtain better estimates of value functions."}, {"heading": "3. Methodology", "text": "This year it is more than ever before."}, {"heading": "3.1. Tracking and Storage of Relevant Transition Sequences", "text": "As described, virtual transition sequences are constructed by connecting two transition sequences to each other. One of them, let's say, consists of errors consisting of mt transitions, has historically been successful - it has received high rewards - in terms of the task and is part of Library L. The other sequence, i.e. errors, is simply a sequence of recent mb transitions performed by the agent. If the agent starts at s0... rti... rtj from the environment and moves through intermediate states, the sequence is assigned to Rt j 0} by performing a series of actions a0... ai... aj, it receives rewards rt0... rtj from the environment. These transitions include the transition sequences that include the transition sequences if they are assigned to the sequence as {sj0 \u00b2 sequence j 0} Rt j 0}, if j \u2212 mb, j \u2212 mb, j \u2212 mb, j \u2212 mb, j \u2212 mb, j \u2212 mb} otherwise (1), where: Syx... {si... T si... ax \u2212 T"}, {"heading": "3.2. Virtual Transition Sequences", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3.3. Replaying the Transition Sequences", "text": "In order to use the transition sequences described above, each of the triads between state and action and reward {s a r} is repeated in the transition sequence Lv as if the agent had actually experienced them. In order to achieve a faster propagation of information, the sequences are repeated in the reverse temporal order in which they were observed, as in Algorithm 2. Such an idea was briefly mentioned in previous studies on the reproduction of experiences [2, 3]. Furthermore, a similar mechanism of reverse reproduction was observed in animals [31]. Likewise, sequences in L can also be repeated from time to time. Reproduction of sequences from L and Lv in this way causes the effects of large absolute TD errors arising from higher up in the sequence to propagate through the respective transitions, which ultimately leads to better estimates of the value function."}, {"heading": "4. Results and Discussion", "text": "We show our approach with modified versions of two standard reinforcement learning tasks: the first is a problem of navigation / puddles world (Figure 3), and the second is a problem in the mountains (Figure 5). In both problems, behavioural strategies are developed to solve a particular task (which we call the primary task) relatively greedily, while the value function for another interesting task (which we call the secondary task) is simultaneously learned in a non-political way. Secondary task is intentionally made more difficult by making appropriate changes to the environment. Such an attitude best demonstrates the effectiveness of our approach and emphasises its advantages over other approaches to repetition of experiences. We characterise the difficulty of the secondary task with a difficulty level of \u03c1, which is the fraction of the implemented behavioural policies that receive a high reward with regard to the secondary task. A low value of B indicates that it is difficult to fulfil the secondary task within the predetermined behavioural policy framework."}, {"heading": "4.1. Navigation/Puddle-World Task", "text": "In the navigation environment, the simulated agent is assigned the task of navigating to specific locations in its vicinity. We consider two locations, Q = secondary character, each representing the primary and secondary task location. The environment is designed so that the location that corresponds to high rewards in terms of the secondary task is far removed from that of the primary task (see Figure 3). In addition, accessibility to the secondary task location is consciously limited by the environment - and is able to detect obstacles on all but one side. These changes contribute to a low value of the primary task, especially when the agent is working with a greedy behavior policy in relation to the primary task (see Figure 3). The agent assumes that he can accurately detect his location in the vicinity and is able to detect obstacles right in front of it up to 1 unit away. He can move in the environment at a maximum speed of 1 unit."}, {"heading": "4.2. Mountain Car Task", "text": "In the mountain car task, the agent is relatively greedy for the primary task, with secondary updates. A primary task is assigned to get out of the trough and visit the starting point P1. The action of the visit point P2 is considered a secondary task. However, the agent is assigned a high reward (100) for fulfilling the respective goals, and a living penalty (-1) is assigned for all other situations. In each step, the agent can choose from three possible actions: (1) acceleration in the positive x-direction, (2) acceleration in the negative x-direction, and (3) application of no control. The environment is discredited in such a way that 120 unique positions and 100 unique velocity values are possible. The mountain is described by the equation y = e \u2212 0.5x sin (4x) so that this point P2 is higher than P1. Also, the average slope leading to P2 is steeper than the one leading to P1."}, {"heading": "5. Conclusion", "text": "Using these sequences, we were able to demonstrate that it is possible to construct virtual experiences in the form of virtual transition sequences that can be reproduced to improve an agent's learning, especially in environments where desirable events rarely occur. We demonstrated the benefits of this approach by applying it to versions of standard augmentation learning tasks, such as the puddle world and mountain stages. In both tasks, a significant improvement in learning speed was observed compared to regular Q-Learning and other forms of experience reproduction. Furthermore, the influence of the various memory parameters used was described and empirically assessed, and possible extensions of this work were briefly discussed. Characterized by controllable memory parameters and the potential to significantly improve the efficiency of exploration at the expense of increasing factors in the calculation of the prime number."}, {"heading": "Acknowledgements", "text": "The authors thank Richard S. Sutton of the University of Alberta for his feedback and many helpful discussions during the development of this work."}], "references": [{"title": "Experience replay for real-time reinforcement learning control", "author": ["S. Adam", "L. Busoniu", "R. Babuska"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "in: International Conference on Learning Representations, Puerto Rico", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Identification and off-policy learning of multiple objectives using adaptive clustering", "author": ["T.G. Karimpanal", "E. Wilhelm"], "venue": "Neurocomputing (In Press) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Self-improving reactive agents based on reinforcement learning", "author": ["L.-J. Lin"], "venue": "planning and teaching, Machine Learning 8 (3-4) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "A", "author": ["R.S. Sutton"], "venue": "G. Barto, Reinforcement learning: An introduction ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "B", "author": ["M. Geist"], "venue": "Scherrer, et al., Off-policy learning with eligibility traces: a survey., Journal of Machine Learning Research 15 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "in: The 10th International Conference on Autonomous Agents and Multiagent Systems- Volume 2, International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Scaling life-long off-policy learning", "author": ["A. White", "J. Modayil", "R.S. Sutton"], "venue": "in: Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on, IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-timescale nexting in a reinforcement learning robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Adaptive Behavior 22 (2) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient exploration in reinforcement learning", "author": ["S.B. Thrun"], "venue": "Tech. rep., Pittsburgh, PA, USA ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["P.S. Thomas", "E. Brunskill"], "venue": "in: International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Quasi-online reinforcement learning for robots", "author": ["B. Bakker", "V. Zhumatiy", "G. Gruener", "J. Schmidhuber"], "venue": "in: Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research 32 (11) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Simulation and the Monte Carlo method", "author": ["R.Y. Rubinstein", "D.P. Kroese"], "venue": "John Wiley & Sons", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland"], "venue": "Ostrovski, et al., Human-level control through deep reinforcement learning, Nature 518 (7540) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "in: International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Babu\u0161ka, The importance of experience replay database composition in deep reinforcement learning, in: Deep Reinforcement Learning", "author": ["T. de Bruin", "J. Kober", "R.K. Tuyls"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T.D. Kulkarni", "R. Barzilay"], "venue": "in: L. M\u00e0rquez, C. Callison-Burch, J. Su, D. Pighin, Y. Marton (Eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, The Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Surprise and curiosity for big data robotics", "author": ["A. White", "J. Modayil", "R.S. Sutton"], "venue": "in: AAAI-14 Workshop on Sequential Decision-Making with Big Data, Quebec City, Quebec, Canada", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient exploration in reinforcement learning", "author": ["S.B. Thrun"], "venue": "Tech. rep. ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Machine learning 13 (1) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Planning by Prioritized Sweeping with Small Backups", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "in: Proceedings of the 30th International Conference on Machine Learning, Cycle 3,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Hippocampal place cells construct reward related sequences through unexplored space", "author": ["H.F. \u00d3lafsd\u00f3ttir", "C. Barry", "A.B. Saleem", "D. Hassabis", "H.J. Spiers"], "venue": "Elife 4 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Reactivation", "author": ["L. Buhry", "A.H. Azizi", "S. Cheng"], "venue": "replay, and preplay: how it might all fit together, Neural plasticity 2011 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Rewarded outcomes enhance reactivation of experience in the hippocampus", "author": ["A.C. Singer", "L.M. Frank"], "venue": "Neuron 64 (6) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Integrated architectures for learning", "author": ["R.S. Sutton"], "venue": "planning, and reacting based on approximating dynamic programming, in: Proceedings of the Seventh Int. Conf. on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1990}, {"title": "Swarm-enabling technology for multi-robot systems", "author": ["M. Chamanbaz", "D. Mateo", "B.M. Zoss", "G. Toki\u0107", "E. Wilhelm", "R. Bouffanais", "D.K.P. Yue"], "venue": "Front. Robot. AI 4 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2017}, {"title": "Building a library of policies through policy reuse", "author": ["F. Fern\u00e1ndez", "M. Veloso"], "venue": "Tech. Rep. CMU-CS-05-174, Computer Science Department, Carnegie Mellon University, Pittsburgh, PA ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Reverse replay of behavioural sequences in hippocampal place cells during the awake state", "author": ["D.J. Foster", "M.A. Wilson"], "venue": "Nature 440 (7084) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "exploration generally exceeds the computational cost of learning [1\u20134].", "startOffset": 65, "endOffset": 70}, {"referenceID": 1, "context": "exploration generally exceeds the computational cost of learning [1\u20134].", "startOffset": 65, "endOffset": 70}, {"referenceID": 2, "context": "exploration generally exceeds the computational cost of learning [1\u20134].", "startOffset": 65, "endOffset": 70}, {"referenceID": 3, "context": "Experience replay [5] is a technique that reuses information gathered from past experiences to improve the efficiency of learning.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "In order to replay stored experiences using this approach, an off-policy [6, 7] setting is a prerequisite.", "startOffset": 73, "endOffset": 79}, {"referenceID": 5, "context": "In order to replay stored experiences using this approach, an off-policy [6, 7] setting is a prerequisite.", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "These algorithms can hence be used to parallelize learning, and, thus gather as much knowledge as possible using real experiences [8\u201310].", "startOffset": 130, "endOffset": 136}, {"referenceID": 7, "context": "These algorithms can hence be used to parallelize learning, and, thus gather as much knowledge as possible using real experiences [8\u201310].", "startOffset": 130, "endOffset": 136}, {"referenceID": 8, "context": "These algorithms can hence be used to parallelize learning, and, thus gather as much knowledge as possible using real experiences [8\u201310].", "startOffset": 130, "endOffset": 136}, {"referenceID": 9, "context": "The problem of learning from limited experience is not new in the field of RL [11, 12].", "startOffset": 78, "endOffset": 86}, {"referenceID": 10, "context": "The problem of learning from limited experience is not new in the field of RL [11, 12].", "startOffset": 78, "endOffset": 86}, {"referenceID": 11, "context": "Particularly for robotics applications, these factors are even more important, as exploration of the environment is typically time and energy expensive [13, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 12, "context": "Particularly for robotics applications, these factors are even more important, as exploration of the environment is typically time and energy expensive [13, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 4, "context": "When the behavior and target policies vary considerably from each other, importance sampling [6, 15] is commonly used in order to obtain better estimates of the value functions.", "startOffset": 93, "endOffset": 100}, {"referenceID": 13, "context": "When the behavior and target policies vary considerably from each other, importance sampling [6, 15] is commonly used in order to obtain better estimates of the value functions.", "startOffset": 93, "endOffset": 100}, {"referenceID": 0, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 14, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 15, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 16, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 1, "context": "Recent works [3, 20] have explored different ways in which transitions may be prioritized.", "startOffset": 13, "endOffset": 20}, {"referenceID": 17, "context": "Recent works [3, 20] have explored different ways in which transitions may be prioritized.", "startOffset": 13, "endOffset": 20}, {"referenceID": 1, "context": "[3] prioritized transitions on the basis of their associated TD errors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[2] involved some variants that replayed sequences of experiences, but these sequences were drawn randomly from the replay memory.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 92, "endOffset": 103}, {"referenceID": 18, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 92, "endOffset": 103}, {"referenceID": 19, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 92, "endOffset": 103}, {"referenceID": 20, "context": "In particular, the modelbased approach of prioritized sweeping [23, 24] prioritizes backups that are expected to result in a significant change in the value function.", "startOffset": 63, "endOffset": 71}, {"referenceID": 21, "context": "In particular, the modelbased approach of prioritized sweeping [23, 24] prioritizes backups that are expected to result in a significant change in the value function.", "startOffset": 63, "endOffset": 71}, {"referenceID": 22, "context": "Replaying sequences of experiences also seems to be biologically plausible [25, 26].", "startOffset": 75, "endOffset": 83}, {"referenceID": 23, "context": "Replaying sequences of experiences also seems to be biologically plausible [25, 26].", "startOffset": 75, "endOffset": 83}, {"referenceID": 24, "context": "In addition, it is known that animals tend to remember experiences that lead to high rewards [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "Some early approaches in RL, such as the dyna architecture [28] also made use of simulated experience to improve the value function estimates.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "Our approach also recognizes the real-world limitations of replay memory [19], and stores only a certain amount of information at a time, specified by memory parameters.", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "[29]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20], where transitions associated with positive rewards are prioritized for replay.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "These sequences are maintained in the library L in a manner similar to the Policy Library through Policy Reuse (PLPR) algorithm [30].", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "Such an idea has been briefly mentioned in previous studies on experience replay [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 1, "context": "Such an idea has been briefly mentioned in previous studies on experience replay [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 28, "context": "In addition, a similar mechanism of reverse replay has been observed in animals [31].", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "Experience replay is one of the most commonly used approaches to improve the sample efficiency of reinforcement learning algorithms. In this work, we propose an approach to select and replay sequences of transitions in order to accelerate the learning of a reinforcement learning agent in an off-policy setting. In addition to selecting appropriate sequences, we also artificially construct transition sequences using information gathered from previous agentenvironment interactions. These sequences, when replayed, allow value function information to trickle down to larger sections of the state/state-action space, thereby making the most of the agent\u2019s experience. We demonstrate our approach on modified versions of standard reinforcement learning tasks such as the mountain car and puddle world problems and empirically show that it enables better learning of value functions as compared to other forms of experience replay. Further, we briefly discuss some of the possible extensions to this work, as well as applications and situations where this approach could be particularly useful.", "creator": "LaTeX with hyperref package"}}}