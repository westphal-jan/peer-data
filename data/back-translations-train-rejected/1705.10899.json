{"id": "1705.10899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Propositional Knowledge Representation in Restricted Boltzmann Machines", "abstract": "Representing symbolic knowledge into a connectionist network is the key element for the integration of scalable learning and sound reasoning. Most of the previous studies focus on discriminative neural networks which unnecessarily require a separation of input/output variables. Recent development of generative neural networks such as restricted Boltzmann machines (RBMs) has shown a capability of learning semantic abstractions directly from data, posing a promise for general symbolic learning and reasoning. Previous work on Penalty logic show a link between propositional logic and symmetric connectionist networks, however it is not applicable to RBMs. This paper proposes a novel method to represent propositional formulas into RBMs/stack of RBMs where Gibbs sampling can be seen as MaxSAT. It also shows a promising use of RBMs to learn symbolic knowledge through maximum likelihood estimation.", "histories": [["v1", "Wed, 31 May 2017 00:24:16 GMT  (52kb,D)", "http://arxiv.org/abs/1705.10899v1", null], ["v2", "Thu, 1 Jun 2017 00:19:24 GMT  (52kb,D)", "http://arxiv.org/abs/1705.10899v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["son n tran"], "accepted": false, "id": "1705.10899"}, "pdf": {"name": "1705.10899.pdf", "metadata": {"source": "CRF", "title": "Propositional Knowledge Representation in Restricted Boltzmann Machines", "authors": ["Son N. Tran"], "emails": ["son.tran@csiro.au"], "sections": [{"heading": "1 Introduction", "text": "Over the last two decades, we have seen the development of neural integration systems, both in the way in which people in individual countries are able to behave, in the way in which they are able to behave, and in the way in which they are able to behave, and in the way in which they are able to behave."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "A symmetrical network is a neural network with bidirectional connections characterized by a quadratic function called energy. However, the weights of the connections are stored in a symmetrical matrix. An SCN acts like a memory in which information is stored in lower energy states. Conclusions in such a system can be considered as a search for the minimum energy, which is either deterministic as in Hoffield networks (HN) [Hopfield, 1982] or stochastical as in Boltzmann machines (BM, 1989]. A simplified version of stochastic SCN, i.e. Boltzmann machines, is called a restricted Boltzmann machine (RBM) in which units are separated in the same layers [Smolensky, 1986; Hinton, 2002]."}, {"heading": "2.2 Penalty Logic", "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move, to move, to move and to move."}, {"heading": "3 Propositional Calculus and RBMs", "text": "In fact, most of them are able to move to another world, in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "4 Representing Propositional Knowledge in RBMs", "text": "We now generalize the symbolic representation of trust subjunctive clauses [Penning et al., 2011; Son Tran and Garcez, 2012; Tran and d'Avila Garcez, 2013; Tran and Garcez, 2016] to show the equivalence between RBMs and a statement, i.e. a set of weighted statement formulas. We then show that by stacking the rules in layers, we can present statements in DBNs."}, {"heading": "4.1 Confidence Rules", "text": "We can present a series of formulas that present the relations between the rules and the RBMs as empirical results."}, {"heading": "4.2 Stacking of Confidence rules", "text": "In many cases, we can imagine a higher level in which we can attain a higher level of knowledge. (h1) The upper level (h2) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the lowest level (h3) of the top level (h3) of the lowest level (h3) of the lowest level (h3)."}, {"heading": "5 Knowledge Approximation", "text": "We have shown how we represent the conceivable knowledge in RBMs / RBMs, although it is possible to find maximum satisfaction is tantamount to minimizing energy functions. For completeness, we are now in a position where we can discuss the ability of using RBMs to gain knowledge from the data. As shown in [Pinkas, 1995], one can apply a symbolic learning system rather than using symbolic learning rules to construct it. We can apply the similar idea to construct an RBM from truth assignments. However, there is more interest in how to gain symbolic knowledge through the learning ability of connectionist networks, rather than using symbolic learning rules to construct it. Learning in a connectionist network is seen as an approximation of the preferred models D = {x)."}, {"heading": "6 Discussion", "text": "This paper deals with two main problems in the unification of symbolic logic and neural networks, although limited Boltzmann machines are preferred. First, we show how to represent a propositional logic program in an uncontrolled energy-based connectionist network. Second, we show the possibility of using statistical learning in RBMs to approximate propositional knowledge. The results of this work may provide a theoretical basis for further research into uncontrolled neural symbolic integration, reasoning and knowledge-learning and extraction. A promising application of this work may be neural symbolic integration and reasoning where background knowledge is encoded in RBMs / DBNs in order to be able to better learn and argue more effectively. Here, we can take advantage of RBMs where inference is efficient. An exciting future work that we want to investigate is the integration of firstborn logic into RBMs / DBNs. Practice would be to restrict the exact rules of learning."}], "references": [{"title": "The connectionist inductive learning and logic programming system", "author": ["Avila Garcez", "Zaverucha", "1999] Artur S. Avila Garcez", "Gerson Zaverucha"], "venue": "Applied Intelligence,", "citeRegEx": "Garcez et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Garcez et al\\.", "year": 1999}, {"title": "Symbolic knowledge extraction from trained neural networks: A sound approach", "author": ["d\u2019Avila Garcez et al", "2001] A.S. d\u2019Avila Garcez", "K. Broda", "D.M. Gabbay"], "venue": "Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "d\u2019AvilaGarcez. Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["Fran\u00e7a et al", "2014] Manoel V.M. Fran\u00e7a", "Gerson Zaverucha", "Artur S"], "venue": "Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Neural-Symbolic Cognitive Reasoning", "author": ["Garcez et al", "2008] Artur S. d\u2019Avila Garcez", "Lus C. Lamb", "Dov M. Gabbay"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton et al", "2006] Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Comput.,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Lee et al", "2009] Honglak Lee", "Peter T. Pham", "Yan Largman", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Acoustic modeling using deep belief networks", "author": ["Mohamed et al", "2012] Abdel-rahman Mohamed", "George Dahl", "Geoffrey Hinton"], "venue": "IEEE Transactions on Audio, Speech & Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "A neuralsymbolic cognitive agent for online learning and reasoning", "author": ["Penning et al", "2011] Leo de Penning", "Artur S. d\u2019Avila Garcez", "Lus C. Lamb", "John-Jules Ch Meyer"], "venue": "In IJCAI,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Propositional non-monotonic reasoning and inconsistency in symmetric neural networks", "author": ["Pinkas", "1991a] Gadi Pinkas"], "venue": "In Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1,", "citeRegEx": "Pinkas and Pinkas.,? \\Q1991\\E", "shortCiteRegEx": "Pinkas and Pinkas.", "year": 1991}, {"title": "Symmetric neural networks and propositional logic satisfiability", "author": ["Pinkas", "1991b] Gadi Pinkas"], "venue": "Neural Comput.,", "citeRegEx": "Pinkas and Pinkas.,? \\Q1991\\E", "shortCiteRegEx": "Pinkas and Pinkas.", "year": 1991}, {"title": "Knowledge, reasoning, and planning", "author": ["Russell", "Norvig", "2003] Stuart Russell", "Peter Norvig"], "venue": "In Artificial Intelligent: A Modern Approach. Pearson Education,", "citeRegEx": "Russell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2003}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": "[Smolensky,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Connectionist ai, symbolic ai, and the brain", "author": ["P. Smolensky"], "venue": "[Smolensky,", "citeRegEx": "Smolensky.,? \\Q1987\\E", "shortCiteRegEx": "Smolensky.", "year": 1987}, {"title": "Constituent structure and explanation in an integrated connectionist/symbolic cognitive architecture", "author": ["Paul Smolensky"], "venue": "[Smolensky,", "citeRegEx": "Smolensky.,? \\Q1995\\E", "shortCiteRegEx": "Smolensky.", "year": 1995}, {"title": "ICML logic extraction from deep belief networks", "author": ["Son Tran", "Garcez", "2012] Son Tran", "Artur Garcez"], "venue": "In ICML 2012 Representation Learning Workshop,", "citeRegEx": "Tran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2012}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Soudry et al", "2014] Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "The extraction of refined rules from knowledgebased neural networks", "author": ["Towell", "Shavlik", "1993] Geoffrey G. Towell", "Jude W. Shavlik"], "venue": "In Machine Learning,", "citeRegEx": "Towell et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1993}, {"title": "Knowledge-based artificial neural networks", "author": ["Towell", "Shavlik", "1994] Geoffrey G. Towell", "Jude W. Shavlik"], "venue": "Artificial Intelligence,", "citeRegEx": "Towell et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1994}, {"title": "Knowledge extraction from deep belief networks for images", "author": ["Tran", "d\u2019Avila Garcez", "2013] Son N. Tran", "Artur d\u2019Avila Garcez"], "venue": "In IJCAI-2013 Workshop on NeuralSymbolic Learning and Reasoning,", "citeRegEx": "Tran et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2013}, {"title": "Deep logic networks: Inserting and extracting knowledge from deep belief networks", "author": ["Tran", "Garcez", "2016] Son Tran", "Artur Garcez"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Knowledge infusion", "author": ["G. Valiant"], "venue": "[Valiant,", "citeRegEx": "Valiant.,? \\Q2006\\E", "shortCiteRegEx": "Valiant.", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "In AI research, there have been much debate over symbolism and connectionism as they are two key opposed paradigms for information processing [Smolensky, 1987; Minsky, 1991].", "startOffset": 142, "endOffset": 173}, {"referenceID": 13, "context": "Despite their difference, there is a strong argument that combination of the two should offer joint benefits [Smolensky, 1995; Valiant, 2006; Garcez et al., 2008].", "startOffset": 109, "endOffset": 162}, {"referenceID": 20, "context": "Despite their difference, there is a strong argument that combination of the two should offer joint benefits [Smolensky, 1995; Valiant, 2006; Garcez et al., 2008].", "startOffset": 109, "endOffset": 162}, {"referenceID": 11, "context": "Also, recent emergence of representation learning shows that unsupervised connectionist networks with latent variables such as restricted Boltzmann machines (RBMs) can learn semantic patterns from large amount of data efficiently [Smolensky, 1986; Hinton, 2002].", "startOffset": 230, "endOffset": 261}, {"referenceID": 12, "context": "Connectionist systems normally refer to a set of models made by interconnected networks of computational neurons (also called \u2019units\u2019) [Smolensky, 1987; Hinton, 1989].", "startOffset": 135, "endOffset": 166}, {"referenceID": 11, "context": "Boltzmann machines, is called restricted Boltzmann machine (RBM) where units in the same layers are disconnected [Smolensky, 1986; Hinton, 2002].", "startOffset": 113, "endOffset": 144}], "year": 2017, "abstractText": "Representing symbolic knowledge into a connectionist network is the key element for the integration of scalable learning and sound reasoning. Most of the previous studies focus on discriminative neural networks which unnecessarily require a separation of input/output variables. Recent development of generative neural networks such as restricted Boltzmann machines (RBMs) has shown a capability of learning semantic abstractions directly from data, posing a promise for general symbolic learning and reasoning. Previous work on Penalty logic show a link between propositional logic and symmetric connectionist networks, however it is not applicable to RBMs. This paper proposes a novel method to represent propositional formulas into RBMs/stack of RBMs where Gibbs sampling can be seen as MaxSAT. It also shows a promising use of RBMs to learn symbolic knowledge through maximum likelihood estimation.", "creator": "LaTeX with hyperref package"}}}