{"id": "1705.10900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology Based Representations", "abstract": "We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like $\\textit{tf-idf}$, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.", "histories": [["v1", "Wed, 31 May 2017 00:43:04 GMT  (474kb,D)", "http://arxiv.org/abs/1705.10900v1", "5 pages, 3 figures. Rep4NLP workshop at ACL 2017"]], "COMMENTS": "5 pages, 3 figures. Rep4NLP workshop at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul michel", "abhilasha ravichander", "shruti rijhwani"], "accepted": false, "id": "1705.10900"}, "pdf": {"name": "1705.10900.pdf", "metadata": {"source": "CRF", "title": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology Based Representations", "authors": ["Paul Michel", "Abhilasha Ravichander", "Shruti Rijhwani"], "emails": ["pmichel1@cs.cmu.edu", "aravicha@cs.cmu.edu", "srijhwan@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Given an embedding model that maps words to n-dimensional vectors, each document can be presented as a finite subset of Rn. Comparing documents then amounts to comparing such subsets. While previous work shows that the distance of terrestrial man (Kusner et al., 2015) or the distance between the weighted average of word vectors (Arora et al., 2017) provides information useful for classification tasks, we would like to go a step further and investigate whether useful information can also be found in the \"form\" of a document embedded in word. Persistent homology is a tool from algebraic topology used to calculate topological signatures (so-called persistence diagrams) in a compact metric manner. * The authors shown also contributed to this work, which has the property of being stable in relation to the Gromov-Haussdorff distance (Gromov et al., 1981)."}, {"heading": "2 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Word embeddings", "text": "As a first step, we calculate word vectors for each document in our corpus using a word2vec model (Mikolov et al., 2013), which was trained on the Googlear Xiv: 170 5.10 900v 1 [cs.C L] of 31 May 201 7News dataset 1. Apart from being a widely used word embedding technique, word2vec is known to have interesting linear properties with respect to analogies (Mikolov et al., 2013), indicating a rich semantic structure."}, {"heading": "2.2 Gromov-Haussdorff Distance", "text": "\"It's as if he was in a position to be in,\" he says."}, {"heading": "2.3 Persistence diagrams", "text": "The efficient calculation of GH distance is still an open problem, despite much recent work in this area. (VB and Sapiro, 2005; Bronstein et al., 2006; Me \ufffd moli, 2007; Agarwal et al., 2015) Fortunately, Carrie et al. (2015) offers us a way to derive a signature that is stable in relation to GH distance. More specifically, with a finite point cloud A-Rn, the persistence chart of Vietori rip filtration is compressible to A, Dg (A), an approach inspired by persistent homology, a subfield of algebraic topology. The rigorous definition of these terms is not the crux of this paper and we will present them only informally; the curious reader is invited to refer to Zhu (2013) for a brief introduction. Further details are given in Delfinado and Edelsbrunner et al."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experiments", "text": "To create a persistence diagram, we convert each document into the set of its word vectors, then use Dionysus (Morozov, 2008- 2016), a C + + library for calculating persistence diagrams, and form the signatures described in 2.3. Then we refer to these diagrams as embedding in persistent homology (PH). Once we have the embedding for each document, they can be used as input for standard cluster or classification algorithms. As a base document presentation, we use the average of word vectors for this document (hereinafter referred to as AW2V embedding), and for clustering, we experiment with K-Means and Gaussian Mixture Models (GMM) on a subset 4 of the dataset of the twenty newsgroups."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Hyper-parameters", "text": "Our method is based on very few hyperparameters. Our most important decisions are listed below. Distance selection We experimented with both Euclidean distance and cosmic similarity (angular distance). After preliminary experiments we found that both were identical and that therefore we only reported results with Euclidean distance.Calculation of the persistence plotThe hyperparameters of the diagram calculation are monotonous and largely determine the degree of approximation. We set them to the highest values, which allowed our experiment to run in a reasonable time5."}, {"heading": "4.2 Document Clustering", "text": "We perform cluster experiments with the characteristics of the base document (AW2V), tf-idf, and our pH signatures. Figure 3 shows the B-Cubed precision, the recall, and the F1 score of each method (metrics as defined in Amigo \u0301 et al. (2009)). To further assess the benefits of pH embedding, we link them to AW2V to obtain a third representation, AW2V + PH. With GMM and AW2V + PH, the F1 score of cluster formation is 0.499. In terms of F1 and precision, we see that tf-idf representations perform better than PH, for reasons we will discuss in later sections. In terms of memory, both PH and AW2V perform fairly well. It is important that all metrics for the pH are well above the random baseline, indicating that some valuable information is contained in them."}, {"heading": "4.3 Sentiment Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Sentence-Level Sentiment Analysis", "text": "The results are presented in Table 1. For comparison, we provide results for one of the most advanced models, a CNN-based set classifier (Kim, 5Selected so that calculating the chart of the longest file in the training data took less than 10 minutes).6In the absence of a canonical split, we use a random 10% of the data set as a 2014 test set. We find that pH embedding alone is not useful in predicting the mood of each set. AW2V performs reasonably in this task, but the combination of the two representations does not affect the accuracy at all."}, {"heading": "4.3.2 Document-Level Sentiment Analysis", "text": "We perform a binary sentiment classification at the document level based on the IMDb Movie Reviews dataset (Maas et al., 2011). In this experiment, we use sentence vectors, each of which is the average of the word vectors in this dataset. Results are presented in Table 2. We compare our results with the paragraph vector approach (Le and Mikolov, 2014). We find that pH embedding in this dataset performs poorly. Similar to the CSP dataset, AW2V embedding provides acceptable results. The combined representation performs slightly better, but not significantly."}, {"heading": "5 Discussion and Analysis", "text": "As can be seen in Figure 3, the pH representation does not perform better than tf-idf or AW2V and is in fact often not much better than chance. One possible reason is linked to the nature of our datasets: The calculation of the persistence diagram is very sensitive to the size of the documents, and the geometry of small documents where the number of words is negligible in terms of the dimensionality of the word vectors is not very plentiful, the resulting topological signatures are very sparse, which poses a problem for both CSP and documents in IMDb and Twenty newsgroups that contain only one line. On the opposite side of the spectrum, persistence diagrams are difficult to calculate without scanning very long documents (which in turn negatively affects the representation of smaller documents).We have conducted an additional experiment on a subset of the IMDb corpus that contains only documents of appropriate length, but can only be explained by the fact that similar results can be achieved by combining AWV."}, {"heading": "6 Related Work", "text": "Learning distributed representations of sentences or documents for downstream classification and information retrieval tasks has recently attracted attention due to their usefulness in multiple applications, whether in representations trained at the trend / sales level Le and Mikolov (2014); Kiros et al. (2015) or purely word vector-based methods Arora et al. (2017). Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied. Topological data analyses have been used for various tasks, such as 3D form classification (Chazal et al., 2009) or protein structure analysis (Xia and Wei, 2014)."}, {"heading": "7 Conclusion", "text": "Based on our experiments, the use of persistence diagrams to represent text does not appear to contribute positively to the documentation of cluster and sentiment classification tasks. Certainly, there are merits of the method, particularly its strong mathematical foundation and domain-independent, unsupervised nature. Theoretically, algebraic topology has the ability to capture structural relationships, and this could potentially benefit syntax-based NLP tasks such as parsing. We plan to investigate this link in the future."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA / I2O under contract number HR0011-15-C-0114. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We thank Matt Gormley, Hyun Ah Song, Shivani Poddar and Hai Pham for their suggestions in writing this paper, and Steve Oudot for pointing to helpful references. We would also like to thank the anonymous ACL critics for their valuable input."}], "references": [{"title": "Computing the gromov-hausdorff distance for metric trees", "author": ["Pankaj K Agarwal", "Kyle Fox", "Abhinandan Nath", "Anastasios Sidiropoulos", "Yusu Wang."], "venue": "International Symposium on Algorithms and Computation. Springer, pages 529\u2013540.", "citeRegEx": "Agarwal et al\\.,? 2015", "shortCiteRegEx": "Agarwal et al\\.", "year": 2015}, {"title": "A comparison of extrinsic clustering evaluation metrics based on formal constraints", "author": ["Enrique Amig\u00f3", "Julio Gonzalo", "Javier Artiles", "Felisa Verdejo."], "venue": "Information retrieval 12(4):461\u2013486.", "citeRegEx": "Amig\u00f3 et al\\.,? 2009", "shortCiteRegEx": "Amig\u00f3 et al\\.", "year": 2009}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."], "venue": "International Conference on Learning Representations. To Appear.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Efficient computation of isometry-invariant distances between surfaces", "author": ["Alexander M Bronstein", "Michael M Bronstein", "Ron Kimmel."], "venue": "SIAM Journal on Scientific Computing 28(5):1812\u2013 1836.", "citeRegEx": "Bronstein et al\\.,? 2006", "shortCiteRegEx": "Bronstein et al\\.", "year": 2006}, {"title": "Stable topological signatures for points on 3d shapes", "author": ["Mathieu Carri\u00e8re", "Steve Y Oudot", "Maks Ovsjanikov."], "venue": "Computer Graphics Forum. Wiley Online Library, volume 34, pages 1\u201312.", "citeRegEx": "Carri\u00e8re et al\\.,? 2015", "shortCiteRegEx": "Carri\u00e8re et al\\.", "year": 2015}, {"title": "Gromov-hausdorff stable signatures for shapes using persistence", "author": ["Fr\u00e9d\u00e9ric Chazal", "David Cohen-Steiner", "Leonidas J Guibas", "Facundo M\u00e9moli", "Steve Y Oudot."], "venue": "Computer Graphics Forum. Wiley Online Library, volume 28, pages 1393\u20131403.", "citeRegEx": "Chazal et al\\.,? 2009", "shortCiteRegEx": "Chazal et al\\.", "year": 2009}, {"title": "An incremental algorithm for betti numbers of simplicial complexes on the 3-sphere", "author": ["Cecil Jose A Delfinado", "Herbert Edelsbrunner."], "venue": "Computer Aided Geometric Design 12(7):771\u2013784.", "citeRegEx": "Delfinado and Edelsbrunner.,? 1995", "shortCiteRegEx": "Delfinado and Edelsbrunner.", "year": 1995}, {"title": "Topological persistence and simplification", "author": ["Herbert Edelsbrunner", "David Letscher", "Afra Zomorodian."], "venue": "Discrete and Computational Geometry 28(4):511\u2013533.", "citeRegEx": "Edelsbrunner et al\\.,? 2002", "shortCiteRegEx": "Edelsbrunner et al\\.", "year": 2002}, {"title": "Structures m\u00e9triques pour les vari\u00e9t\u00e9s riemanniennes", "author": ["Mikhael Gromov", "Jacques Lafontaine", "Pierre Pansu"], "venue": null, "citeRegEx": "Gromov et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Gromov et al\\.", "year": 1981}, {"title": "A brief survey of text mining", "author": ["Andreas Hotho", "Andreas N\u00fcrnberger", "Gerhard Paa\u00df."], "venue": "Ldv Forum.", "citeRegEx": "Hotho et al\\.,? 2005", "shortCiteRegEx": "Hotho et al\\.", "year": 2005}, {"title": "Similarity measures for text document clustering", "author": ["Anna Huang."], "venue": "Proceedings of the sixth new zealand computer science research student conference (NZCSRSC2008), Christchurch, New Zealand. pages 49\u201356.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "In EMNLP.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Nonnegative matrix factorization for interactive topic modeling and document clustering", "author": ["Da Kuang", "Jaegul Choo", "Haesun Park."], "venue": "Partitional Clustering Algorithms, Springer, pages 215\u2013243.", "citeRegEx": "Kuang et al\\.,? 2015", "shortCiteRegEx": "Kuang et al\\.", "year": 2015}, {"title": "From word embeddings to document distances", "author": ["Matt J Kusner", "Yu Sun", "Nicholas I Kolkin", "Kilian Q Weinberger"], "venue": "In ICML", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 2126 June 2014. pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "On the use of gromovhausdorff distances for shape comparison", "author": ["Facundo M\u00e9moli"], "venue": null, "citeRegEx": "M\u00e9moli.,? \\Q2007\\E", "shortCiteRegEx": "M\u00e9moli.", "year": 2007}, {"title": "A theoretical and computational framework for isometry invariant recognition of point cloud data", "author": ["Facundo M\u00e9moli", "Guillermo Sapiro."], "venue": "Foundations of Computational Mathematics 5(3):313\u2013347.", "citeRegEx": "M\u00e9moli and Sapiro.,? 2005", "shortCiteRegEx": "M\u00e9moli and Sapiro.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Unsupervised document classification with informed topic models", "author": ["Timothy A Miller", "Dmitriy Dligach", "Guergana K Savova."], "venue": "ACL .", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Dyonisus : a c++ library for computing persistent homology. http: //mrzv.org/software/dionysus", "author": ["Dmitriy Morozov"], "venue": null, "citeRegEx": "Morozov.,? \\Q2008\\E", "shortCiteRegEx": "Morozov.", "year": 2008}, {"title": "Dependency tree-based sentiment classification using crfs with hidden variables", "author": ["Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter", "citeRegEx": "Nakagawa et al\\.,? 2010", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2010}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the ACL.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Towards computing homology from finite approximations", "author": ["Vanessa Robins."], "venue": "Topology proceedings. volume 24, pages 503\u2013532.", "citeRegEx": "Robins.,? 1999", "shortCiteRegEx": "Robins.", "year": 1999}, {"title": "A metric for distributions with applications to image databases", "author": ["Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas."], "venue": "Computer Vision, 1998. Sixth International Conference on. IEEE, pages 59\u201366.", "citeRegEx": "Rubner et al\\.,? 1998", "shortCiteRegEx": "Rubner et al\\.", "year": 1998}, {"title": "A comparison of document clustering techniques", "author": ["Michael Steinbach", "George Karypis", "Vipin Kumar"], "venue": "In KDD workshop on text mining", "citeRegEx": "Steinbach et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Steinbach et al\\.", "year": 2000}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D. Manning."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Recent trends in hierarchic document clustering: a critical review", "author": ["Peter Willett."], "venue": "Information Processing & Management 24(5):577\u2013597.", "citeRegEx": "Willett.,? 1988", "shortCiteRegEx": "Willett.", "year": 1988}, {"title": "Persistent homology analysis of protein structure, flexibility, and folding", "author": ["Kelin Xia", "Guo-Wei Wei."], "venue": "International journal for numerical methods in biomedical engineering 30(8):814\u2013844.", "citeRegEx": "Xia and Wei.,? 2014", "shortCiteRegEx": "Xia and Wei.", "year": 2014}, {"title": "Document clustering by concept factorization", "author": ["Wei Xu", "Yihong Gong."], "venue": "Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New", "citeRegEx": "Xu and Gong.,? 2004", "shortCiteRegEx": "Xu and Gong.", "year": 2004}, {"title": "Persistent homology: An introduction and a new text representation for natural language processing", "author": ["Xiaojin Zhu."], "venue": "Proceedings of the 23rd International Joint Conference on Artificial Intelligence.", "citeRegEx": "Zhu.,? 2013", "shortCiteRegEx": "Zhu.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "While previous work shows that the Earth Mover\u2019s Distance (Kusner et al., 2015) or distance between the weighted average of word vectors (Arora et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 2, "context": ", 2015) or distance between the weighted average of word vectors (Arora et al., 2017) provides information that is useful for classification tasks, we wish to go a step further and investigate whether useful information can also be found in the \u2018shape\u2019 of a document in word embedding space.", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "These have the property of being stable with respect to the Gromov-Haussdorff distance (Gromov et al., 1981).", "startOffset": 87, "endOffset": 108}, {"referenceID": 23, "context": "We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP) (Pang and Lee, 2005) and IMDb movie review datasets (Maas et al.", "startOffset": 105, "endOffset": 125}, {"referenceID": 16, "context": "We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP) (Pang and Lee, 2005) and IMDb movie review datasets (Maas et al., 2011).", "startOffset": 157, "endOffset": 176}, {"referenceID": 31, "context": "As suggested by Zhu (2013), we posit that the information about the intrinsic geometry of documents, found in the persistence diagrams, might yield information that our classifier can leverage, either on its own or in combination with other representations.", "startOffset": 16, "endOffset": 27}, {"referenceID": 19, "context": "As a first step we compute word vectors for each document in our corpus using a word2vec (Mikolov et al., 2013) model trained on the Google ar X iv :1 70 5.", "startOffset": 89, "endOffset": 111}, {"referenceID": 19, "context": "In addition to being a widely used word embedding technique, word2vec has been known to exhibit interesting linear properties with respect to analogies (Mikolov et al., 2013), which hints at rich semantic structure.", "startOffset": 152, "endOffset": 174}, {"referenceID": 8, "context": "Hence, a more natural metric is the Gromov-Haussdorff distance (Gromov et al., 1981), simply defined as", "startOffset": 63, "endOffset": 84}, {"referenceID": 25, "context": "(2015) proposed a new method for computing a distance between documents based on an instance of the Earth Mover Distance (Rubner et al., 1998) called Word Mover Distance (WMD).", "startOffset": 121, "endOffset": 142}, {"referenceID": 14, "context": "Comparison to the Earth Mover Distance : Kusner et al. (2015) proposed a new method for computing a distance between documents based on an instance of the Earth Mover Distance (Rubner et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 18, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 3, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 17, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 0, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 4, "context": "Fortunately, Carri\u00e8re et al. (2015) provides us with a way to derive a signature which is stable with respect to the GH distance.", "startOffset": 13, "endOffset": 36}, {"referenceID": 28, "context": "The curious reader is invited to refer to Zhu (2013) for a short introduction.", "startOffset": 42, "endOffset": 53}, {"referenceID": 6, "context": "More details are in Delfinado and Edelsbrunner (1995); Edelsbrunner et al.", "startOffset": 20, "endOffset": 54}, {"referenceID": 6, "context": "More details are in Delfinado and Edelsbrunner (1995); Edelsbrunner et al. (2002); Robins (1999).", "startOffset": 20, "endOffset": 82}, {"referenceID": 6, "context": "More details are in Delfinado and Edelsbrunner (1995); Edelsbrunner et al. (2002); Robins (1999).", "startOffset": 20, "endOffset": 97}, {"referenceID": 4, "context": "However, as is argued in Carri\u00e8re et al. (2015), we can truncate the vectors to a dimension fixed across our dataset while preserving the stability property (albeit losing some of the representative ability of the signatures).", "startOffset": 25, "endOffset": 48}, {"referenceID": 1, "context": "Figure 3 shows the B-Cubed precision, recall and F1-Score of each method (metrics as defined in Amig\u00f3 et al. (2009)).", "startOffset": 96, "endOffset": 116}, {"referenceID": 16, "context": "We perform document-level binary sentiment classification on the IMDb Movie Reviews Dataset (Maas et al., 2011).", "startOffset": 92, "endOffset": 111}, {"referenceID": 15, "context": "We compare our results with the paragraph-vector approach (Le and Mikolov, 2014).", "startOffset": 58, "endOffset": 80}, {"referenceID": 13, "context": "tence/paragraph level Le and Mikolov (2014); Kiros et al.", "startOffset": 22, "endOffset": 44}, {"referenceID": 11, "context": "tence/paragraph level Le and Mikolov (2014); Kiros et al. (2015) or purely word vector based methods Arora et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 2, "context": "(2015) or purely word vector based methods Arora et al. (2017).", "startOffset": 43, "endOffset": 63}, {"referenceID": 28, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 9, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 26, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 10, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 30, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 13, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 20, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 22, "context": ", 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied.", "startOffset": 37, "endOffset": 95}, {"referenceID": 11, "context": ", 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied.", "startOffset": 37, "endOffset": 95}, {"referenceID": 27, "context": ", 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied.", "startOffset": 37, "endOffset": 95}, {"referenceID": 5, "context": "Topological data analysis has been used for various tasks such as 3D shapes classification (Chazal et al., 2009) or protein structure analysis (Xia and Wei, 2014).", "startOffset": 91, "endOffset": 112}, {"referenceID": 29, "context": ", 2009) or protein structure analysis (Xia and Wei, 2014).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Topological data analysis has been used for various tasks such as 3D shapes classification (Chazal et al., 2009) or protein structure analysis (Xia and Wei, 2014). However, such techniques have not been used in NLP, primarily because the theory is inaccessible and suitable applications are scarce. Zhu (2013) offers an introduction to using persistent homology in NLP, by creating representations of nursery-rhymes and novels, as well as highlights structural differences between child and adolescent writing.", "startOffset": 92, "endOffset": 310}], "year": 2017, "abstractText": "We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like tf-idf, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.", "creator": "LaTeX with hyperref package"}}}