{"id": "1705.11105", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "HiNet: Hierarchical Classification with Neural Network", "abstract": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by \\cite{vural2004hierarchical} becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to $O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.", "histories": [["v1", "Wed, 31 May 2017 14:08:03 GMT  (928kb,D)", "http://arxiv.org/abs/1705.11105v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhenzhou wu", "sean saito"], "accepted": false, "id": "1705.11105"}, "pdf": {"name": "1705.11105.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhenzhou Wu", "Sean Saito"], "emails": ["hyciswu@gmail.com", "sean.saito@u.yale-nus.edu.sg"], "sections": [{"heading": "1 INTRODUCTION", "text": "A large hierarchical classification with more than 10,000 categories is a challenging task (Partalas et al. (2015)). Traditionally, hierarchical classification can be accomplished by training a classifier on the flat labels (Babbar et al. (2013)) or training a classifier on each hierarchical node (Silla Jr & Freitas (2011)), with each hierarchical node being a decision maker on which subsequent node to go to. However, the second method scales inefficiently with the number of categories (> 10,000 categories). Models such as Hierarchical-SVM (Vural & Dy (2004) become difficult to train when there are 10,000 SVMs throughout the hierarchy. Therefore, the hierarchical tree is flattened for a large number of categories to produce individual labels. As training becomes easier, the data then loses prior information about the labels and their structural relationships. In this paper, we model the large hierarchical neurons directly on the individual labels of the urons."}, {"heading": "2 MODEL", "text": "HiNet has different approaches to training and inference. During training, as shown in Figure 2, the model is forced to independently learn the MAP hypothesis (maximum a posteriori) about predictions at different levels of hierarchy. Since the hierarchical layers contain split information, since the child node on the parent node is conditioned, we use a combined cost function about errors at different levels. A combined cost function allows information to travel across levels equivalent to the transfer of learning between levels. During training, after predicting the posterior distribution at each level of hierarchy, we use a greedy downpour algorithm to efficiently derive the hierarchical track of the MAP (maximum a posteriori) from the posterior predictions at each level (Figure 4)."}, {"heading": "2.1 HIERARCHY AS LAYERS OF NEURONS", "text": "In contrast to the node architecture (Silla Jr & Freitas (2011); Vens et al.ar Xiv: 170 5.11 105v 1 [cs.L G] 31 May 201 7 (2008); Dumais & Chen (2000)), where each node is an object with pointers to its child and parents and requires large memory, the neural network models the connections as a compact matrix that requires much less memory. To model hierarchies of varying lengths, we append a stop neuron (red neuron in Figure 1) at each level, thus ending a path from top to bottom when it reaches the stop neuron."}, {"heading": "2.2 TRAINING", "text": "Figure 2 shows the model for transfer learning with a combined cost function. In the case of an input feature X with multiple output levels {y (1), y (2),..., y (n)}, in which the outputs may have interlevel dependencies p (y (k) | y (1: k \u2212 1), y (k + 1: n), the corresponding label y (k) is applied for each output level of network f\u03b8k (X) = y (k). The combined costs are defined as E = \u2211 n (y (k) \u2212 f\u03b8k (X) 2."}, {"heading": "2.3 INFERENCE", "text": "The model becomes a normalized probability distribution on each plane (1), y (2),., y (k), where y (k), the y (k), the y (k), the y (k), the y (k), the y (k), the y (k), the y (n), the n), the n (n), the n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the n (n), the (n), the n (n)."}, {"heading": "3 RESULTS AND CONCLUSION", "text": "We compared HiNet to a flat network that has the same architecture, except that the output layer for HiNet is hierarchical as shown in Figure??, and flat for Flatten Network. The number of outputs for Flatten Network corresponds to the number of classes in the dataset. Results show that HiNet executes flat networks for both a hierarchical dataset with much lower parameters. We see that the number of parameters in the classification layer for Flatten Network is exponential to the maximum length of the trace. Therefore, the number of parameters in Flatten Network will be exponentially large, while HiNet is always polynomic. HiNet is thus not only better in terms of accuracy, but also much more efficient in the parameter space."}, {"heading": "4 APPENDIX A", "text": "The proof for theorem 2.1 For a greedy downpour that ends at a break point neuron level n with MAP is T (n) on, then p (Sm) \u2264 p (T (n) on) for each sequence Sm = {a1, a2,.. By definition T (1) a1 = a1p (T (n) an) from m \u2265 n (an \u2212 1) p (an \u2212 1) p (an \u2212 1) p, i.e. p (an \u2212 1 p (an \u2212 1) p (Sn). By definition T (1) a1p (T (n) an) an (an \u2212 1) p (an \u2212 1) p (an \u2212 1) an \u2212 p (an \u2212.1 p)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by Vural & Dy (2004) becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only O(n) compared to O(n) in a flatten model, where h is the height of the hierarchy.", "creator": "LaTeX with hyperref package"}}}