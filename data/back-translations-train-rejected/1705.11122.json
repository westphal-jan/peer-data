{"id": "1705.11122", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Controllable Invariance through Adversarial Feature Learning", "abstract": "Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data, leading to better generalization. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved test performance.", "histories": [["v1", "Wed, 31 May 2017 14:57:33 GMT  (1664kb,D)", "http://arxiv.org/abs/1705.11122v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL", "authors": ["qizhe xie", "zihang dai", "yulun du", "eduard hovy", "graham neubig"], "accepted": false, "id": "1705.11122"}, "pdf": {"name": "1705.11122.pdf", "metadata": {"source": "CRF", "title": "Controllable Invariance through Adversarial Feature Learning", "authors": ["Qizhe Xie", "Zihang Dai", "Yulun Du", "Eduard Hovy", "Graham Neubig"], "emails": ["gneubig}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to put themselves into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which"}, {"heading": "2 Related Work", "text": "As a specific case of our problem, in which s takes two values, domain adaptation has attracted a large amount of research interest. Domain adaptation aims to learn domain invariant representations that are transferable to other domains. For example, image classification has shown that hostile education is able to learn invariant representation across domains [Ganin and Lempitsky, 2015, Bousmalis et al., 2016, Tzeng et al., 2017] and allows classifiers that are applicable to the source domain. Moment discrepancy regulations regulations can also effectively remove domain-specific information [Zellinger et al., 2017, Bousmalis et al., 2016] for the same purpose. By learning language invariant representations, classifiers trained on the source language can be applied to the target language [Chen et al., 2016b, Works et al, 2017, Yang]."}, {"heading": "3 Adversarial Invariant Feature Learning", "text": "In this section, we formulate our problem and then present the proposed framework of learning unaltered characteristics. In light of paired observations < x, y >, we are interested in the task of predicting the target, based on the value of x with a discriminatory approach, i.e. directly for the discriminatory distribution p (y | x). Since predicting x can have a highly complicated structure, we use a dedicated model or algorithm to extract an expressive representation h from x. In addition, we have access to some intrinsic attributes of x as well as a previous belief that predicting x should be unpredictable to become s's if we want to extract the representation h from x to obtain the variations necessary to obtain information from s.To achieve the above goal, we employ a deterrent encoder to obtain representation by coding x and s."}, {"heading": "4 Theoretical Analysis", "text": "In this section we have used the fact that the encoder is a deterrent transformation and thus the distribution of E (s, y) pE (s, y) x (x, y) x p (s, y) x p (x, y) x p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x, s) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x) p (e, s) p (e) p) dxHere we have used the fact that the encoder is a deterrent transformation and thus the distribution of E (h)."}, {"heading": "5 Parametric Instantiation of the Proposed Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Models", "text": "To show the general applicability of our framework, we are experimenting with three different tasks, including sentence generation, image classification and fair classifications. Due to the different nature of the data of x and y, we present here the specific model instances we use. Sentence generation We use multilingual machine translation as a test bed for sentence generation. Specifically, we have pairs of translations between multiple source languages and a target language. x is the source sentence to be translated and s is a scalar indication which source language belongs to x. y is the translated sentence for the target language. Remember that s is used as an input of E to get a language-invariant representation. To make full use of s, we use separate encoders for sentences in each language. In other words, h = E (s, x) = Encs (x) = Encs (x), where each Enc is a different encoder. < The representation of a sentence of a STM is captured by the codified states."}, {"heading": "5.2 Optimization", "text": "There are two possible approaches to optimizing our framework in an adverse environment: The first approach is similar to the alternate approach used in Generative Adversarial Nets (GANs) [Goodfellow et al., 2014]. We can train the two opposing components alternately while freezing the third. This approach has more control over the balance between the encoder and the discriminator, effectively avoiding saturation. Another method is to train all three components together with a gradient reversal layer [Ganin and Lempitsky, 2015]. Specifically, the encoder allows gradients to be optimized by both the discriminator and the predictor, with the gradient being negated by the discriminator, in order to push the encoder from the discriminator in the opposite direction. Chen et al. [2016b] found the second approach easier to optimize because the discriminator and the encoder are fully in sync. Therefore, we apply the latter approach to all of our experiments we use in Adam's Learning Rate 001 and Kingma."}, {"heading": "6 Experiments", "text": "In this section, we conduct empirical experiments to evaluate the effectiveness of the proposed framework. First, we present the tasks and corresponding data sets that we are considering, then we present the quantitative results that demonstrate the superior performance of our proposed framework, and discuss some qualitative analyses that confirm that the learned representations have the desired invariant characteristics."}, {"heading": "6.1 Datasets", "text": "Our experiments cover three tasks in different areas: (1) fair classification 4. where predictions should not be influenced by disruptive factors; (2) language-independent generation carried out on the multilingual machine translation problem; (3) light-independent image classification. Fair Classification For fair classification, we use three sets of data to predict the savings, credit scores and health conditions of individuals with variables such as gender or age that we do not want to take into account in our decisions. [Zemel et al., 2013, Louizos et al., 2016]. The German Dataset [Frank et al., 2010] is a small dataset with 1,000 examples describing whether a person has a good credit rating. The sensitive nuisance variable is gender-specific. The income of adults is dataset al [Frank et al., 2010] has 45, 222 data points and the goal of predicting whether a person has over $50,000 with sensitive health."}, {"heading": "6.2 Results", "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "7 Conclusion", "text": "In summary, we propose a generic framework to learn representations that are invariant to a particular factor or feature. We view the representation problem as an adversarial game between an encoder, a discriminator and a predictor. We theoretically analyze the optimal balance of the Minimax game and evaluate the performance of our framework empirically based on three tasks in different areas. We show that an invariant representation is learned, which leads to a better generalization and improvement of the three tasks. We plan to equip the framework with methods to deal with continuous values or structured values in the future."}, {"heading": "A Supplementary Material: Proofs", "text": "The proof for Claim 1: Claim 1. For a solid encoder E, the optimum differentiating power is q \u00b2 D (s | h) = p \u00b2 (s | h). For a solid encoder, the following optimization problem results in qD \u2212 J (E, M, D) s.t. [s qD (s | h) = 1, [s] hThen L = J (E, M, D) \u2212 p \u00b2 s qD (s | h).With a fixed encoder, we have the following optimization problem in qD \u2212 J (E, M, D) s.t. [s qD] s qD (s | h) = 1, [s] hThen L = J (E, M, D) \u2212 p \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (s).s The optimal D (s) fulfils the following equation 0 = v \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (same way), we can (s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (4) and (4)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Domain separation networks", "author": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan"], "venue": "In NIPS,", "citeRegEx": "Bousmalis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "Wit3: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico"], "venue": "In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT),", "citeRegEx": "Cettolo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "In NIPS,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Adversarial deep averaging networks for cross-lingual sentiment classification", "author": ["Xilun Chen", "Yu Sun", "Ben Athiwaratkun", "Claire Cardie", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1606.01614,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky"], "venue": null, "citeRegEx": "Ganin and Lempitsky.,? \\Q2015\\E", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2015}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["Athinodoros S. Georghiades", "Peter N. Belhumeur", "David J. Kriegman"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Approximation by superposition of sigmoidal functions", "author": ["G Gybenko"], "venue": "Mathematics of Control, Signals and Systems,", "citeRegEx": "Gybenko.,? \\Q1989\\E", "shortCiteRegEx": "Gybenko.", "year": 1989}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": "arXiv preprint arXiv:1611.04558,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "ICLR,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "author": ["G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush"], "venue": "ArXiv e-prints,", "citeRegEx": "Klein et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning unbiased features", "author": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "venue": "arXiv preprint arXiv:1412.5244,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "The variational fair autoencoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel"], "venue": null, "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Separating style and content", "author": ["Joshua B Tenenbaum", "William T Freeman"], "venue": null, "citeRegEx": "Tenenbaum and Freeman.,? \\Q1997\\E", "shortCiteRegEx": "Tenenbaum and Freeman.", "year": 1997}, {"title": "Adversarial discriminative domain adaptation", "author": ["Eric Tzeng", "Judy Hoffman", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1702.05464,", "citeRegEx": "Tzeng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2017}, {"title": "Cross-lingual distillation for text classification", "author": ["Ruochen Xu", "Yiming Yang"], "venue": null, "citeRegEx": "Xu and Yang.,? \\Q2017\\E", "shortCiteRegEx": "Xu and Yang.", "year": 2017}, {"title": "SamingerPlatz. Central moment discrepancy (cmd) for domain-invariant representation learning", "author": ["Werner Zellinger", "Thomas Grubinger", "Edwin Lughofer", "Thomas Natschl\u00e4ger", "Susanne"], "venue": null, "citeRegEx": "Zellinger et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zellinger et al\\.", "year": 2017}, {"title": "Learning fair representations", "author": ["Richard S Zemel", "Yu Wu", "Kevin Swersky", "Toniann Pitassi", "Cynthia Dwork"], "venue": null, "citeRegEx": "Zemel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2013}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "In the last few years, the dominant paradigm for finding such a representation has shifted from manual feature engineering based on specific domain knowledge to representation learning that is fully data-driven, and often powered by deep neural networks [Bengio et al., 2013].", "startOffset": 254, "endOffset": 275}, {"referenceID": 9, "context": "Being universal function approximators [Gybenko, 1989], deep neural networks can easily uncover the complicated variations in data [Zhang et al.", "startOffset": 39, "endOffset": 54}, {"referenceID": 28, "context": "Being universal function approximators [Gybenko, 1989], deep neural networks can easily uncover the complicated variations in data [Zhang et al., 2017], leading to powerful representations.", "startOffset": 131, "endOffset": 151}, {"referenceID": 16, "context": "As a typical example, the parameter sharing scheme and pooling mechanism in modern deep convolutional neural networks (CNN) [LeCun et al., 1998] take advantage of the spatial structure of image processing problems, allowing them to induce more generic feature representations than fully connected networks.", "startOffset": 124, "endOffset": 144}, {"referenceID": 18, "context": "For instance, the variational fair auto-encoder (VFAE) [Louizos et al., 2016] employs the maximum mean discrepancy (MMD) to eliminate the negative influence of specific \u201cnuisance variables\u201d, such as removing the lighting conditions of images to predict the person\u2019s identity in the image.", "startOffset": 55, "endOffset": 77}, {"referenceID": 6, "context": "Similarly, under the setting of domain adaptation, standard binary adversarial cost [Ganin and Lempitsky, 2015] and central moment discrepancy (CMD) [Zellinger et al.", "startOffset": 84, "endOffset": 111}, {"referenceID": 26, "context": "Similarly, under the setting of domain adaptation, standard binary adversarial cost [Ganin and Lempitsky, 2015] and central moment discrepancy (CMD) [Zellinger et al., 2017] have been utilized to learn features that are domain invariant.", "startOffset": 149, "endOffset": 173}, {"referenceID": 8, "context": "Specifically, inspired by the recent advancement of adversarial learning [Goodfellow et al., 2014], we formulate the representation learning as a minimax game among three players: an encoder which maps the observed data deterministically into a feature space, a discriminator which looks at the representation and tries to identify a specific type of variation we hope to eliminate from the feature, and a predictor which makes use of the invariant representation to make predictions as in typical discriminative models.", "startOffset": 73, "endOffset": 98}, {"referenceID": 18, "context": "The Variational Fair Autoencoder [Louizos et al., 2016] targets the problem with a Variational Autoencoder [Kingma and Welling, 2014, Rezende et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 2, "context": "For example, in image classification, adversarial training has been shown to able to learn an invariant representation across domains [Ganin and Lempitsky, 2015, Bousmalis et al., 2016, Tzeng et al., 2017] and enables classifiers trained on the source domain to be applicable to the target domain. Moment discrepancy regularizations can also effectively remove domain specific information [Zellinger et al., 2017, Bousmalis et al., 2016] for the same purpose. By learning language-invariant representations, classifiers trained on the source language can be applied to the target language [Chen et al., 2016b, Xu and Yang, 2017]. Works targeting the development of fair, bias-free classifiers also aim to learn representations invariant to \u201cnuisance variables\u201d that could induce bias and hence makes the predictions fair, as data-driven models trained using historical data easily inherit the bias exhibited in the data. Zemel et al. [2013] proposes to regularize the `1 distance between representation distributions for data with different nuisance variables to enforce fairness.", "startOffset": 162, "endOffset": 941}, {"referenceID": 2, "context": "For example, in image classification, adversarial training has been shown to able to learn an invariant representation across domains [Ganin and Lempitsky, 2015, Bousmalis et al., 2016, Tzeng et al., 2017] and enables classifiers trained on the source domain to be applicable to the target domain. Moment discrepancy regularizations can also effectively remove domain specific information [Zellinger et al., 2017, Bousmalis et al., 2016] for the same purpose. By learning language-invariant representations, classifiers trained on the source language can be applied to the target language [Chen et al., 2016b, Xu and Yang, 2017]. Works targeting the development of fair, bias-free classifiers also aim to learn representations invariant to \u201cnuisance variables\u201d that could induce bias and hence makes the predictions fair, as data-driven models trained using historical data easily inherit the bias exhibited in the data. Zemel et al. [2013] proposes to regularize the `1 distance between representation distributions for data with different nuisance variables to enforce fairness. The Variational Fair Autoencoder [Louizos et al., 2016] targets the problem with a Variational Autoencoder [Kingma and Welling, 2014, Rezende et al., 2014] approach with maximum mean discrepancy regularization. Our work is also related to learning disentangled representations, where the aim is to separate different influencing factors of the input data into different parts of the representation. Ideally, each part of the learned representation can be marginally independent to the other. An early work by Tenenbaum and Freeman [1997] propose a bilinear model to learn a representation with the style and content disentangled.", "startOffset": 162, "endOffset": 1619}, {"referenceID": 2, "context": "For example, in image classification, adversarial training has been shown to able to learn an invariant representation across domains [Ganin and Lempitsky, 2015, Bousmalis et al., 2016, Tzeng et al., 2017] and enables classifiers trained on the source domain to be applicable to the target domain. Moment discrepancy regularizations can also effectively remove domain specific information [Zellinger et al., 2017, Bousmalis et al., 2016] for the same purpose. By learning language-invariant representations, classifiers trained on the source language can be applied to the target language [Chen et al., 2016b, Xu and Yang, 2017]. Works targeting the development of fair, bias-free classifiers also aim to learn representations invariant to \u201cnuisance variables\u201d that could induce bias and hence makes the predictions fair, as data-driven models trained using historical data easily inherit the bias exhibited in the data. Zemel et al. [2013] proposes to regularize the `1 distance between representation distributions for data with different nuisance variables to enforce fairness. The Variational Fair Autoencoder [Louizos et al., 2016] targets the problem with a Variational Autoencoder [Kingma and Welling, 2014, Rezende et al., 2014] approach with maximum mean discrepancy regularization. Our work is also related to learning disentangled representations, where the aim is to separate different influencing factors of the input data into different parts of the representation. Ideally, each part of the learned representation can be marginally independent to the other. An early work by Tenenbaum and Freeman [1997] propose a bilinear model to learn a representation with the style and content disentangled. From information theory perspective, Chen et al. [2016a] augments standard generative adversarial networks with an inference network, whose objective is to infer part of the latent code that leads to the generated sample.", "startOffset": 162, "endOffset": 1768}, {"referenceID": 10, "context": "captured by the hidden states of an LSTM encoder [Hochreiter and Schmidhuber, 1997] at each time step.", "startOffset": 49, "endOffset": 83}, {"referenceID": 0, "context": "where we use an LSTM with attention model [Bahdanau et al., 2015] to compute qM (yt|y<t, h).", "startOffset": 42, "endOffset": 65}, {"referenceID": 8, "context": "The first one is similar to the alternating approach used in Generative Adversarial Nets (GANs) [Goodfellow et al., 2014].", "startOffset": 96, "endOffset": 121}, {"referenceID": 6, "context": "Another method is to train all three components together with a gradient reversal layer [Ganin and Lempitsky, 2015].", "startOffset": 88, "endOffset": 115}, {"referenceID": 13, "context": "In all of our experiments, we use Adam [Kingma and Ba, 2014] with a learning rate of 0.", "startOffset": 39, "endOffset": 60}, {"referenceID": 4, "context": "Chen et al. [2016b] found the second approach easier to optimize since the discriminator and the encoder are fully in sync being optimized altogether.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "A three-layer neural network with batch normalization [Ioffe and Szegedy, 2015] is employed for the discriminator.", "startOffset": 54, "endOffset": 79}, {"referenceID": 3, "context": "Multi-lingual Machine Translation For the multi-lingual machine translation task we use French to English (fr-en) and German to English (de-en) pairs from IWSLT 2015 dataset [Cettolo et al., 2012].", "startOffset": 174, "endOffset": 196}, {"referenceID": 20, "context": "We evaluate BLEU scores [Papineni et al., 2002] using the standard Moses multi-bleu.", "startOffset": 24, "endOffset": 47}, {"referenceID": 15, "context": "We use the OpenNMT [Klein et al., 2017] in our multi-lingual MT experiments.", "startOffset": 19, "endOffset": 39}, {"referenceID": 0, "context": "The predictor is a two-layer LSTM with 512 units and attention mechanism [Bahdanau et al., 2015].", "startOffset": 73, "endOffset": 96}, {"referenceID": 22, "context": "[2016] and use Byte Pair Encoding (BPE) subword units [Sennrich et al., 2016] as the cross-lingual input.", "startOffset": 54, "endOffset": 77}, {"referenceID": 0, "context": "The predictor is a two-layer LSTM with 512 units and attention mechanism [Bahdanau et al., 2015]. We follow Johnson et al. [2016] and use Byte Pair Encoding (BPE) subword units [Sennrich et al.", "startOffset": 74, "endOffset": 130}, {"referenceID": 7, "context": "Image Classification We use the Extended Yale B dataset [Georghiades et al., 2001] for our image classification task.", "startOffset": 56, "endOffset": 82}, {"referenceID": 7, "context": "Image Classification We use the Extended Yale B dataset [Georghiades et al., 2001] for our image classification task. It comprises face images of 38 people under 5 different lighting conditions: upper right, lower right, lower left, upper left, or the front. The variable s to be purged is the lighting condition. The label y is the identity of the person. We follow Li et al. [2014], Louizos et al.", "startOffset": 57, "endOffset": 384}, {"referenceID": 7, "context": "Image Classification We use the Extended Yale B dataset [Georghiades et al., 2001] for our image classification task. It comprises face images of 38 people under 5 different lighting conditions: upper right, lower right, lower left, upper left, or the front. The variable s to be purged is the lighting condition. The label y is the identity of the person. We follow Li et al. [2014], Louizos et al. [2016]\u2019s train/test split and no validation is used: 38 \u00d7 5 = 190 samples are used for training and all other 1, 096 data points are used for testing.", "startOffset": 57, "endOffset": 407}, {"referenceID": 27, "context": "We compare our model with two prior works on learning fair representations: Learning Fair Representations (LFR) [Zemel et al., 2013] and Variational Fair Autoencoder (VFAE) [Louizos et al.", "startOffset": 112, "endOffset": 132}, {"referenceID": 18, "context": ", 2013] and Variational Fair Autoencoder (VFAE) [Louizos et al., 2016].", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "Model test (fr-en) test (de-en) Bilingual Enc-Dec [Bahdanau et al., 2015] 35.", "startOffset": 50, "endOffset": 73}, {"referenceID": 12, "context": "3 Multi-lingual Enc-Dec [Johnson et al., 2016] 35.", "startOffset": 24, "endOffset": 46}, {"referenceID": 0, "context": "We compare our model with attention based encoder-decoder trained on bilingual data [Bahdanau et al., 2015] and multi-lingual data [Johnson et al.", "startOffset": 84, "endOffset": 107}, {"referenceID": 12, "context": ", 2015] and multi-lingual data [Johnson et al., 2016].", "startOffset": 31, "endOffset": 53}, {"referenceID": 17, "context": "78 NN + MMD [Li et al., 2014] 0.", "startOffset": 12, "endOffset": 29}, {"referenceID": 18, "context": "82 VFAE [Louizos et al., 2016] 0.", "startOffset": 8, "endOffset": 30}, {"referenceID": 19, "context": "We also visualize the learned representation by t-SNE [Maaten and Hinton, 2008] in comparison to the visualization of original pictures in Figure 2.", "startOffset": 54, "endOffset": 79}], "year": 2017, "abstractText": "Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data, leading to better generalization. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved test performance.", "creator": "LaTeX with hyperref package"}}}