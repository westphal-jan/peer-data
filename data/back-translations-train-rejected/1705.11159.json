{"id": "1705.11159", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Reinforcement Learning for Learning Rate Control", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. We propose an algorithm to automatically learn learning rates using neural network based actor-critic methods from deep reinforcement learning (RL).In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. The introduction of auxiliary actor and critic networks helps the main network achieve better performance. Experiments on different datasets and network architectures show that our approach leads to better convergence of SGD than human-designed competitors.", "histories": [["v1", "Wed, 31 May 2017 15:58:35 GMT  (4040kb,D)", "http://arxiv.org/abs/1705.11159v1", "7 pages, 9 figures"]], "COMMENTS": "7 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chang xu", "tao qin", "gang wang", "tie-yan liu"], "accepted": false, "id": "1705.11159"}, "pdf": {"name": "1705.11159.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning for Learning Rate Control", "authors": ["Chang Xu", "Tao", "Qin", "Gang Wang", "Tie-Yan"], "emails": ["wgzwp}@nbjl.nankai.edu.cn,", "Liu}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2 Related Work", "text": "In this section we will discuss some related work."}, {"heading": "2.1 Improved Gradient Methods", "text": "We focus on improving the gradient-based ML algorithm by automatically learning the learning rate. Various approaches have been proposed to improve the gradient methods, especially for deep neural networks. Since the SGD relies exclusively on a specific example (or mini-batch of examples) to compare the gradient, its model update tends to be unstable at each step, and it takes many steps to normalize the gradients. To solve this problem, it is proposed to accelerate the dynamics of SGD [Jacobs, 1988] by using current gradients. RMSprop [Tieleman and Hinton, 2012] uses the order of magnitude of the most recent gradients to normalize the gradients. It always maintains a sliding average over the root mean square gradients by which it divides the current gradient. Adagrad [Duchi et al, 2011] adjusts the learning rates of the components for selective and larger-scale updates."}, {"heading": "2.2 Reinforcement Learning", "text": "Since our proposed algorithm is based on RL techniques, we give a very brief introduction to RL here, which will simplify the description of our algorithm in the next paragraph.Reinforcement learning [Sutton, 1988] deals with the way an agent acts in a stochastic environment by selecting actions sequentially over a sequence of time steps to maximize cumulative rewards. In RL, a state st encodes the agent's observation of the environment in a time step, and a political function \u03c0 (st) determines how the agent behaves in the state. An action value function (or, Q function) Q\u03c0 (st, at) is usually used to denote the cumulative reward for acting in the state and then following politics afterwards. Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992]."}, {"heading": "3 Method", "text": "In this section, we present an actor-critic algorithm that can automate the control of the learning rate for SGD-based machine learning algorithms. (1) A standard approach to minimizing the loss function is the gradient descent, where the parameters are gradually updated using gradients: \u03c9t + 1 = \u03c9t \u2212 at the learning rate in step t, (2) where at the learning rate in step t and at the local gradient from f in step. One step can be the entire batch of all training data, a mini-batch of dozens / hundreds of examples, or a random sample. It is observed that the performance of SGD-based methods is quite sensitive to the choice of at for non-convex loss function. Unfortunately, f is generally not convex in terms of the parameters used in controlling many MGD-based systems to increase the learning speed. (We take the automatic learning speed for the actor)."}, {"heading": "3.1 Actor Network", "text": "The Actor Network, referred to in RL as the Actor Network, plays the key role in our algorithm: It determines the learning rate control policy for the primary ML algorithm1 on the basis of the current model, training data and possibly historical information during the training process. Note: It could be of enormous dimensions, for example, a widely used image recognition model VGGNet [Simonyan and Zisserman, 2014] has more than 140 million parameters. If the Actor Network takes all these parameters as inputs, its computational complexity would dominate the complexity of the primary algorithm, which is unpredictable. Therefore, we propose to use a function to generate and supply a compact vector st as input of the Actor Network. Following the practice in RL we call the state function which refers to training and training data x as inputs."}, {"heading": "3.2 Critic Network", "text": "Remember that our goal is to find a good policy for controlling the learning rate in order to ensure that a good model can ultimately be learned through the primary ML algorithm. To this end, the stakeholder network must perform a good action at the state school, so that eventually a low educational loss f (\u00b7) can be achieved. In elementary school, the Q function Q\u03c0 (s, a) is often used to denote the long-term reward of the state-active couple, a while after the procedure to take future action. In our problem, Q\u03c0 (st, at) denotes the accumulative reduction of the educational loss from step. We define the immediate reward at step t as the one-step reduction: rt = f \u2212 f \u2212 f + 1. (5) The accumulative value of step t is the discounted reward of the policy at step. Taking into account that both the states and the problem of the Qetric function of the network are unhazardous (Qetric)."}, {"heading": "3.3 Training of Actor and Critic Networks", "text": "The network of critics has its own parameters, which are updated with each step by means of TD learning. Specifically, the critic is trained by minimizing the square error between the estimate Q\u0442 (st, at) and the target yt: yt = rt + \u03b3Q\u0442 (s t + 1, at + 1). (6) The TD error is defined as: \u03b4t = yt \u2212 Q\u0442 (st, at) = rt + \u03b3Q\u0442 (s + 1, \u03c0\u043e (s t + 1) \u2212 Q\u0442 (st, at) (7) The rule for weight updating follows the deterministic algorithm of criticism of politics. The gradients of the network of critics are: \u0395\u0430\u0441\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u0441\u0430\u0441\u0442\u0430\u0441\u0442\u0430\u043d\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0439 (st, at), (8) The political parameters of the network of actors are updated by ensuring that it can output the value with the largest state limit (a), i.e."}, {"heading": "3.4 The Algorithm", "text": "The general algorithm for learning rate is shown in algorithm 1. In each step, we stomp an example (line 3), extract the current state vector (line 4), calculate the learning rate using the actor network (line 6), update the model (line 8-9), calculate TD errors (line 11-15), update the critic network (line 16-18) and stomp another example (line 20) to update the actor network (line 21-26). We would like to have some discussions about the algorithm. First, in the current algorithm, we consider the use of just one example for model updating. It is easy to generalize to a minibatch of random examples. Second, we may notice that we use an example (e.g. xi) for the model and critic network rate, but another example (e.g. xj) for activating the actor network we can reduce during the oscillation."}, {"heading": "4 Experiments", "text": "We conducted a series of experiments to test the performance of our learning algorithm for learning rates and compared them with different basic methods. We report the experimental results in this section."}, {"heading": "4.1 Experimental Setup", "text": "We have specified our actor-critic algorithm in experiments as follows: Given that stochastic mini-batch training is a common practice in deep learning, the actor-critic algorithm is also operated on mini-batches, i.e., each step is a mini-batch in our experiments. The state st = 0 (\u03c9t, Xi) is defined as the average loss of the learning model \u03c9t on the input mini-batch Xi.The actor network is specified as a double-layer short-term memory (LSTM) with 20 units in each shift, considering that a good learning rate for step t depends on and correlates with the learning rates in previous steps, while LSTM is well suited to model sequences with long-distance dependence. The critic network is specified as a simple neural network with a hidden layer and 10 hidden units in each layer to calculate the learning rate to be used in the standard setting in the learning box during the learning step."}, {"heading": "4.2 Experimental Results", "text": "In fact, we are in a position to take the lead in all areas where we are able to achieve our objectives."}, {"heading": "4.3 Further Analysis", "text": "In order to verify our intuitive explanation that our method can stabilize the learning process of the primary ML algorithm, we conducted another experiment here. In this experiment, we investigate the relationship between gradient disagreement and loss in the training process of a simple two-dimensional regression problem. We quantify gradient disagreement by using the Euclidean distance between gradients on current stacks of data and the total gradient. Figure 5 shows the gradient disagreement, training loss and test loss of the SGD and our method. We can observe the correlation between them using the figure. As discussed in Section 3.4, our method would encourage the learning rate to be small by feeding different samples to stakeholder and critic networks when gradient disagreement is large, so that the oscillation of the training process would be facilitated. It is easy to see from the figure that test losses of our method are stable when there is large gradient disagreement."}, {"heading": "5 Conclusions and Future Work", "text": "Experiments on two image classification tasks have shown that our method (1) can successfully adjust the learning rate for different datasets and CNN model structures, resulting in better convergence, and (2) can reduce oscillation during training. In future work, we will explore the following directions. In this work, we have applied our algorithm for controlling the learning rates of SGD. We will refer to other variants of SGD methods. We have focused on learning a learning rate for all model parameters. We will study how to learn an individual learning rate for each parameter. We have considered learning rates using RL techniques. We will consider other hyperparameters, such as step-dependent dropout rates for deep neural networks."}], "references": [{"title": "et al", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham"], "venue": "Tensorflow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow. org, 1,", "citeRegEx": "Abadi et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "An actorcritic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1607.07086,", "citeRegEx": "Bahdanau et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "man", "author": ["Andrew G Barto", "Richard S Sutton", "Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems"], "venue": "and cybernetics, (5):834\u2013846,", "citeRegEx": "Barto et al.. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Learning step size controllers for robust neural network training", "author": ["Christian Daniel", "Jonathan Taylor", "Sebastian Nowozin"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Daniel et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast adaptive k-means clustering: some empirical results", "author": ["Christian Darken", "John Moody"], "venue": "Neural Networks, 1990., 1990 IJCNN International Joint Conference on, pages 233\u2013238. IEEE,", "citeRegEx": "Darken and Moody. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12(Jul):2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural networks", "author": ["Robert A Jacobs. Increased rates of convergence through learning rate adaptation"], "venue": "1(4):295\u2013307,", "citeRegEx": "Jacobs. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Hinton", "2009] Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Proceedings of the IEEE", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner. Gradient-based learning applied to document recognition"], "venue": "86(11):2278\u20132324,", "citeRegEx": "LeCun et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In Neural networks: Tricks of the trade", "author": ["Yann A LeCun", "L\u00e9on Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller. Efficient backprop"], "venue": "pages 9\u201348. Springer,", "citeRegEx": "LeCun et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["Dougal Maclaurin", "David Duvenaud", "Ryan P Adams"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Maclaurin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural networks: tricks of the trade", "author": ["Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": "Springer,", "citeRegEx": "Orr and M\u00fcller. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "ICML (3)", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun. No more pesky learning rates"], "venue": "28:343\u2013 351,", "citeRegEx": "Schaul et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Andrew Senior", "Georg Heigold", "Ke Yang"], "venue": "An empirical study of learning rates in deep neural networks for speech recognition. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6724\u20136728. IEEE,", "citeRegEx": "Senior et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Nicolas Heess", "author": ["David Silver", "Guy Lever"], "venue": "Deterministic policy gradient algorithms.", "citeRegEx": "Silver et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["David Silver", "Aja Huang"], "venue": "Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489,", "citeRegEx": "Silver et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "Simonyan and Zisserman. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Time-derivative models of pavlovian reinforcement", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "pages 497\u2013537,", "citeRegEx": "Sutton and Barto. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "volume 1. MIT press Cambridge,", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "et al", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pages 1057\u20131063,", "citeRegEx": "Sutton et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Machine learning", "author": ["Richard S Sutton. Learning to predict by the methods of temporal differences"], "venue": "3(1):9\u201344,", "citeRegEx": "Sutton. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bar-delta", "author": ["Richard S Sutton"], "venue": "AAAI, pages 171\u2013176,", "citeRegEx": "Sutton. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton", "2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Machine learning", "author": ["Christopher JCH Watkins", "Peter Dayan. Q-learning"], "venue": "8(3-4):279\u2013 292,", "citeRegEx": "Watkins and Dayan. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "et al", "author": ["Kelvin Xu", "Jimmy Ba"], "venue": "Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2(3):5,", "citeRegEx": "Xu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "An observation for SGD methods is that their performances are highly sensitive to the choice of learning rate [LeCun et al., 2012].", "startOffset": 110, "endOffset": 130}, {"referenceID": 11, "context": "Clearly, setting a static learning rate for the whole training process is insufficient, since intuitively the learning rate should decrease when the model becomes more and more close to a (local) optimum as the training goes on over time [Maclaurin et al., 2015].", "startOffset": 238, "endOffset": 262}, {"referenceID": 13, "context": "As suggested in [Orr and M\u00fcller, 2003], one well-principled method for estimating the ideal learning rate that is to decrease the learning rate when the weight vector oscillates, and increase it when the weight vector follows a relatively steady direction.", "startOffset": 16, "endOffset": 38}, {"referenceID": 20, "context": "Combining the two observations, it is not difficult to see that the problem of finding a good policy to control/adjust learning rate falls into the scope of reinforcement learning (RL) [Sutton and Barto, 1998].", "startOffset": 185, "endOffset": 209}, {"referenceID": 21, "context": "We propose an algorithm to learn learning rate within the actor-critic framework [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014], which is widely used in RL.", "startOffset": 81, "endOffset": 157}, {"referenceID": 2, "context": "We propose an algorithm to learn learning rate within the actor-critic framework [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014], which is widely used in RL.", "startOffset": 81, "endOffset": 157}, {"referenceID": 16, "context": "We propose an algorithm to learn learning rate within the actor-critic framework [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014], which is widely used in RL.", "startOffset": 81, "endOffset": 157}, {"referenceID": 13, "context": "couraged to be small when gradients oscillate, which is consistent with the suggestion for ideal learning rate strategy in [Orr and M\u00fcller, 2003].", "startOffset": 123, "endOffset": 145}, {"referenceID": 6, "context": "To solve this problem, momentum SGD [Jacobs, 1988] is proposed to accelerate SGD by using recent gradients.", "startOffset": 36, "endOffset": 50}, {"referenceID": 5, "context": "Adagrad [Duchi et al., 2011] adapts component-wise learning rates, and performs larger updates for infrequent and smaller updates for frequent parameters.", "startOffset": 8, "endOffset": 28}, {"referenceID": 27, "context": "Adadelta [Zeiler, 2012] extends Adagrad by reducing its aggressive, monotonically decreasing learning rate.", "startOffset": 9, "endOffset": 23}, {"referenceID": 7, "context": "Adam [Kingma and Ba, 2014] computes component-wise learning rates using the estimates of first and second moments of the gradients, which combines the advantages of AdaGrad and RMSProp.", "startOffset": 5, "endOffset": 26}, {"referenceID": 15, "context": "[Senior et al., 2013; Sutton, 1992; Darken and Moody, 1990] focus on predefining update rules to adjust learning rates during training.", "startOffset": 0, "endOffset": 59}, {"referenceID": 23, "context": "[Senior et al., 2013; Sutton, 1992; Darken and Moody, 1990] focus on predefining update rules to adjust learning rates during training.", "startOffset": 0, "endOffset": 59}, {"referenceID": 4, "context": "[Senior et al., 2013; Sutton, 1992; Darken and Moody, 1990] focus on predefining update rules to adjust learning rates during training.", "startOffset": 0, "endOffset": 59}, {"referenceID": 14, "context": "[Schaul et al., 2013] proposes a method to choose good learning rate for SGD, which relies on the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Another recent work [Daniel et al., 2016] investigates several hand-tuned features and uses the Relative Entropy Policy Search method as controller to select step size for SGD and RMSprop.", "startOffset": 20, "endOffset": 41}, {"referenceID": 22, "context": "Reinforcement learning [Sutton, 1988] is concerned with how an agent acts in a stochastic environment by sequentially choosing actions over a sequence of time steps, in order to maximize a cumulative reward.", "startOffset": 23, "endOffset": 37}, {"referenceID": 20, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al.", "startOffset": 38, "endOffset": 87}, {"referenceID": 25, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al.", "startOffset": 38, "endOffset": 87}, {"referenceID": 21, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014] can be described under the actor-critic framework.", "startOffset": 112, "endOffset": 188}, {"referenceID": 2, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014] can be described under the actor-critic framework.", "startOffset": 112, "endOffset": 188}, {"referenceID": 16, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014] can be described under the actor-critic framework.", "startOffset": 112, "endOffset": 188}, {"referenceID": 12, "context": "Recently, deep reinforcement learning, which uses deep neural networks to approximate/represent the policy function and/or the value function, have shown promise in various domains, including Atari games [Mnih et al., 2015], Go [Silver et al.", "startOffset": 204, "endOffset": 223}, {"referenceID": 17, "context": ", 2015], Go [Silver et al., 2016], machine translation [Bahdanau et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 1, "context": ", 2016], machine translation [Bahdanau et al., 2016], image recognition [Xu et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 26, "context": ", 2016], image recognition [Xu et al., 2015], etc.", "startOffset": 27, "endOffset": 44}, {"referenceID": 19, "context": "The actor network will be updated using the estimated goodness of a, and the critic network will be updated by minimizing temporal difference (TD) [Sutton and Barto, 1990] error.", "startOffset": 147, "endOffset": 171}, {"referenceID": 18, "context": ", one widely used image recognition model VGGNet [Simonyan and Zisserman, 2014] has more than 140 million parameters.", "startOffset": 49, "endOffset": 79}, {"referenceID": 7, "context": "We compared our method with several mainstream SGD algorithms, including SGD, Adam [Kingma and Ba, 2014], Adagrad [Duchi et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 5, "context": "We compared our method with several mainstream SGD algorithms, including SGD, Adam [Kingma and Ba, 2014], Adagrad [Duchi et al., 2011] and RMSprop [Tieleman and Hinton, 2012].", "startOffset": 114, "endOffset": 134}, {"referenceID": 3, "context": "We also compare our method with a recent work by [Daniel et al., 2016]2.", "startOffset": 49, "endOffset": 70}, {"referenceID": 14, "context": "Another baseline method is \u201cvSGD\u201d [Schaul et al., 2013]2, which automatically adjusts learning rates to minimize the expected error.", "startOffset": 34, "endOffset": 55}, {"referenceID": 9, "context": "To verify the effectiveness of our method on different datasets and model structures, experiments are conducted on two widely used image classification datasets: MNIST [LeCun et al., 1998] and CIFAR-10 [Krizhevsky and Hinton, 2009].", "startOffset": 168, "endOffset": 188}, {"referenceID": 0, "context": "For simplicity, the primary ML algorithm adopted the CNN models and settings from tensorflow [Abadi et al., 2015] tutorial, whose source code can be found at [TensorflowExamples, ] For each of these algorithms and each dataset, we tried the following learning rates 10\u22124, 10\u22123, .", "startOffset": 93, "endOffset": 113}], "year": 2017, "abstractText": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. We propose an algorithm to automatically learn learning rates using neural network based actor-critic methods from deep reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. The introduction of auxiliary actor and critic networks helps the main network achieve better performance. Experiments on different datasets and network architectures show that our approach leads to better convergence of SGD than humandesigned competitors.", "creator": "LaTeX with hyperref package"}}}