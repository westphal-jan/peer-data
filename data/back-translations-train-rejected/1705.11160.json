{"id": "1705.11160", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Learning When to Attend for Neural Machine Translation", "abstract": "In the past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline.", "histories": [["v1", "Wed, 31 May 2017 16:05:49 GMT  (124kb)", "http://arxiv.org/abs/1705.11160v1", "5 pages, 2 figures"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["junhui li", "muhua zhu"], "accepted": false, "id": "1705.11160"}, "pdf": {"name": "1705.11160.pdf", "metadata": {"source": "CRF", "title": "Learning When to Attend for Neural Machine Translation", "authors": ["Junhui Li"], "emails": ["lijunhui@suda.edu.cn", "muhuazhu@tencent.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.11 160v 1 [cs.C L] 31 May 201 7Animals have become an indispensable part of end-to-end neural machine translation models. However, earlier attention models always refer to some source words when predicting a target word, which contradicts the fact that some target words do not have corresponding source words. Motivated by this observation, we propose a novel attention model that is capable of determining when a decoder should take care of source words and when not. Experimental results on NIST translation tasks in Chinese-English show that the new model achieves an improvement of 0.8 BLEU values over a state-of-the-art baseline."}, {"heading": "1 Introduction", "text": "Recent years have been marked by rapid advances in neural machine translation (NMT), most of which are based on encoder decoder systems (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a). In addition, attention mechanisms have become an indispensable component in the field of source words used to generate the current target word (Luong et al., 2015b). The idea of attention mechanisms is to run a translation decoder to selectively focus on a local window of source words used to demonstrate the necessity and effectiveness of such attention mechanisms."}, {"heading": "2 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Attention-based NMT", "text": "We begin by describing an NMT model based on an RNN encoder decoder framework (Sutskever et al., 2014) and attention mechanisms. In view of a source sentence X = {x1,.., xJ} and the corresponding target sentence Y = {y1,.., yK}, the model seeks to maximize an objective function defined as a loglikelihood. \u2212 \u2212 The probability of word prediction is related to the source sentence X and the previously generated words yi,..., yi \u2212 1, as the parameter of the model. (In terms of the decryption phase, the model produces a translation by selecting a target word yi \u2212 \u2212 1. The probability of word prediction is related to the source sentence X and the previously generated words yi,..) yi \u2212 1, such as the following softmax function: P (yi | y < i, X)."}, {"heading": "2.2 Adaptive Attention Model", "text": "Although the attention model presented above has shown its effectiveness in NMT, it cannot say when a decoder should use the information of the attention model and when the decoder should not. Motivated by the work in (Merity et al., 2016; Lu et al., 2016), we introduce the concept of attention guards, which is a latent representation of what a decoder already knows. A decoder can fall back on the attention guard if it decides to \"omit\" the original set for some time. Attention sentinel: The memory of a decoder stores information from both the source set and the language model of the size of the decoder. From the state we learn a new component that can be used if the decoder chooses not to observe the original set. Such a component is called attention monitor. For a decoder using GRU-RNN, the attention guard is a vector."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setup", "text": "Our training data consists of 1.25M sentence pairs extracted from LDC corpora, 3 containing 27.9M Chinese words and 34.5m English words, respectively. In all experiments, we used the NIST 2006 dataset (1664 sentence pairs) for system development and tested the system at NIST 2003, 2004, 2005 datasets (919, 1788, 1082 sentence pairs). In addition, we limited the case-insensitive 4-gram NIST BLEU score (Papineni et al., 2002) as a rating measure. To efficiently train neural networks, we used sentences of up to 50 words in the training data. In addition, we limited the source and target vocabularies to the 16K most common words, which relate to 95.8% and 98.2% of the word marks of source and target sentences."}, {"heading": "3.2 Main Results", "text": "Results show that the best adaptive attention model achieved an improvement of 0.7 BLEU score compared to RNNSearch on the development package, then we evaluated the same model on the test sets and achieved a significant improvement of 0.8 BLEU score compared to RNNSearch (the improvement compared to cdec is 2.0 BLEU score), on the other hand, we found that the adaptive attention model has more parameters than RNNSearch (60.6 million vs. 70.6 million), and more training time is required (153 minutes / epoch vs. 207 minutes / epoch)."}, {"heading": "3.3 Analysis", "text": "To this end, we have translated the set of Chinese sentences from the NIST 2003, 2004 and 2005 data sets, and recorded all predicted target words that have a sentinel gate value greater than or equal to 0.9. From the resulting word list, we present the 15 most common words and their frequency count in Table 2. From the table, it is clear that the translation system tends to rely on the Attention Guardian to generate auxiliary words such as this and that. This observation is consistent with our intuition. As far as Token UNK is concerned, remember that the symbol is an image of OOV words whose lexical information is lost due to mapping. Therefore, relying on the Attention Guardian to predict UNK is an appropriate choice. Finally, states appear in the top word list because this word, most of the time, occurs immediately after the word incorporated in our data."}, {"heading": "4 Related Work", "text": "Bahdanau et al. (2015) propose a model for joint alignment and translation of words. Luong et al. (2015b) propose and compare different attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector to address the problem of sub- and translation. All previous attention models work well, but they cannot say when they should not care about judgments.Our work is inspired by Lu et al. (2016), which propose an adaptive attention model for the task of caption, the main difference being that they build their adaptive attention model on the basis of a spatial attention model that differs from conventional attention models for NMT. Furthermore, our adaptive attention model uses GRU as an activation function, while Lu et al. (Lu et al. 2016) focuses most attention on the NT context (2015b)."}, {"heading": "5 Conclusion", "text": "We introduced a new component called Attention Watchers, on the basis of which we developed an adaptive attention model. Experiments with NIST translation tasks from Chinese to English showed that the model achieved a significant improvement of 0.8 BLEU points."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A guide to recurrent neural networks and back-propagation", "author": ["Mikael Boden."], "venue": "the Dallas project.", "citeRegEx": "Boden.,? 2002", "shortCiteRegEx": "Boden.", "year": 2002}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP. pages 1724\u2013", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Adam Lopz", "Juri Ganitkevitch", "Jonathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of EMNLP. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2014", "shortCiteRegEx": "Koehn.", "year": 2014}, {"title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning", "author": ["Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Socher Richard."], "venue": "arXiv preprint arxiv:1612.01887.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL. pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Socher Richard."], "venue": "arXiv preprint arXiv:1609.07843.", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "andWeiJing Zhu"], "venue": "In Proceedings of ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing 45(1):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov", "Geoffrey E. Hinto."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Srivastava et al\\.,? 2012", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "TACL 5:87\u201399.", "citeRegEx": "Tu et al\\.,? 2016a", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of ACL. pages 76\u201385.", "citeRegEx": "Tu et al\\.,? 2016b", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Contrastive unsupervised word alignment with non-local features", "author": ["Liu Yang", "Sun Maosong."], "venue": "Proceedings of AAAI. pages 857\u2013868.", "citeRegEx": "Yang and Maosong.,? 2015", "shortCiteRegEx": "Yang and Maosong.", "year": 2015}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "The past several years have witnessed rapid progress of end-to-end neural machine translation (NMT)models, most of which are built on the base of encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 172, "endOffset": 240}, {"referenceID": 0, "context": "The past several years have witnessed rapid progress of end-to-end neural machine translation (NMT)models, most of which are built on the base of encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 172, "endOffset": 240}, {"referenceID": 8, "context": "The past several years have witnessed rapid progress of end-to-end neural machine translation (NMT)models, most of which are built on the base of encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 172, "endOffset": 240}, {"referenceID": 9, "context": "In addition, attention mechanisms have become an indispensable component in state-of-the-art NMT systems (Luong et al., 2015b; Tu et al., 2016b).", "startOffset": 105, "endOffset": 144}, {"referenceID": 16, "context": "In addition, attention mechanisms have become an indispensable component in state-of-the-art NMT systems (Luong et al., 2015b; Tu et al., 2016b).", "startOffset": 105, "endOffset": 144}, {"referenceID": 17, "context": "To show how prevalent the phenomenon is, we analyze a set of 900 Chinese-English sentence pairs with manual word alignments (Yang and Maosong, 2015), and find that 25.", "startOffset": 124, "endOffset": 148}, {"referenceID": 0, "context": "We build a new NMT system by integrating an adaptive attention model into the NMT system described in (Bahdanau et al., 2015).", "startOffset": 102, "endOffset": 125}, {"referenceID": 15, "context": "the best of our knowledge, the adaptive attention method discussed here has not been used before for NMT, although the problem we intend to attack is not new (Tu et al., 2016a).", "startOffset": 158, "endOffset": 176}, {"referenceID": 14, "context": "We start by describing an NMT model, which builds on the base of an RNN encoder-decoder framework (Sutskever et al., 2014) and attention mechanisms.", "startOffset": 98, "endOffset": 122}, {"referenceID": 1, "context": "The activation function f(\u00b7) can be a vanilla RNN (Boden, 2002) or sophisticated units such as Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 50, "endOffset": 63}, {"referenceID": 3, "context": "The activation function f(\u00b7) can be a vanilla RNN (Boden, 2002) or sophisticated units such as Gated Recurrent Unit (GRU) (Cho et al., 2014) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 122, "endOffset": 140}, {"referenceID": 5, "context": ", 2014) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 42, "endOffset": 76}, {"referenceID": 12, "context": "where hj = [ \u2212\u2192 h j ; \u2190\u2212 h j ] T represents the annotation vector of the source word xj generated by a bi-directional RNN (Schuster and Paliwal, 1997), and the weight \u03b1i,j is calculated as follows:", "startOffset": 122, "endOffset": 150}, {"referenceID": 9, "context": "Here ei,j measures the similarity between the target word yi and the source word xj , which can be calculated with diverse methods (Luong et al., 2015b).", "startOffset": 131, "endOffset": 152}, {"referenceID": 10, "context": "Motivated from the work in (Merity et al., 2016; Lu et al., 2016), we introduce the concept of attention sentinel, which is a latent representation of what a decoder already knows.", "startOffset": 27, "endOffset": 65}, {"referenceID": 7, "context": "Motivated from the work in (Merity et al., 2016; Lu et al., 2016), we introduce the concept of attention sentinel, which is a latent representation of what a decoder already knows.", "startOffset": 27, "endOffset": 65}, {"referenceID": 7, "context": "The attention sentinel for LSTM-RNN can be defined in a similar way; Readers interested can refer to (Lu et al., 2016) for a detailed description.", "startOffset": 101, "endOffset": 118}, {"referenceID": 11, "context": "We used the case-insensitive 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric.", "startOffset": 52, "endOffset": 75}, {"referenceID": 0, "context": "All the other settings are the same as in (Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 65}, {"referenceID": 4, "context": "\u2022 cdec (Dyer et al., 2010): an open-source hierarchical phrase-based translation system (Chiang, 2007) with default configuration and 4-gram language model trained on the target sentences of training data.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": ", 2010): an open-source hierarchical phrase-based translation system (Chiang, 2007) with default configuration and 4-gram language model trained on the target sentences of training data.", "startOffset": 69, "endOffset": 83}, {"referenceID": 0, "context": "\u2022 RNNSearch: a re-implementation of the attention-based neural machine translation system (Bahdanau et al., 2015) with slight changes from dl2mt tutorial.", "startOffset": 90, "endOffset": 113}, {"referenceID": 13, "context": "5 RNNSearch uses GRU as the activation function of an RNN and incorporates dropout (Srivastava et al., 2012) on the output layer.", "startOffset": 83, "endOffset": 108}, {"referenceID": 18, "context": "We use AdaDelta (Zeiler, 2012) to optimize model parameters.", "startOffset": 16, "endOffset": 30}, {"referenceID": 7, "context": "(Lu et al., 2016) adopt LSTM.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models.", "startOffset": 0, "endOffset": 98}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector in order to attack the problem of under-translation and over-translation.", "startOffset": 0, "endOffset": 162}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector in order to attack the problem of under-translation and over-translation. All the previous attention models work well, but they cannot tell when not to attend to source sentences. Our work is inspired by Lu et al. (2016), which propose an adaptive attention model for the task of image captioning.", "startOffset": 0, "endOffset": 441}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector in order to attack the problem of under-translation and over-translation. All the previous attention models work well, but they cannot tell when not to attend to source sentences. Our work is inspired by Lu et al. (2016), which propose an adaptive attention model for the task of image captioning. The main difference is that they build their adaptive attention model on the base of a spatial attention model, which is different from conventional attention models for NMT. Moreover, our adaptive attention model uses GRU as the RNN activation function while Lu et al. (Lu et al., 2016) adopt LSTM. Regarding the literature of NMT, the most related work is Tu et al. (2016a), which utilize a context gate to trade off the source-side and target-side context.", "startOffset": 0, "endOffset": 894}], "year": 2017, "abstractText": "In the past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline.", "creator": "LaTeX with hyperref package"}}}