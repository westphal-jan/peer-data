{"id": "1705.11168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning", "abstract": "Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.", "histories": [["v1", "Wed, 31 May 2017 16:31:54 GMT  (245kb,D)", "http://arxiv.org/abs/1705.11168v1", "Accepted at RoboNLP 2017"]], "COMMENTS": "Accepted at RoboNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["li lucy", "jon gauthier"], "accepted": false, "id": "1705.11168"}, "pdf": {"name": "1705.11168.pdf", "metadata": {"source": "CRF", "title": "Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning", "authors": ["Li Lucy", "Jon Gauthier"], "emails": ["lucy3@stanford.edu", "jon@gauthiers.net"], "sections": [{"heading": "1 Introduction", "text": "In recent years, the number of those who are able to survive has multiplied, both in the United States and in Europe. (...) In recent years, the number of those who are able to survive has multiplied. (...) In the last ten years, the number of those who are able to survive has doubled. (...) In the last ten years, the number of those who are able to survive has multiplied. (...) In the last ten years, the number of those who are able to survive has increased. (...) In the last ten years, the number of those who are able to survive has increased. (...) In the last ten years, the number of those who are able to survive has increased. (...) In the last ten years, the number of those who are able to survive has increased."}, {"heading": "2 Related work", "text": "This paper uses semantic standard datasets to evaluate the content of distributed word representations. Semantic standard datasets consist of concepts and norms related to their perceptual and conceptual properties as provided by human participants. They are a popular resource within psychology and cognitive science as models of human concept representation and have been used to explain psycholinguistic phenomena ranging from semantic priming and interference (Vigliocco et al., 2004) to the structure of early word learning in children's language acquisition (Hills et al., 2009). Andrew et al. (2009) show how \"experimental\" semantic \"semantic standard information can be used to model human judgments about conceptual similarity. They show that these semantic standard datasets provide information that is different from the information in basic word representations. Our work extends the results of Andrew et al. to a larger natural standard collection evaluated and specific language impacts."}, {"heading": "3 Meaning representations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Distributional meaning", "text": "This paper examines representations generated by two popular, unattended distribution methods. Table 1 shows the statistics of the corpora used to generate these vectors. GloVe: GloVe (Pennington et al., 2014) estimates word representations wi by using them to reconstruct a word-word coexistence matrix X collected from a large text corpus: L = V \u2211 i, j = 1 f (Xij) (wTi wj + bi + bj \u2212 logXij) 2 (1) here f (Xij) is a weighting function for word pairs and bi, bj are learned per-word bias terms. We use two pre-trained GloVe vector datasets: one trains on a concatenation of Wikipedia 2014 and Gigaword 5 (GloVe-WG), and another trains on a common crawl dump (GloVe-CC). 1vec: pre-evaluating all word positions available within a word (2dolodaret)."}, {"heading": "3.2 Semantic norms", "text": "It is indeed the case that we are able to set out in search of new paths to follow in order to achieve our objectives."}, {"heading": "4 The feature view", "text": "We first examine how well the distributional word representations are directly embedded in semantic norms that are contained only in a semantic norm dataset, we construct a binary classification problem that predicts the presence or absence of the attribute for each concept. Specifically, for each attribute we have a label vector with five or more associated concepts. After filtering, we have nf = 267 label vectors in the McRae dataset and in CSL. For each attribute we construct a binary logistic regression model that predicts the presence or absence of the attribute for a concept."}, {"heading": "4.1 Matching word representation sources", "text": "For each characteristic we compare its fit accuracy rated with GloVe-CC word vectors and its rating with word2vec vectors in Figure 2.40 50 60 80 90 1000.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 m (GloVeCC, CSLB) 0.20.00.20.40.6m (Glo Ve CC, Wor dNet) The points are colored according to their property fit.0.0 0.2 0.4 0.7 0.7 m (GloVeCC, CSLB).4050708090M edia, vertical axis; the points are colored, but the correlation between these two factors is low."}, {"heading": "5 The concept view", "text": "The previous classes of perceptual traits are not well encoded by distributional word representations, and that these deficits systematically match across representations. How does this inadequacy of perceptual traits work beyond the calculation of word similarity? We evaluate the similarity between distributional representations and representations from other sources by comparing their predictions of word similarity with each other. For distributional word similarities, we calculate word similarity by cosmic distance: sim (i, j) = (5) We derive from this compact word similarities from the semantic standard datasets with LSA (Landauer et al., 1998). We compute a truncated SVD on the feature matrix Y (0, 1) nc) nf, which is the concatenation of the binary traits introduced in section 4."}, {"heading": "5.1 Domain-level analysis", "text": "Next, we will examine whether some areas of concepts are particularly affected by the deficiencies discussed in the previous sections. We will perform agglomerative clustering using concepts from the CSLB dataset, where LSAi is the vector representation calculated from the semantic authority data for concept i as presented in this section, and FFi is the median feature fit score for a concept i. We will select the weight to provide the most semantically coherent cluster.6The regression model predicts the characteristics of a concept that fit from these basic characteristics: Log (word frequency in brown corpus), Log (# associated characteristics), Log (total yield # feature reports for the concept), # WordNet defects: F = 41,297, p; 61.10 \u2212 defects characteristics for each characteristic: 7ltF < < < 9lt; <"}, {"heading": "6 Conclusion", "text": "We decided to use semantic standard datasets as the gold standard for grounded meanings, and we tested how word representations predicted characteristics within these datasets. We divided these characteristics into high-level categories and found that, despite large variation within the category, several standard distribution representations fell short on average in predicting perceptual characteristics. The difference in predictive accuracy was statistically significant in two of the three representations we evaluated. These deficiencies in encryption corresponded between GloVe and word2vec representations trained on different bodies, indicating that certain classes of characteristics are generally represented."}, {"heading": "Acknowledgements", "text": "We thank Christopher D. Manning, Peng Qi, Pakapol Supaniratisai, Keenon Werling, and members of the NLP communities of Stanford, University of Washington, and Berkeley for useful discussions, and the anonymous critics for their insightful comments."}], "references": [{"title": "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith B. Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa."], "venue": "HLTNAACL.", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Integrating experiential and distributional data to learn semantic representations", "author": ["Mark Andrews", "Gabriella Vigliocco", "David Vinson."], "venue": "Psychological review 116(3):463.", "citeRegEx": "Andrews et al\\.,? 2009", "shortCiteRegEx": "Andrews et al\\.", "year": 2009}, {"title": "Grounded cognition", "author": ["Lawrence W Barsalou."], "venue": "Annu. Rev. Psychol. 59:617\u2013645.", "citeRegEx": "Barsalou.,? 2008", "shortCiteRegEx": "Barsalou.", "year": 2008}, {"title": "Distributional Semantics in Technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Vision and feature norms: Improving automatic feature norm learning through cross-modal maps", "author": ["Luana Bulat", "Douwe Kiela", "Stephen Clark."], "venue": "HLT-NAACL.", "citeRegEx": "Bulat et al\\.,? 2016", "shortCiteRegEx": "Bulat et al\\.", "year": 2016}, {"title": "Is an image worth more than a thousand words? on the fine-grain semantic differences between visual and linguistic representations", "author": ["Guillem Collell", "Marie-Francine Moens."], "venue": "Proceedings of COLING 2016, the 26th Interna-", "citeRegEx": "Collell and Moens.,? 2016", "shortCiteRegEx": "Collell and Moens.", "year": 2016}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "The centre for speech, language and the brain (cslb) concept property norms", "author": ["Barry J Devereux", "Lorraine K Tyler", "Jeroen Geertzen", "Billi Randall."], "venue": "Behavior research methods 46(4):1119\u20131127.", "citeRegEx": "Devereux et al\\.,? 2014", "shortCiteRegEx": "Devereux et al\\.", "year": 2014}, {"title": "What do you know about an alligator when you know the company it keeps? Semantics and Pragmatics 9(17):1\u201363", "author": ["Katrin Erk."], "venue": "https://doi.org/10.3765/sp.9.17.", "citeRegEx": "Erk.,? 2016", "shortCiteRegEx": "Erk.", "year": 2016}, {"title": "From distributional semantics to feature norms: grounding semantic models in human perceptual data", "author": ["Luana Fagarasan", "Eva Maria Vecchi", "Stephen Clark."], "venue": "IWCS.", "citeRegEx": "Fagarasan et al\\.,? 2015", "shortCiteRegEx": "Fagarasan et al\\.", "year": 2015}, {"title": "Retrofitting Word Vectors to Semantic Lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "A synopsis of linguistic theory, 1930-1955", "author": ["John Rupert Firth."], "venue": "Studies in linguistic analysis .", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "A paradigm for situated and goal-driven language learning", "author": ["Jon Gauthier", "Igor Mordatch."], "venue": "arXiv preprint arXiv:1610.03585 .", "citeRegEx": "Gauthier and Mordatch.,? 2016", "shortCiteRegEx": "Gauthier and Mordatch.", "year": 2016}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg."], "venue": "Journal of Artificial Intelligence Research 57:345\u2013420.", "citeRegEx": "Goldberg.,? 2016", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word 10(2-3):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Building a shared world: mapping distributional to modeltheoretic semantic spaces", "author": ["Aur\u00e9lie Herbelot", "Eva Maria Vecchi."], "venue": "EMNLP.", "citeRegEx": "Herbelot and Vecchi.,? 2015", "shortCiteRegEx": "Herbelot and Vecchi.", "year": 2015}, {"title": "Categorical structure among shared features in networks of early-learned nouns", "author": ["Thomas T Hills", "Mounir Maouene", "Josita Maouene", "Adam Sheya", "Linda Smith."], "venue": "Cognition 112(3):381\u2013396.", "citeRegEx": "Hills et al\\.,? 2009", "shortCiteRegEx": "Hills et al\\.", "year": 2009}, {"title": "Virtual embodiment: A scalable long-term strategy for artificial intelligence research", "author": ["Douwe Kiela", "Luana Bulat", "Anita L Vero", "Stephen Clark."], "venue": "arXiv preprint arXiv:1610.07432 .", "citeRegEx": "Kiela et al\\.,? 2016", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "An introduction to latent semantic analysis", "author": ["Thomas K Landauer", "Peter W Foltz", "Darrell Laham."], "venue": "Discourse processes 25(2-3):259\u2013284.", "citeRegEx": "Landauer et al\\.,? 1998", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni."], "venue": "HLT-NAACL.", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "author": ["Ken McRae", "George S Cree", "Mark S Seidenberg", "Chris McNorgan."], "venue": "Behavior research methods 37(4):547\u2013559.", "citeRegEx": "McRae et al\\.,? 2005", "shortCiteRegEx": "McRae et al\\.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "author": ["Philip Resnik"], "venue": "J. Artif. Intell. Res.(JAIR) 11:95\u2013130.", "citeRegEx": "Resnik,? 1999", "shortCiteRegEx": "Resnik", "year": 1999}, {"title": "How Well Do Distributional Models Capture Different Types of Semantic Knowledge", "author": ["Dana Rubinstein", "Effi Levi", "Roy Schwartz", "Ari Rappoport"], "venue": null, "citeRegEx": "Rubinstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics. Association for Computational", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Representing the meanings of object and action words: The featural and unitary semantic space hypothesis", "author": ["Gabriella Vigliocco", "David P Vinson", "William Lewis", "Merrill F Garrett."], "venue": "Cognitive psychology 48(4):422\u2013488.", "citeRegEx": "Vigliocco et al\\.,? 2004", "shortCiteRegEx": "Vigliocco et al\\.", "year": 2004}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "They center around a classic insight from at least as early as Harris (1954); Firth (1957):", "startOffset": 63, "endOffset": 77}, {"referenceID": 11, "context": "They center around a classic insight from at least as early as Harris (1954); Firth (1957):", "startOffset": 78, "endOffset": 91}, {"referenceID": 21, "context": "Popular distributional analysis methods which exploit this intuition such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 23, "context": ", 2013) and GloVe (Pennington et al., 2014) have been critical to the success of many recent", "startOffset": 18, "endOffset": 43}, {"referenceID": 27, "context": "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).", "startOffset": 53, "endOffset": 169}, {"referenceID": 6, "context": "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).", "startOffset": 53, "endOffset": 169}, {"referenceID": 26, "context": "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).", "startOffset": 53, "endOffset": 169}, {"referenceID": 13, "context": "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).", "startOffset": 53, "endOffset": 169}, {"referenceID": 17, "context": "Despite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (Kiela et al., 2016; Gauthier and Mordatch, 2016; Lazaridou et al., 2015).", "startOffset": 282, "endOffset": 355}, {"referenceID": 12, "context": "Despite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (Kiela et al., 2016; Gauthier and Mordatch, 2016; Lazaridou et al., 2015).", "startOffset": 282, "endOffset": 355}, {"referenceID": 19, "context": "Despite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (Kiela et al., 2016; Gauthier and Mordatch, 2016; Lazaridou et al., 2015).", "startOffset": 282, "endOffset": 355}, {"referenceID": 2, "context": "Barsalou, 2008), are used to back up arguments for multimodal learning (at the weakest) or complete embodiment (at the strongest). Kiela et al. (2016) claim the following:", "startOffset": 0, "endOffset": 151}, {"referenceID": 29, "context": "They are a popular resource within psychology and cognitive science as models of human concept representation, and have been used to explain psycholinguistic phenomena from semantic priming and interference (Vigliocco et al., 2004) to the structure of early word learning in child language acquisition (Hills et al.", "startOffset": 207, "endOffset": 231}, {"referenceID": 16, "context": ", 2004) to the structure of early word learning in child language acquisition (Hills et al., 2009).", "startOffset": 78, "endOffset": 98}, {"referenceID": 1, "context": "Andrews et al. (2009) show how \u201cexperiential\u201d semantic norm information can be used to model human judgments of concept similarity.", "startOffset": 0, "endOffset": 22}, {"referenceID": 24, "context": "Rubinstein et al. (2015) confirm that word representations are especially effective at predicting taxonomic features versus attributive features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Collell and Moens (2016) find that word representations fail to pre# word tokens # word types", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "Several studies have used distributional representations to reconstruct aspects of these semantic norm datasets (Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Erk, 2016).", "startOffset": 112, "endOffset": 174}, {"referenceID": 9, "context": "Several studies have used distributional representations to reconstruct aspects of these semantic norm datasets (Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Erk, 2016).", "startOffset": 112, "endOffset": 174}, {"referenceID": 8, "context": "Several studies have used distributional representations to reconstruct aspects of these semantic norm datasets (Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Erk, 2016).", "startOffset": 112, "endOffset": 174}, {"referenceID": 0, "context": "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).", "startOffset": 204, "endOffset": 287}, {"referenceID": 3, "context": "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).", "startOffset": 204, "endOffset": 287}, {"referenceID": 10, "context": "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).", "startOffset": 204, "endOffset": 287}, {"referenceID": 4, "context": "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).", "startOffset": 204, "endOffset": 287}, {"referenceID": 0, "context": "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016). Young et al. (2014) develop an alternative operationalization of denotational meaning using image captioning datasets, and demonstrate gains over distributional representations on textual similarity and entailment datasets.", "startOffset": 205, "endOffset": 309}, {"referenceID": 23, "context": "GloVe: GloVe (Pennington et al., 2014) estimates word representations wi by using them to reconstruct a word-word co-occurrence matrix X collected from a large text corpus:", "startOffset": 13, "endOffset": 38}, {"referenceID": 21, "context": "word2vec: word2vec (Mikolov et al., 2013) estimates word representations by optimizing a skip-gram objective to predict all words wj within a context window c of a word wi given their word representations:", "startOffset": 19, "endOffset": 41}, {"referenceID": 20, "context": "McRae Our initial experiments use the semantic norm dataset from McRae et al. (2005), which consists of 541 concrete noun concepts with associated feature norms, collected from 725 participants.", "startOffset": 65, "endOffset": 85}, {"referenceID": 20, "context": "The dataset groups features into several perceptual and non-perceptual categories: taxonomic, encyclopedic, function, visual-motion, visual-form and surface, visual-colour, sound, tactile, and taste (McRae et al., 2005).", "startOffset": 199, "endOffset": 219}, {"referenceID": 7, "context": "CSLB We reproduce and extend our results on a second semantic norm dataset collected by the Cambridge Centre for Speech, Language and the Brain (CSLB; Devereux et al., 2014).", "startOffset": 144, "endOffset": 173}, {"referenceID": 7, "context": "CSLB We reproduce and extend our results on a second semantic norm dataset collected by the Cambridge Centre for Speech, Language and the Brain (CSLB; Devereux et al., 2014). CSLB contains 638 concepts provided by 123 participants. Their data collection closely followed McRae et al. (2005), though features were included if at least 2 participants named that feature.", "startOffset": 151, "endOffset": 291}, {"referenceID": 18, "context": "We derive compact concept representations from the semantic norm datasets with LSA (Landauer et al., 1998).", "startOffset": 83, "endOffset": 106}, {"referenceID": 22, "context": "As a secondary data source, we also compute word-word similarity judgments from the WordNet taxonomy (Miller, 1995).", "startOffset": 101, "endOffset": 115}], "year": 2017, "abstractText": "Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.", "creator": "LaTeX with hyperref package"}}}