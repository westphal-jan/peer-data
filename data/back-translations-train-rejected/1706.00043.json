{"id": "1706.00043", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Biased Importance Sampling for Deep Neural Network Training", "abstract": "Importance sampling has been successfully used to accelerate stochastic optimization in many convex problems. However, the lack of an efficient way to calculate the importance still hinders its application to Deep Learning. In this paper, we show that the loss value can be used as an alternative importance metric, and propose a way to efficiently approximate it for a deep model, using a small model trained for that purpose in parallel.", "histories": [["v1", "Wed, 31 May 2017 18:25:09 GMT  (941kb,D)", "http://arxiv.org/abs/1706.00043v1", null], ["v2", "Wed, 13 Sep 2017 12:54:33 GMT  (1051kb,D)", "http://arxiv.org/abs/1706.00043v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["angelos katharopoulos", "fran\\c{c}ois fleuret"], "accepted": false, "id": "1706.00043"}, "pdf": {"name": "1706.00043.pdf", "metadata": {"source": "CRF", "title": "Biased Importance Sampling for Deep Neural Network Training", "authors": ["Angelos Katharopoulos", "Fran\u00e7ois Fleuret"], "emails": ["name.surname@idiap.ch"], "sections": [{"heading": "1 Introduction", "text": "The dramatic increase in available training data has made the use of deep neural networks possible, which in turn has improved the state of the art in many areas, particularly computer vision and natural language processing. Due to the complexity of the resulting optimization problem, computational costs are now the core problem in training these large architectures. In training such a model, it seems that not all samples are equally important; many of them are properly treated after a few periods of training, and most can be ignored at this point without affecting the resulting end modelers."}, {"heading": "2 Related Work", "text": "Bordes et al. [2005] developed LASVM, which is an online algorithm that uses value samples to train nucleated support vector machines. Later, Needell et al. [2014] and more recently Zhao and Zhang [2015] developed more general value samples that improve the convergence of the Stochastic Gradient Descent. Specifically, the latter has decisively linked the convergence speed of the SGD to the variance of the gradient estimator and has shown that the target sampling distribution is the one that minimizes this variance. Alain et al. [2015] are the first to our knowledge to attempt to use meaning sample samples for the formation of deep neural networks. They sample them according to the exact gradient standard calculated by a cluster of GPU workers. Even in a cluster of GPUs, they must restrict the networks they use to obtain fully interconnected weights in order to achieve an appropriate weight for the GPU workers."}, {"heading": "3 Importance Sampling", "text": "In the following sections, we will analyze how this method works and present an efficient method for training a deep learning model. In addition, we will show that our meaning sampling method has a close relationship to maximum loss minimization: By using exponentiation of loss as importance instead of loss itself, we can minimize a smooth maximum loss while controlling variance."}, {"heading": "3.1 Exact Importance Sampling", "text": "The goal of training is that we update the parameters of our model iteratively, between two consecutive iterations t and t + 1, where N corresponds to the number of examples in training. (Using Stochastic Gradient Descent with learning rate). (Let us use iterative the parameters of our model, between two consecutive iterations t and t + 1, with the number of examples in training. (Let us use the model iteratively to update the parameters of our model, between two consecutive iterations t and t + 1, with the number of examples in training. (Let us use the number of examples in training). (Let us use the parameters in training.) We have a discrete random variable in the distribution P with probabilities pi and i is an example weighting."}, {"heading": "3.2 Relation to Max Loss Minimization", "text": "Shalev-Shwartz and Wexler [2016] argue that minimizing the maximum loss can lead to better generalization performance, especially if there are a few \"rare\" samples in the training set. Instead of choosing the sample weights so that we get an unbiased estimate of the gradient, we define them according to to\u03b1i = 1Npki with a variable intensity up to the point at which the maximum losses are minimized.Instead of choosing the sample weights so that we get an unbiased estimate for the gradient, we define them according to to\u03b1i = 1Npki, using k (\u2212 1], (12) and using Li = L (xi;), yi) and pi Li, using the EP [2] to get an unbiased estimate for the gradient, we define them according to to\u03b1i = 1Npki, with k (\u2212 1], and with Li = L (xi;), yi) and pi Li, 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-"}, {"heading": "3.3 Approximate Importance Sampling", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3.3.1 Smoothing", "text": "Modern deep learning models often contain stochastic layers, such as dropout, which can lead to vastly different results for different runs on a set of constant parameters and a sample. This fact, combined with the inevitable approximation error of our meaning model, can lead to the prediction of pathological cases of samples that are of low significance (i.e. high weight \u03b1i) but end up with high losses. To alleviate this problem, we use additive smoothing to influence the sample distribution in the direction of uniform sampling. Experimentally, we observe that a good rule of thumb is to add a constant c such that c \u2264 12N \u2211 N i = 1 L (\u0432 (xi; \u03b8t), yi) for all iterations."}, {"heading": "4 Experiments", "text": "We analyze experimentally how our meaning sampling scheme improves convergence speed and generalization performance, first for image classification using a deep conversion network, on the MNIST [LeCun et al., 1998] in \u00a7 4.1 and CIFAR10 [Krizhevsky, 2009] and then for word prediction on the Penn Treebank Dataset [Marcus et al., 1993] in the three different sampling strategies: uniform sampling, which the model itself uses to calculate the meaning, and which uses our approximation, which is defined in \u00a7 3.3. For the hyperparameters that controls the smooth maximum loss, we choose the values k = 1 and k = 0.5."}, {"heading": "4.1 Image classification with MNIST", "text": "Considering the simplicity of the data set, we chose for the model a scaled-down variant of the VGG architecture [Simonyan and Zisserman, 2014], with two revolutionary layers with a double number of filters. Finally, we add two fully connected layers with a rate of 0.5, and a final classification layer with a dropout rate of 0.25. All layers use the ReLU activation function, followed by the same combination of layers with a double number of filters. Finally, we add two fully connected layers with a rate of 0.5, and a final classification layer. All layers use the ReLU activation for 6,000 iterations with a mini-batch size of 128. We calculate the loss and accuracy on the test put all 300 mini-batches."}, {"heading": "4.2 Image classification with CIFAR10", "text": "We developed a VGG-inspired network (with batch normalization) consisting of three convolution pooling blocks with 64, 128 and 256 filters, two fully interconnected layers of sizes 1024 and 512, and a classification layer. Dropout is used in a similar way to the MNIST network with rates of 0.25 after each pooling layer and 0.5 after the fully connected layers. The activation function in all layers is ReLU. Each network is trained for 50,000 iterations with a batch size of 128 samples. After 35,000 iterations, we reduce the learning rate by multiplying them by 10 \u2212 1."}, {"heading": "4.3 Word prediction with Penn Tree Bank", "text": "In order to assess the universality of our approach, we have taken a number of measures that we have taken."}, {"heading": "5 Conclusion", "text": "We have proposed an Importance Sample Scheme suitable for use with Deep Learning models, and an optional, distorted Gradient Estimator that can focus on hard examples in the training set and improve the generalization performance of our models. In addition, we have shown that the loss can be approximated with a model of much lower complexity, paving the way for efficient and practical training of Deep Models with Importance Samples. Subject to further research are experimenting with other models for approximate Importance Sampling, as well as combining our proposed method of measuring Importance with optimized implementations that can significantly improve the training time of the wall clock."}, {"heading": "A Justification for sampling with the loss", "text": "The aim of this analysis is to justify the use of loss as an upper limit compared to the two values studied. (...) The aim of this analysis is to justify the use of loss as a lower limit. (...) The aim of this analysis is to justify the use of loss as a lower limit. (...) The aim of this analysis is to justify the use of loss as a lower limit. (...) We will show that loss is a surrogate for the lower limit by calculating the exact gradient standard for a limited number of samples and comparing it with the losses. (...) The aim of this analysis is to define the most common loss functions for classification and regression as their gradient norm. (...) We will then use this derivative, in \u00a7 A.1.1, together with an upper limit for the deviation of the gradients and show that this deviation is reduced."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Variance reduction in sgd by distributed importance sampling", "author": ["G. Alain", "A. Lamb", "C. Sankar", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1511.06481,", "citeRegEx": "Alain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Fast kernel classifiers with online and active learning", "author": ["A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bordes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2005}, {"title": "Importance sampling tree for large-scale empirical expectation", "author": ["O. Can\u00e9vet", "C. Jose", "F. Fleuret"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Can\u00e9vet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Can\u00e9vet et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "In Twelfth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "author": ["D. Needell", "R. Ward", "N. Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Needell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Needell et al\\.", "year": 2014}, {"title": "On optimal probabilities in stochastic coordinate descent methods", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1310.3438,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2013}, {"title": "Minimizing the maximal loss: How and why", "author": ["S. Shalev-Shwartz", "Y. Wexler"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz and Wexler.,? \\Q2016\\E", "shortCiteRegEx": "Shalev.Shwartz and Wexler.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Accelerating deep neural network training with inconsistent stochastic gradient descent", "author": ["L. Wang", "Y. Yang", "M.R. Min", "S. Chakradhar"], "venue": "arXiv preprint arXiv:1603.05544,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "author": ["P. Zhao", "T. Zhang"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Zhao and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Zhao and Zhang.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "For convex optimization problems, many works [Bordes et al., 2005, Zhao and Zhang, 2015, Needell et al., 2014, Can\u00e9vet et al., 2016, Richt\u00e1rik and Tak\u00e1\u010d, 2013] have taken advantage of the difference in importance among the samples to improve the convergence speed of stochastic optimization methods. On the contrary, important sampling is rarely used in conjunction with Deep Learning models. Zhao and Zhang [2015] and Alain et al.", "startOffset": 46, "endOffset": 415}, {"referenceID": 1, "context": "Zhao and Zhang [2015] and Alain et al. [2015] prove that sampling according to the gradient norm minimizes the variance of the gradient estimates and improves the convergence speed of SGD.", "startOffset": 26, "endOffset": 46}, {"referenceID": 2, "context": "Curriculum learning [Bengio et al., 2009] and its evolution self-paced learning [Kumar et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": ", 2009] and its evolution self-paced learning [Kumar et al., 2010] present the classifier with easy samples first (samples that are likely to have a small loss) and gradually introduce harder and harder samples.", "startOffset": 46, "endOffset": 66}, {"referenceID": 1, "context": "Bordes et al. [2005] developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bordes et al. [2005] developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines. Later Needell et al. [2014] and more recently Zhao and Zhang [2015] developed more general importance sampling methods that improve the convergence of Stochastic Gradient Descent.", "startOffset": 0, "endOffset": 170}, {"referenceID": 1, "context": "Bordes et al. [2005] developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines. Later Needell et al. [2014] and more recently Zhao and Zhang [2015] developed more general importance sampling methods that improve the convergence of Stochastic Gradient Descent.", "startOffset": 0, "endOffset": 210}, {"referenceID": 1, "context": "Alain et al. [2015] are the first ones, to our knowledge, that have attempted to use importance sampling for training Deep Neural Networks.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Alain et al. [2015] are the first ones, to our knowledge, that have attempted to use importance sampling for training Deep Neural Networks. They sample according to the exact gradient norm as computed by a cluster of GPU workers. Even with a cluster of GPUs they have to constrain the networks that they use to fully connected layers in order to be able to compute the gradient norm in a reasonable time. Can\u00e9vet et al. [2016] worked on improving the sampling procedure for importance sampling.", "startOffset": 0, "endOffset": 427}, {"referenceID": 15, "context": "and define Gi = \u03b1i\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi), then we get (this is a different derivation of the result by Wang et al. [2016])", "startOffset": 100, "endOffset": 119}, {"referenceID": 1, "context": "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi \u221d \u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi \u221d \u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162.", "startOffset": 0, "endOffset": 46}, {"referenceID": 1, "context": "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi \u221d \u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162. However, computing the norm of the gradient for each sample is computationally intensive. Alain et al. [2015] use a distributed cluster of workers and constrain their models to fully connected networks while Zhao and Zhang [2015] only consider convex problems and sample according to the Lipschitz constant of the loss of each sample, which is an upper bound of the gradient norm.", "startOffset": 0, "endOffset": 231}, {"referenceID": 1, "context": "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi \u221d \u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162. However, computing the norm of the gradient for each sample is computationally intensive. Alain et al. [2015] use a distributed cluster of workers and constrain their models to fully connected networks while Zhao and Zhang [2015] only consider convex problems and sample according to the Lipschitz constant of the loss of each sample, which is an upper bound of the gradient norm.", "startOffset": 0, "endOffset": 351}, {"referenceID": 13, "context": "Shalev-Shwartz and Wexler [2016] argue that minimizing the maximum loss can lead to better generalization performance, especially if there exist a few \u201crare\u201d samples in the training set.", "startOffset": 0, "endOffset": 33}, {"referenceID": 8, "context": "We analyze experimentally how our importance sampling scheme improves convergence speed and generalization performance, first for image classification with a deep convolution network, on MNIST [LeCun et al., 1998] in \u00a7 4.", "startOffset": 193, "endOffset": 213}, {"referenceID": 6, "context": "1 and CIFAR10 [Krizhevsky, 2009] in \u00a7 4.", "startOffset": 14, "endOffset": 32}, {"referenceID": 9, "context": "2, and then for word prediction using deep recurrent network on the Penn Treebank dataset [Marcus et al., 1993] in \u00a7 4.", "startOffset": 90, "endOffset": 111}, {"referenceID": 5, "context": "In all the experiments, we deviate slightly from our theoretical analysis and algorithm by sampling mini-batches instead of single samples in line 6, and using the Adam optimizer [Kingma and Ba, 2014] instead of plain Stochastic Gradient Descent in lines 8 and 9 of Algorithm 1.", "startOffset": 179, "endOffset": 200}, {"referenceID": 4, "context": "In all the experiments, we deviate slightly from our theoretical analysis and algorithm by sampling mini-batches instead of single samples in line 6, and using the Adam optimizer [Kingma and Ba, 2014] instead of plain Stochastic Gradient Descent in lines 8 and 9 of Algorithm 1. Experiments were conducted using Keras Chollet et al. [2015] with TensorFlow Abadi et al.", "startOffset": 180, "endOffset": 340}, {"referenceID": 0, "context": "[2015] with TensorFlow Abadi et al. [2016], and the code to reproduce the experiments will be provided under an open source license when the paper will be published.", "startOffset": 23, "endOffset": 43}, {"referenceID": 14, "context": "Given the simplicity of the dataset, we chose for model a down-sized variant of the VGG architecture [Simonyan and Zisserman, 2014], with two convolutional layers with 32 filters of size 3\u00d7 3, a max-pooling layer and a dropout layer with a dropout rate of 0.", "startOffset": 101, "endOffset": 131}, {"referenceID": 10, "context": "We used the Penn Treebank language dataset, as preprocessed by Mikolov et al. [2011]1, and a recurrent neural network as the language model.", "startOffset": 63, "endOffset": 85}, {"referenceID": 10, "context": "We used the Penn Treebank language dataset, as preprocessed by Mikolov et al. [2011]1, and a recurrent neural network as the language model. Initially, we split the dataset into sentences and add an \u201cEnd of sentence\u201d token (resulting in a vocabulary of 10,001 words). For each word we use the previous 20 words, if available, as context for the neural network. Our language model is similar to the small LSTM used by Zaremba et al. [2014] with 256 units, a word embedding in R and dropout with rate 0.", "startOffset": 63, "endOffset": 439}], "year": 2017, "abstractText": "Importance sampling has been successfully used to accelerate stochastic optimization in many convex problems. However, the lack of an efficient way to calculate the importance still hinders its application to Deep Learning. In this paper, we show that the loss value can be used as an alternative importance metric, and propose a way to efficiently approximate it for a deep model, using a small model trained for that purpose in parallel. This method allows in particular to utilize a biased gradient estimate that implicitly optimizes a soft max-loss, and leads to better generalization performance. While such method suffers from a prohibitively high variance of the gradient estimate when using a standard stochastic optimizer, we show that when it is combined with our sampling mechanism, it results in a reliable procedure. We showcase the generality of our method by testing it on both image classification and language modeling tasks using deep convolutional and recurrent neural networks. In particular, in case of CIFAR10 we reach 10% classification error 50 epochs faster than when using uniform sampling.", "creator": "LaTeX with hyperref package"}}}