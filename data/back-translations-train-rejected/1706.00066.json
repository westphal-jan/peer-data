{"id": "1706.00066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Descriptions of Objectives and Processes of Mechanical Learning", "abstract": "In [1], we introduced mechanical learning and proposed 2 approaches to mechanical learning. Here, we follow one such approach to well describe the objects and the processes of learning. We discuss 2 kinds of patterns: objective and subjective pattern. Subjective pattern is crucial for learning machine. We prove that for any objective pattern we can find a proper subjective pattern based upon least base patterns to express the objective pattern well. X-form is algebraic expression for subjective pattern. Collection of X-forms form internal representation space, which is center of learning machine. We discuss learning by teaching and without teaching. We define data sufficiency by X-form. We then discussed some learning strategies. We show, in each strategy, with sufficient data, and with certain capabilities, learning machine indeed can learn any pattern (universal learning machine). In appendix, with knowledge of learning machine, we try to view deep learning from a different angle, i.e. its internal representation space and its learning dynamics.", "histories": [["v1", "Wed, 31 May 2017 19:42:41 GMT  (555kb,D)", "http://arxiv.org/abs/1706.00066v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["chuyu xiong"], "accepted": false, "id": "1706.00066"}, "pdf": {"name": "1706.00066.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Chuyu Xiong"], "emails": ["chuyux99@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Mechanical Learning, Learning Machine, Objective and Subjective Patterns, X-Form, Universal Learning, Learning by Teaching, Internal Representation Space, Data Sufficiency, Learning Strategy, Squeaking Up, Embedding in Parameter SpacesIf you want to know the taste of a pear, you have to change the pear by eating it yourself....... All real knowledge comes from direct experience. - Mao ZedongBut although all our knowledge begins with experience, it does not follow that everything comes from experience. - Immanuel KantOur problem,...... is to explain how the transition from a lower level of knowledge to a level that is judged to be higher. - Jean Piaget"}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to achieve our goals, and that we are able to achieve our goals."}, {"heading": "2 Learning Machine", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "IPU \u2013 Information Processing Unit", "text": "We have the problem that most people are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to play, to edit, to play, to edit, to play, to edit, to edit, to edit, to play, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, edit, to edit, to edit, edit, to edit, edit, edit, to edit, edit, to edit, edit, to edit, edit, to edit, edit, edit, to edit, edit, to edit, edit, to edit, edit, to edit, edit, to edit, edit, edit, to edit, edit, edit, to edit, edit, to edit, edit, to edit, edit, edit, to edit, to edit, edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit"}, {"heading": "Data", "text": "The purpose of a learning machine is to learn, i.e. to modify its information processing. However, we would stress that for mechanical learning, learning is driven by data fed into it. Definition 2.2 (Data Sequence) If we have a sequence Ti, i = 1, 2,... and Ti = (bi, oi) where bi is a base pattern, oi is either \u2205 (empty) or a binary vector in the output space, we call this sequence a data sequence. Note: oi could be empty or a vector in the output space. If it is not empty, it means that the vector should be the value of the output at the moment. If it is empty, it means that there is no data for output. Learning machine should be able to learn even oi is empty. Of course, learning with the value of output is often easier and faster. We can easily see that data sequence is the only source of information for a learning machine to modify itself."}, {"heading": "Universal Learning Machine", "text": "In this case, it is that we are able to hide, and that we are able to hide, that we are able to hide."}, {"heading": "Different Level of Learning", "text": "It is obvious that there is a mechanism within the learning machine to differentiate between two things: 1) the learning mechanism only modifies processing, and the learning mechanism itself is not modified; 2) the learning mechanism itself is also modified. But, how can one describe these things well? If M is a universal learning machine, so that for every 2 processing P0 and P1 we have only one data sequence T, so that starting from P0, and applying T to M, their processing becomes P1. This is clear, somehow we are looking at another data sequence, so that processing becomes P0 and P1 again. Since M is universal, this is allowed. But we ask what if we apply the data sequence T? What would happen? Do we still have processing will be P1? There is no guarantee that this will be the case for 2.5 years."}, {"heading": "Some Examples", "text": "Example 2.1 [Perceptron] Perhaps the simplest learning machine is the Perceptron. Perceptron P is a 2-1 IPU, and it is a learning machine. However, it is not universal. As is generally known, P has no AND gate and XOR gate. That is, no matter what, P could not learn this 2 processing. Example 2.2 [RBM is a learning machine] See [4] for RBM. N -1 RBM is a N -1 IPU. It is also a learning machine. There could be many ways to learn it. The most common way is the so-called Gibbs sampling methods. We can see this clearly: Gibbs sampling is a simple set of rules, and processing is modified as data is fed in. However, as we can see in Annex, N -1 RBM is not universal."}, {"heading": "3 Pattern, Examples, Objective and Subjective", "text": "Incoming data drive learning. But, IPU and Learning machine do it bit-wise. So, patterns are very important for the learning machine and learning, and we can look at patterns subjectively, i.e., we are very important to clarify the concept."}, {"heading": "Pattern, Objectively", "text": "In fact, the PS0N is a huge set of PS0N and PS0N, which is the world of the PS0N and PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N of the PS0N."}, {"heading": "Pattern, Subjectively", "text": "When we say that there is an incoming pattern p to a learning machine, what do we mean? When we see this pattern objectively, the meaning is clear: at the entrance room, a binary vector is presented that is a face of the incoming pattern p. This does not depend at all on the learning machine. And that is very clear and ambiguous. However, as our examples have shown, we have to look at patterns subjectively. We have to proceed slowly because there is a lot of confusion here. We have to look at something that is not valid at all. Pattern, 1-significant or 0-significant First, when we discuss patterns subjectively, we have to know: Is 1 significant? or 0 significant, or are both equally significant?"}, {"heading": "Patterns and Learning Machine", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "How Learning Machine Perceives Patterns", "text": "This year, it is so far that it will be able to retaliate, \"he said.\" We have never achieved so much as this year, \"he said.\" We have never achieved so much, \"he said.\" We have never achieved so much as we set out to do. \""}, {"heading": "Pattern, Subjective Operators", "text": "In fact, most of them are able to play by the rules they have imposed on themselves, and they are able to play by the rules they have imposed on themselves, \"he said.\" But it's not that they have to play by the rules. \"He added,\" It's not that they have to play by the rules, it's that they have to play by the rules. \"He added,\" It's not that they have to play by the rules. \"He added,\" But it's not that they have to play by the rules. \""}, {"heading": "X-Form", "text": "If we use the 3 operators in succession, we will have an algebraic expression. Of course, in order to find this algebraic expression, it makes sense to modify its perception, but we want to know what we can construct from such algebraic expressions. First, we see some subjective patterns. This pattern is: either b1 or b3 together. However, the expression has more aspects. As we have an algebraic expression, we can get another value. Actually, this is what we can say, namely: a subjective pattern."}, {"heading": "Sub-Form", "text": "Multiple X shapes could form a new X shape. And any part of an X shape is also an X shape. Such a part could be quite useful. So here we discuss the sub-form. Definition 3.16 (sub-form of an X shape) Suppose e is an X shape, then it is an algebraic expression E (of 3 subjective operations) on a set of basic patterns g = {b1, b2,..., bK}, so that e = E (g) = E (b1, b2,..., bK). A sub-form of e is an algebraic expression It on a subset of g, gs = {bs1,...,., bsJ}, J \u2264 K, so that it = It (bs1,..., bsJ) = It (...), bsJ), and the objective pattern expressed by it is b2."}, {"heading": "4 Learning by Teaching", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "5 Learning without Teaching Sequence", "text": "Learning by teaching is a very special way of advancing learning. From the discussions in the last section, we can clearly see that only when we have complete knowledge of the learning machine and the desired pattern can we possibly design a teaching sequence. In this sense, learning by teaching is very similar to programming - the ability to bring to the machine, not to learn the machine itself. Of course, learning by teaching is still another step beyond programming, and it will give us much more power in dealing with machines than just programming. We focus on N-1 Learning Machine M."}, {"heading": "Typical Mechanical Learning", "text": "From the examples of mechanical learning, typical mechanical learning would look like this: 1. For N -1 learning machine M, the learning objective is often given as objective pattern po, M is expected to learn, and the learning result is that the black set ofM becomes po. 2. To drive mechanical learning, the data sequence intoM is fed in. 3. Feed-in data will drive learning, i.e. the black set ofM will change at the moment the data is fed into M, po, and from pco. 3. Feed-in data will drive learning, i.e. the black set ofM will change."}, {"heading": "Internal Representation Space", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "Learning Methods", "text": "For a learning machine M, in addition to input room, output room and internal representation room E, it is clear that it must also have learning mechanisms or learning methods. So, we must describe learning methods. Now, we know that learning is a dynamic on internal representation space that moves from one X form to another. But, exactly, we have learning machine M, its input room, output room and its internal representation room E, and a learning method LM. As in definition 5.1, we also have target pattern po, and data sequences {bi, oi) | i = 1, 2,."}, {"heading": "Data Sufficiency", "text": "However, it would be nice to use less data to do more when possible. More importantly, we need to understand what data is being used for what purpose. As we already know, learning is actually only there to get a good X-shape. But, an X-shape is usually quite complicated and quite elusive data material. How can a mechanical learning method get it? How can an X-shape itself give a good description of such data? We already know that an X-shape and all its sub-shapes give perception bits. This tells us that X-shape and all its sub-shapes describe the structure of the black group. To say an X-shape, the least necessary data is 2: one is in the black group, another is not in the black group. Of course, data 2 is not sufficient to describe such a sub-shape."}, {"heading": "Examples of Data Sufficient to Support a X-form:", "text": "1. e = b1 + b2 is an X-form. All its subforms are b1 and b2. Thus, {(b1, 1), b2} data is sufficient to e. 2. e = b1 \u00b7 b2 is an X-form. e has no subform. Datasets {b \"} or {b\"} or {b \"\u2032\"} are all data sufficient to support e. 3. e = b1 + (b1 \u00b7 b2) is an X-form. All its subforms are b1 and b1 \u00b7 b2. Dataset {b1, b \"} is sufficient to e. and the same applies to {b1, b\"}."}, {"heading": "Learning Strategies and Learning Methods", "text": "Again, learning is a dynamic of the X-forms, from one X-form to the next. X-form is complicated. How does such a dynamic come to the desired X-form? Such a dynamic is determined by learning methods and learning strategies. We discussed learning methods above, which are well described in Equation (lm). Learning methods have a set of rules on how to move from one X-form to another. Learning strategy is higher than learning method. It will regulate these aspects: which X-forms should be taken into account? What general approach to the X-form? Preferring some X-forms? Or everything from the ground up? etc. So we can see that this strategy is governed. Other strategy also works for other types of data. Different strategy also needs different learning capacities. We should focus on this: Learning is a complicated thing, a strategy and a method that does not fit all situations. There must be many strategies and even more methods. We will discuss some strategies and methods."}, {"heading": "6 More Discussions about Learning Machine", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Learning vs. Approximating", "text": "Very often people say, \"Machine learning is nothing more than a kind of approximation of the probability distribution.\" We would argue that this view is quite far from reality. For the sake of argument, let's look at the dictionary definition first. We use the online dictionary \"www.dictionary.com.\" For learning, however, the definition means \"acquiring knowledge or skills through study, guidance, or experience\"; for approximation, \"close to; close to; close to; estimate.\" They are very different. Of course, like many other couples, these two words actually have something in common, like \"approaching knowledge vs. acquiring knowledge.\" It is hard to make an absolute black and white distinction. However, we would like to point out a big difference: learning means using external information to build the best possible within the model, while approximation is the use of external information to get closer to a pre-selected model."}, {"heading": "Internal Representation Space", "text": "In fact, we are able to put ourselves in a situation in which we are able to survive ourselves, in which we are able, and in which we are able to put ourselves in a situation in which we are able, in which we are able to assert ourselves, in which we are able, in which we are in which we are in which we are in."}, {"heading": "Deterministic vs. Probabilistic", "text": "Our definition makes the learning machine deterministic, and everything that follows is also deterministic. Within this framework, any probabilistic view, if any, is only complementary. However, we would like to point out that 100% deterministic is difficult. Probabilistic view actually has advantages in some aspects, and it is necessary. However, it is often easier to obtain a probability measurement than an exact function. However, we would like to point out that learning machine should be based mainly on deterministic framework. There are compelling reasons for this. Input data actually have rich intrinsic structure, and a learning machine needs to capture such structure in order to learn efficiently and effectively and capture process patterns. If learning machine has 100% probabilistic view, it would treat all things as a random event, its internal representation space would be very flat (i.e., no hierarchy structure), and a learning machine needs to capture a determinability."}, {"heading": "Data Sufficiency", "text": "We use X-form and sub-form to understand this problem. We define data sufficient to support an X-form, and data sufficient to bind an X-form. Such sufficiency sets a theoretical framework for us to understand data: Why do we need such a lot of data for this learning? How much data is necessary for this learning? etc. But to determine such data sufficiency, it is only the beginning. In Section 5, Strategy 2 and Strategy 3 show that data sufficiency actually depends on learning ability. In general, strong ability needs weaker data and vice versa. The relationship between data and learning strategy, methods and machine ability is very important. The data sufficiency we introduce is only the beginning, we need to work more in this direction. Again, this is related to learning theory."}, {"heading": "Learning Strategies and Methods", "text": "A learning machine could be intelligent (i.e. it can learn better by using less data, learning faster, etc.) and it could be stupid (more data, slow learning, etc.). Wisdom or stupidity is usually determined by learning strategies and methods.In Section 5, we show 3 learning strategies. There should be many other learning strategies and methodologies. We invent strategies that lead to higher abstraction and generalization. We should keep inventing here.Actually, we could learn methods.We have defined Level 1 learning machines that cannot change their methodology.Level 1 learning machines are what we focus on most of the time, but in many cases it is good to introduce Level 2 learning machines that can modify their methods.In this sense, one approach should be: We will use learning to obtain more and better learning strategies and methods."}, {"heading": "Mathematical Learning Theory", "text": "If the learning machine is super intelligent, it may only need a few data to learn everything, but we are talking about mechanical learning here. Learning machine follows only a few simple and mechanical rules. It could not learn from scratch. Without a sufficient data base, it will not be able to learn. To understand these problems, it is best to have a general theory about learning: by applying what learning strategy and method, with what skills, on what kind of data, how much data, what a learning machine can learn, with what kind of complexity. We firmly believe that such a theory could be established, and such a theory is mathematical. Although we cannot currently pursue this theory, we can speculate that this theory will make learning clearer. This theory will be able to explain and measure the complexity of learning objects, data structure, efficiency and effectiveness of learning methods and strategies, learning processes and governing equations."}, {"heading": "7 How to Design Effective Learning Machine", "text": "There are two approaches to exploring learning machines: one is to describe learning machines well, the other is to design a particular learning machine. Both approaches should support each other. In this work, we will make the first approach. We will make the second approach elsewhere. However, as we get some good insights from the work here, which will serve as good guidance for designing effective and efficient learning machines, we will briefly discuss some issues of developing learning machines."}, {"heading": "1. It should have very effective and efficient internal representation space", "text": "It is not good to embed the internal representation space in a real parameter space RU, in which U is a huge integer, like many millions. In this way, we lose the ability to navigate and understand immediately. Armed with the knowledge we have discussed, we know that the collection of X-shapes is the internal representation space. It is natural for us to find a mechanism to realize X-shape in design. We should also realize that X-shape is by nature a connecting model."}, {"heading": "2. It should be self-aware about new base patterns", "text": "Self-awareness about new basic patterns is an important attribute. If possible, we should make this attribute ready for a learning system."}, {"heading": "3. It is level 1 learning machine, but with ability to become higher level", "text": "Since learning methods do not change with learning, it is much easier to withdraw when problems arise. If it is not level 1, rollback would be much more difficult or simply impossible. However, it should be prepared to move to level 2 or, if necessary, even to a higher level of learning. It should make full use of methods of evolution and inheritance if necessary."}, {"heading": "4. It should have best possible prior knowledges", "text": "In principle, we want a universal learning machine. But in practice, a learning machine is for some specific tasks. Often, we have prior knowledge of these tasks. To integrate this prior knowledge into learning machines, learning could be greatly improved. Previous knowledge could be incorporated into learning methods and the initial state of the internal representation space."}, {"heading": "5. Allow more learning strategies and methods be available", "text": "It should have more than one strategy and method. It can switch strategies and methods by arguing on the basis of feed-in data. As we have shown in sections 4 and 5 that with certain skills a learning machine becomes universally applicable, it is reasonable to make a learning machine so that it has learning abilities by teaching and learning without teaching. Most likely, these are the minimal abilities of an effective learning machine. Guided by the above principles, we have developed a type of learning machine: OSIPL machine. Details of the OSIPL machine will be discussed elsewhere."}, {"heading": "Appendix", "text": "In the discussions about the learning machine, we have introduced our view of the way mechanical learning is carried out. In this appendix, we would like to use this view of deep learning to see deep learning. Hopefully, we can see some interesting structures and details of deep learning from this particular point of view. By our definition, deep learning is actually mechanical learning when humans do not intervene. Of course, this \"if\" is a big if. We want to limit our discussion to Hinton's original model [4], i.e., we introduce the term \"mechanical learning\" for this purpose: to isolate a learning machine from human intervention so that we can discuss details of a learning machine. We want to limit our discussion to Hinton's original model [4], i.e., a stack of RBMs. Each level of RBM is clearly an N-M learning machine (N andM are dimensions of input and output). Hinton's deep learning model is brought together by stacking RBM."}, {"heading": "2-1 RBM", "text": "However, it is also very useful because we can examine all the details, and such details give us a good guide to more general RBM.2-1 RBM is an IPU. We know that 2-1 IPU has a total of 16 processing patterns (22 2), but we only consider this processing: p (0, 0) = 0, so a total of 8 processing, which we call Pj, j = 0,.7 (see [1]). For 2-1 RBM, any processing p can be written as: for input (i1, i2), the output is o: o = p (i1, i2) = characters (ai1 + bi2), where (a, b), characters (x). We (x) = {1, if x 0, if x < 0The parameters (a, b) determine what the processing really is."}, {"heading": "3-1 RBM", "text": "To discuss it, we can gain some insights for the general RBM = 1 regions = 1 regions. For 3-1 regions RBM we can still write: for each input (i1, i3, i3), pattern (x) = 1, if x < 0However, o = p (i1, i2, i3) = sign (ai1 + bi2 + ci3), where (a, b, c), pattern (x) = 1, if x \u2265 0, if x < 0However, while we can easily write down all possible processing steps of 2-1 RBM, it would be difficult to do this for 3-1 RBM. For 3-1 IPU we know that the number of all possible processing steps is 22 3 = 28 = 256. Since we can only consider such processing steps p: p (0, 0, 0) = 0, the number will be 256 / 2 = 128."}, {"heading": "Stacking RBMs", "text": "If we look at an N -M RBM R1, and an M -L RBM R2 by stacking them together, we get an N -L IPU R: A processing p ofR are composition of processing p1, p2 ofR1, R2: p (i1, i2,.., iN) = p2 (i1, i2,.., iN) and we call it: R = R1 R. The parameter space of R is clearly RNM \u00d7 RML. We know that RNM is cut in some regions, processing is the same in every region. The same is true for RML. Thus, RNM + ML is cut in some regions, processing is the same in each region, and these regions are Cartesian products of regions in RNM and RML. So we know that the number of possible processing R is equal, R1 times R2 \u2212 R \u2212 R2. \u2212 1 \u00d7 M2M (R2M), R2M = 1.5M + R2M = R2M + (R2M + R2M)."}, {"heading": "Deep Learning and Learning Dynamics", "text": "rf\u00fc rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu"}, {"heading": "Disadvantages of Deep Learning", "text": "Finally, we would like to point out that deep learning may not be the best mechanical learning. We list some of its disadvantages below: 1. It affects huge parameter space, but the actual learning dynamics are based on a fixed set of regions (which corresponds to a set of X forms).This indirectness complicates all aspects of learning, in particular, it is almost impossible to know exactly what is happening in learning dynamics. 2. As Theorem 7 in Section 5 shows, successful learning needs sufficient data to support and bind it. This requirement is very costly. 3. The structure of learning is set up by humans. If the structure (how many layers, how large a layer is, how layers fit together, how folding, etc.) is not able to change, this means that learning is limited to a fixed group of regions, which corresponds to a fixed group of X forms."}], "references": [{"title": "Discussion on Mechanical Learning and Learning Machine, arxiv.org, 2016", "author": ["Chuy Xiong"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A theory of the learnable", "author": ["L. Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1984}, {"title": "A Description Logic Primer, arxiv.org", "author": ["Markus Krtzsch", "Frantiek Simanck", "Ian Horrocks"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "In [1], we introduced mechanical learning and proposed 2 approaches to mechanical learning.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In [1], we introduced mechanical learning and discussed some basic aspects of it.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "As we proposed in [1], there are naturally 2 ways to go: to directly realize one learning machine, or to well describe what mechanical learning is really doing.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "As in [1], for the same reason, here we will restrict to spatial learning, not consider it temporal learning.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "IPU \u2013 Information Processing Unit We have discussed mechanical learning in [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "This notation \u00ac is following [5].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "This notation \u00ac is following [3].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "This notation \u00b7 is following [3].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "In [3], there are similar form called conjunction normal form (CNF).", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In [1], we speculated possibilities to unite 5 different learning approaches.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "In [1], we introduced mechanical learning and proposed 2 approaches to mechanical learning. Here, we follow one such approach to well describe the objects and the processes of learning. We discuss 2 kinds of patterns: objective and subjective pattern. Subjective pattern is crucial for learning machine. We prove that for any objective pattern we can find a proper subjective pattern based upon least base patterns to express the objective pattern well. X-form is algebraic expression for subjective pattern. Collection of X-forms form internal representation space, which is center of learning machine. We discuss learning by teaching and without teaching. We define data sufficiency by X-form. We then discussed some learning strategies. We show, in each strategy, with sufficient data, and with certain capabilities, learning machine indeed can learn any pattern (universal learning machine). In appendix, with knowledge of learning machine, we try to view deep learning from a different angle, i.e. its internal representation space and its learning dynamics.", "creator": "LaTeX with hyperref package"}}}