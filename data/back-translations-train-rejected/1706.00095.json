{"id": "1706.00095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox to Speed up Deep Neural Network Training", "abstract": "Deep Neural Network (DNN) are currently of great inter- est in research and application. The training of these net- works is a compute intensive and time consuming task. To reduce training times to a bearable amount at reasonable cost we extend the popular Caffe toolbox for DNN with an efficient distributed memory communication pattern. To achieve good scalability we emphasize the overlap of computation and communication and prefer fine granu- lar synchronization patterns over global barriers. To im- plement these communication patterns we rely on the the Global address space Programming Interface version 2 (GPI-2) communication library. This interface provides a light-weight set of asynchronous one-sided communica- tion primitives supplemented by non-blocking fine gran- ular data synchronization mechanisms. Therefore, Caf- feGPI is the name of our parallel version of Caffe. First benchmarks demonstrate better scaling behavior com- pared with other extensions, e.g., the Intel TM Caffe. Even within a single symmetric multiprocessing machine with four graphics processing units, the CaffeGPI scales bet- ter than the standard Caffe toolbox. These first results demonstrate that the use of standard High Performance Computing (HPC) hardware is a valid cost saving ap- proach to train large DDNs. I/O is an other bottleneck to work with DDNs in a standard parallel HPC setting, which we will consider in more detail in a forthcoming paper.", "histories": [["v1", "Wed, 31 May 2017 21:20:16 GMT  (200kb,D)", "https://arxiv.org/abs/1706.00095v1", null], ["v2", "Fri, 18 Aug 2017 12:24:45 GMT  (201kb,D)", "http://arxiv.org/abs/1706.00095v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["martin kuehn", "janis keuper", "franz-josef pfreundt"], "accepted": false, "id": "1706.00095"}, "pdf": {"name": "1706.00095.pdf", "metadata": {"source": "CRF", "title": "Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox to Speed up Deep Neural Network Training", "authors": ["Martin Kuehn", "Janis Keuper"], "emails": [], "sections": [{"heading": null, "text": "Training these networks is a computationally intensive and time-consuming task. In order to reduce training times to a reasonable amount at a reasonable cost, we extend the popular Caffe toolbox for DNN with an efficient communication pattern with distributed memory. To achieve good scalability, we emphasize the overlap of computation and communication and prefer fine-grained synchronization patterns over global barriers. To implement these communication patterns, we rely on the \"Global address space programming interface\" version 2 (GPI-2) communication library. This interface offers a light set of asynchronous single-sided communication primitives supplemented by non-blocking fine-grained synchronization mechanisms for data. Therefore, CaffeGPI is called our parallel version of Caffe. Initial benchmarks show better scaling behavior compared to other extensions, e.g. IntelTMCaffe. Even within a single symmetric multi-process machine with four processing units, we show that the Caffe is the standard cost favorable."}, {"heading": "1 Introduction", "text": "Dre eeisrteeGsrteee\u00fccnlhsrcnlhsrteeoiiiiiiiietlrrrcnlhsrtee\u00fccsrrteeeeee\u00fccsrteeeeeeirsrrteeVnlrsrteeeeeeeeeeeeeeeeeeeeeeVrlrrrrrteeeeeeeeeerrsrrrsrrrrrrrlrrrrteeeeeeeeeeteerrrrrsrrrsrsrrrrrrrlrrteeeeeeeeeeeeeeteerllllluiuiuiuiuiuiueeeteeteecrrrrrrsrrrrrsrrsrsrrrsrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrteeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeoiuiuiuiuiuiuiueeeeeteeteeteeteecrrrrrrrrrrrrreeeeeeeeeeeteeteeteeteeteeteeteeteeteeteecrrrrrrrrrrrrrrrrrrrrrrrrrrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Parallelization Approach", "text": "Although numerical operations in the formation of DNNs are typically linear algebra operations on dense matrices with a relatively good FLOP byte ratio, the bandwidth of communication networks must be used efficiently to keep latency between training siterations low."}, {"heading": "2.1 Stochastic Gradient Descend", "text": "The Stochastic Gradient Descend (SGD) [6] algorithm is a standard for training DNN and is implemented in the Caffe toolbox. It is the default selection for such famous DNNs as GoogLeNet or AlexNet. The training data xi are divided into lots, which are applied iteratively to the DNN to modify the weights w, which are also applied to the model. Each iteration consists of a forward propagation and a backward propagation. In forward propagation, the data is derived, while the backward propagation calculates a gradient on the weights and is later applied to the modelling."}, {"heading": "2.2 Design Principles", "text": "To exploit the full potential of this parallelization approach, the data transfers of the model and gradients need to be designed very carefully, as the total amount of data to be transmitted during each iteration increases with the number of computing areas. Consequently, a typical HPC networking such as EDR InfiniBand should be used to provide sufficient bandwidth to take advantage of the scalability of the problem. These networks are commonly used to connect the nodes in current HPC clusters. Another important aspect is the efficient use of these networks, which primarily means overlaps in the computation and communication to prevent their runtimes from completely overlapping. Instead, both should ideally take place simultaneously, so that no additional time is needed for communication. Unfortunately, this is not the case for the standard caffe, which enters separate phases for computing and communication (see Figure 2).The second principle is to avoid global synchronization points between the series."}, {"heading": "2.3 Overlapping Computation and Communication", "text": "The next read access to the model of a particular layer occurs in the forward propagation of the following time step. Thus, there is quite a lot of time available, especially for the layers at the bottom of the DNN, to reduce the partial gradients and update the model of that layer (see Figure 4 for illustration).The communication pattern is rotation-based, one layer of DNN per revolution. At each turn, a partial gradient \u2206 w (l, k) r is calculated for that particular layer and forwarded to the receiving layers. Incoming partial gradients of previous layers from other layers are checked, aggregated, if available, and also forwarded to the ranks. Likewise, updates of the model of the previous layers are forwarded to the receiving layers. In all these cases, the data transfers are only triggered, but not waited for completion. In the finalization phase, all the loose schemes that are completed for the next layer are completed, so that the previous layer is immediately completed with the revision."}, {"heading": "3 Implementation", "text": "Our parallel version of the Caffe framework is as minimally invasive as possible. Essentially, the setup routine of the DDN and the backward propagation routines are modified. Additionally, a layer-by-layer model update is introduced in the solver class. As the backward propagation over the layers occurs, the calculations of the gradients are followed by calls to the newly added communication routines to reduce the local gradients and transmit the updated model."}, {"heading": "3.1 Basics of GPI-2", "text": "To implement the overlap, the GPI-2 library was used, a PGAS communication API for C / C + + and Fortran applications. Meeting the specification of the Global Address Space Programming Interface (GASPI) (see web page [9]), it provides truly asynchronous, one-sided communication primitives, complemented by a non-blocking, lightweight, fine-grained synchronization mechanism. GPI-2 uses connections that support Remote Direct Memory Access (RDMA), such as InfininTape networks. In these networks, data transfers can be almost entirely outsourced to the network infrastructure, minimizing the computing resource load. No intermediate copies are required that save memory bandwidth. Apart from that, GPI-2 is a very slim library and gives the user more direct control over the individual data transfers, such as the GPI-2 interface, which can match the usual measurement features of the GPI-2 (since the MPI) perfectly."}, {"heading": "3.2 Implementation of Data Transfers", "text": "In the caffe data structures that define the DNN, the arrays that carry the model and the gradient data are placed within the GPI-2 data segments. With the GPI-2 segment usage function, GPI-2 works perfectly with special memory areas in caffe that are assigned by cudaMallocHost to improve data transmission to the GPU. The GPI-2 library allows you to write remotely (between nodes) and directly to those segments. All data transmissions are triggered with a gaspi write that notifies the library. The recipient of the data piece checks the notification and acts on the received data if necessary. Gradient data is reduced in a reduction tree pattern that aggregates the final gradient to the master rank. The master rank performs the update of the current model to calculate an update for the next iteration to the other tree pattern."}, {"heading": "3.2.1 Reduction of the Gradient", "text": "In order to reduce the local slopes in a binomial, each rank checks its receive buffer for incoming slope data. If available, the slope data is reduced (supplemented) with its own slope data of this layer. If the rank has a receiver in the tree pattern, the decreased slope is forwarded to this receiver by sending a notification to Gaspi. Communication tasks are performed once in the loop over the layers, as shown in Section 2.3. The slope data is processed as they are available. No waiting for a specific data block takes place."}, {"heading": "3.2.2 Broadcast of the model", "text": "The transmission of the model is similar to the reduction of the gradient. Since no reduction steps are required, the incoming model data of the transmitter is simply forwarded to its receiver rows with a call for notification by Gaspi."}, {"heading": "4 First Benchmarks", "text": "In the CaffeGPI benchmark, 4 independent processes are started on the same node, one for each GPU that communicates via the NIC. The SMP machine is a single node of the Taurus cluster at the ZIH in Dresden, which contains 2 Intellect E5-2680v3 CPUs. 64 GB of access memory (RAM) and the 4 GPU are a single node of the Taurus cluster, which includes 2 Intellect E5-2680v3 CPUs."}, {"heading": "5 Conclusion and Future Work", "text": "The preliminary benchmarks presented in this paper show that our CaffeGPI-implemented distributed memory communication pattern scales well on four distributed memory nodes with one GPU per node. Overall performance is similar or even better than Caffe's standard SMP parallel approach on a 4 GPU SMP node. Even on this single 4 GPU SMP node, our CaffeGPI scales much better than the standard Caffe Framework.These results show that data scientists can rely on available HPC resources to train their DNNs within a reasonable timeframe.Our CaffeGPI toolbox can help meet the need for more computing power in data science without having to purchase large amounts of specialized hardware, which is difficult for other computer science tasks. \"We will continue to improve various hardware configurations, such as benchmarking 22NIB1 to improve a DIX communication sample, or a DIM benchmark."}, {"heading": "Acknowledgment", "text": "The authors thank the Center for Information Services and High Performance Computing (ZIH) at TU Dresden for generously allocating computer time. We are also grateful for the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research."}], "references": [{"title": "Imagenet classification with deep convolutional neural  networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010. Springer, 2010, pp. 177\u2013186.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed training of deep neural networks: Theoretical and practical limits of parallel scalability", "author": ["J. Keuper", "F.-J. Preundt"], "venue": "Proceedings of the Workshop on Machine Learning in High Performance Computing Environments, ser. MLHPC \u201916. Piscataway, NJ, USA: IEEE Press, 2016, pp. 19\u201326. [Online]. Available: https://doi.org/10.1109/MLHPC.2016.6", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Asynchronous parallel stochastic gradient descent: A numeric core for scalable distributed machine learning algorithms", "author": ["J. Keuper", "F.-J. Pfreundt"], "venue": "Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments, ser. MLHPC \u201915. New York, NY, USA: ACM, 2015, pp. 1:1\u20131:11. [Online]. Available: http://doi.acm.org/10.1145/2834892.2834893", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "AlexNet [2]: time per iteration 2s 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "6s time till convergence 250h 112h 13h 75h GoogLeNet [3]: time per iteration 1.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "The toolbox Caffe [4] is very popular to build and train DNNs.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "The Stochastic Gradient Descend (SGD) [6] algorithm is a standard for training DNN and is implemented in the Caffe toolbox.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "The two basic parallelization strategies commonly used for SGD are data parallelism or model parallelism [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "dencies immanent of the SGD algorithm unlike the so called \u201dasynchronous SGD\u201d algorithms described in literature [8].", "startOffset": 113, "endOffset": 116}], "year": 2017, "abstractText": "Deep Neural Network (DNN) are currently of great interest in research and application. The training of these networks is a compute intensive and time consuming task. To reduce training times to a bearable amount at reasonable cost we extend the popular Caffe toolbox for DNN with an efficient distributed memory communication pattern. To achieve good scalability we emphasize the overlap of computation and communication and prefer fine granular synchronization patterns over global barriers. To implement these communication patterns we rely on the the \u201dGlobal address space Programming Interface\u201d version 2 (GPI-2) communication library. This interface provides a light-weight set of asynchronous one-sided communication primitives supplemented by non-blocking fine granular data synchronization mechanisms. Therefore, CaffeGPI is the name of our parallel version of Caffe. First benchmarks demonstrate better scaling behavior compared with other extensions, e.g., the IntelTMCaffe. Even within a single symmetric multiprocessing machine with four graphics processing units, the CaffeGPI scales better than the standard Caffe toolbox. These first results demonstrate that the use of standard High Performance Computing (HPC) hardware is a valid cost saving approach to train large DDNs. I/O is an other bottleneck to work with DDNs in a standard parallel HPC setting, which we will consider in more detail in a forthcoming paper.", "creator": "LaTeX with hyperref package"}}}