{"id": "1706.00134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Semantic Refinement GRU-based Neural Language Generation for Spoken Dialogue Systems", "abstract": "Natural language generation (NLG) plays a critical role in spoken dialogue systems. This paper presents a new approach to NLG by using recurrent neural networks (RNN), in which a gating mechanism is applied before RNN computation. This allows the proposed model to generate appropriate sentences. The RNN-based generator can be learned from unaligned data by jointly training sentence planning and surface realization to produce natural language responses. The model was extensively evaluated on four different NLG domains. The results show that the proposed generator achieved better performance on all the NLG domains compared to previous generators.", "histories": [["v1", "Thu, 1 Jun 2017 00:37:46 GMT  (450kb)", "http://arxiv.org/abs/1706.00134v1", "has been accepted to appear at PACLING 2017"], ["v2", "Sat, 3 Jun 2017 00:45:11 GMT  (436kb)", "http://arxiv.org/abs/1706.00134v2", "To be appear at PACLING 2017"], ["v3", "Thu, 6 Jul 2017 15:32:59 GMT  (83kb)", "http://arxiv.org/abs/1706.00134v3", "To be appear at PACLING 2017"], ["v4", "Tue, 11 Jul 2017 14:40:56 GMT  (83kb)", "http://arxiv.org/abs/1706.00134v4", "To be appear at PACLING 2017"]], "COMMENTS": "has been accepted to appear at PACLING 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["van-khanh tran", "le-minh nguyen"], "accepted": false, "id": "1706.00134"}, "pdf": {"name": "1706.00134.pdf", "metadata": {"source": "CRF", "title": "Semantic Refinement GRU-based Neural Language Generation for Spoken Dialogue Systems", "authors": ["Van-Khanh Tran", "Le-Minh Nguyen"], "emails": ["nguyenml}@jaist.ac.jp", "tvkhanh@ictu.edu.vn"], "sections": [{"heading": null, "text": "In this context, it should be noted that the measures in question are measures primarily aimed at combating climate change, and that they are primarily aimed at combating global warming."}, {"heading": "II. RELATED WORK", "text": "The conventional approaches of the NLG traditionally divide the task into two subtasks: sentence planning and surface realization. The sentence planning deals with the mapping of the semantic symbols of the input to a linguistic structure, e.g. a tree-like or template structure. The surface realization then converts the structure into an appropriate sentence. [3] Despite their success and their widespread application in solving NLG problems, these traditional methods still rely on the handmade rule-based generators or anchors. [2] suggested a class-based n-grammar-language generator that can learn to generate the sentences for a given DA and then select the best sentences using a rule-based technical anchor. [10] addressed some of the limitations of the class-based LMs by suggesting a method based on a syntactic dependency tree (LM). A phrase-based generator based on factor-based Lain [was introduced] in a multi-grain [ST11] -oriented STM."}, {"heading": "III. RECURRENT NEURAL LANGUAGE GENERATOR", "text": "The recurring speech generator proposed in this essay is based on an RNN language model [16], which consists of three layers: an input layer, a hidden layer, and an output layer. Input to the network at each step t is a 1 - hot encoding with a token 2 wt conditioned on a recurring hidden layer ht. The output layer yt represents the probability distribution of the next given token wt and hidden ht. From this conditional distribution, we can extract the next token in a generated string and transmit it as the next input to the generator. This process ends when a stop sign is generated [12] or a limitation is reached [14]. The network can generate a sequence of tokens that can be lexicalized to obtain the next token in a generated string and transmit it as the next input to the generator. Furthermore, to ensure that DA intends to return the intuition to a given DA before the expression of the intuition is given."}, {"heading": "A. SRGRU-BASE", "text": "In this model, instead of an input character wt in the RNN model, at each time step t, the input cursor is filtered through a semantic gate, which is calculated as follows: dt = \u03c3 (Wdzz) xt = dt-wt (1) where: Wdz is a trained matrix to embed the given DA representation in the word, and xt is2Input texts are delicalized, in which slot values are tokens.3The process in which slot tokens.3The slot tokens are replaced by its value.new input. Here, Wdz plays a role in sentence planning, as it can directly capture which DA characteristics are useful during generation to encode the input information."}, {"heading": "B. SRGRU-CONTEXT", "text": "To import context information into the gating mechanism, Equation 1 is modified as follows: dt = \u03c3 (Wdzz + Wdhht \u2212 1) xt = dt-wt (7), where: Wdz and Wdh are weight matrices. Wdh behaves like a keyword detector that learns to detect the pattern of generation tokens or the relationship between multiple tokens. In other words, the new input text consists of information from the original input token wt, the dialog act wt \u2212 1. dt is called the refinement gate, because the input tokens are refined by a combination of gating information from the dialog act and the previous hidden state Wdxt."}, {"heading": "C. Training", "text": "The objective function was the negative log probability and was calculated by: F (\u03b8) = \u2212 T \u2211 t = 1y t log pt (11), where yt is the word distribution of the basic concept of truth, pt is the predicted word distribution, T is the length of the input sequence. Generators were trained by treating each sentence as a mini-batch. L2 regularization was added to the objective function for all 10 training examples. Models were initialized with pre-trained word vectors [20] and optimized by stochastic gradient descending and backpropagation over time. To avoid overmatching, early stopping was implemented with a validation set, as proposed in [16]."}, {"heading": "D. Decoding", "text": "The decoding phase consists of two tasks: (i) over-generation and (ii) re-ranking. In the over-generation phase, the forward generator based on the given DA calculates the cost of the forward generator Ffw (\u03b8) so that the re-ranking score R is calculated as follows: R = Ffw (\u03b8) + \u03bbERR (12), which is a trade-off constant that is set to a large value to severely punish nonsensical outputs. ERR, i.e. the number of generated slots that are either redundant or missing, is calculated by: ERR = p + qN (13), where N is the total number of slots in DA, and p, q the number of missing or redundant slots."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "We conducted experiments with four different NLG domains: restaurant search, hotel search, laptop purchase, and television purchase. Restaurant and hotel domains were collected in [7], which include system dialogs, shared slots, and specific domain slots. Laptop and TV data sets were published in [21] with about 13K different DAs on the laptop and 7K different DAs on the TV. These two datasets have a much larger input area, but only one training example for each DA, so the system can partially learn concepts and recombine them and apply them to invisible DAs. Also, the number of dialog types and slots of data sets is larger than in restaurant and hotel data sets. As a result, the NLG tasks for laptop and TV data sets become much more difficult."}, {"heading": "B. Experimental Setups", "text": "The generators were implemented and trained using the TensorFlow library [22] by integrating each of the data sets into the training, validation and testing at a ratio of 3: 1: 1. The hidden layer size should be 80, and the generators were trained at a 70% dropout rate. We perform 5 runs with different random initialization of the network and the training is completed by stopping the anchor early, as in Section III-C. We select a model that provides the highest BLEU score on the validation set. The decoding process used beam search with a beam width of 10. We rely on 1000 to strongly discourage the anchor from selecting expressions containing either redundant or missing slots. For each DA, we have generated 20 candidate expressions and selected the top 5 realizations after the rerking. As the proposed models work correctly, except for the previous generation, all of the results of the GRI were randomly reported in the GRI, as well."}, {"heading": "C. Evaluation Metrics and Baselines", "text": "Note that the slot error rate ERR was calculated as an auxiliary value next to the BLEU score and was calculated by averaging slot errors across each of the top 5 findings in the entire corpus. Both metrics were derived from code taken from an open source benchmark toolkit for Natural Language Generation3.We compared our proposed models with the general GRU (GRU base) and three strong baselines from the NLG toolkit: \u2022 ENCDEC proposed in [23], which applies the attention mechanism to an RNN encoder decoder. \u2022 HLSTM proposed in [6], which uses a heuristic gate toolkit to ensure that all attribute value information is accurately captured during generation. \u2022 SCLSTM proposed in [7], which can learn the gaming signal and the language model together."}, {"heading": "V. RESULTS AND ANALYSIS", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "The proposed models can learn from the unaligned data to generate natural speech reactions based on the given DA. We evaluated our model on four NLG datasets and compared it with the state-of-the-art generators. Results show that the proposed models perform better on all four NLG domains than the existing generators in terms of BLEU and ERR metrics. In the future, we plan to further investigate the multi-domain gating mechanism for NLG, as the refinement gate shows that it is able to handle the invisible slot value pairs."}], "references": [{"title": "Method and apparatus for building an intelligent automated assistant", "author": ["A. Cheyer", "D. Guzzoni"], "venue": "Mar. 18 2014, uS Patent 8,677,377.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["A.H. Oh", "A.I. Rudnicky"], "venue": "Proc. NAACL. ACL, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Trainable sentence planning for complex information presentation in spoken dialog systems", "author": ["A. Stent", "R. Prasad", "M. Walker"], "venue": "Proc. ACL. ACL, 2004, p. 79.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimising information presentation for spoken dialogue systems", "author": ["V. Rieser", "O. Lemon", "X. Liu"], "venue": "Proc. ACL. ACL, 2010, pp. 1009\u20131018.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings", "author": ["O. Du\u0161ek", "F. Jur\u010d\u0131\u0301\u010dek"], "venue": "arXiv preprint arXiv:1606.05491, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking", "author": ["T.-H. Wen", "M. Ga\u0161i\u0107", "D. Kim", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "Proc. SIGDIAL. ACL, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Ga\u0161i\u0107", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "Proc. EMNLP. ACL, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["T.-H. Wen", "D. Vandyke", "N. Mrksic", "M. Gasic", "L.M. Rojas- Barahona", "P.-H. Su", "S. Ultes", "S. Young"], "venue": "arXiv preprint arXiv:1604.04562, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proc. ACL. ACL, 2002, pp. 311\u2013318.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Trainable methods for surface natural language generation", "author": ["A. Ratnaparkhi"], "venue": "Proc. NAACL. ACL, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Stochastic language generation in dialogue using factored language models", "author": ["F. Mairesse", "S. Young"], "venue": "CL, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proc. CVPR, 2015, pp. 3128\u20133137.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Chinese poetry generation with recurrent neural networks.", "author": ["X. Zhang", "M. Lapata"], "venue": "in EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Incorporating unstructured textual knowledge sources into neural dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "L. Charlin", "J. Pineau"], "venue": "NIPS Workshop MLNLU, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Inner attention based recurrent neural networks for answer selection", "author": ["B. Wang", "K. Liu", "J. Zhao"], "venue": "2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Multi-domain neural network language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "L.M. Rojas-Barahona", "P.- H. Su", "D. Vandyke", "S. Young"], "venue": "arXiv preprint arXiv:1603.01232, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Toward multi-domain language generation using recurrent neural networks", "author": ["T.-H. Wen", "M. Ga\u0161ic", "N. Mrk\u0161ic", "L.M. Rojas-Barahona", "P.- H. Su", "D. Vandyke", "S. Young"], "venue": "2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "There are some common and widely approaches to solve NLG problems, including rule-based [1], corpus-based n-gram models [2], and a trainable generator [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "There are some common and widely approaches to solve NLG problems, including rule-based [1], corpus-based n-gram models [2], and a trainable generator [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "There are some common and widely approaches to solve NLG problems, including rule-based [1], corpus-based n-gram models [2], and a trainable generator [3].", "startOffset": 151, "endOffset": 154}, {"referenceID": 3, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "RNNbased models have been used for NLG as a joint training model [6], [7] and an end-to-end training model [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "RNNbased models have been used for NLG as a joint training model [6], [7] and an end-to-end training model [8].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "RNNbased models have been used for NLG as a joint training model [6], [7] and an end-to-end training model [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "inform(name=\u2018Frances\u2019; area=\u2018City Center\u2019) which cannot be directly delexicalized [6], or to generalize to unseen domains [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "inform(name=\u2018Frances\u2019; area=\u2018City Center\u2019) which cannot be directly delexicalized [6], or to generalize to unseen domains [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "The results showed that our proposed method outperforms the previous state-of-the-art methods in terms of BLEU [9] and slot error rate ERR [7] scores.", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "The results showed that our proposed method outperforms the previous state-of-the-art methods in terms of BLEU [9] and slot error rate ERR [7] scores.", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "The surface realization then converts the structure into an appropriate sentence [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "[2] proposed a class-based n-gram language model (LM) generator which can learn to generate the sentences for a given DA and then select the best sentences using a rule-based reranker.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] addressed some of the limitation of the class-based LMs by proposing a method based on a syntactic dependency tree.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "based generator based on factored LMs was introduced in [11], which can learn from a semantically aligned corpus.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "[12], [13] used RNNs in a multi-modal setting to generate captions for images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12], [13] used RNNs in a multi-modal setting to generate captions for images.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "[14] also proposed a generator using RNNs to create Chinese poetry.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] encoded an unstructured textual knowledge source along with previous responses and context to produce a response for technical support queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "For task-oriented dialogue systems, [6] combined a forward RNN generator, a CNN reranker, and a backward RNN reranker to generate utterances.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "A semantically conditioned-based Long ShortTerm Memory (LSTM) generator was proposed in [7], which introduced a control \u201creading\u201d gate to the LSTM cell and can learn the gating mechanism and language model jointly.", "startOffset": 88, "endOffset": 91}, {"referenceID": 15, "context": "The recurrent language generator proposed in this paper is based on a RNN language model [16], which consists of three layers: an input layer, a hidden layer and an output layer.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "This process finishes when a stop sign is generated [12], or some constraint is reached [14].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "This process finishes when a stop sign is generated [12], or some constraint is reached [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "Inspired by work in [17], we propose an intuition: Gating before computation, in which we add gating mechanism before the RNN computation to semantically refine the input tokens.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "In this study, we use GRU, which was recently proposed in [18], instead of LSTM as building computational block for RNN, which is formulated as follows:", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "By taking advantage of gating mechanism from the LSTM model [19] in which the gating mechanism is employed to solve the gradient vanishing and exploding problem, we propose to apply the refinement gate deeper into the GRU cell.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "The models were initialized with pre-trained word vectors [20] and optimized by using stochastic gradient descent and back propagation through time.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "To prevent over-fitting, early stopping was implemented using a validation set as suggested in [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "The Restaurant and Hotel domains were collected in [7] which contain system dialogue acts, shared slots, and specific domain slots.", "startOffset": 51, "endOffset": 54}, {"referenceID": 20, "context": "The Laptop and TV datasets have been released in [21] with about 13K distinct DAs in the Laptop and 7K distinct DAs in the TV.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "The generators were implemented using the TensorFlow library [22] and trained by partitioning each of the datasets into training, validation and testing set in the ratio 3:1:1.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "\u266e reported in [7].", "startOffset": 14, "endOffset": 17}, {"referenceID": 22, "context": "\u2022 ENCDEC proposed in [23] which applies the attention mechanism to an RNN encoder-decoder.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "\u2022 HLSTM proposed in [6] which uses a heuristic gate to ensure that all of the attribute-value information was accurately captured when generating.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "\u2022 SCLSTM proposed in [7] which can learn the gating signal and language model jointly.", "startOffset": 21, "endOffset": 24}], "year": 2017, "abstractText": "Natural language generation (NLG) plays a critical role in spoken dialogue systems. This paper presents a new approach to NLG by using recurrent neural networks (RNN), in which a gating mechanism is applied before RNN computation. This allows the proposed model to generate appropriate sentences. The RNN-based generator can be learned from unaligned data by jointly training sentence planning and surface realization to produce natural language responses. The model was extensively evaluated on four different NLG domains. The results show that the proposed generator achieved better performance on all the NLG domains compared to previous generators.", "creator": "LaTeX with hyperref package"}}}