{"id": "1706.00139", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks", "abstract": "Natural language generation (NLG) is a critical component in a spoken dialogue system. This paper presents a Recurrent Neural Network based Encoder-Decoder architecture, in which an LSTM-based decoder is introduced to select, aggregate semantic elements produced by an attention mechanism over the input elements, and to produce the required utterances. The proposed generator can be jointly trained both sentence planning and surface realization to produce natural language sentences. The proposed model was extensively evaluated on four different NLG datasets. The experimental results showed that the proposed generators not only consistently outperform the previous methods across all the NLG domains but also show an ability to generalize from a new, unseen domain and learn from multi-domain datasets.", "histories": [["v1", "Thu, 1 Jun 2017 01:06:17 GMT  (222kb)", "https://arxiv.org/abs/1706.00139v1", "has been accepted to appear at CoNLL 2017"], ["v2", "Tue, 20 Jun 2017 05:54:56 GMT  (222kb)", "http://arxiv.org/abs/1706.00139v2", "has been accepted to appear at CoNLL 2017"], ["v3", "Sat, 12 Aug 2017 15:41:14 GMT  (1164kb)", "http://arxiv.org/abs/1706.00139v3", "has been accepted to appear at CoNLL 2017. arXiv admin note: text overlap witharXiv:1706.06714"]], "COMMENTS": "has been accepted to appear at CoNLL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["van-khanh tran", "le-minh nguyen"], "accepted": false, "id": "1706.00139"}, "pdf": {"name": "1706.00139.pdf", "metadata": {"source": "CRF", "title": "Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks", "authors": ["Van-Khanh Tran"], "emails": ["nguyenml}@jaist.ac.jp", "tvkhanh@ictu.edu.vn"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.00 139v 3 [cs.C L] 12 August 201 7Critical component in a spoken dialog system. This paper presents an encoder-decoder architecture based on a recursive neural network, in which an LSTM-based decoder is introduced to select semantic elements generated by an attention mechanism via the input elements and produce the necessary expressions. The proposed generator can be trained in both sentence planning and surface realization to generate natural language sets. The proposed model was comprehensively evaluated on four different NLG datasets. Experimental results showed that the proposed generators not only consistently outperform the previous methods in all NLG domains, but also demonstrate the ability to generate from a new, invisible domain and to learn from multi-domain datasets."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Related Work", "text": "Zhang and Lapata (2014) have proposed a generator that uses RNNs to generate Chinese poetry. Xu et al. (2015); Karpathy and Fei-Fei (2015); Vinyals et al. (2015) have also used RNNNs in a multimodal environment to solve caption tasks. RNN-based sequence-to-sequence models have been applied to solve a variety of tasks: conversation modeling (??), machine translation (?) For task-oriented dialogue systems, Wen et al. (2015a) combines a forward RNN generator, a CNN RNN anchor and a rearward-facing RNN anchor to generate utterance. Wen et al al al al al al al al al al al al al al al al al. (2015b) proposed SC-LSTM generator that introduced a control gate to the LSTM cell to jointly learn the mechanism and language."}, {"heading": "3 Recurrent Neural Language Generator", "text": "The recurring language generator proposed in this paper is based on a neural language generator (Wen et al., 2016b), which consists of three main components: (i) an encoder that includes the target meaning representation (MR) as model input, (ii) an aligner that aligns and controls the semantic elements, and (iii) an RNN decoder that generates output sets. At each step, the generator architecture is shown in Figure 1. The encoder first encodes the MR into semantic elements, which are then aggregated and selected using an attention-based mechanism by the aligner. At each step, the input to the RNN decoder is a 1-hot coding of a token 2 weight and an attentive DA representation German. At each time step, the RNN decoder is also calculated how much the feature value vector 1 \u2212 Nex value can be added to the next steps of the calculation and we will add to the next sentence, this 1."}, {"heading": "3.1 Encoder", "text": "The slots and values are separate parameters used on the encoder side. It embeds the source information in a vector representation zi, which represents a concatenation of the vector representation of each slot-value pair, and is calculated by: zi = ui-vi (1), where ui, vi the i-th slot and the i-th value are the vectors and the i-th value the vector concatenation. i-index runs over the L given slot-value pairs. In this thesis, we use a 1-layer, bidirectional LSTM (Bi-LSTM) to encode the sequence of the slot value pairs 4. Bi-LSTM consists of forward and backward LSTMs that read the sequence of the slot-value pairs from left to right and right to left to generate the forward and backward sequence of the hidden states (eLinks \u2192 eei, and \u2212 Eei, \u2192 1, \u2192 \u2212 Ei, and \u2212 Pair)."}, {"heading": "3.2 Aligner", "text": "The aligner uses an attention mechanism to calculate the DA representation as follows: \u03b2t, i = exp et, i \u2211 j exp et, j (3) whereet, i = a (ei, ht \u2212 1) (4) and \u03b2t, i is the weight of the i-th slot-value pair calculated by the attention mechanism, the alignment model a is calculated by: a (ei, ht \u2212 1) = v'a tanh (Waei + Uaht \u2212 1) (5), where va, Wa, Ua are the weight matrices to be learned. Finally, the aligner calculates the dialogue act for embedding dt as follows: dt = a-a-i \u03b2t, iei (6), where a is a vector embedding of the action type."}, {"heading": "3.3 RALSTM Decoder", "text": "The proposed semantic RALSTM cell applied to the decoder page consists of three components: a refinement cell that rests on another cell, and a cell that rests on another cell. (Wrddt + Wrhht \u2212 1) Wrh, where Wrd and Wrh are found in the RNN cell, is a part of the semantic cell that not only learns the vector similarity, but also receives information about the two vectors. Wrh acts like a key phrase that learns to capture the pattern of the generation tokens or the relationship between multiple tokens. In other words, the new input cell wt consists of information about the original input cell that represents the hidden context, and the hidden contexts in which we capture the pattern of the generation tokens or the relationship between multiple tokens."}, {"heading": "3.4 Training", "text": "The objective function was the negative log probability and was calculated by: F (\u03b8) = \u2212 T \u2211 t = 1y t log pt (14), where: yt is the distribution of the ground truth token, pt is the predicted token distribution, T is the length of the input set. The proposed generators were trained by treating each set as a mini-batch with an L2 regulation added to the objective function for all 5 training examples. Models were initialized with a pre-formed glove word embedding vector (Pennington et al., 2014) and optimized over time by stochastic gradient descending and backpropagation (Werbos, 1990). The early stop mechanism was implemented to prevent overmatching by using a validation set as proposed in (Mikolov, 2010)."}, {"heading": "3.5 Decoding", "text": "The decoding consists of two phases: (i) over-generation and (ii) recovery. During over-generation, the generator, which is based on both representations of the given DA, calculates a series of candidate responses by means of a beam search. In the recovery phase, the cost of the generator is calculated such that the recovery value R looks like this: R = F (\u03b8) + \u03bbERR (15), where \u03bb is a trade-off value and is set on a large value to severely punish nonsensical outputs. ERR, which consists of the number of either missing or redundant generated slots, is calculated by: ERR = p + qN (16), where N is the total number of slots in DA, and p, q the number of missing or redundant slots."}, {"heading": "4 Experiments", "text": "We conducted a series of experiments to evaluate the effectiveness of the proposed models using multiple metrics, datasets and model architectures to compare them with previous methods."}, {"heading": "4.1 Datasets", "text": "We examined the proposed models across four different NLG domains: restaurant search, hotel search, laptop purchase, and television purchase. Restaurant and hotel were recorded (Wen et al., 2015b), while laptop and TV data sets were published by (Wen et al., 2016a) with much larger input space but only one training example per DA, so the system must partially implement concepts and be able to recombine and apply them to invisible DAs, making NLG tasks for laptop and TV domains considerably more difficult. Data set statistics are presented in Table 1."}, {"heading": "4.2 Experimental Setups", "text": "The generators were implemented using the TensorFlow library (Abadi et al., 2016) and trained with training, validation and test ratios in a ratio of 3: 1: 1. The hidden layer size, beam size were set to 80 and 10, respectively, and the generators were trained with a dropout rate of 70%. We performed 5 runs with different random initialization of the network and the training is terminated by early stoppage. We then chose a model that yields the highest BLEU value on the validation amount shown in Table 2. Since the trained models may differ depending on the initialization, we also report the results averaged on 5 randomly initialized networks. Note that, with the exception of the results reported in Table 2, all the results shown were averaged over 5 randomly initialized networks. We set the value to 1000 to strongly prevent the rearanchor from selecting statements that either contained redundant or missing slots. Note that for each of the results reported in Table 2, all the results were averaged over 5 randomly initialized networks. We set the value to 1000 to strongly prevent the rearanchor from selecting statements that contained either redundant or missing slots."}, {"heading": "4.3 Evaluation Metrics and Baselines", "text": "We compared the proposed models with three strong baselines recently released as state-of-the-art NLG benchmarks5. \u2022 HLSTM proposed by Wen et al. (2015a), which used a heuristic gate to ensure that all slot value information was accurately captured during generation. \u2022 SCLSTM proposed by Wen et al. (2015b), which can jointly learn the gating signal and the language model. \u2022 Enc-Dec proposed by Wen et al. (2016b), which used the attention-based encoder decoder architecture."}, {"heading": "5 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Results", "text": "In fact, the number of people living in the US is significantly higher than in other countries."}, {"heading": "6 Conclusion and Future Work", "text": "We present an extension of the ARED model, in which a RALSTM component is introduced to select, aggregate and generate the required set of semantic elements produced by the encoder. We evaluated the proposed models on four NLG domains and compared them with the modern generators. The proposed models show empirically a consistent improvement over previous methods in both the BLEU and ERR evaluation metrics. The proposed models also show the ability to expand into a new, invisible area, regardless of how much training data has been fed into that area. In the future, it would be interesting to apply the proposed model to other tasks that can be modelled on the basis of the encoder decoder architecture, i.e. captions, reading comprehension and machine translation."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Distributed dialogue policies for multi-domain statistical dialogue management", "author": ["Milica Ga\u0161i\u0107", "Dongho Kim", "Pirros Tsiakoulis", "Steve Young."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on.", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? 2015", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation .", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep visualsemantic alignments for generating image", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": null, "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Abstractive text summa", "author": ["Bing Xiang"], "venue": null, "citeRegEx": "Xiang,? \\Q2016\\E", "shortCiteRegEx": "Xiang", "year": 2016}, {"title": "Bleu: a method for automatic", "author": ["Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}, {"title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Dongho Kim", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "Proceedings", "citeRegEx": "Wen et al\\.,? 2015a", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Multi-domain neural network language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1603.01232 .", "citeRegEx": "Wen et al\\.,? 2016a", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Toward multidomain language generation using recurrent neural networks", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161ic", "Nikola Mrk\u0161ic", "Lina M Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Proceedings of EMNLP. Association for Computa-", "citeRegEx": "Wen et al\\.,? 2015b", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A networkbased end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksic", "Milica Gasic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562 .", "citeRegEx": "Wen et al\\.,? 2016c", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos."], "venue": "Proceedings of the IEEE 78(10):1550\u20131560.", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Multi-domain learning and generalization in dialog state tracking", "author": ["Jason Williams."], "venue": "Proceedings of SIGDIAL. Citeseer, volume 62.", "citeRegEx": "Williams.,? 2013", "shortCiteRegEx": "Williams.", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML. volume 14, pages 77\u201381.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Review networks for caption generation", "author": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "William W Cohen", "Ruslan R Salakhutdinov."], "venue": "Advances in Neural Information Processing Systems. pages 2361\u20132369.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Xingxing Zhang", "Mirella Lapata."], "venue": "EMNLP. pages 670\u2013680.", "citeRegEx": "Zhang and Lapata.,? 2014", "shortCiteRegEx": "Zhang and Lapata.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": ", 2015a,b) and an end-to-end training model (Wen et al., 2016c).", "startOffset": 44, "endOffset": 63}, {"referenceID": 1, "context": "The Attentional RNN Encoder-Decoder (Bahdanau et al., 2014) (ARED) based approaches have also shown improved performance on a variety of tasks, e.", "startOffset": 36, "endOffset": 59}, {"referenceID": 14, "context": ", image captioning (Xu et al., 2015; Yang et al., 2016), text summarization (Rush et al.", "startOffset": 19, "endOffset": 55}, {"referenceID": 15, "context": ", image captioning (Xu et al., 2015; Yang et al., 2016), text summarization (Rush et al.", "startOffset": 19, "endOffset": 55}, {"referenceID": 6, "context": "The RNNbased models have been applied for NLG as a joint training model (Wen et al., 2015a,b) and an end-to-end training model (Wen et al., 2016c). A recurring problem in such systems is requiring annotated datasets for particular dialogue acts (DAs). To ensure that the generated utterance representing the intended meaning of the given DA, the previous RNN-based models were further conditioned on a 1-hot vector representation of the DA. Wen et al. (2015a) introduced a heuristic gate to ensure that all the slot-value pair was accurately captured during generation.", "startOffset": 73, "endOffset": 460}, {"referenceID": 6, "context": "The RNNbased models have been applied for NLG as a joint training model (Wen et al., 2015a,b) and an end-to-end training model (Wen et al., 2016c). A recurring problem in such systems is requiring annotated datasets for particular dialogue acts (DAs). To ensure that the generated utterance representing the intended meaning of the given DA, the previous RNN-based models were further conditioned on a 1-hot vector representation of the DA. Wen et al. (2015a) introduced a heuristic gate to ensure that all the slot-value pair was accurately captured during generation. Wen et al. (2015b) subsequently proposed a Semantically Conditioned Long Short-term Memory generator (SC-LSTM) which jointly learned the DA gating signal and language model.", "startOffset": 73, "endOffset": 589}, {"referenceID": 10, "context": ", 2002) and slot error rate ERR scores (Wen et al., 2015b).", "startOffset": 39, "endOffset": 58}, {"referenceID": 9, "context": "Zhang and Lapata (2014) proposed a generator using RNNs to create Chinese poetry.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "Xu et al. (2015); Karpathy and Fei-Fei (2015); Vinyals et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 4, "context": "(2015); Karpathy and Fei-Fei (2015); Vinyals et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 4, "context": "(2015); Karpathy and Fei-Fei (2015); Vinyals et al. (2015) also used RNNs in a multimodal setting to solve image captioning tasks.", "startOffset": 8, "endOffset": 59}, {"referenceID": 4, "context": "(2015); Karpathy and Fei-Fei (2015); Vinyals et al. (2015) also used RNNs in a multimodal setting to solve image captioning tasks. The RNN-based Sequence to Sequence models have applied to solve variety of tasks: conversational modeling (???), machine translation (??) For task-oriented dialogue systems, Wen et al. (2015a) combined a forward RNN generator, a CNN reranker, and a backward RNN reranker to generate utterances.", "startOffset": 8, "endOffset": 324}, {"referenceID": 4, "context": "(2015); Karpathy and Fei-Fei (2015); Vinyals et al. (2015) also used RNNs in a multimodal setting to solve image captioning tasks. The RNN-based Sequence to Sequence models have applied to solve variety of tasks: conversational modeling (???), machine translation (??) For task-oriented dialogue systems, Wen et al. (2015a) combined a forward RNN generator, a CNN reranker, and a backward RNN reranker to generate utterances. Wen et al. (2015b) proposed SC-LSTM generator which introduced a control sigmoid gate to the LSTM cell to jointly learn the gating mechanism and language model.", "startOffset": 8, "endOffset": 445}, {"referenceID": 4, "context": "(2015); Karpathy and Fei-Fei (2015); Vinyals et al. (2015) also used RNNs in a multimodal setting to solve image captioning tasks. The RNN-based Sequence to Sequence models have applied to solve variety of tasks: conversational modeling (???), machine translation (??) For task-oriented dialogue systems, Wen et al. (2015a) combined a forward RNN generator, a CNN reranker, and a backward RNN reranker to generate utterances. Wen et al. (2015b) proposed SC-LSTM generator which introduced a control sigmoid gate to the LSTM cell to jointly learn the gating mechanism and language model. A recurring problem in such systems is the lack of sufficient domain-specific annotated data. Wen et al. (2016a) proposed an out-of-domain model which was trained on counterfeited data by using semantically similar slots from the target domain instead of the slots belonging to the out-of-domain dataset.", "startOffset": 8, "endOffset": 700}, {"referenceID": 1, "context": "More recently, RNN encoder-decoder based models with attention mechanism (Bahdanau et al., 2014) have shown improved performances in various tasks.", "startOffset": 73, "endOffset": 96}, {"referenceID": 1, "context": "More recently, RNN encoder-decoder based models with attention mechanism (Bahdanau et al., 2014) have shown improved performances in various tasks. Yang et al. (2016) proposed a review network to the image captioning, which reviews all the information encoded by the encoder and produces a compact thought vector.", "startOffset": 74, "endOffset": 167}, {"referenceID": 1, "context": "More recently, RNN encoder-decoder based models with attention mechanism (Bahdanau et al., 2014) have shown improved performances in various tasks. Yang et al. (2016) proposed a review network to the image captioning, which reviews all the information encoded by the encoder and produces a compact thought vector. Mei et al. (2015) proposed RNN encoderdecoder-based model by using two attention layers to jointly train content selection and surface realization.", "startOffset": 74, "endOffset": 332}, {"referenceID": 1, "context": "More recently, RNN encoder-decoder based models with attention mechanism (Bahdanau et al., 2014) have shown improved performances in various tasks. Yang et al. (2016) proposed a review network to the image captioning, which reviews all the information encoded by the encoder and produces a compact thought vector. Mei et al. (2015) proposed RNN encoderdecoder-based model by using two attention layers to jointly train content selection and surface realization. More close to our work, Wen et al. (2016b) proposed an attentive encoder-decoder based generator which computed the attention mechanism over the slot-value pairs.", "startOffset": 74, "endOffset": 505}, {"referenceID": 8, "context": ", 2015) using RNN-based networks for multi-domain dialogue state tracking, (Wen et al., 2016a) using a procedure to train multi-domain via multiple adaptation steps, or (Ga\u0161i\u0107 et al.", "startOffset": 75, "endOffset": 94}, {"referenceID": 2, "context": ", 2016a) using a procedure to train multi-domain via multiple adaptation steps, or (Ga\u0161i\u0107 et al., 2015; Williams, 2013) adapting of SDS components to new domains.", "startOffset": 83, "endOffset": 119}, {"referenceID": 13, "context": ", 2016a) using a procedure to train multi-domain via multiple adaptation steps, or (Ga\u0161i\u0107 et al., 2015; Williams, 2013) adapting of SDS components to new domains.", "startOffset": 83, "endOffset": 119}, {"referenceID": 4, "context": "This process finishes when an end sign is generated (Karpathy and Fei-Fei, 2015), or some constraints are reached (Zhang and Lapata, 2014).", "startOffset": 52, "endOffset": 80}, {"referenceID": 16, "context": "This process finishes when an end sign is generated (Karpathy and Fei-Fei, 2015), or some constraints are reached (Zhang and Lapata, 2014).", "startOffset": 114, "endOffset": 138}, {"referenceID": 3, "context": "Secondly, the traditional LSTM network proposed by Hochreiter and Schmidhuber (2014) in which the input gate ii, forget gate ft and output gates ot are introduced to control information flow and computed as follows:", "startOffset": 51, "endOffset": 85}, {"referenceID": 7, "context": "Thirdly, inspired by work of Wen et al. (2015b) in which the generator was further conditioned on a 1-hot representation vector s of given dialogue act, and work of Lu et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 7, "context": "Thirdly, inspired by work of Wen et al. (2015b) in which the generator was further conditioned on a 1-hot representation vector s of given dialogue act, and work of Lu et al. (2016) that proposed a visual sentinel gate to make a decision on whether the model should attend to the image or to the sentinel gate, an additional gating cell is introduced on top of the traditional LSTM to gate another controlling vector s.", "startOffset": 29, "endOffset": 182}, {"referenceID": 12, "context": ", 2014) and optimized by using stochastic gradient descent and back propagation through time (Werbos, 1990).", "startOffset": 93, "endOffset": 107}, {"referenceID": 10, "context": "The Restaurant and Hotel were collected in (Wen et al., 2015b), while the Laptop and TV datasets have been released by (Wen et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 8, "context": ", 2015b), while the Laptop and TV datasets have been released by (Wen et al., 2016a) with a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs.", "startOffset": 65, "endOffset": 84}, {"referenceID": 0, "context": "The generators were implemented using the TensorFlow library (Abadi et al., 2016) and trained with training, validation and testing ratio as 3:1:1.", "startOffset": 61, "endOffset": 81}, {"referenceID": 7, "context": "\u2022 HLSTM proposed by Wen et al. (2015a) which used a heuristic gate to ensure that all of the slot-value information was accurately captured when generating.", "startOffset": 20, "endOffset": 39}, {"referenceID": 7, "context": "\u2022 SCLSTM proposed by Wen et al. (2015b) which can jointly learn the gating signal and language model.", "startOffset": 21, "endOffset": 40}, {"referenceID": 7, "context": "\u2022 Enc-Dec proposed by Wen et al. (2016b) which applied the attention-based encoderdecoder architecture.", "startOffset": 22, "endOffset": 41}], "year": 2017, "abstractText": "Natural language generation (NLG) is a critical component in a spoken dialogue system. This paper presents a Recurrent Neural Network based Encoder-Decoder architecture, in which an LSTM-based decoder is introduced to select, aggregate semantic elements produced by an attention mechanism over the input elements, and to produce the required utterances. The proposed generator can be jointly trained both sentence planning and surface realization to produce natural language sentences. The proposed model was extensively evaluated on four different NLG datasets. The experimental results showed that the proposed generators not only consistently outperform the previous methods across all the NLG domains but also show an ability to generalize from a new, unseen domain and learn from multi-domain datasets.", "creator": "LaTeX with hyperref package"}}}