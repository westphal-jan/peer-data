{"id": "1706.00286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Learning to Compute Word Embeddings On the Fly", "abstract": "Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the \"long tail\" of this distribution requires enormous amounts of data. Representations of rare words trained directly on end-tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained against the end task. We show that this improves results against baselines where embeddings are trained on the end task in a reading comprehension task, a recognizing textual entailment task, and in language modelling.", "histories": [["v1", "Thu, 1 Jun 2017 13:12:15 GMT  (198kb,D)", "http://arxiv.org/abs/1706.00286v1", null], ["v2", "Mon, 5 Jun 2017 20:18:27 GMT  (254kb,D)", "http://arxiv.org/abs/1706.00286v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["dzmitry bahdanau", "tom bosc", "stanis{\\l}aw jastrz\\k{e}bski", "edward grefenstette", "pascal vincent", "yoshua bengio"], "accepted": false, "id": "1706.00286"}, "pdf": {"name": "1706.00286.pdf", "metadata": {"source": "CRF", "title": "Learning to Compute Word Embeddings On the Fly", "authors": ["Dzmitry Bahdanau", "Tom Bosc"], "emails": ["bahdanau@iro.umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to survive on our own."}, {"heading": "2 Related Work", "text": "This is an essential step forward from the models that are being retrained from scratch, the approach can be criticized because it is extremely data hungry. Obtaining the necessary amount of data can be difficult, for example in technical domains. Moreover, additional training criteria used in the pre-training approaches are not guaranteed to obtain representations that are useful for the task. There are a number of attempts to achieve a generalization by relying on spelling. (Ling et al., 2015) We used a bi-directional LSTM to read the spelling of rare words and showed1The GLoVembeddings."}, {"heading": "3 On the Fly Embeddings", "text": "In general, a neural network processes speech input by replacing its elements, usually words, with the respective vectors e (xi), which are often referred to as embeddings. (2003) Embeddings are usually either trained from the ground up or pre-trained. If embeddings are trained from the ground up, a limited vocabulary is defined as Vtrain = {w1,.., wn}, usually based on the frequency of training of that word. Words that are not used in Vtrain are replaced by a special token UNK with a manageable embeddedness e (UNK). Unseen test time words w / vtrain are then represented by e (UNK), effectively meaning that the specific meaning of that word is lost. Even if w was included in Vtrainbut, its learned embeddedness e (w) was probably not very informative. The approach proposed in this work is described in Figure 1, as is the use of words."}, {"heading": "4 Experiments", "text": "For each task, we chose a basic model and architecture from literature that we knew would yield reasonable results to explore how an extension with on-the-fly embedding would affect performance. We examined two complementary sources of auxiliary data. First, we used word definitions from WordNet (Miller, 1995). Second, we experimented with the spelling of character-level words as auxiliary data. To this end, we included natural language definitions for all their 147306 lemmata (including multi-word keywords that we consider in this paper) 4. We experimented with the spelling of character-level words as auxiliary data. To fit our use of dictionaries, we added fake definitions in the form of \"Word\" \u2192 \"W,\" \"r,\" \"d.\""}, {"heading": "4.1 Question Answering", "text": "This year it is more than ever before."}, {"heading": "4.2 Entailment prediction", "text": "Dre rf\u00fc ide nlrfhUeae\u00fccnlrfhsrr\u00fceegnrfhsrr\u00fceeegnrrrrVnlrteeeeeeegnr rf\u00fc eid rf\u00fc ide nlrfhUe\u00fcc\u00fceegnrrrrc\u00fcehcnlrrrc\u00fcehnrrrrrc\u00fceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "4.3 Language Modelling", "text": "In the first two months of this year, the number of working women in the US increased by 20%, while the number of working men in the US increased by 20%."}, {"heading": "5 Discussion", "text": "We have shown how various sources of auxiliary information, such as spelling and a dictionary of definitions, can be used to produce useful embeddings for rare words on the fly. While it was previously known that adding spelling information to the model is helpful, it is often difficult or impossible to deduce the meaning directly from the characters. Our more general approach offers endless possibilities to add other data sources and learn end-to-end to extract the relevant bits of information from them. Our experiments with a dictionary of definitions show the feasibility of the approach, as we make improvements over the mere use of spelling in answering questions and semantic classification tasks. \"Our qualitative studies of answering the question confirm our intuition about where the improvement comes from. It is also clear that adding additional auxiliary data would help, and that it would probably also be useful to add definitions from not just Corpedia, but also for adding Phrase from Section 4.1."}, {"heading": "6 Conclusion", "text": "In this paper, we have shown that the introduction of relatively small amounts of auxiliary data and a method of spontaneous embedding of this data builds a bridge between low-data embedding, where embedding must be learned directly from the end task, and data-rich embedding, where embedding can be pre-trained and sufficient external data is available to ensure lexical coverage within the scope. A large representative corpus of embedding foreign words is not always available, and our method is applicable when one has limited access to auxiliary data. Continuous learning from auxiliary sources can be highly data efficient when these sources present compressed relevant information about the word, as dictionary definitions do. A similar desirable aspect of our approach is that it partially returns control over what a language processing system does to engineers or even users: if they are dissatible with the output, they may have auxiliary information to add, or want to add it to be."}, {"heading": "Acknowledgements", "text": "We thank the developers of Theano (Theano Development Team, 2016) and Blocks (van Merri\u00ebnboer et al., 2015) for their great work. We thank NVIDIA for accessing their DGX-1 computers used in this work. Stanis\u0142aw Jastrze bski was supported by the grant number DI 2014 / 016644 of the Polish Ministry of Science and Higher Education. We thank \u00c7ag lar G\u00fcl\u00e7ehre and Alexandre Lacoste for useful discussions."}], "references": [{"title": "How we blessed distributional semantic evaluation", "author": ["M. References Baroni", "A. Lenci"], "venue": "Proceedings of the", "citeRegEx": "Baroni and Lenci,? 2011", "shortCiteRegEx": "Baroni and Lenci", "year": 2011}, {"title": "A neural probabilistic language model", "author": ["PA Stroudsburg", "Y. USA. Association for Computational Linguistics. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of", "citeRegEx": "Stroudsburg et al\\.,? 2003", "shortCiteRegEx": "Stroudsburg et al\\.", "year": 2003}, {"title": "A large annotated corpus for learning natural", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Enhancing and combining sequential and tree LSTM", "author": ["Q. Chen", "X. Zhu", "Z. Ling", "S. Wei", "H. Jiang"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Retrofitting word vectors", "author": ["M. Linguistics. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E.H. Hovy", "N.A. Smith"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Aistats, volume 15, page 275.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Hypernetworks", "author": ["D. Ha", "A. Dai", "Q.V. Le"], "venue": "arXiv preprint arXiv:1609.09106.", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["F. Hill", "K. Cho", "A. Korhonen", "Y. Bengio"], "venue": "Transactions of the Association for Computational Linguistics, 4:17\u201330.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Zero-data learning of new tasks", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 646\u2013651.", "citeRegEx": "Larochelle et al\\.,? 2008", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "T. Lu\u00eds", "L. Marujo", "R.F. Astudillo", "S. Amir", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "arXiv preprint arXiv:1508.02096.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Leveraging lexical resources for learning entity embeddings in multi-relational data", "author": ["T. Long", "R. Lowe", "J.C.K. Cheung", "D. Precup"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers.", "citeRegEx": "Long et al\\.,? 2016", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Vocabulary selection strategies for neural machine translation", "author": ["G. L\u2019Hostis", "D. Grangier", "M. Auli"], "venue": null, "citeRegEx": "L.Hostis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L.Hostis et al\\.", "year": 2016}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["H. Mi", "Z. Wang", "A. Ittycheriah"], "venue": "arXiv preprint arXiv:1605.03209.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of ACM, 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100, 000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2383\u20132392.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation, 4(1):131\u2013139.", "citeRegEx": "Schmidhuber,? 1992", "shortCiteRegEx": "Schmidhuber", "year": 1992}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, abs/1605.02688.", "citeRegEx": "Team,? 2016", "shortCiteRegEx": "Team", "year": 2016}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L. van der Maaten", "G. Hinton"], "venue": null, "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Dynamic coattention networks for question answering", "author": ["C. Xiong", "V. Zhong", "R. Socher"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "Liu", "T.-Y."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM \u201914, pages 1219\u20131228, New York, NY, USA. ACM.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Human behavior and the principle of least effort: An introduction to human ecology", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "Zipf,? \\Q1949\\E", "shortCiteRegEx": "Zipf", "year": 1949}], "referenceMentions": [{"referenceID": 26, "context": "Indeed, Natural Language yields a Zipfian distribution (Zipf, 1949) which tells us that a core set of words (at the head of the distribution) are frequent and ubiquitous, while a significantly larger number (in the long tail) are rare.", "startOffset": 55, "endOffset": 67}, {"referenceID": 14, "context": "This forces model designers to rely on overly large vocabularies, as observed by (Mi et al., 2016; Sennrich et al., 2015), which are parametrically expensive, or to employ vocabulary selection strategies (L\u2019Hostis et al.", "startOffset": 81, "endOffset": 121}, {"referenceID": 20, "context": "This forces model designers to rely on overly large vocabularies, as observed by (Mi et al., 2016; Sennrich et al., 2015), which are parametrically expensive, or to employ vocabulary selection strategies (L\u2019Hostis et al.", "startOffset": 81, "endOffset": 121}, {"referenceID": 13, "context": ", 2015), which are parametrically expensive, or to employ vocabulary selection strategies (L\u2019Hostis et al., 2016).", "startOffset": 90, "endOffset": 113}, {"referenceID": 11, "context": "Some models, such as in the work of Ling et al. (2015), have sought to deal with the open vocabulary ar X iv :1 70 6.", "startOffset": 36, "endOffset": 55}, {"referenceID": 15, "context": "These representations can then be used as the alternative representation for out-of-vocabulary words, or combined with within-vocabulary word embeddings directly trained on the task of interest or obtained from an external data source (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 235, "endOffset": 282}, {"referenceID": 17, "context": "These representations can then be used as the alternative representation for out-of-vocabulary words, or combined with within-vocabulary word embeddings directly trained on the task of interest or obtained from an external data source (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 235, "endOffset": 282}, {"referenceID": 17, "context": "On such datasets, attempting to learn global vectors\u2014for example GloVe embeddings (Pennington et al., 2014)\u2014from external data, would only provide coverage for common words and would be unlikely to be exposed to sufficient (or any) examples of domain-specific technical terms to learn good enough representations.", "startOffset": 82, "endOffset": 107}, {"referenceID": 15, "context": "(Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 0, "endOffset": 47}, {"referenceID": 17, "context": "(Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 0, "endOffset": 47}, {"referenceID": 11, "context": "(Ling et al., 2015) used a bidirectional LSTM to read the spelling of rare words and showed", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "The closest to our work is the study by Hill et al. (2016), in which a recurrent network is trained to produce an embedding of a dictionary definition that is close to the embedding of the headword.", "startOffset": 40, "endOffset": 59}, {"referenceID": 7, "context": "The closest to our work is the study by Hill et al. (2016), in which a recurrent network is trained to produce an embedding of a dictionary definition that is close to the embedding of the headword. The network is shown to be an effective reverse dictionary and a crossword solver. Our approach is different in that we train a dictionary reader in an end-to-end fashion for a specific task, side-stepping the potentially suboptimal auxiliary ranking cost that was used in that earlier work. Their method also relies on the availability of high-quality pretrained embeddings which might not always be the case. Another related work by Long et al. (2016) uses dictionary definitions to provide initialization to a database embedding method, which is different from directly learning to use the definitions like we do.", "startOffset": 40, "endOffset": 653}, {"referenceID": 10, "context": "For example, Larochelle et al. (2008) propose to add classes to a classifier by representing them using their \u201cdescriptions\u201d.", "startOffset": 13, "endOffset": 38}, {"referenceID": 25, "context": "Enhancing word embeddings with auxiliary data from knowledge bases (including wordnet) has a long tradition (Xu et al., 2014; Faruqui et al., 2015).", "startOffset": 108, "endOffset": 147}, {"referenceID": 4, "context": "Enhancing word embeddings with auxiliary data from knowledge bases (including wordnet) has a long tradition (Xu et al., 2014; Faruqui et al., 2015).", "startOffset": 108, "endOffset": 147}, {"referenceID": 18, "context": "In particular, Schmidhuber (1992) predicts updates to the weights of a \u201cfast\u201d network from the output of a \u201cslow\u201d network (which is trained by backpropagation) instead of updating them through gradient descent.", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "Relatedly, Ha et al. (2016) explore architectures wherein an auxiliary network predicts the weights of a \u201cslow\u201d network directly.", "startOffset": 11, "endOffset": 28}, {"referenceID": 8, "context": "For the function f we can consider two choices: a simple mean pooling ed(w) = \u2211k i=1 e (xi)/k or using the last state of an LSTM (Hochreiter and Schmidhuber, 1997), ed(w) = LSTM(e(x1), .", "startOffset": 129, "endOffset": 163}, {"referenceID": 16, "context": "First, we used word definitions from WordNet (Miller, 1995).", "startOffset": 45, "endOffset": 59}, {"referenceID": 17, "context": "In order to measure the performance of models in \u201cdata-rich\u201d scenarios where large amount of unlabelled language data is available for the training of word representations, we used as pretrained word embeddings 300-dimensional GLoVe vectors trained on 840 billion words (Pennington et al., 2014).", "startOffset": 270, "endOffset": 295}, {"referenceID": 18, "context": "We used the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) that consists of approximately 100000 human-generated question-answer pairs.", "startOffset": 56, "endOffset": 80}, {"referenceID": 24, "context": "Our basic model is a simplified version of a coattention proposed in (Xiong et al., 2016).", "startOffset": 69, "endOffset": 89}, {"referenceID": 5, "context": "We transform U0 with a bidirectional LSTM and another ReLU(Glorot et al., 2011) layer to obtain the final context-document representation U2.", "startOffset": 58, "endOffset": 79}, {"referenceID": 5, "context": "We transform U0 with a bidirectional LSTM and another ReLU(Glorot et al., 2011) layer to obtain the final context-document representation U2. Finally, two linear layers followed by a softmax assign to each position of the document probabilities of it being the beginning and the end of the answer span. We refer the reader to the work of Xiong et al. (2016) for more details.", "startOffset": 59, "endOffset": 358}, {"referenceID": 2, "context": "We used Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which consists of around 500k pairs of sentences (hypothesis and premise) and the task is to predict the logical relation (contradiction, neutral or entailment) between them.", "startOffset": 58, "endOffset": 79}, {"referenceID": 2, "context": "Our first model (denoted as \"simple\" in Table 2) is a slightly improved baseline from (Bowman et al., 2015).", "startOffset": 86, "endOffset": 107}, {"referenceID": 9, "context": "The resulting representation is fed to a 2 layer ReLU MLP normalized by Batch Normalization (Ioffe and Szegedy, 2015).", "startOffset": 92, "endOffset": 117}, {"referenceID": 3, "context": "In order to validate that improvements translate to state of the art models we implemented a variant (replacing TreeLSTM by biLSTM) of Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) that achieves close to SOTA accuracy.", "startOffset": 178, "endOffset": 197}, {"referenceID": 0, "context": "Right plot shows tSNE of word embeddings for 3 random categories from BLESS dataset (Baroni and Lenci, 2011) (denoted by color).", "startOffset": 84, "endOffset": 108}, {"referenceID": 11, "context": "Similarly to prior work on using the spelling (Ling et al., 2015) we restrict the softmax output layer to only predict probabilities of the 10k most common words, however, we do not impose such a constraint when the model reads the words.", "startOffset": 46, "endOffset": 65}], "year": 2017, "abstractText": "Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the \u201clong tail\u201d of this distribution requires enormous amounts of data. Representations of rare words trained directly on end-tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained against the end task. We show that this improves results against baselines where embeddings are trained on the end task in a reading comprehension task, a recognizing textual entailment task, and in language modelling.", "creator": "LaTeX with hyperref package"}}}