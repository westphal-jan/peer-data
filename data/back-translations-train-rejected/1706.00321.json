{"id": "1706.00321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Using of heterogeneous corpora for training of an ASR system", "abstract": "The paper summarizes the development of the LVCSR system built as a part of the Pashto speech-translation system at the SCALE (Summer Camp for Applied Language Exploration) 2015 workshop on \"Speech-to-text-translation for low-resource languages\". The Pashto language was chosen as a good \"proxy\" low-resource language, exhibiting multiple phenomena which make the speech-recognition and and speech-to-text-translation systems development hard.", "histories": [["v1", "Thu, 1 Jun 2017 14:30:19 GMT  (35kb,D)", "http://arxiv.org/abs/1706.00321v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan trmal", "gaurav kumar", "vimal manohar", "sanjeev khudanpur", "matt post", "paul mcnamee"], "accepted": false, "id": "1706.00321"}, "pdf": {"name": "1706.00321.pdf", "metadata": {"source": "CRF", "title": "USING OF HETEROGENEOUS CORPORA FOR TRAINING OF AN ASR SYSTEM", "authors": ["Jan Trmal", "Gaurav Kumar", "Vimal Manohar", "Sanjeev Khudanpur", "Matt Post", "Paul McNamee"], "emails": [], "sections": [{"heading": null, "text": "Even if the amount of data seems sufficient, given that the data comes from multiple sources, the preliminary experiments show that there is little to no benefit in merging (linking) the corpora, and that more elaborate methods of using all the data need to be devised. This paper focuses only on the LVCSR part and presents a number of different techniques that have proven useful in benefiting from several different CorporaIndex terms - language translation, pashto, babel, multiple corpora, neural networks, discriminatory training;"}, {"heading": "1. INTRODUCTION", "text": "Pashto belongs to the southeastern Iranian branch of Indo-Iranian languages. It has three main variants: Northern and Central (both spoken mainly in Pakistan) and Southern (spoken mainly in Afghanistan). Each of these variants has a number of dialectical varieties. It is estimated that Pashto has 66 million characters around the world. [1] While written Pashto has existed since the 16th century, the standardization of the writing system is still ongoing. There are a considerable number of words that have more than one publicly accepted way of writing (cf. English consultant vs. consultant). Other problems include current / missing spacing for certain characters (especially those that do not belong to connected Arabic characters), and frequent substitution of visually similar (and similar-sounding) characters. The last problem is emphasized by the fact that for writing Pashto, several different keyboard layouts are used \"in the wild.\" There is, of course, the official Pashto keyboard.The work was supported by NS13000000F CREL2."}, {"heading": "2. SPEECH CORPORA AVAILABLE", "text": "In this section, we give a brief overview of the various corporations we have worked with. We acquired two corporas before the workshop began and another one during the workshop. As the latter is substantially different and not fully suited to our task (LVCSR and machine translation), we have not taken advantage of it and are only reporting the figures here for completeness."}, {"heading": "2.1. Lila consortium Appen Pashto (A)", "text": "The Appen Pashto (data set \"A\") contains about 90 hours of telephone calls (8 kHz). Since the partitions of Xiv: 170 6.00 321v 1 [cs.C L] 1J un2 017train / dev were not externally defined in the Appen data release, we divided the data into 85 hours for training and 5 hours as development units 1. The lexicon was recorded with the corpus and the lexical entries included vowalized representations and romanized word forms. In addition, the lexicon contained four dialect variants of the pronunciation of each word (we assume that these were generated automatically)."}, {"heading": "2.2. IARPA Babel Pashto (B)", "text": "Babel Paschto (record \"B\") is the Full Language Pack2 in the terminology of the Babel program. Simply explained, it is an 80-hour record with 8 kHz sampled telephone call data and the associated lexicon and transcripts. We used the development set defined in the corpus. Although our description of the corpus \"A\" and \"B\" might indicate that the records are similar, our observations showed that this was not the case. We observed that record \"B\" was prepared and transcribed more carefully overall than record \"A.\""}, {"heading": "2.3. TransTac Pashto (T)", "text": "The TransTac Pashto is very different from the two above-mentioned corpora. By its nature, it is more of a scripted language, albeit with a high degree of spontaneity; it is also professionally recorded (in a recording room), so we had to try it down to 8 kHz in order to use it with corpora \"A\" and \"B.\" The dataset was created as follows: The people involved were given one of a limited number of scenarios and then asked to reproduce this scenario. We could not obtain the definitions of the turn / dev partitions for this dataset used in the DARPA TransTac ([2]) project. Therefore, and the fact that we used the speech sampled below, our performance is not directly comparable to the previously reported results for this dataset."}, {"heading": "2.4. Corpus preparation", "text": "As mentioned in the introduction, there is significant variability in the process used to transcribe corpora \"A\" and \"B.\" Our initial efforts aimed to make the transcriptions from both corpora more consistent in their use of characters, which was motivated by our inspection of lexicographs from corpora. Starting with the lexicographs and then removing the vowel characters from all words, the overlap (defined as the number of words that divide the lexicographs) was only 15%. In an attempt to determine the minimum number of character changes that would increase lexicographical overlaps, we developed a simple algorithm that allowed a language expert to determine the edit rules. 1. Take 1000 most common words from each lexicographic (based on the associated transcripts). We chose this number to cover over 90% of the frequency mass in each corpus."}, {"heading": "3. BASELINE ACOUSTIC SYSTEM", "text": "In this section we describe at a high level how we trained the basic system. As the goal was to develop a sufficiently simple, catchy, minimal delay system, we decided to train a system of deep neural networks (or, more precisely, a TDNN, as described in [3]). We used the Kaldi3http: / / festvox.orgtoolkit [4] to train the ASR system. Here are a few details worth mentioning."}, {"heading": "3.1. Speed perturbation", "text": "We used sox4 to obtain two copies of the training data (the first copy used a speed factor of 1.1 and the second 0.9), and our experience confirms that this improves the overall robustness of the resulting models; the network was trained in parallel by averaging the models, as described in [6]."}, {"heading": "3.2. Estimation of pronunciation probabilities", "text": "The pronunciation dictionary rarely contains the probabilities of the individual pronunciation variants, but it is possible to estimate these probabilities based on the orientation of the training data. Furthermore, it is possible to model word-dependent silence probabilities, in addition to modelling the probability of silence, in order to estimate (and smooth out accordingly) the probability of occurrence of each word after silence. See [7] for a detailed analysis of this and several related ideas. In our experience, it is also beneficial to re-evaluate these probabilities several times during the training process."}, {"heading": "3.3. Sequence discriminative training", "text": "For the sequence training, we used the sMBR method [8, 9], which is reported [10] to achieve the best performance (measured with respect to WER), and we found it advantageous to adjust the previous probabilities - used in decoding to convert the posterior TDNN probabilities into probabilities - after the discriminatory training is completed. Historically, the priors are calculated from alignment. As mentioned in [11], the marginalization of the posterior DNN across all acoustic vectors results in better performance, especially when the data is noisy."}, {"heading": "3.4. Duration model rescoring", "text": "After the final grids were created, we used the rescoring method described in [12] to model the duration. We used the software provided by the author of the essay.5 We find the improvements fairly consistent, though less than the numbers given in the original essay."}, {"heading": "3.5. Overall performance of the baseline system", "text": "The total performance of the resulting baseline system is shown in Table 2. Please note that for each test set - Appen, Babel and TransTac - a separate baseline system4http: / / sox.sourceforge.net / 5https: / / github.com / alumae / kaldi-nnet-dur-model is used."}, {"heading": "4. RELATED WORK", "text": "This comparison is not straightforward: a considerable amount of work has been reported in the Babel-Paschto project, but the reported figures are usually a combination of several (sometimes very many) systems. As our team (as part of Team Radical) was involved in this project, we present a comparison of our current system with our most powerful Babel system (hybrid DNN system) from two years ago (see Table 3).The comparison needs to be made carefully because (in order to standardize the corporations in question) we have applied the rule set mentioned in Section 2.4. Table 3 shows that the new training method (B-scale train only without normalization) gives us an absolute gain of 6% over the older system and the normalization rules provide a further gain of 3%."}, {"heading": "5. JOINT MULTI-CORPUS TRAINING", "text": "During the workshop, it became clear that the three corporas did not actually combine well. Corpora \"A\" and \"B\" are closest to us, but even their combination for training did not produce better results - see Table 4. Another piece of evidence can be gleaned from Table 5. The language model, which was created from the training data of the \"T\" dataset, was not useful for the language model interpolation. As the diversity of data proved too high to allow the formation of a single model on all the data that would work well, we decided to train data-specific models, i.e., three models, each of which would be specialized in that given dataset. Furthermore, we tried to find out if there was a way to benefit from the fact that we had several (similar) corpora. The method we used to exploit this fact was the sharing (i.e., training together) of the hidden layers and only the first layers of the datasets."}, {"heading": "6. MAIN RESULTS", "text": "An important lesson from Table 6 is that simply merging several corpora (A and B) to form a single acoustic model in 2 out of 3 test sets (Babel and TransTac) results in significant degradation of the WER. Only the Appen test set benefits from training based on the Babel data. However, the most important new finding is that training separate TDNN acoustic models for each corpus while splitting the internal layers - an idea similar to training multilingual acoustic models - followed by some LM and permanent model optimizations leads to significant improvements in WHO. The Appen WHO decreases from 51.9% to 50.4% (3% relative), while the Babel WHO decreases from 46.8% to 44.3% (5% relative). The TransTac corpus is too different from the Appen and the Babel corpus to derive any benefit from the data pooling."}, {"heading": "7. CONCLUSION", "text": "This paper presents an overview of the low-resource ASR system in Pashto that was developed during the SCALE '15 workshop. Initially, we developed an LVCSR system for the Pashto language with a single pass. This system, which was trained with a corpus obtained by concatenating two different corpses (referred to in this essay as \"A\" and \"B\"), significantly outperformed the performance of the systems developed under the Babel program. Our single pass system achieved comparable if not better performance in terms of very complex systems (multiple passes + combination of systems). Although it was important to achieve WHO as well as possible, the work done during the workshop and consequently the work described in the essay focused on providing insights on how to combine training data for multiple different sources. Paschto is a good proxy for identifying various problems that can be seen \"in the wild.\" While the aspects of having different audio channels (i.e., abundance of impulse responses to the ASR) are considered to be sufficient to solve the system with the ASR, the responses to the ASR are mostly complex."}, {"heading": "8. REFERENCES", "text": "[1] M. Paul Lewis, Gary F. Simons, and Charles D. Fenning, Eds., Ethnologue: Languages of the World, SIL International, Dallas, Texas, eighteenth edition, 2015. [2] Gregory A. Sanders, Brian A. Weiss, Craig Schlenoff, Michelle P. Steves, and Sherri Condon, \"Evaluation Methodology and metrics used to assess the TransTac two-way, speech-to-speech translation systems,\" Computer Speech & Language, vol. 27, no. 2, pp. 528-553, 2013, Special Issue on Speech-speech translation. [3] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur \"A time delay neural network architecture for efficient modeling of long temporal contexts, in Proceedings of Interspeech, ISD."}], "references": [{"title": "Evaluation methodology and metrics employed to assess the TransTac two-way, speech-to-speech translation systems", "author": ["Gregory A. Sanders", "Brian A. Weiss", "Craig Schlenoff", "Michelle P. Steves", "Sherri Condon"], "venue": "Computer Speech & Language, vol. 27, no. 2, pp. 528 \u2013 553, 2013, Special Issue on Speech-speech translation.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts", "author": ["Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The Kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio augmentation for speech recognition", "author": ["Tom Ko", "Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "CoRR, vol. abs/1410.7455, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Pronunciation and silence probability modeling for ASR", "author": ["Guoguo Chen", "Hainan Xu", "Minhua Wu", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel loss function for the overall risk criterion based discriminative training of HMM models", "author": ["Janez Kaiser", "Bogomir Horvat", "Zdravko Kacic"], "venue": "Sixth International Conference on Spoken Language Processing. 2000, ISCA.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Hypothesis spaces for minimum Bayes risk training in large vocabulary speech recognition", "author": ["Matthew Gibson", "Thomas Hain"], "venue": "Proceedings of Interspeech. 2006, ISCA.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["Karel Vesel\u00fd", "Arnab Ghoshal", "Luk\u00e1\u0161 Burget", "Daniel Povey"], "venue": "Proceedings of Interspeech. 2013, pp. 2345\u20132349, ISCA.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised maximum mutual information training of deep neural network acoustic models", "author": ["Vimal Manohar", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network phone duration model for speech recognition", "author": ["Tanel Alum\u00e4e"], "venue": "Proceedings of Interspeech. 2014, ISCA.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We were unable to obtain the definitions of the train/dev partitions for this dataset that were used in the DARPA TransTac ([2]) project.", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "As the objective was to develop a sufficiently simple, single pass, minimum delay system, we opted for training a deep neural network system (or more precisely, a TDNN, as described in [3]).", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "toolkit [4] for training the ASR system.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "We utilized augmentation of the data via speed perturbation (as described in [5]) during training.", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "The network was trained in a parallel fashion using model averaging, as described in [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "See [7] for a detailed analysis of this and several related ideas.", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "For the sequence training we used the sMBR method [8, 9] which is reported [10] to give best performance (measured with respect to WER).", "startOffset": 50, "endOffset": 56}, {"referenceID": 7, "context": "For the sequence training we used the sMBR method [8, 9] which is reported [10] to give best performance (measured with respect to WER).", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "For the sequence training we used the sMBR method [8, 9] which is reported [10] to give best performance (measured with respect to WER).", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "As mentioned in [11], marginalizing of the DNN posteriors over all acoustic vectors gives better performance, especially when the data is noisy.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "After the final lattices were generated, we used the duration modeling rescoring as described in [12].", "startOffset": 97, "endOffset": 101}], "year": 2017, "abstractText": "The paper summarizes the development of the LVCSR system built as a part of the Pashto speech-translation system at the SCALE (Summer Camp for Applied Language Exploration) 2015 workshop on \u201cSpeech-to-text-translation for low-resource languages\u201d. The Pashto language was chosen as a good \u201cproxy\u201d low-resource language, exhibiting multiple phenomena which make the speech-recognition and and speech-to-text-translation systems development hard. Even when the amount of data is seemingly sufficient, given the fact that the data originates from multiple sources, the preliminary experiments reveal that there is little to no benefit in merging (concatenating) the corpora and more elaborate ways of making use of all of the data must be worked out. This paper concentrates only on the LVCSR part and presents a range of different techniques that were found to be useful in order to benefit from multiple different corpora", "creator": "LaTeX with hyperref package"}}}