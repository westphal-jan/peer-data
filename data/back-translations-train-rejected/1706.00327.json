{"id": "1706.00327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "One button machine for automating feature engineering in relational databases", "abstract": "Feature engineering is one of the most important and time consuming tasks in predictive analytics projects. It involves understanding domain knowledge and data exploration to discover relevant hand-crafted features from raw data. In this paper, we introduce a system called One Button Machine, or OneBM for short, which automates feature discovery in relational databases. OneBM automatically performs a key activity of data scientists, namely, joining of database tables and applying advanced data transformations to extract useful features from data. We validated OneBM in Kaggle competitions in which OneBM achieved performance as good as top 16% to 24% data scientists in three Kaggle competitions. More importantly, OneBM outperformed the state-of-the-art system in a Kaggle competition in terms of prediction accuracy and ranking on Kaggle leaderboard. The results show that OneBM can be useful for both data scientists and non-experts. It helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time and cost.", "histories": [["v1", "Thu, 1 Jun 2017 14:44:34 GMT  (1044kb,D)", "http://arxiv.org/abs/1706.00327v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.AI", "authors": ["hoang thanh lam", "johann-michael thiebaut", "mathieu sinn", "bei chen", "tiep mai", "oznur alkan"], "accepted": false, "id": "1706.00327"}, "pdf": {"name": "1706.00327.pdf", "metadata": {"source": "CRF", "title": "One button machine for automating feature engineering in relational databases", "authors": ["Hoang Thanh Lam", "Johann-Michael Thiebaut", "Mathieu Sinn", "Bei Chen", "Tiep Mai", "Oznur Alkan"], "emails": ["t.l.hoang@ie.ibm.com,", "johann-michael.thiebaut@epfl.ch,", "mathsinn@ie.ibm.com,", "beichen2@ie.ibm.com,", "maikhctiep@gmail.com", "oalcan2@ie.ibm.com"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. RELATED WORK", "text": "The automation of data science is a comprehensive topic that includes the automation of five basic steps illustrated in Figure 1. Most related work in the literature focuses on the last two steps: automation of model selection, hyperparameter tuning, and feature engineering. In the following sections, related work on the automation of these last two steps is discussed."}, {"heading": "A. Automatic model selection and tuning", "text": "Auto-Weka [8], [11] and Auto-SkLearn [3] are among the first papers to try to find the best combination of data pre-processing, hyperparameter tuning and model selection, both based on Bayesian optimization [2] to avoid exhaustive parameter enumeration, which builds on existing algorithms and data pre-processing techniques in Weka3 and Scikit-Learn4 and is therefore very practical for practical use. Cognitive Automation of Data Science (CADS) [1], [6] is another system that builds on Weka, SPSS and R to automate model selection and hyperparameter tuning. CADS is made up of three basic components: a repository of analytics algorithm with metadata, a learning control strategy that determines the model and configuration for various analysis tasks, and an interactive user interface."}, {"heading": "B. Automatic feature engineering", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "C. Statistical relational learning", "text": "Our work has in common with the areas of inductive logic programming and statistical relationship learning (StarAI) [4]. StarAI also focuses on finding patterns across multiple tables or informative common features. An additional aspect of this work, however, is the broader view of data transformations."}, {"heading": "III. METHODOLOGY", "text": "OneBM takes a database of tables with a main table. The main table must have a destination column, several key columns and optional attribute columns. Each entry in the main table corresponds to a unit that we use to train a machine learning model to predict its target value. Tables in the database are about foreign keys. Example 1: Figure 2 shows a sample game game database with 4 tables, this simple database serves as a running example of all the work: \u2022 main: contains information about the arrival times of trains. The destination column is the arrival time. Each entry in the main table is uniquely identified by the MessageID column, which corresponds to a message sent by a train upon arrival at a station. \u2022 Event: a log of events that occur at the station where the train arrives. \u2022 Delay: contains information about the train delay. It is similar to the main table, but the information table is converted from the arrival time to the train's arrival time: the train arrives time."}, {"heading": "A. Data collection", "text": "In fact, we are able to put ourselves at the forefront in the way we have done it in the past: in the way we have done it, in the way we have done it, in the way we have done it, in the way we have done it, in the way we have done it, in the way we have done it. \""}, {"heading": "B. Data transformation", "text": "After the data is collected by GroupBy (T ep, d), the attributes are determined by applying a transformation function f to the collected data, where f refers to the function that maps GroupBy (T ep, d) to a vector of fixed size numerical values. By default, OneBM supports the following transformation functions: Data type transformation functions numerically as categorical (un) standardized text for the distribution of labels see Sequence characteristics Timestamps Calendar characteristics Multiple Avg, Variance, Max, Min, Sum, Number of texts see Sequence characteristics Multiple number of items, Unique number, High correlated item time series avg, Max, Min, Sum, Number, Variance, Recent (k), Fast Fourier transformation, Discrete Wavelet transformation, Autocorrelation coefficient sequence number, Unique number, High correlated partial sequences avg, These attributes are supported by the most scientists, etc."}, {"heading": "C. Feature selection", "text": "The function selection is used to remove irrelevant features that were extracted in the previous steps. First, duplicate features are removed. Second, if the training and test data have an implicit sequence that is defined by a column, such as time stamp, then drift features are detected by comparing the distribution between the value of the features in the training and a validation set. If two distributions are different, the feature is identified as a drift feature that can lead to overconfiguration. Drift features are all detected from the feature set.In addition, we also use chi-square hypotheses to test whether there is a dependence between a feature and the target variable. Features that are slightly independent of the target variable are removed. In principle, the function selection is a NP-hard problem. The automation of the function selection is out of the scope of this work. The BneM enhancement is pre-selected by BneM:"}, {"heading": "IV. EFFICIENT IMPLEMENTATION", "text": "This section discusses some optimization strategies that deal with the high calculation costs of the feature engineering process. There are three main techniques for solving the efficiency problems, which will be discussed further in the following sections."}, {"heading": "A. Depth first entity graph traversal", "text": "There are two options for traversing entity graphs: Breadthfirst and Depth-First Traversal. In each traversal, we can cache the associated result to avoid it being recalculated from scratch each time we explore a deeper node in the graph. The number of cached results in the Width-First and Depth-First Traversal is each limited by the maximum width and depth. Due to the fact that the maximum depth can be easily controlled by users, while the maximum width depends on the structure of the entity graph, we choose Depth-First Traversal. Interconnected tables are cached along the connecting path, which reduces both the computing and storage costs. When the maximum depth is reached, cached tables are then freed while different branches of the graph are examined. Example 7: Suppose that we release two paths p1 = A b \u2212 \u2192 C \u2192 7 and p2 = a \u2212 D."}, {"heading": "B. Redundant path removal", "text": "Two paths p1 and p2 are equivalent if for each unit e the relation trees T ep1 and T e p2 are the same. OneBM recognizes equivalent paths and removes redundant paths by transforming them into their canonical form and comparing them with paths traveled. The canonical form of a path p is the shortest path equivalent to p. 8. Example 8: Consider two paths: p1 = A \u2212 \u2192 B a \u2212 \u2192 C 7 \u2192 c and p2 = A \u2212 \u2192 C 7 \u2192 c. Let us assume that column a is the primary key of A, B and C. We can see that p1 is p2 equivalent, so p1 is redundant in relation to p2 and is not taken into account in function extraction."}, {"heading": "C. Sub-sampling the joining results", "text": "OneBM uses sub-sampling to reduce the memory requirements for the joined large tables, at the expense of accuracy when the features are calculated on a sub-sample of the data. To overcome the negative effects of sub-sampling, the sampling rate is not fixed, but is dynamically controlled by a parameter call called MAX-JOINEDSIZE, which is set a priori depending on the availability of system memory. To achieve dynamic sub-sampling data, OneBM estimates the joining size before the joining process and calculates the sampling ratio that leads to the desired joining size. In each training example, a layered uniform sampling is used. When the entries in a table are linked to timestamps, OneBM does not apply uniform sampling, but takes samples that are newer datasets. 100 experiments combine these data sets with OneBM's large data sets."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experiment settings and datasets", "text": "In this section, we will discuss the results of three Kaggle contests (including one in which DSM reported its results), in which a random forest (RF) of 100 trees and an XGBOOST model were used. In the experiments, XGBOOST was trained to convergence, i.e. the number of training steps was set to an unlimited number, the training stopped if no improvement in accuracy was observed in a validation set. No hyperparameter tuning was considered, as it is not the focus of this work. In practice, the results were further improved by combining OneBM with automatic hyperparameter tuning techniques. We apply a simple model selection as follows: In each competition, we first submitted the results of RF and XGBOOST to Kaggle. The results observed on public leaderboards were used to select the better model."}, {"heading": "B. Comparison with DSM in KDD Cup 2014", "text": "In the 2014 KDD Cup, participants were asked to predict which project proposals would be successful based on their data on project descriptions, school and teacher profiles and locations, donation information and project resources requested. Figure 4. The maximum depth of the chart is one, so we set the maximum search depth to 1. The donation table in the competition contains data for training, but no data for test cases. Therefore, we ignored the donation table in the feature extraction process. OneBM's results using Random Forest, xgboost and the comparison to DSM are shown in Table 3. As we can clearly see, even without tuning, OneBM surpassed DSM by improving its ranks in the private leaderboard from 145 to 80, i.e., improving the result from the top 30% to the top 17%. It is important to note that DSM had two numbers that correspond to the results before and after hyper-parameter tuning."}, {"heading": "C. Grupo Bimbo", "text": "In the Grupo Bimbo Inventory Prediction Competition, participants were asked to predict the weekly sales of fresh baked goods on the shelves of over 1 million stores, along its 45,000 routes throughout Mexico. At the moment, the paper was written, the competition was completed. The database contains 4 different tables: the main table with the target variable (the weekly sales in units) of fresh baked goods. Since the rating is based on Root Mean Logarithmic Error (RMSLE), we forecast the logarithmic demand instead of absolute demand. \u2022 City State: geographical location of stores \u2022 Product: additional information, such as product names \u2022 Customers: information on clientele. It is well known that the historical demand series is a good predictor. Therefore, in addition to the given 4 tables, we have created a copy of the main table and designated it as the series with only three columns."}, {"heading": "D. Outbrain click prediction", "text": "This year has been so far that it is only a matter of time before it will be so far, until it will be so far."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "This paper presents a framework for automating feature engineering from relational databases. Experiments with real data have shown that the predefined framework can help data scientists explore the data and allow them to save significant time during the feature engineering phase. Furthermore, the framework has outperformed many participants in Kaggle competitions and outperformed the state-of-the-art DSM system in a Kaggle competition. In the future, we intend to pair OneBM with automatic model selection and hyperparameter tuning systems such as CADS or Auto-Sklearn to further improve the results."}, {"heading": "VII. ACKNOWLEDGEMENTS", "text": "We thank Dr. Olivier Verscheure, Dr. Eric Bouillet, Dr. Pol McAonghusa, Dr. Horst C. Samulowitz, Dr. Udayan Khurana and Tejaswina Pedapat for the useful discussion and support during the development of the project."}], "references": [{"title": "Towards cognitive automation of data science", "author": ["A. Biem", "M. Butrico", "M. Feblowitz", "T. Klinger", "Y. Malitsky", "K. Ng", "A. Perer", "C. Reddy", "A. Riabov", "H. Samulowitz", "D.M. Sow", "G. Tesauro", "D.S. Turaga"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. De Freitas"], "venue": "arXiv preprint arXiv:1012.2599,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Efficient and robust automated machine learning", "author": ["M. Feurer", "A. Klein", "K. Eggensperger", "J. Springenberg", "M. Blum", "F. Hutter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)", "author": ["L. Getoor", "B. Taskar"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Deep feature synthesis: Towards automating data science endeavors", "author": ["J.M. Kanter", "K. Veeramachaneni"], "venue": "In Data Science and Advanced Analytics (DSAA),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "READ: rapid data exploration, analysis and discovery", "author": ["U. Khurana", "S. Parthasarathy", "D.S. Turaga"], "venue": "In Proceedings of the 17th International Conference on Extending Database Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Cognito: Automated Feature Engineering for Supervised Learning", "author": ["U. Khurana", "D. Turaga", "H. Samulowitz", "S. Parthasarathy", "editors"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Auto-weka 2.0: Automatic model selection and hyperparameter optimization in weka", "author": ["L. Kotthoff", "C. Thornton", "H.H. Hoos", "F. Hutter", "K. Leyton- Brown"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Automatic construction and Natural-Language description of nonparametric regression models", "author": ["J.R. Lloyd", "D. Duvenaud", "R. Grosse", "J.B. Tenenbaum", "Z. Ghahramani"], "venue": "In AAAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Evaluation of a tree-based pipeline optimization tool for automating data science", "author": ["R.S. Olson", "N. Bartley", "R.J. Urbanowicz", "J.H. Moore"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference 2016,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "In Proc. of KDD-2013", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Automatic frankensteining: Creating complex ensembles autonomously", "author": ["M. Wistuba", "S. Nicolas", "L. Schmidt-Thieme"], "venue": "In SIAM SDM", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Auto-Weka [8], [11] and Auto-SkLearn [3] are among the first works trying to find the best combination of data preprocessing, hyper-parameter tuning and model selection.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "Auto-Weka [8], [11] and Auto-SkLearn [3] are among the first works trying to find the best combination of data preprocessing, hyper-parameter tuning and model selection.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "Auto-Weka [8], [11] and Auto-SkLearn [3] are among the first works trying to find the best combination of data preprocessing, hyper-parameter tuning and model selection.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Both works are based on Bayesian optimization [2] to avoid exhaustive grid-search parameter enumeration.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "Cognitive Automation of Data Science (CADS) [1], [6] is another system built on top of Weka, SPSS and R to automate model selection and hyper-parameter tuning process.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "Cognitive Automation of Data Science (CADS) [1], [6] is another system built on top of Weka, SPSS and R to automate model selection and hyper-parameter tuning process.", "startOffset": 49, "endOffset": 52}, {"referenceID": 11, "context": "Besides the aforementioned works, Automatic Ensemble [12] is the most recent work which uses stacking and metadata to assist model selection and tuning.", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "TPOT [10] is another system that uses genetic programming to find the best model configuration and preprocessing work-flow.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "Automatic Statistician [9] is similar to the works just described but focuses more on time-series data and interpretation of the models in natural language.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "However, recent work shows that, for a specific type of problem and data such as provided in relational databases, automation of feature engineering is achievable [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "Data Science Machine (DSM) [5] is the first system that automates feature engineering from a database of multiple tables.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "Cognito [7] is another system that automates feature engineering but from a single database table.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Our work has share common points with the fields of Inductive Logic Programming and Statistical Relational Learning (StarAI) [4].", "startOffset": 125, "endOffset": 128}], "year": 2017, "abstractText": "Feature engineering is one of the most important and time consuming tasks in predictive analytics projects. It involves understanding domain knowledge and data exploration to discover relevant hand-crafted features from raw data. In this paper, we introduce a system called One Button Machine, or OneBM for short, which automates feature discovery in relational databases. OneBM automatically performs a key activity of data scientists, namely, joining of database tables and applying advanced data transformations to extract useful features from data. We validated OneBM in Kaggle competitions in which OneBM achieved performance as good as top 16% to 24% data scientists in three Kaggle competitions. More importantly, OneBM outperformed the state-of-the-art system in a Kaggle competition in terms of prediction accuracy and ranking on Kaggle leaderboard. The results show that OneBM can be useful for both data scientists and non-experts. It helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time and cost.", "creator": "LaTeX with hyperref package"}}}