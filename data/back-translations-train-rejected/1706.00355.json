{"id": "1706.00355", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Grounding Symbols in Multi-Modal Instructions", "abstract": "As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability---for instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users' contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal input---i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations---to produce the segmentation of objects with a correspondent association to high-level concepts. To test our framework we present experiments in a table-top object manipulation scenario. Our results show our model learns the user's notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.", "histories": [["v1", "Thu, 1 Jun 2017 15:42:50 GMT  (3040kb,D)", "http://arxiv.org/abs/1706.00355v1", "9 pages, 8 figures, To appear in the Proceedings of the ACL workshop Language Grounding for Robotics, Vancouver, Canada"]], "COMMENTS": "9 pages, 8 figures, To appear in the Proceedings of the ACL workshop Language Grounding for Robotics, Vancouver, Canada", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yordan hristov", "svetlin penkov", "alex lascarides", "subramanian ramamoorthy"], "accepted": false, "id": "1706.00355"}, "pdf": {"name": "1706.00355.pdf", "metadata": {"source": "CRF", "title": "Grounding Symbols in Multi-Modal Instructions", "authors": ["Yordan Hristov", "Svetlin Penkov", "Alex Lascarides"], "emails": ["{yordan.hristov@,", "sv.penkov@,", "alex@inf.,", "s.ramamoorthy@}ed.ac.uk"], "sections": [{"heading": null, "text": "Realistically, this task must be accomplished with small datasets, which consist of the contextual assignment of meanings to terms by a particular user. We present a method for processing a raw stream of intermodal inputs - i.e. verbal instructions, visual perception of a scene, and a simultaneous trace of 3D fixation on eye tracking - to generate the segmentation of objects with a corresponding association to high-level concepts. To test our framework, we present experiments in a scenario for manipulating objects at the table. Our results show that our model learns the user's idea of color and shape from a small number of physical demonstrations that generalize to the identification of physical references for novel combinations of words."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move to another world, in which they are able, in which they are able to integrate, in which they are able, and in which they are able to integrate."}, {"heading": "2 Methods", "text": "Figure 2 shows the architecture of the overall system. It consists of an end-to-end process, from raw voice and video input on the one hand, to learned meanings of symbols, which are grouped conceptually: i.e., a symbol can correspond either to an object in the real world or to a property of an object. The rest of the section is organized as follows - each subsection corresponds to a numbered transition (1 to 4) specified in Figure 2."}, {"heading": "2.1 Natural Language Semantic Parsing", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2.2 Grounding and Learning Instances through Demonstration and Eye tracking (GLIDE)", "text": "Penkov et al. (2017) present a framework for Grounding and Learning Instances by Demonstration and Eye Tracking (GLIDE), in which fixation programs are presented in the form of fixation tracks obtained in task demonstrations in combination with an overarching plan. Probabilistic conclusions about fixation programs can be used to derive latent properties of the environment and to determine locations in the environment that correspond to each instruction in an abstract input plan corresponding to the format discussed above - (Action Target Location)."}, {"heading": "2.2.1 3D Eye-Tracking", "text": "Mobile Eyetrackers provide fixation information in pixel coordinates corresponding to the positions in the image of a first-person view camera. To use information from multiple input sensors, an additional camera can be attached to extend an eyetracker by running a mono-camera SLAM algorithm in the background - ORBSLAM (MurArtal et al., 2015). The SLAM algorithm provides a 6D pose of the eye-tracking glasses within the space frame; this allows the fixation locations to be projected into the 3D world by beam casting and detecting intersections with a 3D model of the environment. As a result, fixations can be displayed as 3D locations, allowing the projection of fixations within any sensor in the environment."}, {"heading": "2.2.2 Model and Location Inference", "text": "In order to solve the problem of tolerance, the conclusion is made on the basis of a generative probability model presented in Figure 4. The sequence of fixations Y1: YT depends on both the current environmental state E and the performed action A. Each action is part of Plan P, which is determined by the performed task T. The sequence of fixations is observed and interpreted in relation to a task that is already known at that time, while the state of the environment and the current action are unknown. The most important consequence task is to determine the structure of the model and to assign each fixation to the action that is its cause. A crucial feature of this procedure is the fact that the distribution of fixations is different when a person participates in the execution of an action, compared with periods of transitions between actions in the plan, which is derived from human sensor imotor behavior."}, {"heading": "2.3 Feature Extraction", "text": "The parser generates a set of symbols S, and GLIDE generates a set of image fields I, each of which is labeled with a subset of symbols from S. We proceed by extracting a number of characteristics drawn from an already existing set F. The characteristics are derived from the objects in the image fields after the background has been removed, using a standard subtraction method - we know that most of the image will be occupied by a fixed object with a uniform color, everything else is a background. For example, the objects in the image fields in Figure 2 (c) are the colored blocks and the background the black stripes around them. Images that contain only or mostly background are considered and discarded as noise in the dataset. For each symbol, we group the extracted characteristics from each image labeled with s, resulting in S-lists of Ms tuples with F-entries in each tupel, with the number of Ms of the images being normalized (see figure 2 for each image)."}, {"heading": "2.4 Symbol Meaning Learning", "text": "Given that the process of object positioning in GLIDE may still produce disruptive results - i.e., the name of an image may be associated with the wrong symbol - we process our distributions to trace them back to data within two standard deviations from the means of the original distributions. We then look for observed characteristics f that are invariant in relation to any symbolic use of a particular symbol within the user instructions - i.e., their distributions are \"narrow\" and exhibit deviations below a predefined threshold (see Figure 5). If we have a set of images that consist of blue objects of different shapes and we extract a set of characteristics from them, we would expect that blue features with lesser characteristics (see Figure l) represent a deviation from the characteristics (see Figure 5)."}, {"heading": "3 Experiments", "text": "We are now presenting the results of the first experiments based on the framework in Figure 2. We are focusing our explanation on steps 3 and 4 in this figure, as these are the relevant and new elements presented here. Input data for Figure 2 (c) come from the process already well described in (Penkov et al., 2017)."}, {"heading": "3.1 Dataset", "text": "For our experiments, we used a total of six symbols that defined S: 3 for color (red, blue, and yellow) and 3 for shape (cell, block, cube). We used four extracted features for F: R, G, B values, and pixel range. The objects used were building blocks that can be stacked together and whose images were collected in a robot manipulation arrangement on the tabletop (see Figure 2 (a)). Based on the empirical statistics of the recognition process in (Penkov et al., 2017), our input data set for the symbol-reading algorithm consists of 75% correctly annotated and 25% incorrectly labeled images. The entire training data set consisted of approximately 2000 labeled image fields, each of which is labeled with two symbols - e.g. blue field, red block, yellow cube, etc. The additional test set was designed in two parts: one in which the color identification would be tested, and one in each of the 48 attributes previously assigned in the algorithm."}, {"heading": "3.2 Experimental Set up", "text": "The idea is to test how well the algorithm can distinguish the learned concepts with slight deviations from concepts it has not seen before: e.g. since the algorithm has been trained on 3 colors and 3 shapes, we would expect that it should recognize different colors of the 3 colors and objects with similar shapes to the original 3, but may not be able to recognize objects with completely different characteristics. In addition, we group different symbols into concept groups. If two symbols are described by the same characteristics, we can safely assume that these two symbols are mutually exclusive: that is, they cannot both describe an object at the same time."}, {"heading": "3.3 Results", "text": "The system successfully learns from the training data set that the color symbols are characterized by the extracted RGB values, while (in contrast) the shape symbols are characterized by the pixel range of the image field - see Figure 7. Faced with a new test image with its extracted characteristics, the algorithm recognizes 93% of the displayed colors and 56% of the displayed shapes. Tables 1 and 2 indicate the confusion matrices for the test set. This shows that the system is more robust in recognizing colors than it is in recognizing shapes. This is due to the fact that while RGB values describe the concept of color well enough, the pixel range alone is not enough to describe the concept of shape. Therefore, for example, the algorithm confuses the rubber duck with a cell and the arrow with a cube, see Figure 8, mainly because they are similar in size!"}, {"heading": "4 Discussion and Future Work", "text": "The experiments in this paper have shown that it is possible to train classifiers for the appearance of objects next to symbols, which are analyzed by a semantic parser, in order to achieve a grounding of instructions that respect the specificity of the scenario within which this association acquires its meaning. Although our framework supports a whole range of complex linguistic structures in the future, the scenarios presented could be simple and the specific methods could be (much) further developed. Nevertheless, we claim that this provides springboards for learning more complex linguistic structures: during the first few demonstrations, a person could use basic concepts such as colors, shapes, orientation and then this newly acquired knowledge to find prepositions (Forbes et al, 2015)."}, {"heading": "5 Conclusion", "text": "We present a framework for the use of intermodal input: a combination of natural voice instructions, video and eye-tracking streams to simultaneously perform semantic parsing and grounding of symbols used in this process within the physical environment, without dependence on existing object models that may not be particularly representative of the specifics of a particular user's contextual use and assignment of meaning within this rich multimodal stream. Instead, we present an online approach that exploits the pragmatics of human sensorimotor behavior to derive clues that enable the grounding of symbols to objects in the stream. Our preliminary experiments demonstrate the usefulness of this framework and show how a robot is able not only to learn the idea of a human's color and shape, but also that it is able to recognize these features in previously invisible objects from a small number of physical demonstrations."}, {"heading": "Acknowledgments", "text": "This work is partially supported by ERC grant 269427 (STAC), a grant from the Xerox University Affairs Committee, and grants EP / F500385 / 1 and BB / F529254 / 1 for the DTC in Neuroinformatics and Computational Neuroscience of the UK EPSRC, BBSRC and MRC. We are very grateful for the feedback from anonymous reviewers."}], "references": [{"title": "Natural language acquisition and grounding for embodied robotic systems", "author": ["M Al-Omari", "P Duckworth", "DC Hogg", "AG Cohn."], "venue": "Proceedings of the 31st Association for the Advancement of Artificial Intelligence. AAAI Press.", "citeRegEx": "Al.Omari et al\\.,? 2016", "shortCiteRegEx": "Al.Omari et al\\.", "year": 2016}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag."], "venue": "Research on Language and Computation 3(2-3):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Interpreting multimodal referring expressions in real time", "author": ["Miles Eldon", "David Whitney", "Stefanie Tellex."], "venue": "International Conference on Robotics and Automation.", "citeRegEx": "Eldon et al\\.,? 2016", "shortCiteRegEx": "Eldon et al\\.", "year": 2016}, {"title": "ERG semantic documentation", "author": ["Dan Flickinger", "Emily M. Bender", "Stephan Oepen."], "venue": "Accessed on 2017-04-29. http://www.delph-in.net/esd.", "citeRegEx": "Flickinger et al\\.,? 2014", "shortCiteRegEx": "Flickinger et al\\.", "year": 2014}, {"title": "Robot programming by demonstration with situated spatial language understanding", "author": ["Maxwell Forbes", "Rajesh PN Rao", "Luke Zettlemoyer", "Maya Cakmak."], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, pages", "citeRegEx": "Forbes et al\\.,? 2015", "shortCiteRegEx": "Forbes et al\\.", "year": 2015}, {"title": "Toward interactive grounded language acqusition", "author": ["Thomas Kollar", "Jayant Krishnamurthy", "Grant P Strimel."], "venue": "Robotics: Science and Systems.", "citeRegEx": "Kollar et al\\.,? 2013", "shortCiteRegEx": "Kollar et al\\.", "year": 2013}, {"title": "Learning from unscripted deictic gesture and language for human-robot interactions", "author": ["Cynthia Matuszek", "Liefeng Bo", "Luke Zettlemoyer", "Dieter Fox."], "venue": "AAAI. pages 2556\u20132563.", "citeRegEx": "Matuszek et al\\.,? 2014", "shortCiteRegEx": "Matuszek et al\\.", "year": 2014}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["Cynthia Matuszek", "Evan Herbst", "Luke Zettlemoyer", "Dieter Fox."], "venue": "Experimental Robotics. Springer, pages 403\u2013415.", "citeRegEx": "Matuszek et al\\.,? 2013", "shortCiteRegEx": "Matuszek et al\\.", "year": 2013}, {"title": "Tell me dave: Contextsensitive grounding of natural language to manipulation instructions", "author": ["Dipendra K Misra", "Jaeyong Sung", "Kevin Lee", "Ashutosh Saxena."], "venue": "The International Journal of Robotics Research 35(1-3):281\u2013300.", "citeRegEx": "Misra et al\\.,? 2016", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Modification", "author": ["Marcin Morzycki."], "venue": "Book manuscript. In preparation for the Cambridge University Press series Key Topics in Semantics and Pragmatics. http://msu.edu/ morzycki/work/book.", "citeRegEx": "Morzycki.,? 2013", "shortCiteRegEx": "Morzycki.", "year": 2013}, {"title": "ORB-SLAM: a versatile and accurate monocular SLAM system", "author": ["Ra\u00fal Mur-Artal", "J.M.M. Montiel", "Juan D. Tard\u00f3s."], "venue": "IEEE Transactions on Robotics 31(5):1147\u20131163. https://doi.org/10.1109/TRO.2015.2463671.", "citeRegEx": "Mur.Artal et al\\.,? 2015", "shortCiteRegEx": "Mur.Artal et al\\.", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng."], "venue": "Proceedings of the 28th international conference on machine learning (ICML11). pages 689\u2013696.", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Grounding the meaning of words through vision and interactive gameplay", "author": ["Natalie Parde", "Adam Hair", "Michalis Papakostas", "Konstantinos Tsiakas", "Maria Dagioglou", "Vangelis Karkaletsis", "Rodney D Nielsen."], "venue": "IJCAI. pages 1895\u20131901.", "citeRegEx": "Parde et al\\.,? 2015", "shortCiteRegEx": "Parde et al\\.", "year": 2015}, {"title": "Physical symbol grounding and instance learning through demonstration and eye tracking", "author": ["Svetlin Penkov", "Alejandro Bordallo", "Subramanian Ramamoorthy"], "venue": null, "citeRegEx": "Penkov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Penkov et al\\.", "year": 2017}, {"title": "Learning spatial relationships between objects", "author": ["Benjamin Rosman", "Subramanian Ramamoorthy."], "venue": "The International Journal of Robotics Research 30(11):1328\u20131342.", "citeRegEx": "Rosman and Ramamoorthy.,? 2011", "shortCiteRegEx": "Rosman and Ramamoorthy.", "year": 2011}, {"title": "Task and context determine where you look", "author": ["Constantin A Rothkopf", "Dana H Ballard", "Mary M Hayhoe."], "venue": "Journal of vision 7.", "citeRegEx": "Rothkopf et al\\.,? 2007", "shortCiteRegEx": "Rothkopf et al\\.", "year": 2007}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov."], "venue": "Advances in neural information processing systems. pages 2222\u20132230.", "citeRegEx": "Srivastava and Salakhutdinov.,? 2012", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2012}, {"title": "Learning to identify new objects", "author": ["Yuyin Sun", "Liefeng Bo", "Dieter Fox."], "venue": "Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE, pages 3165\u20133172.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Learning multi-modal grounded linguistic semantics by playing i spy", "author": ["Jesse Thomason", "Jivko Sinapov", "Maxwell Svetlik", "Peter Stone", "Raymond J Mooney."], "venue": "Proceedings of the Twenty-Fifth international joint conference on Artificial Intelligence", "citeRegEx": "Thomason et al\\.,? 2016", "shortCiteRegEx": "Thomason et al\\.", "year": 2016}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["Josh Tobin", "Rachel Fong", "Alex Ray", "Jonas Schneider", "Wojciech Zaremba", "Pieter Abbeel."], "venue": "arXiv preprint arXiv:1703.06907 .", "citeRegEx": "Tobin et al\\.,? 2017", "shortCiteRegEx": "Tobin et al\\.", "year": 2017}, {"title": "The physical symbol grounding problem", "author": ["Paul Vogt."], "venue": "Cognitive Systems Research 3(3):429\u2013457.", "citeRegEx": "Vogt.,? 2002", "shortCiteRegEx": "Vogt.", "year": 2002}, {"title": "A framework for learning semantic maps from grounded natural language descriptions", "author": ["Matthew R Walter", "Sachithra Hemachandra", "Bianca Homberg", "Stefanie Tellex", "Seth Teller."], "venue": "The International Journal of Robotics Research 33(9):1167\u2013", "citeRegEx": "Walter et al\\.,? 2014", "shortCiteRegEx": "Walter et al\\.", "year": 2014}, {"title": "Learning the spatial semantics of manipulation actions through preposition grounding", "author": ["Konstantinos Zampogiannis", "Yezhou Yang", "Cornelia Ferm\u00fcller", "Yiannis Aloimonos."], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Confer-", "citeRegEx": "Zampogiannis et al\\.,? 2015", "shortCiteRegEx": "Zampogiannis et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Being able to relate abstract symbols to observations with physical properties in the real world is known as the physical symbol grounding problem (Vogt, 2002); which is recognised as being one of the main challenges for human-robot interaction and constitutes the focus of this paper.", "startOffset": 147, "endOffset": 159}, {"referenceID": 21, "context": ", 2013) or fuse high-level natural language inputs with low-level sensory observations in order to produce a semantic map (Walter et al., 2014).", "startOffset": 122, "endOffset": 143}, {"referenceID": 4, "context": "Matuszek et al. (2014); Eldon et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "(2014); Eldon et al. (2016) and Kollar et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "(2014); Eldon et al. (2016) and Kollar et al. (2013) tackle learning symbol grounding in language commands combined with gesture", "startOffset": 8, "endOffset": 53}, {"referenceID": 6, "context": "colour and shape in (Matuszek et al., 2014).", "startOffset": 20, "endOffset": 43}, {"referenceID": 16, "context": "neural networks are also popular for grounding natural language instructions to the shared physical environment (Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011).", "startOffset": 112, "endOffset": 168}, {"referenceID": 11, "context": "neural networks are also popular for grounding natural language instructions to the shared physical environment (Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011).", "startOffset": 112, "endOffset": 168}, {"referenceID": 19, "context": "Tobin et al. (2017) potentially bypasses the need to collect a big dataset by demonstrating that a model trained in simulation can be successfully deployed on a robot in the real world.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Al-Omari et al. (2016) demonstrates a model for incrementally learning the visual representation of words, but relies on temporally aligned videos with corresponding annotated natural language inputs.", "startOffset": 0, "endOffset": 23}, {"referenceID": 18, "context": "(2015) and Thomason et al. (2016) represent the online concept learning problem as a variation of the interactive \u201cI Spy\u201d game.", "startOffset": 11, "endOffset": 34}, {"referenceID": 13, "context": "Penkov et al. (2017) introduce a method called GLIDE (see \u00a72.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "concept of intersective modification (Morzycki, 2013) \u2014i.", "startOffset": 37, "endOffset": 53}, {"referenceID": 3, "context": ", 2004), which are output by parsing the sentence with the wide-coverage English Resource Grammar (Flickinger et al., 2014), are used as an intermediate step in this mapping", "startOffset": 98, "endOffset": 123}, {"referenceID": 1, "context": "EDS are given as dependency graphs (Figure 3) and are a variable-free reduced form of the full Minimal Recursion Semantics (MRS) (Copestake et al., 2005) representation of the natural language input.", "startOffset": 129, "endOffset": 153}, {"referenceID": 13, "context": "Together with the raw video feed and the eye-tracking fixations, the abstract plan becomes an input to GLIDE (Penkov et al., 2017).", "startOffset": 109, "endOffset": 130}, {"referenceID": 15, "context": "physical symbol grounding is based on the idea that \u201ctask and context determine where you look\u201d (Rothkopf et al., 2007).", "startOffset": 96, "endOffset": 119}, {"referenceID": 13, "context": "and inference procedure can be found in (Penkov et al., 2017).", "startOffset": 40, "endOffset": 61}, {"referenceID": 13, "context": "(Penkov et al., 2017).", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "cal statistics of the recognition process in (Penkov et al., 2017), our input dataset to the Symbol Meaning Learning algorithm consists of 75% correctly annotated and 25% mislabelled images.", "startOffset": 45, "endOffset": 66}, {"referenceID": 4, "context": ", prepositional phrases (Forbes et al., 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al.", "startOffset": 24, "endOffset": 75}, {"referenceID": 14, "context": ", prepositional phrases (Forbes et al., 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al.", "startOffset": 24, "endOffset": 75}, {"referenceID": 8, "context": ", 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al., 2016; Zampogiannis et al., 2015) in an online and context specific manner.", "startOffset": 49, "endOffset": 96}, {"referenceID": 22, "context": ", 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al., 2016; Zampogiannis et al., 2015) in an online and context specific manner.", "startOffset": 49, "endOffset": 96}, {"referenceID": 17, "context": "in a hierarchical fashion (Sun et al., 2014).", "startOffset": 26, "endOffset": 44}], "year": 2017, "abstractText": "As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability\u2014for instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users\u2019 contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal input\u2014 i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations\u2014to produce the segmentation of objects with a correspondent association to high-level concepts. To test our framework we present experiments in a table-top object manipulation scenario. Our results show our model learns the user\u2019s notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.", "creator": "LaTeX with hyperref package"}}}