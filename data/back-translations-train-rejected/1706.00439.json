{"id": "1706.00439", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Tensor Contraction Layers for Parsimonious Deep Nets", "abstract": "Tensors offer a natural representation for many kinds of data frequently encountered in machine learning. Images, for example, are naturally represented as third order tensors, where the modes correspond to height, width, and channels. Tensor methods are noted for their ability to discover multi-dimensional dependencies, and tensor decompositions in particular, have been used to produce compact low-rank approximations of data. In this paper, we explore the use of tensor contractions as neural network layers and investigate several ways to apply them to activation tensors. Specifically, we propose the Tensor Contraction Layer (TCL), the first attempt to incorporate tensor contractions as end-to-end trainable neural network layers. Applied to existing networks, TCLs reduce the dimensionality of the activation tensors and thus the number of model parameters. We evaluate the TCL on the task of image recognition, augmenting two popular networks (AlexNet, VGG). The resulting models are trainable end-to-end. Applying the TCL to the task of image recognition, using the CIFAR100 and ImageNet datasets, we evaluate the effect of parameter reduction via tensor contraction on performance. We demonstrate significant model compression without significant impact on the accuracy and, in some cases, improved performance.", "histories": [["v1", "Thu, 1 Jun 2017 18:00:24 GMT  (586kb,D)", "http://arxiv.org/abs/1706.00439v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jean kossaifi", "aran khanna", "zachary c lipton", "tommaso furlanello", "anima anandkumar"], "accepted": false, "id": "1706.00439"}, "pdf": {"name": "1706.00439.pdf", "metadata": {"source": "CRF", "title": "Tensor Contraction Layers for Parsimonious Deep Nets", "authors": ["Jean Kossaifi", "Aran Khanna", "Zachary C. Lipton", "Tommaso Furlanello", "Anima Anandkumar"], "emails": ["jean.kossaifi@imperial.ac.uk", "arankhan@amazon.com", "zlipton@cs.ucsd.edu", "furlanel@usc.edu", "anima@amazon.com"], "sections": [{"heading": "1. Introduction", "text": "After their successful application to computers, speech recognition and natural speech processing, deep neural networks are able to better understand the success of deep neural networks [4]. A class of broad-based techniques within tendencies is a decomposition. While the properties of tendencies have been researched in the last ten years, they are able to learn in latent models [5], and the development of receiver systems is able to detect and compensate tendencies."}, {"heading": "1.1. Tensor Contraction", "text": "Notation: We define tensors as multidimensional arrays, which denote tensors of the first order v as vectors, tensors of the second order M as matrices and by X factor, referring to tensors of the third or greater order. M > denotes the transposition of M.Tensor unfolding: Given a tensor, X factor RD1 \u00b7 D2 \u00b7 \u00b7 DN, the unfolding of X factor is a matrix X [n] \u00b7 RDn, D (\u2212 n), with D (\u2212 n) = N k = 1, k 6 = n Dk and is determined by the assignment of element (d1, d2, \u00b7 \u00b7 \u00b7, dN) to (dn, e), with e \u00b7 K = 1, k 6 = n dk \u00b7 x-form of Dm = k Dm.n-mode of Dm.n-mode product: For a tensor X-type RDDDDDD1 \u00b7 Dk = 2 \u00b7 Dk \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n, n-type Dk = n \u00b7 n."}, {"heading": "1.2. Networks with Large fully connected layers", "text": "Many popular Convolutionary Neural Networks for Computer Vision, such as AlexNet, ResNet, and Inception, require hundreds of millions of parameters to achieve the reported results, which can be problematic when these networks are used to draw conclusions about resource-limited devices, where it may not be easy to perform hundreds of millions of calculations just to classify a single image. Although these widely used architectures have considerable diversity, they also have some similarities, often consisting of blocks of folding, activation, and pooling layers, followed by fully connected layers before the final classification layer. In VGG, both the popular AlexNet [14] and VGG [19] networks follow this meta architecture, both containing two fully connected layers of 4096 hidden units each. In both networks, these fully connected layers hold over 80 percent of the parameters. In VGG, the hidden units contain 119,545,856 of the 138,357,544 total cost, and 4,747 for both of these training units."}, {"heading": "2. Tensor Contraction Layer", "text": "In this paper, we propose to integrate tensor contraction into Convolutionary Neural Networks as an end-to-end trainable layer, applying it to the third activation sensor output through the last Convolutionary Layer. Specifically, with an X \u00b7 \u00b7 size (D1, \u00b7 \u00b7 \u00b7, DN) activation sensor, we are looking for a low-dimensional core G \u00b7 smaller size (R1, \u00b7 \u00b7 \u00b7, RN), so that: G = X \u00b7 \u00b7 V (1) \u00b7 size (2) \u00b7 \u00b7 \u00b7 N (N) (5) with V (k), RRk, k, k (1, \u00b7 \u00b7, N). We use this formulation and define a new layer that picks up the X \u00b7 layer from a previous layer and applies such a projection to it (Figure)."}, {"heading": "2.1. Complexity of the TCL", "text": "In this section we will detail the number of parameters and the complexity of the tensor contraction layer =. \u00b7 Number of parameters Let X be an activation tensor of size (D1, \u00b7 \u00b7 \u00b7, DN), which we pass through a size (R1, \u00b7 \u00b7, RN) tensor contraction layer (R1, \u00b7 \u00b7 \u00b7, RN).This TCL has a fully connected layer Nk = 1 Dk \u00b7 Rk parameters (according to the factors of N \u2212 fashion products) and produces as input a tensor of size (R1, \u00b7 \u00b7 \u00b7 \u00b7, RN).For comparison: a fully connected layer produces an output of the same size, i.e. units hidden with H = N k = 1 Rk and takes the same (flattensor) as input a sum of size (R1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, RN)."}, {"heading": "2.2. Incorporating TCL in a network", "text": "We see several simple ways to integrate the TCL into existing neural network architectures.TCL as an additional layer First, we can insert a layer to shrink the tensor after the last layer, which reduces the dimensionality of the activation tensor 2.2 before introducing it into the subsequent two fully connected layers and the softmax output of the network.Generally speaking, flattening leads to information loss. By applying tensor contraction, we reduce dimensionality efficiently by reducing the multidimensional dependencies in the activation tensor 2.3 TCL as a replacement for a fully connected layer. We can also integrate the TCL into existing architectures by completely replacing fully connected layers. This has the advantage that the number of parameters in our model is significantly reduced by reducing the multidimensional dependencies in the activation tensor. TCL as a replacement for a fully connected layer. We can also integrate the TCL into existing architectures by completely replacing fully connected layers 2.2 2.2 x 2.2 x 2.2. This has the advantage that the number of parameters in our model is significantly reduced by considering an activation tensor of the size (256, 7, 7, 7, 7, 7), 7, 7, 7, 7, 7, 7, 7, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.x x x x x x x x x x x x x x x x an activation tensor of a fully connected layer, or a completely connected one of the TCL (2.x x)."}, {"heading": "3. Experiments", "text": "Our experiments examine the representational power of the TCL and show results on the CIFAR100 dataset [13]. Subsequently, we present some preliminary results on the ImageNet 1k dataset [5]. We assume that a TCL can efficiently represent an activation sensor for processing by subsequent layers of the network, which allows a large reduction in parameters without a decrease in accuracy. We perform our investigation on CIFAR100 using the AlexNet [14] and VGG [19] architectures, each modified so that 32 \u00d7 32 images can be recorded as input. We also present the results with a traditional AlexNet on ImageNet. In all cases, we report on the accuracy (top-1) as well as on the space saved, which we quantify as: Space saving = 1 \u2212 nTCL noriginal, whereby the number of parameters in the fully connected layers of the standard network and TCL is noriginal."}, {"heading": "3.1. Results on CIFAR100", "text": "The CIFAR100 dataset consists of 100 classes, each containing 600 32 x 32 images, with 500 training images and 100 test images per class. In all cases, we report on the performance of the tests that have been set in terms of accuracy (Top1). We have implemented all models using the MXNet library, which has a larger input size, to adjust it for the CIFAR100 by adjusting the step size to the input layer of both networks, so that it accommodates 32 x 32 input images. We examine two sets of experiments that have a larger input size, in order to adjust it for the CIFAR100 by adjusting the input size to the input layer."}, {"heading": "3.1.1 Results", "text": "Table 1 summarizes our results on CIFAR100 using AlexNet, while the results are presented with VGG in Table 2. The first column shows the method, the second column shows whether a tensor contraction has been added and when it is, the size of the contracted core. Columns 3 and 4 show the number of hidden units in the fully connected layers or the size of the TCL used if relevant. Column 5 shows the top-1 accuracy of the validation. Finally, the last column shows the reduction factor in the number of parameters in the fully connected layers (which, as already mentioned, represent more than 80% of the total number of network parameters), with the reference being the original network without any modification (baseline). A first observation is that adding a tensor contraction layer (adding the TCL in Tables 1 and 2) has the minimum impact on the overall performance and simultaneously increases the consistency of the parameters."}, {"heading": "3.2. Results on ImageNet", "text": "In this section, we present preliminary experiments using the larger ILSVRC 2012 dataset (ImageNet) [5], using the AlexNet architecture.ImageNet consists of 1.2 million images for testing and 50,000 for validation, and includes 1,000 labeled classes. For these experiments, we trained each network simultaneously on 4 NVIDIA k80 GPUs using data parallelism and report preliminary results. We report top-1 accuracy of the validation set in all 1000 classes. All experiments were conducted under the same setup. Network architecture We use a standard AlexNet [14]. From the last revolutionary layer, we get an activation sensor of size (stack size, 256, 5, 5). As in the CIFAR100 case, we experiment with several variations of the tensor contraction layer. We first insert a TCL before the fully connected layer, either a larger-holding TCL (i.e., project a smaller layer, or a completely connected layer)."}, {"heading": "3.2.1 Results", "text": "In Table 3, we summarize the results of a standard AlexNet (baseline, first row) with an additional tensor contraction layer (Add TCL) that preserves or reduces the dimensionality of its input (line 2). We also report the result when the first fully joined layer is replaced by a TCL (1 TCL substitution, last line). Simply adding the TCL improves performance, while the increase in parameters in the fully joined layers is negligible. We achieve a similar performance by first adding a TCL to reduce the dimensionality of the activation sensor and reduce the number of hidden units in the fully connected layers, resulting in large space savings without reducing performance. Replacing the first fully connected layer with a size-saving TCL results in a similar space saving with the same performance as the standard network."}, {"heading": "4. Discussion", "text": "We have introduced a new neural network layer that performs tensor contraction on an activation tensor to obtain a low-dimensional representation of it. By using the natural multilinear structure of the data in the activation tensor, where each mode corresponds to a specific modality (i.e. the dimensions of the image and channels), we can reduce the size of the data representation that is passed on to subsequent layers in the network without compromising image recognition accuracy. TCL's greatest practical contribution is the drastic reduction in the number of parameters with little or no performance penalty, which also allows neural networks to perform faster inferences with fewer parameters by increasing their representational power."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D.J. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu", "C. Zhang", "Z. Zhang"], "venue": "CoRR, abs/1512.01274,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Sharing residual units through collective tensor factorization in deep neural networks. 2017", "author": ["Y. Chen", "X. Jin", "B. Kang", "J. Feng", "S. Yan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["N. Cohen", "O. Sharir", "A. Shashua"], "venue": "CoRR, abs/1509.05009,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "R. Vidal"], "venue": "CoRR, abs/1506.07540,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["S. Ioffe", "C. Szegedy"], "venue": "shift. CoRR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Generalization bounds for neural networks through tensor factorization", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": "CoRR, abs/1506.08473,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering", "author": ["A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "CoRR, abs/1511.06530,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM REVIEW,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I.V. Oseledets", "V.S. Lempitsky"], "venue": "CoRR, abs/1412.6553,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D. Vetrov"], "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Training input-output recurrent neural networks through spectral methods", "author": ["H. Sedghi", "A. Anandkumar"], "venue": "CoRR, abs/1603.00954,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Tensor contractions with extended blas kernels on cpu and gpu", "author": ["Y. Shi", "U.N. Niranjan", "A. Anandkumar", "C. Cecka"], "venue": "In 2016 IEEE 23rd International Conference on High Performance Computing (HiPC),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Deep multi-task representation learning: A tensor factorisation approach", "author": ["Y. Yang", "T.M. Hospedales"], "venue": "CoRR, abs/1605.06391,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Recently, tensor methods have been used in attempts to better understand the success of deep neural networks [4, 6].", "startOffset": 109, "endOffset": 115}, {"referenceID": 5, "context": "Recently, tensor methods have been used in attempts to better understand the success of deep neural networks [4, 6].", "startOffset": 109, "endOffset": 115}, {"referenceID": 0, "context": "While the properties of tensors have long been studied, in the past decade they have come to prominence in machine learning in such varied applications as learning latent variable models [1], and developing recommender systems [10].", "startOffset": 187, "endOffset": 190}, {"referenceID": 9, "context": "While the properties of tensors have long been studied, in the past decade they have come to prominence in machine learning in such varied applications as learning latent variable models [1], and developing recommender systems [10].", "startOffset": 227, "endOffset": 231}, {"referenceID": 16, "context": "Several recent papers apply tensor learning and tensor decomposition to deep neural networks for the purpose of devising neural network learning algorithms with theoretical guarantees of convergence [17, 9].", "startOffset": 199, "endOffset": 206}, {"referenceID": 8, "context": "Several recent papers apply tensor learning and tensor decomposition to deep neural networks for the purpose of devising neural network learning algorithms with theoretical guarantees of convergence [17, 9].", "startOffset": 199, "endOffset": 206}, {"referenceID": 19, "context": "Other lines of research have investigated practical applications of tensor decomposition to deep neural networks with aims including multi-task learning [20], sharing residual units [3], and speeding up convolutional neural networks [15].", "startOffset": 153, "endOffset": 157}, {"referenceID": 2, "context": "Other lines of research have investigated practical applications of tensor decomposition to deep neural networks with aims including multi-task learning [20], sharing residual units [3], and speeding up convolutional neural networks [15].", "startOffset": 182, "endOffset": 185}, {"referenceID": 14, "context": "Other lines of research have investigated practical applications of tensor decomposition to deep neural networks with aims including multi-task learning [20], sharing residual units [3], and speeding up convolutional neural networks [15].", "startOffset": 233, "endOffset": 237}, {"referenceID": 19, "context": "Several recent papers apply decompositions for either initialization [20] or post-training [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "Several recent papers apply decompositions for either initialization [20] or post-training [16].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "These techniques then often require additional fine-tuning to compensate for the loss of information [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "We refer the interested reader to the seminal work of Kolda and Bader [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "Both the popular networks AlexNet [14] and VGG [19] follow this meta-architecture, with both containing two fully-connected layers of 4096 hidden units each.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Both the popular networks AlexNet [14] and VGG [19] follow this meta-architecture, with both containing two fully-connected layers of 4096 hidden units each.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Notable work in this direction includes approaches to induce and exploit sparsity in the parameters during training [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 12, "context": "Our experiments investigate the representational power of the TCL, demonstrating results on the CIFAR100 dataset [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 4, "context": "Subsequently, we offer some preliminary results on the ImageNet 1k dataset [5].", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "We conduct our investigation on CIFAR100 using the AlexNet [14] and VGG [19] architectures, each modified to take 32 \u00d7 32 images as inputs.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "We conduct our investigation on CIFAR100 using the AlexNet [14] and VGG [19] architectures, each modified to take 32 \u00d7 32 images as inputs.", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "To avoid vanishing or exploding gradients, and to make the TCL more robust to changes in the initialization of the factors, we added a batch normalization layer [8] before and after the TCL.", "startOffset": 161, "endOffset": 164}, {"referenceID": 1, "context": "We implemented all models using the MXNet library [2] and ran all experiments training with data parallelism across multiple GPUs on Amazon Web Services, with two NVIDIA k80 GPUs.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "In this section, we present preliminary experiments using the larger ILSVRC 2012 (ImageNet) dataset [5], using", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": "Network architecture We use a standard AlexNet [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "In addition, recent work [18] has shown that new extended BLAS primitives can avoid transpositions needed to compute the tensor contractions.", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Tensors offer a natural representation for many kinds of data frequently encountered in machine learning. Images, for example, are naturally represented as third order tensors, where the modes correspond to height, width, and channels. Tensor methods are noted for their ability to discover multi-dimensional dependencies, and tensor decompositions in particular, have been used to produce compact low-rank approximations of data. In this paper, we explore the use of tensor contractions as neural network layers and investigate several ways to apply them to activation tensors. Specifically, we propose the Tensor Contraction Layer (TCL), the first attempt to incorporate tensor contractions as end-to-end trainable neural network layers. Applied to existing networks, TCLs reduce the dimensionality of the activation tensors and thus the number of model parameters. We evaluate the TCL on the task of image recognition, augmenting two popular networks (AlexNet, VGG). The resulting models are trainable end-to-end. Applying the TCL to the task of image recognition, using the CIFAR100 and ImageNet datasets, we evaluate the effect of parameter reduction via tensor contraction on performance. We demonstrate significant model compression without significant impact on the accuracy and, in some cases, improved performance.", "creator": "LaTeX with hyperref package"}}}