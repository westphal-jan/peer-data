{"id": "1706.00504", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Dynamic Stripes: Exploiting the Dynamic Precision Requirements of Activation Values in Neural Networks", "abstract": "Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial computation to offer performance that is proportional to the fixed-point precision of the activation values. The fixed-point precisions are determined a priori using profiling and are selected at a per layer granularity. This paper presents Dynamic Stripes, an extension to Stripes that detects precision variance at runtime and at a finer granularity. This extra level of precision reduction increases performance by 41% over Stripes.", "histories": [["v1", "Thu, 1 Jun 2017 21:57:32 GMT  (252kb,D)", "http://arxiv.org/abs/1706.00504v1", "3 pages, 3 figures"]], "COMMENTS": "3 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["alberto delmas", "patrick judd", "sayeh sharify", "reas moshovos"], "accepted": false, "id": "1706.00504"}, "pdf": {"name": "1706.00504.pdf", "metadata": {"source": "CRF", "title": "Dynamic Stripes: Exploiting the Dynamic Precision Requirements of Activation Values in Neural Networks", "authors": ["Alberto Delmas", "Patrick Judd", "Sayeh Sharify", "Andreas Moshovos"], "emails": ["moshovos}@ece.utoronto.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTIONOur previously described stripes [1] accelerator utilized the variable precision requirements of deep learning neural networks to improve performance and energy efficiency. In the previously disclosed design, the hardware expected the software to communicate the precision required by that layer before processing each layer. Therefore, this precision per layer was not adjusted at runtime to reflect an additional reduction in the precision that might be possible for each layer or even for smaller granularity. However, the underlying processing units are able to take advantage of precision on a much finer granularity than on a layer. In the implementation described above, each chip comprised 16 tiles, each processed 16 filters and 16 weights (synapses) per filter. A series of 256 activations where one bit per cycle is transferred to all tiles. For each layer, the precision per layer is the least of the activations, or the least of the signatures."}, {"heading": "II. SIMPLIFIED EXAMPLE", "text": "In fact, most of us are able to orient ourselves in a different direction from the direction in which we have gone."}, {"heading": "III. DESIGN", "text": "While in the previous section we considered a case where only four activations that form a single group are processed at the same time, other implementations can choose to process more activations in multiple groups. In the example described in Stripes, 256 activations are processed at the same time. They are transmitted into 16 tiles in which each tile processes 16 filters and 16 weights per tile. Each tile has a grid of 16x16 Serial Inner product units or SIPs. SIPs along the same line process process process process process process process process process 16 weights, while the SIPs along the same column process the same process of 16 activations. Accordingly, in this implementation the precision p, specified as (nH, nL) can be dynamically set for the entire group of 256 activations that are processed at the same time."}, {"heading": "A. Modified Serial Inner-Product Unit", "text": "Figure 3 shows an implementation of the modified SerialInner Product Unit. An additional shifter appears at the output of the adder tree. This shifter makes it possible to adjust the output of the adder tree to the correct bit position through the control signal sB, so that it can be cumulated with the current sum held at battery A."}, {"heading": "IV. EVALUATION: PERFORMANCE", "text": "Table I shows the resulting performance improvements of a suitably configured DaDianNao [3] accelerator. Performance measurements were determined using our cycle-accurate simulator, which was expanded to support dynamic detection of activation precision. To achieve these results, we first set the precision per layer, as in the original stripes design, and then the precision was dynamically detected at runtime per subgroup of 16 activations processed at the same time. All 16 subgroups of each group of 256 activations processed at the same time had to stop processing before advancing to the next group of 256 activations. Dynamic Stripes design improves performance through the design of the Stripes baseline, lagging behind the Pragmatic [4] design, which skips zero bits rather than attempting to exploit precision alone."}, {"heading": "V. A NOVEL WAY TO ADJUST PRECISION WITH PRAGMATIC", "text": "In the pragmatic motor motivated by stripes, performance depends on the number of activation bits that are 1. This motor opens the possibility for value-based optimizations for power and energy efficiency, where activation values are adjusted in flight to reduce the number of bits that are 1. For example, we can set a threshold per layer for the number of most significant powers of 2 (MSP2) that should be maintained to maintain accuracy. [2] T. Xanthopoulos and A. P. Chandrakasan, \"A low-power Dct technique uses adaptive bit width and arithmetic activity uses signal correlations and quantization,\" IEEE Journal of Solid-State Circuits, Volume 35, pp. 740-750, May 2000. For example, if an activation has a value of 1010 0101 and MSP2, activation 3, activation can be switched to 1010 0100, whereas effectiveness is switched to 0000."}], "references": [{"title": "Bitpragmatic deep neural network computing", "author": ["P. Judd", "J. Albericio", "A. Moshovos", "\u201cStripes: Bit-serial deep neural network computing", "\u201d Computer Architecture Letters", "2016. [3] Y. Chen", "T. Luo", "S. Liu", "S. Zhang", "L. He", "J. Wang", "L. Li", "T. Chen", "Z. Xu", "N. Sun", "O. Temam", "\u201cDaDianNao: A machine-learning supercomputer", "\u201d in Microarchitecture (MICRO)", "2014 47th Annual IEEE/ACM International Symposium on", "pp. 609\u2013622", "Dec 2014. [4] J. Albericio", "P. Judd", "A. Delm\u00e1s", "S. Sharify", "A. Moshovos"], "venue": "CoRR, vol. abs/1610.06920, 2016. 3", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Our previously described Stripes [1] accelerator exploited the variable precision requirements of deep learning neural networks to improve performance and energy efficiency.", "startOffset": 33, "endOffset": 36}], "year": 2017, "abstractText": "Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial computation to offer performance that is proportional to the fixed-point precision of the activation values. The fixed-point precisions are determined a priori using profiling and are selected at a per layer granularity. This paper presents Dynamic Stripes, an extension to Stripes that detects precision variance at runtime and at a finer granularity. This extra level of precision reduction increases performance by 41% over Stripes.", "creator": "LaTeX with hyperref package"}}}