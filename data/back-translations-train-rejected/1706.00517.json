{"id": "1706.00517", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "CATERPILLAR: Coarse Grain Reconfigurable Architecture for Accelerating the Training of Deep Neural Networks", "abstract": "Accelerating the inference of a trained DNN is a well studied subject. In this paper we switch the focus to the training of DNNs. The training phase is compute intensive, demands complicated data communication, and contains multiple levels of data dependencies and parallelism. This paper presents an algorithm/architecture space exploration of efficient accelerators to achieve better network convergence rates and higher energy efficiency for training DNNs. We further demonstrate that an architecture with hierarchical support for collective communication semantics provides flexibility in training various networks performing both stochastic and batched gradient descent based techniques. Our results suggest that smaller networks favor non-batched techniques while performance for larger networks is higher using batched operations. At 45nm technology, CATERPILLAR achieves performance efficiencies of 177 GFLOPS/W at over 80% utilization for SGD training on small networks and 211 GFLOPS/W at over 90% utilization for pipelined SGD/CP training on larger networks using a total area of 103.2 mm$^2$ and 178.9 mm$^2$ respectively.", "histories": [["v1", "Thu, 1 Jun 2017 22:58:37 GMT  (4011kb,D)", "https://arxiv.org/abs/1706.00517v1", "10 pages, 10 figures, ASAP 2017: The 28th Annual IEEE International Conference on Application-specific Systems, Architectures and Processors"], ["v2", "Thu, 8 Jun 2017 15:30:54 GMT  (4006kb,D)", "http://arxiv.org/abs/1706.00517v2", "ASAP 2017: The 28th Annual IEEE International Conference on Application-specific Systems, Architectures and Processors"]], "COMMENTS": "10 pages, 10 figures, ASAP 2017: The 28th Annual IEEE International Conference on Application-specific Systems, Architectures and Processors", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.NE", "authors": ["yuanfang li", "ardavan pedram"], "accepted": false, "id": "1706.00517"}, "pdf": {"name": "1706.00517.pdf", "metadata": {"source": "CRF", "title": "CATERPILLAR: Coarse Grain Reconfigurable Architecture for Accelerating the Training of Deep Neural Networks", "authors": ["Yuanfang Li", "Ardavan Pedram"], "emails": ["yli03@stanford.edu", "perdavan@stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Multilayer Perceptron (MLP)", "text": "The main purpose of the MLP is to output a prediction (generally a classification label) for a given input = 1. Figure 1 shows a three-layer MLP, where x is the input, y \u00b2 is the output, and h1 and h2 are the activation of the first and second hidden layer, respectively. Wi is the set of weights, so that Wi (j, k) transforms the weight from the jth element of the input to the kth element of the output on layer i. At each neuron of a hidden layer, a nonlinear activation function f is executed on the sum of the weighted inputs. On the output layer, the softmax function y transforms into a vector of the probabilities for the possible classes. A bias can be added by appending a + 1 term to the input vector on each layer. The network is trained on input label pairs (x, y), where x is the input as before and y is represented as the correct label."}, {"heading": "2.1. Stochastic/Minibatch Gradient Descent (SGD/MBGD)", "text": "In SGD (Figure 2 (a)), each training sample goes through the forward and reverse gears and updates the weights. Thus, a single pass through a training set of size K leads to K-updates of the weights [8]. MBGD (Figure 2 (b)) is a variant of SGD that groups training samples into \"minibatches.\" The weights are updated after each minibatch passes through the network, so that a single pass through the training set leads to K / b weight updates, with b being the size of the minibatch. This allows some parallelization of the training process, as multiple samples can be processed at once."}, {"heading": "2.2. Feedback Alignment (FA)", "text": "MLPs are designed to mimic the functioning of the brain, but a BP algorithm such as SGD is biologically implausible as it requires a weight symmetry in the forward and backward trajectories. To get around this, Lillicrap et al. suggest the use of a fixed random feedback weight, Bi, for each layer during the backward trajectory [15]. The change in learning is: e = y-y-y-y-2 = eB T 2 f \u00b2 (h2) \u03b41 = \u03b42B T 1 f \u00b2 (h1) Despite the use of random feedback weights, FA may perform just as well or better than standard MBGD. However, it is necessary to either use the stack normalization or significantly reduce the learning rate to prevent the gradients from becoming too large, especially for deep networks [10]."}, {"heading": "2.3. Direct Feedback Alignment (DFA)", "text": "DFA (Figure 2 (c) [16]) propagates the weights of the last layer back to all previous layers as follows: e = y-y \u03b42 = eB T 2 f \"(h2) \u03b41 = eB T 1 f\" (h1) Like FA, DFA also requires either a batch normalization or a lower learning rate. However, the dimension of the output layer for a typical neural network is significantly smaller than the hidden layers. This means that the bi-layers will be significantly smaller than the corresponding Wi-layers, reducing the number of calculations required to reverse propagate the error by several orders of magnitude. Figure 2 (c) shows that DFA also exhibits parallels during the reverse because the error can be calculated for all layers at once."}, {"heading": "2.4. Pipelined/Continuous Propagation ((MB)CP)", "text": "Continuous propagation allows parallelization across layers by easing the restriction that a complete forward and backward pass is required for each group of samples before the weights can be updated [17]. In CP, forward and backward passes through a layer can take place simultaneously - in particular, the weights can be applied to the current sample as it passes forward, while the previous sample updates the weights as it passes backwards. Figure 2 (d) shows the CP algorithm as the samples are distributed through the layer and time. Once the pipeline is initialized, all layers work simultaneously."}, {"heading": "3. CATERPILLAR Architecture", "text": "This section motivates the design of the CATERPILLAR architecture by showing the available parallelism and location for each of the existing MLP training algorithms in Section 2. Building on this, we propose an architecture that provides the necessary computing and communication functionality."}, {"heading": "3.1. Discussion of Various Algorithms with Respect to Locality and Parallelism", "text": "The formation of the neural network requires a series of GEMV and / or GEMM operations. There are two cases to consider: (1) The weights of the entire network fit into the local memory and (2) it does not fit and some of the weights must be activated. However, in the following we briefly describe different methods of exploiting parallelism and locality in the different training methods. In the SGD algorithm, only a single sample will pass through the entire network at once. Therefore, the calculation core of this algorithm is activated, which is memory-bound and inefficient, especially if the entire network does not fit into the aggregated local memories of each core. For a matrix ofsize m \u00d7 n GEMV, it requires mn weight processes to produce n elements of the output. Thus, each of the proposed variants aimed at BP to overcome this disadvantage by introducing and exploiting various sources of parallelism and local."}, {"heading": "3.2. CGRA for Training", "text": "The shown locality and parallelism sensing shows that for networks that do not fit into the local memory of the cores, an architecture optimized for GEMM will work well by performing a minibatch learning algorithm. However, the same architecture must also support the high communication requirements of the GEMV operation if the network is small enough to be stored locally. In this case, either SGD or CP can be used to train the network without causing memory access and communication costs.Since GEMM and GEMV naturally use the same inner core, an architecture inspired by an array of linear algebra cores (LACs) [18], [23] shows how well the LAC will work. The LAC consists of nr \u00d7 nr processing elements (PEs). Each PE contains a half precision floating point that multiplies the unit (FPU) and a local SRAM."}, {"heading": "3.3. Mapping of Various Learning Methods", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3.4. Architectural Tradeoffs", "text": "In the eeisn eeisrcehnn eeisrVnlrgteeu nvo the eeisrsn eeisrBnlrgte\u00fceerb rf\u00fc ide eeisrg\u00dfe\u00fccnh-eaJnlhsrteeaeaJnlrh ni the eeisn-eaJng0e0e0e0r0e0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0"}, {"heading": "4. Evaluations", "text": "To investigate the interaction between algorithms and architecture, we are conducting two classes of studies: First, we are investigating the convergence rate of different methods in relation to each other; second, we are investigating how this rate is implemented in architecture mapping and how existing parallelism and locality affect the energy and speed of convergence; and third, we are evaluating the proposed architecture and its various features such as memory size, number of PEs per nucleus, memory per PE and number of cores in terms of different learning approaches."}, {"heading": "4.1. Methodology", "text": "Networks, Data Sets, and Algorithms: We explore different network sizes and learning methods that have been tested on a subset of the MNIST dataset to determine convergence and accuracy, and all networks use ReLU to activate the hidden layer. As the networks are trained on only a subset of the complete MNIST dataset, the accuracies achieved are lower. As the purpose of this study is to compare different networks and learning methods and not achieve the best possible accuracy, the difference between the results of convergence and the accuracies achieved by the different networks and learning algorithms shows a similar behavior for the complete dataset."}, {"heading": "4.2. Software Experimental Results", "text": "The network in Figure 5 (a) is small enough that even SGD and CP require many epochs to achieve convergence, although the accuracy achieved is higher than with other algorithms. In Figure 5 (b), the additional hidden layer causes the epochs of convergence for SGD and CP to fall by 60%, so that they converge faster and with a higher accuracy than the minibatch algorithms. In general, SGD and CP are able to achieve the highest accuracy in the fewest epochs of all algorithms, as the weights for each sample in an epoch are updated once. CP also performs as well or better than SGD in all cases, although this is not true for the MBCP."}, {"heading": "4.3. Architecture Experimental Results", "text": "Figures 6-8 show the energy requirements for three networks, as well as the division into FPU energy and storage access energy. Broadcast energy proved negligible and is not considered here. Network 1 in Figure 6 is small and fits fully into local core memory for all configurations. At the same accuracy of 90%, SGD consumes 70% of the energy as MBGD for large minibatches, while CP requires 30%. Investigating energy breakdowns shows that in minibatch algorithms, energy consumption is dominated by the FPU, while energy for SGD storage access energy is 1.5 times higher than FPU energy. Energy costs for CP are evenly distributed between FPU and storage access. This is because both are minibatched."}, {"heading": "35.4566.55 38.38", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is not a country, but a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which"}, {"heading": "5. Related Work", "text": "The maximum performance of up to 10 G Mul-tiply Accumulates (GMACs) is achieved in [32]. However, this work is limited in scope as it focuses either on retraining [30] or on flat 2-layer neural networks [31]. Furthermore, there is no support for implementing different learning algorithms as in the case of Caterpillar Architecture.Gupta et al. have shown that 16-bit fixed accuracy can achieve the same accuracy as floating point. Our preliminary studies suggest that convergence rate and accuracy decreases in networks deeper than two layers of stochastic rounding. Further studies are needed to improve the performance of stochastic rounding compared to floating point. Work in [33] showed that CP can exceed the speed and accuracy of MBGD in CNNs."}, {"heading": "6. Conclusion", "text": "Our study of MLP training shows that the target architecture for different network sizes should support both GEMV (for pipelined back propagation), GEMM (for minibatch algorithms) and hierarchical collective communication. For networks that do not fit the chip, minibatch algorithms perform as well as pipelined back propagation, but for networks that do fit, pipelined back propagation consistently performs best. Fast algorithmic convergence coupled with parallelization of layers and weight localization from an architectural point of view allows Pipelined Continuous Propagation to outperform all other training methods in terms of energy and time to convergence, which makes it a promising training method for use with specialized deep learning architectures."}, {"heading": "Acknowledgments", "text": "We thank Hadi Esmaeilzadeh, Michael James, David Koeplinger, Ilya Sharapov, Vijay Korthikanti and Sara O'Connell for their feedback on the manuscript. This research was partly funded by NSF funding CCF1563113. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation (NSF)."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "NIPS, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov"], "venue": "in Interspeech,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "30 years of adaptive neural networks: perceptron, madaline, and backpropagation", "author": ["B. Widrow"], "venue": "Proceedings of the IEEE, vol. 78, no. 9, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Efficient processing of deep neural networks: A tutorial and survey", "author": ["V. Sze"], "venue": "arXiv preprint arXiv:1703.09039, 2017.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "In-datacenter performance analysis of a tensor processing unit", "author": ["N. Jouppi"], "venue": "ISCA44. IEEE Press, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Large scale distributed deep networks", "author": ["J. Dean"], "venue": "NIPS, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural networks and systolic array design", "author": ["D. Zhang"], "venue": "Series in Machine Perception and Artificial Intelligence, vol. 49, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart"], "venue": "DTIC Document, Tech. Rep., 1985.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1985}, {"title": "Dark memory and accelerator-rich system optimization in the dark silicon era", "author": ["A. Pedram"], "venue": "IEEE Design & Test, vol. 34, no. 2, pp. 39\u201350, 2017.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "How important is weight symmetry in backpropagation?", "author": ["Q. Liao"], "venue": "arXiv preprint arXiv:1510.05067,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep learning with limited numerical precision.", "author": ["S. Gupta"], "venue": "in ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht"], "venue": "NIPS, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "EIE: efficient inference engine on compressed deep neural network", "author": ["S. Han"], "venue": "ISCA43. IEEE Press, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep big simple neural nets excel on handwritten digit recognition", "author": ["D.C. Ciresan"], "venue": "Neural Computation, vol. 22, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Random feedback weights support learning in deep neural networks", "author": ["T.P. Lillicrap"], "venue": "arXiv preprint arXiv:1411.0247, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Direct feedback alignment provides learning in deep neural networks", "author": ["A. N\u00f8kland"], "venue": "NIPS, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Systolic implementation of a pipelined on-line backpropagation", "author": ["R.G. Giron\u00e9s"], "venue": "MicroNeuro. IEEE, 1999, pp. 387\u2013394.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "A high-performance, low-power linear algebra core", "author": ["A. Pedram"], "venue": "ASAP. IEEE, 2011, pp. 35\u201342.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li"], "venue": "20th ACM SIGKDD. ACM, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous stochastic gradient descent for DNN training", "author": ["S. Zhang"], "venue": "IEEE ICASSP, 2013, pp. 6660\u20136663.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Memory-efficient backpropagation through time", "author": ["A. Gruslys"], "venue": "CoRR, 2016. [Online]. Available: http://arxiv.org/abs/1606.03401", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Codesign tradeoffs for high-performance, lowpower linear algebra architectures", "author": ["A. Pedram"], "venue": "IEEE Transactions on Computers, Special Issue on Power efficient computing, vol. 61, no. 12, pp. 1724\u2013 1736, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Collective communication: theory, practice, and experience", "author": ["E. Chan"], "venue": "Concurrency and Computation: Practice and Experience, vol. 19, no. 13, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "High-performance hardware for function generation", "author": ["J. Cao"], "venue": "IEEE ARITH. IEEE, 1997, pp. 184\u2013188.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Faithful powering computation using table lookup and a fused accumulation tree", "author": ["J.-A. Pi\u00f1eiro"], "venue": "IEEE ARITH, 2001, pp. 40\u201347.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Floating point architecture extensions for optimized matrix factorization", "author": ["A. Pedram"], "venue": "IEEE ARITH. IEEE, 2013, pp. 49\u201358.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Architecting efficient interconnects for large caches with CACTI 6.0", "author": ["N. Muralimanohar"], "venue": "IEEE Micro, vol. 28, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "FPU generator for design space exploration", "author": ["S. Galal"], "venue": "IEEE ARITH, 2013, pp. 25\u201334.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "FPGA based implementation of deep neural networks using on-chip memory only", "author": ["J. Park"], "venue": "IEEE ICASSP, 2016, pp. 1011\u20131015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Design of artificial neural network architecture for handwritten digit recognition on FPGA", "author": ["V.T. Huynh"], "venue": "2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "A massively parallel digital learning processor", "author": ["H.P. Graf"], "venue": "NIPS, 2009, pp. 529\u2013536.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Continuous propagation: layer parallelism for training deep networks", "author": ["M. James"], "venue": "2017.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "State of the art Deep Neural Networks (DNNs) are becoming deeper and can be applied to a range of sophisticated cognitive tasks such as image recognition [1] and natural language processing [2].", "startOffset": 154, "endOffset": 157}, {"referenceID": 1, "context": "State of the art Deep Neural Networks (DNNs) are becoming deeper and can be applied to a range of sophisticated cognitive tasks such as image recognition [1] and natural language processing [2].", "startOffset": 190, "endOffset": 193}, {"referenceID": 0, "context": "Convolutional Neural Networks (CNNs) [1] and Recurrent Neural Networks (RNNs) [2] are some of the commonly used network architectures that are inspired by the Multilayer Perceptron (MLP) [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Convolutional Neural Networks (CNNs) [1] and Recurrent Neural Networks (RNNs) [2] are some of the commonly used network architectures that are inspired by the Multilayer Perceptron (MLP) [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) [1] and Recurrent Neural Networks (RNNs) [2] are some of the commonly used network architectures that are inspired by the Multilayer Perceptron (MLP) [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "Most of the community has focused on acceleration of the forward path/inference for DNNs, neglecting the acceleration for training [4], [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 4, "context": "Most of the community has focused on acceleration of the forward path/inference for DNNs, neglecting the acceleration for training [4], [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Training DNNs is a performance and energy costly operation that routinely takes weeks or longer on servers [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "However the nature of computation in training DNNs makes it an excellent candidate for specialized acceleration if the necessary computation/communication functionality is supported [7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "Today, acceleration of the training process is primarily performed on GPUs [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "However, GPUs suffer from fundamental computation, memory, and bandwidth imbalance in their memory hierarchy [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "We focus on training MLPs, an important class of DNNs currently used on state of the art servers [5], with several variants of Backpropagation (BP) [8] learning.", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "We focus on training MLPs, an important class of DNNs currently used on state of the art servers [5], with several variants of Backpropagation (BP) [8] learning.", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "First, MLPs are inherently memory bound and more challenging to accelerate [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Second, several research studies on the principles of BP and optimization in DNNs investigate MLPs because of their simpler network-architecture [10], [11], [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "Second, several research studies on the principles of BP and optimization in DNNs investigate MLPs because of their simpler network-architecture [10], [11], [12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "Second, several research studies on the principles of BP and optimization in DNNs investigate MLPs because of their simpler network-architecture [10], [11], [12].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "Finally, MLPs represent the fully-connected layers of CNNs [1].", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "However, as the size of the network shrinks, and is able to fit on local storage, the overall cost of communication drops [13].", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "Although MLP is a simpler neural network compared to CNNs and RNNs, by increasing the size and number of the hidden layers as well as the number of training samples, it performs as well as other complex models on tasks like digit classification [14].", "startOffset": 245, "endOffset": 249}, {"referenceID": 7, "context": "Thus, a single pass through a training set of size K results in K updates to the weights [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "propose the use of a fixed random feedback weight, Bi, for each layer during the backward pass [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "However, it is necessary to either use batch normalization or significantly lower the learning rate to prevent the gradients from growing too large, especially for deep networks [10].", "startOffset": 178, "endOffset": 182}, {"referenceID": 15, "context": "DFA (Figure 2(c) [16]) backpropagates the last layer\u2019s weights to all previous layers as follows:", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "Continuous propagation allows parallelization across layers by relaxing the constraint that a full forward and backward pass through the network is required for each set of samples before the weights can be updated [17].", "startOffset": 215, "endOffset": 219}, {"referenceID": 18, "context": "In practice, minibatch sizes on nodes range from 2 to 100 [19] and can go up to 10,000+ on clusters [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "In practice, minibatch sizes on nodes range from 2 to 100 [19] and can go up to 10,000+ on clusters [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "This issue is more evident in CNNs than MLPs, as they are usually deeper and have huge sample sizes [21].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "This issue is mitigated with reverse checkpointing and recomputing the activations in earlier layers [22].", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": "CATERPILLAR: (a) Array of cores with ring communication; (b) core with 16\u00d7 16 PEs connected to column and row broadcasts; (c) PE [18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "Since GEMM and GEMV inherently use the same inner kernel, an architecture inspired by an array of Linear Algebra Cores (LACs) [18], [23] will perform well.", "startOffset": 126, "endOffset": 130}, {"referenceID": 22, "context": "Since GEMM and GEMV inherently use the same inner kernel, an architecture inspired by an array of Linear Algebra Cores (LACs) [18], [23] will perform well.", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "Within cores, GEMM occurs as described in [18], but cores must also now be able to pass results to each other between layers.", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "In order to make the complete output Y available to all cores as input for the next layer, an all-gather [24] operation is performed between layers where each core passes its Yi to the next, using the ring communication.", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "To obtain the final output a reduce-scatter [24] operation is performed.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "also showed that the use of feedback alignment led to no performance improvement over traditional gradient descent when applied to a MLP network trained on TIMIT dataset [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "Activation Function: For all algorithms, to calculate the nonlinear activations at each layer, Goldschmidt\u2019s method [25] [26] [27] is used, which can be implemented with a lookup table and the existing FPU in the PE.", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "Activation Function: For all algorithms, to calculate the nonlinear activations at each layer, Goldschmidt\u2019s method [25] [26] [27] is used, which can be implemented with a lookup table and the existing FPU in the PE.", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "Activation Function: For all algorithms, to calculate the nonlinear activations at each layer, Goldschmidt\u2019s method [25] [26] [27] is used, which can be implemented with a lookup table and the existing FPU in the PE.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "Each PE has 16KB of local memory [18] and each core has 512 KB of private SPAD memory.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "Energy and area values for memories, wires, and look-up tables were obtained and estimated from [9] and CACTI [28] respectively.", "startOffset": 96, "endOffset": 99}, {"referenceID": 27, "context": "Energy and area values for memories, wires, and look-up tables were obtained and estimated from [9] and CACTI [28] respectively.", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "The Half-Precision FPU area and energy were obtained from [29].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "Several FPGA implementation efforts have been performed to accelerate the training of neural networks [30] [31] [32].", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "Several FPGA implementation efforts have been performed to accelerate the training of neural networks [30] [31] [32].", "startOffset": 107, "endOffset": 111}, {"referenceID": 31, "context": "Several FPGA implementation efforts have been performed to accelerate the training of neural networks [30] [31] [32].", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "tiply Accumulates (GMACs) is achieved in [32].", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "However, these works are limited in scope as they focus on either retraining [30] or shallow 2-layer neural networks [31].", "startOffset": 77, "endOffset": 81}, {"referenceID": 30, "context": "However, these works are limited in scope as they focus on either retraining [30] or shallow 2-layer neural networks [31].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "have shown that 16-bit fixed precision can achieve the same accuracy as floating-point if stochastic rounding is used [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 32, "context": "The work in [33] showed that CP can outperform MBGD\u2019s speed and accuracy for CNNs.", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "Accelerating the inference of a trained DNN is a well studied subject. In this paper we switch the focus to the training of DNNs. The training phase is compute intensive, demands complicated data communication, and contains multiple levels of data dependencies and parallelism. This paper presents an algorithm/architecture space exploration of efficient accelerators to achieve better network convergence rates and higher energy efficiency for training DNNs. We further demonstrate that an architecture with hierarchical support for collective communication semantics provides flexibility in training various networks performing both stochastic and batched gradient descent based techniques. Our results suggest that smaller networks favor non-batched techniques while performance for larger networks is higher using batched operations. At 45nm technology, CATERPILLAR achieves performance efficiencies of 177 GFLOPS/W at over 80% utilization for SGD training on small networks and 211 GFLOPS/W at over 90% utilization for pipelined SGD/CP training on larger networks using a total area of 103.2 mm and 178.9 mm respectively.", "creator": "LaTeX with hyperref package"}}}