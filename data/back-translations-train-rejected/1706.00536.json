{"id": "1706.00536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Latent Attention Networks", "abstract": "Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.", "histories": [["v1", "Fri, 2 Jun 2017 02:10:39 GMT  (246kb,D)", "http://arxiv.org/abs/1706.00536v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["christopher grimm", "dilip arumugam", "siddharth karamcheti", "david abel", "lawson l s wong", "michael l littman"], "accepted": false, "id": "1706.00536"}, "pdf": {"name": "1706.00536.pdf", "metadata": {"source": "CRF", "title": "Latent Attention Networks", "authors": ["Christopher Grimm", "Dilip Arumugam", "Siddharth Karamcheti", "David Abel", "Lawson L.S. Wong", "Michael L. Littman"], "emails": ["michael_littman}@brown.edu"], "sections": [{"heading": null, "text": "Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing the inner mechanisms of any neural network and its power and limitations. Our data-set-centric method generates visualizations of how a trained network looks for components of its inputs. The calculated \"attention masks\" support enhanced interpretative capability by highlighting which input attributes are critical to determining output. We demonstrate the effectiveness of our framework on a variety of deep-neural network architectures in domains of computer vision, natural language processing, and enhanced learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process regardless of data modality."}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 Related Work", "text": "In recent years, the number of those able to communicate within a network has increased significantly. [19, 10, 2] Typically, the additional attention scheme provides an informative approach that can facilitate learning in a complex, highly structured output space (such as machine translation). Similarly, we use visual attention networks to survey existing content-based attention models to improve performance in a variety of monitored learning tasks, including speech recognition, machine translation, image labeling, and more. [27] We also use stacked attention networks to answer natural language questions via images, and Goyal et al. [9] investigate a complementary method for networks specifically designed to answer questions about visual content; their approach visualizes the network response."}, {"heading": "3 Method", "text": "An essential distinguishing feature of our approach is that we assume minimal knowledge of the network to be visualized. We only require that the network F: Rd 7 \u2192 R 'be made available as a black box function (i.e. we can provide input x to F and receive output F (x), through which gradients can be calculated. As we do not have access to the network architecture, we can only probe the network on its input or output. Specifically, our strategy is to modify the input by selectively replacing components with an attention mask generated by an learned latent Attention Network (LAN)."}, {"heading": "3.1 Latent Attention Network Framework", "text": "A latent attention network is a function A: Rd 7 \u2192 [0, 1] d, which produces an attention mask A (x) of the same shape as x in the face of an input x (for the original network F). The attention mask attempts to identify input components of x that are critical to the production of output F (x). Likewise, the attention mask determines the degree to which each component of x can be corrupted by noise while minimally affecting F (x). (1) To formalize this term, we need two additional design components: LF: R '\u00d7 R '7 \u2192 R a loss function in the output space of F, H: Rd 7 \u2192 R a noise density over the input space of F. (1) We can now complete the specification of the LAN framework. As illustrated in Figure 1, if we give an input value x, we draw a loud vector of A followed by a corx (x)."}, {"heading": "3.2 Latent Attention Network Design", "text": "To define a LAN, we need to provide two components: the loss function LF and the noise distribution H. The choice of these two components depends on the respective visualization task. Typically, the loss function LF is the same as the one F was trained with even though it is not necessary. For example, if a Network F was pre-trained and used as a black box function for a slightly different task, one can visualize the latent attention to the loss of the new task that can be used to check that F takes into account expected parts of the input. Noise distribution H should reflect the expected space of the input data to F, since the importance of the input components is measured in terms of the variation determined by H. In the general context, H could be a uniform distribution over Rd; however, we often operate in much more structured spaces (e.g. images, text). In these structured cases, we suspect that it is important to ensure that the noise vector is near the input areas."}, {"heading": "4 Experiments", "text": "To demonstrate the broad applicability of the LAN framework, we conduct experiments in a variety of typical learning tasks, including number classification, object classification in nature images, and image reconstruction. The goal of these experiments is to demonstrate the effectiveness of LANs to visualize latent attention mechanisms of different network types. In addition, we conduct an exploratory experiment in a topic-oriented modeling task to demonstrate the flexibility of LANs across multiple modalities. While LANs can be implemented with any network architecture, we limit our focus to fully networked LANs to demonstrate the effectiveness of our framework. Specifically, our LAN implementations range from 2-5 fully connected layers, each with less than 1000 hidden units. At a high level, these tasks are as follows (see supplementary material for training details): Translated MNISTData: A set of 28-12 MIST images are set to NIST grades 12."}, {"heading": "5 Results", "text": "Next, we describe the results of our experiments."}, {"heading": "5.1 Translated MNIST Results", "text": "The results are presented in Figure 2. We offer side-by-side visualizations of samples from the Translated MNIST dataset and the corresponding attention maps produced by the LAN network. In these attention maps, there are two striking features: (1) a lump of attention surrounding the number, and (2) a fixed grid pattern against the background. This grid pattern is shown in Figure 3. Below, we defend an interpretation of the grid effect depicted in Figure 3a. Namely, we claim that our attention masks have shown that the classification network operates in two distinct phases: 1. Detect the presence of a number somewhere in the input space. 2. Direct attention to the region where the number was found to determine its class. Under our hypothesis, one would expect classification accuracy to decrease in regions not spanned by the constant grid pattern."}, {"heading": "5.2 CIFAR-10 CNN", "text": "In Figure 4, in addition to the corresponding attention masks produced by the LAN, we provide samples of original images from the CIFAR 10 dataset. Note that the resulting masks capture common visual characteristics in images of the same class, such as tail feathers in birds or hulls / masts in ships. The presence of these characteristics in the mask suggests that the underlying classifier learns the anonymous representation of each class in order to distinguish between images and confirm their classification. We also note that the LAN displays not only high-grade concepts in the learned classifier, but also the ability to compose these concepts in such a way as to distinguish between classes, a characteristic that is most evident between the horse and deer classes, both of which exhibit extremely similar regions of attention for catching legs while they differ in structure to confirm the presence of a head or antlers."}, {"heading": "5.3 Newsgroup-20 Document Classification Results", "text": "Tables 1 and 2 contrast words present in documents with the 15 most important words identified by the appropriate subject classification attention mask. We note that these important words are usually either in the document itself (highlighted in yellow) or closely linked to the category to which the document belongs. The absence of important words from other classes is explained by our choice of \u03b70 noise, which creates more visually appealing attention masks, but does not punish the LAN for ignoring such words. We suspect that category-associated words that are not present in the document occur on a high-dimensional and poorly structured workbag input area due to the capacity constraints of the fully networked LAN architecture."}, {"heading": "5.4 Pong Autoencoder", "text": "In this experiment, we found that the LAN learns a static attention mask. Note that the highlighted areas correspond to regions where the paddle and ball are common. In particular, there is a high degree of uniformity within the mask along the paddle columns. This suggests that instead of tracking the paddle positions, the auto encoder stores pixel values in the columns where the paddles are likely to be. To support this assertion, we illustrate the behavior of the auto encoder when it provides input with a duplicate player paddle at different positions on the screen. As we see in Figure 6, the auto encoder is not able to reconstruct such paddles in any position, often mistaking them for the ball (two left columns)."}, {"heading": "6 Conclusion", "text": "As deep neural networks continue to be applied to a growing collection of tasks, understanding their decision-making processes becomes more and more important. As this task space encompasses more and more areas where there is a small margin for error, the ability to explore and diagnose problems within flawed models becomes critical. In this work, we have proposed Latent Attention Networks as a framework for capturing the latent attention mechanisms of arbitrary neural networks that draw parallels between noise-based input corruption and attention. We have shown that analyzing these attention measurements can effectively diagnose error modes in pre-trained networks and offer unique perspectives on the mechanism by which arbitrary networks perform their assigned tasks. We believe that there are several interesting research guidelines emerging from our framework. First, there are interesting parallels between this work and the popular Generative Adversarial Networks [8] in which it is possible to train F and A simultaneously as adversaries."}, {"heading": "A Translated MNIST Handwritten Digit Classifier", "text": "Here we will examine the attention masks generated by a LAN trained on a number classifier. We will show how LAN's intuition is imparted about the special method that a neural network uses to perform its task and highlight failure modes. Specifically, we will construct a \"translated MNIST\" domain in which the original digits are reduced from 28 x 28 to 12 x 12 and placed at random places in the original 28 x 28 image. Network F is a classifier that indicates the probability that each digit is present in a particular image. The trained network F has the following architecture: Conv (10, 2, (4 x 4), '-ReLU), Conv (20, 2, (4 x 4),' -ReLU), FC (10, softmax). F is trained with the Adam Optimizer for 100 000 iterations with a learning rate of 0.001 and with Late = thousands of -digits Rx."}, {"heading": "B CIFAR-10 CNN", "text": "In this experiment, we show that the LAN framework extends the decision-making of classifiers (based on the Alexnet architecture) with 0.000U = 1000-U to natural images. To avoid overfitting, we extend the CIFAR-10 dataset by applying small random affine transformations to the images. We used \u03b2 = 5.0 words for this experiment. The trained network, F has the following architecture: Conv (64, 2, (5), (5), (5), (ReLU), Conv (64, (3), (ReLU), 1, (3), (ReLU), Conv (64, 1, (3), (3), (3), (3), (3), (3), (ReLU. \"We used documents), Conv (32, (3), -ReLU.\" We learned), FC (384, tanh), FC (192, FC, softball, 10, where each local normalization is applied."}, {"heading": "D Pong Autoencoder", "text": "Here we examine the properties of latent attention masks produced by an autoencoder reconstructing images collected by an agent playing the popular Atari game Pong. We collected 3.2 million images of an agent performing a random policy in order to train a revolutionary autoencoder. The choice to operate according to a random policy was made to maximize the total number of objects the model (and thus LAN) would have to pay attention to in order to succeed. Note that an optimal agent playing the game to maximize its total score must not pay attention to the score pixels itself (x). The pre-trained network F has the following architecture: Conv (16, 2, (5 x 5), '-ReLU), Conv (32, 2, (5 x 5), (5 x, x, x, x, x, x, x, x, 4, x, x, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5', 5, x, 5, x, 5, x, x, x, x, x, 5, x, x, x, x, 5, x, x, 5, x, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}], "references": [{"title": "M\u00c3\u017eller. How to explain individual classification decisions", "author": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael H. Bowling"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Slow feature analysis yields a rich repertoire of complex cell properties", "author": ["Pietro Berkes", "Laurenz Wiskott"], "venue": "Journal of vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "University of Montreal,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Towards transparent ai systems: Interpreting visual question answering models", "author": ["Yash Goyal", "Akrit Mohapatra", "Devi Parikh", "Dhruv Batra"], "venue": "arXiv preprint arXiv:1608.08974,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L. Boyd-Graber", "Hal Daum\u00e9"], "venue": "In ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Rationalizing Neural Predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "Naacl,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin A. Riedmiller", "Andreas Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks", "author": ["Anh Mai Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Why should i trust you?: Explaining the predictions of any classifier", "author": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Explaining classifications for individual instances", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "arXiv preprint arXiv:1506.06579,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["Matthew Zeiler", "Rob Fergus"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D Zeiler", "Graham W Taylor", "Rob Fergus"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 3, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 17, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 19, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 27, "context": "One body of work focuses on visualizing various aspects of networks or their relationship to each datum they take as input [28, 29].", "startOffset": 123, "endOffset": 131}, {"referenceID": 28, "context": "One body of work focuses on visualizing various aspects of networks or their relationship to each datum they take as input [28, 29].", "startOffset": 123, "endOffset": 131}, {"referenceID": 22, "context": "Other work investigates algorithms for eliciting an explanation from trained machine-learning systems for each decision they make [23, 1, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 0, "context": "Other work investigates algorithms for eliciting an explanation from trained machine-learning systems for each decision they make [23, 1, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 23, "context": "Other work investigates algorithms for eliciting an explanation from trained machine-learning systems for each decision they make [23, 1, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 25, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 10, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 21, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 14, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 18, "context": "Attention has primarily been applied to neural networks to improve performance [19, 10, 2].", "startOffset": 79, "endOffset": 90}, {"referenceID": 9, "context": "Attention has primarily been applied to neural networks to improve performance [19, 10, 2].", "startOffset": 79, "endOffset": 90}, {"referenceID": 1, "context": "Attention has primarily been applied to neural networks to improve performance [19, 10, 2].", "startOffset": 79, "endOffset": 90}, {"referenceID": 5, "context": "[6] survey existing content-based attention models to improve performance in a variety of supervised learning tasks, including speech recognition, machine translation, image caption generation, and more.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27] apply stacked attention networks to better answer natural language questions about images, and Goyal et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] investigate a complementary method for networks specifically designed to answer questions about visual content; their approach visualizes which content in the image is used to inform the network\u2019s answer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28] highlight an important distinction for techniques that visualize aspects of networks: dataset-centric methods, which require a trained network and data for that network, and networkcentric methods, which target visualizing aspects of the network independent of any data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For example, Zeiler and Fergus [29] introduce a", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "[30], resulting in highly interpretable feature visualizations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], Simonyan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25], building on the earlier work of Erhan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] and Berkes and Wiskott [5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] and Berkes and Wiskott [5].", "startOffset": 27, "endOffset": 30}, {"referenceID": 22, "context": "A different line of work focuses on strategies for eliciting explanations from machine learning systems to increase interpretability [23, 1, 24].", "startOffset": 133, "endOffset": 144}, {"referenceID": 0, "context": "A different line of work focuses on strategies for eliciting explanations from machine learning systems to increase interpretability [23, 1, 24].", "startOffset": 133, "endOffset": 144}, {"referenceID": 23, "context": "A different line of work focuses on strategies for eliciting explanations from machine learning systems to increase interpretability [23, 1, 24].", "startOffset": 133, "endOffset": 144}, {"referenceID": 15, "context": "[16] forces networks to output a short \u201crationale\u201d that (ideally) justifies the network\u2019s decision in Natural Language Processing tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] advance a similar technique in which neural translation training is augmented by incentivizing networks to jointly align and translate source texts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "1 Latent Attention Network Framework A Latent Attention Network is a function A : R 7\u2192 [0, 1] that, given an input x (for the original network F ), produces an attention mask A(x) of the same shape as x.", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "[12] to classify documents into one of the twenty different categories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "These screens are generated by the Atari Learning Environment [3] emulator played by an agent that takes random actions at each time step.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "First, there are interesting parallels between this work and the popular Generative Adversarial Networks [8].", "startOffset": 105, "endOffset": 108}], "year": 2017, "abstractText": "Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network\u2019s inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \u201cattention masks\u201d support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network\u2019s underlying decision-making process irrespective of the data modality.", "creator": "LaTeX with hyperref package"}}}