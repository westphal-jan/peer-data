{"id": "1706.00633", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test", "abstract": "Though the recent progress is substantial, deep learning methods can be vulnerable to the elaborately crafted adversarial samples. In this paper, we attempt to improve the robustness by presenting a new training procedure and a thresholding test strategy. In training, we propose to minimize the reverse cross-entropy, which encourages a deep network to learn latent representations that better distinguish adversarial samples from normal ones. In testing, we propose to use a thresholding strategy based on a new metric to filter out adversarial samples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to various state-of-the-art networks (e.g., residual networks) and we achieve significant improvements on robust predictions in the adversarial setting.", "histories": [["v1", "Fri, 2 Jun 2017 11:23:12 GMT  (871kb,D)", "http://arxiv.org/abs/1706.00633v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tianyu pang", "chao du", "jun zhu"], "accepted": false, "id": "1706.00633"}, "pdf": {"name": "1706.00633.pdf", "metadata": {"source": "CRF", "title": "Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test", "authors": ["Tianyu Pang", "Chao Du", "Jun Zhu"], "emails": ["{pangty13@mails,", "du-c14@mails,", "dcszj@mail}.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead, in the same way as it has done in the past."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural networks", "text": "A deep neural network (DNN) classifier can generally be expressed as a mapping function F (X, \u03b8): Rd \u2192 RL, where X-Rd is the input variable, \u03b8 denotes all parameters, and L is the number of classes. Here, we focus on DNNs with softmax output coatings. To clarify, we define the softmax function softmax (z): RL \u2192 RL as softmax (z) i = exp (zi) / \u2211 L i = 1 exp (zi), i [L], where [L]: = {1, \u00b7, L}. Let Zf be the output vector of the last hidden layer. This defines a mapping function: X \u2192 Zf to extract good representations. Then, the classifier can be expressed as F (X, \u03b8)."}, {"heading": "2.2 Adversarial sample crafting and robustness", "text": "Although DNN has made significant progress, opposing samples can easily be identified to deceive the network, even if its accuracy is high [19]. We consider the white box attack, where attackers know everything about the target classifier F (X, \u03b8), including architecture, parameters, objective function and training tools. This setting is interesting because if we can effectively defend opposing samples under the white box attack, we may as well do it in other adverse situations where attackers have less knowledge of the target classifier. In the white box setting, attackers can design various algorithms to create opposing samples based on their knowledge. In our experiments, we consider the Fast Gradient Drawing Method (FGSM) [4], which is efficient and has been extensively studied [1, 4, 15, 20, 23]. Specifically, we let x be a normal input."}, {"heading": "3 Algorithms", "text": "In this section, we present a new algorithm to improve robustness against enemy samples. First, we provide some new insights into why there are enemy samples and how we can distinguish them. These insights lead us to a new algorithm."}, {"heading": "3.1 Why adversarial samples exist", "text": "Previous work [4, 22] assumes that the presence of opposing samples is due to certain defects in the training phase. A main support is the universal approximator theorem [8] that DNNs are able to represent functions that resist opposing samples. In practice, however, each DNN has an architecture of limited scope that leads to a limited representational capacity, and the existence of opposing samples is intrinsic, as explained below. For a given DNN classifier, its decision limit is set. Then, each pair of similar cases located on different sides of a decision limit is divided into different classes. Very often, such a pair of input points is indistinguishable to human observers; therefore, it sounds counter-intuitive and irrational to have a jump on the predicted labels. Previous defense attempts to adjust the limits of the training phase, e.g. by adding regulators in the objective decision [26], however, remain close to the adjustment of the objective decision [5]."}, {"heading": "3.2 The insufficiency of confidence and a new metric of non-ME", "text": "We need some parameters to distinguish whether an input x is unreliable for a given constellation or not. (F) We need some parameters to detect a correct predictive probability. (F) We need a high predictive probability. (F) We need some parameters to distinguish whether an input x is unreliable for a given constellation or not. (F) We need a high predictive probability. (F) We need a high predictive probability. (F) We need a high predictive probability. (F) We need a high predictive probability. (F) We need a normal predictive probability. (F) However, we have shown that a well-trained DNN classification is usually not only ambiguous, but also gives a high reliance on its predictions. (4, 19) This makes confidence in the adversarial setting unreliable. This is because the distribution of test probability is similar to the probability for a high predictive probability."}, {"heading": "3.3 Reverse cross-entropy", "text": "Based on the above analysis, we are now designing a new training target to improve the robustness of DNN classifiers. The key is to enforce a DNN classifier to map all normal instances in the neighborhoods opposite extended decision boundaries in the final hidden space. This can be achieved by making the non-maximum elements of F (x, \u03b8) as equal as possible and having high non-ME values for all normal inputs. Specifically, one obvious way to achieve the above goal is to have a cross-entropy term between yR and F (x, \u03b8), where t is the index of the true label, let yR denote its reverse label vector, the tallest element of which is zero and other elements equal to 1L \u2212 1. One obvious way to achieve the above goal is to add a cross-entropy term between yR and F (x, patient) in the traditional cross-target vector: x-entropy-entropy (x) pie-y-entropy (x) (x, patient)."}, {"heading": "3.4 The reverse cross-entropy training procedure", "text": "It is easy to know that the index obtained by F (X, \u03b8) and R), which most (i.e., the least probability) want to conceal, is actually the true label. This simple insight leads to our RCE training method, which consists of two parts, as described below: Reverse Training: Considering the training set D, we train DNN F (X, \u03b8) to be a \"liar\" by minimizing the average RCE loss: \"RCE loss.\" Then we get the reverse \"Liar Network FR (X, yd). Reverse Logs: We negate the final logits that are fed to the layer of the sound boundary to calculate the output predictions as FR."}, {"heading": "3.5 The thresholding test procedure", "text": "With a trained reverse \"liar\" classifier FR (X, \u03b8 \u0445 R), we present a threshold test procedure for robust predictions. The test procedure consists of three key steps, as explained below. Calculate the predicted label and trust: For a test input x, we first calculate the output vector FR (x, \u2082 R) and then assign the predicted label x to be t = arg maxi FR (x, \u03b8 R) i. So the corresponding trust is Confidence = FR (x, \u03b8 R). Calculate the kernel non-ME: We let Non-MER specify the non-ME value according to Equation (1), where FR (x, 300,000 R) is a replacement for F (x, \u03b8 R). Since in practice the non-ME changes change much more slowly than the confidence change, instead of directly using the non-MER value as a measurement variable, we use the kernel non-ME profile value \u2212 J profile value \u2212 J (we do not imply the KER factor \u2212 J as a non-xy _ Score \u2212 J)."}, {"heading": "4 Experiments", "text": "We are now presenting both quantitative and qualitative results to demonstrate the effectiveness of our method to improve the robustness of modern DNN classifiers."}, {"heading": "4.1 Setup", "text": "We use the two widely studied datasets - MNIST and CIFAR-10. MNIST is a collection of 70,000 images of handwritten digits in grades 0 to 9. It has a training set of 60,000 samples and a test set of 10,000 samples. CIFAR-10 consists of 60,000 color images in 10 classes with 6,000 images per class. There are 50,000 training images and 10,000 test images. Each image in both groups is standardized before it is sent to classifiers to eliminate interval dependence between pixels, according to the setup in [6]. We implement Resnet-32, Resnet-56 and Resnet-110 [6] on both datasets. For each network, we use both CE and RCE as training targets, optimized by the same training tools as in [7]. The training steps for both methods are set to 20k on MNIST and 80k on CIFARCE. Table 1 shows the test accuracy, where the results are limited to the conventional classification strategy and outputs only."}, {"heading": "4.2 Junction manifolds in the final hidden space", "text": "To verify that the RCE training procedure tends to map all normal inputs into the neighborhoods opposite extended decision boundaries in the last hidden space, we use the technique t-SNE [16] to visualize the distribution of the last hidden vector zf on the training set. Figure 2 shows the results of the 2-D visualization. To illustrate, we show the results of 1,000 training samples from the MNIST. We move the results to CIFAR-10 to the appendix, which are similar. Results show that the networks trained by the RCE method can successfully map the training samples into the neighborhoods of low-dimensional diversity in the last hidden space. These results also partially confirm the effectiveness of the non-ME as a measurement parameter."}, {"heading": "4.3 The ability of recognizing adversarial samples", "text": "In fact, it is the case that it is a matter of a kind and manner in which one sees oneself in a position to trump oneself, \"he said in an interview with\" Welt am Sonntag \":\" It is a question of whether the world will be able to change the world. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "4.4 The performance of the entire algorithm", "text": "To evaluate the performance of our overall method, we activate the Threshold Output Strategy in the prediction release phase and re-feed the mixed data sets of 20,000 samples into the classifiers. For comparison purposes, we also apply the Threshold Output Strategy in the prediction release phase to baselines and other classifiers using different metrics and the RCE training procedure. Fig. 5 shows the results for MNIST and CIFAR-10, where J in FGSM will be the CE cost function due to its better vulnerability to attack. The \"Accuracy of Results\" refers to the accuracy of the returns to the total number of 20,000 samples. The \"Refuse Rate\" for normal or reverse samples refers to the rate that a classifier refuses to predict and returns NIT on the corresponding 10,000 samples.The results show that our method is much more sensitive than the reuse rate. \""}, {"heading": "4.5 Different values of relaxation factor \u03b7", "text": "In the experiments above, we simply set the relaxation factor \u03b7 to 1. We now examine its influence on the J-score. In Fig. 6, we plot the mean values of the J-score and confidence based on the 10,000 opposing samples created by FGSM based on the test sets of MNIST and CIFAR-10. Note that the ranges of both the J-score and confidence are [0, 1], and for a large Thenon-ME, the J-score tends to be 1, resulting in identical values of the J-score and confidence in all inputs. In Fig. 6, we can check two important points: First, RCE training returns more reliable and reasonable confidence than CE training, and J-score is in fact a more distinguishable measure than confidence in the opposing setting. Second, there are certain mean values of \u03b7 = 0.05 that work best on distinguishability, in the sense that the J-score values decrease the quickest in confidence."}, {"heading": "4.6 Comparison with Bayesian neural networks", "text": "Recent work [14] shows the ability of BNs to defend adversarial samples. Here, we compare our method quantitatively with BNs. We use a fully interconnected network architecture with 2 hidden layers and 100 units in each layer, and apply the SGMCMC method [2] to train the BNN on MNIST. For comparison, we also apply the CE and RCE methods to train the network. We feed the 20,000 mixed samples based on MNIST into these trained networks and impose the threshold output strategy on them. Results are shown in Figure 7. We find that the trained BNN is not as susceptible to FGSM as discriminatory DNNs, partly because FGSM is designed for discriminatory models. Our method works even better than BNN in terms of overall accuracy and rejection rate of contrary samples."}, {"heading": "5 Conclusions and Discussions", "text": "We present a promising method for improving robustness by detecting and filtering enemy samples, which performs well on various modern DNN architectures, such as resnets, fully networked networks and other DNN architectures (see Appendix). In addition, our method performs better than BNN in improving robustness in our experiments. Our method can be implemented using standard algorithms without additional training, and because of its simplicity, our method can easily be extended to other defense strategies, such as enemy training [26]."}, {"heading": "A Proof and supplementary experiments", "text": "We can express the dependence of the logits Z on the input x and the parameters R and explicitly how we do it (we are the index of the true label.Proof.Proof.Proof.Proof.Proof.Proof.Proof.Prof Theorem 1. Let x be a predetermined input in the training data set, y be the one-hot label vector, yR is the reverse label vector corresponding to y, L \u2212 \u2212 gt is the number of different classes \u2212 \u2212 \u2212 \u2212 s is obtained from the reverse training. Under the L \u221e standard, if there is a training error \u03b1 1L that leads to softmax (Z (x, \u03b8) R) \u2212 yR \u2212 softmax. (\u2212 s) then we have the limits of softmax (\u2212 z) softmax (\u2212 Z (x, \u03b8)."}, {"heading": "B t-SNE on the CIFAR-10", "text": "Fig. 9 shows the results of t-SNE visualization on CIFAR-10."}, {"heading": "C Full results of Resnets", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["Nan Ding", "Youhan Fang", "Ryan Babbush", "Changyou Chen", "Robert D Skeel", "Hartmut Neven"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "http://www.deeplearningbook.org", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint arXiv:1412.5068,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Technical report, CiteSeerX,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Adversarial examples in the physical world", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Dropout inference in bayesian neural networks with alphadivergences", "author": ["Yingzhen Li", "Yarin Gal"], "venue": "arXiv preprint arXiv:1703.02914,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song"], "venue": "arXiv preprint arXiv:1611.02770,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Bayesian learning for neural networks, volume 118", "author": ["Radford M Neal"], "venue": "Springer Science & Business Media,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Security and Privacy (EuroS&P),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Security and Privacy (SP),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1929}], "referenceMentions": [{"referenceID": 2, "context": "1 Introduction Deep learning (DL) has obtained substantial progress in many tasks, including image categorization, speech recognition, and natural language processing [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "However, a high-accuracy DL model can be vulnerable in the adversarial setting [4, 26], where adversarial samples are carefully crafted to mislead the model to wrong predictions.", "startOffset": 79, "endOffset": 86}, {"referenceID": 3, "context": "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].", "startOffset": 73, "endOffset": 92}, {"referenceID": 11, "context": "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].", "startOffset": 73, "endOffset": 92}, {"referenceID": 14, "context": "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].", "startOffset": 73, "endOffset": 92}, {"referenceID": 20, "context": "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].", "startOffset": 73, "endOffset": 92}, {"referenceID": 21, "context": "Several algorithms have been developed to craft such adversarial samples [4, 12, 15, 21, 22].", "startOffset": 73, "endOffset": 92}, {"referenceID": 4, "context": "[5] considers adding a regularization term on the norm of gradients in the objective function, but it does not promise anything to gradient sign attacking methods that only require the sign of gradients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "The defensive distillation method [23] trains a model with a high temperature and tests it with a low temperature.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "Furthermore, this defensive distillation method can be easily attacked by new attacking methods [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "the common cross-entropy (CE) loss [3].", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "We apply our method to improve the state-of-the-art residual neural networks (Resnets) [6] on the widely used MNIST [13] and CIFAR-10 [11] datasets.", "startOffset": 87, "endOffset": 90}, {"referenceID": 12, "context": "We apply our method to improve the state-of-the-art residual neural networks (Resnets) [6] on the widely used MNIST [13] and CIFAR-10 [11] datasets.", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "We apply our method to improve the state-of-the-art residual neural networks (Resnets) [6] on the widely used MNIST [13] and CIFAR-10 [11] datasets.", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "Finally, we note that recent work [14] shows that a Bayesian neural network (BNN) can improve the robustness of DNN against adversarial samples.", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "The probability value F (x, \u03b8)t\u0302(x,\u03b8) is often used as the corresponding confidence score on this prediction [3], although it is insufficient as we shall see.", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "Then the CE training procedure intends to minimize the average CE loss to obtain the optimal parameters \u03b8\u2217 = arg min\u03b8 1 N \u2211 i\u2208[N ] LCE(xi, yi), which can be efficiently done by stochastic gradient methods with back-propagation [10, 24].", "startOffset": 227, "endOffset": 235}, {"referenceID": 23, "context": "Then the CE training procedure intends to minimize the average CE loss to obtain the optimal parameters \u03b8\u2217 = arg min\u03b8 1 N \u2211 i\u2208[N ] LCE(xi, yi), which can be efficiently done by stochastic gradient methods with back-propagation [10, 24].", "startOffset": 227, "endOffset": 235}, {"referenceID": 18, "context": "2 Adversarial sample crafting and robustness Though DNN has obtained substantial progress, adversarial samples can be easily identified to fool the network, even when its accuracy is high [19].", "startOffset": 188, "endOffset": 192}, {"referenceID": 3, "context": "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].", "startOffset": 121, "endOffset": 139}, {"referenceID": 3, "context": "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].", "startOffset": 121, "endOffset": 139}, {"referenceID": 14, "context": "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].", "startOffset": 121, "endOffset": 139}, {"referenceID": 19, "context": "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].", "startOffset": 121, "endOffset": 139}, {"referenceID": 22, "context": "In our experiments, we consider the fast gradient sign method (FGSM) [4], which is efficient and has been widely studied [1, 4, 15, 20, 23].", "startOffset": 121, "endOffset": 139}, {"referenceID": 16, "context": "Moreover, to evaluate the ability of a classifier to resist adversarial perturbation, we define the robustness \u2206(x; t\u0302) according to [17] as \u2206(x; t\u0302) := min r \u2016r\u20162 subject to t\u0302(x+ r) 6= t\u0302(x), where r is the perturbation.", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "1 Why adversarial samples exist Previous work [4, 22] hypothesizes that the existence of adversarial samples is because of certain defects in the training phase.", "startOffset": 46, "endOffset": 53}, {"referenceID": 21, "context": "1 Why adversarial samples exist Previous work [4, 22] hypothesizes that the existence of adversarial samples is because of certain defects in the training phase.", "startOffset": 46, "endOffset": 53}, {"referenceID": 7, "context": "One main support is from the universal approximator theorem [8] that DNNs are able to represent functions that resist adversarial samples.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": ", by adding regularizers in the objective function [5] or augmenting the training dataset [26], will only result in the change of the distribution of decision boundaries but the jump on the predicted labels nearby the decision boundary still exists.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "A potential candidate is the confidence F (x, \u03b8)t\u0302(x,\u03b8) on the predicted label t\u0302(x, \u03b8), which has in fact been widely used [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "However, it has been shown that a well-trained DNN classifier usually not only misclassifies adversarial samples but also gives high confidence on its predictions [4, 19], which renders the confidence unreliable in the adversarial setting.", "startOffset": 163, "endOffset": 170}, {"referenceID": 18, "context": "However, it has been shown that a well-trained DNN classifier usually not only misclassifies adversarial samples but also gives high confidence on its predictions [4, 19], which renders the confidence unreliable in the adversarial setting.", "startOffset": 163, "endOffset": 170}, {"referenceID": 13, "context": "In the adversarial setting, attackers can explore the points far from the data distribution [14], and the predictions in these domains mostly result from interpolation with high uncertainty.", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": ", [0, 1]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 5, "context": "Every image in both sets is standardized before feeding to classifiers to remove interval dependence among pixels, following the setup in [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "94% We implement Resnet-32, Resnet-56 and Resnet110 [6] on both datasets.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "For each network, we use both the CE and RCE as the training objective, optimized by the same training tools as in [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 15, "context": "In order to verify that the RCE training procedure tends to map all the normal inputs to the neighborhoods of oppositely elongated decision boundaries in the final hidden space, we apply the technique t-SNE [16] to visualize the distribution of the final hidden vector zf on the training set.", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Note that the value ranges of both J-score and confidence are [0, 1], and when \u03b7 is large the non-ME tends to be 1, which leads to identical values of J-score and confidence on all the inputs.", "startOffset": 62, "endOffset": 68}, {"referenceID": 17, "context": "Bayesian neural networks (BNNs) [18] are a family of models that may defend adversarial samples.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Recent work [14] shows the ability of BNNs on defending adversarial samples.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "We access a fully connected network architecture with 2 hidden layers and 100 units in each layer, and apply the SGMCMC method [2] to train the BNN on MNIST.", "startOffset": 127, "endOffset": 130}, {"referenceID": 0, "context": "References [1] Nicholas Carlini and David Wagner.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D Skeel, and Hartmut Neven.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Shixiang Gu and Luca Rigazio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Kurt Hornik, Maxwell Stinchcombe, and Halbert White.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Sergey Ioffe and Christian Szegedy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Diederik Kingma and Jimmy Ba.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Alex Krizhevsky and Geoffrey Hinton.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Alexey Kurakin, Ian Goodfellow, and Samy Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Yingzhen Li and Yarin Gal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Laurens van der Maaten and Geoffrey Hinton.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Radford M Neal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Anh Nguyen, Jason Yosinski, and Jeff Clune.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "2 Experiments on simple CNN In order to test our method on other convolutional neural networks (CNNs) [3], we implement two neural networks as described in Table 2 and the training details are included in Table 3.", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "The architectures are based on the most basic CNN operations, and we use training tools like Adam [10], dropout [25] and batch normalization [9] in our training procedures.", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "The architectures are based on the most basic CNN operations, and we use training tools like Adam [10], dropout [25] and batch normalization [9] in our training procedures.", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "The architectures are based on the most basic CNN operations, and we use training tools like Adam [10], dropout [25] and batch normalization [9] in our training procedures.", "startOffset": 141, "endOffset": 144}], "year": 2017, "abstractText": "Though the recent progress is substantial, deep learning methods can be vulnerable to the elaborately crafted adversarial samples. In this paper, we attempt to improve the robustness by presenting a new training procedure and a thresholding test strategy. In training, we propose to minimize the reverse cross-entropy, which encourages a deep network to learn latent representations that better distinguish adversarial samples from normal ones. In testing, we propose to use a thresholding strategy based on a new metric to filter out adversarial samples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to various state-of-the-art networks (e.g., residual networks) and we achieve significant improvements on robust predictions in the adversarial setting.", "creator": "LaTeX with hyperref package"}}}