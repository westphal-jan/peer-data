{"id": "1706.00637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Joint Matrix-Tensor Factorization for Knowledge Base Inference", "abstract": "While several matrix factorization (MF) and tensor factorization (TF) models have been proposed for knowledge base (KB) inference, they have rarely been compared across various datasets. Is there a single model that performs well across datasets? If not, what characteristics of a dataset determine the performance of MF and TF models? Is there a joint TF+MF model that performs robustly on all datasets? We perform an extensive evaluation to compare popular KB inference models across popular datasets in the literature. In addition to answering the questions above, we remove a limitation in the standard evaluation protocol for MF models, propose an extension to MF models so that they can better handle out-of-vocabulary (OOV) entity pairs, and develop a novel combination of TF and MF models. We also analyze and explain the results based on models and dataset characteristics. Our best model is robust, and obtains strong results across all datasets.", "histories": [["v1", "Fri, 2 Jun 2017 11:34:37 GMT  (43kb)", "http://arxiv.org/abs/1706.00637v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prachi jain", "shikhar murty", "mausam", "soumen chakrabarti"], "accepted": false, "id": "1706.00637"}, "pdf": {"name": "1706.00637.pdf", "metadata": {"source": "CRF", "title": "Joint Matrix-Tensor Factorization for Knowledge Base Inference", "authors": ["Prachi Jain", "Shikhar Murty", "Soumen Chakrabarti"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 6.00 637v 1 [cs.A I] 2 Jun 2While several matrix factorization and tensor factorization models have been proposed for knowledge base models, they have rarely been compared across different datasets. Is there a single model that performs well across all datasets? If not, what characteristics of a dataset determine the performance of MF and TF models? Is there a common TF + MF model that performs robustly across all datasets? We are conducting a comprehensive evaluation to compare popular KB inference models across popular datasets in the literature. In addition to answering the above questions, we are removing a constraint in the standard evaluation protocol for MF models, proposing an extension to MF models so that they can better deal with vocabulary (OV) entity pairs, and developing a novel combination of TF models and MF models to explain the results and MF models."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2 Background and Experimental Setup", "text": "We propose knowledge bases in order to benefit from unstandardized and canonical relations, even if they are not able to analyze the results."}, {"heading": "2.1 Datasets", "text": "Most KB inference systems have used one or more of four popular KBs for evaluation, including WN18 (eighteen WB relationships (Bordes et al., 2013) and three sets of Freebase (FB); one set is FB15K (Bordes et al., 2013), which has 1,345 relationships; another set is FB15K-237, a subset of FB15K, which includes 237 relationships that rarely overlap in terms of entity pairs (Toutanova et al., 2015); and the fourth set is NYT + FB, which, along with FB triples, also contains dependency path-based textual relationships from the New York Times, mentioning entities aligned with Freebase entities (Riedel et al., 2013). Our literature research shows that no algorithm has been tested on all data sets."}, {"heading": "2.2 Standard Evaluation Protocol", "text": "Since we want to perform these experiments on a scale, we follow one of the usual evaluation protocols, which can be run fully automatically, and this method splits the KB into train (Ttr) and test tuples (Tts), and the system can then access Ttr only during the training. < e * 1, r *, e * 2 > Tts sends a query < e * 1, r *,? > to the trained model, which then classifies all units e2 * E by lowering \"M\" (e * 1, r *, e2), and a higher ranking of e * 2 in this list indicates better performance of the model. Metrics used to compare two algorithms are mean reciprocal (MRR) and the percentage of e * 2s obtained in the top-10 results (HITS @ 10, e2). The test method is typically classified with two modes < 2; 2."}, {"heading": "3 Comparison under Standard and Unified KBI Evaluation Protocols", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Training Details", "text": "We first implement all algorithms in a common framework written with Keras / Theano (Chollet, 2015; Theano Development Team, 2016). We use 100-dimensional vectors for all models. They are trained with mini-batch stochastic gradient descent with AdaGrad to K40 GPUs with a learning rate of 0.5. We calculate 200 negative samples per tupel. We set margin to 1 for maximum margin loss. After previous work (Yang et al., 2015), all entity and entity pair vectors are normalized again to have a unit standard after each batch update. We use a batch size of 20,000 for training. We train all models for 200 epochs. We use early stops on validation sets (a small subset of training sets) to prevent our models from being overly adapted."}, {"heading": "3.2 Preliminary Results", "text": "The first four rows of Table 2 indicate the performance of all models across the datasets. We observe DistMult (DM) as the overall winner among tensor factorization models - E performs well on FB15K-237, while TransE performs well on FB15K, with DM coming out most robust. In TF models on three datasets (FB15K, FB15K237, WN18), our experiments are able to replicate (or improve) various results reported in previous work (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).4 Since NYT + FB is a new test split and F has not been tested on other datasets, these results cannot be directly compared with previous work."}, {"heading": "3.3 KBI Evaluation Protocol", "text": "Consider the second modification from Section 2.2. If the precedence of the possible entities e2 is compared with the score \u03c6 (e-1, r-1, e2) of MF models, the standard evaluation protocol works on a subset E2 rather than on all entities in E. This is because many entity pairs are embedded (e-1, e2), the entity pairs are not even trained in the model, 4 (Yang et al., 2015) report a higher MRR for DM onWN18 and therefore their results will be meaningless. We call these OOV entity pairs. E2 contains all entities for which the entity pair (e-1, e2) is trained."}, {"heading": "3.4 Results Adjusted for KBI Evaluation", "text": "If the KBI accepts such a development, then it will also be able to initiate such a development."}, {"heading": "3.5 Most-Frequent Baselines", "text": "In order to improve our understanding of the difficulty of each data set and the quality of each model, we introduce two baselines for our task. Faced with a query, < e * 1, r *,? >, our first baseline ranks all entities based on the frequency of their occurrence with reference r *, i.e., it ranks each entity e2 based on the cardinality of the set {t | t = < e1, r *, e2 >. A similar baseline ranks each entity e2 based on its frequency of occurrence with e * 1, i.e., based on the cardinality of the set {t | t = < e * 1, r, e2 > Ttr}. We call these baselines MFreq (e2 | r \u00b2) and MFrequencies, which represent the frequency of occurrence with e *."}, {"heading": "4 OOV Training for KB Inference", "text": "The previous section emphasizes the importance of OOV entity pairs in the performance of MF models. In general, a robust model needs to handle invisible entities / entity pairs gracefully. A natural extension is to explicitly model an OOV entity pair vector for F models (and OOV entity vector for TF models). Specifically, we represent a vector (eoov, eoov) vector for F and eoov for TF. 5 This modification means that OOV entity pairs have the same score.5We have also tried to learn several entity pairs of OOV vectors of form (eoov, eoov), but that has not given us better performance. OOV vectors can be trained in many ways. We are developing two baselines that do not explicitly train the vectors. A baseline that assigns an eoooov random value (an eoov)."}, {"heading": "5 Joint MF-TF Models", "text": "Background on common MF-TF models E + F + DM + DM requirement meets this requirement. (2015) We compare TF + DM models (mainly, E + F) and find they have complementary strengths. (2015) We expect them to outperform individual models on artificial datasets and FB models as the results of two models are added. (2013) Early work by Reidel et al. (2013) Also experiments with a common model of NYT + FB. (2015) We implement a common E + DM model and test it on FB15K-237, but no further datasets.We are motivated to develop a model that is robust across all datasets."}, {"heading": "6 Discussion and Future Work", "text": "We now list two observations that suggest important directions for future research in CBs. Furthermore, we automatically calculate OOV rates for entity pairs (Table 4) as a rough prediction of the relative success of the TF and MF families. Finally, in Table 9 we report on singleton and doubleton percentages (for entity pairs). A singleton is an entity pair that occurs only once in the data (Ttr, Tts) and a doubleton is an entity pair that occurs exactly twice. Doubletons have a strong effect in the scenario painted in Table 3. We find that almost every dataset has some idiosyncrasy, which raises the question of whether it is a good representative of the datasets."}, {"heading": "7 Related Work", "text": "Traditional methods for drawing conclusions about KBs include random walks through knowledge charts (Lao et al., 2011), natural logical conclusions (MacCartney and Manning, 2007), and the use of statistical relational learning models such as Markov Logic Network, Bayesian Logic Programs, and Probabilistic Soft Logic (Schoenmackers et al., 2008; Raghavan et al., 2012; Wang and Cohen, 2015), which require (or benefit from) background knowledge of inference rules, which are largely generated through extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gala \u0301 rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain et al., Mausam al., 2016).Neural methods for KB conclusions combine both models in a consistent framework to add new facts directly to KB."}, {"heading": "8 Conclusion", "text": "After replacing the standard evaluation protocol with our proposed OOV cognitive KBI protocol, we find that DistMult (a TF model) is quite robust in a variety of datasets, but F (an MF model) outperforms others in one dataset. F's performance continues to increase by training an OOOV entity pair vector. Finally, we propose common models that combine DistMult and F. We find that adding the loss functions from both models with regulation of F parameters achieves the most robust results in all datasets."}], "references": [{"title": "Global learning of typed entailment rules", "author": ["Ido Dagan", "Jacob Goldberger"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Berant et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2011}, {"title": "Efficient treebased approximation for entailment graph learning. In The 50th Annual Meeting of the Association for Computational Linguistics", "author": ["Ido Dagan", "Meni Adler", "Jacob Goldberger"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Burges et al. (Burges et al.,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Kai-Wei Chang", "Wen-tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "Proceedings of the 2014 Conference", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "Regularizing relation representations by first-order implications", "author": ["Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": null, "citeRegEx": "Demeester et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Demeester et al\\.", "year": 2016}, {"title": "Open information extraction: The second generation", "author": ["Etzioni et al.2011] Oren Etzioni", "Anthony Fader", "Janara Christensen", "Stephen Soderland", "Mausam"], "venue": "In IJCAI,", "citeRegEx": "Etzioni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2011}, {"title": "Amie: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["Christina Teflioudi", "Katja Hose", "Fabian Suchanek"], "venue": "In Proceedings of the 22nd international conference on World Wide", "citeRegEx": "Gal\u00e1rraga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gal\u00e1rraga et al\\.", "year": 2013}, {"title": "Composing relationships with translations", "author": ["Antoine Bordes", "Nicolas Usunier"], "venue": "In Ma\u0300rquez et al. (Ma\u0300rquez et al.,", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Relly: Inferring hypernym relationships between relational phrases", "author": ["Grycner et al.2015] Adam Grycner", "Gerhard Weikum", "Jay Pujara", "James Foulds", "Lise Getoor"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Grycner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grycner et al\\.", "year": 2015}, {"title": "Traversing knowledge graphs in vector space", "author": ["Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang"], "venue": "In Ma\u0300rquez et al. (Ma\u0300rquez et al.,", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Foundations of the parafac procedure: Models and conditions for an \u201cexplanatory\u201d multi-modal factor analysis", "author": ["Richard Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "Harshman.,? \\Q1970\\E", "shortCiteRegEx": "Harshman.", "year": 1970}, {"title": "Knowledge-guided linguistic rewrites for inference rule verification", "author": ["Jain", "Mausam2016] Prachi Jain", "Mausam"], "venue": "In Knight et al. (Knight et al.,", "citeRegEx": "Jain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2016}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Dirt@ sbt@ discovery of inference rules from text", "author": ["Lin", "Pantel2001] Dekang Lin", "Patrick Pantel"], "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Lin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2001}, {"title": "Natural logic for textual inference", "author": ["MacCartney", "Manning2007] Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,", "citeRegEx": "MacCartney et al\\.,? \\Q2007\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2007}, {"title": "Foundations of statistical natural language processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2001}, {"title": "Patty: a taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Lorenzo Rosasco", "Tomaso A. Poggio"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Relation schema induction using tensor factorization with side information", "author": ["Nimishakavi", "Uday Singh Saini", "Partha P. Talukdar."], "venue": "Jian Su, Xavier Carreras,", "citeRegEx": "Nimishakavi et al\\.,? 2016", "shortCiteRegEx": "Nimishakavi et al\\.", "year": 2016}, {"title": "Elementary: Large-scale knowledge-base construction via machine learning and statistical inference", "author": ["Niu et al.2012] Feng Niu", "Ce Zhang", "Christopher R\u00e9", "Jude W. Shavlik"], "venue": "Int. J. Semantic Web Inf. Syst.,", "citeRegEx": "Niu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2012}, {"title": "Learning to read between the lines using bayesian logic programs", "author": ["Raymond J Mooney", "Hyeonseo Ku"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Raghavan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raghavan et al\\.", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Sameer Singh", "Sebastian Riedel"], "venue": null, "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Scaling textual inference to the web", "author": ["Oren Etzioni", "Daniel S Weld"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schoenmackers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Schoenmackers et al\\.", "year": 2008}, {"title": "Learning first-order horn clauses from web text", "author": ["Oren Etzioni", "Daniel S Weld", "Jesse Davis"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schoenmackers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Schoenmackers et al\\.", "year": 2010}, {"title": "Towards combined matrix and tensor factorization for universal schema relation extraction", "author": ["Singh et al.2015] Sameer Singh", "Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Burges et al. (Burges et al.,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Open IE as an intermediate structure for semantic tasks", "author": ["Ido Dagan", "Mausam"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Stanovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stanovsky et al\\.", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Ma\u0300rquez et al. (Ma\u0300rquez et al.,", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Compositional learning of embeddings for relation paths in knowledge bases and text", "author": ["Xi Victoria Lin", "Wen-tau Yih", "Hoifung Poon", "Chris Quirk"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Toutanova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "Complex embeddings for simple link prediction", "author": ["Johannes Welbl", "Sebastian Riedel", "\u00c9ric Gaussier", "Guillaume Bouchard"], "venue": null, "citeRegEx": "Trouillon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trouillon et al\\.", "year": 2016}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["Verga et al.2016a] Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum"], "venue": "In Knight et al. (Knight et al.,", "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Generalizing to unseen entities and entity pairs with row-less universal schema. CoRR, abs/1606.05804", "author": ["Verga et al.2016b] Patrick Verga", "Arvind Neelakantan", "AndrewMcCallum"], "venue": null, "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Joint information extraction and reasoning: A scalable statistical relational learning approach", "author": ["Wang", "Cohen2015] William Yang Wang", "William W. Cohen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "In In International Conference on Learning Representations ICLR,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "\u2217First two authors contributed equally to the paper automatically using statistical regularities in KBs (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012; Jain and Mausam, 2016).", "startOffset": 104, "endOffset": 200}, {"referenceID": 0, "context": "\u2217First two authors contributed equally to the paper automatically using statistical regularities in KBs (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012; Jain and Mausam, 2016).", "startOffset": 104, "endOffset": 200}, {"referenceID": 17, "context": "\u2217First two authors contributed equally to the paper automatically using statistical regularities in KBs (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012; Jain and Mausam, 2016).", "startOffset": 104, "endOffset": 200}, {"referenceID": 23, "context": ", E (Riedel et al., 2013), TransE (Bordes et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": ", 2013), TransE (Bordes et al., 2013), DistMult (Yang et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 36, "context": ", 2013), DistMult (Yang et al., 2015),", "startOffset": 18, "endOffset": 37}, {"referenceID": 18, "context": "Rescal (Nickel et al., 2011) models) learn independent embeddings for e1 and e2, whereas matrix factorization (MF) methods (e.", "startOffset": 7, "endOffset": 28}, {"referenceID": 23, "context": ", F (Riedel et al., 2013) model) learn an embedding per entity-pair (e1, e2).", "startOffset": 4, "endOffset": 25}, {"referenceID": 27, "context": "Except for one paper making some early progress (Singh et al., 2015), their relative benefits have not been studied in detail.", "startOffset": 48, "endOffset": 68}, {"referenceID": 30, "context": "Otherwise an MF algorithm may appear to perform better than it really does, as in the case of F\u2019s performance on FB15K-237 (Toutanova et al., 2015).", "startOffset": 123, "endOffset": 147}, {"referenceID": 30, "context": "is natural (Toutanova et al., 2015) to unify these", "startOffset": 11, "endOffset": 35}, {"referenceID": 5, "context": "(Etzioni et al., 2011).", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "These are E, F (Riedel et al., 2013), TransE (Bordes et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 2, "context": ", 2013), TransE (Bordes et al., 2013), and DistMult (Yang et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 36, "context": ", 2013), and DistMult (Yang et al., 2015).", "startOffset": 22, "endOffset": 41}, {"referenceID": 18, "context": "Some models may also learn matrix embeddings instead of vectors (Nickel et al., 2011; Socher et al., 2013).", "startOffset": 64, "endOffset": 106}, {"referenceID": 28, "context": "Some models may also learn matrix embeddings instead of vectors (Nickel et al., 2011; Socher et al., 2013).", "startOffset": 64, "endOffset": 106}, {"referenceID": 36, "context": "We don\u2019t study these, as they are typically outperformed by the models implemented in this paper (Yang et al., 2015; Trouillon et al., 2016).", "startOffset": 97, "endOffset": 140}, {"referenceID": 32, "context": "We don\u2019t study these, as they are typically outperformed by the models implemented in this paper (Yang et al., 2015; Trouillon et al., 2016).", "startOffset": 97, "endOffset": 140}, {"referenceID": 30, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 32, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 4, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 24, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 27, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 30, "context": "To define a log-likelihood based loss for M , Toutanova et al. (2015) first model an approximate conditional probability:", "startOffset": 46, "endOffset": 70}, {"referenceID": 2, "context": "On the other hand, max-margin loss minimizes a margin-based ranking criterion (Bordes et al., 2013):", "startOffset": 78, "endOffset": 99}, {"referenceID": 29, "context": "Toutanova et al. (2015) compare F with some TF models on one dataset and find that F does not perform as well as TF.", "startOffset": 0, "endOffset": 24}, {"referenceID": 27, "context": "Singh et al. (2015) use a series of artificial experiments to conclude that MF models typically perform well on tasks where there is significant relation synonymy in the data, whereas TF models perform better when there are latent types for each relation that need to be predicted.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "These include WN18 (eighteen Wordnet relations (Bordes et al., 2013)) and three datasets over Freebase (FB).", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "(Bordes et al., 2013) that has 1,345 relations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "Another dataset is FB15K-237, which is a subset of FB15K comprising 237 relations that seldom overlap in terms of entity pairs (Toutanova et al., 2015).", "startOffset": 127, "endOffset": 151}, {"referenceID": 23, "context": "The fourth dataset is NYT+FB, which, along with FB triples, also includes dependency path-based textual relations from New York Times, the mentions of entities in which are aligned with entities in Freebase (Riedel et al., 2013).", "startOffset": 207, "endOffset": 228}, {"referenceID": 2, "context": "The filtered metrics remove the set {e2|\u3008e \u2217 1, r \u2217, e2\u3009 \u2208 Ttr \u222a Tts} from the ranked list (Bordes et al., 2013).", "startOffset": 91, "endOffset": 112}, {"referenceID": 30, "context": "are considered as candidates for ranking (Toutanova et al., 2015; Verga et al., 2016b).", "startOffset": 41, "endOffset": 86}, {"referenceID": 36, "context": "Following previous work (Yang et al., 2015) all entity and entity-pair vectors are re-normalized to have a unit norm after each batch update.", "startOffset": 24, "endOffset": 43}, {"referenceID": 23, "context": "For NYT+FB, previous works had experimented on a test fold with only 80 correct tuples (Riedel et al., 2013).", "startOffset": 87, "endOffset": 108}, {"referenceID": 36, "context": "cate (or improve upon) various results reported in prior works (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).", "startOffset": 63, "endOffset": 127}, {"referenceID": 2, "context": "cate (or improve upon) various results reported in prior works (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).", "startOffset": 63, "endOffset": 127}, {"referenceID": 30, "context": "cate (or improve upon) various results reported in prior works (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).", "startOffset": 63, "endOffset": 127}, {"referenceID": 36, "context": "(Yang et al., 2015) report a higher MRR for DM onWN18.", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "Our evaluation sanitizes the published numbers for F on FB15K-237 (Toutanova et al., 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 27, "context": "Singh et al. (2015) contribute some theoret-", "startOffset": 0, "endOffset": 20}, {"referenceID": 29, "context": "Previous work on word vectors has shown that multiplicative scores often outperform additive ones as they amplify smaller differences and reduce larger ones (Levy and Goldberg, 2014; Stanovsky et al., 2015).", "startOffset": 157, "endOffset": 206}, {"referenceID": 27, "context": "Background on Joint MF-TF Models: Recall that Singh et al. (2015) compare TF andMFmodels (particularly, E and F) and find that they have comple-", "startOffset": 46, "endOffset": 66}, {"referenceID": 27, "context": "Note that row 3 and row 5 are the models reported in (Singh et al., 2015) and (Toutanova et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 30, "context": ", 2015) and (Toutanova et al., 2015), respectively.", "startOffset": 12, "endOffset": 36}, {"referenceID": 23, "context": "Also note that F is outperformed by DM even on non-OOVs; this refutes prior claims that F always performs better than TF models when test entity pairs are seen during training (Riedel et al., 2013; Toutanova et al., 2015).", "startOffset": 176, "endOffset": 221}, {"referenceID": 30, "context": "Also note that F is outperformed by DM even on non-OOVs; this refutes prior claims that F always performs better than TF models when test entity pairs are seen during training (Riedel et al., 2013; Toutanova et al., 2015).", "startOffset": 176, "endOffset": 221}, {"referenceID": 9, "context": "A study similar to ours comparing the latest models that train over relation paths (Guu et al., 2015; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Toutanova et al., 2016) will benefit our understanding of path-based inference.", "startOffset": 83, "endOffset": 153}, {"referenceID": 7, "context": "A study similar to ours comparing the latest models that train over relation paths (Guu et al., 2015; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Toutanova et al., 2016) will benefit our understanding of path-based inference.", "startOffset": 83, "endOffset": 153}, {"referenceID": 31, "context": "A study similar to ours comparing the latest models that train over relation paths (Guu et al., 2015; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Toutanova et al., 2016) will benefit our understanding of path-based inference.", "startOffset": 83, "endOffset": 153}, {"referenceID": 12, "context": "graphs (Lao et al., 2011), natural logic inference (MacCartney and Manning, 2007), and use of statistical relational learning models", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "such as Markov Logic Network, Bayesian Logic Programs, and Probabilistic Soft Logic (Schoenmackers et al., 2008; Raghavan et al., 2012; Wang and Cohen, 2015).", "startOffset": 84, "endOffset": 157}, {"referenceID": 22, "context": "such as Markov Logic Network, Bayesian Logic Programs, and Probabilistic Soft Logic (Schoenmackers et al., 2008; Raghavan et al., 2012; Wang and Cohen, 2015).", "startOffset": 84, "endOffset": 157}, {"referenceID": 26, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 17, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 6, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 8, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 1, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 24, "context": "The original F model has been extended to incorporate first order logic rules, (Rockt\u00e4schel et al., 2015; Demeester et al., 2016), to predict for relations not seen at training time (Verga et al.", "startOffset": 79, "endOffset": 129}, {"referenceID": 4, "context": "The original F model has been extended to incorporate first order logic rules, (Rockt\u00e4schel et al., 2015; Demeester et al., 2016), to predict for relations not seen at training time (Verga et al.", "startOffset": 79, "endOffset": 129}, {"referenceID": 10, "context": "Similarly, other TF models also exist, for example, Parafac (Harshman, 1970), Rescal (Nickel et al.", "startOffset": 60, "endOffset": 76}, {"referenceID": 18, "context": "Similarly, other TF models also exist, for example, Parafac (Harshman, 1970), Rescal (Nickel et al., 2011) and NTN (Socher et al.", "startOffset": 85, "endOffset": 106}, {"referenceID": 28, "context": ", 2011) and NTN (Socher et al., 2013).", "startOffset": 16, "endOffset": 37}, {"referenceID": 19, "context": "More recent models have also been introduced such as a model using holographic embeddings (Nickel et al., 2016), and another with asymmetric embeddings using complex vectors (Trouillon et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 32, "context": ", 2016), and another with asymmetric embeddings using complex vectors (Trouillon et al., 2016).", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": "The learned embeddings can use additional information such as typing (Chang et al., 2014), have been used to mine logical rules (Yang et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 36, "context": ", 2014), have been used to mine logical rules (Yang et al., 2015) and have been used for schema induction (Nimishakavi et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 20, "context": ", 2015) and have been used for schema induction (Nimishakavi et al., 2016).", "startOffset": 48, "endOffset": 74}], "year": 2017, "abstractText": "While several matrix factorization (MF) and tensor factorization (TF) models have been proposed for knowledge base (KB) inference, they have rarely been compared across various datasets. Is there a single model that performs well across datasets? If not, what characteristics of a dataset determine the performance of MF and TF models? Is there a joint TF+MF model that performs robustly on all datasets? We perform an extensive evaluation to compare popular KB inferencemodels across popular datasets in the literature. In addition to answering the questions above, we remove a limitation in the standard evaluation protocol for MF models, propose an extension to MF models so that they can better handle out-ofvocabulary (OOV) entity pairs, and develop a novel combination of TF and MF models. We also analyze and explain the results based on models and dataset characteristics. Our best model is robust, and obtains strong results across all datasets.", "creator": "LaTeX with hyperref package"}}}