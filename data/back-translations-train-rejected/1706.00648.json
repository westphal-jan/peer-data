{"id": "1706.00648", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Dataflow Matrix Machines as a Model of Computations with Linear Streams", "abstract": "We overview dataflow matrix machines as a Turing complete generalization of recurrent neural networks and as a programming platform. We describe vector space of finite prefix trees with numerical leaves which allows us to combine expressive power of dataflow matrix machines with simplicity of traditional recurrent neural networks.", "histories": [["v1", "Wed, 3 May 2017 13:46:05 GMT  (79kb)", "http://arxiv.org/abs/1706.00648v1", "6 pages, accepted for presentation at LearnAut 2017: Learning and Automata workshop at LICS (Logic in Computer Science) 2017 conference. Preprint original version: April 9, 2017; minor correction: May 1, 2017"]], "COMMENTS": "6 pages, accepted for presentation at LearnAut 2017: Learning and Automata workshop at LICS (Logic in Computer Science) 2017 conference. Preprint original version: April 9, 2017; minor correction: May 1, 2017", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.PL", "authors": ["michael bukatin", "jon anthony"], "accepted": false, "id": "1706.00648"}, "pdf": {"name": "1706.00648.pdf", "metadata": {"source": "CRF", "title": "Dataflow Matrix Machines as a Model of Computations with Linear Streams", "authors": ["Michael Bukatin", "Jon Anthony"], "emails": ["bukatin@cs.brandeis.edu", "jsa.aerial@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.00 648v 1 [cs.N E] 3M aycomplete generalization of recurrent neural networks and asa programming platform. We describe the vector space of finite refix trees with numerical leaves, which allows us to combine the expressiveness of dataflow matrix machines with the simplicity of traditional relapsing neural networks."}, {"heading": "1. Introduction", "text": "Looking at a Turing-complete generalization of recursive neural networks (RNNs), four groups of questions arise: a) What is the mechanism that allows access to unlimited memory; b) What is the pragmatic force of the available primitives and is the resulting platform that lends itself to the manual creation of software instead of merely serving as a compilation and machine learning target; c) what are self-referential (and self-modifying) mechanisms, if any; d) what are the implications for machine learning. In Section 2, we review data stream matrix machines, a generalization of RNNNs based on arbitrary linear currents, neurons of arbitrary non-negative input and output, a novel model of unlimited memory, and well-developed self-referential devices that follow [2], [3], [4] and [4]."}, {"heading": "2. Dataflow Matrix Machines: an Overview", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Countable-sized Nets with Finite Active Part", "text": "A popular approach to providing complete Turing generalizations of RNs with unlimited memory is to use an RNN to control a Turing machine tape or any other model of external memory [9], [8], [17]. Another approach is to allow reals of unlimited precision, when in reality a binary extension of a real number is used as a tape of a Turing machine [16]. Data flow matrix machines take a different approach. We consider a countable RNN and thus a countable matrix of connection weights, but with the condition that only a limited number of these weights is not zero at a given time. At a given time, only those neurons that have at least a non-zero connectivity weight are active. Therefore, only a limited part of the network is active at a given time. Memory and network capacity can be added dynamically by gradually turning more weights into non-zero [2]."}, {"heading": "2.2. Dataflow Matrix Machines as a Generalization of Recurrent Neural Networks", "text": "The core of the neural models of the calculations is to generally perform non-linear but relatively local calculations performed by the activation functions in neurons, and linear but potentially quite global calculations that recompute neuron inputs from the outputs of different neurons. The metaphor of a \"two-stroke engine\" applies to traditional RNNNs. On the \"upward motion,\" the activation functions built into neurons are applied to the inputs of neurons and produce the next values of the output currents of neurons. On the \"downward motion,\" the matrix of connectivity weights (network matrix) is applied to the (connecting of) neuron outputs (and the vector of network inputs) and produces the (connecting of the) vectors of the) inputs of the input currents of neurons."}, {"heading": "2.3. Pragmatic Power of DataflowMatrix Machines as a Programming Platform", "text": "The pragmatic power of data flow matrix machines is much higher than the pragmatic power of vanilla RNNs [3]. The ability to handle streams of sparse arrays is critical for the ability to implement various algorithms based on hash maps and similar structures without additional runtime and memory overhang. Neurons with linear activation functions such as identity allow us to imply memory primitives such as accumulators, leaky accumulators, etc. [14] Multiplicative neurons that implement gating mechanisms (\"multiplicative masks\") that serve as blurred conditionals and can be used to attenuate and redirect data flows in the network. Multiplicative neurons are implicitly precursors of modern recursive neural network architectures such as LSTM and Gated Recurrent Unit Networks (which can be used to pre-orchestrate certain network [C] structures."}, {"heading": "2.4. Self-referential Mechanism", "text": "There is a history of research studies suggesting that it may be fruitful for a neural network to reference and update its own weights [15]. However, this is difficult with standard neural networks based on scalar currents. It is necessary to update the network matrix based on individual elements, and it is much easier to encode the location of matrix elements (row and column indexes) within real numbers. This often results in rather complicated and fragile structures that are highly sensitive to small changes in parameters. If networks can process arbitrary linear currents, self-referential mechanisms become much easier. Simply integrate neurons that process matrix currents, which makes these matrices shapes suitable for network matrices in a given context. Then, one can dedicate a particular neuron to the Self and its latest output as a network matrix that will use [the matrix selective matrix to other matrix types]."}, {"heading": "3. Dataflow Matrix Machines Based on the Vector Space Generated by Finite Strings", "text": "There are many kinds of linear flows, there are many kinds of neurons, each type of neuron has its own input and output, and each of its inputs and outputs has a certain kind of linear flows associated with it. This is quite normal in the world of typed programming languages, but it is unfavorable for Lisp-based systems. It also feels more biorealistic to have no strong constraints and to be able to model and restructure the networks at runtime and to transform these networks without fear of runtime exceptions. It turns out that you can build a structure of sufficient universality based on a single vector space, and that this vector space is also expressive enough to represent activation functions of variable input and output parameters through transformations with one input and one output."}, {"heading": "3.1. Vector Space V", "text": "There are several fertile ways to view elements of V.3.1.2. Finite Prefix Trees with Numerical Leaves. One can view vector space V as the space of finite formal linear combinations of elements from L over reals.There are several fertile ways to view elements of V.3.1.2. Finite Prefix Trees with Numerical Leaves. One can associate vector space as the space of finite formal linear combinations from L over reals.There are several fertile ways to associate elements of V.3 Prefix Trees with numerical Leaves. One can associate the term \u03b1l1. ln (\u03b1) R, l1. L), with a path in a tree labeled with the nodes with l1,.. Then an element of V (a finite element of such terms) is associated with an indirect tree of L."}, {"heading": "3.2. DMMs with Variadic Neurons", "text": "In fact, it is such that it is a way in which you are able to put yourself and the others in the drawers. (...) In fact, it is such that most of us are able to refer to the drawers. (...) In fact, it is as if you are directed to the drawers. (...) It is as if you are in the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer, the drawer of the drawer of the drawer, the drawer of the drawer of the drawer, the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer, the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer of the drawer"}, {"heading": "4. DMMs and Machine Learning", "text": "DMMs can simply be seen as a very strong generalization of RNNs. Alternatively, DMMs can be seen as a bridge between RNNs and programming languages. At the same time, there is a strong trend to implement and train networks with sparse connectivity patterns, rather than building them out of individual neurons."}, {"heading": "Appendix A.", "text": "A.1. Linear Streams of Probabilistic SamplesIn this paper, we look at DMMs over real numbers. Sometimes, you have to represent a stream of large vectors, e.g. a stream of probability distributions over a measurable space X. Typically, you would have to approximate such a flow through a stream of samples from these probability distributions. To have a vector space and allow linear combinations with negative coefficients, we look at the space of all finite, signed measurements over X, and we look at samples as pairs < x, s > where the sample X and s is a flag that takes 1 and -1 as values. Let's say we look at flows of finite, signed measurements over X, \u00b51,.."}, {"heading": "Appendix B.", "text": "B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B."}, {"heading": "Appendix C.", "text": "C.1. \"Two-stroke motor\" for a standard DMM \"downward motion\": for all inputs xi, Ck so that there is a non-zero weight wt (i, Ck), (j, Cl): xt + 1i, Ck = \u2211 {(j, Cl) | wt (i, Ck), (j, Cl) 6 = 0} wt (i, Ck), (j, Cl) y t j, Cl. Note that xt + 1i, Ck and y t j, Cl are in fact no longer numbers, but vectors4, so that the type correctness state states that wt (i, Ck), (j, Cl) y t j, Cl) cannot be zero only if xi, Ck and y t j, Cl belong to the same vector space. \"Upward motion\": for all active neurons type C: yt + 11, C,..., t + 1, c, because C, f1, C, + C, are not required (handling x)."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["M. Abadi"], "venue": "Learning on Heterogeneous Distributed Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Dataflow Matrix Machines as Programmable, Dynamically Expandable, Self-referential Generalized Recurrent Neural Networks, May 2016", "author": ["M. Bukatin", "S. Matthews", "A. Radul"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Programming Patterns in Dataflow Matrix Machines and Generalized Recurrent Neural Nets, June 2016", "author": ["M. Bukatin", "S. Matthews", "A. Radul"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Notes on Pure Dataflow Matrix Machines: Programming with Self-referential Matrix Transformations, October 2016", "author": ["M. Bukatin", "S. Matthews", "A. Radul"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks, January 2017", "author": ["C. Fernando"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Differentiable Functional Program Interpreters, November 2016", "author": ["J. Feser", "M. Brockschmidt", "A. Gaunt", "D. Tarlow"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "A Logical Calculus of the Ideas Immanent in Nervous Activity", "author": ["W. McCulloch", "W. Pitts"], "venue": "The Bulletin of Mathematical Biophysics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1943}, {"title": "Evolving Deep Neural Networks, March 2017", "author": ["R. Miikkulainen"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Bayesian Sketch Learning for Program Synthesis, March 2017", "author": ["V. Murali", "S. Chaudhuri", "C. Jermaine"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Differentiable Programming, August 2016", "author": ["A. Nejati"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Neural Networks, Types, and Functional Programming", "author": ["C. Olah"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "On Connectionist Models of Natural Language Processing", "author": ["J. Pollack"], "venue": "PhD thesis, University of Illinois at Urbana-Champaign,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "A \u201cSelf-Referential", "author": ["J. Schmidhuber"], "venue": "Weight Matrix,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "On the computational power of neural nets", "author": ["H. Siegelmann", "E. Sontag"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "In Section 2 we overview dataflow matrix machines, a generalization of RNNs based on arbitrary linear streams, neurons of arbitrary nonnegative input and output arity, a novel model of unbounded memory, and well-developed self-referential facilities, following [2], [3], [4].", "startOffset": 261, "endOffset": 264}, {"referenceID": 2, "context": "In Section 2 we overview dataflow matrix machines, a generalization of RNNs based on arbitrary linear streams, neurons of arbitrary nonnegative input and output arity, a novel model of unbounded memory, and well-developed self-referential facilities, following [2], [3], [4].", "startOffset": 266, "endOffset": 269}, {"referenceID": 3, "context": "In Section 2 we overview dataflow matrix machines, a generalization of RNNs based on arbitrary linear streams, neurons of arbitrary nonnegative input and output arity, a novel model of unbounded memory, and well-developed self-referential facilities, following [2], [3], [4].", "startOffset": 271, "endOffset": 274}, {"referenceID": 6, "context": "One popular approach to providing Turing complete generalizations of RNNs with unbounded memory is to use an RNN as a controller to a Turing machine tape or another model of external memory [9], [8], [17].", "startOffset": 190, "endOffset": 193}, {"referenceID": 13, "context": "Another approach is to allow reals of unlimited precision, in effect using a binary expansion of a real number as a tape of a Turing machine [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "Memory and network capacity can be dynamically added by gradually making more weights to become non-zero [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "The pragmatic power of dataflow matrix machines is considerably higher than the pragmatic power of vanilla RNNs [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 11, "context": "The ability to have multiple inputs allows us to have multiplicative neurons implementing mechanisms for gating (\u201cmultiplicative masks\u201d), which serve as fuzzy conditionals and can be used to attenuate and redirect flows of data in the network [14].", "startOffset": 243, "endOffset": 247}, {"referenceID": 3, "context": "Multiplicative neurons are implicitly present in modern recurrent neural network architectures such as LSTM and Gated Recurrent Unit networks (Appendix C of [4]).", "startOffset": 157, "endOffset": 160}, {"referenceID": 12, "context": "There is a history of research studies suggesting that it might be fruitful for a neural network to be able to reference and update its own weights [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "Then one can dedicate a particular neuron Self and use its latest output as the network matrix [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "For example, one can have updating neurons creating deep copies of network subgraphs and use those to build pseudo-fractal structures in the body of the network [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "One can argue that the ability of the network to transform the matrix defining the topology and weights of this network plays a fundamental role in the context of programming with linear streams, similar to the role of \u03bb-calculus in the context of programming via string rewriting [4].", "startOffset": 281, "endOffset": 284}, {"referenceID": 0, "context": "For example, RNN-related classes in TensorFlow [1] provide strong evidence of that trend.", "startOffset": 47, "endOffset": 50}, {"referenceID": 10, "context": "In recent years, some authors suggested that synthesis of small functional programs and synthesis of neural network topology from small number of modules are closely related problems [13], [12].", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": "In recent years, some authors suggested that synthesis of small functional programs and synthesis of neural network topology from small number of modules are closely related problems [13], [12].", "startOffset": 189, "endOffset": 193}, {"referenceID": 5, "context": "[7], [10]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7], [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "[11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Therefore, thinking somewhat more long-term, if DMMs turn out to be a sufficiently popular platform to handcraft DMM-based software manually, this might provide a corpus of data useful for program synthesis, similarly to the use of a corpus of hand-crafted code in [11], potentially giving this approach an advantage over synthesis of low-level neural algorithms.", "startOffset": 265, "endOffset": 269}, {"referenceID": 4, "context": "The availability of self-referential and self-modifying facilities might be quite attractive from the viewpoint of machine learning, given their potential for learning to learn and for the network to learn to modify itself, especially in the context of large networks which continue to gain experience during their lifetime (such as, for example, PathNet [6]).", "startOffset": 355, "endOffset": 358}], "year": 2017, "abstractText": "We overview dataflow matrix machines as a Turing complete generalization of recurrent neural networks and as a programming platform. We describe vector space of finite prefix trees with numerical leaves which allows us to combine expressive power of dataflow matrix machines with simplicity of traditional recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}