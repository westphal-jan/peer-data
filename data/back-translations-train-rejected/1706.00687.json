{"id": "1706.00687", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Weight Sharing is Crucial to Succesful Optimization", "abstract": "Exploiting the great expressive power of Deep Neural Network architectures, relies on the ability to train them. While current theoretical work provides, mostly, results showing the hardness of this task, empirical evidence usually differs from this line, with success stories in abundance. A strong position among empirically successful architectures is captured by networks where extensive weight sharing is used, either by Convolutional or Recurrent layers. Additionally, characterizing specific aspects of different tasks, making them \"harder\" or \"easier\", is an interesting direction explored both theoretically and empirically. We consider a family of ConvNet architectures, and prove that weight sharing can be crucial, from an optimization point of view. We explore different notions of the frequency, of the target function, proving necessity of the target function having some low frequency components. This necessity is not sufficient - only with weight sharing can it be exploited, thus theoretically separating architectures using it, from others which do not. Our theoretical results are aligned with empirical experiments in an even more general setting, suggesting viability of examination of the role played by interleaving those aspects in broader families of tasks.", "histories": [["v1", "Fri, 2 Jun 2017 13:56:59 GMT  (39kb)", "http://arxiv.org/abs/1706.00687v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "ohad shamir", "shaked shammah"], "accepted": false, "id": "1706.00687"}, "pdf": {"name": "1706.00687.pdf", "metadata": {"source": "CRF", "title": "Weight Sharing is Crucial to Succesful Optimization", "authors": ["Shai Shalev-Shwartz", "Ohad Shamir"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 6.00 687v 1 [cs.L G] 2J unExploiting the great expressiveness of Deep Neural Network architectures is based on the ability to train them. While current theoretical work largely delivers results that show the severity of this task, empirical evidence usually differs from this line, with success stories abounding. A strong position among empirically successful architectures is taken by networks that use extensive weight sharing, either through revolutionary or recurring layers. Furthermore, characterizing certain aspects of different tasks that make them \"more difficult\" or \"easier\" is an interesting direction that is examined both theoretically and empirically. We consider a family of ConvNet architectures to be inadequate and prove that weight sharing can be decisive from an optimizing perspective."}, {"heading": "1 Introduction", "text": "There are many approaches from which deep learning (DL) can be studied. Very popular is the direction of empirical success, where extensive research efforts have led to a state of art that results in overwhelming breakthroughs in a wide range of tasks. It is necessary to read between the lines to gain insight into the difficulties that practitioners face on their way to success, especially when it comes to the success of the Descent Gradient (GD), which aspects of a task cause the general gradient-based DL approaches to succeed or fail. In this paper, we study this question for a simple yet powerful ConvNet architecture: a Convolutionary Layer, Mapping k Image Patches, each dimension d, followed by a non-linear activation, a fully connected layer with a layer FC layer."}, {"heading": "1.1 Related Work", "text": "Recently, several papers have attempted to examine the optimization performance of gradient-based methods for neural networks. To give just a few relevant examples: [16, 5, 20, 8, 9] look at the optimization landscape for different networks and show that it exhibits favorable properties under different assumptions, but does not take into account the behavior of a particular algorithm. Other papers, such as [13, 2, 10, 23] show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods. Closer to our work, [1, 4, 7] provide positive learning results through gradient-based algorithms, but do not show the benefit of a revolutionary architecture for optimization performance compared to a fully networked architecture. The difficulty of learning in the case of Boolean functions that use the degree of the target function has been discussed in the statistical query literature, for example [6]. In terms of techniques, our construction is inspired by the [18] and [19] target functions proposed in the proposed questions."}, {"heading": "2 Empirical Demonstration", "text": "The target function we want to learn is the form g * (x): we also learn a function (x1,.., xk) with which we use the real functions (x1,.,.), with which we use the real functions (x1,.), (x1), (x1,.), (x2), (x2), (x2), (x2), (x3), (x3), (x3), (x3), (x3), (x3), (x3), (x4), (x4), (x4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (4), x4 (x4), x1, x1, x1, x2 (x1), x2 (x2), x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), ("}, {"heading": "3 Sum of Low and High Degree Waves", "text": "In this section we provide our first separation result between the WS- and FC architectures. Leave x = (x1,.., xk), x = (w1,..., wk), x = (x1,.), x = (x1), x = (x1,.), x = (x1,.), x = (x1,.), x = (x1), x = (x1), x = (x1), x = (x1), x = (x1), x (x1), x (x1), x (x1), x (x1), x (x), x (x1), x (x1), x (x), x (x1), x (x1), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x, x, x, x, x, x, x, x (x, x, x, x, x, x, x, x), x (x), x (x), x (x, x, x, x, x (x), x (x), x, xx (x1), x (x1), x (x1), x (x1), x (x1), x (x1), x (x (x1), x (x1), x (x1), x (x1), x (x1), x (x1), x (x1), x (x1), x (x (x1), x (x1), x (x1), x (x (x1), x (x1), x (x1), x (x1), x (x1), x (x1), x (x (x1), x (x1), x (x1), x (x1), x (x1), x (x1), x (x (x (x, x (x, x, x, x, x, x, x, x"}, {"heading": "3.1 Hardness Result for Optimizing F using GD - FC Architecture", "text": "Theorem 1 assumes that k > 1, the following applies to some numerical constants c1, c2, c3, c4: for all w fulfilling this constant (w2,..., wk), it applies to all w fulfilling this constant (w2,..., wk), F (w), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c)), c), c), c), c), c), c), c), c), c), c), c), c), c), c)), c), c), c), c), c), c)), c), c), c), c), c), c), c), c), c)), c), c), c), c), c), c), c), c), c)), c), c), c)), c), c), c), c), c)), c), c), c)), c), c), c), c)), c), c), c), c), c), c), c)),"}, {"heading": "3.2 Positive Result for Optimizing F using GD - WS Architecture", "text": "We show that when using a WS architecture, the lens is strongly convex transformed, allowing simple evidentiary techniques to be applicable.The proof is in Appendix B.2.Theorem 2 Using the WS architecture, F is strongly convex, minimized at u0, and meets maxw0 \u03bbmax (0x2F (w0))) minw0 \u03bbmin (0x2F (w0)) \u2264 5, where \u03bbmax (M) and \u03bbmin (M) are the uppermost and lowest eigenvalues of a positively defined matrix M. Therefore, the gradient descend from any point w (0) and with the corresponding step size in at most 5 \u00d7 log (0x2) iterations will reach a point where the WS architecture is satisfactory."}, {"heading": "4 Sum of Low and High Degree Parities", "text": "This section formalizes our main result, namely a separation between MS and FC architecture to learn a target function, g \u0445 \u0445 h * if g * = g * consists of both high and low frequencies; the negative result for FC architecture is given in Theorem 3 and the positive result for WS architecture in Theorem 4."}, {"heading": "4.1 Definitions, Notation", "text": "To simplify the analysis, we use the erf function (w): erf (x) w (x) w (p) x \u2212 x \u2212 t2dt. We believe that our analysis applies to additional functions, such as the popular Tanh function. Define, for k N, a function family H (k) FC, parameterized by w (Rd) k, and defined by: p (k) w (x) l = 1\u03c3 (w l xl).Define a subclass, parameterized by w0 (k) WS, by: p (k) w0 (l) = 1\u043c (w).Definition of the subclass, parameterized by w0 (k) WS, by: p (k) w0 (x).Definition of the superclass."}, {"heading": "4.2 Non degeneracy depending on \u2016u0\u20162", "text": "We show that at large values of k and small values of u 0 xl pu0 (x) disappear exponentially. We show that the expected norm of the target function is lower depending on k and therefore overcomes this possible degeneration."}, {"heading": "4.3 Exact Gradient Expressions", "text": "To the dynamics (k) w (k) p (k) p (GD) p (K) p (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n) n (K) n n (K) n (K) n (K) n (K) n (K) n (K) n (K n (K n (n) n (K n (n (K n (K n (K n (K n (K n (K n) n (K n (K n (K n (K n (K n (K n (K n) n (K n (K n (K n (K n (K n) n (K n (K n) n (K n) n n n n (K n n (K n (n) n (K n) n n n n n (K n (K n (K n (n) n) n (K n (K n (K n (n) n (K n) n (K n (n (K n (K n (n) n (K n (n) n (K n (n) n (K n (n (K n (n (n (n) n (K n (n) n (n (K n) n (n (n) n (K n) n) n (n (K n) n (K n (n) n (n (K n (n (K n) n (K n (n) n) n (K n) n (K n (K n (K n) n (n (K n (n (n (K n) n) n (K n) n) n (K n) n) n (K n (K n (K n n (K n) n) n) n (K n) n) n (K n (K n) n (K n (K n"}, {"heading": "4.6 Phase 1 - Angle Convergence", "text": "Suppose that, as shown in Section 4.2, the angle between u0, w (t), 0 (t), 0 (t), 0 (t), and (beyond) the angle between u0, w (T), 0 (T) and (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T) (T (T) (T (T) (T) (T (T) (T (T) (T) (T (T) (T (T) (T) (T) (T (T) (T) (T (T) (T (T) (T) (T) (T) (T (T (T) (T) (T (T) (T (T) (T) (T) (T (T) (T) (T) (T) (T (T) (T (T) (T) (T (T) (T) (T) (T (T) (T) (T (T) (T) (T (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T"}, {"heading": "4.7 Phase 2", "text": "We use the same assumptions as in Section 4.6. We start with a theorem showing that the gradient of F (1) steers the weights towards the optimum, using techniques similar to those in [12, 11, 14]. Theorem 6 For some L2 (s) = 0 (1), < w0 \u2212 u0, 0 (w0) > F (1) (w0) > L2 (s) k (w0 \u2212 u0). Theorem 6 After specifying the above result, we next show that if the angle between w0 and u0 is small (as we have shown is the case in polynomic time, after the first phase of the optimization, in Section 4.6), the gradient of F (1, k) has the same property as the gradient of F (1), namely, it also points in a good direction."}, {"heading": "A Learning g\u2217 for Small k", "text": "For the sake of simplicity, let us consider the problem of learning a function g \u0445: {\u00b1 1} k \u2192 R. The argument can easily be extended to the case in which the domain g * is below an additional Lipschitzness assumption [\u2212 1, 1] k, which is the case if the activation of h * is, for example, the tanh function. Let us consider an arbitrary distribution, D, above {\u00b1 1} k. Using term 19.2 in [17], we have found that if m > 2k / k then awaiting the selection of the sample, the probability mass of vectors in {\u00b1 1} k, which does not belong to my sample, finds at most a function g that matches all points in the sample, is sufficient to guarantee that g matches g \u00b2 on all but a fraction of the vectors in {\u00b1 1} k."}, {"heading": "B Proofs of Section 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proofs of Section 3.1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Theorem 1:", "text": "First, we find that F (w) equalscelscel2k 2 E (w) squalscelscel2k 2 E (w) squalscel2k 2 E (w) squalsc2k 2 E (w) squalsc2k 2 E (w) squalsc2k 2 (w) squalsc2k 2 (w) squalsc2k 2 (w) squalsc2k 2 (w1 \u2212 u0) squalsc2k (x1x 1) (w1 \u2212 u0) quelscalsc2k 2 (w1) quelscelscelscelscelscelscelscelscelscel2k 2 (w1) quelscelscelscelscelscelscelscel2k (s) quelscelscelscelscelscel2k (2p) quelscelscelscelscelscelscelscel2k 2 (w1) quelscelscelscelscelscelscel2k 2 (w1) quelscelscelscelscelscelscelscelscel2k (2p) (2p) quelscelscelscelscelscelscelscelscelscelscelscelscelscelscelscel2k (2p) (w1) (w1) quelscelscelscelscelscelscelscelscelscelscelscelscelscelscelscelscel2p)"}, {"heading": "B.2 Proofs of Section 3.2", "text": "The fact that u0.K (-K) -K (-K) -D (-K) -D (-D) -D (-K) -D (-K) -D (-K) -D (-K) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) -D (-D) (D) (-D) (D) (-D) (D) (D) (D) (-D) (D) -D (D) (D) (D) (-D) -D (-D) -D (D) -D (D) -D (D) (D) (D) -D (D) (D) (-D) (D) (D (-D) (-D) -D (-D) (D (-D) -D (D) -D (-D (-D) -D (D) -D (D (-D)) -D (D (-D (D)) -D (D (D (D (D))) -D (D (D (D (-D (D (D))) -D (D (D (D (-D (D (D))) -D (D (D (-D (D (D (D (D (D))) -D (D (D (D (D (D (D))))) -D (D (D (D (D (D (D (D (-D (D (D (D (D)))))) -D (D (D (D (D (D (D (-D (D (D (D)))))) -D (D (D (D (D (D (-D (D (D (-D (D (D (-D)))"}, {"heading": "C Proofs of Section 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proofs of Section 4.2", "text": "The proof for Lemma 2: Suffice it to show that V\u03c3 (u0, u0) = Ex1\u03c3 (u 0 x1) 2 > (1 \u2212 1k), because in this case by the independence of x1,.., xk we have via Ex [(p (k) u0 (x) 2] = (V1 [u 0 x1) 2]) k > (1 \u2212 1 k) k > 14. To show that we have this value from Lemma 1 (u0, u0) > 1 \u2212 1k, note that from Lemma 1 we have this V\u03c3 (u0, u0) = 2\u03c0 sin \u2212 1 (2 \u00b2). Indicate by f (a) the value for which 2\u03c0 sins \u2212 1 (f (a) 1 + f (a))) = 1 \u2212 a. After standard algebraic manipulations we have thatf (a) = 2 \u2212 cos."}, {"heading": "C.2 Proofs of Section 4.3", "text": "The proof for Lemma 3: We start by giving F (1) (k) (p) (k) (k) (w) (w) (w) (w) (w) (w) (x) \u2212 p (x) 2] = Ex [(p (1)) \u2212 p (1) \u2212 p (1) \u2212 p (1) \u2212 p (k) (k) (k) (k) (w) (k) (w) (k) (w) (k) (w) (k) (w) (k) (k) (k) (k) (k) (k) (k) (w) (k) (w) (k) (k) (w) (k) (k) (w) (k) (w) (k) (k) (w) (k) (k) (w) (k) (k) (w) (k) (k) (k) (k) (w) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (w) (k) (k) (w) (k) (k) (w) (k) (k) (k) (k) (k) (w) (k) (k) (k) (k) (k) (k) (w) (k) (k) (k) (k) (k) (w) (k) (k) (k) (w) (k) (k) (k) (w) (k) (k) (k) (k) (k) (w) (k) (k) (w) (k) (k) (k) (w) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (p (k) (k) (k) (k) p (p (k) p (p (k) p (k) p (k) p (p (p (p (p) p (p)"}, {"heading": "C.3 Proofs of Section 4.4", "text": "The Demonstration of Lemma (wj, u0) = 2: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0"}, {"heading": "C.4 Proofs of Section 4.5", "text": "The proof for Theorem 5: It is therefore sufficient to show that the second semester is positive."}, {"heading": "C.4.1 Proof of Theorem 7", "text": "Before examining Theorem 7, we note that two technical lemmas: Lemma 12 + 1 (1) + 2 (2) + 2 (2) + 2 (2) + 2 (2) + 2 (2) + 2 (2) + 2 (1) + 2 (2) + 2 (1) + 2 (2) + 2 (2) + 2 (2) + 2 (2) + 2 (2) + 2 (0) + 2 (0) + 2 (0) + 2 (0) + 2 (0) + 2) + 2 (0) + 2) + 2 (0), 0 (0), 0 (0), 0 (0), 0 (0), 0 (0), 0 (0), 0 (0), 0 (0), 0 (0), 0 (0) (0) (0) (0)."}, {"heading": "C.4.2 Proof of Theorem 8", "text": "Proof of Theorem 8 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 fp (t) < f (1, k) (w0) (w0) (w0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) () () (0 (0) () (0 () (0) (0) () (0 () () (0 () () (0) (0 (0) () () (0 () (0 (0 () () () () (0) (0 () () (0 () () (0 (0 () () (0 () () () (0) (0 () () () () () (0 (0 ("}], "references": [{"title": "Learning polynomials with neural networks", "author": ["Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Weakly learning dnf and characterizing statistical query learning using fourier analysis", "author": ["Avrim Blum", "Merrick Furst", "Jeffrey Jackson", "Michael Kearns", "Yishay Mansour", "Steven Rudich"], "venue": "In Proceedings of the twenty-sixth annual ACM symposium on Theory of computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Globally optimal gradient descent for a convnet with gaussian inputs", "author": ["Alon Brutzkus", "Amir Globerson"], "venue": "arXiv preprint arXiv:1702.07966,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Approximate resilience, monotonicity, and the complexity of agnostic learning", "author": ["Dana Dachman-Soled", "Vitaly Feldman", "Li-Yang Tan", "Andrew Wan", "Karl Wimmer"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Sgd learns the conjugate kernel class of the network", "author": ["Amit Daniely"], "venue": "arXiv preprint arXiv:1702.08503,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["Benjamin D Haeffele", "Ren\u00e9 Vidal"], "venue": "arXiv preprint arXiv:1506.07540,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Identity matters in deep learning", "author": ["Moritz Hardt", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1611.04231,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1506.08473,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["Adam Tauman Kalai", "Ravi Sastry"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "On the computational efficiency of training neural networks", "author": ["Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The landscape of empirical risk for non-convex losses", "author": ["Song Mei", "Yu Bai", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1607.06534,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["Itay Safran", "Ohad Shamir"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Understanding machine learning: From theory to algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Failures of deep learning", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Shaked Shammah"], "venue": "arXiv preprint arXiv:1703.07950,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Distribution-specific hardness of learning neural networks", "author": ["Ohad Shamir"], "venue": "arXiv preprint arXiv:1609.01037,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "author": ["Daniel Soudry", "Yair Carmon"], "venue": "arXiv preprint arXiv:1605.08361,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Schaum\u2019s handbook of formulas and tables", "author": ["Murray R Spiegel"], "venue": "MacGraw Hill, New York,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1968}, {"title": "Computing with infinite networks. Advances in neural information processing systems, pages", "author": ["Christopher KI Williams"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}], "referenceMentions": [{"referenceID": 17, "context": "In [18], it has been shown that no Gradient Based algorithm can succeed in learning h\u2217 if g\u2217 is the parity of the signs of its input.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 4, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 19, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 7, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 8, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 12, "context": "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.", "startOffset": 21, "endOffset": 36}, {"referenceID": 1, "context": "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.", "startOffset": 21, "endOffset": 36}, {"referenceID": 9, "context": "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.", "startOffset": 21, "endOffset": 36}, {"referenceID": 0, "context": "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.", "startOffset": 25, "endOffset": 34}, {"referenceID": 3, "context": "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.", "startOffset": 25, "endOffset": 34}, {"referenceID": 6, "context": "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.", "startOffset": 25, "endOffset": 34}, {"referenceID": 5, "context": "The hardness of learning in the case of Boolean functions, using the degree of the target function, was discussed in the statistical queries literature, for instance in [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 17, "context": "In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.", "startOffset": 85, "endOffset": 93}, {"referenceID": 2, "context": "[3]), to study the difficulty of learning with gradient-based methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "The only difference across this aspect, when learning a degree 1 parity with a convolutional architecture, between known and unknown g, for Gaussian data, is perhaps due to smaller Signal to Noise Ratio in the case of learning g, as suggested in [18].", "startOffset": 246, "endOffset": 250}, {"referenceID": 21, "context": "(1) The following definition and lemma, due to [22], will be useful in our analysis.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "Lemma 1 ([22]) Let \u03c3 be the erf function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Lemma 7 Let c \u2208 [0, 1] and assume |\u3008 wj \u2016wj\u2016 , u0 \u2016u0\u2016 \u3009| < c, or \u2016wj\u2016 \u2264 c/ \u221a 2.", "startOffset": 16, "endOffset": 22}, {"referenceID": 11, "context": "The proof uses monotonicity of \u03c3, with similar techniques as found in [12, 11, 14].", "startOffset": 70, "endOffset": 82}, {"referenceID": 10, "context": "The proof uses monotonicity of \u03c3, with similar techniques as found in [12, 11, 14].", "startOffset": 70, "endOffset": 82}, {"referenceID": 13, "context": "The proof uses monotonicity of \u03c3, with similar techniques as found in [12, 11, 14].", "startOffset": 70, "endOffset": 82}, {"referenceID": 0, "context": "References [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Alon Brutzkus and Amir Globerson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Dana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, and Karl Wimmer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Amit Daniely.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Benjamin D Haeffele and Ren\u00e9 Vidal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Moritz Hardt and Tengyu Ma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Adam Tauman Kalai and Ravi Sastry.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Song Mei, Yu Bai, and Andrea Montanari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Yurii Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Itay Safran and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Shai Shalev-Shwartz and Shai Ben-David.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Daniel Soudry and Yair Carmon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Murray R Spiegel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Christopher KI Williams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "2 in [17] we have that if m > 2/\u01eb then, in expectation over the choice of the sample, the probability mass of vectors in {\u00b11}k that does not belong to my sample is at most \u01eb.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 15, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 19, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 14, "context": "Also, given that F (\u00b7) is strongly convex and satisfies the eigenvalue condition stated in the theorem, the convergence bound for gradient descent follows from standard results (see [15]).", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "Therefore, all the three terms in the argument of the inverse sign are in [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "The claim now follows immediately because for every c \u2208 [0, 1] we have c \u2264 sin(c).", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "For reasonably large k, the argument of the arctan is smaller than 1, and on the interval [0, 1] the value of arctan is larger than half its argument.", "startOffset": 90, "endOffset": 96}], "year": 2017, "abstractText": "Exploiting the great expressive power of Deep Neural Network architectures, relies on the ability to train them. While current theoretical work provides, mostly, results showing the hardness of this task, empirical evidence usually differs from this line, with success stories in abundance. A strong position among empirically successful architectures is captured by networks where extensive weight sharing is used, either by Convolutional or Recurrent layers. Additionally, characterizing specific aspects of different tasks, making them \u201charder\u201d or \u201ceasier\u201d, is an interesting direction explored both theoretically and empirically. We consider a family of ConvNet architectures, and prove that weight sharing can be crucial, from an optimization point of view. We explore different notions of the frequency, of the target function, proving necessity of the target function having some low frequency components. This necessity is not sufficient only with weight sharing can it be exploited, thus theoretically separating architectures using it, from others which do not. Our theoretical results are aligned with empirical experiments in an even more general setting, suggesting viability of examination of the role played by interleaving those aspects in broader families of tasks.", "creator": "LaTeX with hyperref package"}}}