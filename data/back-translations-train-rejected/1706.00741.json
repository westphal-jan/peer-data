{"id": "1706.00741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Prosodic Event Recognition using Convolutional Neural Networks with Context Information", "abstract": "This paper demonstrates the potential of convolutional neural networks (CNN) for detecting and classifying prosodic events on words, specifically pitch accents and phrase boundary tones, from frame-based acoustic features. Typical approaches use not only feature representations of the word in question but also its surrounding context. We show that adding position features indicating the current word benefits the CNN. In addition, this paper discusses the generalization from a speaker-dependent modelling approach to a speaker-independent setup. The proposed method is simple and efficient and yields strong results not only in speaker-dependent but also speaker-independent cases.", "histories": [["v1", "Fri, 2 Jun 2017 16:20:19 GMT  (46kb,D)", "http://arxiv.org/abs/1706.00741v1", "Interspeech 2017 4 pages, 1 figure"]], "COMMENTS": "Interspeech 2017 4 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sabrina stehwien", "ngoc thang vu"], "accepted": false, "id": "1706.00741"}, "pdf": {"name": "1706.00741.pdf", "metadata": {"source": "CRF", "title": "Prosodic Event Recognition using Convolutional Neural Networks with Context Information", "authors": ["Sabrina Stehwien", "Ngoc Thang Vu"], "emails": ["sabrina.stehwien@ims.uni-stuttgart.de", "thang.vu@ims.uni-stuttgart.de"], "sections": [{"heading": "1. Introduction", "text": "Most people who work for the rights of women and men are able to work for the rights of women and men, both for the rights of women and for the rights of men, women and men. (...) Most people who work for the rights of men and women have the same rights and duties as women. (...) Most people who work for the rights of women and men have the same rights and duties as women. (...) Most people who work for the rights of men and women have the same rights and duties as women. (...) Most people who work for the rights of women and men have the same rights and duties as women. (...) Most people who work for the rights of men and women. (...) Most people who work for the rights of women and men, who work for the rights of women and men, the rights of women and the rights of women. (...) Most of women and women. \"(...) Most of men and women.\""}, {"heading": "2. Model", "text": "We apply a CNN model as shown in Figure 1 for PER. The task is designed as a supervised learning task, in which each word is designated as the carrier of a prosodic event or not. Input to CNN is a feature representation of the audio signal of the current word and (optionally) its context. The signal is divided into s overlapping frames and is represented for each frame by a d-dimensional feature vector. Thus, for each utterance, a matrix W-Rd-s is formed, which encompasses all d-characteristics. The number of frames s depends on the duration (signal length) of the word as well as the context window size and the frame displacement. For the convolution operation, we use the 2D cores K (with the width | K |), which encompass all d-characteristics. The following equation expresses the convolution: (W-K) (x, y) = d-K-binding-1-K-conordance (K), j-kiniation (1) (K)."}, {"heading": "2.1. Acoustic Features", "text": "We extract acoustic features from the speech signal using the OpenSMILE toolkit [29]. In this thesis, two distinct sets of features are used: a prosody feature set consisting of 5 features from the OpenSMILE catalog (smoothed out f0, RMSenergy, PCM loudness, intonation probability and harmonics-toNoise ratio) and a Mel feature set consisting of 27 features from the Mel frequency spectrum (similar [23]). Characteristics are calculated for each 20ms frame with a 10ms shift. These two feature sets are used both separately and jointly (concatenated) in the reported experiments. Time intervals specifying the word boundaries provided in the corpus are used to generate the input feature matrices by grouping all frames for each word into an input matrix. Then, zero is added to ensure that all matrices have the same size."}, {"heading": "2.2. Position Indicator Feature", "text": "This type of feature has been suggested for use in neural network models for classifying relations [30, 31]. Previous work has shown the advantages of adding context information to PER [14, 21]. However, the simplest approach is to add features that represent the right and left adjacent segments to form a kind of acoustic context window [11, 13, 24]. However, the caveat of using context windows as input to our CNN model is that it also adds a considerable amount of noise. CNNs \"learning method consists of searching for patterns throughout input and learning abstract global representations of them. Adjacent words may have prosodic events or other prosodic highlighting features that distract from the current word. This effect can be amplified by the fact that the words have variable lengths attached to the input (for this reason, or to indicate the current word)."}, {"heading": "3. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Data", "text": "The data set used in this paper is a subset of BURNC that was manually labeled with prosodic events according to the ToBI labeling standard [12]. The speech data were recorded by 3 female and 2 male speakers and add up to approximately 2 hours and 45 minutes of speech. Table 1 shows the number of words for each speaker in the data sets used for accent and phrase boundary recognition. For the speaker-dependent experiments, the largest speaker subgroup f2b was used according to previous methods [19, 24]. We test our models with 10-fold cross-validation and validated on 1000 words from the respective training set. In the speaker-independent case, the models were trained and tested. As the two tasks are trained and tested separately, we assess the inconsistency in the two data sets as inconsistent for our experiments."}, {"heading": "3.2. Hyperparameters", "text": "The classification model is a two-layer CNN. The first layer consists of 100 two-dimensional cores of the form 6 \u00d7 d and a step of 4 \u00d7 1, d being the number of characteristics. The cores comprise the entire set of characteristics to ensure that all characteristics are learned at the same time. The second layer consists of 100 cores of the form 4 \u00d7 1 and a step of 2 \u00d7 1. The maximum pooling size is set so that the output of each maximum pool on each of the 100 characteristic cards is shaped x. Thus, this hyperparameter varies according to the size of the input matrix, but remains constant in each experiment due to the zero padding. A dropout of p = 0.2 is applied before the Softmax layer. The models are trained for 50 eras with an adaptive learning rate (Adam [32]) and L2 regulation."}, {"heading": "4. Results", "text": "We report the results for each experiment with three context variants: no context (1 word), right and left context words (3 words), and right and left context words with position characteristics (3 words + PF)."}, {"heading": "4.1. Pitch Accent Recognition", "text": "Table 2 shows the results for accent recognition on the dataset of a single speaker and Table 3 shows the results obtained in speaker-independent experiments.The model yields up to 2This avoids too large a discrepancy between validation and test data. 84% detection performance when looking at only the current word with no additional context in the speaker-independent configuration and almost 82% in the speaker-independent experiments.The results show a large drop in performance down to the basic level of the majority class when extending input to the right and left context words.After adding the position information to the characteristics, the accuracy of all tasks from singleword input increases and exceeds that of the speaker-independent case. We achieve up to 86.3% accuracy in accent recognition to f2b, which is comparable to the best previously obtained results on purely acoustic inputs. This indicates that not only the position technique is critical when our 69% contextor is associated with the most of the contexts."}, {"heading": "4.2. Phrase Boundaries", "text": "The results for phrase boundary detection seem to follow a similar pattern as for pitch accent detection. Again, we see a drop in performance when extending from the 1-word to the 3-word input window, although this effect is not as pronounced for phrase boundaries. Adding position indicators improves the results in all cases. In the speaker-dependent task, the combined prosody and Mel feature set provides the best performance, while the small prosodic feature set seems to be the best choice for the speaker-independent task. However, these differences are not as pronounced as for pitch accents. In the f2b experiments, we achieve 90.5% and 88.8% accuracy for detection and classification, respectively, and in the loudspeaker-independent setup, we achieve almost 90% accuracy for detection and 87.3% for classification. In contrast to the results for pitch characteristics, we find that loudspeakers are most accurate at 1b and 1b respectively."}, {"heading": "4.3. Discussion", "text": "An interesting finding in the paper above is the effect of adding context boxes without position features on the two tasks presented. We observe that adding \"uninformed\" context information is more detrimental to the recognition of pitch accents than to phrase boundaries. Although we did not investigate this effect further in the present study, it can be explained as follows. Pitch accents are more likely to be local phenomena that occur on stressed syllables and are more common in the data. Tonal phrase boundaries, as described in ToBI standard3, not only include longer speech ranges (as they consist of an intermediate phrase accent and an intonational phrase boundary tone), but are also more sparse, as they occur only at the end of intonational phrases. This means that the model may be less sensitive to local events or changes in adjacent segments, and that it is less likely that phrase boundaries will occur in two consecutive phrases, such as small ones used in almost all cases."}, {"heading": "5. Conclusion", "text": "This paper presents experimental results that CNNs use for word-based PER on low acoustic characteristics, while emphasizing the effect of the inclusion of context information. We show that the model performs well simply by learning simple frame-based characteristics, and that performance can be increased by adding characteristics to the input that represents the word and its context. Our model generalizes well from a loudspeaker-dependent setup to a loudspeaker-independent setup, yielding an accuracy of 86.3% and 83.6% for pitch detection, respectively. Even in the more demanding task of classifying ToBI types, we get results for loudspeakers that are comparable to previous related work, namely 69% accuracy for pitch accents and 87.3% for pitch boundaries. Furthermore, the presented method can easily be applied to other datasets. Although a more detailed analysis is necessary to evaluate the performance of individual events, we conclude that the task is well suited to their occurrence."}, {"heading": "6. References", "text": "[1] J. Hirschberg and J. B. Pierrehumbert, \"The intonational structur-ing of discourse,\" in: 24th Annual Meeting of the Association for Computational Linguistics, Columbia University, New York, USA, July 10-13, 1986, pp. 136-144. [2] E. Selkirk, \"Sentence prosody: Intonation, stress and phrasing,\" in: The handbook of phonological theory, J. A. Goldsmith, Ed. Oxford, 1995, pp. 550-569. [3] H. Truckenbrodt, \"On the relation between syntactic phrases and phonological phrases,\" Linguistic Inquiry, no., pp. 219-255, 1999. [4] A. Waibel, Prosody and Speech Recognition. Morgan Kaufmann, 1988."}], "references": [{"title": "The intonational structuring of discourse", "author": ["J. Hirschberg", "J.B. Pierrehumbert"], "venue": "24th Annual Meeting of the Association for Computational Linguistics, Columbia University, New York, New York, USA, July 10-13, 1986., 1986, pp. 136\u2013144.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Sentence prosody: Intonation, stress and phrasing", "author": ["E. Selkirk"], "venue": "The handbook of phonological theory, J. A. Goldsmith, Ed. Oxford: Blackwell, 1995, pp. 550\u2013569.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "On the relation between syntactic phrases and phonological phrases", "author": ["H. Truckenbrodt"], "venue": "Linguistic Inquiry, vol. 30, no. 2, pp. 219\u2013 255, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Prosody and Speech Recognition", "author": ["A. Waibel"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Using prosody to improve automatic speech recognition", "author": ["K. Vicsi", "G. Szasz\u00e1k"], "venue": "Speech Communication, vol. 52, no. 5, pp. 413\u2013426, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved speech recognition using acoustic and lexical correlates of pitch accent in a n-best rescoring framework", "author": ["S. Ananthakrishnan", "S. Narayanan"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, Honolulu, Hawaii, 2007, pp. 873\u2013876.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Prosody dependent speech recognition on radio news corpus of American English", "author": ["K. Chen", "M. Hasegawa-Johnson", "A. Cohen", "S. Borys", "S.-S. Kim", "J. Cole", "J.-Y. Choi"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 14, no. 1, pp. 232\u2013245, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Prosody in Speech Understanding", "author": ["R. Kompe"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Prosody modeling for automatic speech recognition and understanding", "author": ["E. Shriberg", "A. Stolcke"], "venue": "Mathematical Foundations of Speech and Language Processing. Springer, 2004, pp. 105\u2013114.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Prosodic models, automatic speech understanding, and speech synthesis: towards the common ground", "author": ["A. Batliner", "B. M\u00f6bius", "G. M\u00f6hler", "A. Schweitzer", "E. N\u00f6th"], "venue": "Proceedings of the European Conference on Speech Communication and Technology (Aalborg, Denmark), vol. 4. ISCA, 2001, pp. 2285\u20132288.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Detecting pitch accent using pitch-corrected energy-based predictors", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "Proceedings of Interspeech, 2007, pp. 2777\u20132780.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "ToBI: A standard for labelling English prosody", "author": ["K. Silverman", "M. Beckman", "J. Pitrelli", "M. Ostendorf", "C. Wightman"], "venue": "Proceedings of ICSLP, 1992, pp. 867\u2013870.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Experiments on automatic prosodic labeling", "author": ["A. Schweitzer", "B. M\u00f6bius"], "venue": "Proceedings of Interspeech, 2009, pp. 2515\u20132518.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling phrasing and prominence using deep recurrent learning", "author": ["A. Rosenberg", "R. Fernandez", "B. Ramabhadran"], "venue": "Proceedings of Interspeech, 2015, pp. 3066\u20133070.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "An automatic prosody labeling system using ann-based syntactic-prosodic model and gmm-based acoustic-prosodic model", "author": ["K. Chen", "M. Hasegawa-Johnson", "A. Cohen"], "venue": "Proceedings of ICASSP, 2004, pp. 509\u2013512.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Using neural networks to locate pitch accents", "author": ["P. Taylor"], "venue": "Proceedings of the 4th European Conference on Speech Communication and Technology, 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Detecting pitch accents at the word, syllable and vowel level", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "HLT-NAACL, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Prosodic prominence detection in speech", "author": ["F. Tamburini"], "venue": "ISSPA2003, 2003, pp. 385\u2013338.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Pitch accent prediction using ensemble machine learning", "author": ["X. Sun"], "venue": "Proceedings of ICSLP-2002, 2002, pp. 16\u201320.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Automatic prosodic event detection using acoustic, lexical and syntactic evidence", "author": ["S. Ananthakrishnan", "S.S. Narayanan"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 16, no. 1, 2008, pp. 216\u2013228.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Context in multi-lingual tone and pitch accent recognition", "author": ["G.-A. Levow"], "venue": "Proceedings of Interspeech, 2005, pp. 1809\u2013 1812.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploiting contextual information for prosodic event detection using auto-context", "author": ["J. Zhao", "W.-Q. Zhang", "H. Yuan", "M.T. Johnson", "J. Liu", "S. Xia"], "venue": "EURASIP J. Audio, Speech and Music Processing, vol. 2013, p. 30, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic classification of lexical stress in English and Arabic languages using deep learning", "author": ["M. Shahin", "J. Epps", "B. Ahmed"], "venue": "Proceedings of Interspeech, 2016, pp. 175\u2013179.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Enhance the word vector with prosodic information for the recurrent neural network based tts system", "author": ["X. Wang", "S. Takaki", "J. Yamagishi"], "venue": "Proceedings of Interspeech, 2016, pp. 2856\u20132860.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "The Boston University Radio News Corpus", "author": ["M. Ostendorf", "P. Price", "S. Shattuck-Hufnagel"], "venue": "Boston University, Technical Report ECS-95-001, 1995.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Speakerindependent automatic detection of pitch accent", "author": ["K. Ren", "S.-S. Kim", "M. Hasegawa-Johnson", "J. Cole"], "venue": "ISCA International Conference on Speech Prosody, 2004, pp. 521\u2013524.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Classification of prosodic events using quantized contour modeling", "author": ["A. Rosenberg"], "venue": "Proceedings of HLT-NAACL, 2010, pp. 721\u2013724.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1929}, {"title": "Recent developments in opensmile, the Munich open-source multimedia feature extractor", "author": ["F. Eyben", "F. Weninger", "F. Gro\u00df", "B. Schuller"], "venue": "Proceedings of the 21st ACM international conference on Multimedia, 2013, pp. 835\u2013838.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining recurrent and convolutional neural networks for relation classification", "author": ["N.T. Vu", "H. Adel", "P. Gupta", "H. Sch\u00fctze"], "venue": "Proceedings of HLT-NAACL 2016, 2016, pp. 534\u2013539.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Relation classification via recurrent neural network", "author": ["D. Zhang", "D. Wang"], "venue": "arXiv preprint arXiv:508.01006v1, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2017.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Frequency of occurrence effects on pitch accent realisation", "author": ["K. Schweitzer", "M. Walsh", "B. M\u00f6bius", "H. Sch\u00fctze"], "venue": "Proceedings of Interspeech, 2010, pp. 138\u2013141.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "For example, knowing what word in an utterance is pitch accented provides important insight into discourse structure such as focus, givenness and contrast [1, 2].", "startOffset": 155, "endOffset": 161}, {"referenceID": 1, "context": "For example, knowing what word in an utterance is pitch accented provides important insight into discourse structure such as focus, givenness and contrast [1, 2].", "startOffset": 155, "endOffset": 161}, {"referenceID": 2, "context": "Phrasing information and boundary tones for example relate to the syntactic structure [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].", "startOffset": 169, "endOffset": 181}, {"referenceID": 4, "context": "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].", "startOffset": 169, "endOffset": 181}, {"referenceID": 5, "context": "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].", "startOffset": 169, "endOffset": 181}, {"referenceID": 6, "context": "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].", "startOffset": 169, "endOffset": 181}, {"referenceID": 7, "context": "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].", "startOffset": 200, "endOffset": 210}, {"referenceID": 8, "context": "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].", "startOffset": 200, "endOffset": 210}, {"referenceID": 9, "context": "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].", "startOffset": 200, "endOffset": 210}, {"referenceID": 10, "context": "PER distinguishes two subtasks: detection typically refers to the binary classification task (presence or absence of a prosodic event), while prosodic event classification encompasses the full multi-class labelling of prosodic event types [11] e.", "startOffset": 239, "endOffset": 243}, {"referenceID": 11, "context": "as described in the ToBI standard [12].", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Typically the recognition of pitch accents is modelled separately from phrase boundaries, although the acoustic features are quite similar [13, 14, 15].", "startOffset": 139, "endOffset": 151}, {"referenceID": 13, "context": "Typically the recognition of pitch accents is modelled separately from phrase boundaries, although the acoustic features are quite similar [13, 14, 15].", "startOffset": 139, "endOffset": 151}, {"referenceID": 14, "context": "Typically the recognition of pitch accents is modelled separately from phrase boundaries, although the acoustic features are quite similar [13, 14, 15].", "startOffset": 139, "endOffset": 151}, {"referenceID": 12, "context": "Many approaches focus on finding appropriate acoustic representations of prosody [13, 11].", "startOffset": 81, "endOffset": 89}, {"referenceID": 10, "context": "Many approaches focus on finding appropriate acoustic representations of prosody [13, 11].", "startOffset": 81, "endOffset": 89}, {"referenceID": 15, "context": "These features generally describe the fundamental frequency (f0) and energy and can be either frame-based [16] or grouped across segments [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "These features generally describe the fundamental frequency (f0) and energy and can be either frame-based [16] or grouped across segments [17].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "Often acoustic-prosodic features also include the duration of certain segments [13, 18, 19].", "startOffset": 79, "endOffset": 91}, {"referenceID": 17, "context": "Often acoustic-prosodic features also include the duration of certain segments [13, 18, 19].", "startOffset": 79, "endOffset": 91}, {"referenceID": 18, "context": "Often acoustic-prosodic features also include the duration of certain segments [13, 18, 19].", "startOffset": 79, "endOffset": 91}, {"referenceID": 19, "context": "Most successful methods that rely on acoustic features also benefit from the addition of lexicosyntactic information [20, 13, 19].", "startOffset": 117, "endOffset": 129}, {"referenceID": 12, "context": "Most successful methods that rely on acoustic features also benefit from the addition of lexicosyntactic information [20, 13, 19].", "startOffset": 117, "endOffset": 129}, {"referenceID": 18, "context": "Most successful methods that rely on acoustic features also benefit from the addition of lexicosyntactic information [20, 13, 19].", "startOffset": 117, "endOffset": 129}, {"referenceID": 20, "context": "Since prosodic events usually span several segments, many cited approaches add features representing the surrounding segment, while others explicitly focus on context modelling [21, 14, 22].", "startOffset": 177, "endOffset": 189}, {"referenceID": 13, "context": "Since prosodic events usually span several segments, many cited approaches add features representing the surrounding segment, while others explicitly focus on context modelling [21, 14, 22].", "startOffset": 177, "endOffset": 189}, {"referenceID": 21, "context": "Since prosodic events usually span several segments, many cited approaches add features representing the surrounding segment, while others explicitly focus on context modelling [21, 14, 22].", "startOffset": 177, "endOffset": 189}, {"referenceID": 22, "context": "[23] combine the output of a CNN that learns high-level features representations from 27 frame-based Mel-spectral features with global (or aggregated) f0, energy and duration features across syllables for lexical stress detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] train a CNN on continuous wavelet transformations of the fundamental frequency for the detection of pitch accents and phrase boundaries in a speaker-dependent task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "As previously pointed out in [19, 17], the large number of different approaches and task descriptions renders the comparison of PER performance methods quite difficult.", "startOffset": 29, "endOffset": 37}, {"referenceID": 16, "context": "As previously pointed out in [19, 17], the large number of different approaches and task descriptions renders the comparison of PER performance methods quite difficult.", "startOffset": 29, "endOffset": 37}, {"referenceID": 24, "context": "Thus, our results are compared only to approaches that use the Boston University Radio News Corpus (BURNC) [25] and purely acoustic features.", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "Good results for pitch accent detection were reported by Sun [19], namely 84.", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "[24] use CNNs to detect pitch accents and phrase boundaries on the f2b speaker, obtaining 86.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] obtain 83.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Rosenberg [27] reports almost 64% accuracy for pitch accents and 72.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "[15] apply their neural-based method to speaker-independent setups using 4 speakers of BURNC and distinguishing 4 event types.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "An early example of a neural network approach was proposed in [16], and relied only on frame-based acoustic features such as f0 and energy.", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "For regularization, we also apply dropout [28] to this last layer.", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "We extract acoustic features from the speech signal using the OpenSMILE toolkit [29].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "In this work, two different feature sets are used: a prosody feature set consisting of 5 features from the OpenSMILE catalogue (smoothed f0, RMS energy, PCM loudness, voicing probability and Harmonics-toNoise Ratio), and a Mel feature set consisting of 27 features extracted from the Mel-frequency spectrum (similar to [23]).", "startOffset": 319, "endOffset": 323}, {"referenceID": 29, "context": "This type of feature has been proposed for use in neural network models for relation classification [30, 31].", "startOffset": 100, "endOffset": 108}, {"referenceID": 30, "context": "This type of feature has been proposed for use in neural network models for relation classification [30, 31].", "startOffset": 100, "endOffset": 108}, {"referenceID": 13, "context": "Previous work has demonstrated the benefits of adding context information to PER [14, 21].", "startOffset": 81, "endOffset": 89}, {"referenceID": 20, "context": "Previous work has demonstrated the benefits of adding context information to PER [14, 21].", "startOffset": 81, "endOffset": 89}, {"referenceID": 10, "context": "The most straighforward approach is to add features that represent the right and left neighbouring segments to form a type of acoustic context window [11, 13, 24].", "startOffset": 150, "endOffset": 162}, {"referenceID": 12, "context": "The most straighforward approach is to add features that represent the right and left neighbouring segments to form a type of acoustic context window [11, 13, 24].", "startOffset": 150, "endOffset": 162}, {"referenceID": 23, "context": "The most straighforward approach is to add features that represent the right and left neighbouring segments to form a type of acoustic context window [11, 13, 24].", "startOffset": 150, "endOffset": 162}, {"referenceID": 11, "context": "The dataset used in this work is a subset of BURNC that has been manually labelled with prosodic events according to the ToBI labelling standard [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "For the speaker-dependent experiments, the largest speaker subset f2b is used in line with previous methods [19, 24].", "startOffset": 108, "endOffset": 116}, {"referenceID": 23, "context": "For the speaker-dependent experiments, the largest speaker subset f2b is used in line with previous methods [19, 24].", "startOffset": 108, "endOffset": 116}, {"referenceID": 26, "context": "For the classification task, we distinguish 5 different ToBI types of pitch accents and phrase boundaries (as in [27]), where the downstepped accents are collapsed into the non-downstepped ones: The pitch accent classes are (1) H* and !H*, (2) L*, (3) L+H* and L+!H*, (4) L*+H and L*+!H and (5) H+!H*.", "startOffset": 113, "endOffset": 117}, {"referenceID": 31, "context": "The models are trained for 50 epochs with an adaptive learning rate (Adam [32]) and L2 regularization.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "A widely-used measure to enable the generalization of prosodic models across speakers is speaker normalization in the form of z-scoring [11, 15, 33].", "startOffset": 136, "endOffset": 148}, {"referenceID": 14, "context": "A widely-used measure to enable the generalization of prosodic models across speakers is speaker normalization in the form of z-scoring [11, 15, 33].", "startOffset": 136, "endOffset": 148}, {"referenceID": 32, "context": "A widely-used measure to enable the generalization of prosodic models across speakers is speaker normalization in the form of z-scoring [11, 15, 33].", "startOffset": 136, "endOffset": 148}], "year": 2017, "abstractText": "This paper demonstrates the potential of convolutional neural networks (CNN) for detecting and classifying prosodic events on words, specifically pitch accents and phrase boundary tones, from frame-based acoustic features. Typical approaches use not only feature representations of the word in question but also its surrounding context. We show that adding position features indicating the current word benefits the CNN. In addition, this paper discusses the generalization from a speaker-dependent modelling approach to a speaker-independent setup. The proposed method is simple and efficient and yields strong results not only in speaker-dependent but also speaker-independent cases.", "creator": "LaTeX with hyperref package"}}}