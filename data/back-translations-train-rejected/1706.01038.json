{"id": "1706.01038", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "Improving Legal Information Retrieval by Distributional Composition with Term Order Probabilities", "abstract": "Legal professionals worldwide are currently trying to get up-to-pace with the explosive growth in legal document availability through digital means. This drives a need for high efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done with the use of disambiguation rules, applied over the rankings obtained through n-gram statistics. After the ranking is done, its results are evaluated for ambiguity, and disambiguation is done if a result is decided to be unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, an analysis of error and improvement cases is presented for a better understanding of the contributions.", "histories": [["v1", "Sun, 4 Jun 2017 06:57:09 GMT  (1559kb)", "http://arxiv.org/abs/1706.01038v1", null], ["v2", "Sat, 10 Jun 2017 10:49:15 GMT  (0kb,I)", "http://arxiv.org/abs/1706.01038v2", "wrong version"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["danilo s carvalho", "duc-vu tran", "van-khanh tran", "le-nguyen minh"], "accepted": false, "id": "1706.01038"}, "pdf": {"name": "1706.01038.pdf", "metadata": {"source": "CRF", "title": "Improving Legal Information Retrieval by Distributional Composition with Term Order Probabilities", "authors": ["Danilo S. Carvalho", "Vu Duc Tran", "Khanh Van Tran", "Minh Le Nguyen"], "emails": ["nguyenml}@jaist.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.01 038v 1 [cs.I R] 4J un2 01"}, {"heading": "1 Introduction", "text": "The question is to what extent this is purely a problem."}, {"heading": "2 Related Work", "text": "Recent developments in the field of legal information Retrieval (LIR) and Legal Question Answers (LQA) comprise the work of Liu, Chen and Ho [LCH15], which used a method called three-phase prediction (TPP) for the query of relevant statutes in Taiwanese criminal law. They used a hierarchical ranking approach for the legal system, which combines several information response techniques, as well as the query of semantic representatives in the LQA. They have an interesting case in the work of Kim et.al [KXLG16] in which a ranking ranking feature for the formation-characteristics-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-features-in-work of Kim et.al [KXLG16]"}, {"heading": "3 Legal Question Answering \u2013 COLIEE", "text": "Answering a legal question involves: (i) collecting the knowledge necessary to understand the given question, and then (ii) returning the appropriate and correct answer. In the context of the Legal Information Extraction / Entailment (COLIEE) 1 contest, the question is a legal declaration that differs from specific cases, and the required knowledge is contained in the law itself, in the form of organized articles that form a fragment of the Japanese Civil Code. Japan's Civil Code consists of a collection of numbered articles, each containing a series of explanations on a specific topic, e.g. employment contracts, mortgages that contain the knowledge of the relevant legal contributions that either coincide or do not agree with the interpretation of the articles that lead to an affirmative or negative answer."}, {"heading": "4 Proposed Approach", "text": "Considering a point of law raised in natural language, the retrieval of legal information consists of two successive phases: 1) relevance analysis and 2) relevance clarification. First, a ranking of a limited number of relevant articles using R2NC [CNCNCN16] (Section 4.1) is compiled. Next, the first two articles in the ranking are rated in terms of ambiguity (the values are too close to each other) and uncertainty (the values are too low) below specified thresholds. If the values of the articles are ambiguous or uncertain, sentence embeddings are obtained for both the question and for each article in the ranking using word2vec [MSC + 13] and TOP [CN17] (Section 4.3). A new ranking is achieved by calculating the highest sentence embedding cosine similarity of each pair (question, article). Finally, the two lists are compared by a set of rules and a crucial set of relevant articles are selected."}, {"heading": "4.1 Relevance analysis", "text": "The Relevance Analysis stage has been fully performed with R2NC [CNCN16], which can be summarized in the following process: 1. Collect the content for each article; 2. Check references between articles and annotations; 3. Tokenize and POS tag; 4. Remove stopwords: determinators, conjunctions, prepositions and punctuation; 5. Lemmatize words; 6. Generate n-grams; 7. Expand the n-grammset by including referenced articles; 8. Associate article numbers and references; 9. Save the model.Except step 4, each step adds new information to the model. The information is obtained from the text, references and morphological analyses, e.g. POS tags, lemmatis. If an article has references, its n-grammset includes the references \"n-grams.\" In this way, all the information necessary for interpreting an individual article is contained within itself."}, {"heading": "4.2 Term Order Probabilities", "text": "The term order probabilities (TOP) [CN17] is an inexpensive method for combining word embedding in sentence or document embedding, while the probability of any term pair (words, n-grams) t1 and t2 appear in this particular order in the corpus. It consists of two steps: 1. Calculate P (t1, t2, d): the probability of any term pair (words, n-grams) t1 and t2 appearing in this particular order in the corpus. P (t1, t2) is calculated as: P (t1, t2, d) = # (t1, t2, d) # (t1, t2, d) + # (t2, t2, d)."}, {"heading": "4.3 Relevance disambiguation", "text": "After a ranking of articles from the previous phase is created, the relevance disambiguity level is triggered if the R2NC values of the first and second article are scarce or ambiguous and fall under the following ambiguity condition: R value (a1, q) \u2212 R value (a2, q) < R threshold where R value (ai, q) is the R2NC value of article ai for question q, and R threshold is the parameter that represents the specified lower threshold of rank ambiguity by R2NC. Then, a candidate using the TOP list of cosmic similarity is created by selecting the top k list of articles under the condition: T cand (q) = {ai | i \u2264 k, j < i | T value (ai, q) \u2212 T value (aj, q) \u2212 T value (aj, q) \u2212 T value (relevant, q, diltq, < where the TOP is < T)."}, {"heading": "5 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "The legal question answering the data set was derived from published data from COLIEE's Joint Problem 4, consisting of a text file containing a fragment of the Japanese Civil Code translated into English and a series of XML files containing training data. The training set for the two tasks consists of 580 pairs (question, relevant articles).The fragment of the Japanese Civil Code contains 1057 articles with a total of about 1700 sentences and 71500 words. Experiments are then conducted to evaluate the methods of retrievaluation of information.Other data used in the experiments include the training segment of the Corpus \"1 billion word Language Model Benchmark\" [CMS + 13] and the full Japanese Civil Code Code5, which has been used to train the Distributional Semantics Model. The combined size of the corpora after weighing is approximately 1.2 billion words.We focus on detailed experiments for R2NC and R2NC + TOP, but not solely on TOCJP's previous attempts to show that TOC2N-N.4P is less similar."}, {"heading": "5.2 Parameter adjustment", "text": "R2NC relative significance parameters were corrected by Leave-One-Out validation of the training data. The best setting was Iq = 0.98, Iart = 02.2 variants of the TOP models were trained as follows. Data for Training Word Embedding models: Lemmatized or non-Lemmatized texts. Data for Training TOP models: the training questions and articles provided, with or without all of Japanese civil law. The best setting was the use of non-Lemmatized text for training the Word Embedding model and Training TOP models without all of Japanese civil law. R2NC + TOP parameters were selected by Leave-One-Out experiments where the parameters were adjusted over certain ranges (Table 1). The adjustment results in higher values between the mean and the limits of each range, which were then lower at the limits, while we achieved constant performance in the middle, whereby averages were selected."}, {"heading": "5.3 Evaluation Method", "text": "For relevance analysis, a leave-one-out validation was used to evaluate the potential recall of the model for a limited number of articles. Phase one performance was evaluated using precision (P), recall (R) and F-measure (F) as metrics (Eqs. (5), (6) and (7). P = CrRt (5) R = CrRl (6) F = 2 (P \u0445 R) P + R (7), where Cr counts correctly retrieved articles for all queries, Rt counts correctly retrieved articles for all queries, Rl counts the relevant articles for all queries, Cq counts correctly confirmed queries as true or false, and Q counts all queries."}, {"heading": "5.4 Competition Results", "text": "The approach presented here took second place in the LIR competition (phase one) and third place was achieved by a pure R2NC run. The results presented in Table 2 indicate a small improvement in both accuracy and recall, which means that the additional relevance disambiguation phase added one or more relevant articles to the retrieved R2NC list without introducing non-relevant articles. Additional experiments were conducted after the competition, with ground truth data provided by the COLIEE organizers. As shown in Table 3, it was possible to achieve further improvements over the competition outcomes by changing R2NC to a more aggressive setting of Iq = 0.99, Iart = 0.01, with extreme punishment of the item length. This setting resulted in marginal gains in training data but a better overall result in the competition data set."}, {"heading": "5.5 Error Analysis and Discussion", "text": "In fact, most of us are able to play by the rules that they have imposed on themselves and are able to play by the rules that they have imposed on themselves."}, {"heading": "6 Conclusion", "text": "Due to the unique combination of complex syntax, domain dependence and high level of abstraction, gathering information in the legal field is a challenging task. Such a combination represents a valuable terrain for the application of semantically motivated NLP techniques capable of a limited level of abstraction. Distribution semantic representations fit into this category of techniques, but despite promising results for the general IR, there is still a lack of performance in the legal field. Nevertheless, in this work we propose a method for combining a purely lexical approach based on n-gram statistics with distribution-specific sentence representations in the context of Competition on Legal Information Extraction / Entailment (COLIEE).The combination is done using disambiguity rules, which are applied when the lexical approach is deemed insufficient to decide on the set of relevant documents for a given query, bearing in mind that the overall distributive analysis is possible to achieve further gains in each experiment, and that further gains for each experiment are weak due to the combination of complex syntax, domain dependence and high level of abstraction."}, {"heading": "Acknowledgements", "text": "This work was supported by JSPS KAKENHI grant number 15K16048, JSPS KAKENHI grant number JP15K12094 and CREST, JST."}], "references": [{"title": "Law, learning and representation", "author": ["Kevin D. Ashley", "Edwina L. Rissland"], "venue": "Artificial Intelligence,", "citeRegEx": "Ashley and Rissland.,? \\Q2003\\E", "shortCiteRegEx": "Ashley and Rissland.", "year": 2003}, {"title": "Legal question answering using paraphrasing and entailment analysis", "author": ["Mi-Young Kim", "Ying Xu", "Yao Lu", "Randy Goebel"], "venue": "In Tenth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Predicting associated statutes for legal problems", "author": ["Yi-Hung Liu", "Yen-Liang Chen", "WuLiang Ho"], "venue": "Information Processing & Management,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "The current information analysis capabilities of legal professionals are still lagging behind the explosive growth in legal document availability through digital means, driving the need for higher efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done by means of disambiguation rules, applied over the lexical rankings when those deemed unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, a analysis of error and improvement cases is presented for a better understanding of the contributions.", "creator": "LaTeX with hyperref package"}}}