{"id": "1706.01069", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "CRNN: A Joint Neural Network for Redundancy Detection", "abstract": "This paper proposes a novel framework for detecting redundancy in supervised sentence categorisation. Unlike traditional singleton neural network, our model incorporates character-aware convolutional neural network (Char-CNN) with character-aware recurrent neural network (Char-RNN) to form a convolutional recurrent neural network (CRNN). Our model benefits from Char-CNN in that only salient features are selected and fed into the integrated Char-RNN. Char-RNN effectively learns long sequence semantics via sophisticated update mechanism. We compare our framework against the state-of-the-art text classification algorithms on four popular benchmarking corpus. For instance, our model achieves competing precision rate, recall ratio, and F1 score on the Google-news data-set. For twenty-news-groups data stream, our algorithm obtains the optimum on precision rate, recall ratio, and F1 score. For Brown Corpus, our framework obtains the best F1 score and almost equivalent precision rate and recall ratio over the top competitor. For the question classification collection, CRNN produces the optimal recall rate and F1 score and comparable precision rate. We also analyse three different RNN hidden recurrent cells' impact on performance and their runtime efficiency. We observe that MGU achieves the optimal runtime and comparable performance against GRU and LSTM. For TFIDF based algorithms, we experiment with word2vec, GloVe, and sent2vec embeddings and report their performance differences.", "histories": [["v1", "Sun, 4 Jun 2017 13:12:45 GMT  (115kb)", "http://arxiv.org/abs/1706.01069v1", "Conference paper accepted at IEEE SMARTCOMP 2017, Hong Kong"]], "COMMENTS": "Conference paper accepted at IEEE SMARTCOMP 2017, Hong Kong", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinyu fu", "eugene ch'ng", "uwe aickelin", "simon see"], "accepted": false, "id": "1706.01069"}, "pdf": {"name": "1706.01069.pdf", "metadata": {"source": "CRF", "title": "CRNN: A Joint Neural Network for Redundancy Detection", "authors": ["Xinyu Fu", "Eugene Ch\u2019ng", "Uwe Aickelin", "Simon See"], "emails": ["Uwe.Aickelin}@nottingham.edu.cn", "ssee@nvidia.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.01 069v 1 [cs.C L] 4J Insufficient redundancy in supervised sentence categorization. Similar to traditional singleton neural network, our model incorporates character-conscious Convolutionary Neural Network (Char-CNN) with character-conscious recurring neural network (Char-RNN) to form a Convolutionary Recursive Neural Network (CRNN). Our model benefits from Char-CNN because only outstanding features are selected and fed into the integrated Char-RNN. Char-RNNeffectively learns long sequence semantics using a complex dupdate mechanism. We compare our model with state-of-the-art text classification algorithms on four popular benchmarking corpus. For example, our model achieves the best possible precision rate, level of memory, and F1 value on the Google News dataset."}, {"heading": "1. Introduction", "text": "In recent years, the number of those who are able to shoot up has multiplied. In recent years, the number of those who are able to shoot up has skyrocketed. In the second half of the year, the number of those who are able to shoot up has skyrocketed. In the third half of the year, the number of those who are able to shoot up has skyrocketed. In the second half of the year, the number of those who are able to shoot up has skyrocketed. in the third half of the year, in the second half of the year, in the second half of the year, in the second half of the year, in the third half of the year, in the third half of the year, in the third half of the year, in the third half of the year, in the third half of the year, in the third half of the year, in the third half of the year, in the third half, in the second half of the year, in the second half of the year, in the second half of the year, in the second half of the year, in the second half of the year, in the second half of the year, in the second half of the year, in the second half of the year, in the third, in the third half of the year, in the third half of the year, in the third, in the second half of the year, in the third, in the third, in the second half of the year, in the third, in the third half of the year, in the number of the number of those who are able to shoot up."}, {"heading": "2. Related Work", "text": "Until recently, scientists have been aware that Convolutionary Deep Neural Networks (CDNN) can be applied to many classification tasks. Previous applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual in-depth amplification derived from primitive 3D buffer information and experiential replay [14], and ImageNet classification [3]. However, these approaches have a limited application in redundancy detection. The emergence of CDNN has also shed some light on the structure of text mining [15]. Kim [16] developed a two-channel CNN, a small variation of Collobert et al. Model [4]. Kim's model was used specifically for polarity scoring and classification issues. Zhang et al. [7] provided an empirical study of the application of character-level Convolutionary Networks for binary textual classification."}, {"heading": "3. Ingredients of the Model", "text": "Our Char-CNN consists of a convoluting layer, RELU, and a max pooling layer. The classic Char-CNN by Vosoughi et al. [6] has a multi-layer structure that appears inefficient when exercising with benchmarking data. We have adapted Char-RNN by Cho et al. [10] and Hochreiter and Schmidhuber [11]. Our CharRNN model has three different variants, including GRU [10], MGU [20], and LSTM [11]. Our Char-RNN differs from the tradition that the entrance gate matches the intermediate result of Char-CNN and the output gate follows a modulated activation."}, {"heading": "3.1. Character-Aware Convolutional Neural Network", "text": "In our model, C represents the container of character quantization, and d is the character embed size. The model is designed to support the same 70 characters supported by Vosoughi et al that are presented below [6]. Suppose a term t-C consists of a sequence of characters with length k, and then the character-level representation of t is encoded as Ck-Rd-k. Each character is encoded as a binary vector v-0-1-d. Since the character quantization consists of 70 non-spaces, d in this case is equal to 70. Considering the above input sequence, the 1-D layer is the discrete kernel function f-i-1, m-R and the corresponding fill algorithm p, and the output function card ishk (i) = RELU (m-i = 1Ckf (i \u00d7 tk + o)), (1) where o = maximum pool + 1 is represented as a constant, the unit ishk (i) = RELU (m = 1Ckf (i \u00d7 tk + o))."}, {"heading": "3.2. Character-Aware Recurrent Neural Network", "text": "It is not the first time that the USA (and the USA) have found themselves in a position to leave the USA. (3) It is the second time that the USA (and the USA) have left the USA. (3) It is the third time that the USA (and the USA) have left the USA. (3) It is the third time that the USA (and the USA) have left the USA. (4) It is the third time that the USA (and the USA) have left the USA. (3) It is the third time that the USA (and the USA) have left the USA. (4) It is the third time that the USA (and the USA) have left the USA. (4) The USA (and the USA). (4) The USA (and the USA). (4) The USA (and the USA). (4) The USA (and the USA). (4) The USA (and the USA)."}, {"heading": "4. Experimental Evaluation", "text": "In this section, we present our model design, dataset statistics, data preprocessing, and experimental settings."}, {"heading": "4.1. Model Design", "text": "We refer to Table 1 as the global configuration of training parameters, followed by activation of the activation level. The \"VALID\" top-up does not refer to an artificial top-up, unlike the \"SAME\" top-up. The initial learning rate is set at 0.01. In addition, the global training process is optimized by the Adam Optimizer [25]. The learning rate is defined by the Adam Optimizer [25]. \u03b1t = lrt \u2212 1 \u2212 \u03b2t21 \u2212 \u03b2 (t \u2212 1) 1, (13) where lrt \u2212 1 is the learning rate for the previous training step. \u03b21 is the exponential degradation rate for the first moment that is estimated and \u03b22 is the second moment that is estimated. In Figure 2, n stands for the padded input sequence length."}, {"heading": "4.2. Data and Pre-processing", "text": "Since the focus of this work is on the development of CRNs, the corresponding benchmark data sets can be found in natural text streams or raw data. We used four popular classification data streams from the Internet for our study, listed below: \u2022 Google News is a small part of Google's online platform [26], especially the Google News channel. \u2022 Twenty news groups were founded by well-known American newspaper publishers."}, {"heading": "5. Results", "text": "In this work, we have constructed a novel CRNN model that uses prominent feature filtering from Char-CNN, compared to other models. Our architecture also benefits from the latest development of the GRU unit, which reduces the algorithmic runtime without compromising performance. Generally, we showed the results independently for each benchmarking data stream. Assessments on precision, recall and F1 score were demonstrated for each of our datasets. Our experiment was divided into a cross-validated training and test data. Participants in our experiment are listed in Table 4. There were 20 frameworks in total for our experimental evaluation. The token random shows the arbitrary initialization of word vector.As shown in Figure 3a, 3b, and 3c (Google News), our model ranked fifth in the precision rate, with 2.44% less than the best."}, {"heading": "6. Discussion", "text": "This year, we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "7. Conclusion", "text": "In this work, we applied the latest development to the sign-conscious, deep neural classification network, Char-CNN and Char-RNN specifically for effective categorization of redundant snippets. We expanded the classical Char-CNN structure into a singleton to connect it to the Char-RNN framework to form an efficient redundancy recognition architecture. This novel framework benefits from the beneficial, prominent feature filtering from the Char-CNN and the long-term memory cells from the Char-RNN. We further recognized the usefulness of an aggregation layer as the penultimate gateway by gluing together the encoding matrix generated from the Char-CNN and Char-RNN for CRNN. We realized that this increases detection accuracy."}], "references": [{"title": "Detections, bounds, and timelines: Umass and tdt-3", "author": ["J. Allan", "V. Lavrenko", "D. Malin", "R. Swan"], "venue": "Proceedings of topic detection and tracking workshop, 2000, pp. 167\u2013174.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics, vol. 18, no. 4, pp. 467\u2013479, 1992.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u2013 2537, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Y. Zhang", "S. Roller", "B. Wallace"], "venue": "arXiv preprint arXiv:1603.00968, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Tweet2vec: Learning tweet embeddings using character-level cnn-lstm encoder-decoder", "author": ["S. Vosoughi", "P. Vijayaraghavan", "D. Roy"], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2016, pp. 1041\u20131044.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 649\u2013657.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "journal of machine learning research, vol. 3, no. Feb, pp. 1137\u20131155, 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Multi-column deep neural network for traffic sign classification", "author": ["D. CiresAn", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks, vol. 32, pp. 333\u2013338, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": "2012 IEEE international conference on Acoustics, speech and signal processing (ICASSP). IEEE, 2012, pp. 4277\u20134280.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Jaskowski"], "venue": "arXiv preprint arXiv:1605.02097, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Y. Zhang", "B. Wallace"], "venue": "arXiv preprint arXiv:1510.03820, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["W. Yin", "H. Schutze"], "venue": "arXiv preprint arXiv:1603.04513, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Glehre", "K. Cho", "Y. Bengio"], "venue": "International Conference on Machine Learning, 2015, pp. 2067\u20132075.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Minimal gated unit for recurrent neural networks", "author": ["G.-B. Zhou", "J. Wu", "C.-L. Zhang", "Z.-H. Zhou"], "venue": "International Journal of Automation and Computing, vol. 13, no. 3, pp. 226\u2013234, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "AAAI, 2015, pp. 2267\u20132273.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Using temporal idf for efficient novelty detection in text streams", "author": ["M. Karkali", "F. Rousseau", "A. Ntoulas", "M. Vazirgiannis"], "venue": "arXiv preprint arXiv:1401.1456, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning, 1995, pp. 331\u2013339.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "The brown corpus: A standard corpus of present-day edited american english", "author": ["W.N. Francis", "H. Kucera"], "venue": "Providence, RI: Department of Linguistics, Brown University [producer and distributor], 1979.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1979}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "Proceedings of the 19th international conference on Computational linguistics- Volume 1. Association for Computational Linguistics, 2002, pp. 1\u20137.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of machine learning research, vol. 9, no. Aug, pp. 1871\u20131874, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1871}, {"title": "From word embeddings to document distances", "author": ["M.J. Kusner", "Y. Sun", "N.I. Kolkin", "K.Q. Weinberger"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), 2015, pp. 957\u2013966.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Advances in neural information processing systems, 2015, pp. 3294\u20133302.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Research on redundancy detection began by traditional bagof-words (BOW), TFIDF frequency matrix, and n-gram language modelling [1], [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "Research on redundancy detection began by traditional bagof-words (BOW), TFIDF frequency matrix, and n-gram language modelling [1], [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "[3] and Collobert et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], CNN has shown state-of-the-art performance on computer vision and varied natural language processing (NLP) problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Whilst traditional methods are good at encoding term association relationships (stock and market), neural models tend to preserve word similarity relationships (stock and security) [5].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "We propose the utilisation of Char-CNN to sift the most salient features because of the prevalence on deep neural network and its applications to various domains like the challenging online text classification and clustering [6], [7], [8].", "startOffset": 225, "endOffset": 228}, {"referenceID": 6, "context": "We propose the utilisation of Char-CNN to sift the most salient features because of the prevalence on deep neural network and its applications to various domains like the challenging online text classification and clustering [6], [7], [8].", "startOffset": 230, "endOffset": 233}, {"referenceID": 7, "context": "We propose the utilisation of Char-CNN to sift the most salient features because of the prevalence on deep neural network and its applications to various domains like the challenging online text classification and clustering [6], [7], [8].", "startOffset": 235, "endOffset": 238}, {"referenceID": 5, "context": "These issues are likely to happen in a human derived works such as newswire streams and social media data sources [6], [7], [8].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "These issues are likely to happen in a human derived works such as newswire streams and social media data sources [6], [7], [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "These issues are likely to happen in a human derived works such as newswire streams and social media data sources [6], [7], [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "Plain CNN, however, suffers from remembering longterm dependencies over sequences, as opposed to the traditional n-gram language modelling [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "[2], the system memory takes to store trigram phrases is enormous, which demands high spatial and time complexity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "One possible solution to this is RNN [6], [10].", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "One possible solution to this is RNN [6], [10].", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "The embedded RNN hidden state and the non-linear activation function are capable of capturing long textual sequence probability distribution over iteration [6], [10].", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "The embedded RNN hidden state and the non-linear activation function are capable of capturing long textual sequence probability distribution over iteration [6], [10].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "By replacing the simple element-wise sigmoid activation function with a more complex long short-term memory (LSTM), the integrity of expressing lengthy snippets can be further improved, in line with Hochreiter and Schmidhuber [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 9, "context": "[10] proposed a gated recurrent unit (GRU) which can dynamically reserve the state information through the pipeline of input signal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 209, "endOffset": 213}, {"referenceID": 2, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 243, "endOffset": 246}, {"referenceID": 14, "context": "The emergence of CDNN has also shed some lights on the text mining society [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "Kim [16] developed a twochannel CNN, a small variation of Collobert et al.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "model [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "[7] offered an empirical investigation on the application of characterlevel convolutional networks for binary textual classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Further, Yin and Schutze [17] exhibited a multi-channel CNN for sentence classification.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "[5] gave a multiple word embeddings based CNN model for sentence categorisation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Traditionally, pure RNNs were applied to fields like statistical machine translation [10], polyphonous music modelling, speech signal understanding [18], and Python programme evaluation [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "Traditionally, pure RNNs were applied to fields like statistical machine translation [10], polyphonous music modelling, speech signal understanding [18], and Python programme evaluation [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Traditionally, pure RNNs were applied to fields like statistical machine translation [10], polyphonous music modelling, speech signal understanding [18], and Python programme evaluation [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "Currently, RNNs have seen promising results on text classification [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "RNN is usually powered by the recurrent units such as tanh [18].", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "Advanced recurrent units such as GRU cells [10], LSTM [11], and MGU [20] are released recently.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "Advanced recurrent units such as GRU cells [10], LSTM [11], and MGU [20] are released recently.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "Advanced recurrent units such as GRU cells [10], LSTM [11], and MGU [20] are released recently.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "MGU is probably better than GRU in that the gating mechanism in MGU not only halves the number of units when building up the model, but also reduce the size of the model by optimising the number of parameters during network interaction [18].", "startOffset": 236, "endOffset": 240}, {"referenceID": 17, "context": "However, there is no indication which approach has the best performance without further investigation [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "[21] proposed a recurrent convolutional neural network (RCNN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], inferred sentence structure in our model is obtained through the pre-trained word2vec embeddings [22] or GloVe [23] embeddings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[21], inferred sentence structure in our model is obtained through the pre-trained word2vec embeddings [22] or GloVe [23] embeddings.", "startOffset": 103, "endOffset": 107}, {"referenceID": 22, "context": "[21], inferred sentence structure in our model is obtained through the pre-trained word2vec embeddings [22] or GloVe [23] embeddings.", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "[8] described a hybrid network with CNN-LSTM to capture syntactic information buried in the contextual data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "shows that DCNN is less effective when the data volume is less than several million [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "[6] has multilayer structure which seems inefficient when trained with the benchmarking data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] and Hochreiter and Schmidhuber [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10] and Hochreiter and Schmidhuber [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Our CharRNN model has three different variations including GRU [10], MGU [20], and LSTM [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Our CharRNN model has three different variations including GRU [10], MGU [20], and LSTM [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "Our CharRNN model has three different variations including GRU [10], MGU [20], and LSTM [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "The model was developed to support the same 70 characters supported by Vosoughi et al, which are presented below [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 23, "context": "where o = m\u2212k+1 is a free constant, m depicts the output size, and RELU stands for the rectified linear unit [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "We now describe three RNN recurrent units, namely GRU [10], MGU [20], and LSTM [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "We now describe three RNN recurrent units, namely GRU [10], MGU [20], and LSTM [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "We now describe three RNN recurrent units, namely GRU [10], MGU [20], and LSTM [11].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Memory information inside the GRU cell is fully exposable [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "\u2019s MGU cell has around 33% less training hyper-parameters and has equivalent performance with GRU [20].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "01 Training Steps 1000 Optimiser Adam [25]", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "Moreover, the global training process is optimised through the Adam optimiser [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "The learning rate decay is defined by Adam optimiser [25].", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "\u03b21 is the exponential decay rate for the 1 moment estimates and \u03b22 is for the 2 nd moment estimates [25].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Additionally, because drop-out layer does not always appear to be effective in practice, we therefore did not include any drop-out layers in this regard [15].", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "They are listed below: \u2022 Google-news is a small portion of excerpts from the online platform giant Google [26], specifically the Google news channel.", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "\u2022 Twenty-news-groups was originated from the well-known American newspaper publishers formed as twenty-news-groups, probably first appeared at Lang\u2019s work [27].", "startOffset": 155, "endOffset": 159}, {"referenceID": 27, "context": "\u2022 Brown Corpus of Standard American English, often abbreviated as the Brown Corpus [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "The Brown Corpus is created by Francis and Kucera at Brwon University in 1960s [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "\u2022 Question Classification was created by Li and Roth [29].", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "The vocabulary processor for the Word-CNN and Word-RNN, LinearSVM [30], KNN-WMD [31], and plain KNN requires the normalised BOW features when constructing the corpus vectors.", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "The vocabulary processor for the Word-CNN and Word-RNN, LinearSVM [30], KNN-WMD [31], and plain KNN requires the normalised BOW features when constructing the corpus vectors.", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 63, "endOffset": 66}, {"referenceID": 19, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 194, "endOffset": 198}, {"referenceID": 31, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 231, "endOffset": 235}, {"referenceID": 30, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 248, "endOffset": 252}, {"referenceID": 31, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 285, "endOffset": 289}, {"referenceID": 31, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 330, "endOffset": 334}, {"referenceID": 29, "context": "We now analyse the performance difference on Word-CNNs, LinearSVMs [30], KNN-WMDs, and KNN series.", "startOffset": 67, "endOffset": 71}, {"referenceID": 31, "context": "Sent2vec [32] encoding schema tries to interpret the sentential information into a single skip-thought vector rather than word level embeddings.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": ", sent2vec [32]) and word level encodings on non-neural network frameworks.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "This paper proposes a novel framework for detecting redundancy in supervised sentence categorisation. Unlike traditional singleton neural network, our model incorporates character-aware convolutional neural network (Char-CNN) with character-aware recurrent neural network (Char-RNN) to form a convolutional recurrent neural network (CRNN). Our model benefits from Char-CNN in that only salient features are selected and fed into the integrated Char-RNN. Char-RNN effectively learns long sequence semantics via sophisticated update mechanism. We compare our framework against the state-of-the-art text classification algorithms on four popular benchmarking corpus. For instance, our model achieves competing precision rate, recall ratio, and F1 score on the Googlenews data-set. For twenty-news-groups data stream, our algorithm obtains the optimum on precision rate, recall ratio, and F1 score. For Brown Corpus, our framework obtains the best F1 score and almost equivalent precision rate and recall ratio over the top competitor. For the question classification collection, CRNN produces the optimal recall rate and F1 score and comparable precision rate. We also analyse three different RNN hidden recurrent cells\u2019 impact on performance and their runtime efficiency. We observe that MGU achieves the optimal runtime and comparable performance against GRU and LSTM. For TFIDF based algorithms, we experiment with word2vec, GloVe, and sent2vec embeddings and report their performance differences.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}