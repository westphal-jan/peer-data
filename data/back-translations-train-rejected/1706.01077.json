{"id": "1706.01077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "Actor-Critic for Linearly-Solvable Continuous MDP with Partially Known Dynamics", "abstract": "In many robotic applications, some aspects of the system dynamics can be modeled accurately while others are difficult to obtain or model. We present a novel reinforcement learning (RL) method for continuous state and action spaces that learns with partial knowledge of the system and without active exploration. It solves linearly-solvable Markov decision processes (L-MDPs), which are well suited for continuous state and action spaces, based on an actor-critic architecture. Compared to previous RL methods for L-MDPs and path integral methods which are model based, the actor-critic learning does not need a model of the uncontrolled dynamics and, importantly, transition noise levels; however, it requires knowing the control dynamics for the problem. We evaluate our method on two synthetic test problems, and one real-world problem in simulation and using real traffic data. Our experiments demonstrate improved learning and policy performance.", "histories": [["v1", "Sun, 4 Jun 2017 14:02:01 GMT  (2570kb,D)", "http://arxiv.org/abs/1706.01077v1", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tomoki nishi", "prashant doshi", "michael r james", "danil prokhorov"], "accepted": false, "id": "1706.01077"}, "pdf": {"name": "1706.01077.pdf", "metadata": {"source": "CRF", "title": "Actor-Critic for Linearly-Solvable Continuous MDP with Partially Known Dynamics", "authors": ["Tomoki Nishi", "Prashant Doshi"], "emails": ["nishi@mosk.tytlabs.co.jp", "pdoshi@cs.uga.edu", "michael.james@tri.global", "danil.prokhorov@toyota.com"], "sections": [{"heading": "1 Introduction", "text": "In this context, we should also mention the fact that the concepts he developed are a method that makes it possible to solve the continuous equation of people with and without a handicap. In this sense, he has set in motion the linear, solvable processes of decision-making. (...) Such a method enables us to quickly solve the continuous equation of people with and without a handicap. (...) Another possibility is that people with and without a handicap are able to determine themselves. (...) Another possibility is that people with and without a handicap are able to determine themselves how they behave. (...)"}, {"heading": "2 Related Work", "text": "Previous approaches to solving L-MDPs are predominantly model-based [17, 18, 21], which efficiently optimize control policies by solving the linear Bellman in discrete or continuous L-MDPs when the system dynamics are fully known. Our method loosens this requirement by providing examples of passive dynamics while we know control dynamics. We also introduce multi-layered neural networks to approximate the value functions in L-MDPs in addition to the radial basic functions previously used. Uchibe and Doya [19] formulate Z-Learning based on the least square TD learning processes for continuous LMDPs. The method optimizes policy while requiring knowledge of control dynamics and transition noise. Movement noise, which negates robotic platforms, is often unknown, so this method cannot be applied to robotic learning."}, {"heading": "3 Preliminaries", "text": "We focus on a discrete time system with a real value state x-Rn and control input x-Rm, whose stochastic dynamic is defined as follows: xk + 1 = xk + A (xk) \u2206 t + D + D + D (n) D + D (n) D + D (n) D + D (n) D + D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D n (n) D n (n) D) D (n) D (n) D (D) D (n) D) D (D) D (n) D (n) D (n) D (D) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D) D (n D) D (n D) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) D (n) n D (n) D (n) n) D (n) n) D (n D (n) n) n D (n (n) n D (n) n D (n) n D (n) n D (n) n D (n) n D (n) n D (n D (n) n) n D (n D (n) n) n D (n) n D (n D (n) n D (n) n D (n D (n) n D (n) n D (n D) n D (n) n D (n D) n D (n D (n) n D) (n D (n D) n D (n D) n D (n D) n D (n D (n D) n D (n D) n D (n"}, {"heading": "4 Passive Actor-Critic for L-MDP", "text": "We present a novel actor-critic method for continuous L-MDP, which we call passive actor critique (pAC). Whereas the actor-critic method usually works from samples actively collected in the environment [6], pAC finds a convergent policy without exploration. Instead, it uses samples of passive state transitions and a known control dynamics model. pAC follows the usual dual scheme of actor-critic: one step to state assessment (critic) and one step to policy improvement (actor) 1. Critical: Estimate the z-value and average costs of the linearized Bellman equation from samples under passive dynamics; 2. Actor: Improve a control policy by optimizing the Bellman equation taking into account the known control dynamics model and Z-value and the costs of criticism."}, {"heading": "4.1 Estimation by Critic using Linearized Bellman", "text": "The critical step of the pAC estimates Z-value and the average cost by minimizing the smallest square error between the true Z-value and the estimated Z-value, which is defined by Z-Z-value. \u2212 Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "4.2 Actor Improvement using Standard Bellman", "text": "The actor component improves a policy by calculating S (Eq. 5) using the critic's estimated Z-\u03b2 values > q values of the critic because we do not assume knowledge of the sound level. It is estimated by minimizing the error with the least square between the V value and the estimated Q value. \u2212 Note from Eq. 6 that a value for S leads to a policy as B is known. \u2212 So we look for the S that yields the optimal policy by minimizing the error because the Q value corresponds to the V value iff u value. Similarly to the critic, we minimize the error with the least square given above, Q-ik k value with the lowest k value."}, {"heading": "4.3 Algorithm", "text": "We show a pseudo-code of pAC in algorithm 1. Z (x) and Zavg are estimated by the critic from samples, and S is estimated by the critic from samples, estimated Z (x) and Z (avg). For the critic, no feedback from the actor is required (unlike the actor's methods for MDPs), since the Z value is approximated only with samples from passive dynamics. We emphasize that the steps of the actor and critic do not use functions A and II, but actually rely on functions B, from Equation 1. As such, the updates use a sample (xk, xk + 1) of passive dynamics and state costs qk. As a result, pAC achieves semi-model-free learning for L-MDPs."}, {"heading": "5 Numerical Experiments", "text": "We evaluate pAC for L-MDPs in two synthetic areas, Car-on-a-Hill and Pendulum, which Todorov has previously used for L-MDPs [18], and in our motivational domain of autonomous fusion on a congested highway."}, {"heading": "5.1 Problem settings", "text": "Car-on-a-hill Car-on-a-hill has a two-dimensional state space, x = [xp, xv] > where xp and xv denote position and velocity, respectively, and a one-dimensional action space. Table 1 shows the dynamics in detail. The state costs are given by, q (x) = 4.0 (exp (\u2212 0.5 (xp \u2212 1) 2 \u2212 \u2212 \u2212 exp (\u2212 0.5 (xp + 1) 2 \u2212 2). The initial states are randomly given in \u2212 2\u03c0 (xp) \u2264 xp \u2264 2\u03c0 and \u2212 kv \u2264. Pendulum The pendulum problem also has a two-dimensional state space similar to the car-on-a-hill and a one-dimensional action space. Table 1 shows the dynamics in detail. State costs are given by qm (x)."}, {"heading": "5.2 Performance Evaluation", "text": "We compared pAC with two other methods: model-based learning based on quadratic programming (QP) and Z-Learning. QP requires all system dynamics and approaches the Z-value with quadratic programming [17]. Z-Learning assumes that B and \u03c3 are available to approximate the Z-value with the critic. QP and Z-Learning calculate the policy with Eq. 6. We are not aware of a completely model-free method for L-MDPs. Model-free PI control (e.g. [15]) assumes that the action costs are available and it equals the assumption of the known transition level in L-MDPs. Table 2 gives the prior knowledge requirement for components of system dynamics in Eq. 1.Gaussian RBFs are used to additionally use the Z-value function in all methods and NNNNs are used in Z-Learning and pAC."}, {"heading": "5.3 Experiment on real-world traffic", "text": "The NGSIM dataset contains vehicle trajectory data recorded by cameras mounted on a building for 45 minutes around the evening rush hour [11]. Vehicle trajectories were extracted from collected videos using a vehicle tracking method [7]. We extracted three vehicle systems (Fig. 1) representing 637 highway merger events. We compared pAC and Z-Learning based on RBFs and NN based on the extracted data from NGSIM. Z-Learning calculated a policy with transition noise estimated based on unknown true dynamics. We used the same state variables, action variables and reward functions used in the simulated freeway merge domain. We calculated the next states xk + 1 under passive dynamics by subtracting state changes caused by actions: xk + 1 dk > Duk > Dx1, where are the next Duk > Duk > Dx1, and where are the next Duk > Dx1 state."}, {"heading": "6 Concluding Remarks", "text": "The passive actor-critic optimizes a policy without active exploration, instead of using samples of passive dynamics and knowledge of control dynamics. This is an initial formulation of the actor-critic scheme in the context of L-MDPs. We evaluated the method using three areas. Results show that pAC performs comparably or better than benchmark methods despite having less prior knowledge. Therefore, pAC represents an important step towards more efficient continuous RL. Passive dynamics data and precise models of control dynamics are needed for pAC. This may seem to limit its applicability, but, as already mentioned, it is well suited for contemporary robotic applications such as automated driving in real traffic. In this case, passive and control dynamics correspond with models of surrounding and autonomous vehicles, with pAC achieving a better evaluation, which are models that are not usually usable for traffic."}], "references": [{"title": "Fitted q-iteration in continuous action-space mdps. In Advances in neural information processing systems", "author": ["Andr\u00e1s Antos", "Csaba Szepesv\u00e1ri", "R\u00e9mi Munos"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Fitted q-iteration by functional networks for control problems", "author": ["Matteo Gaeta", "Vincenzo Loia", "Sergio Miranda", "Stefania Tomasiello"], "venue": "Applied Mathematical Modelling,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Policy search for path integral control", "author": ["Vicen\u00e7 G\u00f3mez", "Hilbert J Kappen", "Jan Peters", "Gerhard Neumann"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A survey of actorcritic reinforcement learning: Standard and natural policy gradients", "author": ["Ivo Grondman", "Lucian Busoniu", "Gabriel AD Lopes", "Robert Babuska"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Nicolas Heess", "Gregory Wayne", "David Silver", "Tim Lillicrap", "Tom Erez", "Yuval Tassa"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Actor-critic algorithms. In Advances in neural information processing systems", "author": ["Vijay R Konda", "John N Tsitsiklis"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Video-based vehicle trajectory data collection", "author": ["Vijay Gopal Kovvali", "Vassili Alexiadis", "PE Zhang"], "venue": "In Transportation Research Board 86th Annual Meeting,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Batch reinforcement learning", "author": ["Sascha Lange", "Thomas Gabel", "Martin Riedmiller"], "venue": "In Reinforcement learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Safe exploration in markov decision processes", "author": ["Teodor M Moldovan", "Pieter Abbeel"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Comparison of car-following models", "author": ["Johan Janson Olstam", "Andreas Tapani"], "venue": "Swedish National Road and Transport Research Institute, Project VTI meddelande,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Markov Decision Processes", "author": ["Martin L. Puterman"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A Generalized Path Integral Control Approach to Reinforcement Learning", "author": ["E. Theodorou", "Jonas Buchli", "Stefan Schaal"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Linearly-solvable markov decision problems", "author": ["Emanuel Todorov"], "venue": "In Advances in neural information processing systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Efficient computation of optimal actions", "author": ["Emanuel Todorov"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Eigenfunction approximation methods for linearly-solvable optimal control problems", "author": ["Emanuel Todorov"], "venue": "In Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Combining learned controllers to achieve new goals based on linearly solvable mdps", "author": ["Eiji Uchibe", "Kenji Doya"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Aggregation methods for lineary-solvable markov decision process", "author": ["Mingyuan Zhong", "Emanuel Todorov"], "venue": "In Proceedings of the World Congress of the International Federation of Automatic Control,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "In this regard, Todorov [16] introduced the linearly-solvable Markov decision process (L-MDP), a subclass of general MDPs, which allows us to quickly solve the continuous Bellman equation exactly under a class of structured dynamics and rewards.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "Specifically, the Bellman equation in L-MDPs is recast as a linearized differential, and its solution is efficiently obtained as a linear eigenfunction when the whole dynamics model is available [18].", "startOffset": 195, "endOffset": 199}, {"referenceID": 8, "context": "However, at the same time, guaranteeing safe exploration has a high computational cost that is shown to be NP-hard [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 16, "context": "In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements.", "startOffset": 52, "endOffset": 60}, {"referenceID": 17, "context": "In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements.", "startOffset": 52, "endOffset": 60}, {"referenceID": 15, "context": "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 16, "context": "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 18, "context": "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 17, "context": "Uchibe and Doya [19] formulate Z-learning based on least-square TD learning for continuous LMDPs.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "As pAC can learn from data containing samples of passive dynamics, it bears resemblance to batch RL methods [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics.", "startOffset": 63, "endOffset": 69}, {"referenceID": 1, "context": "A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics.", "startOffset": 63, "endOffset": 69}, {"referenceID": 13, "context": "Path integral control also learns a policy based on linearized Bellman equation [15, 3].", "startOffset": 80, "endOffset": 87}, {"referenceID": 2, "context": "Path integral control also learns a policy based on linearized Bellman equation [15, 3].", "startOffset": 80, "endOffset": 87}, {"referenceID": 4, "context": "Finally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14].", "startOffset": 98, "endOffset": 105}, {"referenceID": 12, "context": "Finally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14].", "startOffset": 98, "endOffset": 105}, {"referenceID": 15, "context": "L-MDP [17] is a subclass of MDPs [13] defined by a tuple, \u3008X ,U ,P,R\u3009, where X \u2286 R and U \u2286 R are continuous state and action spaces.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "L-MDP [17] is a subclass of MDPs [13] defined by a tuple, \u3008X ,U ,P,R\u3009, where X \u2286 R and U \u2286 R are continuous state and action spaces.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "[4] notes that the Bellman equation for MDPs can be rewritten using the value function V (x) called V-value, state-action value function Q(x,u) called Q-value, and average value Vavg under an policy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "2 (hereafter referred to as the linearized Bellman equation) [18]: ZavgZk = e \u2212qk\u2206t Ep(xk+1|xk)[Zk+1], (4)", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "While the actor-critic method usually operates using samples collected actively in the environment [6], pAC finds a converged policy without exploration.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "B [ 0, 1 ]> [ 0, 1 ]> [ 0.", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": "B [ 0, 1 ]> [ 0, 1 ]> [ 0.", "startOffset": 12, "endOffset": 20}, {"referenceID": 0, "context": "\u03c3 [ 0, 1 ]> [ 0, 2 ]> [ 0, 2.", "startOffset": 2, "endOffset": 10}, {"referenceID": 1, "context": "\u03c3 [ 0, 1 ]> [ 0, 2 ]> [ 0, 2.", "startOffset": 12, "endOffset": 20}, {"referenceID": 16, "context": "We evaluate pAC for L-MDPs on two synthetic domains, Car-on-a-Hill and Pendulum, also used previously on L-MDPs by Todorov [18]; and on our motivating domain of autonomous merging in a congested freeway.", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "Here, x = [dx12, dv12, dx02, dv02] where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j \u2208 [0, 1, 2].", "startOffset": 138, "endOffset": 147}, {"referenceID": 1, "context": "Here, x = [dx12, dv12, dx02, dv02] where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j \u2208 [0, 1, 2].", "startOffset": 138, "endOffset": 147}, {"referenceID": 10, "context": "The dynamics presume that the leading vehicle is driven with a constant speed v2 = 30[m/sec], and the following vehicle is driven by a known car-following model [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 67, "endOffset": 78}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 67, "endOffset": 78}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 67, "endOffset": 78}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 99, "endOffset": 110}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 99, "endOffset": 110}, {"referenceID": 15, "context": "QP requires all system dynamics and approximates the Z-value with quadratic programming [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "[15]) assume action cost is available and it is equivalent to the assumption of the known transition noise level in L-MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The rectified linear function [10] is used as the hidden layers\u2019 activation function.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "Inputs of the perceptron were normalized to the range [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 16, "context": "QP could not learn any reasonable policy in Merging domains because of an issue referred to in [18]: QP might not converge to a principal Eigen pair, instead of converging to a 2nd or higher-order pair.", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "Vehicle trajectories were extracted using a vehicle tracking method from collected videos [7].", "startOffset": 90, "endOffset": 93}], "year": 2017, "abstractText": "In many robotic applications, some aspects of the system dynamics can be modeled accurately while others are difficult to obtain or model. We present a novel reinforcement learning (RL) method for continuous state and action spaces that learns with partial knowledge of the system and without active exploration. It solves linearly-solvable Markov decision processes (L-MDPs), which are well suited for continuous state and action spaces, based on an actor-critic architecture. Compared to previous RL methods for L-MDPs and path integral methods which are model based, the actor-critic learning does not need a model of the uncontrolled dynamics and, importantly, transition noise levels; however, it requires knowing the control dynamics for the problem. We evaluate our method on two synthetic test problems, and one real-world problem in simulation and using real traffic data. Our experiments demonstrate improved learning and policy performance.", "creator": "LaTeX with hyperref package"}}}