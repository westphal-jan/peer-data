{"id": "1706.01084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "Joint Text Embedding for Personalized Content-based Recommendation", "abstract": "Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned effectively from user interaction data, in many cases, such data is not available, especially for newly emerged items.", "histories": [["v1", "Sun, 4 Jun 2017 14:48:28 GMT  (1494kb,D)", "https://arxiv.org/abs/1706.01084v1", null], ["v2", "Fri, 23 Jun 2017 21:55:56 GMT  (1494kb,D)", "http://arxiv.org/abs/1706.01084v2", "typo fixes"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["ting chen", "liangjie hong", "yue shi", "yizhou sun"], "accepted": false, "id": "1706.01084"}, "pdf": {"name": "1706.01084.pdf", "metadata": {"source": "META", "title": "Joint Text Embedding for Personalized Content-based Recommendation", "authors": ["Ting Chen", "Liangjie Hong", "Yue Shi", "Yizhou Sun"], "emails": ["tingchen@cs.ucla.edu", "lhong@etsy.com", "yueshi@acm.org", "yzsun@cs.ucla.edu"], "sections": [{"heading": null, "text": "In this paper, we address the problem of personalized recommendations for entirely new articles with available text information. We view the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendations. Users and text contents are embedded in latent traits. Text embedding functions can be learned end-to-end by predicting user interactions with items. To reduce the scarcity of interaction data and use large amounts of text data with or without user interactions, we also propose a common text embedding model that includes unattended text embedding with a combination module. Experimental results show that our model can significantly improve the effectiveness of recommendation systems on real data sets."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that it will be able to drown the aforementioned cerebrospinal vessels csrteeSe. It is, he says, \"that we are able to experience the cerebrospinal vessels csrteeSe.\" n"}, {"heading": "2 PRELIMINARIES AND RELATEDWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem De nition", "text": "We use X = (x1, \u00b7 \u00b7 \u00b7, xN) to denote the set of texts, the i-th text is represented by a word sequence, i.e. xi = (w1, \u00b7 \u00b7 \u00b7, wt). A matrixC is used to denote the historical interactions between users and texts, where Ci j denotes the interaction between a user i and a text article j, such as click-or-not, like-or-not. 1Given the text information X and historical interaction data C, our goal is to learn a model that can evaluate completely new texts for an existing user i based on that user's interests and the text content."}, {"heading": "2.2 Personalized Recommendation", "text": "Existing methods of personalized recommendation algorithms can be roughly divided into the following categories: (1) collaborative Ltering methods, (2) content-based methods, and (3) hybrid methods.Matrix factorization (MF) techniques [14, 24] is one of the most effective collaborative Ltering methods (CF).In MF, each user or object is associated with latent factor vectors u or v, and the score between a user and an object is calculated by its dot product, i.e. si = uTi vj. Since each element j is associated with latent factors vj, a new element cannot be properly treated, since the training of vj depends on its interaction with users. Content-based methods [21] usually form a model of user and content based on conceptual weighting schemes such as TF-IDF. And cosmic similarity or logistic regression can be used as an agreement between a user and an object."}, {"heading": "2.3 Text Embedding", "text": "Recent advances in deep learning have shown the importance of learning good representations of text and other data types [2, 4, 12, 16, 18, 19]. Text embedding techniques are aimed at mapping text into vector representations that can be used for future predictive tasks. Such models have been proposed to solve the problem of text categorization / categorization [10, 12, 16]. Our task is similar to a personalized problem of text categorization / classification in the sense that we try to classify / classify an article according to its relevance to a given user. We also use user behavior instead of text labels as a monitored signal. 1We view C as implicit feedback in this work, which means that only positive interactions are provided and non-interactions are implicitly treated as negative feedback."}, {"heading": "3 THE PROPOSED MODEL", "text": "In this section, we first present the text embedding framework, which is fully trained to predict user-item interactions. we propose a common text embedding model by incorporating unattended text embedding with a combination function."}, {"heading": "3.1 Supervised Text Embedding Framework", "text": "Within our framework, the text embedding function is very exible. It can be respected by any programmable function that is an x-dimensional embedding vector vector vector vector vector vector vector."}, {"heading": "3.2 Incorporating Unsupervised Text Embedding", "text": "It is about the question to what extent it is about a way in which people move in different areas of life: about the question of how they should behave in the world, how they should behave, how they should behave, how they should behave, how they should behave, how they should behave, how they should behave, how they should behave, how they should behave, \"how they\" do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, do it, do it, do it, do it, do it, do it, do it, do it."}, {"heading": "4 EXPERIMENTS", "text": "In this section we present our empirical studies on two text recommendation data sets from the real world."}, {"heading": "4.1 Data Collections", "text": "The first CiteULike dataset, containing the behavioral data of CiteULike.org user bookmarking articles, was provided in [30]. It contains 5,551 users, 16,980 items and 204,986 interactions. The second dataset is Yahoo! News Feed2. We randomly assigned 10,000 users (with at least 10 clicks) and their clicked messages to form the dataset containing 58,579 items and 515,503 interactions. Since CiteULike and News datasets have both titles and summaries / summaries, we create two datasets for each dataset: one contains only title information (i.e. short text) and the other contains both title and summary / abstract (i.e. long text)."}, {"heading": "4.2 Comparing Methods and Settings", "text": "We compare the following methods in experiments: \u2022 Cosine Similarity Matching [21], which is based on similarities of TF-IDFs between candidates and users. \u2022 CDL (Collaborative Deep Learning) [31], which trains the auto-encoder simultaneously to encode text content and matrix factorization to encode user behavior. \u2022 Content Pre-Training, which first trains text embedding by Paragraph Vector and then uses it as fixed item features for matrix factorization. \u2022 TER. is our proposed Supervised Framework. Note that two variants of the text embedding function f (x) are compared: MoV and CNN. \u2022 TER +. is the common text embedding model AUV and CNN."}, {"heading": "4.3 Performance Comparison", "text": "Table 3 shows MAP and AUC results from different methods across four sets of data. As the results show, our methods (both TER and TER +) consistently beat other baselines and achieve state-of-the-art results. Other important observations can also be made from the results: 1) Visual learning or embedding methods (our methods, pre-trained method, and CDL) can achieve better results compared to traditional TF-IDF-based methods, 2) joint, monitored, and unattended text embedding can achieve better results than single or unattended text embedding, and 3) the advantage of our model for short texts is more significant than for longer texts. We also observe that in some cases Mov outperforms CNN (e.g. in CiteULike data sets), we suspect that this is due to the fact that words in CiteUlike may be more meaningful than h.e user interests, so that simpler embedding features already show the highest level of secrecess.2"}, {"heading": "4.4 Case Studies", "text": "In order to further understand the proposed model, we are conducting several case studies dealing with the layout or closest neighbors of words and articles in the embedding space. To visualize the text embedding, we first select top conferences in Ve domains (ML, DM, CV, HCI, BIO) and then randomly select articles that will be published in these conferences. We are using TSNE [17] to visualize 2D maps for these articles and color them according to their publication ranges. e results are shown in Figure 4, where we have found that our combined model can best distinguish papers from specific domains. Table 4 shows similar words for given queried words, i.e. \"Neural\" and \"Learn,\" in CiteULike datasets. From the result, we clearly see the distinction between the meanings of words learned from both methods. For example, the Neural word cannot be \"embedded\" in CiteLike \"in most of the articles."}, {"heading": "5 RELATEDWORK", "text": "Our work relates to both personalized recommendations and text embedding and comprehensive.Collaborative embedding techniques [15] have been one of the most effective methods in the recipient system. Methods such as matrix factorization [14, 24] are widely used, and recently some methods based on neural networks [26, 31, 33] have also been studied. Content-based methods are proposed [2, 21], but are not well developed to exploit deep semantics of content information. Hybrid methods can improve the so-called \"cold start\" problem by incorporating page information [5, 22, 27] or content information [8, 30, 31]. In our case, we have historical data on user interaction with items, but at the time of the recommendation we are looking at items that have never been seen before that cannot be dealt with directly by most existing matrix factoring methods."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we deal with the problem of content-based recommendations for entirely new texts. A novel common text-based framework is proposed, in which user embedding and text embedding are learned end-to-end based on user-article interactions. The text embedding function is exible and can be specified by deep neural networks. Both supervised and unsupervised text embedding are merged together by a combination module as part of a unified model. Empirical evaluations based on real data sets show that our model can achieve state-of-the-art results in recommending new texts. In terms of future work, it is interesting to explore other ways of incorporating unattended text embedding."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors would like to thank Qian Zhao, Yue Ning and Qingyun Wu for their helpful discussions. Yizhou Sun is partially supported by NSF CAREER # 1741634."}], "references": [{"title": "Ask the GRU: Multi-task Learning for Deep Text Recommendations", "author": ["Trapit Bansal", "David Belanger", "Andrew McCallum"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identi\u0080cation", "author": ["Ting Chen", "Yizhou Sun"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "On Sampling Strategies for Neural Network-based Collaborative Filtering", "author": ["Ting Chen", "Yizhou Sun", "Yue Shi", "Liangjie Hong"], "venue": "In Proceedings of the 23th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Entity Embedding-based Anomaly Detection for Heterogeneous Categorical Events", "author": ["Ting Chen", "Lu-An Tang", "Yizhou Sun", "Zhengzhang Chen", "Kai Zhang"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence (IJCAI\u201916). Miami", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "SVDFeature: a toolkit for feature-based collaborative \u0080ltering", "author": ["Tianqi Chen", "Weinan Zhang", "Qiuxia Lu", "Kailong Chen", "Zhao Zheng", "Yong Yu"], "venue": "Journal of Machine Learning Research 13,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bo\u008aou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Regularized multi\u2013task learning", "author": ["\u008ceodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Content-based recommendations with poisson factorization", "author": ["Prem K Gopalan", "Laurent Charlin", "David Blei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["\u008corsten Joachims"], "venue": "In European conference on machine learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "E\u0082ective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Bag of Tricks for E\u0081cient Text Classi\u0080cation", "author": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Convolutional neural networks for sentence classi\u0080cation", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P Kingma", "Jimmy Lei Ba"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative \u0080ltering model", "author": ["Yehuda Koren"], "venue": "In Proceedings of the 14th ACMSIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Distributed Representations of Sentences and Documents", "author": ["\u008boc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geo\u0082rey Hinton"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "E\u0081cient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Je\u0082rey Dean"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems", "author": ["T Mikolov", "J Dean"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "\u008cumbs up?: sentiment classi\u0080cation using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Content-based recommendation systems", "author": ["Michael J Pazzani", "Daniel Billsus"], "venue": "In \u008ae adaptive web", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Factorization machines", "author": ["Ste\u0082en Rendle"], "venue": "IEEE International Conference on Data Mining", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the twenty-\u0080\u0087h conference on uncertainty in arti\u0080cial intelligence", "author": ["Ste\u0082en Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt- \u008cieme"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Probabilistic matrix factorization", "author": ["Ruslan Salakhutdinov", "Andriy Mnih"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Restricted Boltzmann machines for collaborative \u0080ltering", "author": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geo\u0082rey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Autorec: Autoencoders meet collaborative \u0080ltering", "author": ["Suvash Sedhain", "Aditya Krishna Menon", "Sco\u008a Sanner", "Lexing Xie"], "venue": "In Proceedings of the 24th International Conference on World Wide Web", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Relational learning via collective matrix factorization", "author": ["Ajit P Singh", "Geo\u0082rey J Gordon"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Dropout: a simple way to prevent neural networks from over\u0080\u008aing", "author": ["Nitish Srivastava", "Geo\u0082rey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Document modeling with gated recurrent neural network for sentiment classi\u0080cation", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Collaborative topic modeling for recommending scienti\u0080c articles", "author": ["Chong Wang", "David M Blei"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Collaborative deep learning for recommender systems", "author": ["Hao Wang", "Naiyan Wang", "Dit-Yan Yeung"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Character-level convolutional networks for text classi\u0080cation", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "A Neural Autoregressive Approach to Collaborative Filtering", "author": ["Yin Zheng", "Bangsheng Tang", "Wenkui Ding", "Hanning Zhou"], "venue": "arXiv preprint arXiv:1605.09477", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "Personalized recommendation has gained a lot of a\u008aention during the past few years [15, 25, 30].", "startOffset": 83, "endOffset": 95}, {"referenceID": 28, "context": "Personalized recommendation has gained a lot of a\u008aention during the past few years [15, 25, 30].", "startOffset": 83, "endOffset": 95}, {"referenceID": 13, "context": "Many models and algorithms have been proposed for personalized recommendation, among which, collaborative \u0080ltering techniques such as matrix factorization [14, 24] are shown to be most e\u0082ective.", "startOffset": 155, "endOffset": 163}, {"referenceID": 22, "context": "Many models and algorithms have been proposed for personalized recommendation, among which, collaborative \u0080ltering techniques such as matrix factorization [14, 24] are shown to be most e\u0082ective.", "startOffset": 155, "endOffset": 163}, {"referenceID": 19, "context": "On one hand, traditional content-based [21] recommendation methods are usually based on simple text processing methods such as cosine similarity or logistic regression where both text and users are represented as bag-of-words.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "\u008ce limitations of such representation include the inability to encode similarity between words, as well as losing word order information [10, 19].", "startOffset": 137, "endOffset": 145}, {"referenceID": 17, "context": "\u008ce limitations of such representation include the inability to encode similarity between words, as well as losing word order information [10, 19].", "startOffset": 137, "endOffset": 145}, {"referenceID": 4, "context": "On the other hand, for collaborative \u0080ltering methods, although some of which has been extended to incorporate auxiliary information, text feature extraction functions are usually simple, and cannot leverage recent proposed representation learning techniques for text [5, 22, 27].", "startOffset": 268, "endOffset": 279}, {"referenceID": 20, "context": "On the other hand, for collaborative \u0080ltering methods, although some of which has been extended to incorporate auxiliary information, text feature extraction functions are usually simple, and cannot leverage recent proposed representation learning techniques for text [5, 22, 27].", "startOffset": 268, "endOffset": 279}, {"referenceID": 25, "context": "On the other hand, for collaborative \u0080ltering methods, although some of which has been extended to incorporate auxiliary information, text feature extraction functions are usually simple, and cannot leverage recent proposed representation learning techniques for text [5, 22, 27].", "startOffset": 268, "endOffset": 279}, {"referenceID": 13, "context": "Matrix factorization (MF) techniques [14, 24] is one of the most e\u0082ective collaborative \u0080ltering (CF) methods.", "startOffset": 37, "endOffset": 45}, {"referenceID": 22, "context": "Matrix factorization (MF) techniques [14, 24] is one of the most e\u0082ective collaborative \u0080ltering (CF) methods.", "startOffset": 37, "endOffset": 45}, {"referenceID": 19, "context": "Content based methods [21] usually build model for user and content based on term weighting schemes like TF-IDF.", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 20, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 25, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 7, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 28, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "\u008cere are some work aiming at leveraging neural networks for be\u008aer text recommendations, such as Collaborative Deep Learning [31], and others [1].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "\u008cere are some work aiming at leveraging neural networks for be\u008aer text recommendations, such as Collaborative Deep Learning [31], and others [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "Compared to their work, 1) we treat the problem as a ranking problem instead of a rating prediction problem, thus pairwise loss functions are adopted; 2) our model provide a more general framework, enabling various text embedding functions, thus subsumes [1] as a special case; 3) our model incorporates unsupervised text embedding from large-scale unlabeled corpora.", "startOffset": 255, "endOffset": 258}, {"referenceID": 1, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 3, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 11, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 14, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 16, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 17, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 9, "context": "Such models have been proposed for addressing text classi\u0080cation/categorization problem [10, 12, 16].", "startOffset": 88, "endOffset": 100}, {"referenceID": 11, "context": "Such models have been proposed for addressing text classi\u0080cation/categorization problem [10, 12, 16].", "startOffset": 88, "endOffset": 100}, {"referenceID": 14, "context": "Such models have been proposed for addressing text classi\u0080cation/categorization problem [10, 12, 16].", "startOffset": 88, "endOffset": 100}, {"referenceID": 16, "context": "To represent a text sequence x of length T , we \u0080rst embed each word in the text with an embedding vector w [18, 19], and then use the average of word embeddings to form the text/article embedding as follows:", "startOffset": 108, "endOffset": 116}, {"referenceID": 17, "context": "To represent a text sequence x of length T , we \u0080rst embed each word in the text with an embedding vector w [18, 19], and then use the average of word embeddings to form the text/article embedding as follows:", "startOffset": 108, "endOffset": 116}, {"referenceID": 9, "context": "As demonstrated in [10], ordering information of words can be helpful.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "Due to the page limit, we refer the reader to [12] for more clear detailed descriptions.", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "So for each user i , a pair of a positive item p and a negative item n are both sampled, and similar to [23], the score di\u0082erence between positive and negative items is maximized, leading to a pairwise ranking loss function as follows:", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "To leverage a large-scale text corpus, we adopt Paragraph Vector [16] in our framework.", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "As introduced in [16], the model is trained by maximum likelihood with negative sampling.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "Although unsupervised text embeddings can provide useful text features [16], they might not be directly relevant to the task.", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "So to control the degree of trust for unsupervised text embeddings, we introduce dropout [28] into unsupervised text vectors, i.", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "org, was provided in [30].", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "\u2022 Cosine similarity matching [21], which is based on similarities of TF-IDFs between candidate and user\u2019s historical items.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "\u2022 Regularized multi-task logistic regression [7], which can be seen as one-layer linear text model.", "startOffset": 45, "endOffset": 48}, {"referenceID": 29, "context": "\u2022 CDL (Collaborative Deep Learning) [31], which simultaneously trains auto-encoder for encoding text content, and matrix factorization for encoding user behavior.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "We use Adam [13] with learning rate of 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "We apply TSNE [17] to visualize 2d map for these articles, and color them according to their domains of publication.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 34, "endOffset": 42}, {"referenceID": 22, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 34, "endOffset": 42}, {"referenceID": 24, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 130, "endOffset": 142}, {"referenceID": 29, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 130, "endOffset": 142}, {"referenceID": 31, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 130, "endOffset": 142}, {"referenceID": 1, "context": "Content based methods are proposed [2, 21], but has not been well developed to exploit deep semantics of content information.", "startOffset": 35, "endOffset": 42}, {"referenceID": 19, "context": "Content based methods are proposed [2, 21], but has not been well developed to exploit deep semantics of content information.", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 20, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 25, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 7, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 28, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "Our model is similar to CDL [31], but with following di\u0082erences: (1) we treat the problem as ranking instead of rating prediction problem, (2) we provide a general framework which allows \u0083exible choice", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 121, "endOffset": 128}, {"referenceID": 18, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 121, "endOffset": 128}, {"referenceID": 5, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 235, "endOffset": 246}, {"referenceID": 11, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 235, "endOffset": 246}, {"referenceID": 30, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 235, "endOffset": 246}, {"referenceID": 27, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 252, "endOffset": 256}, {"referenceID": 10, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 268, "endOffset": 272}, {"referenceID": 14, "context": "\u008cere are also unsupervised text embedding techniques [16, 18, 19], which do not require labels but cannot adapt to the task of interest.", "startOffset": 53, "endOffset": 65}, {"referenceID": 16, "context": "\u008cere are also unsupervised text embedding techniques [16, 18, 19], which do not require labels but cannot adapt to the task of interest.", "startOffset": 53, "endOffset": 65}, {"referenceID": 17, "context": "\u008cere are also unsupervised text embedding techniques [16, 18, 19], which do not require labels but cannot adapt to the task of interest.", "startOffset": 53, "endOffset": 65}, {"referenceID": 2, "context": "We further generalize the proposed model and develop e\u0081cient training techniques in [3].", "startOffset": 84, "endOffset": 87}], "year": 2017, "abstractText": "Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned e\u0082ectively from user interaction data, in many cases, such data is not available, especially for newly emerged items. In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. \u008ce text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with li\u008ale or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can signi\u0080cantly improve the e\u0082ectiveness of recommendation systems on real-world datasets.", "creator": "LaTeX with hyperref package"}}}