{"id": "1706.01284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Learning Neural Programs To Parse Programs", "abstract": "In this work, we study an important problem: learning programs from input-output examples. We propose a novel method to learn a neural program operating a domain-specific non-differentiable machine, and demonstrate that this method can be applied to learn programs that are significantly more complex than the ones synthesized before: programming language parsers from input-output pairs without knowing the underlying grammar. The main challenge is to train the neural program without supervision on execution traces. To tackle it, we propose: (1) LL machines and neural programs operating them to effectively regularize the space of the learned programs; and (2) a two-phase reinforcement learning-based search technique to train the model. Our evaluation demonstrates that our approach can successfully learn to parse programs in both an imperative language and a functional language, and achieve 100% test accuracy, while existing approaches' accuracies are almost 0%. This is the first successful demonstration of applying reinforcement learning to train a neural program operating a non-differentiable machine that can fully generalize to test sets on a non-trivial task.", "histories": [["v1", "Mon, 5 Jun 2017 11:44:35 GMT  (1127kb,D)", "http://arxiv.org/abs/1706.01284v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.PL", "authors": ["xinyun chen", "chang liu", "dawn song"], "accepted": false, "id": "1706.01284"}, "pdf": {"name": "1706.01284.pdf", "metadata": {"source": "CRF", "title": "Learning Neural Programs To Parse Programs", "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "2 The Parsing Problem", "text": "In this section we will formally define the parsing problem and outline our approach. Definition 1 (The parsing problem) Suppose there is a context-free language L and a parsing oracle \u03c0 that can parse any instance in L into an abstract syntax tree. Both L and \u03c0 are unknown. The problem is to learn a parsing program P so that the sets of non-terminals and terminals fall apart. Each terminal must come from an input token, but the non-terminals have no such equivalent. To simplify the problem, we assume that the input is already tokenized, and the leaf nodes are called terminals terminals."}, {"heading": "3 LL Machines", "text": "In fact, most of them will be able to play by the rules that they play by the rules, and they will have to play by the rules that they play by the rules."}, {"heading": "4 A Neural Parsing Program", "text": "A parsing program runs an LL machine through a sequence of LL machine statements to parse an input into a parse tree. Specifically, a parsing program that runs an LL machine decides on the next statement to execute after each time step. A key feature of the combination of the LL machine and the parsing program is that this decision can only be made on the basis of three components: (1) the function ID of the top frame; (2) all root nodes of the trees (but not the entire trees) in the list of the top frame; (3) the next input token. As we will explain in Appendix A, we can safely assume that the list can have most K elements in each stack frame. Therefore, the parser only needs to learn a (small) limited number of scenarios to learn all valid inputs.To learn the parsing program, we represent it as a neural one executed by the L machine."}, {"heading": "5 Learning a Neural Parsing Program", "text": "In this section, we will consider two types of supervision for training the model: First, we will consider that for each input-output pair, the execution trace is partially given. Specifically, for each step in the execution trace, the input-output trace is provided, but the arguments of the input-output pair are not. In this case, we are only talking about an input-output trace and refer to an execution trace with input-output pairs. Both cases are not trivial. We will show that the weakly monitored learning algorithm can be used as a subroutine to solve the training problem only with input-output pairs. Our approaches to solving both problems use reinforcement learning-based algorithms, and we will highlight our techniques in the rest of this section."}, {"heading": "5.1 Weakly supervised learning", "text": "The biggest challenge, therefore, is that the training process is very sensitive to the design of the reward functions. Below, we present our conception of the reward functions to address these challenges, which allow the learned model to achieve 100% training accuracy. Further details are provided in Appendix C.Learning to predict the REDUCE arguments n and (c1, cm). For the REDUCE instruction, our intuition is such that if a wrong set of arguments is used, the generated sub-tree will look very different from the generated sub-tree. Therefore, we design the reward function based on the difference between the predicted sub-tree and the ground truth.First, we define the difference between two trees and the generated sub-tree as the generated sub-tree."}, {"heading": "5.2 Training with input-output pairs only", "text": "If the training contains only input-output pairs without input-output pairs, we try to find a satisfaction that can locate all valid input-output pairs. (The main problem with this is that a learned model may correctly analyze some input-output pairs but fail at others.) We observe that for each input-output pair there may be several valid execution examples (see Appendix C for an example), where a model for an input-output pair may not be able to mimic a specific execution track for another pair at the same time. Thus, our goal is to find consistent execution tracks for all input-output pairs in the training. To achieve this goal, we learn the neural parsing program in two phases. First, for each input-output pair we find a set of valid input-output pairs, we find a set of candidate instruction types with a preference in the direction."}, {"heading": "6 Evaluation", "text": "In fact, it is that we are able to assert ourselves, that we are able to adapt to the needs of the people, and that we are able to adapt to the needs of the people, \"he said in an interview with the\" New York Times. \""}, {"heading": "Acknowledgement", "text": "This material is based in part on work supported by the National Science Foundation under grant number TWC-1409915 and Berkeley DeepDrive. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of the National Science Foundation. We thank Richard Shin, Dengyong Zhou, Yuandong Tian, He He He and Yu Zhang for their helpful discussions."}, {"heading": "A LL machines", "text": "First, we show how our LL machines regulate the space of learned programs. To achieve this, we set several constraints on the command types that can be applied at each time step. We designate the current stack top as (fid, l), the length of l as L, and the first character of the current input as tok (tok = EOF if the current input is empty). Meanwhile, we assume that the list of each stack box has at most K elements, and we explain later why this assumption applies. The constraints on when each of the five statements is allowed as tok are as follows: 1. SHIFT: It is allowed if tok 6 = EOF and L < K. 2. CALL: It is allowed if tok 6 = EOF, 0 < L < K, and the command type is not CALL. For its argument fid \u2032, < F, where F > 0 is a hypermar."}, {"heading": "B Model architecture", "text": "We first present our model architecture according to Section 4. The different parts of the network are shown in Figure 5, 6, 7, and 8. Different sets of parameters are shown in different colors, all of which share only the reference table A (shown in green), and then we explain how the model selects the statement to be executed at each step. As for predicting command types, p (inst | fid, l, tok) is the predicted probability distribution across all different command types by the analysis program, which is calculated in the manner described in Section 4. Based on the current state of the LL machine, the LL machine provides a set of valid command types. If it is then included in the group of valid command types for each command type, its probability for the sample is p (inst | fid, l, tok), otherwise its probability is set to 0. Unless otherwise, the model selects the predicted command type with the highest probability for all commands at each step."}, {"heading": "C Training details", "text": "In the following, we will describe the three components of the learning process. (i) We assume that the number of all parameters consists of three components. (i) We apply Adam. (i) We apply the method. (i) We apply the method. (i) We use the method. (i) We apply it. (i) We use the method. (i) We describe the three components. (i) We describe the three components. (i) We describe the three components. (i) We assume that the method consists of three components. (i) We have the model for predicting. (i)"}, {"heading": "D Hyperparameters of Our Proposed Method", "text": "For LL machines, F = 10. Over the capacity of each stacking frame K, K = 3 for DURING and K = 4 for LAMBDA language. In the architecture of the neural parsing program, each LSTM has a layer with its hidden state size D = 50, which is identical to the embedding size. As far as training is concerned, the learning rate \u03b7 = 0.01 without decay. No dropout is used. Gradient weights for the three components \u0445 1, \u0445 2 and \u0445 3 are \u03b31 = 10.0, \u03b32 = 1.0 and \u03b33 = 0.01, respectively. Gradations with L2 standard greater than 5.0 are scaled down to have the standard of 5.0. The model is trained using the Adam Optimizer. All weights are initialized in [\u2212 0.1, 0.1] and 0.1] at random."}, {"heading": "E Hyperparameters of Baseline Models", "text": "For the basic models in our evaluation, i.e. seq2seq and seq2tree, we implement them ourselves. We select their hyperparameters based on [29] and [11] respectively, and further fine-tune our data sets in order to achieve better experimental outcomes. Specifically, in the seq2seq model [29], we apply the attention mechanism described in [29]. The learning rate is 0.01. The drop-out rate is 0.5. Gradations with L2 standard greater than 5.0 have the standard 5.0. The model is trained with the help of the Adam Optimizer. All weights are initialized randomly in [\u2212 0.1, 0.1]."}, {"heading": "F WHILE language", "text": "< < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < > > > > > > > > > > > > > > > < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < > > > > > > > > > > > > > > > > > > > < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "G LAMBDA language", "text": "Below are the grammatical specifications of the LAMBDA language. < Var >: = a | b |... | z < App >: = < Var > < App > < Var > < Bind >: = lam a |... | lam z < Lam >: = < Bind >. < Var > | < Bind >. < App > | < Bind >. < Lam > | < Lam > | < Let > < LetExpr >: = < Var > = < Var > | < App > = < Var > = < App > = < Lam > | < Var > = < Explt; Lam > = < Let > < Let > < Let > < Let > < LetExpr > < Var > < Expar > < Expar < Var < Var < Var >; Var."}, {"heading": "H Python implementation of WHILE language parser", "text": "1d e f n e x t, 0 9e l i o p [0] [0 o l i p] n (0 o p) n (0 o p) n (0 o p) n (0 o p) e l s (0 o p) = 1: 7i f i o p) = = \"w i l e: f l e: f l e (t o p) e: ltr e: ltf l e (t o p)."}, {"heading": "I Python implementation of LAMBDA language parser", "text": "1d e f n e x t I n s = u = u = l = n (s e l f = i l f): 2f i d, t o p = s f. f i d [\u2212 1], s e l f f f f f f f f f. (e e e e.) e f f. (e.) e f. (e.) e f. (e.) n (s) e l l s s (s) e l s s s s s s (s) e l s s (s) e. (s) e. (s) e. (s) e. (s) e."}, {"heading": "J Miscellaneous", "text": "Here is the three-line Python implementation of Quicksort: def qsort (a): if len (a) < = 1: returns a qsort ([x for x in if x < a [0]]) + [x for x in array if x = = a [0]] + qsort ([x for x in if x > a [0]])."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this work, we study an important problem: learning programs from input-output<lb>examples. We propose a novel method to learn a neural program operating a<lb>domain-specific non-differentiable machine, and demonstrate that this method<lb>can be applied to learn programs that are significantly more complex than the<lb>ones synthesized before: programming language parsers from input-output pairs<lb>without knowing the underlying grammar. The main challenge is to train the neural<lb>program without supervision on execution traces. To tackle it, we propose: (1)<lb>LL machines and neural programs operating them to effectively regularize the<lb>space of the learned programs; and (2) a two-phase reinforcement learning-based<lb>search technique to train the model. Our evaluation demonstrates that our approach<lb>can successfully learn to parse programs in both an imperative language and a<lb>functional language, and achieve 100% test accuracy, while existing approaches\u2019<lb>accuracies are almost 0%. This is the first successful demonstration of applying<lb>reinforcement learning to train a neural program operating a non-differentiable<lb>machine that can fully generalize to test sets on a non-trivial task.", "creator": "LaTeX with hyperref package"}}}