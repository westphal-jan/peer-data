{"id": "1706.01303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "The Singularity May Be Near", "abstract": "Toby Walsh in 'The Singularity May Never Be Near' gives six arguments to support his point of view that technological singularity may happen but that it is unlikely. In this paper, we provide analysis of each one of his arguments and arrive at similar conclusions, but with more weight given to the 'likely to happen' probability.", "histories": [["v1", "Wed, 31 May 2017 19:42:06 GMT  (234kb)", "http://arxiv.org/abs/1706.01303v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY", "authors": ["roman v yampolskiy"], "accepted": false, "id": "1706.01303"}, "pdf": {"name": "1706.01303.pdf", "metadata": {"source": "CRF", "title": "The Singularity May Be Near", "authors": ["Roman V. Yampolskiy"], "emails": ["roman.yampolskiy@louisville.edu"], "sections": [{"heading": null, "text": "Toby Walsh in \"The Singularity May Never Be Near\" makes six arguments that support his view that technological singularity can happen but is unlikely. In this essay, we analyze each of his arguments and come to similar conclusions, placing more emphasis on the probability of the \"probable event.\" Keywords: autogenic intelligence, bootstrap fallacy; recursive self-improvement, self-modifying software, singularity;"}, {"heading": "1. Introduction", "text": "In February 2016, Toby Walsh presented his paper \"The Singularity May Never Be Near\" at AAAI16 [23], which was archived on February 20, 2016 (http: / / arxiv.org / abs / 1602.06462), in which Walsh analyzes the concept of technological singularity. He does not argue that AI will not be able to achieve superhuman intelligence; rather, he suggests that it should not lead to the exponential growth out of control. Walsh defends his view with six different arguments. Almost exactly one year earlier, on February 23, 2015, Roman Yampolskiy archived his paper \"From Seed AI to Technological Singularity via Recurently Self-Improving Software\" [28] (http: / / arxiv.org / abs / 1502.06512), which was subsequently published as two peer-reviewed papers at AGI15."}, {"heading": "2. Contrasting Yampolskiy\u2019s and Walsh\u2019s Arguments", "text": "To facilitate the comparison between arguments from On the Limits of Recursely Self-Improving Artificially Intelligent Systems [28], we use Walsh's argumentation, even if our analysis is not based on the same example (e.g. No dog)."}, {"heading": "Fast Thinking Dog", "text": "Walsh argues,... \"speed alone does not lead to increased intelligence,\" and Yampolskiy says, \"In practice, the performance of almost any system can be trivially improved by providing additional computing resources such as more memory, higher sensor resolution, faster processor, or faster definition, which would require the system to develop a faster memory type and not just purchase more memory units of the type it already has access to. In general, hardware improvements are likely to speed up the system, while software improvements (novel algorithms) are needed to achieve meta improvements.\" The original paper suggests that performance in this context is identical to intelligence, and since most of our intelligence testing tools (IQ tests) are time-based, higher speed would actually lead to a higher intelligence quotient, at least in terms of how we currently access intelligence."}, {"heading": "Anthropocentric", "text": "Walsh argues:... \"that human intelligence itself is nothing special,\" and Yampolskiy says: \"We still do not know the minimum intelligence required to begin the RSI [Recursive Self Improvement] process, but we can [argue] that it would be equivalent to the human intelligence that we associate with universal or general intelligence [13], although a system at the subhuman level capable of self-improvement cannot be excluded [6]. It can be argued that even the ability at the human level is not sufficient because we already have programmers (humans or their intellectual equivalence, formalized as functions [18] or human oracles [24, 25]) who have access to their own source code (DNA) but do not understand how DNA (nature) works to create their intelligence."}, {"heading": "Meta-intelligence", "text": "Walsh argues:... \"the strongest arguments against the idea of a technological singularity are, in my view, that it confuses intelligence with the ability to improve intelligence to complete a task,\" quoting Chalmers [6] as an example: \"If we create artificial intelligence through machine learning, it is likely that soon after that we will be able to improve the learning algorithm and expand the learning process, leading to AI +.\" Yampolskiy says: \"Chalmers [6] uses logic and mathematical induction to show that if an AI0 system is able to produce only slightly more capable AI1 system generalizations of this process, it will lead to superintelligent performance in AIn after n generations."}, {"heading": "Diminishing returns", "text": "Walsh argues: \"In the beginning, there are often many low-hanging fruits, but after that, we encounter great difficulties in improving.\" Yampolskiy says:... \"An AI system can improve itself infinitely, but the extent to which its overall intelligence changes may be limited.\" Limitations in detail. \"First, each implemented software system sets in quickly, and after an initial significant improvement period marked by the discovery of\" low-hanging fruits, \"future improvements are likely to be rarer and less significant, creating a bell curve of valuable change.\" Limitations in detail \"First, any implemented software system relies on hardware for memory, communication, and information processing, even if we assume that it does not require a von Neumann (quantum) architecture to run such software. This creates strict theoretical limits for computation, which, despite hardware advances predicted by Moore's law, cannot be overcome by an optimal hardware parama."}, {"heading": "Computational complexity", "text": "Walsh argues:... \"no growth in performance will make undecideable problems decideable,\" and Yampolskiy says: \"Other problems are notoriously insoluble, regardless of their level of intelligence [22]. Assuming that the separation of complexity classes (such as P versus NP) applies [27], it becomes obvious that certain problem classes always remain only approximately solvable, and improvements in solutions are brought about by additional hardware resources, not by higher intelligence.\""}, {"heading": "3. Response to Walsh\u2019s Arguments", "text": "In this section, we provide a novel analysis of all six arguments put forward by Walsh and review and analyze the arguments put forward by Yampolskiy using the maps provided in the previous section."}, {"heading": "Fast Thinking Dog", "text": "The argument makes intuitive sense, since no one has ever been able to train a dog to play chess. However, intuition is not the equivalent of a scientific experiment. Animals have been successfully trained to understand and even use human (sign) language and to perform basic mathematics. People with mental and learning disabilities, long considered a \"lost cause,\" have been successfully trained to perform very complex behaviors using alternative teaching methods and longer training spans. It is quite possible that if one had thousands of years to train a dog, he would learn to play a decent game of chess, after all, he has a neural network very similar to that of humans, and a deep learning artificial intelligence. It can be argued that there is considerable evidence that language and some other skills are functions of specific brain structures that a dog largely lacks."}, {"heading": "Meta-intelligence", "text": "If the system is superior to human performance in all areas, as the definition of super-intelligence requires, it would also be superior in engineering / computer science / AI research. Potentially, it would be able to improve its successor's intelligence to all theoretical / physical limits that could constitute an upper limit of optimization capability. In other words, if it is possible to improve intelligence, a super-intelligent system will do so, but as such, speculation remains, this is probably the strongest of all the objections to the explosion of intelligence."}, {"heading": "Diminishing returns", "text": "It is a mathematical fact that many functions continue to diverge while delivering decreasing yields. Thus, harmonic series are: 1 + 1 / 2 + 1 / 3 + 1 / 4 + 1 / 5 +... = \u221e, which is a highly counterintuitive result, but a proven mathematical fact. Moreover, since the system itself would improve, it is possible that the discoveries it will make about future improvements will also improve in terms of their impact on the overall intelligence of the system. So, while it is possible that decreasing yields will occur, it is equally possible that yields will not decrease."}, {"heading": "Limits of intelligence", "text": "It is also possible that physical constants are not fixed permanently, but change dynamically, as has been proven for some of these physical \"constants.\" It is also possible that the rate of intelligence improvement is lower than the rate at which some of these constants will change. To cite an example from another realm, our universe is expanding faster than the speed of light in terms of the distance between some selected regions, so even on a journey at maximum theoretical speed (light) we will never reach a limit. So, again, this is another open question, and in the process of self-improvement we may or may not reach a limit."}, {"heading": "Computational complexity", "text": "While it is certainly true that undecidable problems remain undecidable, it is not a limitation of the intelligence explosion, as there is no requirement to qualify as super-intelligent, and there are many solvable problems at all levels of difficulty. Walsh rightly points out that most of the limitations associated with computational complexity are only problems with our current models."}, {"heading": "4. Conclusions", "text": "The careful side-by-side analysis of essays by Walsh and Yampolskiy shows an almost identical set of arguments against the possibility of technological singularity. This successful replication in analysis is an encouraging fact in science and lends added weight to common conclusions, but in this essay we provide novel analysis of Walchers / Yampolskiy's arguments, showing that they may not be as strong as it first appears. Future productive analytical directions could focus on a number of inherent advantages that could allow the AI to recursively improve itself [21] and possibly succeed in this challenging area: ability to work continuously (no pauses, sleep, vocations, etc.), omniscience (full and cross-disciplinary knowledge), higher speed and precision (brain versus processor, human memory versus computer memory), communication speed (chemistry versus electricity), duplicability (intelligent code can be modified for singularity), (if singularity can be rapidly modified)."}, {"heading": "Acknowledgements", "text": "The author would like to thank Toby Walsh for encouraging and supporting the work on this paper, as well as the reviewers who provided feedback on an early draft, thereby substantially strengthening the arguments put forward in the paper."}], "references": [{"title": "Guest column: NP-complete problems and physical reality", "author": ["S. Aaronson"], "venue": "ACM Sigact News, 36 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Information in the holographic universe", "author": ["J.D. Bekenstein"], "venue": "Scientific American, 289 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Superintelligence: Paths", "author": ["N. Bostrom"], "venue": "dangers, strategies, Oxford University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantum noise and information", "author": ["H.J. Bremermann"], "venue": "Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1967}, {"title": "On the Provability", "author": ["S. Bringsjord", "K. Arkoudas"], "venue": "Veracity, and Al-Relevance of the Church\u2014 Turing Thesis, Church's Thesis After 70 Years, 1 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "The Singularity: A Philosophical Analysis", "author": ["D. Chalmers"], "venue": "Journal of Consciousness Studies, 17 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Singularity hypotheses: A scientific and philosophical assessment", "author": ["A.H. Eden", "J.H. Moor", "J.H. Soraker", "E. Steinhart"], "venue": "Springer Science & Business Media", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Brain of a white-collar worker", "author": ["L. Feuillet", "H. Dufour", "J. Pelletier"], "venue": "Lancet (London, England), 370 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Brain plasticity and stroke rehabilitation The Willis lecture", "author": ["B.B. Johansson"], "venue": "Stroke, 31 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Universal limits on computation", "author": ["L.M. Krauss", "G.D. Starkman"], "venue": "arXiv preprint astro-ph/0404510 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "The age of intelligent machines", "author": ["R. Kurzweil", "M.L. Schneider", "M.L. Schneider"], "venue": "MIT press Cambridge", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1990}, {"title": "Ultimate Physical Limits to Computation", "author": ["S. Lloyd"], "venue": "Nature, 406 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Why an intelligence explosion is probable", "author": ["R. Loosemore", "B. Goertzel"], "venue": "Singularity Hypotheses, Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Is there a model for RSI", "author": ["M. Mahoney"], "venue": "SL4, Available at: http://www.sl4.org/archive/0806/19028.html, June 20", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Intelligence explosion: Evidence and import", "author": ["L. Muehlhauser", "A. Salamon"], "venue": "Singularity Hypotheses, Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "The physics of information processing superobjects: daily life among the Jupiter brains", "author": ["A. Sandberg"], "venue": "Journal of Evolution and Technology, 5 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Checkers is Solved", "author": ["J. Schaeffer", "N. Burch", "Y. Bjornsson", "A. Kishimoto", "M. Muller", "R. Lake", "P. Lu", "S. Sutphen"], "venue": "Science, 317(5844) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards a theory of AI completeness", "author": ["D. Shahaf", "E. Amir"], "venue": "8th International Symposium on Logical Formalizations of Commonsense Reasoning ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "A Mathematical Theory of Communication", "author": ["C.E. Shannon"], "venue": "Bell Systems Technical Journal, 27(3) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1948}, {"title": "An introduction to G\u00f6del's theorems", "author": ["P. Smith"], "venue": "Cambridge University Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Advantages of artificial intelligences", "author": ["K. Sotala"], "venue": "uploads, and digital minds, International Journal of Machine Consciousness, 4 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "On computable numbers", "author": ["A. Turing"], "venue": "with an application to the Entscheidungsproblem, Proceedings of the London Mathematical Society, 2(42) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1936}, {"title": "The Singularity May Never be Near", "author": ["T. Walsh"], "venue": "2nd International Workshop on AI, Ethics and Society (AIEthicsSociety2016). 30th AAAI Conference on Artificial Intelligence ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Turing Test as a Defining Feature of AI-Completeness", "author": ["R. Yampolskiy"], "venue": "X.-S. Yang, ed., Artificial Intelligence, Evolutionary Computing and Metaheuristics, Springer Berlin Heidelberg", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "AI-Complete", "author": ["R.V. Yampolskiy"], "venue": "AI-Hard, or AI-Easy\u2013Classification of Problems in AI, The 23rd Midwest Artificial Intelligence and Cognitive Science Conference, Cincinnati, OH, USA ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of Types of Self-Improving Software", "author": ["R.V. Yampolskiy"], "venue": "Artificial General Intelligence: 8th International Conference, AGI 2015, AGI 2015, Berlin, Germany, July 22-25, 2015, Proceedings, 9205 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Construction of an NP Problem with an Exponential Lower Bound", "author": ["R.V. Yampolskiy"], "venue": "Arxiv preprint arXiv:1111.0305 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "From Seed AI to Technological Singularity via Recursively Self-Improving Software", "author": ["R.V. Yampolskiy"], "venue": "arXiv preprint arXiv:1502.06512 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Limits of Recursively Self-Improving AGI", "author": ["R.V. Yampolskiy"], "venue": "Artificial General Intelligence: 8th International Conference, AGI 2015, AGI 2015, Berlin, Germany, July 22-25, 2015, Proceedings, 9205 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Levels of organization in general intelligence", "author": ["E. Yudkowsky"], "venue": "Artificial general intelligence, Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 22, "context": "In February of 2016 Toby Walsh presented his paper \u201cThe Singularity May Never Be Near\u201d at AAAI16 [23], which was archived on February 20, 2016 (http://arxiv.", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "Almost exactly a year before, on February 23, 2015, Roman Yampolskiy archived his paper \u201cFrom Seed AI to Technological Singularity via Recursively Self-Improving Software\u201d[28] (http://arxiv.", "startOffset": 171, "endOffset": 175}, {"referenceID": 25, "context": "06512) which was subsequently published as two peer-reviewed papers at AGI15 [26, 29].", "startOffset": 77, "endOffset": 85}, {"referenceID": 28, "context": "06512) which was subsequently published as two peer-reviewed papers at AGI15 [26, 29].", "startOffset": 77, "endOffset": 85}, {"referenceID": 27, "context": "To make it easier to contrast arguments derived from On the Limits of Recursively Self-Improving Artificially Intelligent Systems [28] we use Walsh\u2019s naming of arguments even if our analysis doesn\u2019t rely on the same example (ex.", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "Improvement] process, but we can [argue] that it would be on par with human intelligence which we associate with universal or general intelligence [13], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [6].", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "Improvement] process, but we can [argue] that it would be on par with human intelligence which we associate with universal or general intelligence [13], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [6].", "startOffset": 244, "endOffset": 247}, {"referenceID": 17, "context": "One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [18] or Human Oracles [24, 25]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [18] or Human Oracles [24, 25]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 185, "endOffset": 193}, {"referenceID": 24, "context": "One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [18] or Human Oracles [24, 25]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 185, "endOffset": 193}, {"referenceID": 5, "context": "strongest arguments against the idea of a technological singularity in my view is that it confuses intelligence to do a task with the capability to improve your intelligence to do a task\u201d and cites a quote from Chalmers [6] as an example - \u201cIf we produce an AI by machine learning, it is likely that soon after we will be able to improve the learning algorithm and extend the learning process, leading to AI+\u201d.", "startOffset": 220, "endOffset": 223}, {"referenceID": 5, "context": "Yampolskiy says: \u201cChalmers [6] uses logic and mathematical induction to show that if an AI0 system is capable of producing only slightly more capable AI1 system generalization of that process leads to superintelligent performance in AIn after n generations.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 66, "endOffset": 69}, {"referenceID": 18, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "For many problems such as playing checkers [17] it is possible to completely solve", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "the problem (provide an optimal solution after considering all possible options) after which no additional performance improvement would be possible [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 21, "context": "no amount of growth in performance will make undecidable problems decidable\u201d and Yampolskiy says, \u201cOther problems are known to be unsolvable regardless of level of intelligence applied to them [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 26, "context": "Assuming separation of complexity classes (such as P vs NP) holds [27], it becomes obvious that certain classes of problems will always remain only approximately solvable and any improvements in solutions will come from additional hardware resources not higher intelligence.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "However some recent research has documented that people missing most of their brain could have near normal cognitive capacity [8] and even significant damage to parts of the brain could be overcome due to neuroplasticity [9] suggesting that brain structures are much more general.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "However some recent research has documented that people missing most of their brain could have near normal cognitive capacity [8] and even significant damage to parts of the brain could be overcome due to neuroplasticity [9] suggesting that brain structures are much more general.", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "Anthropocentric The reason some experts believe ([3] - page 339; [11] - chapter 3) that human level of intelligence is special is not because of anthropocentric bias, but because of the Church-Turing Thesis (CTT).", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "Anthropocentric The reason some experts believe ([3] - page 339; [11] - chapter 3) that human level of intelligence is special is not because of anthropocentric bias, but because of the Church-Turing Thesis (CTT).", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "However, it is important to note that the debate regarding provability of the CTT remains open [20, 5].", "startOffset": 95, "endOffset": 102}, {"referenceID": 4, "context": "However, it is important to note that the debate regarding provability of the CTT remains open [20, 5].", "startOffset": 95, "endOffset": 102}, {"referenceID": 20, "context": "Future productive directions of analysis may concentrate on a number of inherent advantages, which may permit AI to recursively self-improve [21] and possibly succeed in this challenging domain: ability to work uninterrupted (no breaks, sleep, vocation, etc.", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "speed (chemical vs electrical), duplicability (intelligent software can be copied), editability (source code unlike DNA can be quickly modified), near-optimal rationality (if not relying on heuristics) [15], advanced communication (ability to share cognitive representations complex concepts), new cognitive modalities (sensors for source code), ability to analyze low level hardware, ex.", "startOffset": 202, "endOffset": 206}, {"referenceID": 29, "context": ") [30].", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": "Interested readers are advised to read the full paper by Yampolskiy [28] as well as a number of excellent relevant chapters in Singularity Hypothesis [7] which address many arguments not considered in this paper.", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "Interested readers are advised to read the full paper by Yampolskiy [28] as well as a number of excellent relevant chapters in Singularity Hypothesis [7] which address many arguments not considered in this paper.", "startOffset": 150, "endOffset": 153}], "year": 2016, "abstractText": "Toby Walsh in \u201cThe Singularity May Never Be Near\u201d gives six arguments to support his point of view that technological singularity may happen but that it is unlikely. In this paper, we provide analysis of each one of his arguments and arrive at similar conclusions, but with more weight given to the \u201clikely to happen\u201d probability.", "creator": "Microsoft\u00ae Word 2013"}}}