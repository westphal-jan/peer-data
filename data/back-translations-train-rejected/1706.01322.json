{"id": "1706.01322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Deep learning evaluation using deep linguistic processing", "abstract": "We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing 'deep' linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail.", "histories": [["v1", "Mon, 5 Jun 2017 13:53:56 GMT  (71kb,D)", "http://arxiv.org/abs/1706.01322v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["alexander kuhnle", "ann copestake"], "accepted": false, "id": "1706.01322"}, "pdf": {"name": "1706.01322.pdf", "metadata": {"source": "CRF", "title": "Deep learning evaluation using deep linguistic processing", "authors": ["Alexander Kuhnle", "Ann Copestake"], "emails": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction & related work", "text": "In recent years, we have established a new level of performance for many tasks in natural language processing (NLP), language, computer vision and artificial intelligence (AI), while observing a move towards simulated environments and artificial data, especially in AI (Bellemare et al., 2016)."}, {"heading": "2 Problems of real-world datasets for deep learning evaluation", "text": "Below, we look at a variety of questions related to the practice of evaluating DNNs on popular real-world datasets for tasks such as VQA."}, {"heading": "2.1 Issues with crowd-sourced data", "text": "The fact that DNNs require immense amounts of data for successful training led to the practice of acquiring online data, such as the Flickr photo platform, and using crowd sourcing, usually via Amazon Mechanical Turk (AMT). For example, MS COCO (Lin et al., 2014) is a caption record that contains more than 300,000 images commented on with more than 2 million captions written by people, while the popular VQA dataset Antol et al. (2015) is based on MS COCO. However, there are various problems related to this practice. Data obtained in this way tends to be relatively simple in terms of syntax and compositional semantics, although they have a degree of lexical complexity. Moreover, reused photos are not - and never intended - to reflect the visual complexity of everyday scenarios (Pal al al al, al 2008)."}, {"heading": "2.2 The Clever Hans effect", "text": "In fact, most of them are able to survive on their own if they do not see themselves in a position to put themselves in the position they are in."}, {"heading": "2.3 Deep neural networks are universal approximators", "text": "It has long been known that DNNs are universal approximators that are capable of performing any (well-behaved) function when configured accordingly. Recent work by Zhang et al. (2017) has shown how powerful common network architectures are at approximating mere noise. Furthermore, their experiments suggest that the adjustment of noise is no more difficult than the adjustment of meaningful data to DNNs. The only discriminatory effect is that the latter model generalizes to new data while the former do not. [1] The ability of DNNs to insert hidden correlations should therefore not be underestimated. Therefore, we conclude that datasets should ideally be large enough to avoid reusing instances at all, whether during training or evaluation, especially in the case of huge but sparsely covered sample spaces. In this respect, the VA dataset is too clear and small to provide a means for detailed evaluation."}, {"heading": "2.4 Three guiding principles for deep learning evaluation", "text": "We propose three simple and principled methods to reduce the risk of encountering such problems: \u2022 Avoid multi-epoch training: Do not try to iterate over a fixed set of instances, as this allows the system to memorize hidden artifacts in the data. \u2022 Instead of keeping the training and test data distributions similar, focus on the true compositional generalization capabilities required by different distributions.2 \u2022 Do at least some clean data experiments, reducing the likelihood of hidden distortions or correlations compared to more \"realistic\" and complex data. For example, the relationship between image and text in multimodal data should be explicitly controlled for tasks such as VQA.1, or rather the structure of the data for which it would be generalized. 2A more asymmetrical dataset represents a more difficult but potentially more interesting task."}, {"heading": "3 Automatic generation of artificial data using deep linguistic systems", "text": "In the following, we describe our approach to automatically generating artificial VQA data using existing deep linguistic processing technologies. We argue that a compositional semantic approach using bidirectional grammar provides us with exactly the kind of data that the principles in Section 2.4 require. We propose this approach as a complementary evaluation step, as it is not intended to replace real evaluation, but rather to cover aspects of evaluation that existing data sets cannot provide."}, {"heading": "3.1 Abstract microworlds", "text": "In the case of our ShapeWorld framework (Kuhnle and Copestake, 2017) - see Figure 1 for an example - these values include the number of units, their shape and color, position, rotation, hue, etc. Such a world model can be easily visualized. In this context, data sets are generators that can generate an unlimited amount of data instances, making several iterations over a fixed set of training instances obsolete. It is important that different data sets restrict this general sampling process in different ways, for example by limiting the number of objects, the available attribute values, the global positioning of units, etc. This addresses the point of specifying different data distributions for training and testing. In addition, it allows the desired partitioning of evaluation instances, which facilitates a detailed investigation of the behavior of each instance and consequently the systematic composition."}, {"heading": "3.2 Controlled and syntactically rich language generation", "text": "Of the most recent abstract datasets mentioned in the introduction, Suhr et al. (2017) uses human-written captions, the SHAPES dataset (Andreas et al., 2016) uses minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) uses a more complex scenario based on functional building blocks, both designed specifically for their microworlds. For the ShapeWorld framework, we have decided to use technology provided by the DELPH-IN Consortium. Specifically, we wanted to use the broad, bi-directional, high-precision English resource grammar (Flickinger, 2000), which and other DELPH-IN grammars available in a number of languages share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al al al al al al al)."}, {"heading": "A pentagon is above a green ellipse, and no green shape is an ellipse.", "text": "However, the same approach could be extended to more complex areas, such as the clip art setting by Zitnick et al. (2016). In the future, we are planning two interesting enhancements to the ShapeWorld framework: on the one hand, etiquette rules can be expressed at the grammar level and integrated into the generation process as a post-processing step to increase linguistic diversity; on the other hand, bi-directional (D) MRS-based grammars for other languages, such as JACY grammar for Japanese (Siegel et al., 2016), could be used simply by translating the internal mapping of atomic DMRS components into corresponding ShapeWorld semantic elements."}, {"heading": "4 Conclusion: Why use generated artificial data?", "text": "Modularity Not only the caption components can be used in a compositional way, but also different constraint sets for the world model sampling process can be recombined with different captioning modules. For example, quantification and spatial relation statements can use both a world generator that creates worlds with multiple entities (Goyal et al., 2016) 5. In contrast, modularity and detailed configurability make our approach easily reusable for a wide range of potentially unforeseen changes in the evaluation focus (or more general shifts in usage).Challenging data of the abstract world model and semantic language representation allow us to generate a variety of data."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Y. Adi", "E. Kermany", "Y. Belinkov", "O. Lavi", "Y. Goldberg"], "venue": "Proceedings of the International Conference on Learning Representations 2017 (ICLR 2017).", "citeRegEx": "Adi et al\\.,? 2017", "shortCiteRegEx": "Adi et al\\.", "year": 2017}, {"title": "Analyzing the behavior of visual question answering models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, pp. 1955\u20131960.", "citeRegEx": "Agrawal et al\\.,? 2016", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, ICCV 2015.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["P. Arthur", "G. Neubig", "S. Nakamura"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), Austin, Texas, USA.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research 47(1), 253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "OpenAI Gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": null, "citeRegEx": "Brockman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brockman et al\\.", "year": 2016}, {"title": "Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go", "author": ["A. Copestake"], "venue": "Proceedings of the 12th Meeting of the European Chapter of the Association for Computational Linguistics, Athens, Greece, pp. 1\u20139.", "citeRegEx": "Copestake,? 2009", "shortCiteRegEx": "Copestake", "year": 2009}, {"title": "Resources for building applications with Dependency Minimal Recursion Semantics", "author": ["A. Copestake", "G. Emerson", "M.W. Goodman", "M. Horvat", "A. Kuhnle", "E. Muszy\u0144ska"], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC-16), Portoro\u017e, Slovenia, pp. 1240\u20131247. European Language Resources Association (ELRA).", "citeRegEx": "Copestake et al\\.,? 2016", "shortCiteRegEx": "Copestake et al\\.", "year": 2016}, {"title": "Minimal Recursion Semantics: An introduction", "author": ["A. Copestake", "D. Flickinger", "C. Pollard", "I.A. Sag"], "venue": "Research on Language and Computation 3(4), 281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "On building a more efficient grammar by exploiting types", "author": ["D. Flickinger"], "venue": "Natural Language Engineering 6(1), 15\u201328.", "citeRegEx": "Flickinger,? 2000", "shortCiteRegEx": "Flickinger", "year": 2000}, {"title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering", "author": ["Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CoRR abs/1612.00837.", "citeRegEx": "Goyal et al\\.,? 2016", "shortCiteRegEx": "Goyal et al\\.", "year": 2016}, {"title": "Focused evaluation for image description with binary forcedchoice tasks", "author": ["M. Hodosh", "J. Hockenmaier"], "venue": "Proceedings of the 5th Workshop on Vision and Language, Berlin, Germany.", "citeRegEx": "Hodosh and Hockenmaier,? 2016", "shortCiteRegEx": "Hodosh and Hockenmaier", "year": 2016}, {"title": "Learning to reason: End-to-end module networks for visual question answering", "author": ["R. Hu", "J. Andreas", "M. Rohrbach", "T. Darrell", "K. Saenko"], "venue": "ArXiv e-prints.", "citeRegEx": "Hu et al\\.,? 2017", "shortCiteRegEx": "Hu et al\\.", "year": 2017}, {"title": "Revisiting Visual Question Answering Baselines, pp", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "727\u2013739. Cham: Springer International Publishing.", "citeRegEx": "Jabri et al\\.,? 2016", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick"], "venue": "CVPR.", "citeRegEx": "Johnson et al\\.,? 2017a", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick"], "venue": "Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Johnson et al\\.,? 2017b", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "Virtual embodiment: A scalable long-term strategy for artificial intelligence research", "author": ["D. Kiela", "L. Bulat", "A.L. Vero", "S. Clark"], "venue": "CoRR abs/1610.07432.", "citeRegEx": "Kiela et al\\.,? 2016", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "Shapeworld - A new test methodology for multimodal language understanding", "author": ["A. Kuhnle", "A. Copestake"], "venue": "ArXiv e-prints.", "citeRegEx": "Kuhnle and Copestake,? 2017", "shortCiteRegEx": "Kuhnle and Copestake", "year": 2017}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin", "T.-Y.", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision (ECCV), Z\u00fcrich.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "The promise of premise: Harnessing question premises in visual question answering", "author": ["A. Mahendru", "V. Prabhu", "A. Mohapatra", "D. Batra", "S. Lee"], "venue": "ArXiv e-prints.", "citeRegEx": "Mahendru et al\\.,? 2017", "shortCiteRegEx": "Mahendru et al\\.", "year": 2017}, {"title": "A roadmap towards machine intelligence", "author": ["T. Mikolov", "A. Joulin", "M. Baroni"], "venue": "CoRR abs/1511.08130.", "citeRegEx": "Mikolov et al\\.,? 2015", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D.D. Cox", "J.J. DiCarlo"], "venue": "PLOS Computational Biology", "citeRegEx": "Pinto et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2008}, {"title": "Corpus-based dialogue simulation for automatic strategy learning and evaluation", "author": ["K. Scheffler", "S. Young"], "venue": "Proceedings of the NAACL Workshop on Adaptation in Dialogue Systems, pp. 64\u201370.", "citeRegEx": "Scheffler and Young,? 2001", "shortCiteRegEx": "Scheffler and Young", "year": 2001}, {"title": "FOIL it! Find one mismatch between image and language caption", "author": ["R. Shekhar", "S. Pezzelle", "Y. Klimovich", "A. Herbelot", "M. Nabi", "E. Sangineto", "R. Bernardi"], "venue": "ArXiv e-prints.", "citeRegEx": "Shekhar et al\\.,? 2017", "shortCiteRegEx": "Shekhar et al\\.", "year": 2017}, {"title": "Jacy: An Implemented Grammar of Japanese", "author": ["M. Siegel", "E.M. Bender", "F. Bond"], "venue": "CSLI Studies in Computational Linguistics. Stanford: CSLI Publications.", "citeRegEx": "Siegel et al\\.,? 2016", "shortCiteRegEx": "Siegel et al\\.", "year": 2016}, {"title": "RNN Approaches to Text Normalization: A Challenge", "author": ["R. Sproat", "N. Jaitly"], "venue": "CoRR abs/1611.00068.", "citeRegEx": "Sproat and Jaitly,? 2016", "shortCiteRegEx": "Sproat and Jaitly", "year": 2016}, {"title": "A simple method to determine if a music information retrieval system is a \u201chorse", "author": ["B.L. Sturm"], "venue": "IEEE Transactions on Multimedia 16(6), 1636\u20131644.", "citeRegEx": "Sturm,? 2014", "shortCiteRegEx": "Sturm", "year": 2014}, {"title": "A corpus of natural language for visual reasoning", "author": ["A. Suhr", "M. Lewis", "J. Yeh", "Y. Artzi"], "venue": "55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada. Association for Computational Linguistics.", "citeRegEx": "Suhr et al\\.,? 2017", "shortCiteRegEx": "Suhr et al\\.", "year": 2017}, {"title": "MazeBase: A sandbox for learning from games", "author": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus"], "venue": "CoRR abs/1511.07401.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "venue": "CoRR abs/1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "CoRR abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "International Conference on Learning Representations 2017 (ICLR 2017).", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 38(4), 627\u2013638.", "citeRegEx": "Zitnick et al\\.,? 2016", "shortCiteRegEx": "Zitnick et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016).", "startOffset": 105, "endOffset": 152}, {"referenceID": 6, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016).", "startOffset": 105, "endOffset": 152}, {"referenceID": 30, "context": "There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al.", "startOffset": 87, "endOffset": 112}, {"referenceID": 32, "context": "(2015), or the bAbI tasks (Weston et al., 2015).", "startOffset": 26, "endOffset": 47}, {"referenceID": 11, "context": "However, recent work (Goyal et al., 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities.", "startOffset": 21, "endOffset": 63}, {"referenceID": 1, "context": "However, recent work (Goyal et al., 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities.", "startOffset": 21, "endOffset": 63}, {"referenceID": 2, "context": "To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 15, "context": ", 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 18, "context": "(2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017).", "startOffset": 37, "endOffset": 65}, {"referenceID": 13, "context": "Moreover, such datasets have been of great importance for the development of new VQA models based on reinforcement learning (Hu et al., 2017; Johnson et al., 2017b).", "startOffset": 124, "endOffset": 164}, {"referenceID": 16, "context": "Moreover, such datasets have been of great importance for the development of new VQA models based on reinforcement learning (Hu et al., 2017; Johnson et al., 2017b).", "startOffset": 124, "endOffset": 164}, {"referenceID": 3, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016). As outlined by Kiela et al. (2016), simulated data is appealing for various reasons.", "startOffset": 106, "endOffset": 189}, {"referenceID": 3, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016). As outlined by Kiela et al. (2016), simulated data is appealing for various reasons. Most importantly, it acts as a prototypical problem presentation, abstracted from its noisy and intertwined real-world appearance. However, with the exception of spoken dialogue systems (e.g., Scheffler and Young (2001)), artificial data is relatively little used in NLP.", "startOffset": 106, "endOffset": 459}, {"referenceID": 3, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016). As outlined by Kiela et al. (2016), simulated data is appealing for various reasons. Most importantly, it acts as a prototypical problem presentation, abstracted from its noisy and intertwined real-world appearance. However, with the exception of spoken dialogue systems (e.g., Scheffler and Young (2001)), artificial data is relatively little used in NLP. There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al. (2015), or the bAbI tasks (Weston et al.", "startOffset": 106, "endOffset": 682}, {"referenceID": 1, "context": ", 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities. To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017).", "startOffset": 8, "endOffset": 427}, {"referenceID": 1, "context": ", 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities. To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017). They all consist of images showing abstract scenes with colored objects, and are introduced with the motivation to provide a challenging and clear evaluation for VQA systems. Johnson et al. (2017a) and Kuhnle and Copestake (2017) investigated popular VQA systems on their dataset, and demonstrate how artificial data provides us with detailed insights previously not possible.", "startOffset": 8, "endOffset": 685}, {"referenceID": 1, "context": ", 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities. To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017). They all consist of images showing abstract scenes with colored objects, and are introduced with the motivation to provide a challenging and clear evaluation for VQA systems. Johnson et al. (2017a) and Kuhnle and Copestake (2017) investigated popular VQA systems on their dataset, and demonstrate how artificial data provides us with detailed insights previously not possible.", "startOffset": 8, "endOffset": 717}, {"referenceID": 19, "context": "For instance, MS COCO (Lin et al., 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 23, "context": "Moreover, repurposed photos do not \u2013 and were never intended to \u2013 reflect the visual complexity of every-day scenarios (Pinto et al., 2008).", "startOffset": 119, "endOffset": 139}, {"referenceID": 3, "context": ", 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al. (2015) is based on MS COCO.", "startOffset": 165, "endOffset": 185}, {"referenceID": 3, "context": ", 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al. (2015) is based on MS COCO. There are, however, various problems related to this practice. Data obtained this way tends to be comparatively simple in terms of syntax and compositional semantics, despite exhibiting a degree of lexical complexity due to its real-world breadth. Moreover, repurposed photos do not \u2013 and were never intended to \u2013 reflect the visual complexity of every-day scenarios (Pinto et al., 2008). Humans given the task of captioning such images will mostly produce descriptions which are syntactically simple. The way that workers on crowd-sourcing platforms are paid gives them an incentive to come up with captions quickly, and hence further increases the tendency to simplicity. Note also that, while this is a form of real-world data, it has very little relationship to the way that a human language learner perceives the world. For instance, the photo/question pairs are presented to a VQA system randomly and with no possibility of detailed interaction with a particular scene. Natural language follows Zipf\u2019s law for many aspects (sentence length, syntactic complexity, word usage, etc), and consequently has an inbuilt simplicity bias when considered in terms of probability mass. The contents of image datasets based on photos also have a Zipfian distribution, but with biases which relate to what people choose to photograph rather than to what they see. Animal images in the VQA dataset are predominantly cats and dogs, sport images mainly baseball and tennis (see Antol et al. (2015) for more statistics).", "startOffset": 165, "endOffset": 1694}, {"referenceID": 0, "context": "Some of the recent findings for DNNs, particularly in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al.", "startOffset": 249, "endOffset": 267}, {"referenceID": 14, "context": ", 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an instance of the Clever Hans effect, and due to unnoticed biases in the datasets? A more fundamental form of this effect is illustrated by recent investigations in image recognition.", "startOffset": 150, "endOffset": 170}, {"referenceID": 9, "context": "Goyal et al. (2016) and Mahendru et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "Goyal et al. (2016) and Mahendru et al. (2017) note how questions rarely talk about objects that are not present in the image, hence an existential question like \u201cDo you see a.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Agrawal et al. (2016) also give the example of questions like \u201cWhat covers the ground?\u201d, which can confidently be answered with \u201csnow\u201d because of biases in common real-world scenes, or, more precisely, biases in the photographs of real-world scenes.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2016) also give the example of questions like \u201cWhat covers the ground?\u201d, which can confidently be answered with \u201csnow\u201d because of biases in common real-world scenes, or, more precisely, biases in the photographs of real-world scenes. Such biases help to explain why some text-only systems turn out to perform well on visual question answering when evaluated on the VQA dataset. Sturm (2014) compared such unexpected cues when evaluation machine learning systems to the story of \u201cClever Hans\u201d, a horse exhibited in the early 20th century which was claimed to understand German and have extensive arithmetical and reasoning abilities.", "startOffset": 0, "endOffset": 407}, {"referenceID": 0, "context": "Some of the recent findings for DNNs, particularly in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an instance of the Clever Hans effect, and due to unnoticed biases in the datasets? A more fundamental form of this effect is illustrated by recent investigations in image recognition. Szegedy et al. (2014) and Nguyen et al.", "startOffset": 250, "endOffset": 671}, {"referenceID": 0, "context": "Some of the recent findings for DNNs, particularly in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an instance of the Clever Hans effect, and due to unnoticed biases in the datasets? A more fundamental form of this effect is illustrated by recent investigations in image recognition. Szegedy et al. (2014) and Nguyen et al. (2015) have shown surprisingly odd system behavior when", "startOffset": 250, "endOffset": 696}, {"referenceID": 26, "context": ", Sproat and Jaitly (2016) and Arthur et al.", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": ", Sproat and Jaitly (2016) and Arthur et al. (2016). The ability to work with raw input data and to pick up correlations/biases, which humans cannot always manifest in explicit symbolic rules, is precisely the strength of DNNs as feature extractors.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "Here, models do well at extremely difficult tasks just by end-to-end training on enough data points, while more detailed investigations find that they, unexpectedly, struggle even with simple abstract abilities like counting or spatial relations (Jabri et al., 2016).", "startOffset": 246, "endOffset": 266}, {"referenceID": 32, "context": "Recent work by Zhang et al. (2017) demonstrated how powerful common network architectures are in approximating mere noise.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": "In the case of our ShapeWorld framework (Kuhnle and Copestake, 2017) \u2014 see figure 1 for an example \u2014 these include the number of entities, their shape and color, position, rotation, shade, etc.", "startOffset": 40, "endOffset": 68}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 15, "context": ", 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds.", "startOffset": 52, "endOffset": 75}, {"referenceID": 10, "context": "In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000).", "startOffset": 116, "endOffset": 134}, {"referenceID": 22, "context": "Of the recent abstract datasets mentioned in the introduction, Suhr et al. (2017) use human-written captions, the SHAPES dataset (Andreas et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds. For the ShapeWorld framework, we decided that we will use technology made available by the DELPH-IN consortium. In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000). This and other DELPH-IN grammars, available for a range of languages, share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al. (2005)).", "startOffset": 55, "endOffset": 673}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds. For the ShapeWorld framework, we decided that we will use technology made available by the DELPH-IN consortium. In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000). This and other DELPH-IN grammars, available for a range of languages, share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al. (2005)). For our system we use a variant of MRS, Dependency MRS (DMRS, Copestake (2009), Copestake et al.", "startOffset": 55, "endOffset": 754}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds. For the ShapeWorld framework, we decided that we will use technology made available by the DELPH-IN consortium. In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000). This and other DELPH-IN grammars, available for a range of languages, share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al. (2005)). For our system we use a variant of MRS, Dependency MRS (DMRS, Copestake (2009), Copestake et al. (2016)), and generate natural language sentences from abstract DMRS graphs using Packard\u2019s parser-generator ACE4.", "startOffset": 55, "endOffset": 779}, {"referenceID": 26, "context": "On the other hand, bidirectional (D)MRSbased grammars for other languages, such as the JACY grammar for Japanese (Siegel et al., 2016), could be used simply by translating the internal mapping of atomic DMRS components to corresponding ShapeWorld-semantic elements.", "startOffset": 113, "endOffset": 134}, {"referenceID": 33, "context": "However, the same approach could be extended to more complex domains, like the clip-art setting of Zitnick et al. (2016). In the future, we plan to implement two interesting extensions for the ShapeWorld framework: On the one hand, paraphrase rules can be expressed on grammar-level and integrated into the generation process as post-processing step for increased linguistic variety.", "startOffset": 99, "endOffset": 121}, {"referenceID": 11, "context": "Flexibility & reusability Real-world or human-created data essentially has to be obtained again for every change/update (Goyal et al., 2016)5.", "startOffset": 120, "endOffset": 140}, {"referenceID": 12, "context": "Another interesting approach is to automatically process data afterwards (Hodosh and Hockenmaier, 2016; Shekhar et al., 2017), resulting in a form of \u201clightly\u201d artificial data.", "startOffset": 73, "endOffset": 125}, {"referenceID": 25, "context": "Another interesting approach is to automatically process data afterwards (Hodosh and Hockenmaier, 2016; Shekhar et al., 2017), resulting in a form of \u201clightly\u201d artificial data.", "startOffset": 73, "endOffset": 125}], "year": 2017, "abstractText": "We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing \u2018deep\u2019 linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail.", "creator": "LaTeX with hyperref package"}}}