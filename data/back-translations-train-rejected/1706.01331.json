{"id": "1706.01331", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Event Representations for Automated Story Generation with Deep Neural Nets", "abstract": "Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.", "histories": [["v1", "Mon, 5 Jun 2017 14:04:48 GMT  (54kb,D)", "https://arxiv.org/abs/1706.01331v1", "8 pages, 1 figure"], ["v2", "Mon, 14 Aug 2017 18:14:02 GMT  (57kb,D)", "http://arxiv.org/abs/1706.01331v2", "8 pages, 1 figure"], ["v3", "Tue, 12 Sep 2017 14:45:22 GMT  (216kb,D)", "http://arxiv.org/abs/1706.01331v3", "Submitted to AAAI'18"]], "COMMENTS": "8 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["lara j martin", "prithviraj ammanabrolu", "xinyu wang", "william hancock", "shruti singh", "brent harrison", "mark o riedl"], "accepted": false, "id": "1706.01331"}, "pdf": {"name": "1706.01331.pdf", "metadata": {"source": "CRF", "title": "Event Representations for Automated Story Generation with Deep Neural Nets", "authors": ["Lara J. Martin", "Prithviraj Ammanabrolu", "Xinyu Wang", "William Hancock", "Shruti Singh", "Brent Harrison", "Mark O. Riedl"], "emails": ["riedl]@gatech.edu"], "sections": [{"heading": "Introduction", "text": "The problem with the automatic generation of stories is the automatic selection of a sequence of events, actions or words that can be told as a story. To date, most action systems have symbolic planning (Meehan 1976; Lebowitz 1987; Pe \u0301 rez y Pe \u0301 rez and Sharples 2009; Riedl and Young 2010) or the justification of cases that cannot be executed; these systems are limited to telling topics covered by domain knowledge. Consequently, it is difficult to determine whether the quality of the stories produced by these systems is a result of algorithms or good knowledge engineering. Open story generation (Li et al. 2013) is the problem of the automatic generation of stories produced by these systems."}, {"heading": "Related Work", "text": "In fact, it is not that this is an attempt to generate stories for predetermined and well-defined areas of characters, places and actions; the creativity of these systems mixed the robustness of manually generated knowledge and algorithm aptitude. (Gerva \u0301 s et al. 2005) These techniques could only be used to generate stories for predetermined and well-defined areas of characters, places and actions. The creativity of these systems mixed the robustness of manually generated knowledge and algorithm aptitude. (Recently, machine learning has been used to learn the domain model from which stories can be created or to identify segments of story content available in an existing repository to assemble stories. The SayAnthing system (Swanson and Gordon 2012) uses textual case-basedreasoning to identify relevant existing story content in online blogs."}, {"heading": "Event Representation", "text": "In this section, we consider what the level of abstraction should be for the inputs into the network that produce the best predictive power while maintaining semantic knowledge. Event sparseness results in a situation where all event successes have a low probability of occurrence."}, {"heading": "Event-to-Event", "text": "The event2event network is a recurring multi-layer encoder network based on (Sutskever, Vinyals, and Le 2014).Unless otherwise stated, our event2event network is trained with input x = wn1, w n 2, w n 3, w n 4, with each w n i being either s, v, o, or m from the nth event, and output y = wn + 11, w n + 1 2, w n + 1 3, w n + 1 4. The experiments described below attempt to determine how different event representations affected Event2event predictions of the subsequent event in a story. We evaluated each event representation using two metrics. Perplexity is the measure of how \"surprised\" a model is by a training set. Here, we use it to get a sense of how well the probability model we have trained can predict the data."}, {"heading": "Experimental Setup", "text": "For each experiment, we made a sequence-to-sequence prediction. < We can predict a set within an initial set. < We can predict a set. < We performed a previous experiment with tensor flow (Abadi et al. 2015), each network was formed using the same parameters (0.5 learning rate, 0.99 learning rate decay, 5.0 maximum gradient, 64 batch size, 1024 model layer size and 4 layers), with only the input / output number, number of epochs and vocabulary being an exception until the total loss was less than 5% per epoch. This took between 40 and 60 epochs for all experiments. Data was divided into 80% training, 10% validation of test data. All reported results were evaluated using the held test data. We evaluated 11 versions of our event representation against a judgment basis."}, {"heading": "Results and Discussion", "text": "The results of the experiments outlined above can be found in Table 1. The original word events had a similar perplexity as the original sentences. These parallels are similar to the observations of Pichotta and Mooney (Pichotta and Mooney 2016b). Deleting words did little to improve the predictability of our event2event network. However, the perplexity improved significantly when character names were replaced by generalized < NE > tags, followed by generalizing other words and combinations. Overall, the generalized events had much better perplexity and turned them into bigrams - with the story - improving BLEU values to almost those of the original word events. Adding gene information improved perplexity. The best perplexity was achieved when multiple generalized events were created from sentences as long as all events were fed at the same time (i.e. no order was imposed on events arising from the same sentence)."}, {"heading": "Event-to-Sentence", "text": "Since converting sentences into (multiple) events is a linear and lossy process for event2event, the translation of events into sentences is not trivial as it must add details back. Thus, the event < relative.n.01, characterize-29.2, male.n.02, feel-ing.n.01 > hypothetically came from the sentence \"Your brother praised the boy for his empathy.\" In reality, this event came from the phrase \"Hisonkel looks at him with disgust.\" Complicating the situation, the event2event encoderdecoder network is not guaranteed to produce an event that has ever been seen in the corpus of training history. Furthermore, our experiments with event representations for event2event suggest that a larger generalization helps to better generate stories. < The larger the generalization, the more difficult it is to translate an event back into a natural sentence."}, {"heading": "Experimental Setup", "text": "In fact, most of us are able to set out to find a solution that enables us to outwit ourselves."}, {"heading": "Results and Discussion", "text": "Although generalizing sentences drastically improves confusion, splitting and trimming sentences yields better BLEU values if the original words are retained. In the case of event2phrase, BLEU values make more sense as a measure because the task is a translation task. Confusion in these experiments seems to correspond to the size of the vocabulary. Generalized events with full-length generalized sentences have better BLEU values than when the original words are used. However, when we work with S + P sentences, the pattern reverses. We believe that because both S + P and word generalization methods reduce the rarity of events when combined with too much information, the neural network struggles to find distinctive patterns. Table 3 shows examples from the entire pipeline as it currently exists, i.e. from one sentence to the next without filling in a sentence (see Figure 1)."}, {"heading": "Future Work", "text": "The question remains how exactly to determine which character names and noun details should be used for the < NE > s and WordNet placeholders. In Figure 1, we suggest adding working memory and long-term memory modules. The working memory module would retain the character names and nouns in a lookup table that were removed during the eventification process. After a partially generalized sentence has been produced by event2Sent, the system can use the working memory search table to fill character names and nouns back into the placeholders. The intuition is that many details - especially character names - may be repeated from one event to the next. In stories, it is common to see a form of deflection between characters. For example, the two events are \"John hits Andrew\" and \"Andrew runs away from John,\" followed by \"John chases Andrew,\" which illustrate the turn pattern."}, {"heading": "Conclusions", "text": "The automated generation of stories depends on the representation of events. We assume that by using our intuition to tell events, we can select a representation for story events that maintains the semantic meaning of textual story data while reducing the rarity of events. In particular, the rarity of events leads to poor performance in story generation. Our experiments with different story representations during event generation support our hypothesis about the representation of events. We found that events that are most abstract from the text of natural language improve the generative ability of a recurring neural network to generate stories the most. Event bigrams do not significantly harm the generative model and are likely to help with coherence, as they involve more history in the process, although the coherence of the story is difficult to measure and has not been evaluated in our experiments. Although generalizing events away from the natural language appears to help with the content of the story generation."}, {"heading": "Acknowledgments", "text": "This work is supported by DARPA W911NF-15-C-0246. The views, opinions and / or conclusions contained in this paper are those of the author and should not be interpreted to represent, express or imply, the official views or guidelines of DARPA or the State Department. Authors would like to thank Murtaza Dhuliawala, Animesh Mehta and Yuval Pinter for their technical contributions."}], "references": [{"title": "TensorFlow: Largescale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["V. van", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "van et al\\.,? \\Q2015\\E", "shortCiteRegEx": "van et al\\.", "year": 2015}, {"title": "Learning latent personas of film characters", "author": ["O\u2019Connor Bamman", "D. Smith 2014] Bamman", "B. O\u2019Connor", "N.A. Smith"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Bamman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2014}, {"title": "and Jurafsky", "author": ["N. Chambers"], "venue": "D.", "citeRegEx": "Chambers and Jurafsky 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "J", "author": ["Finkel"], "venue": "R.; Grenager, T.; and Manning, C.", "citeRegEx": "Finkel. Grenager. and Manning 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Story plot generation based on CBR", "author": ["Gerv\u00e1s"], "venue": "Journal of Knowledge-Based Systems", "citeRegEx": "Gerv\u00e1s,? \\Q2005\\E", "shortCiteRegEx": "Gerv\u00e1s", "year": 2005}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "G", "author": ["Khalifa, A.", "Barros"], "venue": "A.; and Togelius, J.", "citeRegEx": "Khalifa. Barros. and Togelius 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "R", "author": ["R. Kiros", "Y. Zhu", "Salakhutdinov"], "venue": "R.; Zemel, R.; Urtasun, R.; Torralba, A.; and Fidler, S.", "citeRegEx": "Kiros et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["B. Li", "S. Lee-Urban", "G. Johnston", "Riedl"], "venue": "O.", "citeRegEx": "Li et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "S", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "Bethard"], "venue": "J.; and McClosky, D.", "citeRegEx": "Manning et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "M", "author": ["L.J. Martin", "B. Harrison", "Riedl"], "venue": "O.", "citeRegEx": "Martin. Harrison. and Riedl 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["Meehan"], "venue": "R.", "citeRegEx": "Meehan 1976", "shortCiteRegEx": null, "year": 1976}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "A corpus and evaluation framework for deeper understanding of commonsense stories", "author": ["Mostafazadeh"], "venue": "arXiv preprint arXiv:1604.01696", "citeRegEx": "Mostafazadeh,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh", "year": 2016}, {"title": "J", "author": ["N. Mostafazadeh", "M. Roth", "A. Louis", "N. Chambers", "Allen"], "venue": "F.", "citeRegEx": "Mostafazadeh et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "and Sharples", "author": ["R. P\u00e9rez y P\u00e9rez"], "venue": "M.", "citeRegEx": "P\u00e9rez y P\u00e9rez and Sharples 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning Statistical Scripts with LSTM Recurrent Neural Networks", "author": ["Pichotta", "K. Mooney 2016a] Pichotta", "R.J. Mooney"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Pichotta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pichotta et al\\.", "year": 2016}, {"title": "Using Sentence-Level LSTM Language Models for Script Inference", "author": ["Pichotta", "K. Mooney 2016b] Pichotta", "R.J. Mooney"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Pichotta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pichotta et al\\.", "year": 2016}, {"title": "and Cavazza", "author": ["J. Porteous"], "venue": "M.", "citeRegEx": "Porteous and Cavazza 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "R", "author": ["M.O. Riedl", "Young"], "venue": "M.", "citeRegEx": "Riedl and Young 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["M. Roemmele", "S. Kobayashi", "N. Inoue", "Gordon"], "venue": "M.", "citeRegEx": "Roemmele et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "and Abelson", "author": ["R. Schank"], "venue": "R.", "citeRegEx": "Schank and Abelson 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "K", "author": ["Schuler"], "venue": "K.", "citeRegEx": "Schuler 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Gordon", "author": ["R. Swanson"], "venue": "A.", "citeRegEx": "Swanson and Gordon 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2017, "abstractText": "Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a midlevel of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.", "creator": "LaTeX with hyperref package"}}}