{"id": "1706.01340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "abstract": "Using supporting backchannel (BC) cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory (LSTM) networks which have shown to outperform feed-forward based setups on various tasks. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39.", "histories": [["v1", "Fri, 2 Jun 2017 17:05:26 GMT  (476kb,D)", "http://arxiv.org/abs/1706.01340v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.HC cs.LG cs.SD", "authors": ["robin ruede", "markus m\\\"uller", "sebastian st\\\"uker", "alex waibel"], "accepted": false, "id": "1706.01340"}, "pdf": {"name": "1706.01340.pdf", "metadata": {"source": "CRF", "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "authors": ["Robin Ruede", "Markus M\u00fcller", "Sebastian St\u00fcker", "Alex Waibel"], "emails": ["robin.ruede@student.kit.edu", "m.mueller@kit.edu", "sebastian.stueker@kit.edu", "alexander.waibel@kit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to achieve our objectives."}, {"heading": "2 Related Work", "text": "They are based on different types of predictors and use a variety of input modalities, including acoustic characteristics such as pause and pitch, as well as visual cues such as head movements; in addition to these direct characteristics, additional sources of information such as speech models or parts of speech markers appear; they have proposed a method that uses acoustic characteristics; the authors claim that the most important acoustic phenomena for BC predictions occur just before a BC. As characteristics, they used pause information, as well as pitches (falling or rising pitches); they conducted their experiments on a Dutch corpus and reported that the most important feature in their work is the duration of the pause. [27] They suggested using BCs at low pitches and pauses in regions in English and Japanese. But building a rules-based system might prove difficult, as these rules are created manually."}, {"heading": "3 Backchannel Prediction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 BC Utterance Selection", "text": "There are different types of Phrasal BCs, they can be non-binding, positive, negative, questioning, etc. To simplify the problem of predicting BCs, we simply try to predict the trigger times for each type of BC, ignoring the distinction between different types of reactions."}, {"heading": "3.2 Feature Selection", "text": "A neural network is able to independently learn beneficial characteristic representations. Feeding absolute pitch and power values (signal energy) for a given time context therefore allows the network to automatically extract relevant information such as pitch differences and pause triggers, as it has been used in related research [16]. In addition to pitch and power, we have also evaluated other acoustic characteristics such as fundamental frequency variation (FFV) [12] and Mel frequency receiver coefficients (MFCCs). Finally, we have tried to add a coding of the speaker's word history prior to the listener's back channel in order to assess with word2vec [14] whether our setup benefits from multimodal input functions."}, {"heading": "3.3 Training and Neural Network Design", "text": "We assumed that we had two separate but synchronized audio channels and corresponding transcripts: one for the speaker and one for the listener. We had to decide which audio areas we should use to train the network. Because we wanted to predict the BC's online without using future information, we had to train the network to detect audio segments from the speaker track that would potentially cause a BC in the listener track. We chose the beginning of the BC utterance as an anchor and used a fixed context as a positive prediction area before that. We also had to select negative examples so that the network would not always lean toward a BC. We did this by selecting the area a few seconds before each BC because the listener explicitly chose not to give a feedback response in that area. This resulted in a fully balanced training dataset. We first used a prediction architecture that takes all the selected features beyond the pre-defined time context."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We used the Switchboard Data Set [3], which consists of 2,438 English phone calls of five to ten minutes duration, totaling 260 hours. Couples of participants from the United States were encouraged to discuss a specific topic randomly selected from 70 possibilities. Interlocutors and topics were selected so that two people spoke only once and each person discussed a specific topic only once. These phone calls are commented on with transcriptions and word alignments [4] totaling 390k utterances or 3.3 million words. We randomly divided the data set into 2,000 conversations for training, 200 for validation, and 238 for evaluation. We used annotations from the Switchboard Dialog Act Corpus (SwDA) [6] to decide which utterances should be classified as BCs. The SwDA contains categorical annotations for the utterances of about half of the data in the Switchboard Corpus."}, {"heading": "4.2 Extraction", "text": "Since the SwDA is incomplete, we had to identify the utterances as BCs, just by their text. We manually added some additional utterances that were missing from the SwDA transcriptions but are present in the original transcriptions, by going through the most common utterances and selecting those that seemed relevant, such as \"um-hum yeah\" and \"absolute.\" The most common BCs in the records are \"yeah,\" \"um-hum,\" uh-huh \"and\" right, \"totaling 68% of all BC phrases extracted. To select which utterances should be categorized as BCs and used for training, we first filtered sounds and other markers like laughter from the transcriptions. Some utterances such as\" uh-huh \"can be both BCs and speech disorders, so we chose only those that have either silence or another BC context in front of them before we extracted a break from the transcriptions."}, {"heading": "4.3 Training", "text": "We used Theano [24] with lasagna [1] for rapid prototyping and testing of different parameters.1 To evaluate different hyperparameters, we trained several network configurations with different context lengths (500 ms to 2000 ms), context steps (1 to 4 frames), network depths (one to four hidden layers), layer sizes (15 to 125 neurons), activation functions (Tanh and Relu), optimization methods (SGD, Adadelta and Adam [9]), weight initialization methods (constant zero and glorot [2]), and layer types (feed forward and LSTM). The LSTM networks we tested were vulnerable to rapid upgrading. To overcome this, we tried two methods of regulation. The first was drop-out training, where we randomly dropped a certain proportion of neuron outputs in each layer, with the results decreasing by 0.0002."}, {"heading": "4.4 Postprocessing", "text": "To ensure that our prediction does not use future information, we use a Gaussian filter1. Our code for extraction, training, post-processing, and evaluation will be available at https: / / github.com / phiresky / Backchannel Prediction. The repository also includes a script to reproduce all the results of this paper. This code is truncated asymmetrically by a multiple of the standard deviation for the page that would reach into the future, and offset it so that the last frame is \u00b1 0ms away from the forecast target time. This means that the latency of our prediction increases by c \u00b7 \u03c3 ms. If we select c = 0, we truncate the complete right half of the bell curve, keeping the latency at 0 at the expense of the accuracy of the filter."}, {"heading": "4.5 Evaluation", "text": "Since the output of our predictor is only relevant for segments in which only one person speaks, we perform our evaluation on monologue segments. We define a monologue segment as the maximum possible time range in which one person speaks continuously and the other person does not speak continuously for at least five seconds. Thus, we obtain segments in which one participant speaks, and the other produces back channels without taking the curve. We define a prediction time as correct if it is within a given error range from the beginning of a correct BC utterance. We have conducted most of our predictions with an error margin of [0 ms, + 1000 ms], but also provide results for other margins used in related research. For comparison, we have also used a random predictor as the basis for the evaluation time of a predictor rate file and uniform audio formatting."}, {"heading": "5 Results", "text": "We used \"70: 35\" to denote a network layer configuration of input \u2192 70 neurons \u2192 output.We tested different context widths. A context width of nms means that we used the range [\u2212 nms, 0ms] from the beginning of feedback to the context width of 500 ms. We tested only the context width of the input data. We found that training performed worse on each individual frame than in Table 1a, longer contexts caused the predictor to trigger too late. We tested it with every n-th of the input data. Although we did this for performance reasons, we noted that the training was based on each frame."}, {"heading": "6 Conclusion and Future Work", "text": "We presented a new approach to predicting BC's with neural networks. By using more sophisticated methods for network training and different network architectures, we were able to improve the F1 score compared to our previous experiments. In addition to evaluating different hyperparameter configurations, we also experimented with LSTM networks, which led to improved results. Our best system achieved an F1 score of 0.388. We used linguistic features via word2vec only in a very basic way, provided the availability of instant speech recognition will be possible through the use of reference transcripts."}], "references": [{"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Aistats, vol. 9, pp. 249\u2013256", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "ISIP switchboard word alignments", "author": ["D Harkins"], "venue": "URL https://www.isip. piconepress.com/projects/switchboard/", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Learning backchannel prediction model from parasocial consensus sampling: a subjective evaluation", "author": ["L. Huang", "L.P. Morency", "J. Gratch"], "venue": "International Conference on Intelligent Virtual Agents, pp. 159\u2013172. Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Switchboard discourse language modeling project", "author": ["D. Jurafsky", "C Van Ess-Dykema"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Toward Adaptive Generation of Backchannels for Attentive Listening Agents", "author": ["T. Kawahara", "M. Uesato", "K. Yoshino", "K. Takanashi"], "venue": "International Workshop Serien on Spoken Dialogue Systems Technology, pp. 1\u201310", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Prediction and Generation of Backchannel Form for Attentive Listening Systems", "author": ["T. Kawahara", "T. Yamaguchi", "K. Inoue", "K. Takanashi", "N. Ward"], "venue": "Proc. INTERSPEECH, vol. 2016", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on evaluation metrics for backchannel prediction models", "author": ["I. de Kok", "D. Heylen"], "venue": "Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A Survey on Evaluation Metrics for Backchannel Prediction Models", "author": ["Kok", "I.d.", "D. Heylen"], "venue": "Feedback Behaviors in Dialog", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "The fundamental frequency variation spectrum", "author": ["K. Laskowski", "M. Heldner", "J. Edlund"], "venue": "Proceedings of FONETIK 2008 pp. 29\u201332", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "The janus-iii translation system: Speech-to-speech translation in multiple domains", "author": ["L. Levin", "A. Lavie", "M. Woszczyna", "D. Gates", "M. Gavald\u00e1", "D. Koll", "A. Waibel"], "venue": "Machine Translation 15(1), 3\u201325", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "On bayesian methods for seeking the extremum", "author": ["J. Mockus"], "venue": "Proceedings of the IFIP Technical Conference, pp. 400\u2013404. Springer-Verlag, London, UK, UK", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1974}, {"title": "A Probabilistic Multimodal Approach for Predicting Listener Backchannels", "author": ["L.P. Morency", "I. de Kok", "J. Gratch"], "venue": "Autonomous Agents and Multi-Agent Systems 20(1), 70\u201384", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Using Neural Networks for Data-Driven Backchannel Prediction: A Survey on Input Features and Training Techniques", "author": ["M. M\u00fcller", "D. Leuschner", "L. Briem", "M. Schmidt", "K. Kilgour", "S. St\u00fcker", "A. Waibel"], "venue": "International Conference on Human-Computer Interaction, pp. 329\u2013 340. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic transcription for low-latency speech translation", "author": ["J. Niehues", "T.S. Nguyen", "E. Cho", "T.L. Ha", "K. Kilgour", "M. M\u00fcller", "M. Sperber", "S. St\u00fcker", "A. Waibel"], "venue": "Interspeech 2016 pp. 2513\u20132517", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "HMM and Neural Network Based Speech Act Detection", "author": ["K. Ries"], "venue": "Proceedings of the Acoustics, Speech, and Signal Processing, 1999. on 1999 IEEE International ConferenceVolume 01, pp. 497\u2013500. IEEE Computer Society", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Building Autonomous Sensitive Artificial Listeners", "author": ["M. Schroder", "E. Bevacqua", "R. Cowie", "F. Eyben", "H. Gunes", "D. Heylen", "M. Ter Maat", "G. McKeown", "S. Pammi", "M Pantic"], "venue": "IEEE Transactions on Affective Computing 3(2), 165\u2013183", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1), 1929\u20131958", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Dialogue act modeling for automatic tagging and recognition of conversational speech", "author": ["A. Stolcke", "K. Ries", "N. Coccaro", "E. Shriberg", "R. Bates", "D. Jurafsky", "P. Taylor", "R. Martin", "C. Van Ess-Dykema", "M. Meteer"], "venue": "Computational linguistics 26(3), 339\u2013373", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Dialog act modeling for conversational speech", "author": ["A Stolcke"], "venue": "AAAI Spring Symposium on Applying Machine Learning to Discourse Processing, pp. 98\u2013105", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints abs/1605.02688", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "A Rule-Based Backchannel Prediction Model Using Pitch and Pause Information", "author": ["K.P. Truong", "R.W. Poppe", "D.K.J. Heylen"], "venue": "Proceedings of Interspeech 2010, Makuhari, Chiba, Japan, pp. 3058\u20133061. International Speech Communication Association (ISCA)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Phoneme Recognition Using Time-Delay Neural Networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE transactions on acoustics, speech, and signal processing 37(3), 328\u2013339", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1989}, {"title": "Prosodic features which cue back-channel responses in english and japanese", "author": ["N. Ward", "W. Tsukahara"], "venue": "Journal of pragmatics 32(8), 1177\u20131207", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 22, "context": "[25] proposed a method that uses acoustic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] proposed a similar approach triggering BCs at low pitch and pause regions in English and Japanese.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] proposes an approach that incorporates sequential probabilistic models like Hidden Markov Models or Conditional Random Fields.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "In another approach, predicting different types of BC was attempted [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 17, "context": "Detecting BCs in real-time was also proposed [20] in the past.", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "A first approach towards detecting speech acts (including BCs) was proposed by Ries [19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Stolcke also proposed NN based methods for modelling dialogue acts [23, 22].", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "Stolcke also proposed NN based methods for modelling dialogue acts [23, 22].", "startOffset": 67, "endOffset": 75}, {"referenceID": 14, "context": "In the past, we also proposed an NN based approach [17] that was mainly data-driven, requiring only minimal post-processing of the network outputs.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "[11] provides a comparison of different approaches for evaluation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In addition to objective measures, user studies are also a possibility to evaluate BC systems, like we did in the past [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "A general study about the occurrence of BCs with respect to their role in facilitating attentive listening also exists [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "Hence, feeding the absolute pitch and power (signal energy) values for a given time context enables the network to automatically extract the relevant information such as pitch slopes and pause triggers, as used in related research [16].", "startOffset": 231, "endOffset": 235}, {"referenceID": 9, "context": "In addition to pitch and power, we also evaluated using other acoustic features such as the fundamental frequency variation (FFV) [12] and the Mel-frequency cepstral coefficients (MFCCs).", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "Finally, we tried adding an encoding of the speakers\u2019 word history before the listener backchannel using word2vec [14] to assess whether our setup benefits from multimodal input features.", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "These telephone conversations are annotated with transcriptions and word alignments [4] with a total of 390k utterances or 3.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "We used annotations from the Switchboard Dialog Act Corpus (SwDA) [6] to decide which utterances to classify as BCs.", "startOffset": 66, "endOffset": 69}, {"referenceID": 10, "context": "We used the Janus Recognition Toolkit [13] for parts of the feature extraction (power, pitch tracking, FFV, MFCC).", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "We used Theano [24] with Lasagne [1] for rapid prototyping and testing of different parameters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "1 To evaluate different hyperparameters, we trained multiple network configurations with various context lengths (500ms to 2000ms), context strides (1 to 4 frames), network depths (one to four hidden layers), layer sizes (15 to 125 neurons), activation functions (tanh and relu), optimization methods (SGD, Adadelta and Adam [9]), weight initialization methods (constant zero and Glorot [2]), and layer types (feed forward and LSTM).", "startOffset": 325, "endOffset": 328}, {"referenceID": 0, "context": "1 To evaluate different hyperparameters, we trained multiple network configurations with various context lengths (500ms to 2000ms), context strides (1 to 4 frames), network depths (one to four hidden layers), layer sizes (15 to 125 neurons), activation functions (tanh and relu), optimization methods (SGD, Adadelta and Adam [9]), weight initialization methods (constant zero and Glorot [2]), and layer types (feed forward and LSTM).", "startOffset": 387, "endOffset": 390}, {"referenceID": 18, "context": "The first was Dropout training, where we randomly dropped a specific portion of neuron outputs in each layer for each training batch [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "We determined the optimal postprocessing hyperparameters for each network configuration and allowed margin of error automatically using Bayesian optimization [15] with the validation F1-Score as the utility function.", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "We compared the results from [17] with our system.", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "[17] used the same dataset, but focused on offline predictions, meaning their network had future information available, and they evaluated their performance on the whole corpus", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "As low-latency speech recognition is possible [18], one of the next steps would be to combine both systems.", "startOffset": 46, "endOffset": 50}, {"referenceID": 23, "context": "We only tested feed forward neural networks and LSTMs, other architectures like timedelay neural networks [26], also called convolutional neural networks, may also give interesting results.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "(2010) as \u201cparasocial consensus sampling\u201d [5].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "[17] did their evaluation without the constraints defined in subsection 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "(offline) [17] \u2013 \u2013 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "(b) Results with our evaluation method with various margins of error used in other research [10].", "startOffset": 92, "endOffset": 96}], "year": 2017, "abstractText": "Using supporting backchannel (BC) cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory (LSTM) networks which have shown to outperform feed-forward based setups on various tasks. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39.", "creator": "LaTeX with hyperref package"}}}