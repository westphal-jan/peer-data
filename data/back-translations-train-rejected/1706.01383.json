{"id": "1706.01383", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Sparse Stochastic Bandits", "abstract": "In the classical multi-armed bandit problem, d arms are available to the decision maker who pulls them sequentially in order to maximize his cumulative reward. Guarantees can be obtained on a relative quantity called regret, which scales linearly with d (or with sqrt(d) in the minimax sense). We here consider the sparse case of this classical problem in the sense that only a small number of arms, namely s &lt; d, have a positive expected reward. We are able to leverage this additional assumption to provide an algorithm whose regret scales with s instead of d. Moreover, we prove that this algorithm is optimal by providing a matching lower bound - at least for a wide and pertinent range of parameters that we determine - and by evaluating its performance on simulated data.", "histories": [["v1", "Mon, 5 Jun 2017 15:46:52 GMT  (2815kb,D)", "http://arxiv.org/abs/1706.01383v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joon kwon", "vianney perchet", "claire vernade"], "accepted": false, "id": "1706.01383"}, "pdf": {"name": "1706.01383.pdf", "metadata": {"source": "CRF", "title": "Sparse Stochastic Bandits", "authors": ["Joon Kwon", "Vianney Perchet"], "emails": ["joon.kwon@ens-lyon.org", "vianney.perchet@normalesup.org", "claire.vernade@telecom-paristech.fr"], "sections": [{"heading": null, "text": "We are able to use this additional assumption to provide an algorithm whose repentance scales with s instead of d. Furthermore, we prove that this algorithm is optimal by providing a suitable lower limit - at least for a broad and relevant range of parameters that we determine - and by evaluating its performance using simulated data. * J. Kwon was supported by a public grant under the Investissement d'avenir project, the reference ANR-11-LABX-0056-LMH. V. Perchet benefited from the support of the ANR (grant ANR13-JS01-0004-01), the FMJH Gaspard Monge Optimization and Operational Research Program (partially supported by EDF), and the Labex LMH."}, {"heading": "1 Introduction", "text": "We look at the celebrated stochastic multi-armed bandit problem Robbins (1985), where a decision maker sequentially identifies samples of d > 1 processes, also known as arms, aimed at maximizing his cumulative reward. Specifically, we know that these arms are actually defined by their distributions. There are many motivations behind investigating these models, ranging from random clinical trials to maximizing the click rate, portfolio optimization, etc. A algorithm (or policy) anterior observations to the next arm I (t). The performance of a given algorithm is evaluated by its cumulative regret."}, {"heading": "1.1 Contributions", "text": "We introduce the problem of sparse bandits and investigate it by deriving an asymptotic lower limit of regret. We give an analogous result to the sperm limit of Lai and Robbins (1985), and we construct an always available algorithm SPARSEUCB, which uses the optimistic principle of Auer et al. (2002) together with the sparsity information available to the agent to achieve optimum performance, up to constant dates. Specifically, the lower limit we demonstrate distinguishes the possible behavior of each uniformly efficient algorithm according to the value of the sparsity information available to the agent. To correct ideas, we assume that \u00b51 = 1 and for 2 6 i 6 s, \u00b5i = \u00b5, \u2206 i = 1 \u2212 \u00b5. Then we show that the spareness of the problem is highly relevant, so that the regret asymptotically lower limits in T \u2192 + inst Reg (T > s \u00b2 s, max. \u00b2 p, 2.0 p \u00b2 p)."}, {"heading": "2 The Stochastic Sparse Bandits Problem", "text": "We look at the classic stochastic, multi-armed bandit problem, where a decision maker proceeds sequentially from d-N i.i.d. processes ((Xi (t) t > 1) i-d. We become the probability distribution of Xi (t) and E\u03bdi [Xi (t)] = i-i-i-i-i. The decision maker draws an arm I (t) at level t > 1 and receives a reward Xi (t), which is his only observation (specifically, he does not observe Xi (t) for i 6 = I (t). The (expected) cumulative reward of the decision maker after level T > 1 is then T = 1 \u00b5I (t) and his performance is evaluated on the basis of his regret, defined as 3see https: / www.kaggle.com / c / yandex-personalized-web-challenge."}, {"heading": "3 Lower Bound", "text": "This section is dedicated to the detection of a lower limit on the regret of a uniformly efficient algorithm for the thrifty bandit problem. To avoid too heavy expressions, we hold the lower limit ready for problems where the bad arms have a zero expected reward, although the handling of general negative means does not require huge modifications from the given evidence. Our goal is to provide a result that is easily generalizable for any stochastic bandit problem that contains sparing information in the form of a threshold on the expected return of the arms of interest. A generalization of the represented limit can be found in Appendix B.Definition 1. An algorithm is uniformly efficient if for each thrifty bandit problem and all expected regrets (0, 1] the E [Reg (T)]] = o (T\u03b1).We give the limit for Gaussian bandit models with a fixed variance equal to 1 / 4. In this case, a distribution is defined simply by their meaning and the two-back (L)."}, {"heading": "4 Sparse UCB", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 The SPARSEUCB algorithm", "text": "The SPARSEUCB algorithm is formally defined in algorithm 1, but let us first provide an informal description. (The algorithm can be in different phases (designated r, f, and u) depending on whether it has been observed in the past, and its behavior changes radically from one phase to another. (We will now describe the different phases. Round-robin The algorithm begins with a round-robin phase, which corresponds to a round-robin phase, in which each of the d-arms is pulled one after the other, but is handy for reference in the analysis. (Round-robin) The algorithm begins with a round-robin phase, which corresponds to a round-robin phase. (t) Each of the d-arms is drawn once in a row. Then the following sentences are defined: J (t): The following sentences are defined:"}, {"heading": "4.2 Sketch of the proof", "text": "We decompile the event {I (t) = i) of a good arm i = > J (t) becomes in time t > 1 in relation to the various phases of SPARSEUCB: {I (t) = i) = i (t) t Fi (t) t Ui (t) t Vi (t), t > 1, (5) where the various events are defined as follows: \u2022 Ri (t) = i (t) = i, \u03c9 (t) = r} is event of the arm being pulled during a round robin phase; \u2022 Fi (t): = {I (t) = i (t) = i) is the event of the arm i being pulled during a force log phase; \u2022 Ui (t): {I (t) = i (t) = u, 1) is the event of the arm i (t)."}, {"heading": "5 Optimality, ranges of sparsity and constants optimization", "text": "In this section, we will prove that the SPARSEUCB algorithm is optimal for a wide range of parameters up to multiplicative factors. To this end, we will recall the various limits we have reached, up to multiplicative universal constants and additive data-dependent constants."}, {"heading": "5.1 Strong sparsity", "text": "In this case, the lower limit of theorem 1 is rewritten, while SPARSEUCB suffers an expected regret limited by asReg (T) log (T). More importantly, its regret is scaled early with s and is independent of the number of weapons with non-positive averages. The minimax regret of SPARSEUCB is optimal for all of these parameter values."}, {"heading": "5.2 Variants & small improvements", "text": "Of course, the minimax regret of SPARSEUCB has an additional \u221a log (T) term due to the fact that we used UCB as the basic algorithm. In the order we could have replaced UCB with any variant such as UCB-2, improved-UCB, ETC, MOSS... In the same line of thought, the threshold of the force log phase could also have been updated to 2 \u221a log (T / Ni (t)) Ni (t) so that the term \u221a log (T) can be replaced by \u221a log (s), which causes a regret about the scaling inReg (T)."}, {"heading": "6 Experiments", "text": "This section aims at the experimental validation of the theoretical results obtained by us. Empirically, we compare the regrets of UCB and SPARSEUCB for different levels of sparseness: Either we fix d and s and allow the expected yields to vary, or vice versa, and allow s / d to vary. In accordance with the conclusions of section 5, we observe that SPARSEUCB behaves almost optimally for a number of settings over the long term, up to multiplicative constants. We also see that even if SPARSEUCB is not optimal, it is almost always preferable to UCB as soon as a certain sparseness of the problem exists. Without loss of universality, experiments are carried out on problems for which \u00b51 = 0.9 and for 2 6 i 6 s are the same."}, {"heading": "6.1 Varying \u00b5s", "text": "We set d = 15 and s = 7 so that the limit defined in section 5 between weak and strong thrift is reached at \u00b5s = 0.4. We allow these bandit problems to vary in [0,1, 0,7]. We compare the behavior of SPARSEUCB and UCB for these bandit problems and calculate and also show the lower limit of episode 1, i.e. for the smallest class of sparse problems that include our problems - for \u03b5 = \u00b5s. In Figure 3, we present the expected regret, which averages over 100 repetitions for each Monte-Carlo experiment. If the thrift of the problem is not strong, 5Note that the sparse bandit problem in Bernoulli distributions is trivial. However, if s = 0,7 occurs, UCB has a lower regret than SPARSEUCB for a long time, but the asymptotic behavior of the latter tends to show that the UCB will deteriorate in the long term."}, {"heading": "6.2 Influence of the number of arms", "text": "We now fix d = 15, p = 0.9 and p = 0.3 and allow the number of effective weapons to vary in {2, 6, 12}. Note that, given the parameters set, the regime of weak scarcity defined in the previous paragraph only applies if s > 10. In Figure 4, we present the expected regrets of an average of over 100 Monte Carlo repetitions. If s = 12, we are clearly in the weak scarcity regime and there is no real improvement by SPARSEUCB compared to the usual UCB policy. On the contrary, if s gets smaller, SPARSEUCB is getting closer and closer to optimum."}, {"heading": "7 Conclusions and Open Questions", "text": "We introduced a new variant of the famous stochastic, multi-armed bandit problem, which contains additional sparse information about the expected return on the weapons. We characterized the range of parameters that lead to interesting sparse problems and gave a lower limit on the regret that the scales in O (s log (T) are scaled in the area of strong thrift. We offer SPARSEUCB, which in the sparse bandit situation is a good alternative to the classic UCB, as it offers both good theoretical guarantees and good empirical performance. However, in the experiments we noticed that with parameters in the area of weak thrift, one would rather switch to the classic UCB policy, since the price of concentrating on the best weapons paid by SPARSEUCB is too high compared to the resulting improvement of regret. Furthermore, in many real-world applications it seems that the learner often knows the existence of s without knowing its exact value, a new problem, and that a stoical one is used."}, {"heading": "A End of Proof of Lower Bound", "text": "For a Gaussian problem, an asymptotic lower limit on regret is given by solving the following linear optimization problem: f (\u00b5) > inf c 0 ci 0 (6) s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s s \u00b2 s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s \u00b2 s s) s s s s \u00b2 s s s s s \u00b2 s s s s \u00b2 s s s s s \u00b2 s s s s \u00b2 s s s \u00b2 s s s \u00b2 s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s \u00b2 s \u00b2 s s s \u00b2 s s s \u00b2 s \u00b2 s s s \u00b2 s s \u00b2 s s s s s \u00b2 s \u00b2 s s \u00b2 s s s \u00b2 s s s \u00b2 s s s s \u00b2 s s s s s \u00b2 s s \u00b2 s s s s \u00b2 s s \u00b2 s s s \u00b2 s s s s s \u00b2 s s s s s \u00b2 s s s s s s \u00b2 s s s s s \u00b2 s s s s s s \u00b2 s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s"}, {"heading": "B Generalization of Theorem 1", "text": "The lower limit of Theorem 1 can be generalized to a broader class of problems, including sparsity information. We assume that we know \u03b5 > 0 in such a way as to arrive at a result that applies to both our problem and similar problems, such as stochastic Thresholded Bandit 6, for which one would assume that there is a threshold at which stakeholders have an expected return of at least 1%. In this case, changing the variable Thresholded Bandit 6 provides a lower limit on the regret of a uniformly efficient algorithm for this problem. We have chosen to introduce this wilder class of problems in order to provide a generic result and the associated method of proof, but we specify the specific lower limit for our own problem in Theorem 1."}, {"heading": "C Analysis of the SPARSEUCB algorithm", "text": "In this section, we provide the detailed statements and proofs about the upper limit guaranteed by the SPARSEUCB algorithms. (Theorem 4) (T > 1) The SPARSEUCB algorithm guarantees: E [Reg (T)] 6 16 Log (T) 6 16 Log (T) (T) 6 (s) 0 (1) 0 (1) (1) (0) (1) (1) (1) (1) (1) (1) (n) (n) (1) (n) (n) (1) (n) (1) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n), n (n) (n) (n) (n) (n) (n) (n), n (n) (n) (n) (n) (n) (n) (n), n (n) (n (n) (n) (n) (n) (n), n (n (n) (n) (n) (n) (n), n (n (n) (n) (n) (n), n (n), n (n (n) (n) (n) (n) (n), n (n (n), n (n), n (n (n) (n), n (n) (n (n) (n) (n), n (n) (n (n), n (n (n), n (n (n) (n), n (n (n), n (n (n (n) (n), n (n (n) (n (n), n (n (n), n (n (n), n (n), n (n (n), n (n (n), n (n) (n (n) (n), n (n (n), n (n (n), n (n (n (n) (n), n (n), n (n (n), n (n"}], "references": [{"title": "Online-to-confidence-set conversions and application to sparse stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari"], "venue": "In AISTATS,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2012}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Bounded regret in stochastic multi-armed bandits", "author": ["S\u00e9bastien Bubeck", "Vianney Perchet", "Philippe Rigollet"], "venue": "In COLT,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Optimal adaptive policies for sequential allocation problems", "author": ["Apostolos N Burnetas", "Micha\u00ebl N Katehakis"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Burnetas and Katehakis.,? \\Q1996\\E", "shortCiteRegEx": "Burnetas and Katehakis.", "year": 1996}, {"title": "Bandit theory meets compressed sensing for high dimensional stochastic linear bandit", "author": ["Alexandra Carpentier", "R\u00e9mi Munos"], "venue": "In AISTATS,", "citeRegEx": "Carpentier and Munos,? \\Q2012\\E", "shortCiteRegEx": "Carpentier and Munos", "year": 2012}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Combinatorial bandits revisited", "author": ["Richard Combes", "Alexandre Proutiere"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Combes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Combes et al\\.", "year": 2015}, {"title": "Linear programming and extensions", "author": ["George Dantzig"], "venue": "Princeton university press,", "citeRegEx": "Dantzig.,? \\Q2016\\E", "shortCiteRegEx": "Dantzig.", "year": 2016}, {"title": "Explore first, exploit next: The true shape of regret in bandit problems", "author": ["Aur\u00e9lien Garivier", "Pierre M\u00e9nard", "Gilles Stoltz"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Garivier et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Garivier et al\\.", "year": 2017}, {"title": "Sparsity regret bounds for individual sequences in online linear regression", "author": ["S\u00e9bastien Gerchinovitz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gerchinovitz.,? \\Q2013\\E", "shortCiteRegEx": "Gerchinovitz.", "year": 2013}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled markov chains", "author": ["Todd L Graves", "Tze Leung Lai"], "venue": "SIAM journal on control and optimization,", "citeRegEx": "Graves and Lai.,? \\Q1997\\E", "shortCiteRegEx": "Graves and Lai.", "year": 1997}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["\u00c9milie Kaufmann", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kaufmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2015}, {"title": "Gains and losses are fundamentally different in regret minimization: the sparse case", "author": ["Joon Kwon", "Vianney Perchet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kwon and Perchet.,? \\Q2016\\E", "shortCiteRegEx": "Kwon and Perchet.", "year": 2016}, {"title": "Multiple-play bandits in the position-based model", "author": ["Paul Lagr\u00e9e", "Claire Vernade", "Olivier Capp\u00e9"], "venue": "arXiv preprint arXiv:1606.02448,", "citeRegEx": "Lagr\u00e9e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lagr\u00e9e et al\\.", "year": 2016}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Linear multi-resource allocation with semi-bandit feedback", "author": ["Tor Lattimore", "Koby Crammer", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lattimore et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2015}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "In Herbert Robbins Selected Papers,", "citeRegEx": "Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Robbins.", "year": 1985}], "referenceMentions": [{"referenceID": 18, "context": "1 Introduction We consider the celebrated stochastic multi-armed bandit problem Robbins (1985), where a decision maker sequentially samples from d > 1 processes, also called arms, aiming at maximizing its cumulative reward.", "startOffset": 80, "endOffset": 95}, {"referenceID": 13, "context": "Converse statements have also been proved, first by Lai and Robbins (1985) and then by Burnetas and Katehakis (1996): Any consistent policy (i.", "startOffset": 52, "endOffset": 75}, {"referenceID": 4, "context": "Converse statements have also been proved, first by Lai and Robbins (1985) and then by Burnetas and Katehakis (1996): Any consistent policy (i.", "startOffset": 87, "endOffset": 117}, {"referenceID": 4, "context": "Converse statements have also been proved, first by Lai and Robbins (1985) and then by Burnetas and Katehakis (1996): Any consistent policy (i.e. whose regret is always less than T\u03b1 for all \u03b1 > 0) always have a regret larger than \u2211d i=1 log(T ) \u2206i (again, up to some constants). When T is fixed and the parameters \u03bci are chosen to maximize regret, the distribution-independent bounds are of order \u221a dT as first shown in Cesa-Bianchi and Lugosi (2006). The main drawback of those results is that the regret scales linearly with the number of arms d,or with \u221a d in the minimax analysis.", "startOffset": 87, "endOffset": 451}, {"referenceID": 1, "context": "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means.", "startOffset": 111, "endOffset": 142}, {"referenceID": 1, "context": "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means. There have been some works regarding sparsity assumptions in bandit problems. In the full information setting, some of them focus on sparse reward vectors, i.e., at most s components of (X1(t), . . . , Xd(t)) are positive (see for instance Langford et al. (2009); Kwon and Perchet (2016)).", "startOffset": 111, "endOffset": 484}, {"referenceID": 1, "context": "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means. There have been some works regarding sparsity assumptions in bandit problems. In the full information setting, some of them focus on sparse reward vectors, i.e., at most s components of (X1(t), . . . , Xd(t)) are positive (see for instance Langford et al. (2009); Kwon and Perchet (2016)).", "startOffset": 111, "endOffset": 509}, {"referenceID": 1, "context": "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means. There have been some works regarding sparsity assumptions in bandit problems. In the full information setting, some of them focus on sparse reward vectors, i.e., at most s components of (X1(t), . . . , Xd(t)) are positive (see for instance Langford et al. (2009); Kwon and Perchet (2016)). Another considered problem is the one of sparse linear bandits Carpentier et al. (2012); Abbasi-Yadkori et al.", "startOffset": 111, "endOffset": 599}, {"referenceID": 0, "context": "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al.", "startOffset": 8, "endOffset": 37}, {"referenceID": 0, "context": "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al.", "startOffset": 8, "endOffset": 58}, {"referenceID": 0, "context": "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al. (2015) in which the underlying unknown vector of parameter is assumed to be sparse \u2013 that is with a constraint on its L1 norm \u2013 or even spiky in the crude and very specific case where s = 1 Bubeck et al.", "startOffset": 8, "endOffset": 83}, {"referenceID": 0, "context": "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al. (2015) in which the underlying unknown vector of parameter is assumed to be sparse \u2013 that is with a constraint on its L1 norm \u2013 or even spiky in the crude and very specific case where s = 1 Bubeck et al. (2013). However, none of the previously cited work tackles the following concrete problem.", "startOffset": 8, "endOffset": 287}, {"referenceID": 14, "context": "We give an analogous result to the seminal bound of Lai and Robbins (1985), and we construct an anytime algorithm SPARSEUCB that uses the optimistic principle of Auer et al.", "startOffset": 52, "endOffset": 75}, {"referenceID": 1, "context": "We give an analogous result to the seminal bound of Lai and Robbins (1985), and we construct an anytime algorithm SPARSEUCB that uses the optimistic principle of Auer et al. (2002) together with the sparsity information available in order to reach optimal performance, up to constant terms.", "startOffset": 162, "endOffset": 181}, {"referenceID": 11, "context": "The proof relies on changes of measure arguments originating from Graves and Lai (1997). First, consider the set of changes of distributions that modify the best arm without changing the marginal of the best arm in the original sparse bandit problem: B(\u03bc) = { \u03bc\u2032 \u2208 S(d, s) \u2223\u2223\u2223\u2223\u03bc\u20321 = \u03bc1 and max i\u2208[d] \u03bci < max i\u2208[d] \u03bci } .", "startOffset": 66, "endOffset": 88}, {"referenceID": 12, "context": "17 in (Kaufmann et al., 2015) or Proposition 3 in (Lagr\u00e9e et al.", "startOffset": 6, "endOffset": 29}, {"referenceID": 14, "context": ", 2015) or Proposition 3 in (Lagr\u00e9e et al., 2016) can be stated in our case as follows: For all changes of measure \u03bc\u2032 \u2208 B(\u03bc),", "startOffset": 28, "endOffset": 49}, {"referenceID": 8, "context": "(3) Details on this type of informational lower bounds can be found in Garivier et al. (2017.To appear.) and references therein. Now, following general ideas from Graves and Lai (1997) and lower bound techniques from Lagr\u00e9e et al.", "startOffset": 71, "endOffset": 185}, {"referenceID": 8, "context": "(3) Details on this type of informational lower bounds can be found in Garivier et al. (2017.To appear.) and references therein. Now, following general ideas from Graves and Lai (1997) and lower bound techniques from Lagr\u00e9e et al. (2016) and Combes et al.", "startOffset": 71, "endOffset": 238}, {"referenceID": 7, "context": "(2016) and Combes et al. (2015), we may give a variational form of the lower bound on the regret satisfying the above constraint.", "startOffset": 11, "endOffset": 32}, {"referenceID": 8, "context": "This is a linear optimization problem under inequality constraints so there exist algorithmic methods such as the celebrated Simplex algorithm Dantzig (2016) to compute a numeric solution of it.", "startOffset": 143, "endOffset": 158}, {"referenceID": 1, "context": "The proof basically follows the steps of the classic UCB analysis by Auer et al. (2002). 2 Lemma 4.", "startOffset": 69, "endOffset": 88}, {"referenceID": 3, "context": "Another way to slightly improve the guarantees of the algorithm is to change the round-robin phases into sampling phases in which arms are not selected uniformly at random but with probability depending on the past performances of the different arms, as in Bubeck et al. (2013). Unfortunately, this does not improve the leading term (in T ) of the regret, but merely the terms uniformly bounded (in T ).", "startOffset": 257, "endOffset": 278}], "year": 2017, "abstractText": "In the classical multi-armed bandit problem, d arms are available to the decision maker who pulls them sequentially in order to maximize his cumulative reward. Guarantees can be obtained on a relative quantity called regret, which scales linearly with d (or with \u221a d in the minimax sense). We here consider the sparse case of this classical problem in the sense that only a small number of arms, namely s < d, have a positive expected reward. We are able to leverage this additional assumption to provide an algorithm whose regret scales with s instead of d. Moreover, we prove that this algorithm is optimal by providing a matching lower bound \u2013 at least for a wide and pertinent range of parameters that we determine \u2013 and by evaluating its performance on simulated data. *J. Kwon was supported by a public grant as part of the Investissement d\u2019avenir project, reference ANR-11-LABX-0056-LMH. V. Perchet has benefitted from the support of the ANR (grant ANR13-JS01-0004-01), of the FMJH Program Gaspard Monge in optimization and operations research (supported in part by EDF) and from the Labex LMH. C. Vernade was also partially supported by the Machine Learning for Big Data Chair at T\u00e9l\u00e9com ParisTech. Accepted for presentation at Conference on Learning Theory (COLT) 2017 1 ar X iv :1 70 6. 01 38 3v 1 [ cs .L G ] 5 J un 2 01 7", "creator": "LaTeX with hyperref package"}}}