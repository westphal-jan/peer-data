{"id": "1706.01394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Multi-Observation Elicitation", "abstract": "We study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously. To our knowledge, such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly. As compared to traditional loss functions that take only a single data point, these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in empirical risk minimization, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between dimensionality and number of observations, give some geometric characterizations and intuition for relating loss functions and the properties that they elicit, and discuss some implications for both elicitation and machine-learning contexts.", "histories": [["v1", "Mon, 5 Jun 2017 16:07:26 GMT  (2057kb,D)", "http://arxiv.org/abs/1706.01394v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sebastian casalaina-martin", "rafael frongillo", "tom morgan", "bo waggoner"], "accepted": false, "id": "1706.01394"}, "pdf": {"name": "1706.01394.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sebastian Casalaina-Martin", "Rafael Frongillo", "Bo Waggoner"], "emails": ["CASA@MATH.COLORADO.EDU", "RAF@COLORADO.EDU", "TDMORGAN@SEAS.HARVARD.EDU", "BWAG@SEAS.UPENN.EDU"], "sections": [{"heading": "1. Introduction", "text": "It is not as if the choice of the loss function used in ERM could have a great influence on the chosen model of how to select that loss. A growing portion of the work in the declaration of ownership tries to answer this question by considering a loss function as an \"incentive function\" that predicts a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is known that quared loss elicits the mean, and hence least-squares regression finds the best fit for the conditional means of data.1A natural question, which is still in the vector-rated case: For which conditional statistics are there functions they elicits."}, {"heading": "1.1. Related work", "text": "Our work is partly inspired by Frongillo et al. (2015), which suggests a way to gain the reliability (inversion of variance) of an agent's estimate of the inclination of a coin by simply turning it over twice. In our terminology, this results from the fact that variance (1, 2) is attractive. Multi-observation losses were introduced earlier to learn embedding (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), although an explicit property / statistic is never discussed."}, {"heading": "2. Preliminaries", "text": "We are interested in a Y space that constitutes a finite series of objects, unless it is defined differently from the observations y in our context. We are drawn by P'amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160"}, {"heading": "2.1. Illustrative example", "text": "Let us remember our observation that the variance is not (1, 1) irritable and the \"traditional\" solution is to be (2, 1) irritable: to minimize a loss function over two dimensions (say, first and second moments) and to map the result over a link function to the variance. Instead, we observed that it is possible to use (1, 2) irritability: to minimize a loss function that requires two observations over a single scalar, the variance itself. Can this compromise be more extreme? Are there cases where additional observations drastically reduce the report complexity? Consider the 2 standard of a distribution: area losses (p) = area losses (y) 2. In Section 5.2, we show that the report complexity (p) 2 has report complexity | Y | \u2212 1 (where Y is the result) is fixed for 1 observation - no single observation loss function, better than the overall solution for the distribution."}, {"heading": "3. Geometric Fundamentals", "text": "The basic (still) -elicitability of the property states that elicitable properties must be 'located' on one level (Lambert et al., 2008). In fact, this is used to prove that the variance is not (1,1) -elicitable; but the variance is elicitable with two observations. Here, the geometry is not \"broken,\" but merely lives in a higher dimensional space. Since the argument about triggering a property is', a property ', it is often useful to draw the property with a single random draw from a distribution on m-tuples of outcomes.Remark 6 Since P is isomorphic extended to Pm, a property is', which is directly elicitable with m observations, if and only if the induced property is' m: Pm \u2192 R is directly elicitable with 1 observation."}, {"heading": "3.1. Finite Properties", "text": "This corresponds to a \"Multiplechoice Question\" (Lambert and Shoham, 2009).In this section, we must allow \"P \u21d2 R\" to be a fixed function that may assign several possible correct reports to a single distribution; this is necessary for \"borderline cases,\" such as the type of uniform distribution on a finite set. (Likewise, we cannot demand identifiability.) We have a finite set of results Y, the distributions under consideration are all P = \u2206 Y, and \"p\" does not need to be empty.We are interested in understanding what finite properties can be induced by m observations. Previously, this question was investigated in the case of an observation by Lambert (2011), which characterized triggering properties by the shape of their limits: they are intersections of voronoi diagrams in voronoi diagrams in voronoi diagrams in voronoi diagrams Y."}, {"heading": "4. Lower Bounds via Geometry", "text": "In this section, we will discuss lower limits of triggering complexity. For technical reasons, we need P here as a C \u00b0 submultiplicity of \u0394Y with corners. Our lower limits generally also require \u0439 to be a C \u00b0 function, in this case we call it a C \u00b0 property. We will begin in the first subsection by remembering the structure of the plane sets of identifiable properties, and then introduce a technique to obtain the triggering complexity from this lower limit through differential geometry. In the next subsection, we will focus on polynomic properties and explain some results that algebraic geometry uses to obtain sharp boundaries."}, {"heading": "4.1. Preliminaries on identifiable properties", "text": "We start by remembering a general method introduced in Frongillo and Kash (2015c) to show the lower limits at the level of evaluation. (1) We start by remembering a general method introduced in Frongillo and Kash (2015c). (1) We start by remembering a general method that in Frongillo and Kash (2015c) reverts to the lower level of evaluation. (1) We assume that the lower level of evaluation (1) can be contained at a certain level of evaluation. (1) We assume that the lower level of evaluation (1) cannot be at the level of evaluation. (1) We assume that the lower level of evaluation and evaluation (1) cannot be at the level of evaluation."}, {"heading": "4.2. Polynomial properties and lower bounds using algebraic geometry", "text": "The motivation for these lower limits is the intuition that, in general, a polynomial property should not be the zero property of a polynomial with degree K. Of course, this statement may fail in special cases (e.g., Example 1 below). Indeed, there are some subtleties regarding the zeros of a polynomial with degree K that are considered in Appendix C. However, for a general polynomial property this expectation applies (see Note C.4 for a precise definition of the general public), and in Appendix C we provide some elementary techniques to confirm this expectation (see Corollary C.3)."}, {"heading": "5. Examples and Elicitation Frontiers", "text": "We are now combining our lower boundary complexity with upper boundaries to make progress in determining the trigger boundaries of some potential properties of interest. See Figure 4 for a representation of some of the described trigger boundaries. We start with some general, simple but versatile upper boundaries. Lemma 14 For all 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m, let fij: Y \u2192 R be a arbitrary function such that Ep [fij (Y)] actually exists for all p \u00b2 P. Then it (p) = 1 \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s \u00b2 s = s \u00b2 s = s = s = s = s)."}, {"heading": "5.1. Ratios of expectations: index of dispersion and Sharpe ratio", "text": "The dispersion index of a random variable Y with a positive mean is defined as Var (Y) / E [Y] (Cox and Lewis, 1966).The Sharpe ratio of a random variable Y, which is a generally applied measure of the risk-adjusted return on an investment, is similarly defined as E [Y] / \u221a Var (Y) (Sharpe, 1966).Both the dispersion index and the square of the Sharpe ratio are (1, 2) -solvable by the discussion above: Var (Y) = Ep [12 (Y1 \u2212 Y2) 2]], Ep [Y] = Ep [Y1] and Ep [Y] 2 = Ep [Y1Y2], so that any ratio of these terms (1, 2) -solvable. (The link function for the Sharpe ratio is thus the square root.) For example, the dispersion index Sharpe-Ep [Y] is so soluble that each of these terms (1, 2) -Yp is [1] solvable."}, {"heading": "5.2. Norms of distributions", "text": "As we have already discussed, the 2 standard (1, 2) is solvable. In general, k is the k standard (1, k) solvable with the following loss function '(r, y1,.., yk) = (r \u2212 1 {y1 =... = yk}) 2. (This case is also derived from term 15.) This is closely linked to the observation complexity, as we have shown in episode 13 that the k standard is not solvable (1, k \u2212 1). As it turns out, the reporting complexity of the k standard | Y | \u2212 1, which means that it is as difficult to elicit with observation as the entire distribution. This follows from Theorem 2 of Frongillo and Kash (2015c), especially from section 4.2, since the k standard is a convex function of p and one that requires additional algebraic tools."}, {"heading": "5.3. Central Moments", "text": "The n th central moment \u00b5n of a random variable Y is defined as \u00b5n = E [(Y \u2212 E [Y]) factor (n] n = 0 (\u2212 1) i (n i) E [Y] i \u00b7 E [Y] n \u2212 i], (3) what we see is (n, 1) -solvable by simply extracting E [Y] for all i [1,..., n} and then combining the results. As we will show, \u00b5n is also (1, n) -soluble, and furthermore we can achieve other dimensional observation compromises in between, such as (b) nc + 1, d \u221a ne). The key idea is to divide the binomical sum (3) into k subsums and extract the highest power of E [Y] from any point, so that the jest subtotal of E [Y] can be written."}, {"heading": "6. Multi-Observation Regression", "text": "For the first time in a long time, we have been able to provide a complete range of services for the green-egg-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-"}, {"heading": "6.1. Simulation", "text": "Here we describe some simulations performed as proof of the concept of multi-observation regression. Our data points are of the form (x, y) and the form (R) in which x is evenly drawn from the interval [0, 1]. Since x, y = a sin (4\u03c0x) + Z in which a is a constant and Z \u0445 N (0, 1) is drawn independently for each sample, we want to learn Var (Y | X). Our multi-observation loss function is \"(f (x), y1, y2) = (f (x) \u2212 12 (y1 \u2212 y2) 2) 2. We approach (x, y1, y2) samples by sorting the (xi, yi) pairs by xi and making samples of the form (12 (xi + xi + xi + 1), yi + 1), yi, yi + 1). We compare with the single observation approach in which we combine E [Y | X] and [Y | X] to estimate them."}, {"heading": "7. Conclusion and Future Work", "text": "An immediate abundance of directions is testing the upper and lower boundaries at the boundaries of origin for different properties. In particular, our lower boundaries here focus on techniques for lower observation complexity (the (1, m) case), thereby opening approaches to lower boundaries of (d, m) complexity for d \u2265 2. Another direction is to formalize learning guarantees for multi-observation regression under appropriate assumptions about slowly changing conditional distributions."}, {"heading": "Acknowledgments", "text": "Sebastian Casalaina Martin was partially funded by NSA funding H98230-16-1-0053, Tom Morgan was partially funded by NSF funding CCF-1320231 and CNS-1228598, and Bo Waggoner is supported by the Warren Center for Network and Data Sciences at the University of Pennsylvania."}, {"heading": "Appendix A. Overlapping Level Sets: Proof of Theorem 7", "text": "Theorem 7 states that a property cannot be solved if there is a convex combination of one of its levels in the m product space, which is equivalent to a convex combination of another of its levels in the m product space. In order to think about these levels, we need the following q theorems. Theorem A.1 (Theorem 3.5, Frongillo and Kash (2014) The property: P \u00b2 \u2192 R (where P \u00b2 Y \u00b2) is thus directly solvable by the loss function, \"if and only if there is a convex G: conv (P \u00b2) \u2192 R \u00b2 theorem (P \u00b2) R \u00b2 value, and some D \u2212 r \u00b2 values in the loss function,\" if and only if there is a convex G \u00b2 property, so that for all r \u00b2 R and y \u00b2 Y \u00b2 values there are \"(r \u00b2 values)."}, {"heading": "Appendix B. Regression", "text": "In this section, we give an example of how to overcome this obstacle."}, {"heading": "Appendix C. Zero sets of Polynomials over the Real Numbers", "text": "Consider a polynomial f (x1,. xn) in the set R [x1,., xn] of polynomials in n variables with real coefficients. The zero position of f (x1,.,., xn) is by definition the instance f (x1,., xn): = {x1,.,., xn) is irreducible if it cannot be written as a product of two polynomials in R. [x1,., xn] of the non-constant polynomials f (x1,.,.,.,., xn)."}, {"heading": "Appendix D. The Real Nullstellenstatz for Principal Ideals and Open Sets", "text": "The main purpose of this paragraph is to prove the following theorems: 1. It is the generalization of the known real zero position for the principled ideals (e.g., (1), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2)"}], "references": [{"title": "On consistent surrogate risk minimization and property elicitation", "author": ["Arpit Agarwal", "Shivani Agarwal"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Agarwal and Agarwal.,? \\Q2015\\E", "shortCiteRegEx": "Agarwal and Agarwal.", "year": 2015}, {"title": "Introduction to commutative algebra", "author": ["M.F. Atiyah", "I.G. Macdonald"], "venue": null, "citeRegEx": "Atiyah and Macdonald.,? \\Q1969\\E", "shortCiteRegEx": "Atiyah and Macdonald.", "year": 1969}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Robust optimization\u2013a comprehensive survey", "author": ["Hans-Georg Beyer", "Bernhard Sendhoff"], "venue": "Computer methods in applied mechanics and engineering,", "citeRegEx": "Beyer and Sendhoff.,? \\Q2007\\E", "shortCiteRegEx": "Beyer and Sendhoff.", "year": 2007}, {"title": "Real algebraic geometry, volume 36 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)", "author": ["Jacek Bochnak", "Michel Coste", "Marie-Fran\u00e7oise Roy"], "venue": "ISBN 3-540-64663-9", "citeRegEx": "Bochnak et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bochnak et al\\.", "year": 1998}, {"title": "Mathematical analysis: An introduction", "author": ["Andrew Browder"], "venue": "Undergraduate Texts in Mathematics. Springer-Verlag, New York,", "citeRegEx": "Browder.,? \\Q1996\\E", "shortCiteRegEx": "Browder.", "year": 1996}, {"title": "Ideals, varieties, and algorithms", "author": ["David A. Cox", "John Little", "Donal O\u2019Shea"], "venue": "Undergraduate Texts in Mathematics. Springer, Cham, fourth edition,", "citeRegEx": "Cox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cox et al\\.", "year": 2015}, {"title": "The statistical analysis of series of events", "author": ["David R Cox", "Peter AW Lewis"], "venue": "Monographs on Applied Probability and Statistics,", "citeRegEx": "Cox and Lewis.,? \\Q1966\\E", "shortCiteRegEx": "Cox and Lewis.", "year": 1966}, {"title": "The relation between the number of species and the number of individuals in a random sample of an animal population", "author": ["Ronald A. Fisher", "A. Steven Corbet", "Carrington B. Williams"], "venue": "The Journal of Animal Ecology,", "citeRegEx": "Fisher et al\\.,? \\Q1943\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1943}, {"title": "General truthfulness characterizations via convex analysis", "author": ["Rafael Frongillo", "Ian Kash"], "venue": "In Web and Internet Economics,", "citeRegEx": "Frongillo and Kash.,? \\Q2014\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2014}, {"title": "Vector-Valued Property Elicitation", "author": ["Rafael Frongillo", "Ian Kash"], "venue": "In Proceedings of the 28th Conference on Learning Theory, pages", "citeRegEx": "Frongillo and Kash.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2015}, {"title": "On Elicitation Complexity and Conditional Elicitation", "author": ["Rafael Frongillo", "Ian A. Kash"], "venue": "arXiv preprint arXiv:1506.07212,", "citeRegEx": "Frongillo and Kash.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2015}, {"title": "On Elicitation Complexity", "author": ["Rafael Frongillo", "Ian A. Kash"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Frongillo and Kash.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2015}, {"title": "Elicitation for Aggregation", "author": ["Rafael M. Frongillo", "Yiling Chen", "Ian A. Kash"], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Frongillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo et al\\.", "year": 2015}, {"title": "Intersection theory, volume 2 of Ergebnisse der Mathematik und ihrer Grenzgebiete", "author": ["William Fulton"], "venue": null, "citeRegEx": "Fulton.,? \\Q1998\\E", "shortCiteRegEx": "Fulton.", "year": 1998}, {"title": "Making and Evaluating Point Forecasts", "author": ["T. Gneiting"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting.,? \\Q2011\\E", "shortCiteRegEx": "Gneiting.", "year": 2011}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Eliciting truthful answers to multiple-choice questions", "author": ["Nicolas S. Lambert", "Yoav Shoham"], "venue": "In Proceedings of the 10th ACM Conference on Electronic Commerce,", "citeRegEx": "Lambert and Shoham.,? \\Q2009\\E", "shortCiteRegEx": "Lambert and Shoham.", "year": 2009}, {"title": "Eliciting properties of probability distributions", "author": ["Nicolas S. Lambert", "David M. Pennock", "Yoav Shoham"], "venue": "In Proceedings of the 9th ACM Conference on Electronic Commerce,", "citeRegEx": "Lambert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lambert et al\\.", "year": 2008}, {"title": "Elicitation and Evaluation of Statistical Forecasts", "author": ["N.S. Lambert"], "venue": null, "citeRegEx": "Lambert.,? \\Q2011\\E", "shortCiteRegEx": "Lambert.", "year": 2011}, {"title": "Providing Incentives for Better Cost Forecasting", "author": ["Kent Harold Osband"], "venue": "University of California, Berkeley,", "citeRegEx": "Osband.,? \\Q1985\\E", "shortCiteRegEx": "Osband.", "year": 1985}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Mutual fund performance", "author": ["William F Sharpe"], "venue": "Journal of Business,", "citeRegEx": "Sharpe.,? \\Q1966\\E", "shortCiteRegEx": "Sharpe.", "year": 1966}, {"title": "Elicitation and Identification of Properties", "author": ["Ingo Steinwart", "Chlo\u00e9 Pasin", "Robert Williamson", "Siyu Zhang"], "venue": "In Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2014}, {"title": "Learning deep embeddings with histogram loss", "author": ["Evgeniya Ustinova", "Victor Lempitsky"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ustinova and Lempitsky.,? \\Q2016\\E", "shortCiteRegEx": "Ustinova and Lempitsky.", "year": 2016}, {"title": "\u2206Y \u2032) is directly elicitable by the loss function ` if and only if there exists some convex G : conv(P \u2032) \u2192 R\u0304 with G(P \u2032) \u2286 R, some D \u2286 \u03b4G, and some bijection \u03c6 : \u0393(P \u2032) \u2192 D with \u0393(p) = \u03c6\u22121(D \u2229 \u03b4Gp)", "author": ["Frongillo", "Kash"], "venue": null, "citeRegEx": "Frongillo and Kash,? \\Q2014\\E", "shortCiteRegEx": "Frongillo and Kash", "year": 2014}, {"title": "Lem. 4.5.2) states the following: Let B \u2286 Kn be an open ball (including the case where B = Kn) and let U1 and U2 be two disjoint nonempty semialgebraic open subsets of B", "author": ["Bochnak et al", "Cor"], "venue": "(Bochnak et al.,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 18, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 15, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 23, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 0, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 18, "context": "The question of how many such auxiliary statistics are required gives rise to the concept of elicitation complexity; since the variance cannot be elicited with one but can with two, we say it is 2-elicitable (Lambert et al., 2008; Frongillo and Kash, 2015c).", "startOffset": 208, "endOffset": 257}, {"referenceID": 3, "context": "observations are readily obtained include: active learning, uncertainty quantification & robust engineering design (Beyer and Sendhoff, 2007), and replication of scientific experiments.", "startOffset": 115, "endOffset": 141}, {"referenceID": 16, "context": "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.", "startOffset": 77, "endOffset": 151}, {"referenceID": 21, "context": "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.", "startOffset": 77, "endOffset": 151}, {"referenceID": 24, "context": "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.", "startOffset": 77, "endOffset": 151}, {"referenceID": 13, "context": "Related work Our work is inspired in part by Frongillo et al. (2015) which proposes a way to elicit the confidence (inverse of variance) of an agent\u2019s estimate of the bias of a coin by simply flipping it twice.", "startOffset": 45, "endOffset": 69}, {"referenceID": 14, "context": "Lambert et al. (2008); Steinwart et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Lambert et al. (2008); Steinwart et al. (2014)).", "startOffset": 0, "endOffset": 47}, {"referenceID": 9, "context": "Following Frongillo and Kash (2015a), we will often assume that properties are identifiable.", "startOffset": 10, "endOffset": 37}, {"referenceID": 14, "context": "The question of elicitation complexity, studied by Lambert et al. (2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest \u0393 via some elicitable \u0393\u0302 : P \u2192 Rd; one hopes that d is much smaller than |Y|.", "startOffset": 51, "endOffset": 73}, {"referenceID": 9, "context": "(2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest \u0393 via some elicitable \u0393\u0302 : P \u2192 Rd; one hopes that d is much smaller than |Y|.", "startOffset": 11, "endOffset": 38}, {"referenceID": 18, "context": "Geometric Fundamentals The most basic (yet powerful) lower bound in property elicitation says that elicitable properties\u2019 level sets must be convex sets (Lambert et al., 2008).", "startOffset": 153, "endOffset": 175}, {"referenceID": 9, "context": "A tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some \u201cextension\u201d of that property on the convex hull of that set.", "startOffset": 43, "endOffset": 69}, {"referenceID": 9, "context": "A tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some \u201cextension\u201d of that property on the convex hull of that set. So while the higher-dimensional approach is helpful, it does not preclude reasoning about the space Pm as a manifold inside \u2206Ym . Most significantly, Pm is not a convex space, which makes lower bounds on elicitation complexity nontrivial as well. However, the result of Frongillo and Kash (2014) shows that it suffices to provide lower bounds for elicitation on the convex hull of Pm, which we will denote conv(Pm).", "startOffset": 43, "endOffset": 538}, {"referenceID": 17, "context": "This corresponds to a \u201cmultiplechoice question\u201d (Lambert and Shoham, 2009).", "startOffset": 48, "endOffset": 74}, {"referenceID": 17, "context": "This corresponds to a \u201cmultiplechoice question\u201d (Lambert and Shoham, 2009). In this section, we must allow \u0393 : P \u21d2 R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for \u201cboundary\u201d cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = \u2206Y , and \u0393(p) must be nonempty. We are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex \u2206Y .", "startOffset": 49, "endOffset": 669}, {"referenceID": 17, "context": "This corresponds to a \u201cmultiplechoice question\u201d (Lambert and Shoham, 2009). In this section, we must allow \u0393 : P \u21d2 R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for \u201cboundary\u201d cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = \u2206Y , and \u0393(p) must be nonempty. We are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex \u2206Y . In our setting, a Voronoi diagram is specified by a finite set of points {xr : r \u2208 R} \u2286 RY m , with each cell Tr = {x : \u2016x \u2212 xr\u2016 \u2264 \u2016x \u2212 xr\u2032\u2016\u2200r \u2208 R} consisting of those points in RYm closest in Euclidean distance to xr. Using the geometric constructions above, we can simply apply the main result of Lambert (2011) to finite properties in the m-product space; the result is a characterization of elicitable finite properties with m observations.", "startOffset": 49, "endOffset": 1131}, {"referenceID": 9, "context": "Preliminaries on identifiable properties We start by recalling a general method, introduced in Frongillo and Kash (2015c), for showing lower bounds on elicitation complexity: Given a property \u0393, if one can show that no level set from any \u0393\u0302, which is m-identifiable and directly elicitable with m observations, can be contained in a particular level set of \u0393, then \u0393 cannot be (d,m)-elicitable.", "startOffset": 95, "endOffset": 122}, {"referenceID": 9, "context": "This method was used successfully in Frongillo and Kash (2015c) to show lower bounds on the report complexity (d) of", "startOffset": 37, "endOffset": 64}, {"referenceID": 5, "context": ", Browder (1996), Theorem 6.", "startOffset": 2, "endOffset": 17}, {"referenceID": 15, "context": "By a very natural extension, this technique also applies to ratios of expectations, as they are elicitable (Gneiting, 2011): construct two unbiased estimators, and elicit the ratio of their means.", "startOffset": 107, "endOffset": 123}, {"referenceID": 7, "context": "Ratios of expectations: index of dispersion and Sharpe ratio The index of dispersion of a random variable Y with positive mean is defined to be Var(Y )/E[Y ] (Cox and Lewis, 1966).", "startOffset": 158, "endOffset": 179}, {"referenceID": 22, "context": "The Sharpe ratio of a random variable Y , which is a commonly-used measure of the risk-adjusted return of an investment, is defined similarly as E[Y ]/ \u221a Var(Y ) (Sharpe, 1966).", "startOffset": 162, "endOffset": 176}, {"referenceID": 9, "context": "This follows from Theorem 2 of Frongillo and Kash (2015c), specifically Section 4.", "startOffset": 31, "endOffset": 58}, {"referenceID": 8, "context": "Multi-Observation Regression One of the earliest problems in modern statistics was the estimation of biodiversity in a geographic region (Fisher et al., 1943).", "startOffset": 137, "endOffset": 158}], "year": 2017, "abstractText": "We study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously. To our knowledge, such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly. As compared to traditional loss functions that take only a single data point, these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in empirical risk minimization, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between dimensionality and number of observations, give some geometric characterizations and intuition for relating loss functions and the properties that they elicit, and discuss some implications for both elicitation and machine-learning contexts.", "creator": "LaTeX with hyperref package"}}}