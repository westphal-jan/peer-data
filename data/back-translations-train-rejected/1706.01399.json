{"id": "1706.01399", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "abstract": "Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for text generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch by employing curriculum learning, slowly increasing the length of the generated text, and by training the RNN simultaneously to generate sequences of different lengths. We show that this approach vastly improves the quality of generated sequences compared to the convolutional baseline.", "histories": [["v1", "Mon, 5 Jun 2017 16:10:58 GMT  (22kb)", "https://arxiv.org/abs/1706.01399v1", null], ["v2", "Mon, 12 Jun 2017 17:22:19 GMT  (22kb)", "http://arxiv.org/abs/1706.01399v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ofir press", "amir bar", "ben bogin", "jonathan berant", "lior wolf"], "accepted": false, "id": "1706.01399"}, "pdf": {"name": "1706.01399.pdf", "metadata": {"source": "CRF", "title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "authors": ["Ofir Press", "Amir Bar", "Ben Bogin", "Jonathan Berant", "Lior Wolf"], "emails": ["ofir.press@cs.tau.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.01 399v 2 [cs.C L] 12. Jun 2017Generative Adversarial Networks (GANs) have recently proven to be promising in image generation, and the training of GANs for speech generation has proved to be more difficult due to the indistinguishable nature of text generation with relapsing neural networks. Consequently, in the past, either pre-training was used with maximum probability or revolutionary networks were used for generation. In this work, we show that recurring neural networks can be trained to create text with GANs from scratch using curriculum learning, slowly teaching the model to generate sequences of increasing and variable length. We empirically demonstrate that our approach greatly improves the quality of the sequences generated compared to a Convolutionary Baseline. 1"}, {"heading": "1 Introduction", "text": "Generative adversarial networks (Goodfellow et al., 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017). However, in text generation, the training of GANs with recurring neural networks (RNNs) is based on a greater challenge, mainly due to the indistinguishable nature of the creation of discrete symbols. Consequently, previous work on the use of GANs for text generation is based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; sequence generation of text sequences, 2017; Liang et al al al al al., 2017; Shetty et al., 2017). Denotes equal contribution. Author order determined by coin flip. 1 Code for our models and evaluation methods is athgips / / thub / amib / bar."}, {"heading": "2 Motivation", "text": "While models trained with a Maximum Probability Target (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are disadvantages in using ML that indicate training with GANs. Firstly, the use of ML suffers from exposure bias, i.e. the model is exposed to only gold data during training, but at test time it takes into account its own predictions, and therefore false predictions quickly accumulate, resulting in poor text generation.Secondly, the ML loss function is very stringent. In training with ML, the model aims to match all probability mass to the i-th character of the training set, taking into account the previous i-1 characters, and considers any deviation from the gold sequence to be incorrect, although there are many possible sequences tagged with a particular prefix, the model aims to match all probability mass to the i-th character of the training set, and considers any deviation from the gold sequence to be very stringent."}, {"heading": "3 Preliminaries", "text": "Gulrajani et al. (2017) and Hjelm et al. (2017), which use the Enhanced Waterstone GAN Target (Arjovsky et al., 2017; Gulrajani et al., 2017), which we also use. Hjelm et al. (2017), have a similar setup, but use the Boundary Seeking GAN Target (Arjovsky et al., 2017; Gulrajani et al. (2017), which uses the generator G in Gulrajani et al. (2017), is a generator G in Gulrajani et al. (2017), which transforms an interference vector z-N (0, 1) into a matrix M-R32 \u00d7 V, where V is the size of the character vowel, and 32 is the length of the generated text."}, {"heading": "4 Recurrent Models", "text": "We employ a GRU (Cho et al., 2014) based on RNN for our generator and discriminator. The generator is initialized by feeding it with a sound vector z as a hidden state, and an embedded discriminated start-of-sequence symbol as input. The generator then generates a sequence of distributions over characters that use a softmax layer over the hidden state at any time. Instead, we employ a continuous relaxation and provide a weighted average representation at any time, which is given by the output distribution of step i \u2212 1. Formally, we let the probability of creating the character calculate i \u2212 1, and let the character (c) embed the character c, then the input to theGRU."}, {"heading": "5 Results", "text": "This year, we're going to be able to put ourselves at the top, \"he said."}, {"heading": "6 Conclusion", "text": "For the first time, we will show a GAN target trained RNN that learns to generate natural language from scratch. Furthermore, we will show that our model generalizes to sequences that are longer than those seen during the training. In the future, we plan to apply these models to tasks such as captions and translation and compare them with models that are most likely to be trained."}], "references": [{"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou."], "venue": "arXiv preprint arXiv:1701.07875 .", "citeRegEx": "Arjovsky et al\\.,? 2017", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 26th annual international conference on machine learning. ACM, pages 41\u201348.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Maximum-likelihood augmented discrete generative adversarial networks", "author": ["Tong Che", "Yanran Li", "Ruixiang Zhang", "R. Devon Hjelm", "Wenjie Li", "Yangqiu Song", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1702.07983 .", "citeRegEx": "Che et al\\.,? 2017", "shortCiteRegEx": "Che et al\\.", "year": 2017}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "TomasMikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L Elman."], "venue": "Cognition 48(1):71\u201399.", "citeRegEx": "Elman.,? 1993", "shortCiteRegEx": "Elman.", "year": 1993}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Improved training of wasserstein gans", "author": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville."], "venue": "arXiv preprint arXiv:1704.00028 .", "citeRegEx": "Gulrajani et al\\.,? 2017", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2017}, {"title": "Boundary-seeking generative adversarial networks", "author": ["R Devon Hjelm", "Athul Paul Jacob", "Tong Che", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1702.08431 .", "citeRegEx": "Hjelm et al\\.,? 2017", "shortCiteRegEx": "Hjelm et al\\.", "year": 2017}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Professor Forcing: A New Algorithm for Training Recurrent Networks", "author": ["A. Lamb", "A. Goyal", "Y. Zhang", "S. Zhang", "A. Courville", "Y. Bengio."], "venue": "arXiv preprint arXiv:1610.09038 .", "citeRegEx": "Lamb et al\\.,? 2016", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1701.06547 .", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Recurrent topictransition GAN for visual paragraph generation", "author": ["Xiaodan Liang", "Zhiting Hu", "Hao Zhang", "Chuang Gan", "Eric P. Xing."], "venue": "arXiv preprint arXiv:1703.07022 .", "citeRegEx": "Liang et al\\.,? 2017", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Statistical language models based on neural networks", "author": ["Tom\u00e1\u0161 Mikolov."], "venue": "Ph.D. thesis, Brno University of Technology.", "citeRegEx": "Mikolov.,? 2012", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero."], "venue": "arXiv preprint arXiv:1411.1784 .", "citeRegEx": "Mirza and Osindero.,? 2014", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "arXiv preprint arXiv:1511.06434 .", "citeRegEx": "Radford et al\\.,? 2015", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Speaking the same language: Matching machine to human captions by adversarial training", "author": ["Rakshith Shetty", "Marcus Rohrbach", "Lisa Anne Hendricks", "Mario Fritz", "Bernt Schiele."], "venue": "arXiv preprint arXiv:1703.10476 .", "citeRegEx": "Shetty et al\\.,? 2017", "shortCiteRegEx": "Shetty et al\\.", "year": 2017}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Adversarial Neural Machine Translation", "author": ["L.Wu", "Y. Xia", "L. Zhao", "F. Tian", "T. Qin", "J. Lai", "T.-Y. Liu."], "venue": "arXiv preprint arXiv:1704.06933 .", "citeRegEx": "L.Wu et al\\.,? 2017", "shortCiteRegEx": "L.Wu et al\\.", "year": 2017}, {"title": "Improving neural machine translation with conditional sequence generative adversarial nets", "author": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu."], "venue": "arXiv preprint arXiv:1703.04887 .", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu."], "venue": "arXiv preprint arXiv:1609.05473 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Generating text via adversarial training", "author": ["Yizhe Zhang", "Zhe Gan", "Lawrence Carin."], "venue": "NIPS Workshop on Adversarial Training.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Generative adversarial networks (Goodfellow et al., 2014) have achieved state-of-the-art results in image generation (Goodfellow et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 6, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 15, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 0, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 7, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 20, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 11, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 19, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 12, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 21, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 16, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 10, "context": "wgan or joint training (Lamb et al., 2016; Che et al., 2017) of the generator and discriminator with a supervised maximum-likelihood loss.", "startOffset": 23, "endOffset": 60}, {"referenceID": 2, "context": "wgan or joint training (Lamb et al., 2016; Che et al., 2017) of the generator and discriminator with a supervised maximum-likelihood loss.", "startOffset": 23, "endOffset": 60}, {"referenceID": 7, "context": "Recently, two initial attempts to generate text using purely generative adversarial training were conducted by Gulrajani et al. (2017) and Hjelm et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 7, "context": "Recently, two initial attempts to generate text using purely generative adversarial training were conducted by Gulrajani et al. (2017) and Hjelm et al. (2017). In these works, a convolutional neural network (CNN) was trained to produce sequences of 32 characters.", "startOffset": 111, "endOffset": 159}, {"referenceID": 17, "context": "Our main contribution is a model that employs an RNN for both the generator and discriminator, similar to current state-of-the-art approaches for language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016).", "startOffset": 166, "endOffset": 230}, {"referenceID": 13, "context": "Our main contribution is a model that employs an RNN for both the generator and discriminator, similar to current state-of-the-art approaches for language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016).", "startOffset": 166, "endOffset": 230}, {"referenceID": 9, "context": "Our main contribution is a model that employs an RNN for both the generator and discriminator, similar to current state-of-the-art approaches for language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016).", "startOffset": 166, "endOffset": 230}, {"referenceID": 5, "context": "We succeed in training the model by using curriculum learning (Elman, 1993; Bengio et al., 2009): At each stage we increase the maximal length of generated sequences, and train over sequences of variable length that are shorter than that maximal length.", "startOffset": 62, "endOffset": 96}, {"referenceID": 1, "context": "We succeed in training the model by using curriculum learning (Elman, 1993; Bengio et al., 2009): At each stage we increase the maximal length of generated sequences, and train over sequences of variable length that are shorter than that maximal length.", "startOffset": 62, "endOffset": 96}, {"referenceID": 5, "context": "In this paper, we extend the setup of Gulrajani et al. (2017) and present a method for generating text with GANs.", "startOffset": 38, "endOffset": 62}, {"referenceID": 17, "context": "objective (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are drawbacks to using ML, that suggest training with GANs.", "startOffset": 57, "endOffset": 121}, {"referenceID": 13, "context": "objective (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are drawbacks to using ML, that suggest training with GANs.", "startOffset": 57, "endOffset": 121}, {"referenceID": 9, "context": "objective (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are drawbacks to using ML, that suggest training with GANs.", "startOffset": 57, "endOffset": 121}, {"referenceID": 0, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well.", "startOffset": 55, "endOffset": 102}, {"referenceID": 7, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well.", "startOffset": 55, "endOffset": 102}, {"referenceID": 0, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well. Hjelm et al. (2017) have a similar setup, but employ the Boundary-Seeking GAN objective.", "startOffset": 56, "endOffset": 149}, {"referenceID": 0, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well. Hjelm et al. (2017) have a similar setup, but employ the Boundary-Seeking GAN objective. The generator G in Gulrajani et al. (2017) is a CNN that transforms a noise vector z \u223c N(0, 1) into a matrix M \u2208 R32\u00d7V , where V is the size of the character vocabulary, and 32 is the length of the generated text.", "startOffset": 56, "endOffset": 261}, {"referenceID": 7, "context": "A disadvantage of the generators in Gulrajani et al. (2017) and Hjelm et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 7, "context": "A disadvantage of the generators in Gulrajani et al. (2017) and Hjelm et al. (2017) is that they use CNNs for generation, and thus the i-th generated character is not directly conditioned on the entire history of i\u22121 generated characters.", "startOffset": 36, "endOffset": 84}, {"referenceID": 4, "context": "We employ a GRU (Cho et al., 2014) based RNN for our generator and discriminator.", "startOffset": 16, "endOffset": 34}, {"referenceID": 7, "context": "Table 1: Samples and evaluation of the baseline model from Gulrajani et al. (2017).", "startOffset": 59, "endOffset": 83}, {"referenceID": 7, "context": "An advantage of a recurrent generator compared to the convolutional generator of Gulrajani et al. (2017) and Hjelm et al.", "startOffset": 81, "endOffset": 105}, {"referenceID": 7, "context": "An advantage of a recurrent generator compared to the convolutional generator of Gulrajani et al. (2017) and Hjelm et al. (2017) is that can output sequences of varying lengths, as we empirically show in Section 5.", "startOffset": 81, "endOffset": 129}, {"referenceID": 7, "context": "Our baseline model trains the generator and discriminator over sequences of length 32, similar to how CNNs were trained in Gulrajani et al. (2017). We found that training this baseline was difficult and resulted in nonsensical text.", "startOffset": 123, "endOffset": 147}, {"referenceID": 14, "context": "This could be viewed as a conditional GAN (Mirza and Osindero, 2014), where the first i \u2212 1 characters are the input and the final character is the output.", "startOffset": 42, "endOffset": 68}, {"referenceID": 3, "context": "(2017), we follow their setup and train our models on the Billion Word dataset (Chelba et al., 2013).", "startOffset": 79, "endOffset": 100}, {"referenceID": 5, "context": "To directly compare to Gulrajani et al. (2017), we follow their setup and train our models on the Billion Word dataset (Chelba et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 0, "context": "In contrast to Arjovsky et al. (2017) and Gulrajani et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 0, "context": "In contrast to Arjovsky et al. (2017) and Gulrajani et al. (2017), where the generator is trained once for every 10 training iterations of the discriminator, we found that training the generator for 50 iterations every 10 training iterations of the discriminator resulted in superior performance.", "startOffset": 15, "endOffset": 66}, {"referenceID": 0, "context": "In contrast to Arjovsky et al. (2017) and Gulrajani et al. (2017), where the generator is trained once for every 10 training iterations of the discriminator, we found that training the generator for 50 iterations every 10 training iterations of the discriminator resulted in superior performance. In addition, instead of using noise vectors sampled from the N(0, 1) distribution as in Gulrajani et al. (2017), we sample noise vectors from the N(0, 10) distribution, since we found this leads to a greater variance in the generated", "startOffset": 15, "endOffset": 409}, {"referenceID": 7, "context": "Following Gulrajani et al. (2017), we train all our models on sequences whose maximum length is 32 characters.", "startOffset": 10, "endOffset": 34}, {"referenceID": 7, "context": "Following Gulrajani et al. (2017), we train all our models on sequences whose maximum length is 32 characters. Table 1 shows results of the baseline model of Gulrajani et al. (2017), and Table 2 presents results of our models with various combinations of extensions (Curriculum Learning, Variable Length, and Teacher Helping).", "startOffset": 10, "endOffset": 182}], "year": 2017, "abstractText": "Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. We empirically show that our approach vastly improves the quality of generated sequences compared to a convolutional baseline. 1", "creator": "LaTeX with hyperref package"}}}