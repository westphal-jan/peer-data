{"id": "1706.01417", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "A method for the online construction of the set of states of a Markov Decision Process using Answer Set Programming", "abstract": "Non-stationary domains, that change in unpredicted ways, are a challenge for agents searching for optimal policies in sequential decision-making problems. This paper presents a combination of Markov Decision Processes (MDP) with Answer Set Programming (ASP), named {\\em Online ASP for MDP} (oASP(MDP)), which is a method capable of constructing the set of domain states while the agent interacts with a changing environment. oASP(MDP) updates previously obtained policies, learnt by means of Reinforcement Learning (RL), using rules that represent the domain changes observed by the agent. These rules represent a set of domain constraints that are processed as ASP programs reducing the search space. Results show that oASP(MDP) is capable of finding solutions for problems in non-stationary domains without interfering with the action-value function approximation process.", "histories": [["v1", "Mon, 5 Jun 2017 16:48:23 GMT  (612kb,D)", "http://arxiv.org/abs/1706.01417v1", "Submitted to IJCAI 17"]], "COMMENTS": "Submitted to IJCAI 17", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["leonardo a ferreira", "reinaldo a c bianchi", "paulo e santos", "ramon lopez de mantaras"], "accepted": false, "id": "1706.01417"}, "pdf": {"name": "1706.01417.pdf", "metadata": {"source": "CRF", "title": "A method for the online construction of the set of states of a Markov Decision Process using Answer Set Programming", "authors": ["Leonardo A. Ferreira", "Reinaldo A. C. Bianchi", "Paulo E. Santos", "Ramon Lopez de Mantaras", "Bernardo do Campo"], "emails": ["leonardo.ferreira@metodista.br,", "rbianchi@fei.edu.br,", "psantos@fei.edu.br,", "mantaras@iiia.csic.es"], "sections": [{"heading": "1 Introduction", "text": "A key issue in Artificial Intelligence (AI) is equipping autonomous agents with the ability to operate in changing domains by adjusting the processes of agents at a cost that corresponds to the complexity of domain changes, a skill known as elaboration tolerance [McCarthy, 1987; McCarthy, 1998]. Consider, for example, an autonomous robot that navigates in an unknown environment. Unforeseen events can occur that block passages (or were previously unavailable), and the autonomous agent should be able to find new solutions in this altered domain, using previously acquired knowledge from the observed changes in the environment without running a complete code explosion that blocks a new cycle of domain exploration from scratch."}, {"heading": "2 Background", "text": "This section introduces Markov Decision Processes (MDP), Reinforcement Learning (RL) and Answer Set Programming (ASP), which form the basis of the work reported in this paper."}, {"heading": "2.1 MDP and Reinforcement Learning", "text": "In a sequential decision problem, an agent is required to perform a series of actions in an environment in which Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q \u00b7 Q: 170 6.01 417v 1 [cs.A I] 5 Jun 201 7Find the solution to a particular problem. Such a sequence of actions, which provides a workable solution, is known as politics (\u03c0) leading the agent from a starting state to a destination state [Bellman, 1957; Bellman and Dreyfus, 1971]. In the face of a number of workable solutions, an optimal political solution can be found by applying Bellman's principle of optimism [Bellman and Dreyfus, 1971] which states that \"an optimal policy has the property that, regardless of the initial state and the initial decision, the remaining decisions must constitute an optimal policy in relation to the state resulting from the first decision.\""}, {"heading": "2.2 Answer Set Programming", "text": "It is as if the breaking points of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the setting point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the setting point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the breaking point of the setting"}, {"heading": "3 Online ASP for MDP: oASP(MDP)", "text": "Given the sets S and A of an MDP, an RL method M can approximate an action value function Q (s, a). If S is constructed state by state while the agent interacts with the world, M is still able to approximate Q (s, a), using only the current and past states for this. By using selection rules in ASP, it is possible to describe a transition t (s, a, s) in the form 1 {s'} 1: - a for each action an \"As\" and each state s \"S. By describing possible transitions for each action in each state as a logic program, an ASP engine can be used to provide a set of observed states. Thus, a set of actions can be defined as for each state and finally an action value function that can be used to continue operating in that environment. This is the essence of the oASP (MDP) method represented in the algorithm."}, {"heading": "16 end", "text": "17 Update Q (s, a)'s value within the meaning of M. 18 Update the current state s \u2190 s \u2032. 19 by the end of the episode"}, {"heading": "20 end", "text": "This year, we will be able to go in search of a solution that will enable us, that will enable us to find a solution that will enable us, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position where we are."}, {"heading": "4 Tests and Results", "text": "The oASP (MDP) algorithm was evaluated using tests performed in non-deterministic, non-stationary, grid world domains. Two sets of tests were taken into account, where one of the following domain variables was randomly changed in each set: the number and location of walls in the grid (first test, Section 4.1), and the transition probabilities (second test, Section 4.2). Four actions were allowed in the test domains considered in this work: go up, go down, go left and right. Each action has a predefined probability of guiding the agent in the desired direction and also for moving the agent to an orthogonal (undesirable) position. The transition probability for each action depends on the grid world and is defined for each test as described below. In all tests, the initial state was fixed at the lowest square (e.g. cell \"S\" in Figure 1) and the target state was fixed."}, {"heading": "4.1 First test: changes in the wall\u2013free-space ratio", "text": "In the first test, the size of the network was set at 10 \u00d7 10 and the transition probabilities were assigned to 90% for the movement in the desired direction and 5% for the movement in each of the two directions that are orthogonal to the desired ones. In this test, changes in the environment occurred in the number and location of walls in the network. First, the domain without walls (0%) starts, then changes to a world where 10% of the network is occupied by walls placed in random places, and finally, the network world changes to a situation where 25% of the network is occupied by walls. Each change occurs after 1000 episodes. Results obtained in the first test are shown in Figure 2. Figure 2a shows that the RMSD values decrease faster than those of Q-Learning, thereby achieving the optimal pre-Q-learning policy."}, {"heading": "4.2 Second test: changes in the transition probabilities", "text": "In this context, it should be noted that this is not an isolated case, but a group of people who are able to survive themselves. In this case, we are talking about people who are able to survive themselves and about people who are able to survive themselves. In this case, we are talking about people who are able to survive themselves, and we are talking about people who are able to survive themselves. In this case, we are talking about people who are able to survive themselves and we are talking about people who are able to survive themselves."}, {"heading": "5 Related Work", "text": "Previous attempts to combine RL with ASP include [Zhang et al., 2015], which suggest the use of ASP to find a predefined plan for an RL agent. This plan is described as a hierarchical MDP and RL is used to find the optimal policy for this MDP. However, changes in the environment as used in the current work were not considered in [Zhang et al., 2015]. Analogous methods were used by [Khandelwal et al., 2014; Yang et al., 2014], in which an agent interacts with an environment and updates the cost function of an action. While [Khandelwal et al., 2014] uses the action language BC [Yang et al., 2014] ASP uses to find a description of the environment. Although both methods take into account the cost of action, neither of them uses Reinforcement Learning and they do not deal with changes in the action value function RP during interaction with the environment."}, {"heading": "6 Conclusion", "text": "This paper presented the oASP (MDP) method for approximating the action value functions of Markov Decision Processes, in non-stationary areas, with unknown states and unknown transition and reward functions. This method is based on a combination of Reinforcement Learning (RL) and Answer Set Programming (ASP). The main advantage of RL is that it does not require an a priori knowledge of transition and reward functions, but it relies on the presence of a complete knowledge of the domain states. In oASP (MDP), ASP is used to construct the state set of an MDP that can be used by an RL algorithm. ASP programs representing domain states and transitions are interacted as the agent with the environment, providing an efficient solution to find optimal strategies in changing environments.Tests were performed in two non-stationary, non-deterrent domains, the domain ministries."}], "references": [{"title": "Theory and Practice of Logic Programming", "author": ["Chitta Baral", "Michael Gelfond", "Nelson Rushton. Probabilistic reasoning with answer sets"], "venue": "9(1):57,", "citeRegEx": "Baral et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Press", "author": ["Richard Ernest Bellman", "Stuart E. Dreyfus. Applied dynamic programming. Princeton Univ"], "venue": "4 edition,", "citeRegEx": "Bellman and Dreyfus. 1971", "shortCiteRegEx": null, "year": 1971}, {"title": "Indiana University Mathematics Journal", "author": ["Richard Bellman. A Markovian decision process"], "venue": "6(4):679\u2013684,", "citeRegEx": "Bellman. 1957", "shortCiteRegEx": null, "year": 1957}, {"title": "Mathematics of Operations Research", "author": ["Eyal Even-Dar", "Sham. M. Kakade", "Yishay Mansour. Online markov decision processes"], "venue": "34(3):726\u2013 736,", "citeRegEx": "Even.Dar et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards deep symbolic reinforcement learning", "author": ["Marta Garnelo", "Kai Arulkumaran", "Murray Shanahan"], "venue": "arXiv preprint arXiv:1609.05518 [cs], September", "citeRegEx": "Garnelo et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Answer set solving in practice", "author": ["Martin Gebser", "Roland Kaminski", "Benjamin Kaufmann"], "venue": "Morgan & Claypool Publishers,", "citeRegEx": "Gebser et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The stable model semantics for logic programming", "author": ["Gelfond", "Lifschitz", "1988] Michael Gelfond", "Vladimir Lifschitz"], "venue": "Proceedings of International Logic Programming Conference and Symposium,", "citeRegEx": "Gelfond et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Gelfond et al\\.", "year": 1988}, {"title": "A tribute to Judea Pearl", "author": ["Michael Gelfond", "Nelson Rushton. Causal", "probabilistic reasoning in P-log. Heuristics", "Probabilities", "Causality"], "venue": "pages 337\u2013359,", "citeRegEx": "Gelfond and Rushton. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "chapter Answer sets", "author": ["Frank Michael Gelfond. van Harmelen", "Vladimir Lifschitz", "Bruce. Handbook of Knowledge Representation Porter"], "venue": "page 285\u2013316. Elsevier,", "citeRegEx": "Gelfond. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Planning in action language BC while learning action costs for mobile robots", "author": ["Khandelwal et al", "2014] Piyush Khandelwal", "Fangkai Yang", "Matteo Leonetti", "Vladimir Lifschitz", "Peter Stone"], "venue": "In Proceedings of the Twenty-Fourth International Conference on Automated", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Artificial Intelligence", "author": ["Vladimir Lifschitz. Answer set programming", "plan generation"], "venue": "138(1):39\u201354,", "citeRegEx": "Lifschitz. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Communications of the ACM", "author": ["John McCarthy. Generality in artificial intelligence"], "venue": "30(12):1030\u2013 1035,", "citeRegEx": "McCarthy. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "of the Fourth Symposium on Logical Formalizations of Commonsense Reasoning (Common Sense 98)", "author": ["John McCarthy. Elaboration tolerance. In Proc"], "venue": "volume 98, London, UK,", "citeRegEx": "McCarthy. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Reinforcement learning an introduction \u2013 Second edition", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "progress (Draft). MIT Press,", "citeRegEx": "Sutton and Barto. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "PhD thesis", "author": ["Christopher J.C.H. Watkins. Learning from deSuttonlayed rewards"], "venue": "University of Cambridge England,", "citeRegEx": "Watkins. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Planning in answer set programming while learning action costs for mobile robots", "author": ["Fangkai Yang", "Piyush Khandelwal", "Matteo Leonetti", "Peter Stone"], "venue": "AAAI Spring 2014 Symposium on Knowledge Representation and Reasoning in Robotics (AAAI-SSS),", "citeRegEx": "Yang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Mathematics of Operations Research", "author": ["Jia Yuan Yu", "Shie Mannor", "Nahum Shimkin. Markov decision processes with arbitrary reward processes"], "venue": "34(3):737\u2013757,", "citeRegEx": "Yu et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "IEEE Transactions on Robotics", "author": ["Shiqi Zhang", "Mohan Sridharan", "Jeremy L. Wyatt. Mixed logical inference", "probabilistic planning for robots in unreliable worlds"], "venue": "31(3):699\u2013713,", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "This ability is called elaboration tolerance [McCarthy, 1987; McCarthy, 1998].", "startOffset": 45, "endOffset": 77}, {"referenceID": 12, "context": "This ability is called elaboration tolerance [McCarthy, 1987; McCarthy, 1998].", "startOffset": 45, "endOffset": 77}, {"referenceID": 13, "context": "Reinforcement Learning (RL) is an AI framework in which an agent interacts with its environment in order to find a sequence of actions (a policy) to perform a given task [Sutton and Barto, 2015].", "startOffset": 170, "endOffset": 194}, {"referenceID": 4, "context": "However, in spite of having the optimal solution to a particular task, a RL agent may still perform poorly on a new task, even if the latter is similar to the former [Garnelo et al., 2016].", "startOffset": 166, "endOffset": 188}, {"referenceID": 11, "context": "Non-monotonic reasoning can be used as a tool to increase the generality of domain representations [McCarthy, 1987] and may provide the appropriate element to build agents more adaptable to changing situations.", "startOffset": 99, "endOffset": 115}, {"referenceID": 10, "context": "In this work we consider Answer Set Programming (ASP) [Gelfond and Lifschitz, 1988; Lifschitz, 2002], which is a declarative non-monotonic logic programming language, to bridge the gap between RL and elaboration tolerant solutions.", "startOffset": 54, "endOffset": 100}, {"referenceID": 2, "context": "Such sequence of actions, that forms a feasible solution, is known as a policy (\u03c0) which leads the agent from an initial state to a goal state [Bellman, 1957; Bellman and Dreyfus, 1971].", "startOffset": 143, "endOffset": 185}, {"referenceID": 1, "context": "Such sequence of actions, that forms a feasible solution, is known as a policy (\u03c0) which leads the agent from an initial state to a goal state [Bellman, 1957; Bellman and Dreyfus, 1971].", "startOffset": 143, "endOffset": 185}, {"referenceID": 1, "context": "Given a set of feasible solutions, an optimal policy \u03c0\u2217 can be found by using Bellman\u2019s Principle of Optimality [Bellman and Dreyfus, 1971], which states that \u201can optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision\u201d; \u03c0\u2217 can be defined as the policy that maximises/minimises a desired reward/cost function.", "startOffset": 112, "endOffset": 139}, {"referenceID": 14, "context": "One method that can be used to find an optimal policy for MDPs, which does not need a priori knowledge of the transition and reward functions, is the reinforcement learning model-free off-policy method known as QLearning [Watkins, 1989; Sutton and Barto, 2015].", "startOffset": 221, "endOffset": 260}, {"referenceID": 13, "context": "One method that can be used to find an optimal policy for MDPs, which does not need a priori knowledge of the transition and reward functions, is the reinforcement learning model-free off-policy method known as QLearning [Watkins, 1989; Sutton and Barto, 2015].", "startOffset": 221, "endOffset": 260}, {"referenceID": 14, "context": "Since Q-Learning is a well-known and largely used RL method, we omit its detailed description here, which can be found in [Watkins, 1989; Sutton and Barto, 2015].", "startOffset": 122, "endOffset": 161}, {"referenceID": 13, "context": "Since Q-Learning is a well-known and largely used RL method, we omit its detailed description here, which can be found in [Watkins, 1989; Sutton and Barto, 2015].", "startOffset": 122, "endOffset": 161}, {"referenceID": 10, "context": "Answer Set Programming (ASP) is a declarative nonmonotonic logic programming language that has been successfully used for NP-complete problems such as planning [Lifschitz, 2002; Zhang et al., 2015; Yang et al., 2014].", "startOffset": 160, "endOffset": 216}, {"referenceID": 17, "context": "Answer Set Programming (ASP) is a declarative nonmonotonic logic programming language that has been successfully used for NP-complete problems such as planning [Lifschitz, 2002; Zhang et al., 2015; Yang et al., 2014].", "startOffset": 160, "endOffset": 216}, {"referenceID": 15, "context": "Answer Set Programming (ASP) is a declarative nonmonotonic logic programming language that has been successfully used for NP-complete problems such as planning [Lifschitz, 2002; Zhang et al., 2015; Yang et al., 2014].", "startOffset": 160, "endOffset": 216}, {"referenceID": 8, "context": "ASP is based on the stable model semantics of logic programs [Gelfond, 2008].", "startOffset": 61, "endOffset": 76}, {"referenceID": 8, "context": "ASP programs are executed by computing stable models, which is usually accomplished by inference engines called answer set solvers [Gelfond, 2008].", "startOffset": 131, "endOffset": 146}, {"referenceID": 5, "context": "4 using ZeroMQ for providing messages exchanges between agent and environment and Clingo [Gebser et al., 2013] was used as the ASP Engine.", "startOffset": 89, "endOffset": 110}, {"referenceID": 17, "context": "Previous attempts at combining RL with ASP include [Zhang et al., 2015], which proposes the use of ASP to find a predefined plan for a RL agent.", "startOffset": 51, "endOffset": 71}, {"referenceID": 17, "context": "However, changes in the environment, as used in the present work, were not considered in [Zhang et al., 2015].", "startOffset": 89, "endOffset": 109}, {"referenceID": 15, "context": "Analogous methods were proposed by [Khandelwal et al., 2014; Yang et al., 2014], in which an agent interacts with an environment and updates an action\u2019s cost function.", "startOffset": 35, "endOffset": 79}, {"referenceID": 15, "context": ", 2014] uses the action language BC, [Yang et al., 2014] uses ASP to find a description of the environment.", "startOffset": 37, "endOffset": 56}, {"referenceID": 0, "context": "An approach to non-deterministic answer set programs is P-Log [Baral et al., 2009; Gelfond and Rushton, 2010].", "startOffset": 62, "endOffset": 109}, {"referenceID": 7, "context": "An approach to non-deterministic answer set programs is P-Log [Baral et al., 2009; Gelfond and Rushton, 2010].", "startOffset": 62, "endOffset": 109}, {"referenceID": 3, "context": "Works related to non-stationary MDPs such as [Even-Dar et al., 2009; Yu et al., 2009], which deal only with changes in reward function, are more associated with RL alone than with a hybrid method such as oASP(MDP), since RL methods are already capable of handling changes in the reward and transition functions.", "startOffset": 45, "endOffset": 85}, {"referenceID": 16, "context": "Works related to non-stationary MDPs such as [Even-Dar et al., 2009; Yu et al., 2009], which deal only with changes in reward function, are more associated with RL alone than with a hybrid method such as oASP(MDP), since RL methods are already capable of handling changes in the reward and transition functions.", "startOffset": 45, "endOffset": 85}, {"referenceID": 4, "context": "A proposal that closely resembles oASP(MDP) is [Garnelo et al., 2016].", "startOffset": 47, "endOffset": 69}, {"referenceID": 4, "context": "Also, a comparison of oASP(MDP) with the framework proposed in [Garnelo et al., 2016] is an interesting subject for future research.", "startOffset": 63, "endOffset": 85}], "year": 2017, "abstractText": "Non-stationary domains, that change in unpredicted ways, are a challenge for agents searching for optimal policies in sequential decision-making problems. This paper presents a combination of Markov Decision Processes (MDP) with Answer Set Programming (ASP), named Online ASP for MDP (oASP(MDP)), which is a method capable of constructing the set of domain states while the agent interacts with a changing environment. oASP(MDP) updates previously obtained policies, learnt by means of Reinforcement Learning (RL), using rules that represent the domain changes observed by the agent. These rules represent a set of domain constraints that are processed as ASP programs reducing the search space. Results show that oASP(MDP) is capable of finding solutions for problems in non-stationary domains without interfering with the action-value function approximation process.", "creator": "LaTeX with hyperref package"}}}