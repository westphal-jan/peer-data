{"id": "1706.01450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "A Joint Model for Question Answering and Question Generation", "abstract": "We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model's novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking.", "histories": [["v1", "Mon, 5 Jun 2017 17:58:52 GMT  (189kb,D)", "http://arxiv.org/abs/1706.01450v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["tong wang", "xingdi yuan", "adam trischler"], "accepted": false, "id": "1706.01450"}, "pdf": {"name": "1706.01450.pdf", "metadata": {"source": "META", "title": "A Joint Model for Question Answering and Question Generation", "authors": ["Tong Wang", "Xingdi Yuan", "Adam Trischler"], "emails": ["<tong.wang@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "It is not just a question of access to huge amounts of information, but it is also a question of dealing with other people who are able to identify themselves and understand each other. It is also a question of behaviour and behaviour. It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question of behaviour. (...) It is a question. (...) It is a question. (...) It is a question. (...) It is a question. (... It is a question. (...) It is a question. (... It is a question. (...)."}, {"heading": "2. Related Work", "text": "In machine translation, for example, it has been shown that the quality of translation significantly improves compared to models trained with a single language pair when the attention mechanism is shared in a neural translation model and trained jointly on multiple language pairs. Wang & Jiang (2016) proposed one of the first neural models for the SQuAD datasets in the question. SQuAD defines an extractive QA task that includes answers to word spans in the corresponding document. Wang & Jiang (2016) showed that learning to refer to boundaries is more effective than pointing to the tokens that form an answer. Many later studies have adopted this boundary model and achieved near-human performance on the task (Wang et al., 2017; Shen et al.)."}, {"heading": "3. Model Description", "text": "The question of whether the condition of the question is sequence of words in the question-generation mode (q-gen). We also have a binary variable to indicate whether a data point is intended for an a-gen or q-gen. Intuitively, this should help the model to learn the two modalities more easily."}, {"heading": "4. Training and Inference", "text": "The optimization goal for updating the model parameters \u03b8 is to maximize the negative log probability of the generated sequences with respect to training data D: L = \u2212 \u2211 x \u2022 D log p (w \u0442t | w < t, x; \u03b8).Here w < t corresponds to embedding y (t \u2212 1) in Equation (1) and (4).During training, gold targets are used to force sequence generation for training, i.e. w < t = w {q, a} < t, while generation during inference is conditioned by the previously generated words, i.e. w < t = w < < t t. For words with multiple occurrences, since their exact references in the document cannot be reproduced, we aggregate the probability of these words in the encoder and the time decoder (similar to Kadlec et al. 2016)."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Dataset", "text": "We are conducting our experiments with the SQuAD corpus (Rajpurkar et al., 2016), a machine-comprehension data set consisting of over 100k crowdsourced question-answer pairs to 536 Wikipedia articles. Simple pre-processing is performed, including reducing all texts in the data set and using NLTK (Bird, 2006) for word tokenization. SQuAD's test split is hidden from the public. Therefore, we extract 5,158 question-answer pairs (which are included in 23 Wikipedia articles) from the training set as a validation set and use the official development data to report test results. Note that the answers in this data set are strictly extractive, and therefore limit the Pointer-Softmax module to pointing to all decoding steps in response generation mode."}, {"heading": "5.2. Baseline Models", "text": "Similarly, the Q-gen model is trained to generate questions from documents and answers only; the joint training (in the JointQA model) is realized by alternately feeding the model with answer generation and question generation data between mini-batches; in addition, we compare response generation performance with the sequence model variant of the Match-LSTM (mLSTM) model (Wang & Jiang, 2016); and, unlike existing neural QA models that point to the start and end limits of extractive responses, this model predicts a sequence of document positions as an answer, making it most similar to our QA setup (Wang & Jiang, 2016)."}, {"heading": "5.3. Quantitative Evaluation", "text": "We use F1 and Exact Match (EM, Rajpurkar et al. 2016) against the gold answer sequences to evaluate the answer generation, and BLEU2 (Papineni et al., 2002) against the gold question sequences to evaluate the question generation. However, existing studies have shown that the task of question generation often has a linguistic variance that is semantically permissible; this makes it inappropriate to evaluate a generated question solely by matching it with a gold sequence (Yuan et al., 2017). Therefore, we choose to evaluate the quality of the questions generated Y q even with two pre-trained neural models: We use a language model to calculate the perplexity of Y q, and a QA model to answer Y q. We measure the F1 score of the answer generated by this QA model. We choose mLSTM as a pre-trained QA model and train it on the quadriplexity basis of SAD performance intersected with the official SAD Spalite (5.AD) in section 5.AD."}, {"heading": "5.4. Analysis and Discussion", "text": "The evaluation results are presented in Table 1. We see that the performance of the A gene with the common model significantly improves: both F1 and EM increase by about 10 percentage points. The performance of the Q gene deteriorates after the joint training, but the decline is relatively small. In addition, as pointed2We use Microsoft COCO Caption Evaluation scripts (https: / / github.com / tylin / coco-caption) to calculate BLEU points. 3http: / / mattmahoney.net / dc / textdata 4https: / / data.quora.com / First-Quora-Dataset-Release-Question-Pairsout of previous studies, automatic metrics often do not show a good correlation with the human-rated generational quality (Yuan et al., 2017). Therefore, we consider the overall result to be positive.Meanwhile, although our model does not perform as well as mLSTM on the QA task."}, {"heading": "5.5. Qualitative Examples", "text": "Qualitatively, we have observed interesting \"attention shifts\" before and after joint training: in the positive case in Table 2, the gold question asks for the direct object Nixon of the verb endorse, while the A-gene model predicts the indirect object Kennedy instead. In contrast, the common model asks for the appointment of the vice president during the question, which presumably \"primes\" model attention to the right answer Nixon. Similarly, in the negative case, attention seems to shift to the questions in the common model through joint training toward an answer that is incorrect but closer to the generated question. Note that the examples in Table 2 come from the validation group and therefore it is not possible for the common model to memorize the gold answers from the question generation mode - the priming effect must come from some form of knowledge transfer between Q-gene and A-gene through joint training."}, {"heading": "5.6. Implementation Details", "text": "The implementation details of the proposed model are as follows: The encoder vocabulary indexes all words in the data set. The decoder vocabulary uses the 100 words sorted by frequency in the gold questions, which encourages the model to generate common words (e.g. Wh words and function words) from the decoder vocabulary and to copy rarer words (e.g. topical words and units) from the document. The word embed matrix is initialized with the 300-dimensional GloVe vectors (Pennington et al., 2014).The dimensionality of the character representations is 32. The number of hidden units is 384 for both encoders / decoders RNN cells. The drop-out is applied at a rate of 0.3 to all embedding layers as well as between the hidden states in the encoder / decoder RNs over time."}, {"heading": "6. Conclusion", "text": "We hypothesized that answering questions can benefit from synergistic interaction between the two tasks by sharing parameters and training them together. Our proposed model uses an attention-based sequenceto-sequence architecture that learns to dynamically switch between copying words from the document and generating words from a vocabulary. Experiments with the model confirm our hypothesis: the common model significantly outperforms its QA counterpart compared to the SQuAD dataset. Although the evaluation results are still lower than those obtained by specific QA models, the proposed model nevertheless demonstrates the effectiveness of joint training between QA and question setting, offering a new perspective and a promising direction to advance the study of QA."}], "references": [{"title": "Automatic gapfill question generation from text books", "author": ["Agarwal", "Manish", "Mannem", "Prashanth"], "venue": "In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Nltk: the natural language toolkit", "author": ["Bird", "Steven"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "Bird and Steven.,? \\Q2006\\E", "shortCiteRegEx": "Bird and Steven.", "year": 2006}, {"title": "Ask the right questions: Active question reformulation with reinforcement learning", "author": ["Buck", "Christian", "Bulian", "Jannis", "Ciaramita", "Massimiliano", "Gesmundo", "Andrea", "Houlsby", "Neil", "Gajewski", "Wojciech", "Wang", "Wei"], "venue": "arXiv preprint arXiv:1705.07830,", "citeRegEx": "Buck et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Buck et al\\.", "year": 2017}, {"title": "Ranking automatically generated questions using common human queries", "author": ["Chali", "Yllias", "Golestanirad", "Sina"], "venue": "In The 9th International Natural Language Generation conference,", "citeRegEx": "Chali et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chali et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Gated-attention readers for text comprehension", "author": ["Dhingra", "Bhuwan", "Liu", "Hanxiao", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Learning to ask: Neural question generation for reading comprehension", "author": ["Du", "Xinya", "Shao", "Junru", "Cardie", "Claire"], "venue": "arXiv preprint arXiv:1705.00106,", "citeRegEx": "Du et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Du et al\\.", "year": 2017}, {"title": "Searchqa: A new q&a dataset augmented with context from a search engine", "author": ["Dunn", "Matthew", "Sagun", "Levent", "Higgins", "Mike", "Guney", "Ugur", "Cirik", "Volkan", "Cho", "Kyunghyun"], "venue": "arXiv preprint arXiv:1704.05179,", "citeRegEx": "Dunn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dunn et al\\.", "year": 2017}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat", "Orhan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1601.01073,", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Gulcehre", "Caglar", "Ahn", "Sungjin", "Nallapati", "Ramesh", "Zhou", "Bowen", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1603.08148,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Manning", "Christopher D", "Surdeanu", "Mihai", "Bauer", "John", "Finkel", "Jenny Rose", "Bethard", "Steven", "McClosky", "David"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Generating natural questions about an image", "author": ["Mostafazadeh", "Nasrin", "Misra", "Ishan", "Devlin", "Jacob", "Mitchell", "Margaret", "He", "Xiaodong", "Vanderwende", "Lucy"], "venue": "arXiv preprint arXiv:1603.06059,", "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Nguyen", "Tri", "Rosenberg", "Mir", "Song", "Xia", "Gao", "Jianfeng", "Tiwary", "Saurabh", "Majumder", "Rangan", "Deng", "Li"], "venue": "arXiv preprint arXiv:1611.09268,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Generating natural language questions to support learning on-line", "author": ["Popowich", "David Lindberg Fred", "Winne", "John Nesbit Phil"], "venue": "ENLG", "citeRegEx": "Popowich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Popowich et al\\.", "year": 2013}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Rajpurkar", "Pranav", "Zhang", "Jian", "Lopyrev", "Konstantin", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Seo", "Minjoon", "Kembhavi", "Aniruddha", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"], "venue": "arXiv preprint arXiv:1611.01603,", "citeRegEx": "Seo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Shen", "Yelong", "Huang", "Po-Sen", "Gao", "Jianfeng", "Chen", "Weizhu"], "venue": "arXiv preprint arXiv:1609.05284,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Active comprehension: Problem-solving schema with question generation for comprehension of complex short stories", "author": ["Singer", "Harry", "Donlan", "Dan"], "venue": "Reading Research Quarterly,", "citeRegEx": "Singer et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Singer et al\\.", "year": 1982}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Trischler", "Adam", "Wang", "Tong", "Yuan", "Xingdi", "Harris", "Justin", "Sordoni", "Alessandro", "Bachman", "Philip", "Suleman", "Kaheer"], "venue": "arXiv preprint arXiv:1611.09830,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Wang", "Shuohang", "Jiang", "Jing"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["Wang", "Wenhui", "Yang", "Nan", "Wei", "Furu", "Chang", "Baobao", "Zhou", "Ming"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Semi-supervised qa with generative domain-adaptive nets", "author": ["Yang", "Zhilin", "Hu", "Junjie", "Salakhutdinov", "Ruslan", "Cohen", "William W"], "venue": "arXiv preprint arXiv:1702.02206,", "citeRegEx": "Yang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Machine comprehension by text-to-text neural question generation", "author": ["Yuan", "Xingdi", "Wang", "Tong", "Gulcehre", "Caglar", "Sordoni", "Alessandro", "Bachman", "Philip", "Subramanian", "Sandeep", "Zhang", "Saizheng", "Trischler", "Adam"], "venue": null, "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 20, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 8, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 25, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 17, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 27, "context": ", 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 150, "endOffset": 206}, {"referenceID": 22, "context": ", 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 150, "endOffset": 206}, {"referenceID": 21, "context": ", 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 150, "endOffset": 206}, {"referenceID": 7, "context": "Alongside QA, question generation has also gained increased popularity (Du et al., 2017; Yuan et al., 2017).", "startOffset": 71, "endOffset": 107}, {"referenceID": 3, "context": "Among its many applications, question generation has been used to improve QA systems (Buck et al., 2017; Serban et al., 2016; Yang et al., 2017).", "startOffset": 85, "endOffset": 144}, {"referenceID": 28, "context": "Among its many applications, question generation has been used to improve QA systems (Buck et al., 2017; Serban et al., 2016; Yang et al., 2017).", "startOffset": 85, "endOffset": 144}, {"referenceID": 1, "context": "To tackle the joint task, we construct an attentionbased (Bahdanau et al., 2014) sequence-to-sequence model (Sutskever et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 24, "context": ", 2014) sequence-to-sequence model (Sutskever et al., 2014) that takes a document as input and generates a question (answer) conditioned on an answer (question) as output.", "startOffset": 35, "endOffset": 59}, {"referenceID": 10, "context": "To address the mixed extractive/abstractive nature of the generative targets, we use the pointer-softmax mechanism (Gulcehre et al., 2016) that learns to switch between copying words from the document and generating words from a prescribed vocabulary.", "startOffset": 115, "endOffset": 138}, {"referenceID": 5, "context": "Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011; Firat et al., 2016).", "startOffset": 70, "endOffset": 114}, {"referenceID": 9, "context": "Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011; Firat et al., 2016).", "startOffset": 70, "endOffset": 114}, {"referenceID": 5, "context": "Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011; Firat et al., 2016). In machine translation, for instance, Firat et al. (2016) demonstrated that translation quality clearly improves over models trained with a single language pair when the attention mechanism in a neural translation model is shared and jointly trained on multiple language pairs.", "startOffset": 71, "endOffset": 174}, {"referenceID": 27, "context": "Many later studies adopted this boundary model and achieved nearhuman performance on the task (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 94, "endOffset": 150}, {"referenceID": 22, "context": "Many later studies adopted this boundary model and achieved nearhuman performance on the task (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 94, "endOffset": 150}, {"referenceID": 21, "context": "Many later studies adopted this boundary model and achieved nearhuman performance on the task (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 94, "endOffset": 150}, {"referenceID": 17, "context": "However, the boundarypointing mechanism is not suitable for more open-ended tasks, including abstractive QA (Nguyen et al., 2016) and question generation.", "startOffset": 108, "endOffset": 129}, {"referenceID": 27, "context": "While \u201cforcing\u201d the extractive boundary model onto abstractive datasets currently yields stateof-the-art results (Wang et al., 2017), this is mainly because current generative models are poor and NLG evaluation is unsolved.", "startOffset": 113, "endOffset": 132}, {"referenceID": 16, "context": "Partly to address this limitation, end-to-end-trainable neural models have recently been proposed for question generation in both vision (Mostafazadeh et al., 2016) and language.", "startOffset": 137, "endOffset": 164}, {"referenceID": 7, "context": "For example, Du et al. (2017) used a sequence-tosequence model with an attention mechanism derived from the encoder states.", "startOffset": 13, "endOffset": 30}, {"referenceID": 7, "context": "For example, Du et al. (2017) used a sequence-tosequence model with an attention mechanism derived from the encoder states. Yuan et al. (2017) proposed a similar architecture but in addition improved model performance through policy gradient techniques.", "startOffset": 13, "endOffset": 143}, {"referenceID": 6, "context": "(2017) devised a semi-supervised training framework that trained a QA model (Dhingra et al., 2016) on both labeled data and artificial data generated by a separate generative component.", "startOffset": 76, "endOffset": 98}, {"referenceID": 8, "context": "(2017) used policy gradient with a QA reward to train a sequence-to-sequence paraphrase model to reformulate questions in an existing QA dataset (Dunn et al., 2017).", "startOffset": 145, "endOffset": 164}, {"referenceID": 21, "context": "The generated questions were then used to further train an existing QA model (Seo et al., 2016).", "startOffset": 77, "endOffset": 95}, {"referenceID": 24, "context": "Yang et al. (2017) devised a semi-supervised training framework that trained a QA model (Dhingra et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Buck et al. (2017) used policy gradient with a QA reward to train a sequence-to-sequence paraphrase model to reformulate questions in an existing QA dataset (Dunn et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "Our proposed model adopts a sequence-to-sequence framework (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 1, "context": ", 2014) with an attention mechanism (Bahdanau et al., 2014) and a pointer-softmax decoder (Gulcehre et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 10, "context": ", 2014) and a pointer-softmax decoder (Gulcehre et al., 2016).", "startOffset": 38, "endOffset": 61}, {"referenceID": 10, "context": "The RNN-based decoder employs the pointer-softmax mechanism (Gulcehre et al., 2016).", "startOffset": 60, "endOffset": 83}, {"referenceID": 20, "context": "We conduct our experiments on the SQuAD corpus (Rajpurkar et al., 2016), a machine comprehension dataset consisting of over 100k crowd-sourced question-answer pairs on 536 Wikipedia articles.", "startOffset": 47, "endOffset": 71}, {"referenceID": 15, "context": "The categorization relies on Stanford CoreNLP (Manning et al., 2014) to generate constituency parses, POS tags, and NER tags for answer spans (see Rajpurkar et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 18, "context": "The word embedding matrix is initialized with the 300dimensional GloVe vectors (Pennington et al., 2014).", "startOffset": 79, "endOffset": 104}], "year": 2017, "abstractText": "We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model\u2019s novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking.", "creator": "LaTeX with hyperref package"}}}