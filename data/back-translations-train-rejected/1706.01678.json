{"id": "1706.01678", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Text Summarization using Abstract Meaning Representation", "abstract": "Summarization of large texts is still an open problem in language processing. In this work we develop a full fledged pipeline to generate summaries of news articles using the Abstract Meaning Representation(AMR). We first generate the AMR graphs of stories then extract summary graphs from the story graphs and finally generate sentences from the summary graph. For extracting summary AMRs from the story AMRs we use a two step process. First, we find important sentences from the text and then extract the summary AMRs from those selected sentences. We outperform the previous methods using AMR for summarization by more that 3 ROGUE-1 points. On the CNN-Dailymail corpus we achieve results competitive with the strong lead-3 baseline till summary graph extraction step.", "histories": [["v1", "Tue, 6 Jun 2017 10:04:45 GMT  (272kb,D)", "http://arxiv.org/abs/1706.01678v1", "10 pages, 4 figures"], ["v2", "Sun, 9 Jul 2017 20:48:14 GMT  (272kb,D)", "http://arxiv.org/abs/1706.01678v2", "10 pages, 4 figures"], ["v3", "Mon, 17 Jul 2017 21:27:44 GMT  (276kb,D)", "http://arxiv.org/abs/1706.01678v3", "10 pages, 4 figures, Update: Added more results , corrected figures and tables"]], "COMMENTS": "10 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shibhansh dohare", "harish karnick", "vivek gupta"], "accepted": false, "id": "1706.01678"}, "pdf": {"name": "1706.01678.pdf", "metadata": {"source": "CRF", "title": "Text Summarization using Abstract Meaning Representation", "authors": ["Shibhansh Dohare", "Harish Karnick"], "emails": ["sdohare@cse.iitk.ac.in", "hk@cse.iitk.ac.in"], "sections": [{"heading": null, "text": "Text Summarization using Abstract Meaning RepresentationShibhansh Dohare Department of CSEIndian Institute of Technology, Kanpur sdohare @ cse.iitk.ac.inHarish Karnick Department of CSEIndian Institute of Technology, Kanpur hk @ cse.iitk.ac.inAbstractSummarization of large texts is still an open problem in language processing. In this thesis, we develop a complete pipeline for creating summaries of news articles using the Abstract Meaning Representation (AMR). We first generate the AMR diagrams of stories, then extract summaries of diagrams from the story graphs, and finally sentences from the summary. To extract summary AMRs from the story AMRs, we use a two-step process. First, we find important sentences from the text and then extract the summary AMRs from these selected sentences. We surpass previous methods with AMR for summarizing more than 3 points UGUE-1."}, {"heading": "1 Introduction", "text": "It has a significant application for summarizing large texts such as stories, newspaper articles, and even larger texts such as books, where people simply do not have the time to read the full text. Existing methods for summarizing can generally be divided into two categories: Extractive methods that pick up words and sentences from the text. These methods are inherently limited in the sense that they can never generate summaries for large and complicated documentations that require reformulation of sentences and that contain information from the full text to generate summaries."}, {"heading": "2 AMR: Parsing and generation", "text": "AMR diagrams are based on rooted, directed diagrams marked with vertex and edge. Consider, for example, the sentence \"I looked carefully around me.\" The graphical representation generated with ARMICA Saphra and Lopez (2015) of the AMR parse of this sentence generated by the JAMR parser is shown above in Figure 1. The nodes in AMR are labeled with concepts as in the example above Animal represents a concept. Edges contain information about the relationships between the concepts. The topic here is the relationship between the node Boa and image. AMR relies heavily on propbank for semantic relationships (marginal labels). Concepts can also be run-01, where the index 01 represents the first sense of the word. More details about the AMR parser can be found as formulas formulas."}, {"heading": "3 Datasets", "text": "We used two sets of data for the task. First, the AMR Corpora - Knight et al. (2014) DEFT Phase 2 AMR annotation R1 AMR Bank. We use the section on the proxy report because it is the only one relevant for the task, it contains the gold standard AMRs for the news articles selected from the Gigaword Corpus and their summaries. The reports contain an average of 17.5 sentences and the summary is 1 or 2 sentences. The training and test sets contain an average of 298 and 33 summary pairs. Second, the CNN Dailymail corpus. This data set is better suited for the task as we need to create larger summaries of about 3 or 4 sentences. It has about 300k document summary pairs and the stories have an average of 39 sentences. The problem with this data set is that it is only sets and we do not have their gold standard AMRs. We use automatic parsers to obtain the AMRs, but it will automatically affect the quality of the gold generation so that we generate the final results."}, {"heading": "4 Pipeline", "text": "The pipeline consists of three steps, first converting all predefined narrative sentences into summary representations, followed by extracting a summary graph from the story graph, and finally generating sentences from these extracted summaries. In the following sections, we will explain each of the methods in more detail."}, {"heading": "4.1 Story to AMR", "text": "We use JAMR version 2 Flanigan et al. (2014) as an open-source version and have a performance that is close to state-of-the-art. We have the gold standard AMR parser for the AMR bank, but we are still analyzing the input stories to investigate the impact of using JAMR instead of the gold standard AMR. The cost of parsing the entire CNN / Dailymail corpus is enormous, the time taken for each 500 document summary pairs was about 36 hours. The machine we used had 24 CPU cores and 128 GB of RAM. We could only run 8 processes in parallel on this machine (the number of CPU cores was the bottleneck)."}, {"heading": "4.2 Story AMRs to Summary AMRs", "text": "After parsing (step 1) we have the AMR diagrams for the records of history. In this step we need the AMR diagrams of the summary records. We divide this task into two parts. First we have to find the important records from the history and then extract the key information from these set AMR diagrams."}, {"heading": "4.2.1 Selecting Important sentences", "text": "Our algorithm is based on the idea that only a few sentences are important from the standpoint of the summary sentence 79. i.e. there are few sentences that contain the most important information. and from these sentences we can generate the summary. Below, we give a justification for this hypothesis that we can use a few sentences to generate the summary sentence for news articles. The sentences of history (avg length 28.1 tokens) in the CNN-Dailymail corpus tend to be larger than the summary sentences (avg length 19.4 tokens) and most of the information contained in a summary sentence can be found in just one sentence from the story. To test this hypothesis, we will find maximum information for each summary sentence that contains the sentence from the story. We will use ROGUE-1 Lin (2004) Recall scores (measures the ratio of words in the target summary that are included in the predicted summary."}, {"heading": "4.2.2 Extracting summary graph", "text": "To extract summarizing graphs from the selected records, we use the structure of the AMR graphs. We observe that in an Abstract Meaning Diagram, the order of the attributes (children) of a node is important, and that only the first few attributes are important. This allows us to drop the attributes that come later, thus helping to extract the key information. We use this feature of the AMR to extract the key information from the set."}, {"heading": "4.3 Summary Generation", "text": "To generate sentences from the extracted AMR diagrams, we use already available generators Konstas et al. (2017) Flanigan et al. (2016). We use Flanigan et al. (2016). Generators can significantly influence the results, in the next section we analyze the effect of using generators. In order to analyze the effectiveness of each individual step, we need a method to evaluate the summary quality before generating. We use the ideas used by Liu et al. (2015) to evaluate the summary generation quality from the extracted AMR diagrams. We use the alignments provided in the AMR bank. Alignments simply map the words in the original sentence to the node or edge in the AMR diagram. To generate the summary quality, we find the words in the selected AMR diagram in line with the sentence and return them in no particular order than the predicted summary from the Uff. Although we cannot use the result from the Uff1, because we do not have a valid sequence from the AMR, the result is not generated by a valid E."}, {"heading": "5 Results and Analysis", "text": "We use the standard ROGUE metric to evaluate summaries. We specify recall values, accuracy, and F1 values for ROGUE-1, and only the F1 values for ROGUE-2 and ROGUE-L. ROGUE-1 recall and precision are measured when the reference values and predicted summaries overlap. ROGUE-2 uses Bigram overlaps, and ROGUE-L uses the longest common sequence between target and predicted summaries for evaluation."}, {"heading": "5.1 Results on the Proxy report section", "text": "We compare our various methods with the methods of Liu et al. (2015), which use the values of Rogue 1 Recall, Precision and F1. In AMR extraction, we dropped the children of the root node, who are neither among the two children nor associated with the root with the edge called: time. In LDA-based sentence selection, we use top 5 characteristics to select the sentence. Our first selection based on coexistence plus the first sentence exceeds the method of Liu et al. (2015) by about three and a half ROGUE-1 points. The results after the last generation step are in Table 2. The results clearly show that there is a significant decrease in the final results due to the generator, especially in ROGUE-1 Precision."}, {"heading": "5.2 Effectiveness of generator", "text": "Consider the rogue values of the best action set (on ROGUE-1 Recall, as in Section 4.3) as a summary for the two cases, first, if we use this set directly as a summary, and second, if we use the sentences generated from the gold standard AMR diagrams of this set as a summary. It is clear from the results that there is a significant decrease of about 15 rogue points in the R1 F1 result procedure. There may be several possible reasons for so many errors in the creation step. We are thinking of some possible reasons for generating errors. First, and probably \"the reason,\" that generators will never be able to obtain values that correspond to the original sentences on the ROGUE metric. AMR formalism tries to match the sentences that have the same meaning to the same AMR diagrams, so that there will be one-too many illustrations of AMR diagrams to set sets that correspond to the original sentences. Therefore, we may not attempt to use the automatic generators that have the same meaning, but to create the original sentence."}, {"heading": "5.3 Analyzing the effectiveness of extraction from AMR", "text": "To extract the AMR summary chart from the AMR chart set, we used the idea that important information with each AMR tree resides in some initial subtrees associated with the root. Measuring the effectiveness of extracting summary charts is not easy. We use the method used by Liu et al. (2015) to measure the quality of the generated summary after AMR extraction (see Section 4.3 for details of the evaluation at this step).The goal of extracting summary charts from the set charts is to omit the less important information from the sentences. If we are able to achieve this perfectly, the ROGUE-1 memory values we receive should remain almost the same (since we do not add any new information), and the ROGUE-1 precision should rise (since we have discarded some useless information from the sentences), and thus we achieve the ROGUE-1 overall GUE-1 GUI-1 GUI-1 GUI-1-1-1 GUI-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-2-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-"}, {"heading": "5.4 Results on the CNN-Dailymail corpus", "text": "In Table 4 we report on the results of the CNNDailymail corpus. We present the results up to Step 2 of the pipeline using the leading sentences as important sentences, we have used the non-anonymized data set. If we look at the results of the original Lead 3 baseline and the results up to Step 2 of the Lead 3 (full AMR), there is a decrease of about 5 ROGUE-1 points in the results, reason for such a significant decrease is the absence of stopwords in the summary created after Step 2. ROGUE considers stopwords such as \"a,\" \"that\" is, \"etc. as a summary assessment, while these words are not mapped during the AMR generation. Further, we discuss the problems with ROGUE in 6.2. The results we achieve after extracting from the leading 4 sentences compete with the Lead 3 baseline at Step 2 of the pipeline."}, {"heading": "6 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Comparing various abstractive and extractive methods with our approach", "text": "Most extractive methods tend to select some sentences and use them directly as a summary, so the target summary is not written in most cases. For the CNN-Dailymail corpus, the extractive state of the art is Nallapati et al. (2017) (SummaRunNer) directly selects sentences from the story for the summary. The story sentences in the corpus are about 1.5 times as long as the sentences in the story, so the story sentences are not true representatives of the summary sentences and therefore the extractive methods do not produce a summary that is similar to the target summary. In contrast to these methods, our method produces summary sentences of the size of the target summaries. The abstract methods for the summary suffer from the lack of generalization like all other supervised learning methods in the sense that they are not able to work well on the text outside the domain of the given corpus. In our method, since we do not use a learning method, we should be able to achieve similar results from messages that are largely different from those generated from a sentence."}, {"heading": "6.2 Need of an new dataset and evaluation metric", "text": "ROGUE metric, by it's design has many properties that a lot of important problem is unsuitable for the evaluation of multiple summary sentences. First, ROGUE matches exact words rather than the stems, it also takes stopwords into account for the evaluation. One of the reasons why ROGUE like metrics might never be suitable for the evaluation of abstract summaries is its inability to know whether the sentences have been restructured. A good evaluation metric should be one in which we compare the meaning of the sentence and not the exact words. We will now show why the CNN-Dailymail corpus is not suitable for an abstract summary. The nature of the summary points in the corpus is highly extractive (Section 4.2.1 for details), in which most summary points are simply taken from a few sentences in history. Tough, this is a good enough reason to start searching for better dataset, it's not the biggest problem with the dataset metaset."}, {"heading": "6.3 Possible future directions", "text": "Since this proposed algorithm is a step-by-step process, we can focus on improving each step to achieve better results. The parser and generator are not perfect and therefore introduce errors in the pipeline. The JAMR parser is close to the state of the art, while the generator we use is not close to the state of the art. Many of the recent work has been invested in developing better generators specifically Konstas et al. (2017) (Neural AMR), which use better generators, should directly improve the quality of the final generated summary. The most exciting improvements can be made in the methods for extracting short diagrams. It is the step that is completely new and not much work has been done to extract AMRs for the summary. One of the possibilities may be to learn linearized short diagrams from the linearized history of AMRs. To make this pipeline generalizable for any type of text, we will have to make the AMRs hypothesis similar to the one that the AMRs must be extracted from."}, {"heading": "7 Conclusion", "text": "We show that our methods for AMR extraction are better than those of Liu et al. (2015). We also observed the problems with the existing long-text data set and the evaluation metric for the summary."}, {"heading": "A Supplemental Material", "text": "The following appendix contains a detailed description of the AMR extraction step (Section 4.2.2). Figure 3 shows the AMR analysis of the gold standard AMR diagram of the sentence \"Absurd as it may seem to me, a thousand miles from any human habitation and in mortal danger, I took out of my pocket a sheet of paper and my fountain pen\" from the Corpus del Corpus del Little Prince. Figure 4 shows how we shorten the diagram, here we have retained only the first two arguments of the root knot and dropped the remaining lower trees."}], "references": [{"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Martha Palmer", "Philipp Koehn", "Nathan Schneider."], "venue": "Pro-", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "Journal of Machine Learning Research, 3:9931022. http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Generation from abstract meaning representation using tree transducers", "author": ["Jeffrey Flanigan", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of", "citeRegEx": "Flanigan et al\\.,? 2016", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Deft phase 2 amr annotation r1 ldc2015e86", "author": ["Kevin Knight", "Laura Baranescu", "Claire Bonial", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Daniel Marcu", "Martha Palmer", "Nathan Schneider."], "venue": "philadelphia: Linguistic data consor-", "citeRegEx": "Knight et al\\.,? 2014", "shortCiteRegEx": "Knight et al\\.", "year": 2014}, {"title": "Neural amr: Sequence-to-sequence models for parsing and generation", "author": ["Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Konstas et al\\.,? 2017", "shortCiteRegEx": "Konstas et al\\.", "year": 2017}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C. Lin."], "venue": "Text Summarization Branches Out, Post-Conference Workshop of ACL 2004. Barcelona, Spain.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Toward abstractive summarization using semantic representations", "author": ["Fei Liu", "Flanigan Jeffrey", "Thomson Sam", "Sadeh Norman", "Smith Noah A."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Compu-", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents", "author": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."], "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17).", "citeRegEx": "Nallapati et al\\.,? 2017", "shortCiteRegEx": "Nallapati et al\\.", "year": 2017}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "a glar G ulehre", "Bing Xiang"], "venue": "In Computational Natural Language Learning", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Addressing the data sparsity issue in neural amr parsing", "author": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. Association for", "citeRegEx": "Peng et al\\.,? 2017", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Amrica: an amr inspector for cross-language alignments", "author": ["Naomi Saphra", "Adam Lopez."], "venue": "System Demonstrations of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Saphra and Lopez.,? 2015", "shortCiteRegEx": "Saphra and Lopez.", "year": 2015}, {"title": "Get to the point: Summarization with pointer-generator networks", "author": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."], "venue": "http://aclweb.org/anthology/P16-1001.", "citeRegEx": "See et al\\.,? 2017", "shortCiteRegEx": "See et al\\.", "year": 2017}, {"title": "Amr-to-text generation as a traveling salesman problem", "author": ["Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Song et al\\.,? 2016", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Camr at semeval-2016 task 8: An extended transition-based amr parser", "author": ["Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Compuational", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Amr parsing with an incremental joint model", "author": ["Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Abstract meaning Representation (AMR) as introduced by Banarescu et al. (2013). It focuses on capturing the meaning of the text, by giving a specific meaning representation to the text.", "startOffset": 55, "endOffset": 79}, {"referenceID": 7, "context": "The work on summarization using ARMs was started by Liu et al. (2015). The idea is to generate AMR representation of the story sentences.", "startOffset": 52, "endOffset": 70}, {"referenceID": 6, "context": "First we complete the pipeline to generate summary sentences from the story sentences using AMR as an intermediate representation. Second, we develop a novel algorithm to extract summary AMR graphs from the document AMR graphs which outperforms Liu et al. (2015)\u2019s method to ar X iv :1 70 6.", "startOffset": 26, "endOffset": 263}, {"referenceID": 8, "context": "generate summary AMR graphs and finally we explain the problems with the the ROGUE evaluation metric, the CNN-Dailymail corpus Nallapati et al. (2016) and existing methods used for its summarization.", "startOffset": 127, "endOffset": 151}, {"referenceID": 9, "context": "The graphic representation as generated using ARMICA Saphra and Lopez (2015) of the AMR parse of this sentence generated by JAMR parser is given above in Figure 1.", "startOffset": 53, "endOffset": 77}, {"referenceID": 0, "context": "Further details about the AMR can be found in the AMR guidelines Banarescu et al. (2015).", "startOffset": 65, "endOffset": 89}, {"referenceID": 2, "context": "There is Alignment based parsing Flanigan et al. (2014) (JAMR), Zhou et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 2, "context": "There is Alignment based parsing Flanigan et al. (2014) (JAMR), Zhou et al. (2016) which uses graph based algorithms for concept and relation identification.", "startOffset": 33, "endOffset": 83}, {"referenceID": 2, "context": "There is Alignment based parsing Flanigan et al. (2014) (JAMR), Zhou et al. (2016) which uses graph based algorithms for concept and relation identification. Grammar based parsers like Wang et al. (2016) (CAMR) generate output by performing shift reduce transformations on output of a dependency parser.", "startOffset": 33, "endOffset": 204}, {"referenceID": 5, "context": "parsing Konstas et al. (2017); Peng et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 5, "context": "parsing Konstas et al. (2017); Peng et al. (2017) is based on using seq2seq models for parsing, the main problem in using neural methods for parsing is the absence of a huge corpus of human generated AMRs.", "startOffset": 8, "endOffset": 50}, {"referenceID": 5, "context": "parsing Konstas et al. (2017); Peng et al. (2017) is based on using seq2seq models for parsing, the main problem in using neural methods for parsing is the absence of a huge corpus of human generated AMRs. Peng et al. (2017) reduced the vo-", "startOffset": 8, "endOffset": 225}, {"referenceID": 5, "context": "cabulary size to tackle this problem while Konstas et al. (2017) used larger external corpus of external sentences.", "startOffset": 43, "endOffset": 65}, {"referenceID": 11, "context": "Song et al. (2016) reformed the problem as a traveling salesman problem.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Konstas et al. (2017) used seq2seq learning methods.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "First is the AMR Corpora - Knight et al. (2014) DEFT Phase 2 AMR Annotation R1 AMR Bank.", "startOffset": 27, "endOffset": 48}, {"referenceID": 2, "context": "We use JAMR version 2 Flanigan et al. (2014) as its openly available and has a performance close to", "startOffset": 22, "endOffset": 45}, {"referenceID": 6, "context": "We use ROGUE-1 Lin (2004) Recall scores (measures the ratio of words in the target summary that are contained in the predicted summary to the total number of words in the target summary) as the metric for the information contained in the story sentence.", "startOffset": 15, "endOffset": 26}, {"referenceID": 1, "context": "Then we use Latent Dirichlet Allocation (LDA) Blei et al. (2003) to find the key feature words.", "startOffset": 46, "endOffset": 65}, {"referenceID": 3, "context": "To generate sentences from the extracted AMR graphs we use already available generators Konstas et al. (2017) Flanigan et al.", "startOffset": 88, "endOffset": 110}, {"referenceID": 2, "context": "(2017) Flanigan et al. (2016). We use", "startOffset": 7, "endOffset": 30}, {"referenceID": 7, "context": "We use the ideas used by Liu et al. (2015) for the evaluation of the summary generation quality from the extracted AMR graphs.", "startOffset": 25, "endOffset": 43}, {"referenceID": 6, "context": "In the next section we give the results produced by the full pipeline on the gold standard section. To compare our method with Liu et al. (2015) method we compare our results after step 2 of the pipeline, we\u2019ve extracted the AMR from the sentence AMRs.", "startOffset": 65, "endOffset": 145}, {"referenceID": 6, "context": "In table 1 we report the results of using the pipeline upto step 2 on the proxy report section of the AMR Bank. We compare our various methods with Liu et al. (2015)\u2019s methods using the Rogue 1 Recall, Precision and F1 scores.", "startOffset": 50, "endOffset": 166}, {"referenceID": 7, "context": "Our first co-occurrence based sentence selection plus first the sentence outperforms Liu et al. (2015)\u2019s method by around three and a half ROGUE-1 points.", "startOffset": 85, "endOffset": 103}, {"referenceID": 7, "context": "Method Rogue-1 Recall Rogue-1 Precision Rogue-1 F1 Liu et al. (2015) 51.", "startOffset": 51, "endOffset": 69}, {"referenceID": 7, "context": "We employ the method used by Liu et al. (2015) to measure the quality of generated summary after AMR extraction (refer to section 4.", "startOffset": 29, "endOffset": 47}, {"referenceID": 9, "context": "abstractive model (Nallapati et al., 2016) - 35.", "startOffset": 18, "endOffset": 42}, {"referenceID": 12, "context": "65 pointer-generator + coverage (See et al., 2017) - 39.", "startOffset": 32, "endOffset": 50}, {"referenceID": 12, "context": "lead-3 baseline (original) (See et al., 2017) - 40.", "startOffset": 27, "endOffset": 45}, {"referenceID": 8, "context": "57 lead-3 baseline (anonymized) (Nallapati et al., 2017)* - 39.", "startOffset": 32, "endOffset": 56}, {"referenceID": 8, "context": "5 SummaRuNner (Nallapati et al., 2017)* - 39.", "startOffset": 14, "endOffset": 38}, {"referenceID": 8, "context": "For the CNN-Dailymail corpus the extractive current state of the art Nallapati et al. (2017)(SummaRunNer) directly picks sentences from the story for the summary.", "startOffset": 69, "endOffset": 93}, {"referenceID": 12, "context": "As the point raised by See et al. (2017) the choice of the reference summaries is highly subjective, there are multiple possible ways to choose the summary points and the summary points in the corpus might not be the best possible summary.", "startOffset": 23, "endOffset": 41}, {"referenceID": 5, "context": "A lot of recent work has gone into developing better generators specifically Konstas et al. (2017) (Neural AMR) using better generators should directly improve the quality of the final generated summary.", "startOffset": 77, "endOffset": 99}, {"referenceID": 7, "context": "used by Liu et al. (2015). We also observed the problems with the existing long text dataset and the evaluation metric for summarization.", "startOffset": 8, "endOffset": 26}], "year": 2017, "abstractText": "Summarization of large texts is still an open problem in language processing. In this work we develop a full fledged pipeline to generate summaries of news articles using the Abstract Meaning Representation(AMR). We first generate the AMR graphs of stories then extract summary graphs from the story graphs and finally generate sentences from the summary graph. For extracting summary AMRs from the story AMRs we use a two step process. First, we find important sentences from the text and then extract the summary AMRs from those selected sentences. We outperform the previous methods using AMR for summarization by more that 3 ROGUE-1 points. On the CNN-Dailymail corpus we achieve results competitive with the strong lead-3 baseline till summary graph extraction step.", "creator": "LaTeX with hyperref package"}}}