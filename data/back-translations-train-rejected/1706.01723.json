{"id": "1706.01723", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "A General-Purpose Tagger with Convolutional Neural Networks", "abstract": "We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks: without task-specific tuning of hyper-parameters, it achieves state-of-the-art results in part-of-speech tagging, morphological tagging and supertagging. The CNN tagger is also robust against the out-of-vocabulary problem, it performs well on artificially unnormalized texts.", "histories": [["v1", "Tue, 6 Jun 2017 12:11:50 GMT  (48kb,D)", "http://arxiv.org/abs/1706.01723v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiang yu", "agnieszka fale\\'nska", "ngoc thang vu"], "accepted": false, "id": "1706.01723"}, "pdf": {"name": "1706.01723.pdf", "metadata": {"source": "CRF", "title": "A General-Purpose Tagger with Convolutional Neural Networks", "authors": ["Xiang Yu", "Ngoc Thang Vu"], "emails": ["xiangyu@ims.uni-stuttgart.de", "falensaa@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "Recently, character composition models have shown great success in many NLP tasks, mainly due to their robustness in handling vocabulary (OOV) words by capturing subword information. Among character composition models, bidirectional long-term memory (LSTM) models and Convolutionary Neural Networks (CNN) are widely used in many tasks, e.g. part-time work (POS) tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimara, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-jussa and Fonollosa, 2016) and dependency sparing (Ballesteros et al., 2015; Yu and Vu, 2017).In this paper we present a state-of-the-art general-purpose NLP workshop exemplary, the CNC."}, {"heading": "2 Model", "text": "Our proposed CNN tagger has two main components: the character composition model and the context encoding model. Both components are essentially CNN models that capture different levels of information: the first CNN captures morphological information from character n-grams, the second captures contextual information from word n-grams. Figure 1 shows a diagram of both models of the tagger.ar Xiv: 170 6.01 723v 1 [cs.C L] 6J un2 017"}, {"heading": "2.1 Character Composition Model", "text": "The character composition model is similar to that of Yu and Vu (2017), where multiple folding filters are used to capture characters of different sizes; the outputs of each folding filter are fed through a maximum pooling layer, and the pooling outputs are concatenated to represent the word."}, {"heading": "2.2 Context Encoding Model", "text": "The context encoding model captures the context information of the target word by scanning through the word representations of its context window. Word representation can be only word embedding (~ w), only compound vectors (~ c), or concatenation of the two (~ w + ~ c). A context window consists of N words on both sides of the target word and the target word itself. To display the target word, we associate a binary attribute with 1 for each of the word representations, indicating the destination, and 0 otherwise, similar to Vu et al. (2016). In addition to the binary attribute, we also associate a position that encodes the relative position of each context word, similar to Gehring et al. (2017)."}, {"heading": "2.3 Hyper-parameters", "text": "We use four folding filters with sizes 3, 5, 7 and 9. Each filter has an output channel of 25 dimensions, so the composite vector is 100-dimensional. We apply Gaussian noise with a standard deviation of 0.1, which is applied to the composite vector during the training. For the context encoding model, we take a context window of 15 (7 words on both sides of the target word) as input and predict the day of the target word. We also apply four folding filters with sizes 2, 3, 4 and 5, each filter being stacked by a different filter of the same size and the output having 128 dimensions, so that the context representation is 512-dimensional. We apply a 512-dimensional hidden layer with ReLU nonlinearity in front of the forecast layer."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data", "text": "We use treebanks from version 1.2 of Universal Dependencies2 (UD), and in the case of multiple treebanks for one language we use only the canonical treebank. There are a total of 22 treebanks, as in Plank et al. (2016).3 Each treebank is divided into train, dev and test sets, we use the dev sets for the early stop and test on the test sets."}, {"heading": "3.2 Tasks", "text": "We evaluate our method using three tagging tasks: POS tagging (POS), morphological tagging (MORPH) and supertagging (STAG).For POS tagging, we use universal POS tags, which are an extension of Petrov et al. (2012).The universal tag sentences try to capture the \"universal\" characteristics of words and facilitate cross-linguistic learning, so the tag sentence is very rough and ignores most of the language-specific characteristics of morphological traits. Morphological tags encode the language-specific morphological characteristics of words, e.g. number, gender, case. They are presented in the UD tree banks as a string containing several key value pairs of morphological traits. 4Supertags (Joshi and Bangalore, 1994) are tags that encode more syntactical information than standard POS tags, e.g. the head seal or the subcategory frame."}, {"heading": "3.3 Setups", "text": "We base our models on the two art markers MarMot5 (referred to as CRF) and bilstm-aux6 (referred to as LSTM). We train the markers with the recommended hyperparameters from the documentation. To ensure a fair comparison (especially between LSTM and CNN), we generally treat the three tasks the same and do not use task-specific settings on them, i.e. we use the same characteristics and the same model hyperparameters in each task. In addition, we do not use pre-trained word embeddings. For the LSTM markers, we use the recommended hyperparameters in documentation7, including 64-dimensional word embeddings (~ w) and 100-dimensional composed vectors (~ c). We educate the markers, ~ w + ~ c + ~ c + ~ c markers, as in Plank et al. (2016). We educate the CNN markers with the same dimensions for the Marx5 and CRM (referred to as STM)."}, {"heading": "3.4 Results", "text": "The test results for the three tasks are presented in Table 1 in three groups.The first group of seven columns are the results for POS, where both LSTM and CNN have three variations of input functions: word only (~ w), characters only (~ c), and both (~ w + ~ c).For MORPH and STAG, we use only the setting ~ w + ~ c for both LSTM and CNN.In the macro average, three taggers cut close to the POS task, with the CNN tagger slightly outperforming.In the MORPH task, CNN is slightly ahead of CRF again, while LSTM is about two points behind the task. In the STAG task, CNN outperforms both taggers by a wide margin: 2 points higher than LSTM and 8 points higher than CRF. Taking into account the input functions of the LSTM taggers and CNN taggers, both taggers are close to the input of only ~ w, indicating that the two taggers are comparable in context."}, {"heading": "3.5 Unnormalized Text", "text": "Unfortunately, we do not have social media data to test the markers. However, we are designing an experiment to simulate unnormalized text by systematically editing the words in the dev sets with three operations: Insert, Delete, and Replace. For example, if we change a word abcdef to position 2 (0-based), the modified words in the dev sets would be abxcdef, abdef, and abxdef, where x is a random character from the alphabet of the language. For each operation, we create a group of modified dev sets in which all words that are longer than two characters would be subject to the operation with a probability of 0.25, 0.5, 0.75, or 1. For each language, we use the models trained on the normal training sets, and predict POS for the three groups of modified dev sets. The average accuracy in Figure CR4 indicates that the words are unnormalized in most cases, with all 4 points almost normal."}, {"heading": "100 insertion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Conclusion", "text": "In this post, we propose a universal tagger that uses two CNNs for both character composition and context encoding. On the universal dependency treebanks (v1.2), the tagger achieves state-of-the-art results for POS tagging and morphological tagging, and to the best of its knowledge is also best suited for supertagging. It works well across various tagging tasks without optimizing hyperparameters, and is also robust against unnormalized text."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "A. Noah Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["R. Marta Costa-juss\u00e0", "R. Jos\u00e9 A. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics,", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Cicero dos Santos", "Victor Guimar\u00e3es."], "venue": "Proceedings of the Fifth Named Entity Workshop. Association for Computational Linguistics, pages 25\u201333.", "citeRegEx": "Santos and Guimar\u00e3es.,? 2015", "shortCiteRegEx": "Santos and Guimar\u00e3es.", "year": 2015}, {"title": "Learning character-level representations for part-ofspeech tagging", "author": ["Cicero dos Santos", "Bianca Zadrozny."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14). pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Stacking or supertagging for dependency parsing \u2013 what\u2019s the difference? In Proceedings of the 14th International Conference on Parsing Technologies", "author": ["Agnieszka Fale\u0144ska", "Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Wolfgang Seeker."], "venue": "Association", "citeRegEx": "Fale\u0144ska et al\\.,? 2015", "shortCiteRegEx": "Fale\u0144ska et al\\.", "year": 2015}, {"title": "Guiding a constraint dependency parser with supertags", "author": ["Kilian A. Foth", "Tomas By", "Wolfgang Menzel."], "venue": "ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association", "citeRegEx": "Foth et al\\.,? 2006", "shortCiteRegEx": "Foth et al\\.", "year": 2006}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin."], "venue": "arXiv preprint arXiv:1705.03122 .", "citeRegEx": "Gehring et al\\.,? 2017", "shortCiteRegEx": "Gehring et al\\.", "year": 2017}, {"title": "Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing", "author": ["Aravind K. Joshi", "Srinivas Bangalore."], "venue": "Proceedings of the 15th Conference on Computational Linguistics Volume 1. Association for Computational Linguis-", "citeRegEx": "Joshi and Bangalore.,? 1994", "shortCiteRegEx": "Joshi and Bangalore.", "year": 1994}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proceedings of the Thirtieth AAAI", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "W. Alan Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceed-", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Efficient Higher-Order CRFs for Morphological Tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "In Proceedings of EMNLP.", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Improving Dependency Parsers with Supertags", "author": ["Hiroki Ouchi", "Kevin Duh", "Yuji Matsumoto."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2:", "citeRegEx": "Ouchi et al\\.,? 2014", "shortCiteRegEx": "Ouchi et al\\.", "year": 2014}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912). European Language Resources Association (ELRA), Is-", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Combining recurrent and convolutional neural networks for relation classification", "author": ["Thang Ngoc Vu", "Heike Adel", "Pankaj Gupta", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Vu et al\\.,? 2016", "shortCiteRegEx": "Vu et al\\.", "year": 2016}, {"title": "Character composition model with convolutional neural networks for dependency parsing on morphologically rich languages", "author": ["Xiang Yu", "Ngoc Thang Vu."], "venue": "arXiv preprint arXiv:1705.10814 .", "citeRegEx": "Yu and Vu.,? 2017", "shortCiteRegEx": "Yu and Vu.", "year": 2017}], "referenceMentions": [{"referenceID": 13, "context": "part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimar\u00e3es, 2015), language modeling (Ling et al.", "startOffset": 29, "endOffset": 80}, {"referenceID": 9, "context": ", 2016), named entity recognition (dos Santos and Guimar\u00e3es, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-juss\u00e0 and Fonollosa, 2016) and dependency parsing (Ballesteros et al.", "startOffset": 86, "endOffset": 123}, {"referenceID": 8, "context": ", 2016), named entity recognition (dos Santos and Guimar\u00e3es, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-juss\u00e0 and Fonollosa, 2016) and dependency parsing (Ballesteros et al.", "startOffset": 86, "endOffset": 123}, {"referenceID": 1, "context": ", 2016), machine translation (Costa-juss\u00e0 and Fonollosa, 2016) and dependency parsing (Ballesteros et al.", "startOffset": 29, "endOffset": 62}, {"referenceID": 0, "context": ", 2016), machine translation (Costa-juss\u00e0 and Fonollosa, 2016) and dependency parsing (Ballesteros et al., 2015; Yu and Vu, 2017).", "startOffset": 86, "endOffset": 129}, {"referenceID": 15, "context": ", 2016), machine translation (Costa-juss\u00e0 and Fonollosa, 2016) and dependency parsing (Ballesteros et al., 2015; Yu and Vu, 2017).", "startOffset": 86, "endOffset": 129}, {"referenceID": 15, "context": "Yu and Vu (2017) compared the performance of CNN and LSTM as character composition model for dependency parsing, and concluded that CNN performs better than LSTM.", "startOffset": 0, "endOffset": 17}, {"referenceID": 13, "context": "In these three tagging tasks, we compare our tagger with the bilstm-aux tagger (Plank et al., 2016) and the CRF-based morphological tagger MarMot (M\u00fcller et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 10, "context": ", 2016) and the CRF-based morphological tagger MarMot (M\u00fcller et al., 2013).", "startOffset": 54, "endOffset": 75}, {"referenceID": 15, "context": "The character composition model is similar to Yu and Vu (2017), where several convolution filters are used to capture character n-grams of different sizes.", "startOffset": 46, "endOffset": 63}, {"referenceID": 13, "context": "nary feature to each of the word representations with 1 indicating the target and 0 otherwise, similar to Vu et al. (2016). Additional to the binary feature, we also concatenate a position embedding to encode the relative position of each context word, similar to Gehring et al.", "startOffset": 106, "endOffset": 123}, {"referenceID": 6, "context": "Additional to the binary feature, we also concatenate a position embedding to encode the relative position of each context word, similar to Gehring et al. (2017).", "startOffset": 140, "endOffset": 162}, {"referenceID": 13, "context": "There are in total 22 treebanks, as in Plank et al. (2016).3 Each treebank splits into train, dev, and test sets, we use the dev sets for early stop, and test on the test sets.", "startOffset": 39, "endOffset": 59}, {"referenceID": 12, "context": "For POS tagging we use Universal POS tags, which is an extension of Petrov et al. (2012). The", "startOffset": 68, "endOffset": 89}, {"referenceID": 7, "context": "4 Supertags (Joshi and Bangalore, 1994) are tags that encode more syntactic information than standard POS tags, e.", "startOffset": 12, "endOffset": 39}, {"referenceID": 5, "context": "We use dependency-based supertags (Foth et al., 2006) which are extracted from the dependency treebanks.", "startOffset": 34, "endOffset": 53}, {"referenceID": 11, "context": "Adding such tags into feature models of statistical dependency parsers significantly improves their performance (Ouchi et al., 2014; Fale\u0144ska et al., 2015).", "startOffset": 112, "endOffset": 155}, {"referenceID": 4, "context": "Adding such tags into feature models of statistical dependency parsers significantly improves their performance (Ouchi et al., 2014; Fale\u0144ska et al., 2015).", "startOffset": 112, "endOffset": 155}, {"referenceID": 13, "context": "org We use all training data for Czech, while Plank et al. (2016) only use a subset.", "startOffset": 46, "endOffset": 66}, {"referenceID": 11, "context": "We use the standard Model 1 from Ouchi et al. (2014), where each tag consists of head direction, dependency label and dependent direction.", "startOffset": 33, "endOffset": 53}, {"referenceID": 13, "context": "~ w, ~c and ~ w + ~c models as in Plank et al. (2016). We train the CNN taggers with the same dimensionalities for word representations.", "startOffset": 34, "endOffset": 54}, {"referenceID": 10, "context": "For the CRF tagger, we predict POS and MORPH jointly as in the standard setting for MarMot, which performs much better than with separate predictions, as shown in M\u00fcller et al. (2013) and in our preliminary experiments.", "startOffset": 163, "endOffset": 184}], "year": 2017, "abstractText": "We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks: without task-specific tuning of hyper-parameters, it achieves state-of-theart results in part-of-speech tagging, morphological tagging and supertagging. The CNN tagger is also robust against the outof-vocabulary problem, it performs well on artificially unnormalized texts.", "creator": "LaTeX with hyperref package"}}}