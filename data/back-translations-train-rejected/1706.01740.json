{"id": "1706.01740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Label-Dependencies Aware Recurrent Neural Networks", "abstract": "In the last few years, Recurrent Neural Networks (RNNs) have proved effective on several NLP tasks. Despite such great success, their ability to model \\emph{sequence labeling} is still limited. This lead research toward solutions where RNNs are combined with models which already proved effective in this domain, such as CRFs. In this work we propose a solution far simpler but very effective: an evolution of the simple Jordan RNN, where labels are re-injected as input into the network, and converted into embeddings, in the same way as words. We compare this RNN variant to all the other RNN models, Elman and Jordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language Understanding (SLU). Thanks to label embeddings and their combination at the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNNs, but far fewer than LSTM and GRU, is more effective than other RNNs, but also outperforms sophisticated CRF models.", "histories": [["v1", "Tue, 6 Jun 2017 13:10:49 GMT  (132kb,D)", "http://arxiv.org/abs/1706.01740v1", "22 pages, 3 figures. Accepted at CICling 2017 conference. Best Verifiability, Reproducibility, and Working Description award"]], "COMMENTS": "22 pages, 3 figures. Accepted at CICling 2017 conference. Best Verifiability, Reproducibility, and Working Description award", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yoann dupont", "marco dinarelli", "isabelle tellier"], "accepted": false, "id": "1706.01740"}, "pdf": {"name": "1706.01740.pdf", "metadata": {"source": "CRF", "title": "Label-Dependencies Aware Recurrent Neural Networks", "authors": ["Yoann Dupont", "Marco Dinarelli", "Isabelle Tellier"], "emails": ["yoa.dupont@gmail.com,", "marco.dinarelli@ens.fr,", "isabelle.tellier@univ-paris3.fr"], "sections": [{"heading": null, "text": "In this work, we propose a solution that is much simpler, but very effective: an evolution of the simple Jordanian RNN, in which labels are injected into the network as input and converted into embedding in the same way as words. We compare this RNN variant with all other RNN models, Elman and Jordan RNN, LSTM and GRU, in two well-known spoken language understanding tasks (SLU). Thanks to the embedding of labels and their combination on the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNN, but far less than LSTM and GRU, is more effective than other RNN, but also better than sophisticated CRF models."}, {"heading": "1 Introduction", "text": "Over the past few years, the difference between these two models has been evident in the way two layers are joined. [1, 2, 3] This layer has proven to be very effective in several areas of language processing. (NLP) tasks such as part-of-speech tagging (POS tagging), chunking, named entity recognition (NER), terminology (SLU), machine translation and even more. (4, 5, 7, 8, 9, 10] These models are particularly effective thanks to their recursive architecture, which allows information to be retained in the past and reused at the current processing stage.) Xiv: 170 6.01 740v In the literature of RNNNN, several architectures have been proposed. In the first place Elman and Jordan RNs, introduced in [2, 1], and also known as simple RNNNNs."}, {"heading": "2 Recurrent Neural Networks (RNNs)", "text": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most advanced RNNNs such as LSTM and GRU [3, 12]. We also describe training and inference procedures and the RNN variant that we propose."}, {"heading": "2.1 Elman and Jordan RNNs", "text": "The difference between these two models is an activation function, H, O and R are the parameters on the hidden, output and recurrent layer, respectively (distortion is omitted) the equations easier to keep. hElmant \u2212 1 is the hidden layer of activity calculated in previous steps and used as context in the Elman RNN, while yt \u2212 1 is used as context for the predicted labels used as context in the predicted labels."}, {"heading": "2.3 LD-RNN : Label-Dependencies Aware Recurrent Neural Networks", "text": "In fact, it is as if most people are able to put themselves and themselves in the centre, and that they are able to put themselves and themselves in the centre of attention. (...) In fact, it is as if people are able to put themselves in the centre. (...) In fact, it is as if they are able to put themselves and themselves in the centre. (...) It is as if they are able to put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre."}, {"heading": "2.4 Learning and Inference", "text": "In fact, it is a very strange development that has come to a head in recent years. (...) In fact, most people who are able to move are able to move. (...) It is as if they were able to move. (...) It is as if they were able to move. (...) It is as if they were able to move. (...) It is as if they were able to move. (...) It is as if they were able to move. (...) It is as if they are able. (...) (...) It is as if they are able. (...) It is as if they are able. (...) It is as if they are able. (...) It is as if they are able. (...) It is as if they are able. (...) It is as if they are able. (...) It is as if they are. (...) It is as if they are able. (...)."}, {"heading": "2.5 Toward More Sophisticated Networks: Character-Level Convolution", "text": "Although word embedding offers a very fine encoding of word characteristics, several works such as [13, 33, 14, 15] have shown that more effective models can be achieved with a folding layer over word characteristics. Indeed, character-level information is very useful to enable a model that goes beyond rare flexed surface shapes and even beyond vocabulary. Word embedding is actually much less effective in such cases. The folding layer over word characters also has the advantage of being very general: it can be applied in the same way to different languages, allowing the reuse of the same system to different languages and tasks. In this paper, we focus on a folding layer that is used for words in [7]. For each word w of length | w, we define Ech (w, i) as embedding the character i of the word."}, {"heading": "2.6 RNN Complexities", "text": "Improved modeling of label dependencies in these papers is the only option we have. (In this section, we provide the number of labels in relation to the number of labels in relation to the number of labels; N is the number of labels; N is the embedding size in which we use the same size for word and label; dw is the window size used for the context words; and dl is the number of labels we use as context in LD-RNN. We analyze the hidden layer of all nets, and the embedding layer for LD-RNN. The other layers are exactly the same nets described in this paper.For the labels, we use the hidden layer of all nets, and the embedding layer for LD-RNN."}, {"heading": "3 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Corpora for Spoken Language Understanding", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "3.2 Settings", "text": "In fact, it is the case that most of us are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they are able to move, are able to"}, {"heading": "3.3 Results", "text": "All results shown in this section are averages over 6 runs. Embedding was learned once for all experiments."}, {"heading": "3.3.1 Incremental Results with Different Level of Information", "text": "In this section, we describe results obtained with incremental levels of information given to the models as input: i) Only words (previous labels are always given as input) given with words in the tables; ii) Words and classes Words + classes; iii) Words and character convolution Words + CC; iv) All possible inputs Words + classes + CC. The results of the ATIS task are presented in Table 3, the results on MEDIA are in Table 4.The results in these tables show that the models exhibit similar behavior in the two tasks. In particular, ATIS improves the addition of the different levels of information step by step and the best performance is achieved by integrating words, labels and character conversion, although some of the improvements do not appear statistically significant, taking into account the small size of this corpus.This observation is confirmed by the results on MEDIA, where the addition of the character level conversion leads to a slight degradation of performance. To understand the reason for this, we did not base the CC on the first phase of the training, but rather the two we analyzed the CC."}, {"heading": "3.3.2 Comparison with the State-of-the-Art", "text": "In this section, we compare our results with the results we have found in the literature. To be fair, the comparison is made on the basis of the same input information: words and classes. In the tables we use, it is very difficult to get an idea of how our RNN variant works. In practice, we behave in a different way with respect to LSTM + CRF models used in the past. We have conducted an experiment with Penn Treebank [41], with a similar data pre-processing that uses exactly the same data that use a signature activation function, and only use words such as the LSTM + CRF variant, which reaches an accuracy of 96.83, which is comparable to the 96.9 models of the LSTM + CRF models we have achieved in the past."}, {"heading": "3.4 Results Discussion", "text": "In order to understand the high performance of the LD-RNN variant in relation to the MEDIA task, we have made some simple analyses of the results of the model, comparing them to the output of a Jordan RNN trained with our own system in the same conditions as the LD-RNN models. The main difference between these two models is the general tendency of the Jordan RNN to divide a single concept into two or more concepts, especially for concepts triggered by long surface shapes such as command lines. This concept is used to move the general user in a dialogue (e.g. hotel reservations, pricing information, etc.) The Jordan RNN divides this concept into several concepts by introducing a blank label associated with a stop word. This is due to the limitation of this model in order to obtain the correct labels."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed a new variant of RNN for sequence labeling that, in addition to the word context, uses a broad context of label embedding to predict the next label in a sequence. We considered our variant to be more effective in modeling label dependencies. The results of two spoken language understanding tasks show that i) our variant performs the same performance on a simple task as ATIS as much more complex models such as LSTM and GRU, which are considered the most effective RNNNs; ii) in the MEDIA task, where the modeling of label dependencies is critical, our variant outperforms all other RNNNs, including LSTM and GRU, by a wide margin. Compared to the best models in the literature regarding the Concept Error Rate (CER), our RNN variant results are more effective and reach a state of the art of 10.09."}, {"heading": "5 Acknowledgements", "text": "This work was partly financed by the French ANR project Democratic ANR-15CE38-0008."}], "references": [{"title": "Serial order: A parallel, distributed processing approach", "author": ["M.I. Jordan"], "venue": "Advances in Connectionist Theory: Speech. Erlbaum,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "COGNITIVE SCIENCE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M.Y. Hwang", "Y. Shi", "D. Yu"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Is it time to switch to word embedding and recurrent neural networks for spoken language understanding", "author": ["V. Vukotic", "C. Raymond", "G. Gravier"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Trans. Neur. Netw", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "arXiv preprint", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf", "author": ["X. Ma", "E. Hovy"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Practical very large scale CRFs", "author": ["T. Lavergne", "O. Capp\u00e9", "F. Yvon"], "venue": "Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), Association for Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Models cascade for tree-structured named entity detection", "author": ["M. Dinarelli", "S. Rosset"], "venue": "Proceedings of International Joint Conference of Natural Language Processing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Improving recurrent neural networks for sequence labelling", "author": ["M. Dinarelli", "I. Tellier"], "venue": "CoRR abs/1606.02555", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "New recurrent neural network variants for sequence labeling", "author": ["M. Dinarelli", "I. Tellier"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A. Joshi"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Spoken language understanding: A survey", "author": ["R. De Mori", "F. Bechet", "D. Hakkani-Tur", "M. McTear", "G. Riccardi", "G. Tur"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Expanding the scope of the atis task: The atis-3 corpus", "author": ["D.A. Dahl", "M. Bates", "M. Brown", "W. Fisher", "K. Hunicke-Smith", "D. Pallett", "C. Pao", "A. Rudnicky", "E. Shriberg"], "venue": "Proceedings of the Workshop on Human Language Technology. HLT \u201994,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Results of the french evalda-media evaluation campaign for literal understanding", "author": ["H. Bonneau-Maynard", "C. Ayache", "F. Bechet", "A. Denis", "A. Kuhn", "F. Lef\u00e8vre", "D. Mostefa", "M. Qugnard", "S. Rosset", "Servan", "J.S. Vilaneau"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "CoRR abs/1206.5533", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "Proceedings of IEEE. Volume", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1990}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["J.P.C. Chiu", "E. Nichols"], "venue": "CoRR abs/1511.08308", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K. Paliwal"], "venue": "Trans. Sig. Proc", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["C. Raymond", "G. Riccardi"], "venue": "Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Text chunking using transformation-based learning", "author": ["L. Ramshaw", "M. Marcus"], "venue": "Proceedings of the 3rd Workshop on Very Large Corpora,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JOURNAL OF MACHINE LEARNING RESEARCH", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Etude des reseaux de neurones recurrents pour etiquetage de sequences", "author": ["M. Dinarelli", "I. Tellier"], "venue": "Actes de la 23eme conf  \u221a \u00a9rence sur le Traitement Automatique des Langues Naturelles,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "COMPUTATIONAL LINGUISTICS", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1993}, {"title": "A step beyond local observations with a dialog aware bidirectional GRU network for Spoken Language Understanding", "author": ["V. Vukotic", "C. Raymond", "G. Gravier"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Discriminative reranking for spoken language understanding", "author": ["M. Dinarelli", "A. Moschitti", "G. Riccardi"], "venue": "IEEE Transactions on Audio, Speech and Language Processing (TASLP)", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Hypotheses selection criteria in a reranking framework for spoken language understanding", "author": ["M. Dinarelli", "S. Rosset"], "venue": "In: Conference of Empirical Methods for Natural Language Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Comparing stochastic approaches to spoken language understanding in multiple languages", "author": ["S. Hahn", "M. Dinarelli", "C. Raymond", "F. Lef\u00e8vre", "P. Lehen", "R. De Mori", "A. Moschitti", "H. Ney", "G. Riccardi"], "venue": "IEEE Transactions on Audio, Speech and Language Processing (TASLP)", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Large Margin Rank Boundaries for Ordinal Regression", "author": ["R. Herbrich", "T. Graepel", "Obermayer", "K. In"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}, {"title": "Optimizing crfs for slu tasks in various languages using modified training criteria", "author": ["S. Hahn", "P. Lehnen", "G. Heigold", "H. Ney"], "venue": "Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (ROVER)", "author": ["J.G. Fiscus"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 55, "endOffset": 64}, {"referenceID": 1, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 55, "endOffset": 64}, {"referenceID": 2, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 55, "endOffset": 64}, {"referenceID": 3, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 4, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 5, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 6, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 7, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 8, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 9, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 1, "context": "At first Elman and Jordan RNNs, introduced in [2, 1], and known also as simple RNNs, have been adapted to NLP.", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "At first Elman and Jordan RNNs, introduced in [2, 1], and known also as simple RNNs, have been adapted to NLP.", "startOffset": 46, "endOffset": 52}, {"referenceID": 10, "context": "These two recurrent models have shown limitations in learning relatively long contexts [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "In order to overcome this limitation the RNNs known as Long Short-Term Memory (LSTM) have been proposed [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": "Recently, a simplified and, apparently, more effective variant of LSTM has been proposed, using Gated Recurrent Units and thus named GRU [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "In order to overcome this limitation, sophisticated hybrid RNN+CRF models have been proposed [13, 14, 15], where the traditional output layer is replaced by a CRF neural layer.", "startOffset": 93, "endOffset": 105}, {"referenceID": 13, "context": "In order to overcome this limitation, sophisticated hybrid RNN+CRF models have been proposed [13, 14, 15], where the traditional output layer is replaced by a CRF neural layer.", "startOffset": 93, "endOffset": 105}, {"referenceID": 14, "context": "In order to overcome this limitation, sophisticated hybrid RNN+CRF models have been proposed [13, 14, 15], where the traditional output layer is replaced by a CRF neural layer.", "startOffset": 93, "endOffset": 105}, {"referenceID": 14, "context": "In [15] for example, the best result of POS tagging on the Penn Treebank corpus is an accuracy of 97.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "55, which is reached using word embeddings trained using GloVe [16], on huge amount of unlabeled data.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "The model of [15] without pre-trained embeddings reaches an accuracy of 96.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "3 on the same data [17].", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "We achieved the same result on the same data with a CRF model trained from scratch using the incremental procedure described in [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "9 [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "Our intuition is that using several label embeddings as context, a RNN is able to model correctly label-dependencies, the same way as more sophisticated models explicitly designed for sequence labeling like CRFs [20].", "startOffset": 212, "endOffset": 216}, {"referenceID": 20, "context": "This paper is a straight follow-up of [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "ii) The use of ReLU hidden layer and dropout regularization [22] at the hidden and embedding layers for improved regularized models.", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "2 accuracy or even better, were already published starting from 2003 [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 23, "context": "2 accuracy or even better, were already published starting from 2003 [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 24, "context": "We propose instead to evaluate all the models on two different and widely used tasks of Spoken Language Understanding [25], which provide more variate evaluation settings: ATIS [26] and MEDIA [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "We propose instead to evaluate all the models on two different and widely used tasks of Spoken Language Understanding [25], which provide more variate evaluation settings: ATIS [26] and MEDIA [27].", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "We propose instead to evaluate all the models on two different and widely used tasks of Spoken Language Understanding [25], which provide more variate evaluation settings: ATIS [26] and MEDIA [27].", "startOffset": 192, "endOffset": 196}, {"referenceID": 0, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 94, "endOffset": 100}, {"referenceID": 1, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 94, "endOffset": 100}, {"referenceID": 2, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "The evolution of the LSTM layer named GRU (Gated Recurrent Units) [12], combines together forget and input gates, and the previous hidden layer with the cell state:", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": "Such representations can encode very fine syntactic and semantic properties, as it has already been proved by word2vec [28] or GloVe [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Such representations can encode very fine syntactic and semantic properties, as it has already been proved by word2vec [28] or GloVe [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "The idea of using label embeddings has been introduced in [29] for dependency parsing, resulting in a very effective parser.", "startOffset": 58, "endOffset": 62}, {"referenceID": 28, "context": "In this paper we go ahead with respect to [29] by using several label embeddings as context to predict the label at current position in a sequence.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "As we already mentioned, this limitation lead research toward hybrid RNN+CRF models [13, 14, 15].", "startOffset": 84, "endOffset": 96}, {"referenceID": 13, "context": "As we already mentioned, this limitation lead research toward hybrid RNN+CRF models [13, 14, 15].", "startOffset": 84, "endOffset": 96}, {"referenceID": 14, "context": "As we already mentioned, this limitation lead research toward hybrid RNN+CRF models [13, 14, 15].", "startOffset": 84, "endOffset": 96}, {"referenceID": 29, "context": "Reusing an example from [30]: if Paris is replaced by Rome in a text, this has no impact on several NLP tasks, as they are both proper nouns in POS tagging, localization in Named Entity Recognition etc.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "In Jordan RNNs used for NLP like [8, 9, 10], labels are represented either with the probability distribution computed by the softmax, or with the one-hot representation computed from the probability distribution.", "startOffset": 33, "endOffset": 43}, {"referenceID": 8, "context": "In Jordan RNNs used for NLP like [8, 9, 10], labels are represented either with the probability distribution computed by the softmax, or with the one-hot representation computed from the probability distribution.", "startOffset": 33, "endOffset": 43}, {"referenceID": 9, "context": "In Jordan RNNs used for NLP like [8, 9, 10], labels are represented either with the probability distribution computed by the softmax, or with the one-hot representation computed from the probability distribution.", "startOffset": 33, "endOffset": 43}, {"referenceID": 30, "context": "We use the traditional back-propagation algorithm with momentum to learn our networks [31].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "Given the recurrent nature of the networks, the Back-Propagation Through Time (BPTT) is often used [32].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "However [5] has shown that RNNs for language modeling learn best with only N = 5 previous steps.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Since the BPTT algorithm is quite expensive, [9] chose to explicitly use the contextual information provided by the recurrent connection, and to use the traditional back-propagation algorithm, apparently without performance loss.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "To our knowledge however, these networks are effectively learned using only one previous hidden state [13, 14, 15].", "startOffset": 102, "endOffset": 114}, {"referenceID": 13, "context": "To our knowledge however, these networks are effectively learned using only one previous hidden state [13, 14, 15].", "startOffset": 102, "endOffset": 114}, {"referenceID": 14, "context": "To our knowledge however, these networks are effectively learned using only one previous hidden state [13, 14, 15].", "startOffset": 102, "endOffset": 114}, {"referenceID": 12, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 32, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 13, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 14, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 6, "context": "In this paper we focus on a convolution layer similar to the one used in [7] for words.", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "the Max function is the so-called max-pooling [7].", "startOffset": 46, "endOffset": 49}, {"referenceID": 33, "context": "The RNNs introduced in this paper are proposed as forward, backward and bidirectional models [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "Bidirectional models are described in details in [34].", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "In the development phase of our systems, we noticed no difference in terms of performance between the two types of bidirectional models described in [34].", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": "We evaluated our models on two tasks of Spoken Language Understanding (SLU) [25]:", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "The ATIS corpus (Air Travel Information System) [26] was collected for building a spoken dialog system able to provide flight information in the United States.", "startOffset": 48, "endOffset": 52}, {"referenceID": 34, "context": "We use the version of the corpus published in [35], where some word classes are available, such as city names, airport names, time expressions etc.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "More details about this corpus can be found in [26].", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "We are aware of the existence of two version of the ATIS corpus: the official version published starting from [35], and the version associated to the tutorial of deep learning made available by the authors of [9].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "We are aware of the existence of two version of the ATIS corpus: the official version published starting from [35], and the version associated to the tutorial of deep learning made available by the authors of [9].", "startOffset": 209, "endOffset": 212}, {"referenceID": 8, "context": "Following the tutorial of [9] we have been able to download the second version of the ATIS corpus.", "startOffset": 26, "endOffset": 29}, {"referenceID": 35, "context": "The results we obtained are comparable with those published in [36], in part from same authors of [9].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "The results we obtained are comparable with those published in [36], in part from same authors of [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 26, "context": "The French corpus MEDIA [27] was collected to create and evaluate spoken dialog systems providing touristic information about hotels in France.", "startOffset": 24, "endOffset": 28}, {"referenceID": 36, "context": "The task resulting from the corpus MEDIA can be modeled as a sequence labeling task by chunking the concepts over several words using the traditional BIO notation [37].", "startOffset": 163, "endOffset": 167}, {"referenceID": 37, "context": "\u2022 Neural Network Language Models (NNLM), like the one described in [38], are", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "We ran also some experiments using embeddings trained with word2vec [28].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "This outcome is similar to the one obtained in [10].", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "We stop training the model if the accuracy is not improved for 5 consecutive epochs (also known as Early stopping strategy [31]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "We initialize all the weights with the \u201cso called\u201d Xavier initialization [31], theoretically motivated in [39] as keeping the standard deviation of the weights during the training phase when using ReLU, which is the type of hidden layer unit we chose for our variant of RNN.", "startOffset": 73, "endOffset": 77}, {"referenceID": 38, "context": "We initialize all the weights with the \u201cso called\u201d Xavier initialization [31], theoretically motivated in [39] as keeping the standard deviation of the weights during the training phase when using ReLU, which is the type of hidden layer unit we chose for our variant of RNN.", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "We combine dropout and L2 regularization [31], the best value for the dropout probability is 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "These values are the same found in [10] and comparable to those of [36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": "These values are the same found in [10] and comparable to those of [36].", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "In the tables we use E-RNN for Elman RNN, J-RNN for Jordan RNN, I-RNN for the improved RNN proposed by [40].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "10 In order to give an idea of how our RNN variant compares to LSTM+CRF models like the one of [15], we ran an experiment on the Penn Treebank [41].", "startOffset": 95, "endOffset": 99}, {"referenceID": 40, "context": "10 In order to give an idea of how our RNN variant compares to LSTM+CRF models like the one of [15], we ran an experiment on the Penn Treebank [41].", "startOffset": 143, "endOffset": 147}, {"referenceID": 14, "context": "9 achieved by the LSTM+CRF model of [15] without pre-trained embeddings.", "startOffset": 36, "endOffset": 40}, {"referenceID": 41, "context": "On this task we compare to results published in [42] and [40].", "startOffset": 48, "endOffset": 52}, {"referenceID": 39, "context": "On this task we compare to results published in [42] and [40].", "startOffset": 57, "endOffset": 61}, {"referenceID": 41, "context": "The GRU RNNs of [42] and our variant LD-RNN obtain equivalent results (95.", "startOffset": 16, "endOffset": 20}, {"referenceID": 41, "context": "The way they are used in [42] allows to learn long contexts on the input side (words),", "startOffset": 25, "endOffset": 29}, {"referenceID": 39, "context": "Comparing our results on the ATIS task with those published in [40] with a Jordan RNN, which uses the same label context as our models, we can conclude that the", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "forward backward bidirectional [42] LSTM 95.", "startOffset": 31, "endOffset": 35}, {"referenceID": 41, "context": "23% [42] GRU 95.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "53% [40] E-RNN 94.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "71% [40] J-RNN 94.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "89% [40] I-RNN 95.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "forward backward bidirectional [10] CRF 86.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "00% [10] E-RNN 81.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "94% \u2013 \u2013 [10] J-RNN 83.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "25% \u2013 \u2013 [42] LSTM 81.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "07% [42] GRU 83.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "63% [40] E-RNN 82.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "13% [40] J-RNN 83.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "29% [40] I-RNN 84.", "startOffset": 4, "endOffset": 8}, {"referenceID": 42, "context": "Model CER [43] CRF 11.", "startOffset": 10, "endOffset": 14}, {"referenceID": 43, "context": "7% [44] CRF 11.", "startOffset": 3, "endOffset": 7}, {"referenceID": 44, "context": "5% [45] CRF 10.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In this context we note that a traditional Jordan RNN, the J-RNN of [40], which is the only traditional model to explicitly use previous label information as context, is more effective than the other traditional models, including LSTM and GRU (84.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "00 F1 with the CRF of [10]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "The only models outperforming CRFs on the MEDIA task are the I-RNN model of [40] and our LD-RNN variant, both using label embeddings.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Even if results on MEDIA discussed so far are very competitive, this task has been designed for Spoken Language Understanding (SLU) [25].", "startOffset": 132, "endOffset": 136}, {"referenceID": 44, "context": "12 In order to place our results on an absolute ranking among models designed for the MEDIA task, we propose a comparison in terms of CER to the best models published in the literature, namely [45], [44] and [43].", "startOffset": 193, "endOffset": 197}, {"referenceID": 43, "context": "12 In order to place our results on an absolute ranking among models designed for the MEDIA task, we propose a comparison in terms of CER to the best models published in the literature, namely [45], [44] and [43].", "startOffset": 199, "endOffset": 203}, {"referenceID": 42, "context": "12 In order to place our results on an absolute ranking among models designed for the MEDIA task, we propose a comparison in terms of CER to the best models published in the literature, namely [45], [44] and [43].", "startOffset": 208, "endOffset": 212}, {"referenceID": 44, "context": "The best individual models published by [45], [44] and [43] are CRFs, achieving a CER of 10.", "startOffset": 40, "endOffset": 44}, {"referenceID": 43, "context": "The best individual models published by [45], [44] and [43] are CRFs, achieving a CER of 10.", "startOffset": 46, "endOffset": 50}, {"referenceID": 42, "context": "The best individual models published by [45], [44] and [43] are CRFs, achieving a CER of 10.", "startOffset": 55, "endOffset": 59}, {"referenceID": 44, "context": "We note that the large gap between these CRF models is due to the fact that the CRF of [45] is trained with an improved margin criterion, similar to the large margin principle of SVM [46, 47].", "startOffset": 87, "endOffset": 91}, {"referenceID": 45, "context": "We note that the large gap between these CRF models is due to the fact that the CRF of [45] is trained with an improved margin criterion, similar to the large margin principle of SVM [46, 47].", "startOffset": 183, "endOffset": 191}, {"referenceID": 46, "context": "We note that the large gap between these CRF models is due to the fact that the CRF of [45] is trained with an improved margin criterion, similar to the large margin principle of SVM [46, 47].", "startOffset": 183, "endOffset": 191}, {"referenceID": 42, "context": "We note also that comparing significance tests published in [43], a difference of 0.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "Moreover, instead of taking the mean of CER of several experiments, following a strategy similar to [8], one can run several experiments and keep the model obtaining the best CER on the development data of the target task.", "startOffset": 100, "endOffset": 103}, {"referenceID": 47, "context": "09, the best absolute result on this task so far, even better than the ROVER model [48] used in [45], which combines 6 individual models, including the individual CRF model achieving 10.", "startOffset": 83, "endOffset": 87}, {"referenceID": 44, "context": "09, the best absolute result on this task so far, even better than the ROVER model [48] used in [45], which combines 6 individual models, including the individual CRF model achieving 10.", "startOffset": 96, "endOffset": 100}, {"referenceID": 37, "context": "This second reason is what made neural networks popular for learning word embeddings in earlier publications [38].", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "Interestingly, this behavior suggests that LD-RNN could still benefit from a CRF neural layer like those used in [13, 14, 15].", "startOffset": 113, "endOffset": 125}, {"referenceID": 13, "context": "Interestingly, this behavior suggests that LD-RNN could still benefit from a CRF neural layer like those used in [13, 14, 15].", "startOffset": 113, "endOffset": 125}, {"referenceID": 14, "context": "Interestingly, this behavior suggests that LD-RNN could still benefit from a CRF neural layer like those used in [13, 14, 15].", "startOffset": 113, "endOffset": 125}], "year": 2017, "abstractText": "In the last few years, Recurrent Neural Networks (RNNs) have proved effective on several NLP tasks. Despite such great success, their ability to model sequence labeling is still limited. This lead research toward solutions where RNNs are combined with models which already proved effective in this domain, such as CRFs. In this work we propose a solution far simpler but very effective: an evolution of the simple Jordan RNN, where labels are re-injected as input into the network, and converted into embeddings, in the same way as words. We compare this RNN variant to all the other RNN models, Elman and Jordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language Understanding (SLU). Thanks to label embeddings and their combination at the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNNs, but far fewer than LSTM and GRU, is more effective than other RNNs, but also outperforms sophisticated CRF", "creator": "LaTeX with hyperref package"}}}