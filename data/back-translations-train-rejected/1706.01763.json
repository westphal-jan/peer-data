{"id": "1706.01763", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Adversarial-Playground: A Visualization Suite for Adversarial Sample Generation", "abstract": "With growing interest in adversarial machine learning, it is important for machine learning practitioners and users to understand how their models may be attacked. We propose a web-based visualization tool, \\textit{Adversarial-Playground}, to demonstrate the efficacy of common adversarial methods against a deep neural network (DNN) model, built on top of the TensorFlow library. Adversarial-Playground provides users an efficient and effective experience in exploring techniques generating adversarial examples, which are inputs crafted by an adversary to fool a machine learning system. To enable Adversarial-Playground to generate quick and accurate responses for users, we use two primary tactics: (1) We propose a faster variant of the state-of-the-art Jacobian saliency map approach that maintains a comparable evasion rate. (2) Our visualization does not transmit the generated adversarial images to the client, but rather only the matrix describing the sample and the vector representing classification likelihoods", "histories": [["v1", "Tue, 6 Jun 2017 13:43:11 GMT  (245kb,D)", "https://arxiv.org/abs/1706.01763v1", "under review ; 8 pages; 3 figures"], ["v2", "Fri, 16 Jun 2017 16:38:09 GMT  (380kb,D)", "http://arxiv.org/abs/1706.01763v2", "8 pages; 3 figures"]], "COMMENTS": "under review ; 8 pages; 3 figures", "reviews": [], "SUBJECTS": "cs.CR cs.AI cs.LG", "authors": ["andrew norton", "yanjun qi"], "accepted": false, "id": "1706.01763"}, "pdf": {"name": "1706.01763.pdf", "metadata": {"source": "CRF", "title": "ADVERSARIAL-PLAYGROUND: A Visualization Suite for Adversarial Sample Generation", "authors": ["Andrew Norton", "Yanjun Qi"], "emails": ["yanjun}@virginia.edu"], "sections": [{"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution that will enable us to find a solution."}, {"heading": "2 Background of Adversarial Examples", "text": "Studies of the behavior of machine learning models in adversarial environments typically fall into one of three categories: (1) poisoning attacks, in which specially created samples are injected into the training of a learning model; (2) privacy-conscious methods aimed at preserving the privacy of information in training data; or (3) identity attacks, in which the adversary aims to generate inputs that are misclassified by a target classifier. In order to formalize the extent of the permitted change, circumvention algorithms minimize the difference between the \"seed image\" and the resulting evasion pattern based on a selected standard (distance function). In some cases, the adversary specifies the \"target class\" of an circumvention class \"x, classifying the class\" X \"as inaccurate."}, {"heading": "2.1 L0 Norm", "text": "A simple way to determine the magnitude of the difference between two images is to count the number of pixels that differ between them. That is, if x is our original image and x \u2032 = x + r is the evasive image (for an appropriate value of r), then we can calculate the L0 distance between x and x \u2032, where [\u00b7] is the Iverson bracket [8]. Their approach calculates a highlighting map of a given input and ranks pixels based on their contribution to the classification. Then, they perform a combinatorial search across all the pixel pairs to find the optimal two pixels to the opponent, repeating this until either the modified image is misclassified or the L0 distance between the modified and unmodified image is exceeded."}, {"heading": "2.2 L2 Norm", "text": "A disadvantage of the L0 standard is that it is not differentiable. As the L2 standard is differentiable, it is easier to understand from a theoretical standpoint. It measures the Euclidean standard distance between two vectors; using the same notation as before, with x as the2The Iverson parenthesis is defined as follows: [P] = {1 P is true 0 other output vector and x \u2032 = x + r for the convex input, this standard is calculated by the following (albeit slow) methods for evasive sample generation: Szegedy et al. presented the problem as a convex optimization problem using the L2 standard [10]. This problem was then solved using the usual (albeit slow) method of box-limited L-BFGS."}, {"heading": "2.3 L\u221e Norm", "text": "A third commonly used standard is the L standard, also called the Chebyshev distance, which measures the maximum change between two vectors along a characteristic. That is, if x is the starting vector and x \u00b2 = x + r is the opposing input, the distance between them is calculated using the Fast Gradient Sign (FGS) method. Unlike most previous approaches, which require an iterative change of the evasive sample away from its source class, FGS performs exactly one update step to obtain the evasive input. Informally, the algorithm performs a step of decreasing the gradient, but moves (1) away from the minimum of the loss function and (2) modifies the input sample instead of the model. If x is our original input, J (winding, bypass) is the special function for the bypass sign, increasing the distance between the two groups."}, {"heading": "3 System Organization: Webserver with", "text": "Client-side visualization As a web application, ADVERSARIAL-PLAYGROUND shares the tasks of visualization and computation between client and server. Via the client, the user adjusts hyperparameters and sends an AJAX request for a generated sample to the server. Once the TensorFlow backend has generated the opposite image and classification probability, the server returns this data to the client. Finally, this information is graphically presented to the user using the Plotly JavaScript library (Figure 1). When accessing the application, the user has the choice of several attack methods - some targeted and some untargeted. After selecting an attack, the user can adjust model parameters, select the seed image and specify a target class (if applicable). When the user submits his desired parameters, the server starts creating a contrary sample against a preset CNN model, the results are written to the IR and the real-time model."}, {"heading": "3.1 Design Decisions", "text": "When creating our system, we made several design decisions. Here we present the reasons for the three biggest decisions we made at system level: the structure of ADVERSARIAL-PLAYGROUND as a web-based application, the use of client and server-side code and the display of images with the client and not with the server."}, {"heading": "3.1.1 Web-based Framework", "text": "The first question we asked ourselves was whether we wanted a web-enabled framework or a desktop application. It was a relatively simple design decision, as a web-based user interface allows a large number of users to use the application without the need for an installation process on each computer. By eliminating one installation step, we encourage potential users who are only casually interested in adversarial machine learning to explore what it is. This supports the educational goals of the software package."}, {"heading": "3.1.2 Client-Server with Python back-end", "text": "Beyond a web-based framework, we had to decide whether ADVERSARIAL PLAYGROUND would be client-based or use server-side capabilities. TensorFlow Playground was written entirely in JavaScript and other client-side technologies, allowing a lightweight server to host the service for many users. However, opposing samples are usually generated on larger, deeper networks than those of users of the TensorFlow Playground, making a JavaScriptonly approach prohibitive. Instead, we opted for a GPU-enabled server running Python with TensorFlow to generate the opposing samples. In addition to a speed advantage, this allows our CNN base model to be easily replaced by other TensorFlow graphics. Also, adding new attack strategies is easier because TensorFlow has been used in much research on circumvention strategies."}, {"heading": "3.1.3 Client-side rendering", "text": "After we decided to generate the examples on the server side, it was tempting to generate the output images on the server as well, which leads us to the third design decision we made: client-side rendering of MNIST images and probability charts. In our prototype, we used server-side rendering of these images using the Python library and then uploaded the image directly from the client. However, this approach had several drawbacks. If we were just waiting for the standard Matplotlib tools, we had to write each generated image to disk and provide the user with the unique URL; this requires large volumes of disk, depending on the number of users. In addition, each image had about 20 kilobytes; waiting for the image to transfer increased the latency that the user experienced. Fortunately, client-side rendering of the images using the JavaScript library Plotly.S updated the need to transfer the entire image from MOST to each PNG and to the MOST values, reducing the need for each of these values to signify PNG within the 28."}, {"heading": "3.2 MNIST Dataset", "text": "ADVERSARIAL-PLAYGROUND uses the popular MNIST \"handwritten digits\" dataset to visualize evasive attacks. This dataset contains 70,000 images of handwritten digits (0 to 9), of which 60,000 images are used as training data and the remaining 10,000 images are used for testing. Each sample is a 28 x 28 pixel 8-bit grayscale image. Users of our system receive a collection of multiple seed images, selected from each of the 10 classes of testset3. We decided to support the MNIST dataset only for our visualization, as it is a common dataset used in evasive discussions. Many hostile machine learning papers work with some form of image data; as this is easy to visualize, we thought it would be best to work with one of the MNIST datasets, CIFAR or ImageNet datasets, which are much higher than IST, but both the Item and Item results are very high."}, {"heading": "3.3 Software Manual", "text": "We have published the entire project code on GitHub in order to provide a high-quality, easy-to-use evasion demonstration software package.Setup Although the package is minimal, it requires a computer running Python 3.5, TensorFlow 1.0 (or higher), the default SciPy stack, and the Python package bottles.We already have the code on Windows, Linux, and Mac operating systems.To install the web app, clone the GitHub repository, and install the requirements using Pip3 r installation requirements.tx. A pre-rehearsed MNIST model is already stored in the GitHub repository; all that is required to launch the web app is python3 run.py. Once launched, the app runs on the local host: 9000.Usage To use the application, the user selects a model from the navigation bar at the top of the web app."}, {"heading": "4 Faster approach to JSMA", "text": "The ADVERSARIAL-PLAYGROUND focuses on a number of already implemented attack models. It was important to give the user a choice between targeted and non-targeted approaches, as well as models that use different standards. Therefore, we have implemented approaches based on Saliency Map (one directly from Papernot, et al. [8] and one faster from our own development), and slightly modified the Cleverhans implementation of the Fast Gradient Sign Method (FGS) for non-targeted attacks. As the FGS method is almost identical to that used in Cleverhans, we encourage the reader to consider implementation details [4]. In the next two sections, we will review the details of the Jacobian Saliency Map approach from the work of Papernot, et al., [8] and our enhancement Fast Jacobian Saliency Map Apriori."}, {"heading": "4.1 Jacobian Saliency Map Approach (JSMA)", "text": "The Jacobian Saliency Map Approach (JSMA) adjusts the initial input to maintain the similarity based on the L0. Applied to the MNIST model, the approach is as follows: 1. Calculate the predictive derivative of the classifier, according to which the sample is incorrectly classified or while the sample and seed image differ by no more than two pixels (an adjustable threshold); 3. Change the two pixels and update the current sample; 4. Repeat until the opposing sample is incorrectly classified or while the sample and seed image differ by no more than two pixels (an adjustable threshold); the first and last step are relatively inexpensive in time."}, {"heading": "4.2 Fast Jacobian Saliency Map Apriori (FJSMA)", "text": "The Apriori algorithm is a fast, greedy \"bottom-up\" approach to determining item sets with minimal support [1]. It achieves its speed by a priori eliminating certain suboptimal item sets. Similarly, our algorithm eliminates some (p, q) pairs by a priori knowledge of the sample. Instead of looking exhaustively at each feature pair (p, q), we arrange the elements in the feature set according to the value of the Jacobian at this coordinate. (This is the contribution that each element makes to \u03b1 in algorithm 1.) We then force the selection of p from the best k features and select q from the entire feature set with p removed. Since this p choice means that its contribution to \u03b1 is large, it is likely that the product \u2212 \u03b1 x-\u03b2-top feature is also generous."}, {"heading": "4.3 Experimental Results", "text": "The improved speed of FJSMA compared to JSMA is particularly beneficial to the real-time environment of FJSMA. The FJSMA ratio is able to be found in the areas where the FJSMA ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio"}, {"heading": "5 Discussion and Future Work", "text": "In this paper, we present a web-based tool for visualizing the performance of evasive algorithms for deep neural networks, which helps both researchers and students understand and compare the effects of opposing examples against DNNs. In addition, we offer an improvement to the Jacobian Saliency Map Approach (JSMA), originally developed by Papernot and others. [8] This improvement, which we call Fast Jacobian Saliency Map Apriori (FJSMA), uses a priori heuristic methods to significantly reduce the search space while maintaining essentially the same evasion rate - an important advantage for visualization. A simple extension of this work is to increase the variety of supported evasion methods, for example by fixing the new attacks on L0, L2 and the L-shaped norms from Carlini and Wagner's recent work."}], "references": [{"title": "Fast algorithms for mining association rules in large databases", "author": ["R. AGRAWAL", "R. SRIKANT"], "venue": "Proceedings of the 20th International Conference on Very Large Data Bases (San Francisco, CA, USA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Towards evaluating the robustness of neural networks", "author": ["N. CARLINI", "D. WAGNER"], "venue": "CoRR abs/1608.04644", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "cleverhans v0.1: an adversarial machine learning library", "author": ["I.J. GOODFELLOW", "N. PAPERNOT", "P.D. MC- DANIEL"], "venue": "CoRR abs/1610.00768", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. GOODFELLOW", "J. SHLENS", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1412.6572", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HIN- TON"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Microsoft Malware Competition Challenge", "author": ["MICROSOFT CORPORATION"], "venue": "https://www.kaggle.com/c/malware-classification", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. PAPERNOT", "P.D. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Face Recognition", "author": ["O.M. PARKHI", "A. VEDALDI", "A. ZISSERMAN"], "venue": "In British Machine Vision Conference", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ERHAN", "I.J. GOODFELLOW", "R. FERGUS"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Understanding neural networks through deep visualization", "author": ["J. YOSINSKI", "J. CLUNE", "A.M. NGUYEN", "T.J. FUCHS", "H. LIPSON"], "venue": "CoRR abs/1506.06579", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs) are an essential tool for many machine learning tasks, especially image classification [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "Unfortunately, recent studies of evasive samples show that intelligent attackers can force these models to misclassify samples by performing nearly imperceptible modifications to the sample before attempting classification [5, 10].", "startOffset": 223, "endOffset": 230}, {"referenceID": 8, "context": "Unfortunately, recent studies of evasive samples show that intelligent attackers can force these models to misclassify samples by performing nearly imperceptible modifications to the sample before attempting classification [5, 10].", "startOffset": 223, "endOffset": 230}, {"referenceID": 5, "context": "While machine learning models may appear to be effective for many security tasks like malware classification [7] and face recognition [9], it is important to realize that these classification techniques were not designed to withstand manipulations made by intelligent and adaptive adversaries.", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "While machine learning models may appear to be effective for many security tasks like malware classification [7] and face recognition [9], it is important to realize that these classification techniques were not designed to withstand manipulations made by intelligent and adaptive adversaries.", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "Our proposed package follows the spirit of TensorFlow Playground \u2014 a web-based educational tool that helps users understand how neural networks work [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 1, "context": "Most state-of-the-art evasion algorithms are slow due to expensive optimization and the large feature space involved in image classification [3, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 2, "context": "Most state-of-the-art evasion algorithms are slow due to expensive optimization and the large feature space involved in image classification [3, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 1, "context": "The latter category provides a useful classification scheme for evasion algorithms, suggested by Carlini and Wagner [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": ", suggested using the L0 norm for evaluating the similarity of the initial sample and adversarial result [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "This algorithm \u2014 as well as the L\u221e fast gradient sign method \u2014 was included in the cleverhans package by Goodfellow, Papernot, and McDaniel [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 8, "context": ", posed the issue as a convex optimization problem using the L2 norm [10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "The Fast Gradient Sign (FGS) method uses the L\u221e norm in generating evading inputs; this method is commonly used due to its speed [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "[8] and a faster one of our own development) for targeted methods and lightly modified the cleverhans implementation of the Fast Gradient Sign Method (FGS) for untargeted attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "As the FGS method is nearly identical to that found in cleverhans, we encourage the reader to consider [4] for implementation details.", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": ", [8] and our improvement, Fast Jacobian Saliency Map Apriori.", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "used Algorithm 1 for this selection process [8].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "The Apriori algorithm is a fast, greedy, \u201cbottom-up\u201d approach to determining item sets with minimal support [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "We compare our FJSMA implementation (using a variety of values for k and \u03a5) to the implementation of JSMA found in Cleverhans [4] as of April 5, 2017.", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "For example, including the new attacks based on L0, L2, and L\u221e norms from Carlini and Wagner\u2019s recent paper [3] would be a good step in comparing the performance of multiple evasion strategies.", "startOffset": 108, "endOffset": 111}], "year": 2017, "abstractText": "With growing interest in adversarial machine learning, it is important for practitioners and users of machine learning to understand how their models may be attacked. We present a web-based visualization tool, ADVERSARIALPLAYGROUND, to demonstrate the efficacy of common adversarial methods against a convolutional neural network. ADVERSARIAL-PLAYGROUND provides users an efficient and effective experience in exploring algorithms for generating adversarial examples \u2014 samples crafted by an adversary to fool a machine learning system. To enable fast and accurate responses to users, our webapp employs two key features: (1) We split the visualization and evasive sample generation duties between client and server while minimizing the transferred data. (2) We introduce a variant of the Jacobian Saliency Map Approach that is faster and yet maintains a comparable evasion rate 1.", "creator": "LaTeX with hyperref package"}}}