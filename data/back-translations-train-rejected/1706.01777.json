{"id": "1706.01777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Deep Factorization for Speech Signal", "abstract": "Speech signals are complex intermingling of various informative factors, and this information blending makes decoding any of the individual factors extremely difficult. A natural idea is to factorize each speech frame into independent factors, though it turns out to be even more difficult than decoding each individual factor. A major encumbrance is that the speaker trait, a major factor in speech signals, has been suspected to be a long-term distributional pattern and so not identifiable at the frame level. In this paper, we demonstrated that the speaker factor is also a short-time spectral pattern and can be largely identified with just a few frames using a simple deep neural network (DNN). This discovery motivated a cascade deep factorization (CDF) framework that infers speech factors in a sequential way, and factors previously inferred are used as conditional variables when inferring other factors. Our experiment on an automatic emotion recognition (AER) task demonstrated that this approach can effectively factorize speech signals, and using these factors, the original speech spectrum can be recovered with high accuracy. This factorization and reconstruction approach provides a novel tool for many speech processing tasks.", "histories": [["v1", "Mon, 5 Jun 2017 15:02:39 GMT  (2458kb,D)", "http://arxiv.org/abs/1706.01777v1", null], ["v2", "Sun, 25 Jun 2017 10:10:35 GMT  (2458kb,D)", "http://arxiv.org/abs/1706.01777v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["dong wang", "lantian li", "ying shi", "yixiang chen", "zhiyuan tang"], "accepted": false, "id": "1706.01777"}, "pdf": {"name": "1706.01777.pdf", "metadata": {"source": "CRF", "title": "Deep Factorization for Speech Signal", "authors": ["Dong Wang", "Lantian Li", "Ying Shi", "Yixiang Chen", "Zhiyuan Tang"], "emails": ["wangdong99@mails.tsinghua.edu.cn", "lilt13@mails.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 Speaker factor learning", "text": "In this section, we present a DNN structure that can learn speaker characteristics at the frame level, as shown in Figure 6. This structure consists of a Convolutionary Component (CN) and a Delayed Component (TD), connected by a bottleneck layer of 512 units. The convolutional component consists of two CN layers followed by a max pooling layer. The TD component consists of two TD layers followed by a P-standard layer. The settings for the two components are shown in Figure 6. A simple calculation shows that with this configuration, the length of the effective context window is 20 frames. Output of the P-standard layer is projected into a feature layer consisting of 40 units. After length normalization, the activations of these units form a speaker factor that represents the speaker characteristics involved in the input language segment. For model training, the feature layer is fully connected to the output layer, whose units correspond to the target speaker structure in the speaker structure."}, {"heading": "3 Cascaded deep factorization", "text": "Due to the highly complex mixing of several informative factors, it is almost impossible to replace speech signals with conventional linear factorization methods, such as JFA [4]. Fortunately, ASR research has shown that the linguistic factor can be derived from a DNN structure without knowing other factors. However, the previous section states that the loudspeaker factor is sufficiently important to rely on deep neural models, and the rationality of linguistics and the speaker is secondary."}, {"heading": "4 Spectrum reconstruction", "text": "A major difference between CDF and conventional factor analysis [8] is that in CDF, each factor is derived separately, without explicit limitations defined between the factors (e.g. the linear Gaussian relationship as in JFA), which is indispensable for flexible factorization, but also avoids an important question: How do these factors compose to produce the speech signal? To answer this question, we reconstruct the spectrum using the CDF-derived factors. We define the linguistic factor q, the speaker factor s, and the emotion factor e. For each language frame, we try to use these three factors to restore the spectrum x. Assuming that they are entangled, the reconstruction is in the form: ln (x) = ln {f (q)} + ln {g (s)} + ln {h (e)} + where f, h, h, the non-linear restoration function is implied for each of the spectra, or s respectively."}, {"heading": "5 Related work", "text": "The idea of learning loudspeaker factors was motivated by Ehsan et al. [9], who used a vanilla DNN to learn frame-level representations of loudspeakers, but these representations were rather weak and did not perform well in SRE tasks. Since then, various DNN structures have been studied, e.g. RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13]. These varied studies showed reasonable performance, but most of them were based on end-to-end training, which focused on better performance in speaker verification rather than factor learning. The CDF approach is also related to the phonetic DNN-i vector approach proposed by Lei [14] and Kenny [15], with the linguistic factor (phonetic posteriors) initially derived from an ASR system that systematically conveys SR information, which is then used as an auxiliary factor (the speaker factor)."}, {"heading": "6 Experiment", "text": "In this section, we first present the data used in the experiments and then report on the results of speaker factor learning. We also present CDF-based factorization and reconstruction of emotional language."}, {"heading": "6.1 Database", "text": "ASR Database: The WSJ database was used to train the ASR system.The training set contains three sets (devl92, eval92 and eval93), including 27 speakers and 1, 049 utterances in total.SRE database: The Fisher database was used to train the SRE systems.The training set consists of 2, 500 male and 2, 500 female speakers, with 95, 167 utterances randomly selected from the Fisher database.The SRE database has about 120 seconds of speech signals. It was used for training the UBM, T-Matrix and LDA / PLDA models of an i-vector base system, and the DNN model was proposed in Section 2. The test set consists of 500 male and 500 female speakers randomly selected from the Fisher database.There is no overlap between the speakers of the Chinese base system and the 910 seconds of training."}, {"heading": "6.2 ASR baseline", "text": "First, we build a DNN-based ASR system based on the WSJ database, from which the linguistic factor is generated in the following CDF experiments.The Kaldi toolkit [25] is used to build the DNN model according to the Kaldi-WSJ-s5 net recipe. The DNN structure consists of 4 hidden layers, each with 1 024 units. Input characteristic is fbanks, and the output layer distinguishes 3 383 GMM pdfs. In the official 3-gram language model, the word error rate (WHO) of this system is 9.16%. The linguistic factor is represented by 42-dimensional telephone posteriors derived from the output of the ASR-DNN."}, {"heading": "6.3 Speaker factor learning", "text": "In this section, we will experiment with the DNN structure proposed in Section 2 to learn the loudspeaker factors. Two models are examined: one follows the architecture shown in Figure 6, with only the raw characteristics (Fbank) comprising the input; the other model uses both the raw characteristics and the linguistic factors produced by the DNN system; the first model is trained by IDF, while the second model is trained by CDF; the Fisher database is used to train the model; and the 40-dimensional frame-level factors are read from the last hidden layer of the DNN structure."}, {"heading": "6.4 Emotion recognition by CDF", "text": "In the previous experiment, we partially demonstrated the CDF approach with the learning task of the speaker factor. This section provides further evidence with an emotion recognition task. To this end, we first build a DNN-based AER baseline. The DNN model consists of 6 time-delayed hidden layers, each containing 200 units. After each TD layer, a P-norm layer reduces the dimensionality from 200 to 40. The output layer consists of 8 units, corresponding to the 8 emotions in the database. This DNN model produces posterior frame emotion. The posterior frame level is achieved by averaging the posterior frame level, which achieves the expression-level emotion decision.Three CDF configurations are examined depending on which factor is used as a condition: the linguistic factor (+ ling.), the speaker factor (+ spk.), the posterior factor level is measured by the mean (+ k) and (both)."}, {"heading": "6.5 Spectrum reconstruction", "text": "The reconstruction model was discussed in Section 4 and presented in Figure 3. This model is trained using the CHEAVD database. Figure 5 shows the reconstruction of a test expression in the CHEAVD database. It turns out that these three factors can reconstruct the spectrum patterns extremely well, confirming once again that the speech signal has been well factored and the formula for reconstructing the coils is broadly correct. Finally, the three components spectra (linguistics, speaker and emotion) are highly interesting and all deserve comprehensive investigation. For example, the speaker spectrum could be a new tool for voice analysis and be very useful for forensic applications."}, {"heading": "7 Conclusions", "text": "This paper presents a DNN model for learning short-term speaker characteristics and a cascaded approach to factoring speech signals into independent informative factors. Two interesting things have been found: First, speaker characteristics are indeed short-term spectral patterns and can be identified by deep learning from a very short speech segment; second, speech signals at the frame level can be well factored in by the CDF approach. We also found that speech spectrum can be largely reconstructed from factors derived from the CDF using deep neural models, confirming the accuracy of factorization. Successful factoring and reconstruction of speech signals has very important implications and can be widely used. To name just a few: it can be used to design very short-sighted speech codes to change the characteristics of speakers or emotions in speech synthesis or speech transformation in order to remove background noises in order to embed auditory signs. All are highly interesting and currently under investigation."}, {"heading": "8 Appendix A: Model details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 ASR system", "text": "The ASR system was built according to the Kaldi WSJ s5 nnet recipe. The input function consisted of 40-dimensional banks, with a symmetrical 5-frame window for gluing adjacent frames together. It contained 4 hidden layers, and each layer had 1 024 units. The output layer consisted of 3 383 units, which corresponds to the total number of pdfs of the GMM system, which was formed according to the WSJ s5 gmm recipe. The language model was the official 3-gram model (\"tgpr\") of the WSJ, which consists of 19 982 words."}, {"heading": "8.2 SRE system", "text": "The i-vector SRE was based on 19-dimensional MFCs plus log energy as its primary feature. This primary feature was augmented by its first and second order derivatives, resulting in a 60-dimensional feature vector. UBM consisted of 2 048 Gaussian components, and the dimensionality of the i-vector space was 400. The entire system was trained using the Kaldi-SRE08 reciprocator. In the IDF d-vector system, the architecture was based on Figure 6. The input feature was 40-dimensional Fbanks with a symmetrical 4-frame window for splitting adjacent frames, resulting in a total of 9 frames. The number of output units was 5000, corresponding to the number of speakers in the training set.In the CDF d-vector system, the linguistic factor was extended to the bottleneck layer in the form of 42-dimensional telephone posteriors, as shown in Figure 7."}, {"heading": "8.3 AER system", "text": "The input characteristic of the DNN model of the AER baseline was 40-dimensional fbanks, with a symmetrical 4-frame window for splitting adjacent frames. The time delay component with two time delay layers was used to extend the time context, and the length of the effective context window was 20 frames. It contained 6 hidden layers, and each layer had 200 units. By activating the P standard, the dimensionality of the output of the previous layer was reduced to 40. ACC and MAP definitions are specified in equations 1 - 3.Pi = TPiTPi + FPi, (1) MAP = 1s x x 0 s = 1 Pi, (2) ACC = 0 = 1 TPi = 1 (TPi + FPi), (3) where s denotes the number of emotion categories. Pi is the precision of the ith motion class TPi and the number of errors in the FPi and the number of the correct Emotion categories."}, {"heading": "8.4 Spectrum reconstruction", "text": "The spectrum reconstruction is based on the following convolution assumption: ln (x) = ln {f (q)} + ln {g (s)} + where f, g, h are the non-linear restore function for q, s and e, each implemented as DNN. The former represents the residual spectrum assumed to be Gaussian. The DNN structure for spectrum reconstruction consists of two parts: a component for generating the factor spectrum and a component for spectrum folding. The former generates component spectrum for each factor (e.g. f (q), g (s), h (e), and the latter assembles the three component spectra. The dimensional properties of the linguistic components, the loudspeaker and the emotion factors are 42, 40 and 40, respectively. With a symmetrical 4-frame window, the input dimensions of the three spectral components are reproduced."}, {"heading": "9 Appendix B: Samples of spectrum reconstruction", "text": "Here we give further examples to illustrate the reconstruction of the spectrum."}, {"heading": "9.1 Training set", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.2 Evaluation set", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Survey on speech emotion recognition: Features, classification schemes, and databases", "author": ["M. El Ayadi", "M.S. Kamel", "F. Karray"], "venue": "Pattern Recognition, vol. 44, no. 3, pp. 572\u2013587, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital signal processing, vol. 10, no. 1-3, pp. 19\u201341, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 4, pp. 1435\u20131447, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous latent variables", "author": ["C.M. Bishop"], "venue": "Pattern recognition and machine learning, 2006, ch. 12, pp. 583\u2013586.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["E. Variani", "X. Lei", "E. McDermott", "I.L. Moreno", "J. Gonzalez-Dominguez"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 4052\u20134056.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "N. Shazeer"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5115\u20135119.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end attention based text-dependent speaker verification", "author": ["S.-X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, 2016, pp. 171\u2013178.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural network-based speaker embeddings for end-to-end speaker verification", "author": ["D. Snyder", "P. Ghahremani", "D. Povey", "D. Garcia-Romero", "Y. Carmiel", "S. Khudanpur"], "venue": "Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, 2016, pp. 165\u2013170.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speaker: an end-to-end neural speaker embedding system", "author": ["C. Li", "X. Ma", "B. Jiang", "X. Li", "X. Zhang", "X. Liu", "Y. Cao", "A. Kannan", "Z. Zhu"], "venue": "arXiv preprint arXiv:1705.02304, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1695\u20131699.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Proc. Odyssey, 2014, pp. 293\u2013298.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Learning to learn. Springer, 1998, pp. 95\u2013133.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for speech and language processing", "author": ["D. Wang", "T.F. Zheng"], "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2015 Asia- Pacific. IEEE, 2015, pp. 1225\u20131237. 9", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving DNN speaker independence with i-vector inputs", "author": ["A. Senior", "I. Lopez-Moreno"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 225\u2013229.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural network based multi-factor aware joint training for robust speech recognition", "author": ["Y. Qian", "T. Tan", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2231\u20132240, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling speaker variability using long short-term memory networks for speech recognition", "author": ["X. Li", "X. Wu"], "venue": "INTERSPEECH, 2015, pp. 1086\u20131090.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative joint training with multitask recurrent model for speech and speaker recognition", "author": ["Z. Tang", "L. Li", "D. Wang", "R. Vipperla"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 3, pp. 493\u2013504, 2017.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Building a chinese natural emotional audio-visual database", "author": ["W. Bao", "Y. Li", "M. Gu", "M. Yang", "H. Li", "L. Chao", "J. Tao"], "venue": "Signal Processing (ICSP), 2014 12th International Conference on. IEEE, 2014, pp. 583\u2013587.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Mec 2016: the multimodal emotion recognition challenge of ccpr 2016", "author": ["Y. Li", "J. Tao", "B. Schuller", "S. Shan", "D. Jiang", "J. Jia"], "venue": "Chinese Conference on Pattern Recognition. Springer, 2016, pp. 667\u2013678.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2579\u20132605, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": ", automatic emotion recognition (AER) [2].", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "In fact, most of the famous SRE techniques are based on factorization models, including the Gaussian mixture model-universal background model (GMM-UBM) [3], the joint factor analysis 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "(JFA) [4] and the i-vector model [5].", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "(JFA) [4] and the i-vector model [5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "Considering that the linguistic factor can be inferred from a short segment as well [6], our finding indicates that most of the significant variations of speech signals can be well explained.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "In our experiment, the natural stochastic gradient descent (NSGD) [7] algorithm was employed for optimization.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": ", JFA [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": ", JFA [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "4 Spectrum reconstruction A key difference between CDF and the conventional factor analysis [8] is that in CDF each factor is inferred individually, without any explicit constraint defined among the factors (e.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "5 Related work The idea of learning speaker factors was motivated by Ehsan et al [9], who employed a vanilla DNN to learn frame-level representations of speakers.", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "The CDF approach is also related to the phonetic DNN i-vector approach proposed by Lei [14] and Kenny [15], where the linguistic factor (phonetic posteriors) is firstly inferred using an ASR system, which is then used as an auxiliary knowledge to infer the speaker factor (the i-vector).", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "The CDF approach is also related to the phonetic DNN i-vector approach proposed by Lei [14] and Kenny [15], where the linguistic factor (phonetic posteriors) is firstly inferred using an ASR system, which is then used as an auxiliary knowledge to infer the speaker factor (the i-vector).", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Finally, the CDF approach is related to multi-task learning [16] and transfer learning [17, 18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "Finally, the CDF approach is related to multi-task learning [16] and transfer learning [17, 18].", "startOffset": 87, "endOffset": 95}, {"referenceID": 16, "context": "Finally, the CDF approach is related to multi-task learning [16] and transfer learning [17, 18].", "startOffset": 87, "endOffset": 95}, {"referenceID": 17, "context": "[19] found that involving the speaker factor in the input feature improved ASR system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Qin [20] and Li et al.", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "[21] found that ASR and SRE systems can be trained jointly, by borrowing information from each other.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "AER database: The CHEAVD database [23] was used to train the AER systems.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "This database was selected from Chinese movies and TV programs and used as the standard database for the multimodal emotion recognition challenge (MEC 2016) [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 23, "context": "The Kaldi toolkit [25] is used to train the DNN model, following the Kaldi WSJ s5 nnet recipe.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "Visualization The discriminative capability of the speaker factor can also be examined by projecting the feature vectors to a 2-dimensional space using t-SNE [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 7, "context": "Following the convention of Ehsan et al [9], the utterance-level representations derived from DNN are called d-vectors, and accordingly the SRE system is called a d-vector system.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "The model is a typical linear Gaussian factorization model and it has been demonstrated to produce state-ofthe-art performance in SRE [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] L.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Speech signals are complex intermingling of various informative factors, and this information blending makes decoding any of the individual factors extremely difficult. A natural idea is to factorize each speech frame into independent factors, though it turns out to be even more difficult than decoding each individual factor. A major encumbrance is that the speaker trait, a major factor in speech signals, has been suspected to be a long-term distributional pattern and so not identifiable at the frame level. In this paper, we demonstrated that the speaker factor is also a short-time spectral pattern and can be largely identified with just a few frames using a simple deep neural network (DNN). This discovery motivated a cascade deep factorization (CDF) framework that infers speech factors in a sequential way, and factors previously inferred are used as conditional variables when inferring other factors. Our experiment on an automatic emotion recognition (AER) task demonstrated that this approach can effectively factorize speech signals, and using these factors, the original speech spectrum can be recovered with high accuracy. This factorization and reconstruction approach provides a novel tool for many speech processing tasks.", "creator": "LaTeX with hyperref package"}}}