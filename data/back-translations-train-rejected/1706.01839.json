{"id": "1706.01839", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Assessing the Linguistic Productivity of Unsupervised Deep Neural Networks", "abstract": "Increasingly, cognitive scientists have demonstrated interest in applying tools from deep learning. One use for deep learning is in language acquisition where it is useful to know if a linguistic phenomenon can be learned through domain-general means. To assess whether unsupervised deep learning is appropriate, we first pose a smaller question: Can unsupervised neural networks apply linguistic rules productively, using them in novel situations? We draw from the literature on determiner/noun productivity by training an unsupervised, autoencoder network measuring its ability to combine nouns with determiners. Our simple autoencoder creates combinations it has not previously encountered and produces a degree of overlap matching adults. While this preliminary work does not provide conclusive evidence for productivity, it warrants further investigation with more complex models. Further, this work helps lay the foundations for future collaboration between the deep learning and cognitive science communities.", "histories": [["v1", "Tue, 6 Jun 2017 16:16:51 GMT  (62kb,D)", "http://arxiv.org/abs/1706.01839v1", "To be presented at the 39th Annual Meeting of the Cognitive Science Society"]], "COMMENTS": "To be presented at the 39th Annual Meeting of the Cognitive Science Society", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lawrence phillips", "nathan hodas"], "accepted": false, "id": "1706.01839"}, "pdf": {"name": "1706.01839.pdf", "metadata": {"source": "CRF", "title": "Assessing the Linguistic Productivity of Unsupervised Deep Neural Networks", "authors": ["Lawrence Phillips", "Nathan Hodas"], "emails": ["(Lawrence.Phillips@pnnl.gov)", "(Nathan.Hodas@pnnl.gov)"], "sections": [{"heading": null, "text": "Keywords: deep learning; language acquisition; linguistic productivity; unsupervised learning; determinants"}, {"heading": "Introduction", "text": "Computer modeling has long played an important role in cognitive science, enabling researchers to explore the implications of cognitive theories and determine which traits are needed to account for certain phenomena (J. L. McClelland, 2009). Over time, a variety of modeling traditions have been used. While the goals of cognitive modelers have remained largely the same, increases in computer performance and architecture play a role in these shifts (J. L. McClelland, 2009). Following this pattern, recent advances in deep learning (DL) can lead to an increase in interest in cognitive modelers, which is largely the same as the increase in computer performance and architectures that play a role in these shifts (J. L. McClelland, 2009)."}, {"heading": "The Special Case of Determiners", "text": "In fact, it is such that most people who are in a position, are in a position, to go into another world, in which they are able to discover another world, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they are able to move, are able to move, are able to move, in which they are able to move, in"}, {"heading": "Deep Learning for Language Acquisition", "text": "A \"shallow\" ANN is one that postulates a single hidden layer of neurons between the input and output layers. Deep networks contain several hidden layers that allow these networks to learn more complex functions in practice, and the model parameters can be trained using the backpropogation algorithm. However, adding several hidden layers opens up a whole range of possible architectures, not all of which are necessarily applicable to problems in cognitive science or language acquisition. While the most common neural networks are discriminatory, i.e. categorizing data into specific classes, a variety of techniques have been proposed to enable truly generative neural networks, which are capable of recording input data and generating complex outputs such as images or texts that make them ideal for modeling human behavior."}, {"heading": "Corpora", "text": "In order to train our neural network, we use child-friendly language from several American-English corporations in the CHILDES database (MacWhinney, 2000), in particular the CDS statements from Bloom 1970, Brent, Brown, Kuczaj, Providence, Sachs and Suppes corpora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973; Kuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983; Suppes, 1974).The combined corpora contains nearly 1 million statements and spans a wide age range, including speech aimed at children from 6 months to 5 years. Relevant information about the corpora used can be found in the table. Because we are interested in seeing what the AE can learn from data encountered by children."}, {"heading": "Corpora Age Range N. Utterances", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Neural Network Architecture", "text": "In fact, the fact is that most of them are able to play by the rules that they have established over the past few years, and that they are able to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules that we have set ourselves. \""}, {"heading": "Baseline Models", "text": "Since the AE is in the process of reproducing its input data, one might wonder whether similar results could not be achieved by a simpler distribution model. To assess this, we also measure the performance of an n-gram language model. We train bigram and trigram language models using the modified Kneser-Ney smoothing (Heafield, Pouzyrevsky, Clark, & Koehn, 2013) implemented in the KenLM model toolkit to estimate the distribution statistics of the training corpus. Sentences are generated from the n-gram language model by selecting a seed word and then taking a new word from the amount of possible n-grams. The smoothing process allows the model to generate previously unseen n-gram sentences. Scanning of new words continues for each utterance until the end result token is generated or a maximum of 10 tokens is achieved (the maximum size for the AE would be able to generate the first one gram from the AE)."}, {"heading": "Productivity Measures", "text": "We measure the productivity of our autoencoders using the overlap value described in C. Yang (2011). Empirical overlap values are simply calculated as a percentage of unique nouns appearing immediately after the determinants a and the determinants S. The expected overlap is calculated using three numbers from the corpus under consideration: the number of unique nouns N, the number of unique determinants D, and the total number of pairs of nouns / determinants S. The expected overlap is defined as 1: O (N, D, S) = 1 NN: r = 1 O (r, D, S) = 1: 1 (1) = 1 pper = 1 pper = 1 pper = 1 pper = 1 pper = 1 pper = 1 pper = 1 pper = 1 pper = 1 pper = 1 pper."}, {"heading": "Results", "text": "This year is the highest in the history of the country."}, {"heading": "Conclusion", "text": "In the field of language learning, there is great interest in integrating deep learning methods into cognitive modelling, but a number of important hurdles remain. In the field of language learning, deep learning is willing to answer questions about the ability to learn complex linguistic phenomena without access to innate linguistic knowledge. However, it remains unclear whether unsupervised versions of deep learning models are capable of capturing even simple linguistic phenomena. In this preliminary study, we note that a simple auto-encoder with a sufficient level of dropout is capable of reflecting the productivity of its training data, although it is unclear whether this proves productivity in and of itself. Future work will need to investigate whether more complex models might be able to generate text with higher productivity, as well as further investigate how certain model decisions affect performance. It would also be worthwhile comparing AEs with simpler models such as a basic STM model of language learning techniques to provide additional motivational work to children as a combination of learning techniques."}, {"heading": "Acknowledgments", "text": "The authors thank the reviewers for their thoughtful comments and Lisa Pearl for the initial discussion on productivity."}], "references": [{"title": "Language development: Form and function in emerging grammars", "author": ["L. Bloom"], "venue": null, "citeRegEx": "Bloom,? \\Q1970\\E", "shortCiteRegEx": "Bloom", "year": 1970}, {"title": "The role of exposure to isolated words in early vocabulary development", "author": ["M. Brent", "J. Siskind"], "venue": "Cognition,", "citeRegEx": "Brent and Siskind,? \\Q2001\\E", "shortCiteRegEx": "Brent and Siskind", "year": 2001}, {"title": "A first language: The early stages", "author": ["R. Brown"], "venue": null, "citeRegEx": "Brown,? \\Q1973\\E", "shortCiteRegEx": "Brown", "year": 1973}, {"title": "The probabilistic mind", "author": ["N. Chater", "M. Oaksford"], "venue": null, "citeRegEx": "Chater and Oaksford,? \\Q2008\\E", "shortCiteRegEx": "Chater and Oaksford", "year": 2008}, {"title": "The acquisition of regular and irregular", "author": ["S. Kuczaj"], "venue": null, "citeRegEx": "Kuczaj,? \\Q1977\\E", "shortCiteRegEx": "Kuczaj", "year": 1977}, {"title": "How cognitive modeling can benefit", "author": ["M.D. Lee"], "venue": null, "citeRegEx": "Lee,? \\Q2011\\E", "shortCiteRegEx": "Lee", "year": 2011}, {"title": "The place of modeling in cognitive", "author": ["J.L. McClelland"], "venue": null, "citeRegEx": "McClelland,? \\Q2009\\E", "shortCiteRegEx": "McClelland", "year": 2009}, {"title": "Evaluating learning-strategy components", "author": ["L. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2014\\E", "shortCiteRegEx": "Pearl", "year": 2014}, {"title": "Zipf\u2019s word frequency law in natural language: A critical review and future directions", "author": ["S. Piantadosi"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "Piantadosi,? \\Q2014\\E", "shortCiteRegEx": "Piantadosi", "year": 2014}, {"title": "Do young children have adult-like syntactic categories? Zipf\u2019s law and the case of the determiner", "author": ["J.M. Pine", "D. Freudenthal", "G. Krajewski", "F. Gobet"], "venue": null, "citeRegEx": "Pine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pine et al\\.", "year": 2013}, {"title": "Syntactic categories in the speech of young children: The case of the determiner", "author": ["J.M. Pine", "H. Martindale"], "venue": "Journal of Child Language,", "citeRegEx": "Pine and Martindale,? \\Q1996\\E", "shortCiteRegEx": "Pine and Martindale", "year": 1996}, {"title": "Talking about the there and then: The emergence of displaced reference in parent-child discourse", "author": ["J. Sachs"], "venue": null, "citeRegEx": "Sachs,? \\Q1983\\E", "shortCiteRegEx": "Sachs", "year": 1983}, {"title": "Workshop on deep learning and the brain", "author": ["A. Saxe"], "venue": null, "citeRegEx": "Saxe,? \\Q2014\\E", "shortCiteRegEx": "Saxe", "year": 2014}, {"title": "Do infants possess innate knowledge structures? The con side", "author": ["L.B. Smith"], "venue": "Developmental Science,", "citeRegEx": "Smith,? \\Q1999\\E", "shortCiteRegEx": "Smith", "year": 1999}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the conference on empirical methods in natural language processing (pp. 151\u2013161)", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "The semantics of children\u2019s language", "author": ["P. Suppes"], "venue": "American Psychologist,", "citeRegEx": "Suppes,? \\Q1974\\E", "shortCiteRegEx": "Suppes", "year": 1974}, {"title": "Connectionist models of cognition", "author": ["M.S. Thomas", "J.L. McClelland"], "venue": "Cambridge handbook of computational cognitive modelling,", "citeRegEx": "Thomas and McClelland,? \\Q2008\\E", "shortCiteRegEx": "Thomas and McClelland", "year": 2008}, {"title": "Abstract categories or limited-scope formulae? The case of children\u2019s determiners", "author": ["V. Valian", "S. Solt", "J. Stewart"], "venue": "Journal of Child Language,", "citeRegEx": "Valian et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Valian et al\\.", "year": 2009}, {"title": "An autoencoder neural-network based low-dimensionality approach to excitation modeling for HMM-based text-tospeech", "author": ["S. Vishnubhotla", "R. Fernandez", "B. Ramabhadran"], "venue": "In IEEE International Conference on Acoustics Speech and Signal Processing (pp. 4614\u20134617)", "citeRegEx": "Vishnubhotla et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vishnubhotla et al\\.", "year": 2010}, {"title": "A statistical test for grammar", "author": ["C. Yang"], "venue": "In Proceedings of the 2nd workshop on Cognitive Modeling and Computational Linguistics", "citeRegEx": "Yang,? \\Q2011\\E", "shortCiteRegEx": "Yang", "year": 2011}, {"title": "Universal grammar, statistics or both", "author": ["C.D. Yang"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Yang,? \\Q2004\\E", "shortCiteRegEx": "Yang", "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "While the 1980s saw the rise in popularity of connectionism (Thomas & McClelland, 2008), more recently symbolic Bayesian models have risen to prominence (Chater & Oaksford, 2008; Lee, 2011).", "startOffset": 153, "endOffset": 189}, {"referenceID": 12, "context": "Following this pattern, recent advances in the area of deep learning (DL) have led to a rise in interest from the cognitive science community as demonstrated by a number of recent workshops dedicated to DL (Saxe, 2014; J. McClelland, Hansen, & Saxe, 2016; J. McClelland, Frank, & Mirman, 2016).", "startOffset": 206, "endOffset": 293}, {"referenceID": 13, "context": "tion where researchers have argued over whether particular aspects of language could ever be learned by a child without the use of innate, language-specific mechanisms (Smith, 1999; C. D. Yang, 2004; Chater & Christiansen, 2010; Pearl, 2014).", "startOffset": 168, "endOffset": 241}, {"referenceID": 7, "context": "tion where researchers have argued over whether particular aspects of language could ever be learned by a child without the use of innate, language-specific mechanisms (Smith, 1999; C. D. Yang, 2004; Chater & Christiansen, 2010; Pearl, 2014).", "startOffset": 168, "endOffset": 241}, {"referenceID": 19, "context": "Yang, 2011). This simple measure has been the source of some controversy. C. Yang (2011) argues that early", "startOffset": 0, "endOffset": 89}, {"referenceID": 9, "context": "Differences in pre-processing have lead researchers to draw opposite conclusions from similar data, making interpretation quite difficult (C. Yang, 2011; Pine et al., 2013).", "startOffset": 138, "endOffset": 172}, {"referenceID": 9, "context": "Yang, 2011; Pine et al., 2013). Indeed, most corpora involving individual children are small enough that Meylan et al. (2017) argue it is impossible to make a statistically significant claim as to child productivity.", "startOffset": 12, "endOffset": 126}, {"referenceID": 9, "context": "Yang, 2011; Pine et al., 2013). Indeed, most corpora involving individual children are small enough that Meylan et al. (2017) argue it is impossible to make a statistically significant claim as to child productivity. For analyzing whether or not text generated by a DL model is productive or not, we thankfully do not need to fully address the problem of inferring child productivity. Ideally, the model would demonstrate a similar level of overlap to the data it was exposed to. We make use of the overlap statistic from Yang because it is more easily comparable to other works and has been better studied than the more recent Bayesian metric of Meylan et al. (2017).", "startOffset": 12, "endOffset": 668}, {"referenceID": 0, "context": "In particular, we make use of the CDS utterances in the Bloom 1970, Brent, Brown, Kuczaj, Providence, Sachs, and Suppes corpora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973; Kuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983; Suppes, 1974).", "startOffset": 128, "endOffset": 245}, {"referenceID": 2, "context": "In particular, we make use of the CDS utterances in the Bloom 1970, Brent, Brown, Kuczaj, Providence, Sachs, and Suppes corpora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973; Kuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983; Suppes, 1974).", "startOffset": 128, "endOffset": 245}, {"referenceID": 4, "context": "In particular, we make use of the CDS utterances in the Bloom 1970, Brent, Brown, Kuczaj, Providence, Sachs, and Suppes corpora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973; Kuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983; Suppes, 1974).", "startOffset": 128, "endOffset": 245}, {"referenceID": 11, "context": "In particular, we make use of the CDS utterances in the Bloom 1970, Brent, Brown, Kuczaj, Providence, Sachs, and Suppes corpora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973; Kuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983; Suppes, 1974).", "startOffset": 128, "endOffset": 245}, {"referenceID": 15, "context": "In particular, we make use of the CDS utterances in the Bloom 1970, Brent, Brown, Kuczaj, Providence, Sachs, and Suppes corpora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973; Kuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983; Suppes, 1974).", "startOffset": 128, "endOffset": 245}, {"referenceID": 19, "context": "Yang (2011). Words both in the child-directed corpus and the autoencoder-generated output are tagged using the default part-of-speech tagger from NLTK.", "startOffset": 0, "endOffset": 12}, {"referenceID": 19, "context": "Yang (2011) set equal to 1 and which we optimize over the training corpus using least squares estimation and set at 1.", "startOffset": 0, "endOffset": 12}, {"referenceID": 8, "context": "It should be noted that Zipfian distributions are not perfect models of word frequencies (Piantadosi, 2014), but assigning empirically-motivated values to the determiner probabilities and Zipfian parameter a represents an improvement upon the original measure.", "startOffset": 89, "endOffset": 107}, {"referenceID": 19, "context": "Expected overlap scores were calculated as in Yang (2011). Empirical overlap was calculated as the percent of unique nouns that appeared immediately following both a and the.", "startOffset": 46, "endOffset": 58}, {"referenceID": 19, "context": "Yang (2011) report a correlation of 0.", "startOffset": 0, "endOffset": 12}], "year": 2017, "abstractText": "Increasingly, cognitive scientists have demonstrated interest in applying tools from deep learning. One use for deep learning is in language acquisition where it is useful to know if a linguistic phenomenon can be learned through domain-general means. To assess whether unsupervised deep learning is appropriate, we first pose a smaller question: Can unsupervised neural networks apply linguistic rules productively, using them in novel situations? We draw from the literature on determiner/noun productivity by training an unsupervised, autoencoder network measuring its ability to combine nouns with determiners. Our simple autoencoder creates combinations it has not previously encountered and produces a degree of overlap matching adults. While this preliminary work does not provide conclusive evidence for productivity, it warrants further investigation with more complex models. Further, this work helps lay the foundations for future collaboration between the deep learning and cognitive science communities.", "creator": "TeX"}}}