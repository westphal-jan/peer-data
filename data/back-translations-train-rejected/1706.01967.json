{"id": "1706.01967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic Similarity", "abstract": "Semantic similarity measures are an important part in Natural Language Processing tasks. However Semantic similarity measures built for general use do not perform well within specific domains. Therefore in this study we introduce a domain specific semantic similarity measure that was created by the synergistic union of word2vec, a word embedding method that is used for semantic similarity calculation and lexicon based (lexical) semantic similarity methods. We prove that this proposed methodology out performs word embedding methods trained on generic corpus and methods trained on domain specific corpus but do not use lexical semantic similarity methods to augment the results. Further, we prove that text lemmatization can improve the performance of word embedding methods.", "histories": [["v1", "Tue, 6 Jun 2017 20:45:30 GMT  (1663kb,D)", "https://arxiv.org/abs/1706.01967v1", "6 Pages, 3 figures"], ["v2", "Fri, 9 Jun 2017 01:54:32 GMT  (1664kb,D)", "http://arxiv.org/abs/1706.01967v2", "6 Pages, 3 figures"]], "COMMENTS": "6 Pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["keet sugathadasa", "buddhi ayesha", "nisansa de silva", "amal shehan perera", "vindula jayawardana", "dimuthu lakmal", "madhavi perera"], "accepted": false, "id": "1706.01967"}, "pdf": {"name": "1706.01967.pdf", "metadata": {"source": "CRF", "title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic Similarity", "authors": ["Keet Sugathadasa", "Buddhi Ayesha", "Nisansa de Silva", "Amal Shehan Perera", "Vindula Jayawardana", "Dimuthu Lakmal", "Madhavi Perera"], "emails": ["keetmalin.13@cse.mrt.ac.lk"], "sections": [{"heading": null, "text": "This year it is more than ever before."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "This section illustrates the background to the techniques used in this study and the work carried out by others in various fields relevant to this research. The following subsections are the most important key areas of this study."}, {"heading": "A. Lexical Semantic Similarity Measures", "text": "The lexical semantic similarity of two entities is a measure of the similarity of the semantic content of these entities, most commonly calculated using topological similarity within an ontology such as WordNet [14]. Wu and Palmer proposed a method to determine the similarity between two words in the range of 0 to 1. [11] In comparison, Jiang and Conrath proposed a method to measure the lexical semantic similarity between word pairs using corpus statistics and lexical taxonomy [12]. Hirst & St-Onge's system [13] quantifies the extent to which the relevant synsets are connected by a path that is not too long and often does not change direction. [10] The strengths of each of these algorithms were evaluated using [15]."}, {"heading": "B. Word Vector Embedding", "text": "Traditionally, words in natural language processing systems are treated as atomic units, ignoring the correlation between words as represented only by indexes in a vocabulary. [6] To address the shortcomings of this approach, a distributed representation of words and phrases by word embedding has been proposed [16]. The idea is to create vector representations for each word in a text document, along with word meanings and relationships between the words, which are all assigned to a common vector space. A number of word vector embedding systems have been proposed, such as GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19]. GloVe uses a word for adjacent word mapping when learning dense embedding using a matrix factorization mechanism. LDA uses a similar approach via matrices, but the concept of netural is based on the 2W attribution of relevant words."}, {"heading": "C. Legal Information Systems", "text": "Schweighofer [21] claims that there is a huge vacuum to address the information crisis afflicting legal applications, a vacuum highlighted by the fact that, while important, there is a1https: / / code.google.com / p / word2vec / scarcity of legal information systems. Although the two main commercial systems, WestLaw2 and LexisNexis3, are widely used, they only offer a query-based search, where justice officials must remember keywords that are predefined when retrieving relevant legal information, there are still difficulties in accessing this information. One of the most popular legal information systems is KONTERM [21], which was designed to represent document structures and content, but also suffered from scalability problems that come closest to our proposed model."}, {"heading": "III. METHODOLOGY", "text": "This section describes the research carried out. Each section below deals with a component of the general methodology. An overview of the methodology we propose is illustrated in Fig. 1. The first phase of this study was to collect the necessary legal cases from online repositories. We obtained over 35,000 legal case documents relating to various areas of legal practice from Findlaw [23] through web crawling. Therefore, the system tends to be generalised far beyond many aspects of law."}, {"heading": "A. Text Lemmatization", "text": "The linguistic process of mapping the flexed forms of a word to the core problem of the word is called lemmatization [24]. The flexed text of natural language would contain words of all flexed forms. However, the standard Word2vec model does not result in a lemmatizer before the word vector embedding is calculated, which results in each of the flexed forms of a single word ending in a separate embedding vector. This, in turn, leads to many disadvantages and inefficiencies. Maintaining a separate vector for each flexed form of each word leads to the model needlessly inflating a consumption memory. In particular, this leads to problems in constructing the model. Furthermore, a separate vector for each flexed form weakens the model, because the values for words stemming from the same dilemma are distributed over these multiple vectors. For example, if we search for similar words that the noun \"judge,\" we get the following words as: judges, judges, judges, judges, judges, judges, judges, judges, judges, judges."}, {"heading": "B. Training word2vec models", "text": "At this stage we trained wod2vec models. One was on the raw legal text corpus and the other was on the lemmatized legal text corpus. Each input legal text corpus contained text from over 35,000 legal case documents totaling 20 billion words. The training lasted over 2 days. As mentioned in Section III-A, the model formed by the raw legal text corpus is the word2vecLRmodel shown in Fig. 1. The model trained on lemmatized legal text corpus is word2vecLLL to build the word2vecLSmodel. Below are the important parameters we specified in the training of these models. \u2022 Size (context window size): 10 \u2022 Learning model: CBOW \u2022 min-count: 5 \u2022 Training algorithm: hierarchical softmax."}, {"heading": "C. Lexical Semantic Similarity Enhancements", "text": "In this step we used the proven lexical model: The first step was that we provide access to the curricula and syllabuses of individual syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses of the syllabuses of the syllabuses and syllabuses of the syllabuses and syllabuses."}, {"heading": "D. Query processing", "text": "In order to use and test our models, we have developed a query processing system that allows a user to enter a query into the legal domain via the provided user interface. Then, the system performs the query and applies natural language processing techniques such as PoS tagging until the query passes through the NLP pipeline to be lemmatized. We used the same Stanford Core NLP pipeline that we used in Section III-A for this task, which is to bring all models to the same level in order to compare them equally. We have shown this step in Figure 1."}, {"heading": "E. Experiments", "text": "The gold standard consists of 100 concepts, each of which contains 5 words most closely related to the given concept in the legal field, selected by the legal experts from a pool of over 1500 words. Accuracy levels of these experiments are measured in terms of precision and retrieval [1], which are common space measures when retrieving information. The logical functionality of these two is based on comparing an expected result and the effective result of the evaluated system. If the gold standard is word vector G and the word vector returned by the model is W (same naming conventions as Section III-C1), retrieval in this study is calculated using Eq.11. This measures the completeness of the model. Our retrieval calculation used the same function proposed in [1]. Recall = | G'W | | G | (11) The accuracy calculation in this study is not as clear as described in [1]."}, {"heading": "IV. RESULTS", "text": "This section includes results from the four different models (word2vecG, word2vecLR, word2vecLL and word2vecLLS) introduced in Section III-B. Results shown in Table II were obtained for different k values, where k is the number of words requested by each of the models. As expected, the formula 1 of each model increases with k, increasing the possibility of finding the correct similar words against the gold standard. Given the fact that the task here is to return the expected set of words, the recall is more important than precision (i.e.: false negatives are more harmful than false positives).In this light, it is obvious that the word2vecLLLSperforms are better than all other models, because they consistently have the highest memory of all values of k. Furthermore, the word2vecLecL and LecdS study LecdLvecL models we have considered as all models L."}, {"heading": "V. CONCLUSION AND FUTURE WORKS", "text": "The first assertion of the hypothesis is that a word embedding model formed on a small domain-specific corpus can execute a word embedding model formed on a large but generic corpus. word2vecLRmodel's success over the word2vecGmodel justifies this assertion. In Section III-A, we proposed the second assertion: word lemmatization, which removes inflected forms from words and improves the performance of a word embedding model. word2vecLLmodel, which achieves better results than word2vecLRmodel, proves this assertion to be true. The third assertion was made in Section III-C. There, we suggested that the use of lexical semantic similarity measures trained via a machine learning system can improve the overall performance of the system. The significant improvement we show for the word2vLecmodel is a sample based on the Sample."}], "references": [{"title": "Ontology-based information extraction: An introduction and a survey of current approaches", "author": ["D.C. Wimalasuriya", "D. Dou"], "venue": "Journal of Information Science, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Representation of change in controlled medical terminologies", "author": ["D.E. Oliver", "Y. Shahar", "E.H. Shortliffe", "M.A. Musen"], "venue": "Artificial intelligence in medicine, vol. 15, no. 1, pp. 53\u201376, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Discovering inconsistencies in pubmed abstracts through ontology-based information extraction", "author": ["N. de Silva", "D. Dou", "J. Huang"], "venue": "ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB), p. to appear, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of machine learning research, vol. 3, no. Feb, pp. 1137\u20131155, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Semap-mapping dependency relationships into semantic frame relationships", "author": ["N. de Silva", "C. Fernando", "M. Maldeniya", "D. Wijeratne", "A. Perera", "B. Goertzel"], "venue": "17th ERU Research Symposium, vol. 17. Faculty of Engineering, University of Moratuwa, Sri Lanka, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach", "author": ["H.T. Ng", "H.B. Lee"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 40\u201347.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Safs3 algorithm: Frequency statistic and semantic similarity based semantic classification use case", "author": ["N. de Silva"], "venue": "Advances in ICT for Emerging Regions (ICTer), 2015 Fifteenth International Conference on. IEEE, 2015, pp. 77\u201383.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, ser. ACL \u201994. Stroudsburg, PA, USA: Association for Computational Linguistics, 1994, pp. 133\u2013138. [Online]. Available: http://dx.doi.org/10.3115/981732.981751", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "Proc of 10th International Conference on Research in Computational Linguistics, ROCLING97, 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Lexical chains as representations of context for the detection and correction of malapropisms", "author": ["G. Hirst", "D. St-Onge"], "venue": "WordNet: An electronic lexical database, vol. 305, pp. 305\u2013332, 1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Introduction to wordnet: An on-line lexical database", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross", "K.J. Miller"], "venue": "International journal of lexicography, vol. 3, no. 4, pp. 235\u2013244, 1990.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "2016) Wordnet similarity for java (ws4j)", "author": ["H. Shima"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Gaussian lda for topic models with word embeddings.", "author": ["R. Das", "M. Zaheer", "C. Dyer"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "word2vec explained: Deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Y. Goldberg", "O. Levy"], "venue": "arXiv preprint arXiv:1402.3722, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple word embedding model for lexical substitution", "author": ["O. Melamud", "O. Levy", "I. Dagan", "I. Ramat-Gan"], "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, 2015, pp. 1\u20137.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Legal expert system kontermautomatic representation of document structure and contents", "author": ["E. Schweighofer", "W. Winiwarter"], "venue": "International Conference on Database and Expert Systems Applications. Springer, 1993, pp. 486\u2013497.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Gov2vec: Learning distributed representations of institutions and their legal text", "author": ["J.J. Nay"], "venue": "arXiv preprint arXiv:1609.06616, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Rules for mediation in findlaw for legal professionals", "author": ["J. Hughes"], "venue": "1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Stemming and lemmatization in the clustering of finnish text documents", "author": ["T. Korenius", "J. Laurikkala", "K. J\u00e4rvelin", "M. Juhola"], "venue": "Proceedings of the thirteenth ACM international conference on Information and knowledge management. ACM, 2004, pp. 625\u2013633.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "The stanford corenlp natural language processing toolkit.", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Semantic Similarity measurements based on linguistic features are a fundamental component of almost all Natural Language Processing (NLP) tasks: Information Retrieval, Information Extraction, and Natural Language Understanding [1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 1, "context": "Another field which suffers similarly from this issue is the medical industry [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "Later studies such as [5] utilize these repositories.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "Methods that treat words as independent atomic units is not sufficient to capture the expressiveness of language [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "A solution to this is word context learning methods [7]\u2013 [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "A solution to this is word context learning methods [7]\u2013 [9].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Another solution is lexical semantic similarity based methods [10].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "First for word context learning, we used a Word Embedding [7] method, word2vec [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "First for word context learning, we used a Word Embedding [7] method, word2vec [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "of lexical semantic similarity measures [11]\u2013[13] to augment and improve the result.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "of lexical semantic similarity measures [11]\u2013[13] to augment and improve the result.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Lexical Semantic similarity of two entities is a measure of the likeness of the semantic content of those entities, most commonly calculated with the help of topological similarity existing within an ontology such as WordNet [14].", "startOffset": 225, "endOffset": 229}, {"referenceID": 8, "context": "Wu and Palmer proposed a method to give the similarity between two words in the 0 to 1 range [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "In comparison, Jiang and Conrath proposed a method to measure the lexical semantic similarity between word pairs using corpus statistics and lexical taxonomy [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Hirst & St-Onge\u2019s system [13] quantifies the amount that the relevant synsets are connected by a path that is not too long and that does not change direction often.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "The strengths of each of these algorithms were evaluated in [10] by means of [15].", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "The strengths of each of these algorithms were evaluated in [10] by means of [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Traditionally, in Natural Language Processing systems, words are treated as atomic units ignoring the correlation between the words as they are represented just by indices in a vocabulary [6].", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "To solve the inadequacies of that approach, distributed representation of words and phrases through word embeddings was proposed [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "A number of Word Vector Embedding systems have been proposed such as: GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "A number of Word Vector Embedding systems have been proposed such as: GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "A number of Word Vector Embedding systems have been proposed such as: GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "word2vec supports two main training models: Skip-gram [20] and Continuous Bag Of Words (CBOW) [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "word2vec supports two main training models: Skip-gram [20] and Continuous Bag Of Words (CBOW) [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "Schweighofer [21], claims that there is a huge vacuum that should be addressed in eradicating the information crisis that the applications in the field of law suffer from.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "One of the most popular legal information retrieval systems is KONTERM [21], which was developed to represent document structures and contents.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "The currently existing implementation that is closest to our proposed model is Gov2Vec [22], which is a system that creates vector representations of words in the legal domain, by creating a vocabulary from across all corpora on, supreme court opinions, presidential actions and official summaries of congressional bills.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "It uses a neural network [7] to predict the target word with the mean of its context words\u2019 vectors.", "startOffset": 25, "endOffset": 28}, {"referenceID": 20, "context": "We obtained over 35000 legal case documents, pertaining to various areas of practices in law, from Findlaw [23] by web crawling.", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "The linguistic process of mapping inflected forms of a word to the word\u2019s core lemma is called lemmatization [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "For this task we use the Stanford CoreNLP library [25].", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "The lexical semantic similarity calculated between wi and wj using the Wu & Palmer\u2019s model [11] was given by wup(wi, wj).", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "The lexical semantic similarity calculated by Jiang & Conrath\u2019s model [12] was given by jcn(wi, wj).", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "hso(wi, wj) was used to indicate the lexical semantic similarity calculated using the Hirst & St-Onge\u2019s system [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "The returned xnorm is a double value that has the range [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "The accuracy levels of these experiments are measured in terms of precision and recall [1], which are common place measures in information retrieval.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "Our recall calculation used the same function suggested in [1].", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "The precision calculation in this study is not as clear cut as it is described in [1].", "startOffset": 82, "endOffset": 85}], "year": 2017, "abstractText": "Semantic similarity measures are an important part in Natural Language Processing tasks. However Semantic similarity measures built for general use do not perform well within specific domains. Therefore in this study we introduce a domain specific semantic similarity measure that was created by the synergistic union of word2vec, a word embedding method that is used for semantic similarity calculation and lexicon based (lexical) semantic similarity methods. We prove that this proposed methodology out performs word embedding methods trained on generic corpus and methods trained on domain specific corpus but do not use lexical semantic similarity methods to augment the results. Further, we prove that text lemmatization can improve the performance of word embedding methods. keywords: Word Embedding, Semantic Similarity, Neural Networks, Lexicon, word2vec", "creator": "LaTeX with hyperref package"}}}