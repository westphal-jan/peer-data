{"id": "1706.01991", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Unsupervised Neural-Symbolic Integration", "abstract": "Symbolic has been long considered as a language of human intelligence while neural networks have advantages of robust computation and dealing with noisy data. The integration of neural-symbolic can offer better learning and reasoning while providing a means for interpretability through the representation of symbolic knowledge. Although previous works focus intensively on supervised feedforward neural networks, little has been done for the unsupervised counterparts. In this paper we show how to integrate symbolic knowledge into unsupervised neural networks. We exemplify our approach with knowledge in different forms, including propositional logic for DNA promoter prediction and first-order logic for understanding family relationship.", "histories": [["v1", "Tue, 6 Jun 2017 21:58:50 GMT  (56kb,D)", "https://arxiv.org/abs/1706.01991v1", null], ["v2", "Thu, 22 Jun 2017 04:11:21 GMT  (56kb,D)", "http://arxiv.org/abs/1706.01991v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["son n tran"], "accepted": false, "id": "1706.01991"}, "pdf": {"name": "1706.01991.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Neural-Symbolic Integration", "authors": ["Son N. Tran"], "emails": ["son.tran@csiro.au"], "sections": [{"heading": "1 Introduction", "text": "Over the last two decades, we have seen ourselves able to be in a position, to be in a position, to be in the position we are in."}, {"heading": "2 Confidence Rules: Revisit", "text": "A rule of trust [Tran and d'Avila Garcez, 2013; Tran and Garcez, 2016] is a propositional formula in the form: c: h \u2194 t-xk (1) where h is called a hypothesis, c is a non-negative real value called a trustworthy. Consequently, the model holding the hypothesis can be found. If there is a target variable y, the conclusion of such a variable will be similar to Ponens mode as shown in Table 1An interesting feature of the confidence rules is that it can be represented in an RBM where Gibbs sampling can be seenar Xiv."}, {"heading": "3 Knowledge Encoding", "text": "In many cases, background knowledge contains a set of if-then formulas (or equivalent horn clauses), and this section shows how to convert them into trust rules for both first-order proposition and first-order logic forms."}, {"heading": "3.1 Proposition Logic", "text": "A propository if-then formula has the formula: y-xt-t-t-xk (xk) and then the confidence rules: c: hy-y-t-xt-t-k-xkc: ht-t-c: hk-xk-t-k (xk) and then the confidence rules: c: hy-y-t-xxxxxkc: ht-t-c: hk-xk-xk-t-k (xk). Fortunately, this can be solved by combining these rules into an RBM, xk with a max-pooling hidden unit leading to an RBM with the energy function: c: 12: E = \u2212 c-xhy (y-t-xk-xk-hxt formula \u2212 hxt)."}, {"heading": "3.2 First-order Logic", "text": "A logical formula of first order can also be converted into a set of trust rules. First, let's consider a predicate: P (x, y), which can be represented in a sentence DNF as follows: A, B | P (a, b) = truepx = a = py = b, otherwise they are false; PP is the sentence indicating that if1The proof is similar to [Tran, 2017] 20 < < 1the value of P (a, b). Any conjunction in this DNF can then be represented as a rule of trust. Now, let's consider a formula of first order, which we are also able to present in a set of trust rules."}, {"heading": "4 Empirical Evaluation", "text": "In this section, we will apply the encryption approaches discussed in the previous section to integrate knowledge into unattended networks."}, {"heading": "4.1 DNA promoter", "text": "The DNA promoter data set consists of a background theory with 14 logical if-then rules [Towell and Shavlik, 1994]. The rules include four symbols contact, minus 10, minus 35, conformation, which are not observed in the data. This is suitable for hierarchical models, as shown in previous work [Towell and Shavlik, 1994; Tran and Garcez, 2016]. In this experiment, we group the rules using hypothetical symbols to eliminate the invisible symbols. Then, we code the rules in an RBM following the theory in Section 3.1. The trust values are empirically selected. We test the normal RBMs and the RBMs with coded rule using leave-one-out method, both achieving 100% accuracy. To evaluate the effectiveness of our approach, we divide the data into nine different training test sets, with the number of training samples 10, 20, 40, 60, 80, 90."}, {"heading": "4.2 Kinship", "text": "In this experiment, we are using the approach we discussed in section 3.2 for discovering and arguing the relationship between the two countries. (1) What is the relationship between the two countries? (2) One person has a relationship with which to identify. (2) Another person has a relationship in which to refer to the other. (2) Another person is that he has a relationship with other countries. (3) Another person is that he has a relationship with other countries. (3) Another person is that he has a relationship with other countries. (3) Another person is that he has a relationship with other countries. (3) Another person is that he has a relationship with other countries. (4)"}, {"heading": "5 Conclusions", "text": "This work is based on the theoretical recognition that any propositional formula can be represented in RBMs [Tran, 2017]. We show that converting background knowledge in the form of if-then rules into confidence rules for coding is efficient. In the experiments, we evaluate our approaches to predicting DNA promoters and arguing relationships to demonstrate the validity of the approach."}], "references": [{"title": "11(1):5977", "author": ["Artur S. Avila Garcez", "Gerson Zaverucha. The connectionist inductive learning", "logic programming system. Applied Intelligence"], "venue": "July", "citeRegEx": "Avila Garcez and Zaverucha. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "2(1):1\u2013127", "author": ["Yoshua Bengio. Learning deep architectures for ai. Found. Trends Mach. Learn."], "venue": "January", "citeRegEx": "Bengio. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "d\u2019AvilaGarcez", "author": ["Manoel V.M. Fran\u00e7a", "Gerson Zaverucha", "Artur S"], "venue": "Fast relational learning using bottom clause propositionalization with artificial neural networks. Machine Learning, 94(1):81\u2013104,", "citeRegEx": "Fran\u00e7a et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "d\u2019Avila Garcez", "author": ["S Artur"], "venue": "Lus C. Lamb, and Dov M. Gabbay. Neural-Symbolic Cognitive Reasoning. Springer Publishing Company, Incorporated,", "citeRegEx": "Garcez et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 1\u201312", "author": ["Geoffrey E. Hinton. Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society"], "venue": "Hillsdale, NJ: Erlbaum,", "citeRegEx": "Hinton. 1986", "shortCiteRegEx": null, "year": 1986}, {"title": "d\u2019Avila Garcez", "author": ["Leo de Penning", "Artur S"], "venue": "Lus C. Lamb, and John-Jules Ch Meyer. A neuralsymbolic cognitive agent for online learning and reasoning. In IJCAI, pages 1653\u20131658,", "citeRegEx": "Penning et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Constituent structure and explanation in an integrated connectionist/symbolic cognitive architecture", "author": ["Paul Smolensky"], "venue": "C. Mcdonald, editor, Connectionism: Debates on Psychological Explanation, pages 221\u2013290. Blackwell, Cambridge,", "citeRegEx": "Smolensky. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher et al", "2013] Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "editors", "author": ["Ilya Sutskever", "Geoffrey E Hinton. Using matrices to model symbolic relationship. In D. Koller", "D. Schuurmans", "Y. Bengio", "L. Bottou"], "venue": "Advances in Neural Information Processing Systems 21, pages 1593\u20131600. Curran Associates, Inc.,", "citeRegEx": "Sutskever and Hinton. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Artificial Intelligence", "author": ["Geoffrey G. Towell", "Jude W. Shavlik. Knowledge-based artificial neural networks"], "venue": "70(1-2):119\u2013165,", "citeRegEx": "Towell and Shavlik. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Tran and Artur d\u2019Avila Garcez", "author": ["N Son"], "venue": "Knowledge extraction from deep belief networks for images. In IJCAI-2013 Workshop on NeuralSymbolic Learning and Reasoning,", "citeRegEx": "Tran and d.Avila Garcez. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep logic networks: Inserting and extracting knowledge from deep belief networks", "author": ["Son Tran", "Artur Garcez"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, PP(99):1\u201313,", "citeRegEx": "Tran and Garcez. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Propositional knowledge representation in restricted boltzmann machines", "author": ["Son N. Tran"], "venue": "https://arxiv.org/abs/1705.10899,", "citeRegEx": "Tran. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Knowledge infusion", "author": ["G. Valiant"], "venue": "[Valiant,", "citeRegEx": "Valiant.,? \\Q2006\\E", "shortCiteRegEx": "Valiant.", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 6, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 0, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 13, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 3, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 5, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 2, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 11, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 9, "context": "In previous work, supervised neural networks have been used intensively for the integration based on the analogy of modus ponens inference with symbolic rules and forward passing in neural networks [Towell and Shavlik, 1994; Avila Garcez and Zaverucha, 1999].", "startOffset": 198, "endOffset": 258}, {"referenceID": 0, "context": "In previous work, supervised neural networks have been used intensively for the integration based on the analogy of modus ponens inference with symbolic rules and forward passing in neural networks [Towell and Shavlik, 1994; Avila Garcez and Zaverucha, 1999].", "startOffset": 198, "endOffset": 258}, {"referenceID": 12, "context": "Recent work shows that any propositional formula can be represented in restricted Boltzmann machines [Tran, 2017].", "startOffset": 101, "endOffset": 113}, {"referenceID": 5, "context": "Several attempts have been made recently to integrate symbolic representation and RBMs [Penning et al., 2011; Tran and Garcez, 2016].", "startOffset": 87, "endOffset": 132}, {"referenceID": 11, "context": "Several attempts have been made recently to integrate symbolic representation and RBMs [Penning et al., 2011; Tran and Garcez, 2016].", "startOffset": 87, "endOffset": 132}, {"referenceID": 12, "context": "In this paper, we show how to encode symbolic knowledge in both propositional and firstorder forms into the RBM by extending the theory in [Tran, 2017].", "startOffset": 139, "endOffset": 151}, {"referenceID": 10, "context": "A confidence rule [Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016] is a propositional formula in the form:", "startOffset": 18, "endOffset": 72}, {"referenceID": 11, "context": "A confidence rule [Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016] is a propositional formula in the form:", "startOffset": 18, "endOffset": 72}, {"referenceID": 12, "context": "equivalently as maximising the total (weighted) satisfiability [Tran, 2017].", "startOffset": 63, "endOffset": 75}, {"referenceID": 5, "context": "In fact, in such tasks as knowledge extraction, transfer, and integration Confidence rules have been already employed [Penning et al., 2011; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016].", "startOffset": 118, "endOffset": 194}, {"referenceID": 10, "context": "In fact, in such tasks as knowledge extraction, transfer, and integration Confidence rules have been already employed [Penning et al., 2011; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016].", "startOffset": 118, "endOffset": 194}, {"referenceID": 11, "context": "In fact, in such tasks as knowledge extraction, transfer, and integration Confidence rules have been already employed [Penning et al., 2011; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016].", "startOffset": 118, "endOffset": 194}, {"referenceID": 11, "context": "For knowledge integration previous work separates the if-and-only-if symbol in Confidence rules into two if-then rules to encode in a hierarchical networks [Tran and Garcez, 2016].", "startOffset": 156, "endOffset": 179}, {"referenceID": 12, "context": "This is because it violates the condition that the DNF of a formula should have at most one conjunct is true given an assignment [Tran, 2017].", "startOffset": 129, "endOffset": 141}, {"referenceID": 12, "context": "The proof is similar as in [Tran, 2017] 0 < < 1", "startOffset": 27, "endOffset": 39}, {"referenceID": 9, "context": "1 DNA promoter The DNA promoter dataset consist of a background theory with 14 logical if-then rules [Towell and Shavlik, 1994].", "startOffset": 101, "endOffset": 127}, {"referenceID": 9, "context": "This is suitable for hierarchical models as shown in previous works [Towell and Shavlik, 1994; Tran and Garcez, 2016].", "startOffset": 68, "endOffset": 117}, {"referenceID": 11, "context": "This is suitable for hierarchical models as shown in previous works [Towell and Shavlik, 1994; Tran and Garcez, 2016].", "startOffset": 68, "endOffset": 117}, {"referenceID": 4, "context": "2 for relation discovery and reasoning tasks with Kinship dataset [Hinton, 1986; Sutskever and Hinton, 2008].", "startOffset": 66, "endOffset": 108}, {"referenceID": 8, "context": "2 for relation discovery and reasoning tasks with Kinship dataset [Hinton, 1986; Sutskever and Hinton, 2008].", "startOffset": 66, "endOffset": 108}, {"referenceID": 8, "context": "Previous approaches are using matrices/tensors to represents the relations making it difficult to explain [Sutskever and Hinton, 2008].", "startOffset": 106, "endOffset": 134}, {"referenceID": 1, "context": "In this experiment, we use auto-encoder [Bengio, 2009] for the right part for the purpose of efficient learning.", "startOffset": 40, "endOffset": 54}, {"referenceID": 8, "context": "For comparison, the matrices based approach such as [Sutskever and Hinton, 2008] achieves 0.", "startOffset": 52, "endOffset": 80}], "year": 2017, "abstractText": "Symbolic has been long considered as a language of human intelligence while neural networks have advantages of robust computation and dealing with noisy data. The integration of neural-symbolic can offer better learning and reasoning while providing a means for interpretability through the representation of symbolic knowledge. Although previous works focus intensively on supervised feedforward neural networks, little has been done for the unsupervised counterparts. In this paper we show how to integrate symbolic knowledge into unsupervised neural networks. We exemplify our approach with knowledge in different forms, including propositional logic for DNA promoter prediction and firstorder logic for understanding family relationship.", "creator": "LaTeX with hyperref package"}}}