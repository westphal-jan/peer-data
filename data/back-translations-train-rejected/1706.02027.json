{"id": "1706.02027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Question Answering and Question Generation as Dual Tasks", "abstract": "We study the problem of joint question answering (QA) and question generation (QG) in this paper. Our intuition is that QA and QG have intrinsic connections and these two tasks could improve each other. On one side, the QA model judges whether the generated question of a QG model is relevant to the answer. On the other side, the QG model provides the probability of generating a question given the answer, which is a useful evidence that in turn facilitates QA. In this paper we regard QA and QG as dual tasks. We propose a novel training framework that trains the models of QA and QG simultaneously, and explicitly leverages their probabilistic correlation to guide the training process. We implement a QG model based on sequence-to-sequence learning, and a QA model based on recurrent neural network. All the parameters involved in these two tasks are jointly learned with back propagation. Experimental results on two datasets show that our approach improves both QA and QG tasks.", "histories": [["v1", "Wed, 7 Jun 2017 02:06:58 GMT  (25kb)", "https://arxiv.org/abs/1706.02027v1", null], ["v2", "Fri, 4 Aug 2017 07:25:53 GMT  (43kb)", "http://arxiv.org/abs/1706.02027v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["duyu tang", "nan duan", "tao qin", "zhao yan", "ming zhou"], "accepted": false, "id": "1706.02027"}, "pdf": {"name": "1706.02027.pdf", "metadata": {"source": "CRF", "title": "Question Answering and Question Generation as Dual Tasks", "authors": ["Duyu Tang", "Nan Duan", "Tao Qin", "Zhao Yan", "Ming Zhou"], "emails": ["dutang@microsoft.com", "nanduan@microsoft.com", "taoqin@microsoft.com", "v-zhaoya@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.02 027v 2 [cs.C L] 4A ug2 01"}, {"heading": "Introduction", "text": "It is obvious that the input and output of these two tasks are (almost) undone, which is referred to in this paper as \"duality,\" a duality that connects QA and QG and could potentially help improve the respective tasks. The input and output of these two tasks could (almost) be undone, which is referred to as \"duality.\" This duality connects QA and QG, and could potentially help improve the respective other tasks. The input and output of these two tasks are (almost) undone, which is referred to as \"duality.\""}, {"heading": "The Proposed Framework", "text": "In this section, we first formulate the task of QA and QG and then present the proposed algorithm for the joint training of QA and QG models. Furthermore, we describe the connections and differences between this work and existing studies."}, {"heading": "Task Definition and Notations", "text": "There are different types of QA tasks in natural language processing. In this work, we take the selection of answer sentences (Yang, Yih and Meek 2015) as a QA task, in which a question q and a list of answer sentences for candidates A = {a1, a2,..., a | A |} are used as input, and output an answer sentence ai from the candidate list, which is most likely to be the answer. This QA task is typically considered a ranking problem. Our QA model is abbreviated as fqa (a, q; successqa), which is parameterized by successqa, and the output is a real scalar. The task of QG takes a sentence a as input and outputs a question q, which could be answered by a. In this work, we consider QG as a generational problem and develop a generative model based on the one."}, {"heading": "Algorithm Description", "text": "We describe the proposed algorithm in this subsection. Overall, the framework comprises three components, namely a QA model, a QG model, and a regularization term that reflects the duality of QA and QG. Accordingly, the training objective of our framework comprises three parts described in Algorithm 1. Since the goal of a QA model is to predict whether a question-answer pair is correct or not, it is necessary to use negative QA pairs whose designations are zero. For each correct question-answer pair, the QG-specific target qqqa is minimized: the following loss function, lqg (q, a) = \u2212 Plogqg (3), where the answer QA is correct."}, {"heading": "Relationships with Existing Studies", "text": "Our work differs from (Yang et al. 2017) in that they consider reading comprehension (QA) to be the main task and the creation of questions to be the auxiliary task to advance the main task RC. In our work, the roles of QA and QG are the same, and our algorithm allows QA and QG to improve each other's performance simultaneously. Our approach differs from generative domain adaptive networks (Yang et al. 2017) in that we do not pre-empt the QA model. Our QA and QG models are learned together by random initialization. Furthermore, our QA task differs from the RC in that the answer in our task is more a sentence than a span of text from a sentence. Our approach is inspired by dual learning (Xia et al. 2016; Xia et al. 2017), which uses the duality between two tasks to improve each other."}, {"heading": "The Question Answering Model", "text": "We describe the details of the question answer (QA) model in this section. (QA) QA model QA-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI (QA-HI-HI-HI-HI) QA-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI-HI (QA-HI-HI-HI-HI-HI) HI HR HI-HI HR (QA-HI-HI-HI-HI-HI-HI) QA-HI-HI-HI-HI-HI HI HI-HI HI HI-HI HI HI HI-HI HI HI-HI HI-HI HI HI-HI HI-HI HI-HI HI HI-HI HI-HI HI HI-HI HI HI-HI HI (QA-HR) HR HR HR HR HI-HI-HI-HI HI-HI HI-HI-HI HI-HI-HI HI HI-HI-HI HI HI-HI-HI HI HI-HI-HI HI-HI HI HI-HI HI-HI HI-HI HI HI HI HI-HI HI-HI HI-HI HI-HI HI-HI HI HI-HI HI HI HI HI HI-HI-HI HI HI HI-HI HI-HI-HI HI HI HI (QA-HR HI-HI-HI-HI HI-HI-HI-HI HI HI HI HI HI-HI-HI HI-HI HI-HI HI HI HI"}, {"heading": "The Question Generation Model", "text": "We describe the issue Generation (QG) based on the GRU unit, which is in line with our Qata model, inspired by the recent success of sequence-to-sequence learning in neural machine translation. < http: / / www.QG model first calculates the representation of the answer set with an encoder and then takes the answer vector to generate a question sequentially with a decoder. We will present the details of the encoder and the decoder, respectively. The goal of the encoder is to represent a fixed-length variable sentence as a continuous vector, which could be implemented with different neural network architectures, such as Convolutionary Neural Net Architectures (Kalchbrenner and Blunsom 2013)."}, {"heading": "Experiment", "text": "In this section we describe the experimental framework and report on empirical results."}, {"heading": "Experimental Setting", "text": "We are conducting experiments with three sets of data, including MARCO (Nguyen et al. 2016), SQUAD (Rajpurkar et al. 2016), and WikiQA (Yang, Yih, and Meek 2015). MARCO and SQUAD data sets were originally designed for the task of reading comprehension (RC), the purpose of which is to answer a question with a span of text from a document. Despite our QA task (Answer Set Selection), we are using these two sets of data for two reasons. The first is that to our knowledge they are the QA data sets that contain the largest manually labeled question pairs. The second is that we were able to derive two QA data sets for selecting answer sets from the original MARCO and SQUAD data sets, assuming that the answer sets containing the correct answer range are correct."}, {"heading": "Implementation Details", "text": "We train the parameters of the QA model and the QG model at the same time. We randomly initialize the parameters in both models with a combination of fan-in and fan-out (Glorot and Bengio 2010). The parameters of the word embedding matrices are shared in the QAmodel and in the QGmodel. To learn questions and answers, we use two different embedding matrices for question and answer words. The vocabulary is the most common 30K words from the questions and answers in the training data. We set the dimension of the word embedding as 300, the hidden length of the encoder and decoder in the QG model as 512, the hidden length of GRU in the QA model as 100, the dimension of the word embedding as 10, the word embedding as 10, the vocabulary size of the word embedding as 10, the hidden length of the word embedding as 30."}, {"heading": "Results and Analysis", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "Discussion", "text": "In this work, \"duality\" means that the QA task and the QG task are equally important. This characteristic makes our work different from that of the Generative Domain Adaptive Nets (Yang et al. 2017) and Generative Adversarial Nets (GAN), which both have a main task and regard another task as a tool. There are various ways to use the \"duality\" of QA and QG to improve both tasks. We categorize them into two groups: the first group is about the training process and the second group is about the inference process. From this perspective, dual learning (Xia et al. 2016) is a solution that incorporates duality into the training process."}, {"heading": "Related Work", "text": "Our work relates to existing studies on the answering of questions (QA) and the generation of questions (QG). There are different types of QA tasks, including the text level QA (Yu et al. 2014), knowledge based on QA (Berant et al. 2013), community-based QA (Qiu et al. 2015) and reading comprehension (Rajpurkar et al. 2016; Nguyen et al. 2016). Our work is one of text-based QA tasks where the answer is a sentence. In recent years, neural networking approaches (Hu et al. 2014; Yin et al. 2016) have shown promising capabilities in modeling the semantic relationship between sentences and achieving strong performances on QA tasks. Question generation has also attracted a lot of attention in recent years. QG is very necessary in real life, as it is always time consuming to create large QA datasets."}, {"heading": "Conclusion", "text": "In this paper, we focus on the joint training of the QA model and the QG model. We use the \"duality\" of QA and QG tasks and introduce a training framework to exploit the likely correlation between the two tasks. In our approach, \"duality\" is used as a regularization term to influence the learning of QA and QG models. We implement simple but effective QA and QG models, both of which are neural network-based approaches. Experimental results show that the proposed training framework improves both QA and QG on three data sets."}, {"heading": "Conference on Empirical Methods in Natural Language", "text": "[Qiu and Huang 2015] Qao, X., and Huang, X. 2015. Convolutional neural tensor network architecture for communitybased question answering. In IJCAI, 1305-1311. [Rajpurkar et al. 2016] Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. Squad: 100,000 + questions for machine understanding of the text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2383-2392. [Serban et al. 2016] Serban, I. V.; Garc\u0131'a-Dura n, A.; Gulcehre, C.; Ahn, S.; Chandar, S.; Courville, A.; and Bengio, Y. 2016. Generating factoid questions with recurrent neural networks."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio"], "venue": "Journal of Machine Learning Research 3:1137\u20131155", "citeRegEx": "Bengio,? \\Q2003\\E", "shortCiteRegEx": "Bengio", "year": 2003}, {"title": "Semantic parsing on freebase from questionanswer pairs", "author": ["Berant"], "venue": "In EMNLP,", "citeRegEx": "Berant,? \\Q2013\\E", "shortCiteRegEx": "Berant", "year": 2013}, {"title": "S", "author": ["Y. Chali", "Hasan"], "venue": "A.", "citeRegEx": "Chali and Hasan 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation", "author": ["Cho"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Learning to ask: Neural question generation for reading comprehension", "author": ["Shao Du", "X. Cardie 2017] Du", "J. Shao", "C. Cardie"], "venue": null, "citeRegEx": "Du et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Du et al\\.", "year": 2017}, {"title": "and Bengio", "author": ["X. Glorot"], "venue": "Y.", "citeRegEx": "Glorot and Bengio 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Y", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Bengio"], "venue": "2014. Generative adversarial nets. In Advances in neural information processing systems, 2672\u2013", "citeRegEx": "Goodfellow et al. 2014", "shortCiteRegEx": null, "year": 2680}, {"title": "V", "author": ["J. Gu", "Z. Lu", "H. Li", "Li"], "venue": "O.", "citeRegEx": "Gu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Hu,? \\Q2014\\E", "shortCiteRegEx": "Hu", "year": 2014}, {"title": "J", "author": ["D. Jurafsky", "Martin"], "venue": "H.", "citeRegEx": "Jurafsky and Martin 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "and Blunsom", "author": ["N. Kalchbrenner"], "venue": "P.", "citeRegEx": "Kalchbrenner and Blunsom 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep questions without deep understanding", "author": ["Basu Labutov", "I. Vanderwende 2015] Labutov", "S. Basu", "L. Vanderwende"], "venue": "In ACL", "citeRegEx": "Labutov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labutov et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Luong,? \\Q2015\\E", "shortCiteRegEx": "Luong", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Pham Luong", "T. Manning 2015] Luong", "H. Pham", "C.D. andManning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "and Sch\u00fctze", "author": ["C.D. Manning"], "venue": "H.", "citeRegEx": "Manning and Sch\u00fctze 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "C", "author": ["Manning"], "venue": "D.; Raghavan, P.; Sch\u00fctze, H.; et al.", "citeRegEx": "Manning et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Meng"], "venue": "arXiv preprint arXiv:1503.01838", "citeRegEx": "Meng,? \\Q2015\\E", "shortCiteRegEx": "Meng", "year": 2015}, {"title": "Neural variational inference for text processing", "author": ["Yu Miao", "Y. Blunsom 2016] Miao", "L. Yu", "P. Blunsom"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Generating natural questions about an image. arXiv preprint arXiv:1603.06059", "author": ["Mostafazadeh"], "venue": null, "citeRegEx": "Mostafazadeh,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh", "year": 2016}, {"title": "2016. Ms marco: A human generated machine reading comprehension", "author": ["Nguyen"], "venue": null, "citeRegEx": "Nguyen,? \\Q2016\\E", "shortCiteRegEx": "Nguyen", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "C", "author": ["J. Pennington", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Pennington. Socher. and Manning 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Huang", "author": ["X. Qiu"], "venue": "X.", "citeRegEx": "Qiu and Huang 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Rajpurkar"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rajpurkar,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar", "year": 2016}, {"title": "I", "author": ["Serban"], "venue": "V.; Garc\u0131\u0301a-Dur\u00e1n, A.; Gulcehre, C.; Ahn, S.; Chandar, S.; Courville, A.; and Bengio, Y.", "citeRegEx": "Serban et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Shen"], "venue": "In Proceedings of the Conference on Information and Knowledge Management,", "citeRegEx": "Shen,? \\Q2014\\E", "shortCiteRegEx": "Shen", "year": 2014}, {"title": "and Zhao", "author": ["L. Song"], "venue": "L.", "citeRegEx": "Song and Zhao 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Dual learning for machine", "author": ["Xia"], "venue": null, "citeRegEx": "Xia,? \\Q2016\\E", "shortCiteRegEx": "Xia", "year": 2016}, {"title": "W", "author": ["Z. Yang", "J. Hu", "R. Salakhutdinov", "Cohen"], "venue": "W.", "citeRegEx": "Yang et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yih Yang", "Y. Meek 2015] Yang", "W.-t. Yih", "C. Meek"], "venue": "In EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs. Transactions of the Association for Computational Linguistics 4:259\u2013272", "author": ["Yin"], "venue": null, "citeRegEx": "Yin,? \\Q2016\\E", "shortCiteRegEx": "Yin", "year": 2016}, {"title": "K", "author": ["Yu, L.", "Hermann"], "venue": "M.; Blunsom, P.; and Pulman, S.", "citeRegEx": "Yu et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "M", "author": ["Zeiler"], "venue": "D.", "citeRegEx": "Zeiler 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural question generation from text: A preliminary study", "author": ["Zhou"], "venue": "arXiv preprint arXiv:1704.01792", "citeRegEx": "Zhou,? \\Q2017\\E", "shortCiteRegEx": "Zhou", "year": 2017}], "referenceMentions": [], "year": 2017, "abstractText": "We study the problem of joint question answering (QA) and question generation (QG) in this paper. Our intuition is that QA and QG have intrinsic connections and these two tasks could improve each other. On one side, the QA model judges whether the generated question of a QG model is relevant to the answer. On the other side, the QG model provides the probability of generating a question given the answer, which is a useful evidence that in turn facilitates QA. In this paper we regard QA and QG as dual tasks. We propose a training framework that trains the models of QA and QG simultaneously, and explicitly leverages their probabilistic correlation to guide the training process of both models. We implement a QG model based on sequence-to-sequence learning, and a QA model based on recurrent neural network. As all the components of the QA and QG models are differentiable, all the parameters involved in these two models could be conventionally learned with back propagation. We conduct experiments on three datasets. Empirical results show that our training framework improves both QA and QG tasks. The improved QA model performs comparably with strong baseline approaches on all three datasets.", "creator": "LaTeX with hyperref package"}}}