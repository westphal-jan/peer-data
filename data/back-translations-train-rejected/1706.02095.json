{"id": "1706.02095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Macquarie University at BioASQ 5b -- Query-based Summarisation Techniques for Selecting the Ideal Answers", "abstract": "Macquarie University's contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a trivial system that selected the first $n$ snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.", "histories": [["v1", "Wed, 7 Jun 2017 09:04:29 GMT  (194kb,D)", "http://arxiv.org/abs/1706.02095v1", "Conference paper"], ["v2", "Fri, 11 Aug 2017 07:11:19 GMT  (194kb,D)", "http://arxiv.org/abs/1706.02095v2", "As published in BioNLP2017. 9 pages, 5 figures, 4 tables"]], "COMMENTS": "Conference paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["diego molla-aliod"], "accepted": false, "id": "1706.02095"}, "pdf": {"name": "1706.02095.pdf", "metadata": {"source": "CRF", "title": "Macquarie University at BioASQ 5b \u2013 Query-based Summarisation Techniques for Selecting the Ideal Answers", "authors": ["Diego Moll\u00e1"], "emails": ["diego.molla-aliod@mq.edu.au"], "sections": [{"heading": "1 Introduction", "text": "eiD rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die"}, {"heading": "2 Simple Runs", "text": "In the first rwdeeeeirsrVnreeueeeerrrrrf\u00fc ide rf\u00fc ide eeirrmtlrteeaeVnlrsrtee\u00fceegnln rf\u00fc ide rf\u00fc ide eeirrf\u00fc eVnlrrrrteeeirln rf\u00fc ide eeirmtlrteeaeVnlrrrrteeeeeeVnlrteeeeeerln-eaeVnlrrrrrrteeeeeVnlrlrlllllllln-eaeeeeeeeeaeSrrrrnlllllllluuuiueeerrrrrlllllc-eFreeeeeeeteerrrrc-rrrrnlllllllllc-eeeeeeeeFrllllllllllllllllllllllrnllln-eeeeeeeeeeerrrrrrrrrrrnlnln-rrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrrlrlrlrlrrlrrlrrlrrlrlrlrlrrrrlrlrrrrrrrrrrrrrrrlrrrrrrlrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3 Regression Approaches", "text": "This year it is so far that it will be able to retaliate, \"he said.\" We have never lost as much time as this year, \"he said."}, {"heading": "4 Deep Learning Approaches", "text": "In fact, most of them are able to outdo themselves if they do not orient themselves in a different direction. Most of them are able to orient themselves in a different direction than to go in the other direction. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them are able to outperform themselves. Most of them have outperformed themselves. Most of them have outperformed themselves. Most of them are able to outperform themselves."}, {"heading": "5 Submission Results", "text": "At the time of writing, the human evaluations had not yet been published, and only the ROUGE results of all 5 batches were available. Table 4 shows the F1 value of ROUGE-SU4.Figure 5 shows the same information as a chart showing our runs and all runs of other participating systems with higher ROUGE values. Figure 4 shows that in the first three runs only one run of another participant was among our results (shown as a dashed line in the figure), runs 4 and 5 show consistent results of our runs and improved results of other runs. The results are consistent with our experiments, although the absolute values are higher than those in our experiments. This is probably because we have used the entire BioASQ 5b training set for our cross-validation results, and these data are the aggregation of the training sets of the BioASQ tasks of previous years. It is possible that the data of recent years are of higher quality, and it may be useful to develop learning approaches to these."}, {"heading": "6 Conclusions", "text": "Our experiments show that a trivial base system that returns the first n snippets appears to be hard to beat, so it follows that the sequence of snippets matters. Although the judges were not given specific instructions on the sequence of snippets, it would be interesting to examine the criteria by which they present the snippets. Our regression runs were not significantly better than simpler approaches, and the deep learning runs reported the lowest results. However, note that the input functions used in the deep learning runs did not contain information about the snippets. Table 3 shows that the results with deep learning are comparable to the results with tf.idf and SVD, so it is possible that an extension of the system that contains information from the snippets has information from the snippets, or that this information is better applied to the other systems described here."}, {"heading": "Acknowledgments", "text": "Some of the experiments in this research were conducted in cloud computers as part of a Microsoft Azure for Research Award."}], "references": [{"title": "Automatic condensation of electronic publications by sentence selection. Information Processing and Management 31(5):675\u2013685", "author": ["Ronald Brandow", "Karl Mitze", "Lisa F. Rau"], "venue": null, "citeRegEx": "Brandow et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Brandow et al\\.", "year": 1995}, {"title": "Neocognitron: A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics 36(4):193\u2013202", "author": ["Kunihiko Fukushima"], "venue": null, "citeRegEx": "Fukushima.,? \\Q1980\\E", "shortCiteRegEx": "Fukushima.", "year": 1980}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Extractive summarization using continuous vector space models", "author": ["Mikael Kageback", "Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi"], "venue": "In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)", "citeRegEx": "Kageback et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kageback et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Biomedical questionfocused multi-document summarization: ILSP and AUEB at BioASQ3", "author": ["Prodromos Malakasiotis", "Emmanouil Archontakis", "Ion Androutsopoulos"], "venue": "In CLEF 2015 Working Notes", "citeRegEx": "Malakasiotis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malakasiotis et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Workshop at ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Results of the fifth edition of the BioASQ Challenge", "author": ["Anastasios Nentidis", "Konstantinos Bougiatiotis", "Anastasia Krithara", "Georgios Paliouras", "Ioannis Kakadiaris"], "venue": "In Proceedings BioNLP 2017", "citeRegEx": "Nentidis et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Nentidis et al\\.", "year": 2017}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "Technical report", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "An overview of the BIOASQ large-scale biomedical", "author": ["las Baskiotis", "Patrick Gallinari", "Thierry Arti\u00e9res", "Axel-Cyrille Ngonga Ngomo", "Norman Heino", "Eric Gaussier", "Liliana Barrio-Alvers", "Michael Schroeder", "Ion Androutsopoulos", "Georgios Paliouras"], "venue": null, "citeRegEx": "Baskiotis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baskiotis et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "The fifth run of the BioASQ challenge (Nentidis et al., 2017), in particular, had three tasks:", "startOffset": 38, "endOffset": 61}, {"referenceID": 0, "context": "In fact, for the task of summarisation of other domains such as news, it has been observed that a baseline that returns the first sentences often outperformed other methods (Brandow et al., 1995).", "startOffset": 173, "endOffset": 195}, {"referenceID": 5, "context": "The regression setup and features are based on the work by Malakasiotis et al. (2015), who reported the best results in BioASQ 3b (2015).", "startOffset": 59, "endOffset": 86}, {"referenceID": 5, "context": "The regression setup and features are based on the work by Malakasiotis et al. (2015), who reported the best results in BioASQ 3b (2015). The target scores used to train the SVR system were the F1 ROUGE-SU4 score of each individual candidate sentence.", "startOffset": 59, "endOffset": 137}, {"referenceID": 5, "context": "Note that this feature was not used by Malakasiotis et al. (2015).", "startOffset": 39, "endOffset": 66}, {"referenceID": 5, "context": "As in the work by Malakasiotis et al. (2015), we used word2vec to compute the word vectors.", "startOffset": 18, "endOffset": 45}, {"referenceID": 5, "context": "\u2022 Weighted pairwise cosine similarities, also based in the work by Malakasiotis et al. (2015). In particular, now each word vector was multiplied by the tf.", "startOffset": 67, "endOffset": 94}, {"referenceID": 6, "context": "This embedding space has the property that some semantic relations between words are also mapped in the embedded space (Mikolov et al., 2013).", "startOffset": 119, "endOffset": 141}, {"referenceID": 5, "context": "This embedding space has the property that some semantic relations between words are also mapped in the embedded space (Mikolov et al., 2013). It is therefore natural to apply vector arithmetics such as the sum or the mean of word embeddings of a sentence in order to obtain the sentence embedding. In fact, this approach has been used in a range of applications, on its own, or as a baseline against which to compare other more sophisticated approaches to obtain word embeddings, e.g. work by Yu et al. (2014) and Kageback et al.", "startOffset": 120, "endOffset": 511}, {"referenceID": 3, "context": "(2014) and Kageback et al. (2014). To accommodate for different sentence lengths, in our experiments we use the mean of word embeddings instead of the sum.", "startOffset": 11, "endOffset": 34}, {"referenceID": 1, "context": "CNN: Convolutional Neural Nets (CNN) were originally developed for image processing, for tasks where the important information may appear on arbitrary fragments of the image (Fukushima, 1980).", "startOffset": 174, "endOffset": 191}, {"referenceID": 4, "context": "as described by Kim (2014). In particular, the embeddings of the words in a sentence (or question) are arranged in a matrix where each row represents a word embedding.", "startOffset": 16, "endOffset": 27}, {"referenceID": 2, "context": "LSTM has been applied successfully to applications that process sequences of samples (Hochreiter et al., 1997).", "startOffset": 85, "endOffset": 110}, {"referenceID": 2, "context": "LSTM has been applied successfully to applications that process sequences of samples (Hochreiter et al., 1997). Our experiments use TensorFlow\u2019s implementation of LSTM cells as described by Pham et al. (2013).", "startOffset": 86, "endOffset": 209}, {"referenceID": 10, "context": "An alternative similarity metric that we have also tried is as proposed by Yu et al. (2014). Their similarity metric allows for interactions between different components of the sentence vectors, by applying a d\u00d7 d weight matrix W , where d is the sentence embedding size, and adding a bias term:", "startOffset": 75, "endOffset": 92}], "year": 2017, "abstractText": "Macquarie University\u2019s contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a trivial system that selected the first n snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.", "creator": "LaTeX with hyperref package"}}}