{"id": "1706.02124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks", "abstract": "Ladder networks are a notable new concept in the field of semi-supervised learning by showing state-of-the-art results in image recognition tasks while being compatible with many existing neural architectures. We present the recurrent ladder network, a novel modification of the ladder network, for semi-supervised learning of recurrent neural networks which we evaluate with a phoneme recognition task on the TIMIT corpus. Our results show that the model is able to consistently outperform the baseline and achieve fully-supervised baseline performance with only 75% of all labels which demonstrates that the model is capable of using unsupervised data as an effective regulariser.", "histories": [["v1", "Wed, 7 Jun 2017 10:50:47 GMT  (403kb,D)", "http://arxiv.org/abs/1706.02124v1", null], ["v2", "Mon, 18 Sep 2017 18:49:26 GMT  (403kb,D)", "http://arxiv.org/abs/1706.02124v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marian tietz", "tayfun alpay", "johannes twiefel", "stefan wermter"], "accepted": false, "id": "1706.02124"}, "pdf": {"name": "1706.02124.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks", "authors": ["Marian Tietz", "Tayfun Alpay", "Johannes Twiefel", "Stefan Wermter"], "emails": ["tietz@informatik.uni-hamburg.de", "alpay@informatik.uni-hamburg.de", "twiefel@informatik.uni-hamburg.de", "wermter@informatik.uni-hamburg.de"], "sections": [{"heading": null, "text": "Keywords: semi-monitored learning, recurrent neural networks, conductor networks, phoneme detection"}, {"heading": "1 Introduction", "text": "While tasks such as image or text classification have greatly benefited from this availability, there are still a number of areas, such as speech recognition, where the majority of the research community does not have free access to large amounts of marked data. A promising approach to solving this problem is semi-supervised learning, where models trained with marked data can be further improved by training with unmarked data. Newer methods, such as graph-based training [10], sparse autoencoders ([4]; SSSAE), and in particular the PCB network (LN) [11], a stacked denoising autoencoder (DAE) with short-term connections, show promising results for semi-supervised training of neural networks. The LN has shown that it delivers state-of-the-art results while being compatible with many existing future-oriented networks."}, {"heading": "2 The Ladder Network Architecture", "text": "rf\u00fc ide rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the r"}, {"heading": "3 Recurrent Ladder Networks", "text": "In this section we will explain our modelling options for the RLN. To extend the original LN to support repetition in the encoder, both the noise injection scheme and the decoder need to be adapted, as recurring layers use additional context layers. Overall, we propose two methods of noise injection and two decoder variants (see Fig. 2). Our supervised base model will be the RLN encoder, as it encodes the task closely with the complete RLN, but has no possibility of using uncontrolled data. The resulting six model combinations are no decoders with feed-forward noise (ND-FFN), no decoders with recurrent noise (ND-RN), recurrent decoders with feed-forward noise (RD-FFN) and recurrent noise (RD-FFRN)."}, {"heading": "3.1 Noise Injection", "text": "In the case of feed, the noise is applied directly to the pre-activations, so that the output of the layer and the concatenation are affected, i.e. z = W x + n with n \u0445 N (0, \u03c32). However, this would put the noise into the context memory of the recurring layers even after receiving the noisy output from the previous layer and effectively amplify the noise even further. Therefore, we apply the noise only to the pre-activation and the concatenation without directly disturbing the context memory. A hidden layer lifts and its noisy counterpart h are therefore updated as follows: ht = f (zt) = f (W x \u0445t + Uht \u2212 1), (2) h-t = f (z \u0442t) = f (zt + n), (3) where f (\u00b7) is the activation function, xt the input function, W the input weight matrix, and U the hidden to hidden weight matrix are each updated."}, {"heading": "3.2 Recurrent Decoder", "text": "The decoder path in an encoder models the inverse information flow of the encoder path. We propose two modeling options for the decoder path in an RLN. The first (Fig. 2c) is a recursive layer with g (\u00b7, \u00b7) as the activation function: u (l) t = V z (l + 1) t + Oz (l) t \u2212 1, (4) z (l) t = g (z (l) t, u (l) t))), (5) where V are the input weights, O the hidden to hidden weights, u (l) t the pre-activation of the recursive decoder and z (l) t the noisy pre-activation of the l encoder layer in the time step t from the linkage. The second modeling option is simply to use a feed network (Fig. 2d) in the decoder [11]. The batch layer normalization for the N and the normalization layer is strongly used in the normalization layer."}, {"heading": "4 Experiments", "text": "The audio samples of the corpus are reduced in dimensionality by using libROSA1 to calculate 13 Mel Frequency Cepstral Components (MFCC). [3] The 39-dimensional feature vectors are normalized to have a variance of mean and unit of zero. We grouped slightly confused phonemes of the English phoneme alphabet as described by Halberstadt [8], resulting in 39 phoneme classes that are predictable. We use the Connectionist Temporal Classification (CTC) [6] for the supervised cost of Csup to solve the problem of label alignment."}, {"heading": "4.1 Training Procedure", "text": "The models are four-layer networks consisting of a GRU layer with 192 units with tanh (\u00b7) activation and a feed-forward output layer with Softmax activation and the inverse layers in the decoder. The noisy Softmax output serves to classify phonemes during training for additional regularization. Since the encoder performance is likely to correlate with the RLN performance, hyperparameters, including layer size and learning rate, were determined empirically by a grid search using the encoder described in Section 3, i.e. an RLN with \u03bbi = 0, which also serves as a baseline. DAE cost weights (\u03bb0, \u03bb1, \u03bb2) = (1000, 10, 0.1) and the MLP combine g (\u00b7) were both taken from Rasmus et 11."}, {"heading": "5 Results & Discussion", "text": "The overall results after hyperparameter optimization for each monitored data division, as well as the results of other approaches, are therefore presented in Table 1. As can be seen in Table 1, the RLN consistently exceeds the basic configuration, even in fully monitored training, and is able to achieve the same performance as the baseline with 25% less marked data, showing that the RLN complements the encoder well and demonstrates the compatibility of the LN with existing models. On average, the RD models perform better than the FFD models for most others, indicating that the recurring decoder is better at filtering noise. This also explains why the RD models perform better than the higher values compared to the FFD models."}, {"heading": "6 Conclusion", "text": "We have shown that the recurring conductor network is able to work similarly well to similarly parameterized BLSTM models, while using only 50% of the marked data, demonstrating the ability of the RLN to effectively regulate itself by using unattended training data. Current state-of-the-art methods performed better overall, but not surprisingly, as these models use up to 160 times more parameters. We argue that this gap could potentially be closed by scaling our models, as demonstrated in BLSTM models by Graves et al. [7] The proposed recurring decoder proved to be better at denocizing than the upstream decoder. Furthermore, we have found that recurring noise injections do not work as expected, and we believe that it needs the help of normalization (e.g. batch normalization) to work efficiently."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Ladder networks are a notable new concept in the field of<lb>semi-supervised learning by showing state-of-the-art results in image<lb>recognition tasks while being compatible with many existing neural archi-<lb>tectures. We present the recurrent ladder network, a novel modification of<lb>the ladder network, for semi-supervised learning of recurrent neural net-<lb>works which we evaluate with a phoneme recognition task on the TIMIT<lb>corpus. Our results show that the model is able to consistently outper-<lb>form the baseline and achieve fully-supervised baseline performance with<lb>only 75% of all labels which demonstrates that the model is capable of<lb>using unsupervised data as an effective regulariser.", "creator": "LaTeX with hyperref package"}}}