{"id": "1706.02241", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Insights into Analogy Completion from the Biomedical Domain", "abstract": "Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods.", "histories": [["v1", "Wed, 7 Jun 2017 16:24:32 GMT  (130kb,D)", "http://arxiv.org/abs/1706.02241v1", "Accepted to BioNLP 2017. (10 pages)"]], "COMMENTS": "Accepted to BioNLP 2017. (10 pages)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["denis newman-griffis", "albert m lai", "eric fosler-lussier"], "accepted": false, "id": "1706.02241"}, "pdf": {"name": "1706.02241.pdf", "metadata": {"source": "CRF", "title": "Insights into Analogy Completion from the Biomedical Domain", "authors": ["Denis Newman-Griffis", "Albert M. Lai", "Eric Fosler-Lussier"], "emails": ["newman-griffis.1@osu.edu,", "amlai@wustl.edu,", "fosler@cse.ohio-state.edu"], "sections": [{"heading": "1 Introduction", "text": "Analogical thinking has long been a staple of mathematical semantics research because it allows judging how well implicit semantic relationships between pairs of concepts are represented in a semantic model. In particular, the recent boom in text vector spatial models (VSMs) research (Turney and Pantel, 2010) has used analogy as a standalone method of evaluating VSMs without using a complete NLP system, largely due to observations of \"linguistic regularities\" as context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014).In the Analogy Complete Problem, a system with an example term pair and a quantity is presented, e.g. London: England: and the task is right to fill the lesson correctly."}, {"heading": "2 Related work", "text": "Analogical thinking has been studied both for itself and as a component of downstream tasks using a range of systems. Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships, but supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004) and later for synonymy, antagonymy and some world knowledge (Turney, 2008; Herdag, Delen and Baroni, 2009). Analogical thinking has also been used to support downstream tasks, including word disambiguation (Federici et al, 1997) and morphological analysis (Lepage and Goh, 2009; Lavalle, e and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic assessment of the properties of a VSM."}, {"heading": "3 Analogy completion task", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Standard methodology", "text": "Given an analogy a: b: c: d, the evaluation task is to guess d from the vocabulary, with a, b, c given as evidence. Recent methods for this include using the vector difference between the embedded representations of the related pairs to evaluate all terms in the vocabulary according to how well they complete the analogy, and selecting the best fit. The vector difference is most commonly used in one of three ways where cos has cosmic similarity: argmaxd \u0445 V (cos (d, b \u2212 a + c))) (1) argmaxd \u0441V (cos (d \u2212 c, b \u2212 a)) (2) argmaxd \u0441V cos (d, c) cos (d, a) + (3) cos (according to the terminology of Levy and Goldberg (2014), we refer to equation 1 as 3COSADD, equation 2 as PAIRWISTAN, and equation 3 as COS and COD (COS and COD) have been selected."}, {"heading": "3.2 Assumptions", "text": "The first refers to the single-target assumption: namely, that there is a single correct answer for each given analogy. Since target d is chosen via argmax if we consider the following two analogies: Flu:: Fever:? Cough Flu::: Fever:? We absolutely must get at least one answer wrong. Gladkova et al. (2016) Convert these analogies into a single case: Flu::: Fever:? Cough, carelessness::? Carelessness is a correct assumption. However, this still misses our desire to get both correct answers when possible. Relationships with multiple correct goals are in all of Google, BATS, and Sem Paradise."}, {"heading": "4 BMASS", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 Evaluation", "text": "We evaluate how well biomedical word embedding can work on our dataset and examine modifications of the standard evaluation method to loosen the assumptions described in Section 3.2. We use the embedding exercised by Chiu et al. (2016) on the PubMed citation database with a sentence with a window size of 2 (PM-2) and another sentence with a window size of 30 (PM-30). All other word2vec hyperparameters have been matched by Chiu et al to a combination of similarity and kinship and named entity recognition tasks. Additionally, we use the hyperparameters they identified (minimum frequency = 5, vector dimension = 200, negative samples = 10, sample = 1e-4.5Examples of each relationship, along with their assignments to UMLS REL / RELA values, are available online."}, {"heading": "5.1 Modifications to the standard method", "text": "For a given analogy a: b:: c:? d we refer to < a, b > as a sample object and < c, d > as a query pair;? d means the target response. The single response sets analogies in the format a: b: c: d, with a single sample object b and a single correct object d, taking the first object listed for each term pair, forcing the adoption of the single response. The multiple response takes the first object listed for the sample object, but takes all valid answers aside, i.e. a: b: c: [d1, d2,.]; this is similar to the approach of Gladkova et al al al al al al al al. (2016) There are approximately 16k analogies in our dataset with multiple valid answers. All info considers all the copy to be both the object and the emplaque."}, {"heading": "5.2 MWEs and candidate answers", "text": "As mentioned in Section 4, the terms in our analogy dataset may be multi-word expressions (MWEs). We follow the common basic approach of representing an MWE as the average of its constituent words (Mikolov et al., 2013b; Chen et al., 2013; Wieting et al., 2016). For phrasal terms containing one or more words derived from our embedding vocabulary, we look only at the vocabulary: So, if \"parathyroid\" is not in the vocabulary, then the embedding of parathroid hypertensiveness factors is hypertensive + Factor2For each individual analogy a: b: c:? d, the vocabulary of candidate sets to complete the analogy, we are presented as illogical by calculating averaged word embedding for each UMLS term that appears in PubMed abstracts."}, {"heading": "5.3 Metric comparison", "text": "Figure 1 shows AccR, MAP and MRR results for each relationship in BMASS using PM-2 embedded in the multi-answer setting. Overall, the performance between the relationships varies widely, with all three measures remaining below 0.1 in most cases, reflecting previous results from other analog datasets (Levy and Goldberg, 2014; Gladkova et al., 2016; Drozd et al., 2016). MAP further concretizes these differences by reporting all correct answers for a given analogy, with a low AccR reflecting many wrong answers for a given analogy, but a higher MAP value indicating that the correct answers are relatively close to the guess. MRR, on the other hand, reports more optimistically how close we are to an answer (B2) or associated with (C6), where a low AccR reflects many wrong answers, but a higher MAP indicates that the correct answers are relatively close to the guess."}, {"heading": "5.4 Analogy settings", "text": "To compare across the settings Single-Answer, MultiAnswer, and All-Info, we first look at AccR for each BMASS relationship shown for PM2 embedding in Figure 2 (the observed patterns are similar to the other embedding). Unsurprisingly, multiple responses in MultiAnswer and All-Info increase AccR slightly in most cases. Surprisingly, however, the embedding of additional sample objects in the All-Info setting resulted in very different results. In some cases, such as the same type (H2), the associated substance (L5), and the pathogen (C4), the additional examples resulted in a marked improvement in accuracy. In other cases, the accuracy even decreased: Form (L1) and the form with free acid or base form (L6) are the most striking examples, with absolute decreases of 4% and 8% from the MultiAnswer for PM-2 (the decreases are similar to other embedding) blocking the experiments in some cases (V)."}, {"heading": "5.5 Embedding methods", "text": "On average of all relationships, the five embedding settings we tested were approximately the same, with our trained embedding performing slightly better than the pre-trained embedding of Chiu et al. (2016); summary AccR, MAP and MRR performances are in Table 4. Interestingly, at the level of individual relationships, Figure 3 shows the MAP performance in the Multi-Answer setting. The four word2vec samples tend to behave similarly, with some inconsistent deviations. Interestingly, GloVe outperforms the other embedding in several long-distance relationships, including regulated embedding (B1) and trade names (L4). GloVe performs much better in the relationships, which is reflected in the higher standard deviations in Table 4. While GloVe consistently outperforms the word2vec embedding in free acid or base form (L6) than the salt 2c-acid reform (L6) and (L6)."}, {"heading": "5.6 Error analysis", "text": "When checking individual a: b:: c:? d predictions, several interesting patterns emerge. A number of errors arise directly from our Word averaging approach for MWEs: words occurring in b or c, as in Gosorelin: ici 118630:: Letrozole: * ici 164384. Prefix substitutions also occurred, as in Mianserine hydrochloride: Mianserin:: Scopolamine hydrobromide: * Scopolamine methyl bromide. Frequently, the b terms c predominate, which results in many of the top assumptions being variations on b. In an analogy, sodium acetyl salicyclate: Aspirin:: intravenous immunoglobulins:? Immunoglobulin g:? the top assumptions were: * aspirin prophylaxis, * aspirin prophylaxis, * aspirin prophylaxis, in proximity to aspirin * and in aspirin aspirin *."}, {"heading": "6 Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7 Conclusions", "text": "We identified three key assumptions in the standard methodology for analog-based evaluation of multiple word pairs: Single Answer (that there is a single correct answer for an analogy), SameRelationship (that the specimen and the query pairs are related in the same way), and Informativity (that the specimen pair is informative in relation to the query pair), and we showed that these assumptions are not included in the most recent benchmark data sets or in biomedical data. However, in order to loosen these assumptions, we modified the analog evaluation to allow for multiple correct answers and multiple specimen pairs, and reported that Mean Average Precision and Mean Reciprocal Recall were automatically generated via the ranged vocabulary, in addition to the accuracy of the high-level Choice.We presented the BioMedical Analogic Similarity Set (BMASS), a novel analog completion database for the large number of existing domain ALD relationships."}, {"heading": "Acknowledgments", "text": "We would like to thank the Ohio CLLT group and the anonymous reviewers for their helpful comments. Denis is a doctoral student at the National Institutes of Health Clinical Center in Bethesda, MD."}], "references": [{"title": "Learning New Facts", "author": ["Andrew Y. Ng"], "venue": null, "citeRegEx": "Ng.,? \\Q2013\\E", "shortCiteRegEx": "Ng.", "year": 2013}, {"title": "How to Train Good Word", "author": ["Sampo Pyysalo"], "venue": null, "citeRegEx": "Pyysalo.,? \\Q2016\\E", "shortCiteRegEx": "Pyysalo.", "year": 2016}, {"title": "BagPack: A General Framework to Represent Semantic Relations", "author": ["Ama\u00e7 Herda\u011fdelen", "Marco Baroni."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics. Association for Computational Linguistics, Athens,", "citeRegEx": "Herda\u011fdelen and Baroni.,? 2009", "shortCiteRegEx": "Herda\u011fdelen and Baroni.", "year": 2009}, {"title": "SemEval-2012 Task 2: Measuring Degrees of Relational Similarity", "author": ["David Jurgens", "Saif Mohammad", "Peter Turney", "Keith Holyoak."], "venue": "*SEM 2012, Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Jurgens et al\\.,? 2012", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Multilingual Reliability and \u201dSemantic\u201d Structure of Continuous Word Spaces", "author": ["Maximilian K\u00f6per", "Christian Scheible", "Sabine im Walde."], "venue": "Proceedings of the 11th International Conference on Computational Semantics. Association for Com-", "citeRegEx": "K\u00f6per et al\\.,? 2015", "shortCiteRegEx": "K\u00f6per et al\\.", "year": 2015}, {"title": "Unsupervised Morphological Analysis by Formal Analogy", "author": ["Jean-Fran\u00e7ois Lavall\u00e9e", "Philippe Langlais"], "venue": null, "citeRegEx": "Lavall\u00e9e and Langlais.,? \\Q2010\\E", "shortCiteRegEx": "Lavall\u00e9e and Langlais.", "year": 2010}, {"title": "Towards automatic acquisition of linguistic features", "author": ["Yves Lepage", "Chooi Ling Goh."], "venue": "Proceedings of the 17th Nordic Conference of Computational Linguistics (NODALIDA 2009). Northern European Association for Language Technology", "citeRegEx": "Lepage and Goh.,? 2009", "shortCiteRegEx": "Lepage and Goh.", "year": 2009}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, Ann Arbor,", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015a", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "2015b. Do Supervised Distributional Methods Really Learn Lexical Inference Relations", "author": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Issues in evaluating semantic spaces using word analogies", "author": ["Tal Linzen."], "venue": "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. pages 13\u201318.", "citeRegEx": "Linzen.,? 2016", "shortCiteRegEx": "Linzen.", "year": 2016}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 pages 1\u201312.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and Their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc., NIPS \u201913,", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Cognition and Thought: An Information Processing Approach", "author": ["Walter R Reitman."], "venue": "John Wiley and Sons, New York, NY.", "citeRegEx": "Reitman.,? 1965", "shortCiteRegEx": "Reitman.", "year": 1965}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Schnabel et al\\.,? 2015", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Unsupervised Morphology Induction Using Word Embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations", "author": ["Peter D Turney."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1. Association for Computational Linguistics, Stroudsburg,", "citeRegEx": "Turney.,? 2008", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "WordNet Sits the S.A.T. A Knowledge-based Approach to Lexical Analogy", "author": ["Tony Veale"], "venue": "In Proceedings of the 16th European Conference on Artificial Intelligence", "citeRegEx": "Veale.,? \\Q2004\\E", "shortCiteRegEx": "Veale.", "year": 2004}, {"title": "Towards Universal Paraphrastic Sentence Embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the 4th International Conference on Learning Representations.", "citeRegEx": "Wieting et al\\.,? 2016", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "In particular, the recent boom of research on learning vector space models (VSMs) for text (Turney and Pantel, 2010) has leveraged analogy completion as a standalone method for evaluating VSMs without using a full NLP system.", "startOffset": 91, "endOffset": 116}, {"referenceID": 13, "context": "This is due largely to the observations of \u201clinguistic regularities\u201d as linear offsets in context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014; Pennington et al., 2014).", "startOffset": 120, "endOffset": 193}, {"referenceID": 7, "context": "This is due largely to the observations of \u201clinguistic regularities\u201d as linear offsets in context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014; Pennington et al., 2014).", "startOffset": 120, "endOffset": 193}, {"referenceID": 14, "context": "This is due largely to the observations of \u201clinguistic regularities\u201d as linear offsets in context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014; Pennington et al., 2014).", "startOffset": 120, "endOffset": 193}, {"referenceID": 10, "context": "cluding that the use of cosine similarity often reduces to just reflecting nearest neighbor structure (Linzen, 2016), and that there is significant variance in performance between different kinds of relations (K\u00f6per et al.", "startOffset": 102, "endOffset": 116}, {"referenceID": 15, "context": "Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships.", "startOffset": 55, "endOffset": 70}, {"referenceID": 20, "context": "Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy,", "startOffset": 73, "endOffset": 86}, {"referenceID": 18, "context": "antonymy, and some world knowledge (Turney, 2008; Herda\u011fdelen and Baroni, 2009).", "startOffset": 35, "endOffset": 79}, {"referenceID": 2, "context": "antonymy, and some world knowledge (Turney, 2008; Herda\u011fdelen and Baroni, 2009).", "startOffset": 35, "endOffset": 79}, {"referenceID": 10, "context": "The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 13, "context": "Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al.", "startOffset": 41, "endOffset": 64}, {"referenceID": 3, "context": "The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K\u00f6per et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": "The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K\u00f6per et al. (2015), the semantic relation-", "startOffset": 49, "endOffset": 181}, {"referenceID": 8, "context": "Additionally, Levy et al. (2015b) demonstrate that even for some lexical relations where embeddings appear to perform well, they are actually learning", "startOffset": 14, "endOffset": 34}, {"referenceID": 7, "context": "Following the terminology of Levy and Goldberg (2014), we refer to Equation 1 as 3COSADD,", "startOffset": 29, "endOffset": 54}, {"referenceID": 4, "context": "In order to generate analogy data for this task, recent datasets have followed a similar process (Mikolov et al., 2013a,c; K\u00f6per et al., 2015; Gladkova et al., 2016).", "startOffset": 97, "endOffset": 165}, {"referenceID": 4, "context": "2 While this is similar to the Single-Target assumption, high-level nature of several of the Sem-Para relations (hypernymy, antonymy, and synonymy) suggests that some of the difficulty observed by K\u00f6per et al. (2015) is due to violations of Informativity.", "startOffset": 197, "endOffset": 217}, {"referenceID": 11, "context": "We train word2vec (Mikolov et al., 2013a) samples with the continuous bag-of-words (CBOW) and skip-gram (SGNS) models, trained for 10 iterations, and GloVe (Pennington et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 14, "context": ", 2013a) samples with the continuous bag-of-words (CBOW) and skip-gram (SGNS) models, trained for 10 iterations, and GloVe (Pennington et al., 2014) samples, trained for 50 iterations.", "startOffset": 123, "endOffset": 148}, {"referenceID": 10, "context": ") AccR, as with standard accuracy, necessitates ignoring a, b, or c if they are the top results (Linzen, 2016).", "startOffset": 96, "endOffset": 110}, {"referenceID": 12, "context": "resenting an MWE as the average of its component words (Mikolov et al., 2013b; Chen et al., 2013; Wieting et al., 2016).", "startOffset": 55, "endOffset": 119}, {"referenceID": 21, "context": "resenting an MWE as the average of its component words (Mikolov et al., 2013b; Chen et al., 2013; Wieting et al., 2016).", "startOffset": 55, "endOffset": 119}, {"referenceID": 7, "context": "1 in the majority of cases; this mirrors previous findings on other analogy datasets (Levy and Goldberg, 2014; Gladkova et al., 2016; Drozd et al., 2016).", "startOffset": 85, "endOffset": 153}, {"referenceID": 10, "context": "hood over-reporting observed by Linzen (2016), we saw guesses very similar to c, regardless of a or b, as with acute inflammations:acutely inflamed::endoderm:*embryonic endoderm; other near guesses included *endoderm cell and epi-", "startOffset": 32, "endOffset": 46}, {"referenceID": 8, "context": "Levy et al. (2015a) and Chiu et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Levy et al. (2015a) and Chiu et al. (2016), among others, show significant impact of embedding hyperparameters on downstream performance.", "startOffset": 0, "endOffset": 43}], "year": 2017, "abstractText": "Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods.", "creator": "LaTeX with hyperref package"}}}