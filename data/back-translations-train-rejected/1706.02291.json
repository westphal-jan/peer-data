{"id": "1706.02291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Sound Event Detection Using Spatial Features and Convolutional Recurrent Neural Network", "abstract": "This paper proposes to use low-level spatial features extracted from multichannel audio for sound event detection. We extend the convolutional recurrent neural network to handle more than one type of these multichannel features by learning from each of them separately in the initial stages. We show that instead of concatenating the features of each channel into a single feature vector the network learns sound events in multichannel audio better when they are presented as separate layers of a volume. Using the proposed spatial features over monaural features on the same network gives an absolute F-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and 2.7% on the TUT-SED 2009 dataset that is fifteen times larger.", "histories": [["v1", "Wed, 7 Jun 2017 06:01:48 GMT  (90kb,D)", "http://arxiv.org/abs/1706.02291v1", "Accepted for IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017)"]], "COMMENTS": "Accepted for IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017)", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["sharath adavanne", "pasi pertil\\\"a", "tuomas virtanen"], "accepted": false, "id": "1706.02291"}, "pdf": {"name": "1706.02291.pdf", "metadata": {"source": "CRF", "title": "SOUND EVENT DETECTION USING SPATIAL FEATURES AND CONVOLUTIONAL RECURRENT NEURAL NETWORK", "authors": ["Sharath Adavanne", "Pasi Pertil\u00e4", "Tuomas Virtanen"], "emails": [], "sections": [{"heading": null, "text": "This year it is so far that it will only take one year to reach an agreement."}, {"heading": "2.1. Binaural mel-band energies", "text": "Sound sources with different spatial locations have different intensities in the binaural channels. In addition, most overlapping sound events have different frequency distributions in Xiv: 170 6.02 291v 1 [cs.S D] 7J un2 017 the spectrum. The combination of these intensity differences in different frequency bands can be exploited to differentiate overlapping sound events. This idea is motivated by the human-used interaural intensity difference (IID) [9].Log mel band energies (henceforth called mel) extracted from both binaural channels using 40 mel bands in 40 ms hammering windows as characteristics. A neural network capable of performing linear operations that include the difference can learn to draw the IID information from these channel-wise energies."}, {"heading": "2.2. Time difference of arrival vs cross-correlation", "text": "Based on how the sound sources are spatially related to the binaural microphones, they may have different TDOA values. Furthermore, sound events that overlap do not always have the same frequency distribution in the spectrum, and the combination of this TDOA difference in different frequency bands can be exploited by a network to differentiate overlapping sound events. We implemented them by splitting the spectral frame into five mel bands and calculating the TDOA values in each of the bands. TDOA difference is estimated using the GCC-PHAT [11]. GCC-PHAT for each mel band b is extracted separately: Rb (\u2206 12, t) = N \u2212 1 Hb (k, t) \u00b7 X-HCC 2 (k, t) | X1 (k, t), PHCC (k, t), X2 (k, t), PHT2, XTk-4 (XB)."}, {"heading": "2.3. Dominant frequencies vs auto-correlation", "text": "It has been shown that the three most dominant frequencies and their orders of magnitude (in the future referred to as Domfreq) help in the SED task, motivated by the idea that overlapping sound events do not always have the same dominant frequencies, and the network can learn to differentiate these overlapping events using the dominant frequencies; the number of people who parabolically interpolate in the channels is high. STFT [12] in the range of 100 to 4000 Hz is from each of the binaural channels in frames of 40 ms, and we continue to use this feature in this range. Pitch is a perspective property that human listeners use to detect overlapping sound events."}, {"heading": "100 3x3 filters, 2D CNN, ReLUs Batch normalization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1x2 max pool", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50% dropout", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "100 3x3 filters, 2D CNN, ReLUs Batch normalization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1x2 max pool", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50% dropout", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "100 3x3 filters, 2D CNN, ReLUs Batch normalization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1x2 max pool", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50% dropout", "text": "TDOA features are layered on T x 5 x 3, and the 60 values of GCC-PHAT are layered on T x 60 x 3. Because the dimensions of mel, GCC-PHAT, and ACR are high, we use three CNN layers followed by maximum pooling to reduce the dimension of the final feature card to T x 5 x 100. When using TDOA and dom-freq features, a single 100-filter CNN layer is used without maximum pooling. To keep the time information for final tone entry and offset detection intact, we do not apply a maximum pooling sequence in Timeline (T) axis. After CNNs, the feature cards are merged by linking and two consecutive bidirectional short-time memories (LSTM) are fed in. The output layer is a fully connected time distribution layer that has as many units as the number of classes are activated simultaneously in the data class to activate a signature T."}, {"heading": "4.1. Datasets", "text": "The proposed SED system is evaluated on two real data sets - TUT-Sound Events 2009 (TUT-SED 2009) [19] and TUT-Sound Events 2016 (TUT-SED 2016) [20]. Both data sets were recorded with in-ear microphones. TUT-SED 2009 was used for the SED in a monaural context [14], but no previous work has reported on the use of binaural recordings on this data set. TUT-SED 2016 was published as part of the DCASE-2016 challenge [21] to enable public benchmarking. TUT-SED 2009 is fifteen times larger than TUT-SED 2016, showing significant improvements over TUT-SED 2009."}, {"heading": "4.2. Metrics", "text": "(ii) Falsely positive (FP (k)): Total number of events active in both the reference and the system output segment. (iii) Falsely positive (FP (k): Total number of events active in the system output segment, but not in the reference output segment. (iii) Falsely negative (FN (k): Total number of events active in the reference segment, but not in the system output segment. (2) The first metric, F score, is then calculated as, F = 2 \u00b7 \u2211 K = 1 TP (k) 2 \u00b7 \u2211 K = 1 TP (k) + \u2211 K k k = 1 FP = 1 KP = 1 KP = 1 KP = 1 K = 1 K (k) K (k) The second metric K (k) = K (K) K (K) (K = K) K (K) (K) K (K = K) K (K) K (K) K = 1 KP = 1 KP = 1 KP = 1 KP = 1 K (K) = 1 K (k) The second metric K = 1 K = K = K (K) K (K) K = K (K) K = K (K) K = K = K (K) K = K = K (K) K = K = K = K = K (K) K = K = K = K = K = K (K)"}, {"heading": "4.3. Baseline", "text": "The proposed CBRNN architecture with binaural features is compared to the modern monaural SED system introduced in [14], which used 40 monaural log mel-band energies (mel-monaural) as characteristics. The network had three CNN's with 96 filters each, followed by max pooling in frequency axis, reducing the dimension to one. Subsequently, CNN's feature map was fed to three LSTMs with 256 units each."}, {"heading": "4.4. Results", "text": "This year, the time has come for us to be able to start looking for a new partner who is able to find a new home."}], "references": [{"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "IEEE International Workshop on Machine Learning for Signal Processing (MLSP), 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Environmental sound recognition with time-frequency audio features", "author": ["S. Chu", "S. Narayanan", "C.J. Kuo"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio surveillance: A systematic review", "author": ["M. Crocco", "M. Cristani", "A. Trucco", "V. Murino"], "venue": "ACM Computing Surveys (CSUR), 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic surveillance of the acoustic activity in our living environment", "author": ["A. Harma", "M.F. McKinney", "J. Skowronek"], "venue": "IEEE International Conference on Multimedia and Expo (ICME), 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Audio keywords generation for sports video analysis", "author": ["M. Xu", "C. Xu", "L. Duan", "J.S. Jin", "S. Luo"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Acoustic event detection: SVM-based system and evaluation setup in CLEAR\u201907", "author": ["A. Temko", "C. Nadeu", "J. Biel"], "venue": "Springer-Verlag, Berlin, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Spatial diffuseness features for DNN-based speech recognition in noisy and reverberant environments", "author": ["A. Schwarz", "C. Huemmer", "R. Maas", "W. Kellermann"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multichannel audio source separation with deep neural networks", "author": ["A.A. Nugraha", "A. Liutkus", "E. Vincent"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "On our perception of sound direction", "author": ["J.W. Strutt"], "venue": "Philosophical Magazine, 1907.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1907}, {"title": "Sound event detection in multichannel audio using spatial and harmonic features", "author": ["S. Adavanne", "G. Parascandolo", "P. Pertil\u00e4", "T. Heittola", "T. Virtanen"], "venue": "Detection and Classification of Acoustic Scenes and Events (DCASE), 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "The generalized correlation method for estimation of time delay", "author": ["C. Knapp", "C. Carter"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, 1976.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1976}, {"title": "Sinusoidal Peak Interpolation, in Spectral Audio Signal Processing, accessed 23.06.2016, online book, 2011 edition. [Online]. Available: https://ccrma.stanford.edu/\u223cjos/sasp/ Sinusoidal Peak Interpolation.htm", "author": ["J.O. Smith"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Auditory scene analysis: The perceptual organization of sound", "author": ["A.S. Bregman"], "venue": "MIT Press, 1990.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Convolutional recurrent neural networks for polyphonic sound event detection", "author": ["E. Cakir", "G. Parascandolo", "T. Heittola", "H. Huttunen", "T. Virtanen"], "venue": "IEEE/ACM TASLP Special Issue on Sound Scene and Event Analysis, 2017, accepted for publication.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, vol. abs/1502.03167, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research (JMLR), 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980 [cs.LG], 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Audio context recognition using audio event histograms", "author": ["T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen"], "venue": "European Signal Processing Conference (EUSIPCO), 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "TUT database for acoustic scene classification and sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "European Signal Processing Conference (EU- SIPCO), 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Metrics for polyphonic sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "Applied Sciences, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For example, recognizing environmental sounds [1][2] will give an idea about the local biodiversity.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "For example, recognizing environmental sounds [1][2] will give an idea about the local biodiversity.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "Detecting sound events such as glass breaking and alarm detection can be used for surveillance [3][4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Detecting sound events such as glass breaking and alarm detection can be used for surveillance [3][4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Furthermore, the detected sound events can be used as a mid-level representation to help retrieval of content based query [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "[6] proposed to use multichannel audio, and combined classification likelihoods across channels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Similar multichannel features have been proposed in automatic speech recognition (ASR) [7] and source separation [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "Similar multichannel features have been proposed in automatic speech recognition (ASR) [7] and source separation [8].", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "to exploit the spatial data available at their ears (multichannel) to identify both isolated and polyphonic sound events [9], we can potentially train the SED systems to learn similar spatial information with multichannel data.", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "Recently, such spatial features motivated by the binaural hearing of humans were proposed and shown to be promising for SED task in [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "This idea is motivated from the interaural intensity difference (IID) used by humans [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "The TDOA is estimated using the GCC-PHAT [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "In [10], it was shown that the three most dominant frequencies and their magnitudes (referred as dom-freq in future) helped in the SED task.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "The dom-freq values were picked from thresholded parabolically-interpolated STFT [12] in the 100 to 4000 Hz range from each of the binaural channels in frames of 40 ms.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "The pitch is a perceptual feature which human listeners have been using to recognize overlapping sound events [13].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "The best results to date in polyphonic SED was reported in [14], where an architecture exploiting the combined modeling capacities of a convolutional neural network (CNN), recurrent neural network (RNN) and fully connected (FC) layer termed as the convolutional recurrent neural network (CRNN) was proposed.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Batch normalization [15] is used in all the CNN layers.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "A 50% dropout [16] is utilized in all CNNs and LSTMs to avoid over-fitting of the network.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "The combined architecture was trained by backpropagation through time [17] using Adam optimizer [18] and binary cross-entropy objective.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "The combined architecture was trained by backpropagation through time [17] using Adam optimizer [18] and binary cross-entropy objective.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "The proposed SED system is evaluated on two real-life datasets -TUT Sound Events 2009 (TUT-SED 2009) [19] and TUT Sound Events 2016 Development set (TUT-SED 2016) [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 19, "context": "The proposed SED system is evaluated on two real-life datasets -TUT Sound Events 2009 (TUT-SED 2009) [19] and TUT Sound Events 2016 Development set (TUT-SED 2016) [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "TUT-SED 2009 has been used for SED in monaural context [14], but no previous work has reported using the binaural recordings on this dataset.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "The recordings have been manually annotated, and the annotated events have been grouped into 61 event classes [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "The home context has ten recordings with 11 sound event classes, and the residential area has 12 recordings with seven sound event classes [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "The SED system output is evaluated with the reference in fixed length intervals, also called as segment-based evaluation [22].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "The proposed CBRNN architecture with binaural features is compared with the state of the art monaural SED system introduced in [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "CRNN baseline [14] 0.", "startOffset": 14, "endOffset": 18}], "year": 2017, "abstractText": "This paper proposes to use low-level spatial features extracted from multichannel audio for sound event detection. We extend the convolutional recurrent neural network to handle more than one type of these multichannel features by learning from each of them separately in the initial stages. We show that instead of concatenating the features of each channel into a single feature vector the network learns sound events in multichannel audio better when they are presented as separate layers of a volume. Using the proposed spatial features over monaural features on the same network gives an absolute F-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and 2.7% on the TUT-SED 2009 dataset that is fifteen times larger.", "creator": "LaTeX with hyperref package"}}}