{"id": "1706.02292", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition", "abstract": "This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with the state-of-the-art method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the 'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset.", "histories": [["v1", "Wed, 7 Jun 2017 06:06:14 GMT  (59kb,D)", "http://arxiv.org/abs/1706.02292v1", "Accepted for Sound and Music Computing (SMC 2017)"]], "COMMENTS": "Accepted for Sound and Music Computing (SMC 2017)", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["miroslav malik", "sharath adavanne", "konstantinos drossos", "tuomas virtanen", "dasa ticha", "roman jarina"], "accepted": false, "id": "1706.02292"}, "pdf": {"name": "1706.02292.pdf", "metadata": {"source": "CRF", "title": "STACKED CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR MUSIC EMOTION RECOGNITION", "authors": ["Miroslav Malik", "Sharath Adavanne", "Konstantinos Drossos", "Tuomas Virtanen", "Dasa Ticha", "Roman Jarina"], "emails": ["firstname.lastname@fel.uniza.sk", "firstname.lastname@tut.fi"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, it is the case that one is able to go in search of a solution that is able to find a solution, that is able to find a solution that is able to find a solution, that is able to find a solution, and that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2. PROPOSED METHOD", "text": "We evaluate the method with two separate audio features - baseline (Section 3.2.1) and Raw (Section 3.2.2). Our proposed method, illustrated in Figure 1, is a neural network architecture obtained by stacking a CNN with two branches of FC and RNN each for arousal and valence. This combined CRNN architecture maps the entered audio features into their respective excitation and value values. The output of the method, arousal and value values, is in the range of [-1, 1]. In our method, the local displacement invariant properties of the audio features are represented by a CNN with a receptive filter size of 3 \u00d7 3. The CNN function card acts as an input to two parallel but identical branches, one for arousal and the other maison x. Each of these branches consists of the FC, the piedirectional grecurrent unit (GRU)."}, {"heading": "3. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "For the evaluation of our method, we used the dataset provided for the MediaEval EiM task [15]. The annotations used the two-dimensional continuous emotional space of the Russell [36]. The values of arousal and valence associated with each 500 ms segment were used by five to seven annotators per song from the Amazon Mechanical Turk.The training set consisted of 431 audio clips with a length of 45 seconds from the Free Music Archive. The first 15 seconds of the annotations were used to customize the annotation.The annotations and characteristics for the remaining 30 seconds were provided as a development kine.This is equivalent to 60 annotations per audio file (30 s audio with annotations every 500 ms).Each of the annotations is in the range of [-1, 1] for both arousal and valence. Negative values represent low excitation / valence and positive climages.The evaluation set consists of 58 complete web songs from the Multidataset Development for MediaEval."}, {"heading": "3.2 Audio features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Baseline features", "text": "Most of the research is based on this feature set, hence we call it the Basic Feature Set. It consists of the mean and standard deviation of 65 low-level acoustic descriptors and their first-order derivatives from the 2013 INTERSPEECH Computational Paralinguistic Challenge [37] extracted with the openSMILE toolbox [38]. The feature set includes Mel Frequency Cepstral coefficients (MFCCs), spectral features such as flux, centroid, kurtosis and rolloff, as well as language-related features such as jitter and shimmer. The total set of features is 260 and they were extracted from non-overlapping segments of the length 500 ms. The mean and standard deviation of the features for these 500 ms was estimated from frames of 60 ms with 50 ms (approximately 80%) overlap."}, {"heading": "3.2.2 Raw audio feature", "text": "The above basic characteristics consist of averages, standard deviations, first and second order derivatives of different raw characteristics. Neural networks themselves can extract such statistics directly from the raw characteristics. Therefore, we argue that these characteristics are not the best choice for neural networks. To examine the performance of the proposed network based on the raw characteristics, we only use the log melband energies. Mel band related characteristics were previously used for MediaEval EiM task [18] and MIREX AMC task [39]. The Librosa Python library [40] was used to extract the Mel band characteristics from 500 segments, similar to the Basic Feature Set."}, {"heading": "3.3 Metric", "text": "The RMSE is used in many areas as a statistical measure for measuring model power. It represents a standard deviation of the differences in predicted values from the line of best fit at sample level. Considering N predicted samples y-n and the corresponding reference samples yn, the RMSE can be written between them as follows: RMSE = 270 N = 1 (y-n-yn) 2N (1)"}, {"heading": "3.4 Baseline", "text": "The proposed method will be compared with the system proposed in [25]. To our knowledge, this method has achieved the best results on MediaEval's EiM dataset using the basic audio features. This result was achieved using an ensemble of six bidirectional LSTM networks (BLSTM) with five layers and 256 units each, trained on a sequence length of 10. The first two layers of these networks were pre-trained with the basic characteristics, the six networks of the ensemble were selected to cover all training data and sequence lengths of 10, 20, 30 and 60. The output of the network was merged with SVR with a third-order polynomial core and an artificial neural network (ANN) with 14 nodes. The average output of SVR and ANN was used as final excitation and valence values. In terms of the complexity of this basic method, a five-layer 256 input and output unit will have about ST2 million BLM parameters."}, {"heading": "8, time distributed FC", "text": "In addition, SVR and ANN increase the complexity of the overall method."}, {"heading": "3.5 Evaluation procedure", "text": "The hyper-parameter estimation of the proposed method was performed by changing the number of layers of CNN, FC and GRU from one to three, and the number of units for each of these layers varied in the set of {4, 8, 16, 32}. Identical failure rates were used for CNN and GRU and varied in the set of {0.25, 0.5, 0.75}. Parameters were decided on the basis of the best RMSE value on the development set, with the base features, the mini-batch size L1 and the maximum sequence length of 60. The mini-batch size of 32 was selected from the set of {16, 32, 64, 128}, based on the variance in the training set NMSO score on the development set using the base functions, the mini-batch size L1 and the maximum sequence length of 60. The mini-batch size of 32 was selected from the set of {16, 64, 128}, based on the variance in the training set NMSO score and the number of sequences of values per unit, the best number of CRU was taken by the same number of units as the best RSE was taken."}, {"heading": "4. RESULTS AND DISCUSSION", "text": "Tables 1a and 1b present the RMSE values based on the development and evaluation data sets for the baseline and protocol melband functions, respectively. These values are the mean and standard deviation over five separate runs of the method with different initialization of network weights. The proposed CRNN method with basic functions yielded an average RMSE of 0.242 to evaluation set (see average RMSE characteristics for sequence length 60 in Table 1a). The Li et al. baseline method yielded an average RMSE of 0.255 (see Table 2). In comparison, the CRNN method has only about 30 k parameters (about 400 times less) and performs significantly better in the sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence than the Li et al. Potentially, further improvement can be achieved by using an ensemble of the proposed method that covers different sequence lengths and all training data as proposed in the Li et al. System. The configuration of the NCRB parameters Li-Li-MSE had the same NCRB method Li-MSE and Li-MSE parameters, see NCRB-Li-Li-MSE for example."}, {"heading": "5. CONCLUSION", "text": "In this paper, we proposed a method consisting of stacked convolutionary and recurrent neural networks for continuous prediction of emotions in the two-dimensional V-A. The proposed method used significantly fewer parameters than the Li et al. System, and the results obtained exceeded the results of the Li et al. System. The proposed CRNN was evaluated with different sequence lengths, and the smaller sequence lengths showed better performance than the longer lengths. Instead of basic functions, a log-mel-band energy function feature was proposed, and the proposed CRNN saw information that corresponded to those of basic functions solely from the melting band functions."}, {"heading": "6. REFERENCES", "text": "[1] P. N. Juslin and K. R. Scherer, Vocal expression of Affect. Oxford University Press, 2005, pp. 65-135. [2] P. N. Juslin and P. Laukka, \"Communication of emotions in vocal expression and music performance: different channels, same code?\" Psychological bulletin, vol. 129, no. 5, pp. IES 2016, September 2003. [3] - \"Expression, perception, and induction of musical emotion: A review and a questionnaire study of daily listening,\" Journal of New Music Research, vol. 33, no. 3, pp. 217-238, 2004. [4] S. Zhang, Q. Tian, S. Jiang, and induction of musical emotion: A. Florective study of daily listening, vol."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with state-of-theart method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the \u201cMediaEval2015 emotion in music\u201d dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset.", "creator": "LaTeX with hyperref package"}}}