{"id": "1706.02295", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Generative-Discriminative Variational Model for Visual Recognition", "abstract": "The paradigm shift from shallow classifiers with hand-crafted features to end-to-end trainable deep learning models has shown significant improvements on supervised learning tasks. Despite the promising power of deep neural networks (DNN), how to alleviate overfitting during training has been a research topic of interest. In this paper, we present a Generative-Discriminative Variational Model (GDVM) for visual classification, in which we introduce a latent variable inferred from inputs for exhibiting generative abilities towards prediction. In other words, our GDVM casts the supervised learning task as a generative learning process, with data discrimination to be jointly exploited for improved classification. In our experiments, we consider the tasks of multi-class classification, multi-label classification, and zero-shot learning. We show that our GDVM performs favorably against the baselines or recent generative DNN models.", "histories": [["v1", "Wed, 7 Jun 2017 10:19:30 GMT  (1660kb,D)", "http://arxiv.org/abs/1706.02295v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chih-kuan yeh", "yao-hung hubert tsai", "yu-chiang frank wang"], "accepted": false, "id": "1706.02295"}, "pdf": {"name": "1706.02295.pdf", "metadata": {"source": "CRF", "title": "Generative-Discriminative Variational Model for Visual Recognition", "authors": ["Chih-Kuan Yeh", "Yao-Hung Hubert Tsai", "Yu-Chiang Frank Wang"], "emails": ["jason6582@gmail.com", "yaohungt@cs.cmu.edu", "ycwang@citi.sinica.edu.tw"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Related Work", "text": "Preventing overfitting in DNN: In order to train DNNs with sufficient generalizability and avoid overadjustments, researchers have proposed various regulatory mechanisms for learning parameter weights (e.g., [30]). Recently, a number of work on preventing overadjustments in DNN has been accomplished by suppressing the correlation between network activities [25, 12, 4]. Alternatively, we encourage parsimonious representations as a form of regulation. Nevertheless, the main idea of DNN regulation is to limit the search space of network parameters. Together with a generative variable learning framework, we regulate the distribution of derived latent representation to fit into a specific time frame."}, {"heading": "3 Generative-Discriminative Variational Model (GDVM)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Variational Model for Supervised Learning", "text": "Let X = {x1,.., xN} denote a set of N training instances, and Y = {y1,.., yN} denote the corresponding training results. For supervised learning, the goal is to learn a function F with a loss metric L, so that the expected loss of L (F (Xt), Yt) for given test data Xt and its basic truth results Yt would be minimized. For deep learning, F is often written by layers of neural networks \u03a6.In this essay, we introduce a latent variable z, which generatively models the conditional probability P (y | x) for supervised learning tasks. With the introduced latent variable z, the maximization of the above conditional probability can be written as P (y | x). = In this essay, the maximizing probability of P (z) is represented as maximizable."}, {"heading": "3.2 The Variational Bound", "text": "With the introduced latent variable z, we now set P (y | x) in relation to Ez (Q-QP (y | x, z), which is the key to the variable Bayesian methods. First, we derive the Kullback-Leiber divergence (KL divergence, abbreviated as KL) between Q (z | x) and P (z | x, y) as follows: KL (Q (z | x, y))) = Ez \u00b2 Q (\u00b7 | x) [logQ (z | x) \u2212 logP (z | x, y)]. (2) Applying the Bayes rule to P (z | x, y) results in the following equation containing Ez \u00b2 QP (y | x) and P (y | x): logP (y | x) \u2212 KL (x) \u2212 xix = KZ = K \u00b2, y)."}, {"heading": "3.3 Optimizing The Variational Bound", "text": "As noted above, the optimization of the limit of variation of P (y | x) is the maximization of Q (Q \u00b7 Q (3). Specifically, this is realized by maximizing the RHS of (3), with additional insights such as we are now discussing. Let Q (\u00b7) and P (\u00b7) be two neural networks and z as a hidden layer. As shown in Figure 1, we are approaching the maximization of P (z | x) in (3) [logP (y | x, z)] in (3) by optimizing this network structure. On the other hand, the minimization of KL (z) | | P (z) | P (z | x)) in (3) is considered to regulate the distributed layer of z, which forces the output distribution of Q (z | x) to match a given prior (e.g. N (0, I) in our work. With the networks Q () and \u00b7 (we can maximize the limit of variation)."}, {"heading": "3.4 Additional Remarks", "text": "Comparison to variation models: Our proposed variational learning architecture can be viewed as a deep-set graphical model with latent variables such as Kingma et al. [13]. In [13], 1Alternatively, we can draw several z from the previous distribution N (0; I) and use the average of P (y | x, z) as a prediction. Variational atoencoder (VAE) is a conditional graphical model whose inputs x form a Gaussian prediction of variables z for the production of the results y. To generalize these learning settings, the conditional variation crystal autoencoder (CVAE) is a consistent graphical model whose inputs x-modulate."}, {"heading": "4 Experiments", "text": "In order to evaluate the performance of our proposed GDVM, we conduct experiments on multi-class classification, multi-class classification and zero-shot learning. We compare our method with a CNN base model and with GSNN [24], which is implemented by setting \u03b2 = 0 in (5) without recurring connection. In our experiments, we use the same network structures (i.e. networks \u03a6 and \u00b5) for the CNN base model, GSNN and our proposed model, and perform parameter selection by validation using 20% of the training data."}, {"heading": "4.1 Multi-Class Classification", "text": "We look at the benchmark data set of CIFAR10 [14] for multi-class classification2. CIFAR10 consists of 60,000 32 x 32 images with 10 categories. We use 50,000 images of all categories for training and the remainder for testing purposes. For the basic CNN architecture, we build a network based on VGG [22] with fewer folds and fully connected neurons due to the small image size in CIFAR10. We select the dimension for z as 64. The detailed network structure is represented in the supplementary representation, which is learned using an SGD optimizer with learning rate 0.05 and dynamics 0.9. We perform validation over the total epochs in the range of [200, 300, 400] and select \u03b2 in (5) from the range of [0.1, 0.5, 1.0]. Finally, we report on the average accuracy of 5 passes for evaluation and comparability of 5 computational abilities. In order to evaluate the ability of our GDVM to alleviate the overfit data, we do not evaluate 20,000 for training exercises, we do not evaluate 10,000 for use [5000]."}, {"heading": "4.2 Multi-Label Classification on NUS-WIDE LITE", "text": "Unlike the classification of multiple classes, the classification of multiple labels requires predicting multiple labels to one input instance. With the proposed generative-discriminatory variation model, we want to verify that the derived latent representation would show promising capabilities in describing data with multiple labels, while the difference between the data can still be properly identified. To conduct the experiments, we consider the multi-label dataset of NUS-WIDE LITE [3], which includes 55,615 images with a total of 81 concepts (i.e. labels).In light of the pre-defined training and testing sets of this dataset, we apply Alexnet folding layers that are pre-trained on ImageNet, as the network components are of interest for all methods, and Binary Cross Entropy is used to calculate the loss function. The detailed network structure is presented in the supplement. An Adam optimizer with a learning rate of 0.001-F1 is used effectively, and the results of the glide are presented."}, {"heading": "4.3 Zero-Shot Learning", "text": "Finally, we use our GDVM to solve a more difficult task of zero-shot learning, for which we need to recognize examples of classes that are not seen during training. We take the AWA dataset [16] and follow the same data split as in [1]. We choose Cross Modal Transfer (CMT) [23] as the CNN model for zero-shot learning, which performs the detection using the semantic space derived from training data of the classes we see. When we train our GDVM for zero-shot learning, we consider the input images as x and the output y as the semantic vector of the corresponding class. In other words, for target outputs y as semantic vectors of interest, we approach zero-shot learning by solving a regression task with our proposed model. The folding layers in the network structures are those of pre-trained GoogLeNet, with detailed network structures that are presented in the supplement."}, {"heading": "4.4 Computational Cost", "text": "As shown in Figure 1, the difference between the network architecture of our GDVM and that of the standard DNN is that we introduce an additional layer \u03a3 (x) with an operating layer N (\u00b5 (x), \u03a3 (x) = \u00b5 (x) + \u00b7 \u03a3 (x). Since we share all layers with \u00b5 (x) except for the highest layer, the additional cost of training such network components would be marginal. For quantitative comparisons, we train a neural network with a mini-batch size of 100 on CIFAR10 and evaluate the results over 100 epochs. For the CNN output transmitter with suspensions, we considered training time per image at 296.52 microseconds, while ours (also with suspensions) at 299.22 microseconds. Calculation time estimates were performed on a GTX 1080 GPU. Therefore, we confirm that our GDVM was able to achieve promising classification performance without significantly reducing training time."}, {"heading": "5 Conclusion", "text": "In this paper, we presented a novel generative-discriminatory variation model (GDVM) that uniquely develops a generative model with a deterministically discriminatory goal for supervised learning. Our GDVM is implemented via stochastic neural networks with Gaussian latent variables, while the network architecture can easily be built on existing CNN structures with marginal computing costs. We discussed the differences between our GDVM and existing generative or noise-scolding DNN models and explained why GDVM can be regarded as maximum margin models without explicitly determining the scope. As confirmed by a variety of classification tasks, our GDVM has been shown to be preferred over base or state-of-the-art DNN models, especially when the size of the training set was not large enough."}, {"heading": "6 Supplement: Multi-Class Classification on MNIST", "text": "We perform multi-class classification experiments using MNIST's handwritten numerical data set, which consists of 60,000 images from 10 categories. [14] After dividing the data from [14], we use the CNN architecture to train our GDVM (see Section 7 for detailed structure), and use the RMSProp Optimizer with learning rates of 0.001 and 0.9. With 20% of the training data as a validation set, we select \u03b2 from [0.1, 0.5, 1.0, 5.0, 10.0] over the total epochs in the range of [25, 50, 100, 150]. Finally, we report on the average accuracy of the test data of 10 runens.Classification results: We conduct experiments with [500, 1000, 2000, 4000] training instances. During the classification experiments, we list the dimensions for the latent layer z as 64. Table 4 and compare the results. From this table, we see that our GDVM generally achieves improved or comparable results with the NN's average resolution, or we compare the DN's with the 28."}, {"heading": "7 Supplement: Details of The Network Architectures", "text": "We now present the network architecture of our generative-discriminatory variation model. CIFAR10 \"s network architectures are shown in Tables 5 and 6, and MNIST\" s in Tables 7 and 8. For the multi-class classification on NUSWIDE LITE, our network designs are listed in Tables 9 and 10, and those for zero-shot learning on AWA are listed in Tables 11 and 12. For experiments with DNN models without dropouts, we simply remove all the layers of failure when learning networks."}], "references": [{"title": "Evaluation of output embeddings for fine-grained image classification", "author": ["Z. Akata", "S. Reed", "D. Walter", "H. Lee", "B. Schiele"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2927\u20132936,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1607.07539,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Nus-wide: a real-world web image database from national university of singapore", "author": ["T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y. Zheng"], "venue": "Proceedings of the ACM international conference on image and video retrieval, page 48. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Reducing overfitting in deep networks by decorrelating representations", "author": ["M. Cogswell", "F. Ahmed", "R. Girshick", "L. Zitnick", "D. Batra"], "venue": "arXiv preprint arXiv:1511.06068,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Tutorial on variational autoencoders", "author": ["C. Doersch"], "venue": "arXiv preprint arXiv:1606.05908,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, pages 2672\u20132680,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Large scale max-margin multi-label classification with priors", "author": ["B. Hariharan", "L. Zelnik-Manor", "M. Varma", "S. Vishwanathan"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 423\u2013430,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, 18(7):1527\u20131554,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "stat, 1050:10,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(3):453\u2013465,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "Nips, volume 2, page 5,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning deep parsimonious representations", "author": ["R. Liao", "A. Schwing", "R. Zemel", "R. Urtasun"], "venue": "Advances in Neural Information Processing Systems, pages 5076\u20135084,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep generative deconvolutional image model", "author": ["Y. Pu", "W. Yuan", "A. Stevens", "C. Li", "L. Carin"], "venue": "Artificial Intelligence and Statistics, pages 741\u2013750,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["S. Reed", "K. Sohn", "Y. Zhang", "H. Lee"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1431\u20131439,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Artificial Intelligence and Statistics, pages 448\u2013455,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "Advances in neural information processing systems, pages 935\u2013943,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "H. Lee", "X. Yan"], "venue": "Advances in Neural Information Processing Systems, pages 3483\u20133491,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "On deep generative models with applications to recognition", "author": ["J. Susskind", "V. Mnih", "G. Hinton"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning stochastic feedforward neural networks", "author": ["Y. Tang", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems, pages 530\u2013538,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "Advances in neural information processing systems, pages 25\u201332,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research, 11(Dec):3371\u20133408,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "venue": "European Conference on Computer Vision, pages 835\u2013851. Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image inpainting with perceptual and contextual losses", "author": ["R. Yeh", "C. Chen", "T.Y. Lim", "M. Hasegawa-Johnson", "M.N. Do"], "venue": "arXiv preprint arXiv:1607.07539,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].", "startOffset": 145, "endOffset": 152}, {"referenceID": 14, "context": "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].", "startOffset": 145, "endOffset": 152}, {"referenceID": 26, "context": "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].", "startOffset": 202, "endOffset": 206}, {"referenceID": 24, "context": "For example, dropout [25] is a regularization technique which randomly removes a fraction of neurons during the training phase, and batch normalization [12] imposes constraints for normalizing the produced representations.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "For example, dropout [25] is a regularization technique which randomly removes a fraction of neurons during the training phase, and batch normalization [12] imposes constraints for normalizing the produced representations.", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "For example, Gaussian Processes Latent Variable Models (GPLVM) [17] construct low-dimensional manifolds by observing a small number of instances.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "Variational autoencoders (VAE) [13] derives deep latent spaces by utilizing stochastic variational inference, which scales efficiently to the datasets with varying sizes.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "[21] apply generative Restricted Boltzmann Machines for learning Deep Boltzmann Machines, which is latter used to fine-tune a classification network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] use a gated MRF at the lowest level of deep belief network for recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": ", [30]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "More recently, a line of works in preventing overfitting in DNN is achieved by suppressing correlation between network activations [25, 12, 4].", "startOffset": 131, "endOffset": 142}, {"referenceID": 11, "context": "More recently, a line of works in preventing overfitting in DNN is achieved by suppressing correlation between network activations [25, 12, 4].", "startOffset": 131, "endOffset": 142}, {"referenceID": 3, "context": "More recently, a line of works in preventing overfitting in DNN is achieved by suppressing correlation between network activations [25, 12, 4].", "startOffset": 131, "endOffset": 142}, {"referenceID": 17, "context": "[18] encourage parsimonious representations as a form of regularization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Deep Generative Models: Generative models aim to capture data distributions so that a proper feature representation can be produced for achieving learning tasks like image inpainting [33] and feature disentanglement [20, 2].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "Deep Generative Models: Generative models aim to capture data distributions so that a proper feature representation can be produced for achieving learning tasks like image inpainting [33] and feature disentanglement [20, 2].", "startOffset": 216, "endOffset": 223}, {"referenceID": 1, "context": "Deep Generative Models: Generative models aim to capture data distributions so that a proper feature representation can be produced for achieving learning tasks like image inpainting [33] and feature disentanglement [20, 2].", "startOffset": 216, "endOffset": 223}, {"referenceID": 20, "context": "A number of deep generative models have been proposed and attracted the attention from the researchers, including Deep Boltzmann machine [21] and Deep Belief Networks [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "A number of deep generative models have been proposed and attracted the attention from the researchers, including Deep Boltzmann machine [21] and Deep Belief Networks [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 6, "context": "Recently, Generative Adversarial Network (GAN) [7] and Variational Autoencoder [13] have shown remarkable performances on a variety of learning tasks.", "startOffset": 47, "endOffset": 50}, {"referenceID": 12, "context": "Recently, Generative Adversarial Network (GAN) [7] and Variational Autoencoder [13] have shown remarkable performances on a variety of learning tasks.", "startOffset": 79, "endOffset": 83}, {"referenceID": 25, "context": "Recently, deep generative models can achieve promising results on recognition tasks [26, 21, 19] by combining a discriminative objective function with a generative one.", "startOffset": 84, "endOffset": 96}, {"referenceID": 20, "context": "Recently, deep generative models can achieve promising results on recognition tasks [26, 21, 19] by combining a discriminative objective function with a generative one.", "startOffset": 84, "endOffset": 96}, {"referenceID": 18, "context": "Recently, deep generative models can achieve promising results on recognition tasks [26, 21, 19] by combining a discriminative objective function with a generative one.", "startOffset": 84, "endOffset": 96}, {"referenceID": 27, "context": "Conditional Generative Models: Stochastic Feed-forward Neural Network (SFNN) [28] and Conditional Variational Autoencoder (CVAE) [24, 32] are both directed graphical models, which derive distributions of output variables based on the input data for structured output prediction (e.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "Conditional Generative Models: Stochastic Feed-forward Neural Network (SFNN) [28] and Conditional Variational Autoencoder (CVAE) [24, 32] are both directed graphical models, which derive distributions of output variables based on the input data for structured output prediction (e.", "startOffset": 129, "endOffset": 137}, {"referenceID": 31, "context": "Conditional Generative Models: Stochastic Feed-forward Neural Network (SFNN) [28] and Conditional Variational Autoencoder (CVAE) [24, 32] are both directed graphical models, which derive distributions of output variables based on the input data for structured output prediction (e.", "startOffset": 129, "endOffset": 137}, {"referenceID": 12, "context": "Thus, as suggested in [13, 6], we wish to perform such sampling while satisfying (1).", "startOffset": 22, "endOffset": 29}, {"referenceID": 5, "context": "Thus, as suggested in [13, 6], we wish to perform such sampling while satisfying (1).", "startOffset": 22, "endOffset": 29}, {"referenceID": 12, "context": "This is the reason why we apply the reparameterization trick [13] in constructing network components in Q, which relates the output variable z to N (\u03bc(x),\u03a3(x)) and maintains its dependency on Q.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "[13] did.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In [13],", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "To generalize to supervised learning settings, Conditional Variational Autoencoder (CVAE) [24] is a conditional directed graphical model, whose inputs x modulate a Gaussian prior on latent variables z for producing the outputs y.", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Extended from CVAE, Gaussian stochastic neural network (GSNN) [24] applies the same network architecture for \u03b8r and \u03b8p.", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "However, most MMC-based approaches [29, 5, 8] aim at maximizing a pre-determined margin of data from different classes.", "startOffset": 35, "endOffset": 45}, {"referenceID": 4, "context": "However, most MMC-based approaches [29, 5, 8] aim at maximizing a pre-determined margin of data from different classes.", "startOffset": 35, "endOffset": 45}, {"referenceID": 7, "context": "However, most MMC-based approaches [29, 5, 8] aim at maximizing a pre-determined margin of data from different classes.", "startOffset": 35, "endOffset": 45}, {"referenceID": 24, "context": "Comparison to Noise-Imposing Learning Methods: We note that, adding noise to a hidden layer has previously been seen in Dropout [25], while denoising autoencoder [31] and data augmentation methods [15] can be considered as imposing noisy/variation information in the input layer.", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "Comparison to Noise-Imposing Learning Methods: We note that, adding noise to a hidden layer has previously been seen in Dropout [25], while denoising autoencoder [31] and data augmentation methods [15] can be considered as imposing noisy/variation information in the input layer.", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Comparison to Noise-Imposing Learning Methods: We note that, adding noise to a hidden layer has previously been seen in Dropout [25], while denoising autoencoder [31] and data augmentation methods [15] can be considered as imposing noisy/variation information in the input layer.", "startOffset": 197, "endOffset": 201}, {"referenceID": 23, "context": "We compare our method to a baseline CNN as well as GSNN [24], which is implemented via setting \u03b2 = 0 in (5) without recurrent connection.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "We consider the benchmark dataset of CIFAR10 [14] for multi-class classification2.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "For the baseline CNN architecture, we build a network based on VGG [22] with fewer convolution and fully connected neurons due to the small size of images in CIFAR10.", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "To perform the experiments, we consider the multi-label dataset of NUS-WIDE LITE [3], which includes 55,615 images with a total of 81 concepts (i.", "startOffset": 81, "endOffset": 84}, {"referenceID": 15, "context": "We take the AWA dataset [16], and follow the same data split as did in [1].", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "We take the AWA dataset [16], and follow the same data split as did in [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 22, "context": "We choose Cross Modal Transfer (CMT) [23] as the", "startOffset": 37, "endOffset": 41}], "year": 2017, "abstractText": "The paradigm shift from shallow classifiers with hand-crafted features to endto-end trainable deep learning models has shown significant improvements on supervised learning tasks. Despite the promising power of deep neural networks (DNN), how to alleviate overfitting during training has been a research topic of interest. In this paper, we present a Generative-Discriminative Variational Model (GDVM) for visual classification, in which we introduce a latent variable inferred from inputs for exhibiting generative abilities towards prediction. In other words, our GDVM casts the supervised learning task as a generative learning process, with data discrimination to be jointly exploited for improved classification. In our experiments, we consider the tasks of multi-class classification, multi-label classification, and zero-shot learning. We show that our GDVM performs favorably against the baselines or recent generative DNN models.", "creator": "LaTeX with hyperref package"}}}