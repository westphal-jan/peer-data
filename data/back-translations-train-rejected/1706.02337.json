{"id": "1706.02337", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Network", "abstract": "We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.", "histories": [["v1", "Wed, 7 Jun 2017 18:51:31 GMT  (29326kb,D)", "http://arxiv.org/abs/1706.02337v1", "CVPR 2017 Spotlight"]], "COMMENTS": "CVPR 2017 Spotlight", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["xiao yang", "ersin yumer", "paul asente", "mike kraley", "daniel kifer", "c lee giles"], "accepted": false, "id": "1706.02337"}, "pdf": {"name": "1706.02337.pdf", "metadata": {"source": "CRF", "title": "Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks", "authors": ["Xiao Yang", "Ersin Yumer", "Paul Asente", "Mike Kraley", "Daniel Kifer", "C. Lee Giles"], "emails": ["xuy111@psu.edu", "mkraley}@adobe.com", "dkifer@cse.psu.edu", "giles@ist.psu.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Background", "text": "In fact, most of them are people who are able to survive on their own, without feeling able to flourish."}, {"heading": "3. Method", "text": "Our method performs a supervised pixel-by-pixel segmentation training with a specialized multimodal, fully revolutionary network that uses a text embedding map along with the visual signals. In addition, our MFCN architecture supports two unattended learning tasks to improve the learned document rendering: a reconstruction task based on an auxiliary decoder and a consistency task evaluated in the main decoder branch along with the loss of segmentation per pixel."}, {"heading": "3.1. Multimodal Fully Convolutional Network", "text": "As shown in Fig. 2, our MFCN model consists of four parts: an encoder, two decoders and a bridge. The encoder and decoder parts roughly follow the architecture guidelines set out by Noh et al. [42]. However, several changes have been made to better address document segmentation. First, we observe that several semantic classes, such as section headings and captions, tend to occupy relatively small areas. Furthermore, the correct identification of certain regions is often based on small visual keywords, such as lists identified by small balls or numbers in front of each element. This suggests that low-threshold characteristics must be used. Since max pooling naturally loses information during downsampling, FCN often leads poorly for small objects. [34] Attempting to avoid this problem with the help of skip connections is simply avoiding independent predictions based on different scales, which is not a satisfactory solution."}, {"heading": "3.2. Text Embedding Map", "text": "Traditional semantic image segmentation models learn the semantic meanings of objects from a visual perspective. However, our task also requires understanding the text in images from a linguistic perspective. Therefore, we build a text that embeds a map and feeds it into our multimodal model to use both visual and textual representations. We treat a sentence as the minimal unit that conveys certain semantic meanings, and represent it with a low-dimensional vector. Our sentence embedding is created by averaged embedding for individual words. This is a simple but effective method that has proven useful in many applications, including sentiment analysis [27] and text classification [28]. Using such embedding, we create a text embedding map as follows: for each pixel within the range of a sentence, we use the corresponding sentence as input. Pixels belonging to the same sentence have the same embedding capability, which do not belong to any sentence."}, {"heading": "3.3. Unsupervised Tasks", "text": "Although we provide a large amount of labeled data for training, they are limited in the variations in their arrangements. (b) To this end, we define two unmonitored loss functions to allow the use of real documents and encourage better representation. (b) It has been shown that reconstruction can help to learn better representations and thus improve performance for monitored tasks. (b) There will therefore be a second decoding path (Figure 2 - axillary decoder), called Drec, and a reconstruction loss of intermedia features. This auxiliary decoder exists only during training. (Let al, l = 1, 2, \u00b7 L be the activation of the lth layer of the encoder, and a0 be the input image.) For a feed-forward convolutionary network, al is a feature map of the size Cl \u00d7 Hl \u00d7 Wl."}, {"heading": "4. Synthetic Document Data", "text": "Since our MFCN aims to generate a segmentation mask of the entire document image, pixel-loaded annotations are required for the task being reviewed. Although there are several publicly available records for page segmentation [45, 51, 7], there are only a few hundred to a few thousand pages in each. In addition, the types of annotations are limited, for example to text, figures and tables, but our goal is to perform a much more granular segmentation. To address these problems, we have created a synthetic data engine capable of generating large-format, pixel-by-pixel annotated documents. Our synthetic document engine uses two methods to generate documents, the first of which generates a fully automated and random layout of partial data scraped from the Web. Let's generate LaTeX source files that combine paragraphs, image elements, sections, tablets, sections, and segments, and subsections."}, {"heading": "5. Implementation Details", "text": "Fig. 2 summarizes the architecture of our model. The auxiliary decoder exists only in the training phase. All revolutionary layers have a core size of 3 x 3 and a step width of 1. Pooling (in the encoders) and unpooling (in the decoders) have a core size of 2 x 2. Immediately after each convolution and above all non-linear functions.We subtract and reduce each input image per channel so that its longer side is less than 384 pixels. No other pre-processing is used. We use Adadelta [56] with a mini-stack size of 2. During semi-supervised training, mini-stacks of synthetic and real documents are used alternately so that their longer side is less than 384 pixels. In synthetic documents, both per pixel classification loss and unmonitored losses due to reverse propagation are activated."}, {"heading": "6. Experiments", "text": "We used three datasets for evaluations: ICDAR2015 [7], SectLabel [36] and our new dataset called DSSE-200. ICDAR2015 [7] is a dataset used in the biennial ICDAR page segmentation competitions [8], with a greater emphasis on appearance. ICDAR2015 \"s evaluation set consists of 70 sampled pages of contemporary magazines and technical articles. SectLabel [36] consists of 40 scientific papers with 347 pages in the field of computer science. Each line of text in these papers manually receives a semantic designation such as text, section header or list element. In addition to these two datasets, we introduce DSSE-2001, which offers both appearance and semantic designations. DSSE-200 contains 200 pages of journals and academic papers. Regions on one page are assigned labels from the following datasets: 200 illustration, table, section, and text, section, and SE contains."}, {"heading": "6.1. Ablation Experiment on Model Architecture", "text": "The results are shown in Table 1. Note that these results do not include textual information or unattended learning tasks. The purpose of this experiment is to find the best \"base\" architecture that can be used in the following experiments. All models are retrained from scratch and evaluated on the DSSE-200 datasets. As a simple starting point (Table 1 Model 1), we train a simple encoder-decoder-style model for document segmentation. It consists of a feed-forward-convolutional network as encoder and a decoder decoder implemented by a fully revolutionary network. Upsampling is done by bilinear interpolation. This model achieves an average IoU of 61.4%. Next, we add connections to the model, which results in modeling. This model is similar to the SharpMask model. We observe an average Io5.4%, better than the model."}, {"heading": "6.2. Adding Textual Information", "text": "We are now examining the importance of text information in our multimodal model. We are taking the best architecture, Model5, as a pure vision model and integrating a text embedding map via a bridge module shown in Figure 2. This combined model is aligned with our synthetic documents. As shown in Table 2, the use of text also improves performance for text classes. Accuracy for section header, caption, list and paragraph is increased by 1.1%, 0.1%, 1.7% and 2.2%. We rely on existing OCR engines [48] when extracting text, but they are not always reliable for scanned documents of low quality. To quantitatively analyze the effects of using extracted text, we are comparing the performance of using extracted text with real text. The comparison is performed on a subset of our synthetic dataset (200 images), as true text is of course available. As shown in Table 2, the use of real text leads to a noticeable improvement of this effect text (2.4% of ISE improvement), with a 6.3% improvement in ISE."}, {"heading": "6.3. Unsupervised Learning Tasks", "text": "Here we examine how the proposed two unsupervised learning tasks - reconstruction and consistency tasks - can complement the pixel-by-pixel classification during training. We take the best model in paragraph 6.2 and change only the training objectives. Our model is then semi-supervised as described in paragraph 5. The results are in Table 3. Adding the reconstruction task slightly by 0.6%, the addition of the consistency task results in a boost of 1.9%. These results justify our hypothesis that the use of regional information is beneficial. Combining the two tasks results in an average IoU of 75.9%."}, {"heading": "6.4. Comparisons with Prior Art", "text": "Table 4 and 5 present comparisons with several methods that have previously described the performance of ICDAR2015 and SectLabel datasets, while other methods cannot. It should be noted that our MFCN model simultaneously solves a binary segmentation problem and does not predict fine-grained classes. To allow fair comparison, we change the number of output channels from the last layer to 3 (background, figure and text) and refine this last layer. Our binary MFCN model reaches 94.5%, 91.0% and 77.1% IoU values for non-text (background and figure), text and figure regions, outperforming other models."}, {"heading": "7. Conclusion", "text": "The proposed model uses both visual and textual information. In addition, we propose an efficient method of generating synthetic data that delivers ground truth per pixel. Our unattended auxiliary tasks help to increase performance when tapping unlabeled real documents, which facilitates better imaging learning. We have shown that both the multimodal approach and unattended tasks can enhance performance, and our results indicate that we have improved the state of the art on already established benchmarks. In addition, we are providing the large synthetic dataset (135,000 pages) and a new benchmark dataset: DSSE-200."}, {"heading": "Acknowledgment", "text": "This work began during Xiao Yang's internship at Adobe Research. This work was supported by NSF funding CCF 1317560 and Adobe Systems Inc."}, {"heading": "A. Synthetic Document Data", "text": "In the first method, we create LaTeX source files in which elements such as paragraphs, illustrations, tables, captions, section headings and lists are randomly arranged using the \"textblock\" environment from the \"textpos\" package. Compiling these LaTeX files results in one-, two- or three-column PDFs. The generation process is carried out in Algorithm 1.Algorithm 1 Synthetic document Generation 1: s \u2190 a string with preambles and necessary package examples of a LaTeX source file 2: Choose a LaTeX source file of type T: {single-column, two-column, three-column, set drawings} 3: while the space on the page remains 4: Choose an element type E: {Figure, table, heading, section header, section header, list, paragraph, paragraph} 5: Choose an example of the source file} 3: You, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, E, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragraph, paragrap"}, {"heading": "C. Post-processing", "text": "We apply an optional post-processing step to clean up segment masks for documents in PDF format. First, we get candidate boxes by using Adobe Acrobat's automatic tagging capabilities [2] and analyzing the results. Boxes are stored in a tree structure, and the box of each node can be a TextRun (a string), TextLine (potentially a line of text), paragraph (potentially a paragraph), or container (potentially numbers or tables). Note that we ignore the semantic meanings associated with these boxes, and use the boxes only as candidate-bounding boxes in postprocessing. Figure 9 (2) and 10 (2) illustrate candidate-bounding boxes for each document."}, {"heading": "D. Additional Visualization Results", "text": "Figures 9 and 10 show additional visualization examples for synthetic documents, and Figure 11 shows additional examples for real documents. Colors of the segmentation labels are: Figure, Table, Section Header, Caption, List and Paragraph. Processing. Colors of the segmentation labels are: Figure, Table, Section Header, Caption, Section Header, Caption, Caption, List and Paragraph."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We present an end-to-end, multimodal, fully convolu-<lb>tional network for extracting semantic structures from doc-<lb>ument images. We consider document semantic structure<lb>extraction as a pixel-wise segmentation task, and propose a<lb>unified model that classifies pixels based not only on their<lb>visual appearance, as in the traditional page segmentation<lb>task, but also on the content of underlying text. Moreover,<lb>we propose an efficient synthetic document generation pro-<lb>cess that we use to generate pretraining data for our net-<lb>work. Once the network is trained on a large set of synthetic<lb>documents, we fine-tune the network on unlabeled real doc-<lb>uments using a semi-supervised approach. We systemati-<lb>cally study the optimum network architecture and show that<lb>both our multimodal approach and the synthetic data pre-<lb>training significantly boost the performance.", "creator": "LaTeX with hyperref package"}}}