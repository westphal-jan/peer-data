{"id": "1706.02361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "The Effects of Noisy Labels on Deep Convolutional Neural Networks for Music Classification", "abstract": "Deep neural networks (DNN) have been successfully applied for music classification including music tagging. However, there are several open questions regarding generalisation and best practices in the choice of network architectures, hyper-parameters and input representations. In this article, we investigate specific aspects of neural networks to deepen our understanding of their properties. We analyse and (re-)validate a large music tagging dataset to investigate the reliability of training and evaluation. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting and scaling. Using a trained network, we compute label vector similarities which is compared to groundtruth similarity.", "histories": [["v1", "Wed, 7 Jun 2017 19:54:39 GMT  (316kb,D)", "http://arxiv.org/abs/1706.02361v1", null], ["v2", "Sun, 10 Sep 2017 23:47:42 GMT  (185kb,D)", "http://arxiv.org/abs/1706.02361v2", "The section that overlapped witharXiv:1709.01922is completely removed in this version. Title is changed"]], "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.MM cs.SD", "authors": ["keunwoo choi", "george fazekas", "kyunghyun cho", "mark sandler"], "accepted": false, "id": "1706.02361"}, "pdf": {"name": "1706.02361.pdf", "metadata": {"source": "CRF", "title": "On the Robustness of Deep Convolutional Neural Networks for Music Classification", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk"], "sections": [{"heading": null, "text": "The results show several import aspects of music tagging and neural networks. We show that networks are effective despite relatively high error rates in ground truth datasets. We then show that many commonly used pre-processing techniques are redundant, except in terms of compression. INTRODUCTIONIC tags provide valuable insights into the relationships between music tags. These results underscore the utility of data-driven methods to address automatic music tagging."}, {"heading": "A. The Importance of Music Tagging", "text": "Labeling music can be seen as a classification problem for multiple labels, because music can be correctly associated with more than one real label, such as \"Rock,\" \"Guitar,\" \"Happy,\" and \"90s.\" This example also highlights the fact that labeling music from the perspective of music informatics can be viewed as several different tasks. Since tags can be related to genres, instrumentation, mood, and era, the problem is a combination of genre classification, instrument recognition, mood and time recognition, and possibly others. Below, we highlight three aspects of the task that emphasize its importance in music informatics research (MIR). First, jointly created tags reveal a lot of information about music consumption habits. Tag counts show how listeners label music in the real world, which is very different from the decision of a limited number of experts (see section III-A)."}, {"heading": "B. Contributions and organisation of this paper", "text": "This work focuses on the robustness and generalisability of deep convolutionary neural networks for the automatic tagging of popular music. Special attention is given to audio pre-processing strategies and the choice of hyperparameters associated with the network architecture. Our goal is to dispel the myth and widespread fallacy that large enough training data is sufficient to solve any problem. Key contributions to this work are: i) An analysis of the largest and most commonly used public record for tagging music, including an assessment of the distribution of labels within this dataset. ii) We validate the basic truth and discuss the impact of noise (e.g. mislabeling) on training and evaluation. ii) We compare audio input representations and pre-processing methods, and finally iv) analyze the trained network to gain valuable insights into the way social tags are associated with major problems."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "Music tagging refers to common classification and regression problems such as genre classification and emotion prediction. Using conventional machine learning systems, the research focused on extracting relevant music characteristics and applying a suitable classifier or regressor. For example, the first auto tagging algorithm [3] suggested the use of medium audio descriptors such as Mel-frequency Cepstral Coefficient (MFCCs) and an AdaBoost [6] classifier. As most audio characteristics are extracted frame by frame, aggregates such as mean, variance and percentiles are also commonly used to represent the distribution of characteristics. Subsequently, [7] vector quantization and clustering were proposed as an alternative to parametric representations. A current trend in music tagging is the use of data-driven methods to learn characteristics rather than design them at the neural level. \"These approaches are often referred to as multi-level learning approaches, also referred to in terms of representation or representation."}, {"heading": "A. Music tagging datasets and their properties", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "B. Labelling strategies", "text": "We finally need to consider a range of labelling strategies. Therefore, audio elements in a record can be labelled \"strong\" or \"weak,\" indicating several different aspects of labelling. First, there is a question of whether only positive labelling is used. In this scenario, only positive associations between the labels and the tracks are provided, meaning that a listener (or announcer) applies a label in case he recognises a relationship between the label and the music."}, {"heading": "C. Convolutional neural networks", "text": "ConvNets are a special type of neural network introduced into computer vision to simulate the behavior of the human visual system [13]. ConvNets have revolutionary layers, each of which consists of revolutionary cores. ConvNets sweep across the input, resulting in a weight distribution that greatly reduces the number of parameters compared to conventional layers that do not vibrate and are instead fully interconnected. ConvNets are trained to find and capture local patterns that are relevant to the task by spreading errors backwards and descending gradients. Researchers in music informatics are increasingly using deep learning techniques. ConvNets are already used for chord recognition [14], genre classification [16], music recording [17], instrument detection [18] and music tagging [1], [8] and music tagging at the signal level [9].A ConvNet is suitable when the input data represent multi-dimensional hierarchies."}, {"heading": "D. Evaluation of tagging algorithms", "text": "There are several methods of evaluating tagging algorithms. Since the target is binary to show whether an ith tag is true (yi, 0, 1), ratings metrics can be used for classifications such as \"precision\" and \"callback\" if the prediction is also binary. However, optimal thresholds are an additional challenge and discard information. Instead, the range under Curve Operational Characteristics Receiver (AUC-ROC or simply AUC) is often used as a rating measure. An ROC curve is created by plotting the true positive rate against the false positive rate. As both rates are between [0, 1], the range below the curve also ranges between [0, 1]. However, the effective range of the AUC is [0.5, 1] because a random classification yields 0.5 of AUC if the true positive rate increases at exactly the same rate of false positive rates."}, {"heading": "III. EXPERIMENT SET I - PREPARATION: TAG DATASET ANALYSIS", "text": "In this section, we present the results of experiments analyzing the Million Song dataset. Section III-A deals with interrelations between tags. In Section III-B, we review the basic truth of the dataset to determine the reliability of the research that uses it. We select the MSD because it is the largest public dataset available for training music taggers. It also provides searchable track identifiers for audio signals that allow us to access the audio and validate the tags manually by listening to it. The tags in the MSD are collected using the Last.fm API, which provides access to crowd-sourced music tags. We use the Top 50 tags sorted by popularity (occurrences count) in the dataset. The tags include genres (rock, pop, jazz, funk), epochs (60s - 00s) and moods (sad, happy, cold). The number of clips is 1.37 / Sali.612.633 / 28.3D each set by the Salias / 28.348)."}, {"heading": "A. Tag co-occurrences in the MSD", "text": "This year, it has come to the point where it will only take a few days to get a result."}, {"heading": "IV. EXPERIMENT SET II - TRAINING AND EFFECTIVE PREPROCESSING", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "B. Time-frequency representations", "text": "STFT and Meld Spectrogram were the most popular input representations for deep learning in music. Meld spectrograms provide an efficient and perceptual representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [20], [35], [36]. However, a STFT closer to the original signal and neural networks is able to learn a representation that is more optimal for the task. However, this requires large amounts of training data, as reported in [1], where the use of meld spectrograms exceeds STFT with smaller datasets. Figure 5 shows the AUC values obtained with Meld Spectrogram vs. STFT, while the size of the training data used is varied with logarithmic scaling. Although there are small differences in the AUC values up to 0.007, none of them exceeds the others, especially when enough data is provided."}, {"heading": "C. Log-scaling of magnitudes", "text": "In this section we will discuss how logarithmic scaling of magnitudes, i.e. decibel scaling, affects performance, which is considered standard pre-processing in music data extraction, motivated by the human perception of loudness [34], which has a logarithmic relationship with the physical energy of sound. Although learning a logarithmic function is a trivial task for neural networks, it can be difficult to implicitly learn optimal nonlinear mapping when embedded in a complicated task. Nonlinear mapping has also been shown to affect performance in visual image recognition using neural networks [38]. Figure 6 compares the histograms of time frequency magnets with the standardization of zero-methane units. On the left, a logarithmically compressed melting spectrogram shows an approximate distribution without extreme values."}, {"heading": "D. Analysis of scaling effects and frequency-axis weights", "text": "Finally, we will discuss the effects of manipulating the order of magnitude [39] and [40] transcription. Preliminary experiments suggest that there may be two independent aspects to investigate; i) frequency-axis weights and ii) size scaling of each individual element in the training environment. Our experiment is designed to isolate these two effects. We tested two input representations log-melting spectrogram vs. melting spectrogram, with three frequency-weighting schemes per frequency, A-weighting and bypass, as well as two scaling methods approach of two (from), yielding 2 \u00d7 3 \u00d7 2 = 12 configurations overall. We will summarize the mechanism of each block as follows. \u2022 Pro-frequency stdd: Compute means and standard deviations over time, i.e., per frequency and standardization, and standardize each frequency with these values. The average frequency response becomes flat (even)."}, {"heading": "V. EXPERIMENT SET III - APPLICATION: UTILISING TRAINED NETWORK", "text": "A trained labeling network can provide information that goes beyond the specific task. In this section, the trained weights are used to analyze how the network \"understands\" music content through its label in Section V-A. This analysis also provides a way to detect unidentified relationships between labels and music content."}, {"heading": "A. Analysis of predicted label vectors", "text": "The goal of label vector analysis is to better understand network education and assess its ability to represent domain knowledge, i.e., relationships between music tags that are not explicitly represented in the data. In this part of the study, we use the trained ConvNets described in Section IV-D with the optimal hyperparameters, i.e., melting spectrogram input, decibel scaling, and pro-sample standardization (using bypass as described in Section IV-D. In ConvNet, the base layer has a dense connection from the last convolution layer, i.e. VS The weights are represented as matrix W, RN \u00d7 50, where N is the number of feature maps (N = 32 in our case) and the number of predicted labels 50. After the training, the columns of W can be interpreted as N-dimensional latent vectors, since they combine the information in the networks to make the final prediction."}, {"heading": "VI. CONCLUSIONS", "text": "In this article, we examined several aspects of deep convolutionary neural networks for music marking. We analyzed MSD, the largest data set available for the training of a music marker, from a novel perspective and reported on a study aimed at confirming MSD as the basic truth for this task. We found that, overall, the data set is reliable, despite several sources of interference affecting training and evaluation. We demonstrated that pre-processing can affect performance. We quantified this by the size of the training data required to achieve similar performance. Among several pre-processing techniques tested in this study, only a logarithmic scale of magnitude resulted in a significant improvement. Finally, we used label vectors to analyze the network's ability to explain similarities between semantic tags."}], "references": [{"title": "Automatic tagging using deep convolutional neural networks", "author": ["K. Choi", "G. Fazekas", "M. Sandler"], "venue": "The 17th International Society of Music Information Retrieval Conference, New York, USA. International Society of Music Information Retrieval, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Social tagging and music information retrieval", "author": ["P. Lamere"], "venue": "Journal of new music research, vol. 37, no. 2, pp. 101\u2013114, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic generation of social tags for music recommendation", "author": ["D. Eck", "P. Lamere", "T. Bertin-Mahieux", "S. Green"], "venue": "Advances in neural information processing systems, 2008, pp. 385\u2013392.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic models of musical mood: Comparison between crowd-sourced and curated editorial tags", "author": ["P. Saari", "M. Barthet", "G. Fazekas", "T. Eerola", "M. Sandler"], "venue": "Multimedia and Expo Workshops (ICMEW), 2013 IEEE International Conference on. IEEE, 2013, pp. 1\u20136.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning for music classification and regression tasks", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "https://arxiv.org/abs/1703.09179, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "icml, vol. 96, 1996, pp. 148\u2013156.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Content-based musical similarity computation using the hierarchical dirichlet process.", "author": ["M.D. Hoffman", "D.M. Blei", "P.R. Cook"], "venue": "IS- MIR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-level and multi-scale feature aggregation using pre-trained convolutional neural networks for music auto-tagging", "author": ["J. Lee", "J. Nam"], "venue": "arXiv preprint arXiv:1703.01793, 2017.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"], "venue": "ISMIR 2011: Proceedings of the 12th International Society for Music Information Retrieval Conference, October 24-28, 2011, Miami, Florida. University of Miami, 2011, pp. 591\u2013596.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards musical query-by-semantic-description using the cal500 data set", "author": ["D. Turnbull", "L. Barrington", "D. Torres", "G. Lanckriet"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 439\u2013446.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluation of algorithms using games: The case of music tagging.", "author": ["E. Law", "K. West", "M.I. Mandel", "M. Bay", "J.S. Downie"], "venue": "in ISMIR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Rethinking automatic chord recognition with convolutional neural networks", "author": ["E.J. Humphrey", "J.P. Bello"], "venue": "Machine Learning and Applications, 11th International Conference on, vol. 2. IEEE, 2012, pp. 357\u2013362.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio musical genre classification using convolutional neural networks and pitch and tempo transformations", "author": ["L. Li"], "venue": "2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Musical onset detection with convolutional neural networks", "author": ["J. Schl\u00fcter", "S. B\u00f6ck"], "venue": "6th International Workshop on Machine Learning and Music (MML), Prague, Czech Republic, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep content-based music recommendation", "author": ["A. Van den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2643\u20132651.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional neural networks for predominant instrument recognition in polyphonic music", "author": ["Y. Han", "J. Kim", "K. Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 1, pp. 208\u2013221, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms", "author": ["J. Lee", "J. Park", "K.L. Kim", "J. Nam"], "venue": "arXiv preprint arXiv:1703.01789, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Boundary detection in music structure analysis using convolutional neural networks", "author": ["K. Ullrich", "J. Schl\u00fcter", "T. Grill"], "venue": "Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint beat and downbeat tracking with recurrent neural networks", "author": ["S. B\u00f6ck", "F. Krebs", "G. Widmer"], "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale audio event discovery in one million youtube videos", "author": ["A. Jansen", "D. Ellis", "D. Freedman", "J.F. Gemmeke", "W. Lawrence", "X. Liu"], "venue": "Proceedings of ICASSP, 2017.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Weakly supervised learning", "author": ["L. Torresani"], "venue": "Computer Vision. Springer, 2014, pp. 883\u2013885.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural networks, vol. 4, no. 2, pp. 251\u2013257, 1991.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1991}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade. Springer, 2012, pp. 9\u201348.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "librosa: 0.4.1", "author": ["B. McFee", "M. McVicar", "C. Raffel", "D. Liang", "O. Nieto", "E. Battenberg", "J. Moore", "D. Ellis", "R. YAMAMOTO", "R. Bittner", "D. Repetto", "P. Viktorin", "J.F. Santos", "A. Holovaty"], "venue": "Oct. 2015. [Online]. Available: https://doi.org/10.5281/zenodo.32193", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "2017 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412.6980", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras: Deep learning library for theano and tensorflow", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5590, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "kapre: Keras audio preprocessors", "author": ["K. Choi"], "venue": "GitHub repository: https://github.com/keunwoochoi/kapre, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to the psychology of hearing", "author": ["B.C. Moore"], "venue": "JOURNAL OF  LATEX CLASS FILES,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "A deep bag-of-features model for music auto-tagging", "author": ["J. Nam", "J. Herrera", "K. Lee"], "venue": "arXiv preprint arXiv:1508.04999, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["J. Schluter", "S. Bock"], "venue": "Acoustics, Speech and Signal Processing, IEEE International Conference on. IEEE, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Auditory toolbox", "author": ["M. Slaney"], "venue": "Interval Research Corporation, Tech. Rep, vol. 10, p. 1998, 1998.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Understanding how image quality affects deep neural networks", "author": ["S.F. Dodge", "L.J. Karam"], "venue": "CoRR, vol. abs/1604.04004, 2016. [Online]. Available: http://arxiv.org/abs/1604.04004", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to pinpoint singing voice from weakly labeled examples", "author": ["J. Schl\u00fcter"], "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 44\u201350.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "An end-to-end neural network for polyphonic music transcription", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "arXiv preprint arXiv:1508.01774, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "MUSIC tags are descriptive keywords that convey various types of high-level information about recordings such as mood (sad, angry, happy), genre (jazz, classical) and instrumentation (guitar, strings, vocal, instrumental) [1].", "startOffset": 222, "endOffset": 225}, {"referenceID": 1, "context": "New and rarely accessed tracks however often miss the tags necessary to support them, which leads to wellknown problems in music information management [2].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "Music tag prediction is often called music auto-tagging [3].", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "Tag counts show how listeners label music in the real-world, which is very different from the decision of a limited number of experts (see Section III-A) [4].", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "The first study on automatic music tagging proposed the use of tags to enhance music recommendation [3] for this particular reason.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "In the context of deep learning, tags can particularly be considered a good source task for transfer learning [5], a method of reusing a trained neural network in a related task, after adapting to a smaller and more specific dataset.", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "For example, the first auto-tagging algorithm [3] proposed the use of mid-level audio descriptors such as Mel-Frequency Cepstral Coefficients (MFCCs) and an AdaBoost [6] classifier.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "For example, the first auto-tagging algorithm [3] proposed the use of mid-level audio descriptors such as Mel-Frequency Cepstral Coefficients (MFCCs) and an AdaBoost [6] classifier.", "startOffset": 166, "endOffset": 169}, {"referenceID": 6, "context": "Subsequently, vector quantisation and clustering was proposed in [7] as an alternative to parametric representations.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Convolutional Neural Networks (denoted \u2018ConvNets\u2019 hereafter) have been providing state-of-the-art performance for music tagging in recent works [8], [1], [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": "Convolutional Neural Networks (denoted \u2018ConvNets\u2019 hereafter) have been providing state-of-the-art performance for music tagging in recent works [8], [1], [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 8, "context": "Convolutional Neural Networks (denoted \u2018ConvNets\u2019 hereafter) have been providing state-of-the-art performance for music tagging in recent works [8], [1], [9].", "startOffset": 154, "endOffset": 157}, {"referenceID": 9, "context": "For instance, in the Million Song Dataset (MSD) [10], one of the largest and most commonly used groundtruth sets for music tagging, there are 522,366 tags outnumbering 505,216 tracks.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "To the best of our knowledge, CAL500 [11] is the biggest music tag dataset (500 songs) that is strongly labelled.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Most recent research has been relying on collaboratively created, and therefore weaklylabelled datasets such as MagnaTagATune [12] (5,405 songs) and the MSD [10] containing 505,216 songs if only tagged items are counted.", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "Most recent research has been relying on collaboratively created, and therefore weaklylabelled datasets such as MagnaTagATune [12] (5,405 songs) and the MSD [10] containing 505,216 songs if only tagged items are counted.", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "ConvNets are a special type of neural network introduced in computer vision to simulate the behaviour of the human vision system [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "ConvNets have already been used for chord recognition [14], genre classification [15], onset detection [16], music recommendation [17], instrument recognition [18] and music tagging", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "ConvNets have already been used for chord recognition [14], genre classification [15], onset detection [16], music recommendation [17], instrument recognition [18] and music tagging", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "ConvNets have already been used for chord recognition [14], genre classification [15], onset detection [16], music recommendation [17], instrument recognition [18] and music tagging", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "ConvNets have already been used for chord recognition [14], genre classification [15], onset detection [16], music recommendation [17], instrument recognition [18] and music tagging", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "ConvNets have already been used for chord recognition [14], genre classification [15], onset detection [16], music recommendation [17], instrument recognition [18] and music tagging", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "[1], [8], [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[1], [8], [9].", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "[1], [8], [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "Recently, several works proposed learning 2D representations by applying one dimensional convolution to the raw audio signal [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "Moreover, these approaches have been shown to learn representations that are similar to conventional time-frequency representations that are cheaper to compute [8], [19].", "startOffset": 160, "endOffset": 163}, {"referenceID": 18, "context": "Moreover, these approaches have been shown to learn representations that are similar to conventional time-frequency representations that are cheaper to compute [8], [19].", "startOffset": 165, "endOffset": 169}, {"referenceID": 19, "context": "They provide state-of-the-art performances in several music information retrieval tasks including music segmentation [20], beat detection [21] and tagging [9], as well as in non-music tasks such as acoustic event detection [22].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "They provide state-of-the-art performances in several music information retrieval tasks including music segmentation [20], beat detection [21] and tagging [9], as well as in non-music tasks such as acoustic event detection [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 8, "context": "They provide state-of-the-art performances in several music information retrieval tasks including music segmentation [20], beat detection [21] and tagging [9], as well as in non-music tasks such as acoustic event detection [22].", "startOffset": 155, "endOffset": 158}, {"referenceID": 21, "context": "They provide state-of-the-art performances in several music information retrieval tasks including music segmentation [20], beat detection [21] and tagging [9], as well as in non-music tasks such as acoustic event detection [22].", "startOffset": 223, "endOffset": 227}, {"referenceID": 0, "context": "As both rates range between [0, 1], the area under the curve also ranges between [0, 1].", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "As both rates range between [0, 1], the area under the curve also ranges between [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 1, "context": "4Music by a female vocalist is more often tagged as female vocalists than female vocalist [2]", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "85 [1] and 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "90 [9].", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "This may be because even with such noise, the network is weakly supervised by stochastically correct feedbacks, where the noise is alleviated by a large number of training examples [23].", "startOffset": 181, "endOffset": 185}, {"referenceID": 23, "context": "Although deep neural networks are known to be universal function approximators [24], training efficiency and performance may vary significantly with different training methods as well as generic techniques including preprocessing the input data [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "Although deep neural networks are known to be universal function approximators [24], training efficiency and performance may vary significantly with different training methods as well as generic techniques including preprocessing the input data [25].", "startOffset": 245, "endOffset": 249}, {"referenceID": 25, "context": "The preprocessing is performed using Librosa [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 26, "context": "a prior benchmark [27], where the model we selected was denoted k2c2, indicating 2D kernels and convolution axes.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "Exponential linear unit (ELU) is used as an activation function in all convolutional layers [28].", "startOffset": 92, "endOffset": 96}, {"referenceID": 28, "context": "For the acceleration of stochastic gradient descent, we use adaptive optimisation based on ADAM [29].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "The experiment is implemented in Python with Keras [30]", "startOffset": 51, "endOffset": 55}, {"referenceID": 30, "context": "and Theano [31] as deep learning frameworks and Kapre [32], developed for real-time time-frequency conversion and normalisations on the GPU.", "startOffset": 11, "endOffset": 15}, {"referenceID": 31, "context": "and Theano [31] as deep learning frameworks and Kapre [32], developed for real-time time-frequency conversion and normalisations on the GPU.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "[33], which has been shown to yield a stable training procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [17], [20], [35], [36].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [17], [20], [35], [36].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [17], [20], [35], [36].", "startOffset": 158, "endOffset": 161}, {"referenceID": 16, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [17], [20], [35], [36].", "startOffset": 163, "endOffset": 167}, {"referenceID": 19, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [17], [20], [35], [36].", "startOffset": 169, "endOffset": 173}, {"referenceID": 34, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [17], [20], [35], [36].", "startOffset": 175, "endOffset": 179}, {"referenceID": 35, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [34] and have been shown to perform well in various tasks [1], [8], [17], [20], [35], [36].", "startOffset": 181, "endOffset": 185}, {"referenceID": 0, "context": "This requires large amounts of training data however, as reported in [1] where using melspectrograms outperformed STFT with a smaller dataset.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "This rebuts a previous result in [1] because melspectrograms did not have a clear advantage here even with a small training data size.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "\u2022 STFT in [1]: 6000/129=46.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "3 Hz (512-point FFT with 12 kHz sampling rate) \u2022 Melspectrogram in [1] and our work: 35.", "startOffset": 67, "endOffset": 70}, {"referenceID": 36, "context": "9 Hz for frequency < 1 kHz (96 mel-bins and by [37] and [26])", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "9 Hz for frequency < 1 kHz (96 mel-bins and by [37] and [26])", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "In [1], the frequency resolution of the STFT was lower than that of the melspectrogram to enable comparing them with similar number of frequency bins.", "startOffset": 3, "endOffset": 6}, {"referenceID": 33, "context": "The procedure is motivated by the human perception of loudness [34] which has logarithmic relationship with the physical energy of sound.", "startOffset": 63, "endOffset": 67}, {"referenceID": 37, "context": "A nonlinear mapping was also shown to affect the performance in visual image recognition using neural networks [38].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "This method has been used in the literature for tagging [1], singing voice detection [39] and transcription [40].", "startOffset": 56, "endOffset": 59}, {"referenceID": 38, "context": "This method has been used in the literature for tagging [1], singing voice detection [39] and transcription [40].", "startOffset": 85, "endOffset": 89}, {"referenceID": 39, "context": "This method has been used in the literature for tagging [1], singing voice detection [39] and transcription [40].", "startOffset": 108, "endOffset": 112}, {"referenceID": 33, "context": ", human perception of sound energy [34], which is a function of frequency.", "startOffset": 35, "endOffset": 39}, {"referenceID": 40, "context": "This is due to batch normalization [41] which compensates", "startOffset": 35, "endOffset": 39}], "year": 2017, "abstractText": "Deep neural networks (DNN) have been successfully applied for music classification including music tagging. However, there are several open questions regarding generalisation and best practices in the choice of network architectures, hyperparameters and input representations. In this article, we investigate specific aspects of neural networks to deepen our understanding of their properties. We analyse and (re-)validate a large music tagging dataset to investigate the reliability of training and evaluation. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting and scaling. Using a trained network, we compute label vector similarities which is compared to groundtruth similarity. The results highlight several import aspects of music tagging and neural networks. We show that networks can be effective despite of relatively large error rates in groundtruth datasets. We subsequently show that many commonly used input preprocessing techniques are redundant except magnitude compression. Lastly, the analysis of our trained network provides valuable insight into the relationships between music tags. These results highlight the benefit of using data-driven methods to address automatic music tagging.", "creator": "LaTeX with hyperref package"}}}