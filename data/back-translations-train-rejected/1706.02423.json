{"id": "1706.02423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Seamless Integration and Coordination of Cognitive Skills in Humanoid Robots: A Deep Learning Approach", "abstract": "This study investigates how adequate coordination among the different cognitive processes of a humanoid robot can be developed through end-to-end learning of direct perception of visuomotor stream. We propose a deep dynamic neural network model built on a dynamic vision network, a motor generation network, and a higher-level network. The proposed model was designed to process and to integrate direct perception of dynamic visuomotor patterns in a hierarchical model characterized by different spatial and temporal constraints imposed on each level. We conducted synthetic robotic experiments in which a robot learned to read human's intention through observing the gestures and then to generate the corresponding goal-directed actions. Results verify that the proposed model is able to learn the tutored skills and to generalize them to novel situations. The model showed synergic coordination of perception, action and decision making, and it integrated and coordinated a set of cognitive skills including visual perception, intention reading, attention switching, working memory, action preparation and execution in a seamless manner. Analysis reveals that coherent internal representations emerged at each level of the hierarchy. Higher-level representation reflecting actional intention developed by means of continuous integration of the lower-level visuo-proprioceptive stream.", "histories": [["v1", "Thu, 8 Jun 2017 01:15:00 GMT  (1791kb)", "http://arxiv.org/abs/1706.02423v1", "Accepted in the IEEE Transactions on Cognitive and Developmental Systems (TCDS), 2017"]], "COMMENTS": "Accepted in the IEEE Transactions on Cognitive and Developmental Systems (TCDS), 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.RO", "authors": ["jungsik hwang", "jun tani"], "accepted": false, "id": "1706.02423"}, "pdf": {"name": "1706.02423.pdf", "metadata": {"source": "CRF", "title": "Seamless Integration and Coordination of Cognitive Skills in Humanoid Robots: A Deep Learning Approach", "authors": ["Jungsik Hwang", "Jun Tani"], "emails": ["jungsik.hwang@gmail.com).", "tani1216jp@gmail.com)."], "sections": [{"heading": null, "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "II. RELATED WORKS", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "III. THE DEEP NEURAL NETWORK MODEL", "text": "In this section, we describe the Visuo-Motor Deep Dynamic Neural Network (VMDNN) in detail. The proposed model is designed to process and integrate the direct perception of visual motor patterns in a hierarchical structure characterized by different spatio-temporal constraints imposed on each part of the hierarchy. It has several characteristic features: First, the model can perform low visual motor processing without hand-developed methods for feature extraction using deep learning schemes; second, the model processes dynamic visual motor patterns in a hierarchical structure indispensable for cortical computation [15, 34]; third, perception and action are tightly intertwined within the system, enabling the model to form multimodal representations across sensory modalities; and third, the VMDNN model consists of three types of subnets: (1) MNSTN-Handnet-based subnet (subnet-predynamic) images (subnet-predynamic) and (subnet-predynamic) images."}, {"heading": "A. MSTNN Subnetwork", "text": "In our study, we used Multiple Spatio-Temporal Scales Neural Network (MSTNN) to process dynamic visual images perceived by a robot performing a visual-motor task. MSTNN is an augmented revolutionary neural network (CNN) [35] that uses leaky integrator neural units with different time constants. Although conventional CNN models have demonstrated the ability to process spatial data such as static images, they lack the ability to process spatial-time dynamic patterns. To perform a visual-motor task successfully, the robot must extract both spatial and temporal characteristics latently contained in the sequential observations. Unlike conventional CNN models, which only use spatial constraints, the MSTNN model has shown that it can process both spatial and temporal patterns by imposing multiple spatial-temporal characteristics (Neural 36-local scatio-17)."}, {"heading": "B. MTRNN Subnetwork", "text": "In the current study, we used Multiple Timescales Recurrent Neural Network (MTRNN) to generate robot behavior and control robot attention. MTRNN is a hierarchical neural network model that consists of a multi-continuous neural network with leaky integrator neurons [18]. MTRNN has proven to be superior in modelling the sequential action of the robot by using its temporal hierarchy. To be more precise, the lower level in the MTRNN has a smaller time constant that exhibits fast dynamics, while the higher level has a greater time constant that exhibits slow dynamics. Based on this temporal hierarchy, an MTRNN can learn compositional action sequences, since a meaningful functional hierarchy is created within the system [18, 34, 37]. Consequently, the overall behavior of the robot, including neural awareness, and the number of attention-grabbing situations, as well as a number of different functional hierarchies, is consistent with each other."}, {"heading": "C. PFC Subnetwork", "text": "The PFC layer is a recursive neural network consisting of a series of leakage integrator neurons equipped with recurring loops to process abstract sequential information; the PFC layer receives inputs from both the VS layer in the MSTNN subnetwork and the MS layer in the MTRNN subnetwork, which means that both abstracted visual information (VS) and proprioceptive information (MS) are integrated into the PFC layer; and the PFC layer has a forward link to the MS layer to control the robot's behavior and attention; the PFC layer can be characterized by several key features. First, neurons in the PFC layer are assigned the largest time constant."}, {"heading": "D. Problem Formulation", "text": "Fig. 1 illustrates the structure of the VMDNN model. Input into the model () is an observation of the world in time step t, which can be retrieved from the robot camera at the beginning to the end of the task. Observation is an image at pixel level, which is represented as a matrix H \u00b7 W, where H is a height and W is a width of the image. Behavioural signals of the robot and attention control signals are generated at the output level MO at each time step t. Allow: 1, 2,... denotes the output of the model in time step t, in which n denotes the number of neurons at the output level MO.In the calculation of forward dynamics, the problem is defined in relation to the behavior output and the attention signal () at each time step."}, {"heading": "E. Forward Dynamics for Action Generation", "text": "In fact, it is as if it were a reactionary act, capable of hiding in a position."}, {"heading": "F. Training Phase", "text": "In this context, it should be noted that these two are the best candidates capable of establishing themselves in the region."}, {"heading": "A. Robotic Platform", "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move, to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move and to move."}, {"heading": "B. Network Configuration", "text": "In recent years, it has been shown that people are able to decide for themselves what they want and what they do not want. (...) In recent years, it has been shown that people are able to decide for themselves. (...) In recent years, it has been shown that politics is able to put people's interests at the centre. (...) In recent years, it has been shown that people are able to put people's interests at the centre of society. (...) In recent years, it has been shown that people are able to put people's interests at the centre of society. \"(...)"}, {"heading": "C. Task", "text": "The objective of the work was to capture the object on the screen. Overall, the task was as follows: At the beginning of the work, the robot was placed in the starting position, placing its head on the table on which two objects with long and long arms were placed. During the work, the robot was shown on the screen. (Fig.) The robot proved to be one of the main actors. (Fig.) The robot proved to be one of the main actors. (Fig.) The robot proved to be one of the main actors. (Fig.)"}, {"heading": "A. Generalization Performances", "text": "Table 1 shows the success rate of each individual network state. Each study was classified as \"successful\" when the robot detected and lifted an object. In general, the MSTNN vision with the slow PFC state performed better than other conditions. In this state, the model successfully learned the training experiments (TR = 98.5%) and was able to generalize learned skills under different test conditions. This model was able to generalize the achievement and detection of skills when the objects were randomly located (OBJ = 85.0%), when the gestures of the new subject were shown (SUB = 96.5%), and when the randomly located objects were specified by the OBJ SUB model."}, {"heading": "B. Development of Internal Representation", "text": "In order to solve the problem, it is necessary that people feel able to understand themselves and understand what they are doing. (...) In fact, it is not that people are able to understand themselves. (...) In fact, it is that they are able to understand themselves. (...) In fact, it is that they are able to understand themselves. (...) In fact, it is that they are able to understand themselves. (...) In fact, it is that people are able to understand themselves. (...) In fact, it is that people are able to understand themselves. (...) In order to solve the problem, it is that people are able to understand themselves. (...)"}, {"heading": "VI. DISCUSSION", "text": "During the experiments, we verified several key aspects of the proposed model, which we will discuss in detail in this section."}, {"heading": "A. Self-organized Coordinated Dynamic Structure", "text": "The proposed model has a coordinated dynamic mechanism throughout the network that enables the robot to learn targeted behaviors through the seamless coordination of cognitive abilities. In terms of downward causality, it enabled the model to be dynamic by locating it at the lower level, leading to the emergence of a new dynamic in the minds of humans."}, {"heading": "B. Memory and Pre-planning Capability", "text": "The results indicate that the proposed model is capable of developing and deploying working memories. We found that the robot was able to maintain task-related information at higher levels during the task phases and dynamically combine it with object perceptions. For example, the robot retained the human intention categorized at the beginning of the task and combined it with object perception so that it could reach and capture the target object. Furthermore, the proposed model showed robust performance even when the visual input was completely and unexpectedly locked. This memory capability was achieved both by the time hierarchy of the model and by the recurring connections of the PFC layer. In particular, when the time constants of the higher level were greater than those of the lower level, the model showed the most robust performance under the various circumstances, including experiments with the novel object configurations as well as by the recurring connections of the PFC layer."}, {"heading": "VII. CONCLUSION", "text": "The current study introduced the Visuo-Motor Deep Dynamic Neural Network (VMDNN) model, which can learn to read human intentions and generate corresponding behaviors in robots by coordinating multiple cognitive processes, including visual recognition, attention switching, memory and recall with working memory, action preparation and generation in a seamless way. the simulation study on the model with the iCub simulator showed that the robot could categorize human intentions by observing gestures that preserve context information and perform appropriate targeted actions. Finally, the analysis showed that a synergistic coordination between these cognitive processes can be developed when end-to-end learning of tutored experiences is performed across the entire network, enabling dense interaction between subnetworks. Finally, the aforementioned cognitive mechanism can be developed through the downward triggering of these cognitive mechanisms in relation to scalalalaltime differentiation."}], "references": [{"title": "Towards Deep Developmental Learning", "author": ["O. Sigaud", "A. Droniou"], "venue": "IEEE Transactions on Cognitive and Developmental Systems, vol. 8, pp. 99-114, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep unsupervised network for multimodal perception, representation and classification", "author": ["A. Droniou", "S. Ivaldi", "O. Sigaud"], "venue": "Robotics and Autonomous Systems, vol. 71, pp. 83-98, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A Survey of the Ontogeny of Tool Use: From Sensorimotor Experience to Planning", "author": ["F. Guerin", "N. Kruger", "D. Kraft"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 5, pp. 18-45, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Developmental robotics: a survey", "author": ["M. Lungarella", "G. Metta", "R. Pfeifer", "G. Sandini"], "venue": "Connection Science, vol. 15, pp. 151-190, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Developmental robotics: From babies to robots", "author": ["A. Cangelosi", "M. Schlesinger", "L.B. Smith"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Epigenetic robotics\u2014modelling cognitive development in robotic systems", "author": ["L. Berthouze", "T. Ziemke"], "venue": "Connection Science, vol. 15, pp. 147-150, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Cognitive developmental robotics as a new paradigm for the design of humanoid robots", "author": ["M. Asada", "K.F. MacDorman", "H. Ishiguro", "Y. Kuniyoshi"], "venue": "Robotics and Autonomous Systems, vol. 37, pp. 185-193, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Symbol emergence in robotics: a survey", "author": ["T. Taniguchi", "T. Nagai", "T. Nakamura", "N. Iwahashi", "T. Ogata", "H. Asoh"], "venue": "Advanced Robotics, vol. 30, pp. 706-728, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Self-Organization and Compositionality in Cognitive Brains: A Neurorobotics Study", "author": ["J. Tani"], "venue": "Proceedings of the IEEE, vol. 102, pp. 586-605, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, pp. 1798-1828, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1828}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436-444, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, vol. 61, pp. 85-117, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "The International Journal of Robotics Research, vol. 34, pp. 705-724, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A Deep Learning Neural Network for Number Cognition: A bi-cultural study with the iCub", "author": ["A. Di Nuovo", "V.M. De La Cruz", "A. Cangelosi"], "venue": "2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2015, pp. 320-325.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "From pixels to torques: Policy learning with deep dynamical models", "author": ["N. Wahlstr\u00f6m", "T.B. Sch\u00f6n", "M.P. Deisenroth"], "venue": "arXiv preprint arXiv:1502.02251, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequences", "author": ["M. Jung", "J. Hwang", "J. Tani"], "venue": "PLoS ONE, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment", "author": ["Y. Yamashita", "J. Tani"], "venue": "PLoS Computational Biology, vol. 4, p. e1000220, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Incremental Learning in a 14 DOF Simulated iCub Robot: Modeling Infant Reach/Grasp Development", "author": ["P. Savastano", "S. Nolfi"], "venue": "Biomimetic and Biohybrid Systems: First International Conference, Living Machines 2012, Barcelona, Spain, July 9-12, 2012. Proceedings, T. J. Prescott, N. F. Lepora, A. Mura, and P. F. M. J. Verschure, Eds., ed Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 250-261.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A Robotic Model of Reaching and Grasping Development", "author": ["P. Savastano", "S. Nolfi"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 5, pp. 326-336, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Letting structure emerge: connectionist and dynamical systems approaches to cognition", "author": ["J.L. McClelland", "M.M. Botvinick", "D.C. Noelle", "D.C. Plaut", "T.T. Rogers", "M.S. Seidenberg"], "venue": "Trends in Cognitive Sciences, vol. 14, pp. 348-356, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "The basis of shared intentions in human and robot cognition", "author": ["P.F. Dominey", "F. Warneken"], "venue": "New Ideas in Psychology, vol. 29, pp. 260-274, 12// 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding and sharing intentions: the origins of cultural cognition", "author": ["M. Tomasello", "M. Carpenter", "J. Call", "T. Behne", "H. Moll"], "venue": "Behav Brain Sci, vol. 28, pp. 675-91; discussion 691-735, Oct 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "The Role of Intention in Cognitive Robotics", "author": ["D. Vernon", "S. Thill", "T. Ziemke"], "venue": "Toward Robotic Socially Believable Behaving Systems - Volume I : Modeling Emotions, A. Esposito and C. L. Jain, Eds., ed Cham: Springer International Publishing, 2016, pp. 15-27.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "How infants use vision for grasping objects", "author": ["M.E. McCarty", "R.K. Clifton", "D.H. Ashmead", "P. Lee", "N. Goubet"], "venue": "Child Dev, vol. 72, pp. 973-87, Jul-Aug 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "The effect of visual feedback of the hand on the reaching and retrieval behavior of young infants", "author": ["R.E. Lasky"], "venue": "Child Dev, vol. 48, pp. 112-7, Mar 1977.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1977}, {"title": "Infant grasp learning: a computational model", "author": ["E. Oztop", "N.S. Bradley", "M.A. Arbib"], "venue": "Exp Brain Res, vol. 158, pp. 480-503, Oct 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Self-discovery of motor primitives and learning grasp affordances", "author": ["E. Ugur", "E. \u015eahin", "E. Oztop"], "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012, pp. 3260-3267.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Staged Development of Robot Skills: Behavior Formation, Affordance Learning and Imitation with Motionese", "author": ["E. Ugur", "Y. Nagai", "E. Sahin", "E. Oztop"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 7, pp. 119-139, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Human motion based intent recognition using a deep dynamic neural model", "author": ["Z. Yu", "M. Lee"], "venue": "Robotics and Autonomous Systems, vol. 71, pp. 134-149, 9// 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours", "author": ["L. Pinto", "A. Gupta"], "venue": "arXiv preprint arXiv:1509.06825, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal integration learning of robot behavior using deep neural networks", "author": ["K. Noda", "H. Arie", "Y. Suga", "T. Ogata"], "venue": "Robotics and Autonomous Systems, vol. 62, pp. 721-736, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Development of Compositional and Contextual Communication of Robots by Using the Multiple Timescales Dynamic Neural Network", "author": ["G. Park", "J. Tani"], "venue": "presented at the The 5th Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics, Rhode Island, USA, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Development of hierarchical structures for actions and motor imagery: a constructivist view from synthetic neuro-robotics study", "author": ["R. Nishimoto", "J. Tani"], "venue": "Psychol Res, vol. 73, pp. 545-58, Jul 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097-1105.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple spatio-temporal scales neural network for contextual visual recognition of human actions", "author": ["M. Jung", "J. Hwang", "J. Tani"], "venue": "2014 Joint IEEE International Conferences on Development and Learning and Epigenetic Robotics (ICDL-Epirob), 2014, pp. 235-241.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Neuro-robotics study on integrative learning of proactive visual attention and motor behaviors", "author": ["S. Jeong", "H. Arie", "M. Lee", "J. Tani"], "venue": "Cognitive Neurodynamics, vol. 6, pp. 43-59, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent Slow Feature Analysis for Developing Object Permanence in Robots", "author": ["H. Celikkanat", "E. Sahin", "S. Kalkan"], "venue": "presented at the IROS 2013 Workshop on Neuroscience and Robotics, Tokyo, Japan, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Neurological basis of language and sequential cognition: evidence from simulation, aphasia, and ERP studies", "author": ["P.F. Dominey", "M. Hoen", "J.M. Blanc", "T. Lelekov-Boissard"], "venue": "Brain Lang, vol. 86, pp. 207-25, Aug 2003.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "Beyond Gazing, Pointing, and Reaching: A Survey of Developmental Robotics", "author": ["M. Lungarella", "G. Metta"], "venue": "vol. 101, C. G. Prince, L. Berthouze, H. Kozima, D. Bullock, G. Stojanov, and C. Balkenius, Eds., ed: Lund University Cognitive Studies, 2003, pp. 81-89.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade. vol. 7700, G. Montavon, G. B. Orr, and K.-R. M\u00fcller, Eds., ed: Springer Berlin Heidelberg, 2012, pp. 9-48.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems, vol. 19, p. 153, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "iCub: the design and realization of an open humanoid platform for cognitive and neuroscience research", "author": ["N.G. Tsagarakis", "G. Metta", "G. Sandini", "D. Vernon", "R. Beira", "F. Becchi"], "venue": "Advanced Robotics, vol. 21, pp. 1151-1175, 2007.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "An open-source simulator for cognitive robotics research: the prototype of the iCub humanoid robot simulator", "author": ["V. Tikhanoff", "A. Cangelosi", "P. Fitzpatrick", "G. Metta", "L. Natale", "F. Nori"], "venue": "presented at the Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems, Gaithersburg, Maryland, 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Achieving \"synergy\" in cognitive behavior of humanoids via deep learning of dynamic visuo-motor-attentional coordination", "author": ["J. Hwang", "M. Jung", "N. Madapana", "J. Kim", "M. Choi", "J. Tani"], "venue": "2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids), 2015, pp. 817-824.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, p. 85, 2008.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Networks of the Brain", "author": ["O. Sporns"], "venue": "Cambridge, MA: MIT press,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Activity in the lateral prefrontal cortex reflects multiple steps of future events in action plans", "author": ["H. Mushiake", "N. Saito", "K. Sakamoto", "Y. Itoyama", "J. Tanji"], "venue": "Neuron, vol. 50, pp. 631-41, May 18 2006.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "Linking Objects to Actions: Encoding of Target Object and Grasping Strategy in Primate Ventral Premotor Cortex", "author": ["C.E. Vargas-Irwin", "L. Franquemont", "M.J. Black", "J.P. Donoghue"], "venue": "The Journal of Neuroscience, vol. 35, pp. 10888-10897, 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Convergence and divergence in a neural architecture for recognition and memory", "author": ["K. Meyer", "A. Damasio"], "venue": "Trends Neurosci, vol. 32, pp. 376-82, Jul 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "ECA: An enactivist cognitive architecture based on sensorimotor modeling", "author": ["O.L. Georgeon", "J.B. Marshall", "R. Manzotti"], "venue": "Biologically Inspired Cognitive Architectures, vol. 6, pp. 46-57, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Decoding the activity of grasping neurons recorded from the ventral premotor area F5 of the macaque monkey", "author": ["J. Carpaneto", "M.A. Umilta", "L. Fogassi", "A. Murata", "V. Gallese", "S. Micera"], "venue": "Neuroscience, vol. 188, pp. 80-94, Aug 11 2011.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 1, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 2, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 3, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 4, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 5, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 6, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 7, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 8, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 9, "context": "Deep learning is a fast-growing field in machine learning and artificial intelligence with remarkable advances, such as text recognition, speech recognition, image recognition and many others (See [10-12] for recent reviews of deep learning).", "startOffset": 197, "endOffset": 204}, {"referenceID": 10, "context": "Deep learning is a fast-growing field in machine learning and artificial intelligence with remarkable advances, such as text recognition, speech recognition, image recognition and many others (See [10-12] for recent reviews of deep learning).", "startOffset": 197, "endOffset": 204}, {"referenceID": 11, "context": "Deep learning is a fast-growing field in machine learning and artificial intelligence with remarkable advances, such as text recognition, speech recognition, image recognition and many others (See [10-12] for recent reviews of deep learning).", "startOffset": 197, "endOffset": 204}, {"referenceID": 0, "context": "One of the most important characteristics of deep learning is that deep networks can autonomously extract task-related features in high-dimensional data, such as images and action sequences, without the necessity of hand-engineered feature extraction methods [1, 10, 12].", "startOffset": 259, "endOffset": 270}, {"referenceID": 9, "context": "One of the most important characteristics of deep learning is that deep networks can autonomously extract task-related features in high-dimensional data, such as images and action sequences, without the necessity of hand-engineered feature extraction methods [1, 10, 12].", "startOffset": 259, "endOffset": 270}, {"referenceID": 11, "context": "One of the most important characteristics of deep learning is that deep networks can autonomously extract task-related features in high-dimensional data, such as images and action sequences, without the necessity of hand-engineered feature extraction methods [1, 10, 12].", "startOffset": 259, "endOffset": 270}, {"referenceID": 0, "context": "So, deep learning provides an important tool for robotics, because through deep learning a robot can learn directly from its huge-dimensional sensorimotor data acquired through dynamic interaction with the environment [1].", "startOffset": 218, "endOffset": 221}, {"referenceID": 12, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 13, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 14, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 15, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 0, "context": "Also, Sigaud and Droniou [1] have pointed out that it is still unclear how higher-level representations can be built by stacking several networks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 16, "context": "The VMDNN is composed of three different types of subnetwork: the Multiple Spatio-Temporal scales Neural Network (MSTNN) [17], the Multiple Timescales Recurrent Neural Network (MTRNN) [18] and the PFC (Prefrontal Cortex) subnetworks.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "The VMDNN is composed of three different types of subnetwork: the Multiple Spatio-Temporal scales Neural Network (MSTNN) [17], the Multiple Timescales Recurrent Neural Network (MTRNN) [18] and the PFC (Prefrontal Cortex) subnetworks.", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "The MSTNN has demonstrated an ability to recognize dynamic visual scenes [17], and the MTRNN to learn compositional actions [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "The MSTNN has demonstrated an ability to recognize dynamic visual scenes [17], and the MTRNN to learn compositional actions [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "This approach is based on the previous studies which emphasized the importance of perception-action coupling in robotic manipulation [13] as well as in developmental robotics [40].", "startOffset": 133, "endOffset": 137}, {"referenceID": 39, "context": "This approach is based on the previous studies which emphasized the importance of perception-action coupling in robotic manipulation [13] as well as in developmental robotics [40].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Here, it is worth noting that artificial neural networks are meant to model the essential features of the nervous system, not its detailed implementation [19-21].", "startOffset": 154, "endOffset": 161}, {"referenceID": 19, "context": "Here, it is worth noting that artificial neural networks are meant to model the essential features of the nervous system, not its detailed implementation [19-21].", "startOffset": 154, "endOffset": 161}, {"referenceID": 20, "context": "Here, it is worth noting that artificial neural networks are meant to model the essential features of the nervous system, not its detailed implementation [19-21].", "startOffset": 154, "endOffset": 161}, {"referenceID": 4, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 21, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 22, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 23, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 4, "context": "In addition, reaching and grasping are fundamental skills that have significant influences on the development of perceptual and cognitive abilities [5].", "startOffset": 148, "endOffset": 151}, {"referenceID": 24, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 81, "endOffset": 89}, {"referenceID": 25, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 81, "endOffset": 89}, {"referenceID": 18, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 19, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 26, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 27, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 28, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 13, "context": "In a robotic context, they require robust perception and action systems as well as simultaneous coordination of a set of cognitive skills, making hand-designing features demanding and time-consuming [14].", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "Due to the remarkable success of deep learning in various fields, recent studies have attempted to employ deep learning in the field of robotics (See [1] for a recent review).", "startOffset": 150, "endOffset": 153}, {"referenceID": 14, "context": "[15] employed a deep neural network architecture in order to study number cognition in a humanoid robot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] introduced a deep network architecture which could learn from different sensory modalities, including vision, audition and proprioception.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "Yu and Lee [30] employed a deep learning approach on reading human intention.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "[14] proposed a two-stage cascaded detection system to detect robotic grasps in an RGB-D view of a scene and conducted experiments on different robotic platforms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Pinto and Gupta [31] also addressed the problem of detecting robotic grasps, adopting a convolutional neural network (CNN) model to predict grasp location and angle.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "[16], addressed the pixels-to-torques problem by introducing a reinforcement learning algorithm that enabled their agent to learn control policy from pixel information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] introduced a deep auto-encoder-based computational framework designed to integrate sensorimotor data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed a deep neural network model which learned a control policy that linked raw image percepts to motor torques of the robot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Park and Tani [33] investigated how a robot could infer the underlying intention of human gestures and generate corresponding behaviors of a humanoid robot.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 176, "endOffset": 184}, {"referenceID": 32, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 176, "endOffset": 184}, {"referenceID": 31, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 12, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "Second, the model processes dynamic visuomotor patterns in a hierarchical structure essential to cortical computation [15, 34].", "startOffset": 118, "endOffset": 126}, {"referenceID": 33, "context": "Second, the model processes dynamic visuomotor patterns in a hierarchical structure essential to cortical computation [15, 34].", "startOffset": 118, "endOffset": 126}, {"referenceID": 34, "context": "The MSTNN is an extended Convolutional Neural Network (CNN) [35] employing leaky integrator neural units with different time constants [17, 36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "The MSTNN is an extended Convolutional Neural Network (CNN) [35] employing leaky integrator neural units with different time constants [17, 36].", "startOffset": 135, "endOffset": 143}, {"referenceID": 35, "context": "The MSTNN is an extended Convolutional Neural Network (CNN) [35] employing leaky integrator neural units with different time constants [17, 36].", "startOffset": 135, "endOffset": 143}, {"referenceID": 16, "context": "Unlike conventional CNN models that utilize spatial constraints only, the MSTNN model has been shown that it can process both spatial and temporal patterns by imposing multiple spatio-temporal scales constraints on local neural activity [17, 36].", "startOffset": 237, "endOffset": 245}, {"referenceID": 35, "context": "Unlike conventional CNN models that utilize spatial constraints only, the MSTNN model has been shown that it can process both spatial and temporal patterns by imposing multiple spatio-temporal scales constraints on local neural activity [17, 36].", "startOffset": 237, "endOffset": 245}, {"referenceID": 17, "context": "The MTRNN is a hierarchical neural network model consisting of a multiple continuous time recurrent neural networks with leaky integrator neurons [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Due to this temporal hierarchy, an MTRNN can learn compositional action sequences as a meaningful functional hierarchy emerges within the system [18, 34, 37].", "startOffset": 145, "endOffset": 157}, {"referenceID": 33, "context": "Due to this temporal hierarchy, an MTRNN can learn compositional action sequences as a meaningful functional hierarchy emerges within the system [18, 34, 37].", "startOffset": 145, "endOffset": 157}, {"referenceID": 36, "context": "Due to this temporal hierarchy, an MTRNN can learn compositional action sequences as a meaningful functional hierarchy emerges within the system [18, 34, 37].", "startOffset": 145, "endOffset": 157}, {"referenceID": 17, "context": "Consequently, the entire behavior of the robot including reaching and grasping as well as visual attention control can be decomposed into a set of primitives for their flexible recombination adapting to various situations [18].", "startOffset": 222, "endOffset": 226}, {"referenceID": 37, "context": "As a result, the PFC subnetwork exhibits the slowest-scale dynamics and this enables the PFC subnetwork to carry more information about a situation [38].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "recurrent connections which are essential to handle dynamic sequential data [1, 9, 39].", "startOffset": 76, "endOffset": 86}, {"referenceID": 8, "context": "recurrent connections which are essential to handle dynamic sequential data [1, 9, 39].", "startOffset": 76, "endOffset": 86}, {"referenceID": 38, "context": "recurrent connections which are essential to handle dynamic sequential data [1, 9, 39].", "startOffset": 76, "endOffset": 86}, {"referenceID": 39, "context": "Lungarella and Metta [40] argued that perception and action are not separated but tightly coupled, with this coupling is gradually getting refined during developmental process.", "startOffset": 21, "endOffset": 25}, {"referenceID": 40, "context": "Please note that the hyperbolic tangent recommended in [41] was used as an activation function to enhance convergence.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "Studies have demonstrated that pre-training is an efficient method for initializing network parameters [10, 13, 43].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "Studies have demonstrated that pre-training is an efficient method for initializing network parameters [10, 13, 43].", "startOffset": 103, "endOffset": 115}, {"referenceID": 41, "context": "Studies have demonstrated that pre-training is an efficient method for initializing network parameters [10, 13, 43].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "The pre-training method in our study is similar to that of [13] in which the visual part of the model was pre-trained prior to the end-to-end learning phase of the experiment.", "startOffset": 59, "endOffset": 63}, {"referenceID": 35, "context": "In this condition, the system operates as an MSTNN model, and it was trained as a typical classifier using BPTT as described in [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 34, "context": "[35] with the weight decay rate of 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "iCub [44] is a child-like humanoid robot consisting of 53 degrees of freedom (DOFs) distributed in the body.", "startOffset": 5, "endOffset": 9}, {"referenceID": 43, "context": "We used a simulation of the iCub [45] in our experiments.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "The iCub simulator accurately models the actual robot\u2019s physical interaction with the environment, making it an adequate research platform for studying developmental robotics [15, 45].", "startOffset": 175, "endOffset": 183}, {"referenceID": 43, "context": "The iCub simulator accurately models the actual robot\u2019s physical interaction with the environment, making it an adequate research platform for studying developmental robotics [15, 45].", "startOffset": 175, "endOffset": 183}, {"referenceID": 43, "context": "Then, the interfacing program operated the robot based on the VMDNN model\u2019s outputs using the motor controller provided in the iCub software package [45].", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "In addition, the network output the level of extension or flexion of finger joints in order to control grasping similar to [20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "In our experiments, the VMDNN model also controlled visual attention which is essential to generate adequate robot\u2019s behavior [37].", "startOffset": 126, "endOffset": 130}, {"referenceID": 36, "context": "[37] demonstrated that MTRNN could seamlessly coordinate visual attention and motor behaviors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "The structure of the VMDNN model used in this study was found empirically in our preliminary experiments [46].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Note that the structure of the VMDNN model including the number of layers in each subnetwork can be extended depending on the complexity of the task since the \u2018deeper\u2019 structure can enhance learning of complex functions in visuomotor patterns [11].", "startOffset": 243, "endOffset": 247}, {"referenceID": 16, "context": "For instance, more number of layers in the MSTNN subnetwork can be employed to process more complex visual images [17].", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Similarly, more complex robot\u2019s behavior can be learned by employing more number of layers in the MTRNN subnetwork as reported in [33].", "startOffset": 130, "endOffset": 134}, {"referenceID": 44, "context": "The proper values for the time constant at each level of the model were found heuristically in our preliminary study [46].", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "This result shows the importance of the internal contextual dynamics of the proposed model and highlights a difference between the proposed model and the previous study [13] which was prone to occlusion due to the lack of capability of keeping memory.", "startOffset": 169, "endOffset": 173}, {"referenceID": 45, "context": "In order to reveal the model\u2019s self-organized internal representation, we analyzed neural activation during training trials using the t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction algorithm [47].", "startOffset": 221, "endOffset": 225}, {"referenceID": 46, "context": "Sporns [48] argues that cognitive functions develop in human brains through anatomical spatio-temporal constraints including connectivity and timescales among local regions.", "startOffset": 7, "endOffset": 11}, {"referenceID": 47, "context": "This result is analogous to findings in [49] that higher-level task-related information was encoded in the PFC and the lower-level arm movements were encoded in the primary motor cortex of monkeys.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "This finding is in line with the previous studies [13, 40] in which the importance of coupling perception and action was emphasized.", "startOffset": 50, "endOffset": 58}, {"referenceID": 39, "context": "This finding is in line with the previous studies [13, 40] in which the importance of coupling perception and action was emphasized.", "startOffset": 50, "endOffset": 58}, {"referenceID": 48, "context": "This dynamic transformation of visual information into behavioral information was also observed in the experiments with macaque monkey\u2019s brains [50].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 3, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 49, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 50, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 8, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 16, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 17, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 32, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 33, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 35, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 36, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 44, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 12, "context": "This internal contextual dynamics of the proposed model highlights a key difference between the proposed model and the previous study [13] which lacked capability for keeping memory.", "startOffset": 134, "endOffset": 138}, {"referenceID": 51, "context": "This result is analogous to findings in [53] which reported that F5 neurons in the brain of the macaque monkey encoded grip-specific information even when no movement was intended.", "startOffset": 40, "endOffset": 44}], "year": 2017, "abstractText": "This study investigates how adequate coordination among the different cognitive processes of a humanoid robot can be developed through end-to-end learning of direct perception of visuomotor stream. We propose a deep dynamic neural network model built on a dynamic vision network, a motor generation network, and a higher-level network. The proposed model was designed to process and to integrate direct perception of dynamic visuomotor patterns in a hierarchical model characterized by different spatial and temporal constraints imposed on each level. We conducted synthetic robotic experiments in which a robot learned to read human's intention through observing the gestures and then to generate the corresponding goal-directed actions. Results verify that the proposed model is able to learn the tutored skills and to generalize them to novel situations. The model showed synergic coordination of perception, action and decision making, and it integrated and coordinated a set of cognitive skills including visual perception, intention reading, attention switching, working memory, action preparation and execution in a seamless manner. Analysis reveals that coherent internal representations emerged at each level of the hierarchy. Higher-level representation reflecting actional intention developed by means of continuous integration of the lower-level visuo-proprioceptive stream.", "creator": "Microsoft\u00ae Word 2010"}}}