{"id": "1706.02490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Where is my forearm? Clustering of body parts from simultaneous tactile and linguistic input using sequential mapping", "abstract": "Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning). In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences. In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation. For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations. A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso). With the goal at arriving at the correct \"body categories\", we demonstrate how a sequential mapping algorithm outperforms one-step mapping. In addition, the effect of data set size and noise in the linguistic input are studied.", "histories": [["v1", "Thu, 8 Jun 2017 09:31:42 GMT  (622kb,D)", "http://arxiv.org/abs/1706.02490v1", "pp. 155-162"]], "COMMENTS": "pp. 155-162", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG cs.RO", "authors": ["karla stepanova", "matej hoffmann", "zdenek straka", "frederico b klein", "angelo cangelosi", "michal vavrecka"], "accepted": false, "id": "1706.02490"}, "pdf": {"name": "1706.02490.pdf", "metadata": {"source": "CRF", "title": "Where is my forearm? Clustering of body parts from simultaneous tactile and linguistic input using sequential mapping", "authors": ["Karla \u0160t\u011bp\u00e1nov\u00e1", "Mat\u011bj Hoffmann", "Zden\u011bk Straka", "Frederico B. Klein", "Angelo Cangelosi", "Michal Vavre\u010dka"], "emails": [], "sections": [{"heading": null, "text": "Humans and animals are constantly exposed to an uninterrupted flow of sensory information from different modalities, while at the same time forming more compressed representations such as concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between sensorimotor concepts and linguistic elements needs to be established. There is evidence that children learn language by simply defining possible meanings based on multiple exposure to utterances in different contexts (inter-situational learning). In existing models, mapping between modalities usually takes place in a single step, using direct reference frequencies and meaning cooperations.In this essay, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm along with a publicly available Matlab implementation. To demonstrate, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body mapping, \"a hand mapping algorithm that we listen to the human subcategories while listening to the robot."}, {"heading": "1 Introduction", "text": "In fact, it is not that it is a purely formal matter, as it is in most cases. It is rather that it is a purely formal matter. It is not that it is a purely formal matter. It is rather that it is a purely formal matter. It is not that it is a purely formal matter. It is that it is a purely formal matter. It is that it is a purely formal matter. It is that it is that it is a purely formal matter. It is not that it is a formal matter. It is that it is that it is a formal matter."}, {"heading": "2 Materials and Methods", "text": "This section introduces the inputs and their pre-processing channels: tactile input (Section 2.1) and linguistic input (Section 2.2). A total of 9 body parts of the right half of the upper body of the robot were stimulated: upper body / chest, upper arm, forearm, palm and 5 fingertips. Tactile stimulation coincided with an expression of the body part. Afterwards, the single-stage and sequential mapping algorithms (Section 2.3.1 and 2.4) are presented and a description of the evaluation (Section 2.5)."}, {"heading": "2.1 Tactile inputs and processing", "text": "In order to generate tactile stimulation in relation to different body parts, we built on our previous work on the humanoid robot iCub, in particular on the \"tactile homunculus\" (Hoffmann et al., 2017) - a primary representation of the artificially sensitive skin that covers the robot (see Figure 1 - one half of the upper body of the robot). In the current work, the skin was no longer physically stimulated, but the activations were emulated and then transmitted to the \"homunculus,\" as described below."}, {"heading": "2.1.1 Emulated tactile input", "text": "We developed a software module YARP (Metta et al., 2006) to generate virtual skin contacts1. A skin part was randomly selected and then stimulated; the number of pressure sensitive elements (henceforth taxis) for different skin parts was 440 for the torso, 380 for the upper arm, 230 for the forearm, and 104 for the hand (44 for the palm and 5 x 12 for the fingertips) - a total of 1154 taxels. After the skin part was randomly selected, a small region was also randomly selected within this part for tactile stimulation - 10 taxels at a time, corresponding to the triangular modules that make up the skin. For the hand, the situation was slightly different: the entire hand was treated as a skin part. Then, within the hand, a random selection was made between 5 subregions on the palm (8 to 10 taxels) and 5 fingertips (12 taxels each), with the entire skin part treated virtually for approximately 2000 seconds."}, {"heading": "2.1.2 First layer \u2013 \u201ctactile homunculus\u201d", "text": "The input layer of the \"tactile homunculus\" (Hoffmann et al., 2017) consists of a vector, a (t), activating 1154 taxels at the time t - the output of the previous section - with binary values (1 when a taxel is stimulated, otherwise 0).The output layer then forms a 7 x 24 (a total of 168 \"neurons\") grid - see Figure 1 B. This layer is a compressed representation of the skin surface - the receptive fields of the neurons (the parts of the skin to which they react) are schematically color coded. However, this code (and the \"clustering\") is not available as part of the tactile input. The output layer is represented as a single vector x (t) = [x1 (t),..., x168 (t)].The activation of the output neurons, xi (t), is presented as point products of the weight vector corresponding to the neuron the output (as follows the activation):"}, {"heading": "2.1.3 Second layer \u2013 GMM", "text": "The output of the first layer, the vector x (t) (168 elements, continuously evaluated), serves as an input into the second tactile processing layer. This layer aims to cluster individual body parts and to represent them as abstract models. Subsequently, the resulting models T j are mapped in the multimodal layer to clusters in the language layer. To process the results from the first layer, we used a Gaussian mixing model (GMM), which is a convex mixture of the D-dimensional Gaussian densities l (x | \u03b8 j). In this case, each tactile model T j is described by a series of parameters. The posterior probabilities p (2001 j | x) are calculated as follows: p (2001 j | x) = J \u00b2 j = 1rkj l (x | 2001 j), (2) l (x | 2001 j)."}, {"heading": "2.2 Linguistic inputs and processing", "text": "In our case, where we have 9 different body parts, these are \"trunk,\" \"upper arm,\" \"forearm,\" \"palm,\" \"little finger,\" \"ring finger,\" \"middle finger,\" \"index finger\" and \"thumb.\" Linguistic and tactile input is processed simultaneously. We conducted experiments with spoken speech - one-word expressions uttered by a native speaker. To process this data, we used the CMU-Sphinx (a flexible model-based open source speech recognition system by Markov) (Lamere et al., 2003) and achieved 100% accuracy of word recognition. Word forms are extracted from audio input and compared with pre-learned speech models using the p (wnt | Li) protocol scale of audio matching. Based on this data, subordinate probabilities can be calculated."}, {"heading": "2.3 Cross-situational learning", "text": "One possible way to establish a mapping between sensorimotor concepts and linguistic elements is to use frequencies of reference and meaning co-events, i.e. those with the highest co-event are mapped together (Smith et al., 2006; Xu and Tenenbaum, 2007). This method is commonly referred to as cross-situational learning and presupposes the availability of the ideal associative learner who can track and store all co-events in all studies, memorizing and representing the word-object co-event matrix of input internally, allowing the learner to then select the most strongly associated speaker (Yu and Smith, 2012)."}, {"heading": "2.3.1 One-step mapping", "text": "The simplest one-step word-to-speaker learning algorithm accumulates only word-to-word pairs. This can be considered a Hebrew learning: The connection between a word and an object is strengthened when the pair appears together in an attempt. To expand this basic idea, we can also enable forgetting by introducing a parameter \u03b7 that can capture memory decline (Yu and Smith, 2012). Suppose we observe an object font in each attempt and hear a corresponding word wnt (Nt possible associations), we can describe the actualization of the strength of the association between word model L (i) and object - in our case tactile model T (j) - as follows: A (i, j) = R \u2211 t = 1\u03b7 (t) Nt; Indian word wnt (wnt, i). (o n t, j)."}, {"heading": "2.4 Sequential mapping algorithm", "text": "In order to capture the dynamic competition between the models, we extend the basic single-stage mapping algorithm for cross-situational learning by sequentially adding inhibitory links. Inhibitory mechanisms and situational-temporal dynamics have already been partially incorporated into the model of cross-situational learning proposed by McMurray et al. (2012). Although our model has some similarities to the model proposed by McMurray, it is based on different computational mechanisms. Once a reliable mapping between a language and tactile model has been found, inhibitory links between this tactile model and all other language models are added. Thanks to this mechanism, the principle of mutual exclusivity (the fact that children prefer mapping when the object has only one label for multiple labels (Markman, 1990) is guaranteed. Mapping between tactile models T j and language models L j is found using the following iterative procedure (1. Tactile and language data are assigned)."}, {"heading": "2.5 Evaluation", "text": "The accuracy of the learned assignment is calculated as follows: We cluster output activations from the tactile homunculus and assign each data point to the most likely cluster. Then, we find indices m (i) for all clusters as defined in Equation 5 for one-step assignment and Equation 6 for sequential assignment. Based on this assignment, we can assign each data point to the language label. Then, these language labels are compared with the basic truth (the name of the body part corresponds to the language label prior to the application of noise). Accuracy is then calculated as: acc = T P / N (7), where T P (true positive) is the number of correctly assigned data points and N is the number of all data points."}, {"heading": "3 Results", "text": "We investigated the performance of steps compared to sequential mapping algorithms in terms of the ability to group individual body parts from simultaneous tactile and linguistic input, i.e. all skin regions of the same body part should \"learn\" that they belong together (such as the forearm) thanks to the simultaneous occurrence of body part labels. Furthermore, the effects of the size of data sets and noise levels in the linguistic domain are investigated (Section 3.1). A detailed analysis of the mapping accuracy of individual body parts and a rear projection on the tactile homunculus are shown in Sections 3.2 and 3.3 respectively."}, {"heading": "3.1 Comparison of accuracy of one-step mapping to sequential mapping", "text": "The performance of the single-stage and sequential mapping algorithms is illustrated in Fig. 2. Comparison is made for different dataset sizes (i.e. for 6 different datasets with a number of data points from 64 to 63806) and noise levels. As you can see, the accuracy of sequential mapping remains very stable and exceeds the single-stage mapping for all values of noise (in the linguistic range) and all dataset sizes. For smaller datasets, we see a steeper decline in accuracy with increasing noise in the voice data."}, {"heading": "3.2 Accuracy of mapping for individual body parts", "text": "The accuracy calculated in the previous section and in Fig. 2 is a general accuracy and we do not take into account the number of data points per individual body part. To examine the performance more closely, we also focused on the accuracy of the sequential mapping of individual body parts. The results for the data set with 3190 and 638 data points, respectively, can be seen in Fig. 3 Upper and Bottom. Accuracy for all body parts decreases with increasing noise in voice input. The accuracy for fingers is significantly lower - this is due to the lower number of samples per finger (see Section 2.1.1). Comparing the upper and lower panels in Fig. 3, a lower performance is shown with greater variance, especially for the fingers."}, {"heading": "3.3 Projecting results of sequential mapping back onto homunculus", "text": "Once the tactile data of homunculus are bundled and these clusters are assigned to suitable language clusters (representing the expressions of body parts), we can project these labels back onto the original tactile homunculus. Considering that xi (t) activations of Neuron i are in the homunculus, D is the entire dataset consisting of vectors of homunculus activations for each data point, and finger label (d) is the linguistic designation assigned to a data point d, based on the sequential mapping procedure described in section 2.4, we can project the results of sequential mapping on the homunculus in the following way: Firstly, we calculate the strength of the activation of each finger i for a given language designation as follows: nki = \u2022 x (t) \u2022 Dk xi (t), i (1,.,.,.,.,., 168), by including the finger categories, finger, finger fingers, finger fingers, finger fingers, fingers, fingers, fingers, fingers, fingers, fingers, (many), (8, finger, finger, fingers, fingers, fingers, fingers, fingers, terse, fingers, fingers, fingers, fingers, fingers, fingers, fingers, (many)."}, {"heading": "4 Discussion and Conclusion", "text": "In order to solve the problem, we must abide by the rules we have set ourselves."}, {"heading": "5 Acknowledgement", "text": "K.S. and M.H. were supported by the Czech Science Foundation in the framework of the project GA17-15697Y. M.H. was additionally supported by a Marie Curie Intra European Fellowship (iCub Body Scheme 625727) within the 7th Framework Programme of the European Community. Z.S. was supported by the funding agency of the CTU Prague project SGS16 / 161 / OHK3 / 2T / 13. M.V. was supported by the European research project TRADR, which was funded by the EU FP7 Programme, ICT: Cognitive Systems, Interaction, Robotics (project No. 609763)."}], "references": [{"title": "Putting words in perspective", "author": ["A.M. Borghi", "A.M. Glenberg", "M.P. Kaschak"], "venue": "Memory & Cognition,", "citeRegEx": "Borghi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Borghi et al\\.", "year": 2004}, {"title": "Tropicals: A computational embodied", "author": ["D. Caligiore", "A.M. Borghi", "D. Parisi", "G. Baldassarre"], "venue": null, "citeRegEx": "Caligiore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Caligiore et al\\.", "year": 2010}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the royal statistical society. Series B (methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Sensory disturbances from cerebral lesions", "author": ["H. Head", "H.G. Holmes"], "venue": null, "citeRegEx": "Head and Holmes,? \\Q1911\\E", "shortCiteRegEx": "Head and Holmes", "year": 1911}, {"title": "The encoding of proprioceptive inputs in the brain: knowns and unknowns from a robotic perspective", "author": ["M. Hoffmann", "N. Bednarova"], "venue": "In Kognice a ume\u030cly\u0301 z\u030civot XVI [Cognition and Artificial", "citeRegEx": "Hoffmann and Bednarova,? \\Q2016\\E", "shortCiteRegEx": "Hoffmann and Bednarova", "year": 2016}, {"title": "Body schema in robotics: A review", "author": ["M. Hoffmann", "H. Marques", "A. Hernandez Arieta", "H. Sumioka", "M. Lungarella", "R. Pfeifer"], "venue": "Autonomous Mental Development, IEEE Transactions", "citeRegEx": "Hoffmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2010}, {"title": "Robotic homunculus: Learning of artificial skin representation in a humanoid robot motivated by primary somatosensory cortex", "author": ["M. Hoffmann", "Z. Straka", "I. Farkas", "M. Vavrecka", "G. Metta"], "venue": "IEEE Transactions on Cognitive and Developmental Sys-", "citeRegEx": "Hoffmann et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2017}, {"title": "The cmu sphinx-4 speech recognition system", "author": ["P. Lamere", "P. Kwok", "E. Gouvea", "B. Raj", "R. Singh", "W. Walker", "M. Warmuth", "P. Wolf"], "venue": "In IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP 2003), Hong Kong,", "citeRegEx": "Lamere et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lamere et al\\.", "year": 2003}, {"title": "Words for parts of the body. Words and the mind: How words capture human experience", "author": ["A. Majid"], "venue": null, "citeRegEx": "Majid,? \\Q2010\\E", "shortCiteRegEx": "Majid", "year": 2010}, {"title": "Constraints children place on word meanings", "author": ["E.M. Markman"], "venue": "Cognitive Science,", "citeRegEx": "Markman,? \\Q1990\\E", "shortCiteRegEx": "Markman", "year": 1990}, {"title": "Word learning emerges from the interaction of online referent selection and slow associative learning", "author": ["B. McMurray", "J.S. Horst", "L.K. Samuelson"], "venue": "Psychological review,", "citeRegEx": "McMurray et al\\.,? \\Q2012\\E", "shortCiteRegEx": "McMurray et al\\.", "year": 2012}, {"title": "Yarp: yet another robot platform", "author": ["G. Metta", "P. Fitzpatrick", "L. Natale"], "venue": "International Journal on Advanced Robotics Systems,", "citeRegEx": "Metta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metta et al\\.", "year": 2006}, {"title": "Bayesian body schema estimation using tactile information obtained through coordinated random movements", "author": ["T. Mimura", "Y. Hagiwara", "T. Taniguchi", "T. Inamura"], "venue": "Advanced Robotics,", "citeRegEx": "Mimura et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mimura et al\\.", "year": 2017}, {"title": "Hierarchical action learning by instruction through interactive grounding of body parts and proto-actions", "author": ["M. Petit", "Y. Demiris"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Petit and Demiris,? \\Q2016\\E", "shortCiteRegEx": "Petit and Demiris", "year": 2016}, {"title": "Exploration behaviors, body representations, and simulation processes for the development of cognition in artificial agents", "author": ["G. Schillaci", "V.V. Hafner", "B. Lara"], "venue": "Frontiers in Robotics and AI,", "citeRegEx": "Schillaci et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schillaci et al\\.", "year": 2016}, {"title": "Evidence for multiple, distinct representations of the human body", "author": ["J. Schwoebel", "H.B. Coslett"], "venue": "Journal of cognitive neuroscience,", "citeRegEx": "Schwoebel and Coslett,? \\Q2005\\E", "shortCiteRegEx": "Schwoebel and Coslett", "year": 2005}, {"title": "Cross-situational learning: a mathematical approach", "author": ["K. Smith", "A.D. Smith", "R.A. Blythe", "P. Vogt"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Smith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2006}, {"title": "Estimating number of components in gaussian mixture model using combination of greedy and merging algorithm", "author": ["K. \u0160tep\u00e1nov\u00e1", "M. Vavre\u010dka"], "venue": "Pattern Analysis and Applications,", "citeRegEx": "\u0160tep\u00e1nov\u00e1 and Vavre\u010dka,? \\Q2016\\E", "shortCiteRegEx": "\u0160tep\u00e1nov\u00e1 and Vavre\u010dka", "year": 2016}, {"title": "Segmenting the body into parts: evidence from biases in tactile perception", "author": ["Vignemont", "d. F", "A. Majid", "C. Jola", "P. Haggard"], "venue": null, "citeRegEx": "Vignemont et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vignemont et al\\.", "year": 2009}, {"title": "Word learning as bayesian inference", "author": ["F. Xu", "J.B. Tenenbaum"], "venue": "Psychological review,", "citeRegEx": "Xu and Tenenbaum,? \\Q2007\\E", "shortCiteRegEx": "Xu and Tenenbaum", "year": 2007}, {"title": "Modeling crosssituational word\u2013referent learning: Prior questions", "author": ["C. Yu", "L.B. Smith"], "venue": "Psychological review,", "citeRegEx": "Yu and Smith,? \\Q2012\\E", "shortCiteRegEx": "Yu and Smith", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Spurred by the account of Head and Holmes (1911) and their proposal of superficial and postural schema, a number of different concepts has been proposed since: body schema, body image, and corporeal schema being only some of them.", "startOffset": 26, "endOffset": 49}, {"referenceID": 3, "context": "Spurred by the account of Head and Holmes (1911) and their proposal of superficial and postural schema, a number of different concepts has been proposed since: body schema, body image, and corporeal schema being only some of them. Body schema is usually thought of as more \u201clow-level\u201d, sensorimotor representation of the body used for action. Body image is an umbrella term uniting higher level representations, for perception more than for action, and accessible to consciousness. Schwoebel and Coslett (2005) amassed evidence for distinguishing between three types of body representations: body schema, body structural description, and body semantics\u2014constituting a kind of hierarchy.", "startOffset": 26, "endOffset": 511}, {"referenceID": 18, "context": "Vignemont et al. (2009) focused on how body segmentation between hand and arm could appear based on a combined tactile and visual perception.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "Interestingly, recent research (Majid, 2010) showed that there are some cross-linguistic variabilities in naming body parts and this may in turn override or influence the \u201cbottom-up\u201d multimodal (nonlinguistic) body part categorization.", "startOffset": 31, "endOffset": 44}, {"referenceID": 5, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots.", "startOffset": 67, "endOffset": 114}, {"referenceID": 14, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots.", "startOffset": 67, "endOffset": 114}, {"referenceID": 6, "context": "Our own work on the iCub humanoid robot has thus far focused on learning primary representations\u2014tactile (Hoffmann et al., 2017) and proprioceptive (Hoffmann and Bednarova, 2016).", "startOffset": 105, "endOffset": 128}, {"referenceID": 4, "context": ", 2017) and proprioceptive (Hoffmann and Bednarova, 2016).", "startOffset": 27, "endOffset": 57}, {"referenceID": 4, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots. Petit and Demiris (2016) developed an algorithm for the iCub humanoid robot to associate labels for body parts and later proto-actions with their embodied counterparts.", "startOffset": 68, "endOffset": 178}, {"referenceID": 4, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots. Petit and Demiris (2016) developed an algorithm for the iCub humanoid robot to associate labels for body parts and later proto-actions with their embodied counterparts. These could then be recombined in a hierarchical fashion (e.g., \u201cclose hand\u201d consists of folding individual fingers). Mimura et al. (2017) used Dirichlet process Gaussian mixture model with latent joint to provide a Bayesian body schema estimation based on tactile information.", "startOffset": 68, "endOffset": 461}, {"referenceID": 0, "context": "Borghi et al. (2004), for example, studied the interaction of object names with situated action on the same objects.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "We made use of a newly proposed sequential mapping algorithm which extends an idea of one-step mapping (Smith et al., 2006) and compared its overall accuracy to one-step mapping as well as to accuracies of segmenting individual body parts.", "startOffset": 103, "endOffset": 123}, {"referenceID": 6, "context": "In particular, the \u201ctactile homunculus\u201d (Hoffmann et al., 2017)\u2014a primary representation of the artificial sensitive skin the robot is covered with (see Fig.", "startOffset": 40, "endOffset": 63}, {"referenceID": 11, "context": "We created a YARP (Metta et al., 2006) software module to generate virtual skin contacts1.", "startOffset": 18, "endOffset": 38}, {"referenceID": 6, "context": "The input layer of the \u201ctactile homunculus\u201d (Hoffmann et al., 2017) consists of a vector, a(t), of activations of 1154 taxels at time t\u2014the output of the previous section\u2014that have binary values (1 when a taxel is stimulated, 0 otherwise).", "startOffset": 44, "endOffset": 67}, {"referenceID": 2, "context": "Mixture of Gaussians is trained by the EM algorithm (Dempster et al., 1977).", "startOffset": 52, "endOffset": 75}, {"referenceID": 17, "context": "In future, we plan to use an adaptive extension of GMM algorithm such as gmGMM (\u0160tep\u00e1nov\u00e1 and Vavre\u010dka, 2016) to detect this number autonomously.", "startOffset": 79, "endOffset": 109}, {"referenceID": 7, "context": "To process this data, we made use of CMU Sphinx (an open-source flexible Markov model-based speech recognizer system) (Lamere et al., 2003) and achieved 100% accuracy of word recognition.", "startOffset": 118, "endOffset": 139}, {"referenceID": 16, "context": "One possible way how to establish mapping between sensorimotor concepts and linguistic elements is to use frequencies of referent and meaning co-occurrences, that is, the ones with the highest co-occurrence are mapped together (Smith et al., 2006; Xu and Tenenbaum, 2007).", "startOffset": 227, "endOffset": 271}, {"referenceID": 19, "context": "One possible way how to establish mapping between sensorimotor concepts and linguistic elements is to use frequencies of referent and meaning co-occurrences, that is, the ones with the highest co-occurrence are mapped together (Smith et al., 2006; Xu and Tenenbaum, 2007).", "startOffset": 227, "endOffset": 271}, {"referenceID": 20, "context": "This allows the learner to subsequently choose the most strongly associated referent (Yu and Smith, 2012).", "startOffset": 85, "endOffset": 105}, {"referenceID": 6, "context": "Arrows illustrate the relationship in orientation between skin parts and the learned map (Hoffmann et al., 2017).", "startOffset": 89, "endOffset": 112}, {"referenceID": 20, "context": "To extend this basic idea, we can enable also forgetting by introducing a parameter \u03b7, which can capture the memory decay (Yu and Smith, 2012).", "startOffset": 122, "endOffset": 142}, {"referenceID": 9, "context": "Thanks to this mechanism, mutual exclusivity principle (the fact that children prefer mapping where object has only one label to multiple labels (Markman, 1990)) is guaranteed.", "startOffset": 145, "endOffset": 160}, {"referenceID": 9, "context": "The inhibitory mechanisms and situationtime dynamics were already partially included into the model of cross-situational learning proposed by McMurray et al. (2012). Even though our model shares some similarities with the model proposed by McMurray, it stems from different computational mechanisms.", "startOffset": 142, "endOffset": 165}, {"referenceID": 18, "context": "However, more realistic, non-uniform touch and, in particular, the addition of additional modalities (proprioception, vision) should enable bottom-up non-linguistic body part category formation, as described by (Vignemont et al., 2009), for example.", "startOffset": 211, "endOffset": 235}, {"referenceID": 1, "context": "Further study of the brain areas involved in this processing is needed, in order to develop models more closely inspired by the functional cortical networks, like in (Caligiore et al., 2010) that model the experimental findings of (Borghi et al.", "startOffset": 166, "endOffset": 190}, {"referenceID": 0, "context": ", 2010) that model the experimental findings of (Borghi et al., 2004).", "startOffset": 48, "endOffset": 69}], "year": 2017, "abstractText": "Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning). In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences. In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation. For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations. A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso). With the goal at arriving at the correct \u201cbody categories\u201d, we demonstrate how a sequential mapping algorithm outperforms one-step mapping. In addition, the effect of data set size and noise in the linguistic input are studied.", "creator": "LaTeX with hyperref package"}}}