{"id": "1706.02501", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Unlocking the Potential of Simulators: Design with RL in Mind", "abstract": "Using Reinforcement Learning (RL) in simulation to construct policies useful in real life is challenging. This is often attributed to the sequential decision making aspect: inaccuracies in simulation accumulate over multiple steps, hence the simulated trajectories diverge from what would happen in reality.", "histories": [["v1", "Thu, 8 Jun 2017 10:10:44 GMT  (1799kb,D)", "http://arxiv.org/abs/1706.02501v1", "Extended abstract for RLDM17 (3rd Multidisciplinary Conference on Reinforcement Learning and Decision Making)"]], "COMMENTS": "Extended abstract for RLDM17 (3rd Multidisciplinary Conference on Reinforcement Learning and Decision Making)", "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["rika antonova", "silvia cruciani"], "accepted": false, "id": "1706.02501"}, "pdf": {"name": "1706.02501.pdf", "metadata": {"source": "CRF", "title": "Unlocking the Potential of Simulators: Design with RL in Mind", "authors": ["Rika Antonova", "Silvia Cruciani"], "emails": ["cruciani}@kth.se"], "sections": [{"heading": null, "text": "In our work, we show the need to consider another important aspect: the incongruity in the simulation of control fields. We draw attention to the need to model control and dynamics, since over-simplifying assumptions about the application of RL policies could lead to policy failures in real systems. We design a simulator to solve a rotating task (of interest in robotics) and show that even a simple simulator designed with RL in mind outperforms high-speed simulators when it comes to learning a policy to be applied on a real robot system. We show that a phenomenon that is difficult to model - friction - can be successfully exploited even when RL-compatible simulators are implemented with a simple dynamic and noise model."}, {"heading": "Acknowledgements", "text": "We thank Christian Smith and Danica Kragic for their guidance in the \"Pivoting Task\" work supported by the European Union Framework Programme H2020-645403 RobDREAM. Robotics, Perception and Learning Group: https: / / www.kth.se / en / csc / forskning / rpl... at the School of Computer Science and Communicationar Xiv: 170 6.02 501v 1 [cs.L G] 8J un2 017"}, {"heading": "1 Introduction", "text": "Using simulators to learn strategies that are applicable to the real world is a challenge: Important aspects of action and environmental dynamics may be difficult and costly to model. Even if sufficient modelling knowledge and computational resources are available, inaccuracies that exist even in high-fidelity simulators can make the strategies learned useless in practice. In the case of sequential decision making, even a small discrepancy between the simulated and the real world could accumulate over several steps of policy execution. Despite these challenges, there is potential when simulators are designed with RL in mind. In our work, we try to understand which aspects of simulators are particularly important for learning RL strategies. We will first focus on the field of robotics and present an approach to learn robust strategies in a simulator that takes into account both the uncertainty of dynamics in the real world and the control of robots."}, {"heading": "2 Why Use Simulators for Learning?", "text": "The recent success of Reinforcement Learning in games (Atari, Go) and robotics make RL potentially the next candidate to solve difficult decision-making problems (under uncertainty) in a variety of areas. A number of attempts to apply RL algorithms to real-world problems have been successful, but the success has been largely limited to sample-based approaches. However, Batch RL is only a small fraction of a variety of RL algorithms, so the potential for using a wider range of RL algorithms that we can apply to simulations can be tapped. A number of fields have already developed general-purpose simulators, and task-specific simulators are often developed to solve concrete research problems."}, {"heading": "3 Challenges and Opportunities of Simulation-based Learning: Insights from Robotics", "text": "This year, it has reached the point where it will be able to leave the country without being able to reform it."}, {"heading": "4 Building RL-compatible Simulator for Solving a Pivoting Task", "text": "In this section, we present a brief summary of our research work described in [1] with the aim of providing an example that is comprehensible to an interdisciplinary audience. We describe our approach of building a task-specific simulator for a pivoting task, then learning an RL strategy in the simulation and applying it to the real robot2 (without further adjustments)."}, {"heading": "4.1 Dynamic and Friction Models", "text": "Our simulation system consists of a parallel claw, which is attached to a rotation point, which can rotate around a single axis. This system is an underactivated double-jointed planar arm, in which the underactivated member corresponds to the pivot point. We assume that we can control the desired acceleration on the first link. The dynamic model of the system is given by: (I + mr2 + mr. + mr. (tr.)??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.2 Learning Robust Policies", "text": "We formulate the pivoting task as an MDP. State space consists of states that are observed at each time step. We will learn from a robust environment, while at each time step we observe a real policy, while at each time step we pursue a real policy. However, the measures are: on the surface of the simulator, but the RL algorithm does not have explicit access to these. Reward is given at each time step that higher rewards are given if the angle of the tool is closer to the target angle: rt = - | We have implemented the dynamics of transition through the simulator, but the RL algorithm does not have explicit access to these. Reward is given at each time step that higher rewards are given if the angle of the tool is closer to the target angle: rt = \u2212 | We have implemented a real dynamic of transition between the simulators, but the RL algorithm has no explicit access to these rewards if the angle of the tool is closer to the target angle: rt = \u2212 | We have implemented a real dynamic of transition between the simulators, but the RL algorithm has no explicit access to these rewards."}, {"heading": "5 Conclusion & Implications for other Fields for Designing RL-compatible Simulators", "text": "Our experience in building an RL-compatible simulator for a particular robotic phenomenon is based on a few general suggestions. For example, focused dynamics modeling could make modeling the robot arm unnecessary if the motion is fixed to a certain level, where even simple dynamic equations are sufficient. In areas that already have general simulators, such \"focusing\" is not possible."}], "references": [{"title": "Reinforcement learning for pivoting task", "author": ["Rika Antonova", "Silvia Cruciani", "Christian Smith", "Danica Kragic"], "venue": "arXiv preprint arXiv:1703.00472,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Using local trajectory optimizers to speed up global optimization in dynamic programming", "author": ["Christopher G Atkeson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1604.06778,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "A general framework for open-loop pivoting", "author": ["A. Holladay", "R. Paolini", "M.T. Mason"], "venue": "In 2015 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Jens Kober", "J Andrew Bagnell", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Friction models and friction compensation", "author": ["H. Olsson", "K.J. strm", "M. Gfvert", "C. Canudas De Wit", "P. Lischinsky"], "venue": "Eur. J. Control,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Swing-up regrasping algorithm using energy control", "author": ["A. Sintov", "A. Shapiro"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "In-hand manipulation using gravity and controlled slip", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "K. Pauwels", "C. Smith", "D. Kragic"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Adaptive control for pivoting with visual and tactile feedback", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "C. Smith", "D. Kragic"], "venue": "In 2016 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Solving this task is of interest to the Robotics community; our approach to solving this task is described in detail in [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "Several recently successful algorithms, like Trust Region Policy Optimization (TRPO) [8] and Deep Deterministic Policy Gradient (DDPG) [6] are not designed to be particularly sample-efficient.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "Several recently successful algorithms, like Trust Region Policy Optimization (TRPO) [8] and Deep Deterministic Policy Gradient (DDPG) [6] are not designed to be particularly sample-efficient.", "startOffset": 135, "endOffset": 138}, {"referenceID": 1, "context": "[2]) and experimented with various approaches of injecting noise in the deterministic simulators to increase tolerance to slight modeling inaccuracies ( [5] gives a summary and further references).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[2]) and experimented with various approaches of injecting noise in the deterministic simulators to increase tolerance to slight modeling inaccuracies ( [5] gives a summary and further references).", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "In this section we present a short summary of our research work described in [1], with an intent to give an example that is comprehensible to an interdisciplinary audience.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "The limitations of the previous approaches include: only open loop control with no control of the gripping force [4], movement restricted to the vertical plane [9], gripper in a fixed position, thus motion of the tool is determined only by the gravitational torque and torsional friction [11].", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "The limitations of the previous approaches include: only open loop control with no control of the gripping force [4], movement restricted to the vertical plane [9], gripper in a fixed position, thus motion of the tool is determined only by the gravitational torque and torsional friction [11].", "startOffset": 160, "endOffset": 163}, {"referenceID": 10, "context": "The limitations of the previous approaches include: only open loop control with no control of the gripping force [4], movement restricted to the vertical plane [9], gripper in a fixed position, thus motion of the tool is determined only by the gravitational torque and torsional friction [11].", "startOffset": 288, "endOffset": 292}, {"referenceID": 6, "context": "When the tool moves with respect to the gripper, we model the friction torque \u03c4f as viscous friction and Coulomb friction [7]: \u03c4f =\u2212\u03bcv\u03c6\u0307tl\u2212\u03bccfn sgn(\u03c6\u0307tl), in which \u03bcv and \u03bcc are the viscous and Coulomb friction coefficients and sgn(\u00b7) is the signum function.", "startOffset": 122, "endOffset": 125}, {"referenceID": 9, "context": "Since most robots are not equipped with tactile sensors to measure the normal force fn at the contact point, as in [10] we express it as a function of the distance dfing between the fingers using a linear deformation model: fn=k(d0\u2212dfing), where k is a stiffness parameter, d0 is the distance at which fingers initiate contact with the tool.", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "We then trained a model-free deep RL policy search algorithm TRPO [8] on our simulated setting.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "TRPO has been shown to be competitive with (and sometimes outperform) other recent continuous state and action RL algorithms [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "We trained TRPO (using rllab [3] implementation as a starting point and parameters as reported in [8] for simulated control tasks) with a fully connected network with 2 hidden layers (with 32, 16 nodes) for policy and Q function approximators.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "We trained TRPO (using rllab [3] implementation as a starting point and parameters as reported in [8] for simulated control tasks) with a fully connected network with 2 hidden layers (with 32, 16 nodes) for policy and Q function approximators.", "startOffset": 98, "endOffset": 101}], "year": 2017, "abstractText": "Using Reinforcement Learning (RL) in simulation to construct policies useful in real life is challenging. This is often attributed to the sequential decision making aspect: inaccuracies in simulation accumulate over multiple steps, hence the simulated trajectories diverge from what would happen in reality. In our work we show the need to consider another important aspect: the mismatch in simulating control. We bring attention to the need for modeling control as well as dynamics, since oversimplifying assumptions about applying actions of RL policies could make the policies fail on real-world systems. We design a simulator for solving a pivoting task (of interest in Robotics) and demonstrate that even a simple simulator designed with RL in mind outperforms high-fidelity simulators when it comes to learning a policy that is to be deployed on a real robotic system. We show that a phenomenon that is hard to model \u2013 friction \u2013 could be exploited successfully, even when RL is performed using a simulator with a simple dynamics and noise model. Hence, we demonstrate that as long as the main sources of uncertainty are identified, it could be possible to learn policies applicable to real systems even using a simple simulator. RL-compatible simulators could open the possibilities for applying a wide range of RL algorithms in various fields. This is important, since currently data sparsity in fields like healthcare and education frequently forces researchers and engineers to only consider sample-efficient RL approaches. Successful simulator-aided RL could increase flexibility of experimenting with RL algorithms and help applying RL policies to real-world settings in fields where data is scarce. We believe that lessons learned in Robotics could help other fields design RL-compatible simulators, so we summarize our experience and conclude with suggestions.", "creator": "LaTeX with hyperref package"}}}