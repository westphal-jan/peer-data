{"id": "1706.02596", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Reading Twice for Natural Language Understanding", "abstract": "Despite the recent success of neural networks in tasks involving natural language understanding (NLU) there has only been limited progress in some of the fundamental challenges of NLU, such as the disambiguation of the meaning and function of words in context. This work approaches this problem by incorporating contextual information into word representations prior to processing the task at hand. To this end we propose a general-purpose reading architecture that is employed prior to a task-specific NLU model. It is responsible for refining context-agnostic word representations with contextual information and lends itself to the introduction of additional, context-relevant information from external knowledge sources. We demonstrate that previously non-competitive models benefit dramatically from employing contextual representations, closing the gap between general-purpose reading architectures and the state-of-the-art performance obtained with fine-tuned, task-specific architectures. Apart from our empirical results we present a comprehensive analysis of the computed representations which gives insights into the kind of information added during the refinement process.", "histories": [["v1", "Thu, 8 Jun 2017 14:10:22 GMT  (395kb,D)", "http://arxiv.org/abs/1706.02596v1", null], ["v2", "Wed, 25 Oct 2017 14:54:53 GMT  (320kb,D)", "http://arxiv.org/abs/1706.02596v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["dirk weissenborn"], "accepted": false, "id": "1706.02596"}, "pdf": {"name": "1706.02596.pdf", "metadata": {"source": "CRF", "title": "Reading Twice for Natural Language Understanding", "authors": ["Dirk Weissenborn"], "emails": ["dirk.weissenborn@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "This year, only once in history will there be such a process."}, {"heading": "2 Contextual Word Representations", "text": "In this paper, we propose to calculate contextually refined word representations before working on the present NLU task. An illustration of our (incremental) refinement strategy can be found in Figure 1. In the following, we formally define this approach. We designate the hidden dimensionality of our model with n and a fully connected layer with FC (u) = Wu + b, W-Rn \u00b7 m, b-Rn, u-Rm."}, {"heading": "2.1 Non-contextual Word Representations", "text": "The formal definition of this combination is given in Equation 1.ep + w = ReLU (Ue p w), U-Rn \u00b7 n \u00b2 gw = \u03c3 (FC ([ep + w echarw]) ew = gw \u00b7 ep + w + (1 \u2212 gw) \u00b7 echarw (1) echarw is calculated using a single-layer revolutionary neural network, using n revolutionary filters of width 5, followed by a max-pooling operation over time. Such a combination of pre-trained and character-based word embedding is common practice. Our approach follows [21, 26]."}, {"heading": "2.2 Contextual Word Representations", "text": "To create contextual word representations eXw, we start from a predefined context, which we formally define as a text set X = (X1,..., XLX).The embedded texts are individually embedded in this work by a bidirectional recursive neural network, a BiLSTM [1].The resulting output X-RLX \u00d7 2n is then projected onto X-RLX \u00b7 n, followed by a fully connected layer, followed by a ReLU nonlinearity (Eq. 2).X-X = BiLSTM (X) X-i = ReLU (FC (X-i)))) (2) To update the non-contextual representation of w, we maximize all representations of events that match in any X-poch X form."}, {"heading": "2.3 Incremental Reading", "text": "Instead of reading all the context given by the NLU task at once, we suggest reading the context X incrementally and updating the word representations after each read step. To do this, we must divide X into K ordered partitions Xk X, k Xk = X, Xk Xk \u2032 = \u2205. The subsequent task-specific model reads X starting with X1 and updates the non-contextual word representations ew = e0w as in Equation. 5. This process repeats K times, resulting in intermediate representations ekw for each partition k. The subsequent task-specific model uses the final representations eXw = e K for word w as its respective word embedding. This allows interaction between different parts of X already during the calculation of the contextual representations. In the case of document QA X1, the question q may contain the question X2, while X2 contains the supporting document p, which is used to extract the answer. Thus, the word system first reads anew followed by a new one followed by the word scheme, whereby each W reproduces a new one."}, {"heading": "2.4 Integrating External Knowledge", "text": "Different kinds of external knowledge could potentially be used to improve the performance of an NLU model, but these come in different formats, such as (subject, predicate, object) triples, a very prominent format, but we can also imagine using encyclopaedia definitions in free text form. However, since we do not want to limit ourselves by using a particular format or ontology, we consider external knowledge to be simple assertions in free text form. For example, in a triple (monkey, typeof, animal) assertion, we can derive the free text \"monkey type of animal.\" Crucially, this allows for a seamless integration of external knowledge into our contextual, refined process."}, {"heading": "3 Experimental Setup", "text": "Task-Specific Models One goal of our study is to enable simple, universal models to approach the state of the art by using contextual word representations explained in \u00a7 2. Therefore, we chose single-layer BiLSTM-based encoders with a task-specific, forward-facing neural network at the forefront of prediction. Such models are common baselines for NLU tasks and can be regarded as general reading architectures as opposed to highly tuned task-specific NLU systems that achieve current state-of-the-art results. All models are trained end-to-end together with our refinement architecture. We provide the exact model implementations and training details in Appendix A.Question Answering We apply our QA models to 2 current QA benchmark datasets, namely SQuAD [19] and NewsQA [24]. The task is to provide an answer-rich within a very extensive set of datasets, both of which are pre-provided in a document."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 To Read or not to Read", "text": "Table 1 shows the results of our 3 benchmark datasets for different context reading configurations. In all benchmarks, we see that our task-specific models perform rather poorly without context-related word representations, which is particularly pronounced for QA datasets. Reading only p (support for QA / premise in RTE) improves the results for all tasks, but only with marginal improvements in NewsQA. Reading only q (the question or hypothesis) is similarly helpful for reading p for SNLI, but there is a big improvement in the QA task. However, reading both q and p in the context of refinement improves the performance for all tasks even further, while the order of incremental reading does not seem to be important. Discussion The most important finding of this experiment is that reading the context before processing the task helps a lot and that reading all contexts is better than reading only parts of it. The most notable improvement in reading q in the performance of the QA task may be explained by the fact that the question of the document can be answered during the question of the question."}, {"heading": "4.2 Integrating external knowledge", "text": "In addition to providing only input by the task itself, we also provided access to external knowledge from ConceptNet as explained in \u00a7 2.4. We trained the same model as in the other experiment only that the system may read the top 10 additional claims during the training and the top 0 / 10 / 100 claims, each during the test. The first thing to note is that reading CN consistently goes beyond the datasets with notable improvements to SNLI and minor improvements to QS. Accessing no additional claims during the reading (CN (0)) significantly reduces performance, i.e., exceeding 10 that the model has been trained does not further improve performance. Changing our simple retrieval heuristic to an oracle that either does not provide assertions or the best possible results on the model we can improve is the accuracy of 87.9% accuracy for the 150-dimensional model, a remarkable prediction about our present-day behavior."}, {"heading": "4.3 Comparing to State-of-the-Art", "text": "Comparing our approach to the state of the art in Table 3 shows that our simple models perform very well without hyperparameter tuning, especially when compared only with comparable task-specific architectures. In these models, only the Document Reader [3] on SQuAD performs better, but unlike our single-layer BiLSTM, it uses a 3-layer BiLSTM and relies on handmade features. However, this is noteworthy when you consider that state-of-the-art results are typically achieved by extensively fine-tuning task-specific architectures and hyperparameters."}, {"heading": "4.4 Analysis of Contextual Word Representations", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "5 Related Work", "text": "There has been an abundance of work and studies in the recent past on universal word embedding [15, 18, 17, 11], which have been evaluated against more artificial word references or similarity benchmarks, and little progress has been made in creating and examining contextual word embedding. Hu et al. [8] Context representations for individual contexts are learned by summarizing a non-contextual representation along word embedding. Ultimately, their model aims to clarify our universally valid word assessments by using local contexts."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a refinement strategy for incorporating context information into contextual word representation. Our approach is applicable before any type of neural NLU model that works with word representation, and is suitable for introducing external knowledge in the form of free text. Detailed analysis shows that our refinement models are sensitive to the provided context and construct contextual word representations that vary in different contexts and contain task-specific information. We show that the use of contextual word representations instead of their non-contextual counterparts significantly increases the performance of otherwise low-performing baselines in three competitive benchmarks and closes the gap between universal reading architectures and advanced, state-of-the-art models. We believe that our refinement strategy can provide an important universal building block for building NLU architectures."}, {"heading": "Acknowledgments", "text": "We would like to thank Chris Dyer and Nils Rethmeier for comments on an early draft of this work, which was supported by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (01IW14002), BBDC (01IS14013E) and Software Campus (01IS12050, subproject GeNIE)."}], "references": [{"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Potts Christopher", "Christopher D Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Reading Wikipedia to Answer Open-Domain Questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "author": ["Bhuwan Dhingra", "Zhilin Yang", "William W Cohen", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Different contexts lead to different word embeddings", "author": ["Wenpeng Hu", "Jiajun Zhang", "Nan Zheng"], "venue": "In COLING,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "TACL, 3:211\u2013225,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention", "author": ["Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "context2vec: Learning generic context embedding with bidirectional lstm", "author": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan"], "venue": "In CoNLL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1310.4546,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Neural Semantic Encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom"], "venue": "ICLR, abs/1509.06664,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Bi-Directional Attention Flow for Machine Comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hananneh Hajishirzi"], "venue": "In ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Reading and Thinking : Re-read LSTM Unit for Textual Entailment Recognition", "author": ["Lei Sha", "Baobao Chang", "Zhifang Sui", "Sujian Li"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Representing General Relational Knowledge in ConceptNet 5", "author": ["Robert Speer", "Catherine Havasi"], "venue": "In LREC,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In ICLR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "Making Neural QA as Simple as Possible but not Simpler", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling", "author": ["Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "For example, in document question answering (QA) [19], the \u201ccontext\u201d consists of both the question being asked and the document where the answer is expected to be found, and both are necessary to capture the function and meaning of the words they are composed of.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "Neural NLU systems typically represent a word by a single, context-agnostic n-dimensional word embedding [15, 18].", "startOffset": 105, "endOffset": 113}, {"referenceID": 16, "context": "Neural NLU systems typically represent a word by a single, context-agnostic n-dimensional word embedding [15, 18].", "startOffset": 105, "endOffset": 113}, {"referenceID": 15, "context": "Following studies [17, 5], however, quickly realized that a single representation can hardly account for lexical ambiguity and proposed to model words by multiple representations instead.", "startOffset": 18, "endOffset": 25}, {"referenceID": 3, "context": "Following studies [17, 5], however, quickly realized that a single representation can hardly account for lexical ambiguity and proposed to model words by multiple representations instead.", "startOffset": 18, "endOffset": 25}, {"referenceID": 6, "context": "[8] show that general-purpose contextual representations can also be computed from the local context instead of keeping multiple representations for single words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] showed that bidirectional recurrent neural networks (BiRNNs) can compute contextual representations which help in disambiguating tokens in their local context.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "For instance, recent work on QA [26, 3] shows that providing simple, task-specific word level information, in particular a feature determining whether a word occurs in the question or not, in addition to the words themselves can have a dramatic impact on the performance", "startOffset": 32, "endOffset": 39}, {"referenceID": 1, "context": "For instance, recent work on QA [26, 3] shows that providing simple, task-specific word level information, in particular a feature determining whether a word occurs in the question or not, in addition to the words themselves can have a dramatic impact on the performance", "startOffset": 32, "endOffset": 39}, {"referenceID": 19, "context": "Our approach follows [21, 26].", "startOffset": 21, "endOffset": 29}, {"referenceID": 23, "context": "Our approach follows [21, 26].", "startOffset": 21, "endOffset": 29}, {"referenceID": 21, "context": "ConceptNet ConceptNet1 [23] (CN) is freely-available, multi-lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources, such as Wiktionary2, Open Multilingual WordNet3, OpenCyc or DBpedia4.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Question Answering We apply our QA models on 2 recent QA benchmark datasets, namely SQuAD[19] and NewsQA[24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "Recognizing Textual Entailment We test on the frequently used SNLI dataset [2], a collection of 570k sentence pairs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 23, "context": "This kind of feature can easily be captured in our refinement process when including the question [26, 3].", "startOffset": 98, "endOffset": 105}, {"referenceID": 1, "context": "This kind of feature can easily be captured in our refinement process when including the question [26, 3].", "startOffset": 98, "endOffset": 105}, {"referenceID": 23, "context": "FastQA [26] 77.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "[3] 79.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "1 FastQA [26] 55.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "M-LSTM [24, 25] 48.", "startOffset": 7, "endOffset": 15}, {"referenceID": 23, "context": "4 FastQAExt [26] 56.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "[12] 84.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "2 NSE [16] 84.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "Re-read LSTM [22] 87.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "5 TreeLSTM [4] 88.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "For these models, only the Document Reader [3] performs better on SQuAD but in contrast to our single layer BiLSTM it uses a 3-layer BiLSTM and relies on hand-crafted features.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "Figure 2c shows a TSNE-plot [13] of contextual updates, i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.", "startOffset": 81, "endOffset": 97}, {"referenceID": 16, "context": "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.", "startOffset": 81, "endOffset": 97}, {"referenceID": 15, "context": "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.", "startOffset": 81, "endOffset": 97}, {"referenceID": 9, "context": "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.", "startOffset": 81, "endOffset": 97}, {"referenceID": 12, "context": "Contextual Word Representations Context2Vec [14] learns unsupervised context representations along word embeddings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "[8] compute contextual representations for individual context tokens by summing a non-contextual representation with a computed representation of the local context.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[27] utilize external knowledge in form of DBpedia comments (short abstracts/definitions) to improve the answering ability of a model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In a prior step they encode comments for DBpedia entities via Doc2Vec[10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "[28] incorporate a recall mechanism into a standard LSTM cell and similarly encode external pieces of knowledge by a single representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] exploit linguistic knowledge using MAGE-GRUs, an adapation of GRUs to handle graphs, however, external knowledge has to be present in form of triples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "For instance, neural semantic encoders (NSE) [16] utilize neural memory with soft addressing to incrementally read and write information from.", "startOffset": 45, "endOffset": 49}], "year": 2017, "abstractText": "Despite the recent success of neural networks in tasks involving natural language understanding (NLU) there has only been limited progress in some of the fundamental challenges of NLU, such as the disambiguation of the meaning and function of words in context. This work approaches this problem by incorporating contextual information into word representations prior to processing the task at hand. To this end we propose a general-purpose reading architecture that is employed prior to a task-specific NLU model. It is responsible for refining context-agnostic word representations with contextual information and lends itself to the introduction of additional, context-relevant information from external knowledge sources. We demonstrate that previously non-competitive models benefit dramatically from employing contextual representations, closing the gap between general-purpose reading architectures and the state-of-the-art performance obtained with fine-tuned, task-specific architectures. Apart from our empirical results we present a comprehensive analysis of the computed representations which gives insights into the kind of information added during the refinement process.", "creator": "LaTeX with hyperref package"}}}