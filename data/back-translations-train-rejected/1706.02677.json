{"id": "1706.02677", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour", "abstract": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.", "histories": [["v1", "Thu, 8 Jun 2017 16:51:53 GMT  (163kb,D)", "http://arxiv.org/abs/1706.02677v1", "Tech report"]], "COMMENTS": "Tech report", "reviews": [], "SUBJECTS": "cs.CV cs.DC cs.LG", "authors": ["priya goyal", "piotr doll\\'ar", "ross girshick", "pieter noordhuis", "lukasz wesolowski", "aapo kyrola", "rew tulloch", "yangqing jia", "kaiming he"], "accepted": false, "id": "1706.02677"}, "pdf": {"name": "1706.02677.pdf", "metadata": {"source": "CRF", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour", "authors": ["Priya Goyal", "Piotr Doll\u00e1r", "Ross Girshick", "Pieter Noordhuis", "Lukasz Wesolowski", "Aapo Kyrola", "Andrew Tulloch", "Yangqing Jia", "Kaiming He"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, it is in such a way that we are able to move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to another world, in which we move to a world, in which we move to another world, in which we move to a world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to a world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we move to another world, in which we"}, {"heading": "2. Large Minibatch SGD", "text": "We begin by examining the formulation of the stochastic gradient descent (SGD), which will form the basis of our discussions in the following sections. We consider supervised learning by minimizing a loss L (w) of the form: L (w) = 1 | X | \u2211 x-X l (x, w). (1) Here w are the weights of a network, X is a labeled training set, and l (x, w) is the loss calculated from samples x-X and their designations y. Typically, l consists of a predictive loss (e.g. cross-entropy loss) and a regulation loss on w.Minibatch of stochastic gradient descent [31], which is commonly referred to simply as SGD in newer literature, although it works on minibatches, the following update performs: wt + 1 = wt-n-x-B remark (x, wt). (2) Here, the X is a minibatch of B and the n is used in the discussion."}, {"heading": "2.1. Learning Rates for Large Minibatches", "text": "\"We must adjust to the fact that we are able to learn a large number of people.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"We.\" \"\" \"\" \"We.\". \"\" \"\" \"\" \"We.\". \"\" \"\" \"We.\".. \"\" \"\" We.. \"..\" \"\" \"We.......\" \"\" \"\" We...... \"\" \"\" \"We......\" \"\" \"\" \"\" \"We......\" \"\" \"\" \"\" \"\" We....... \"\" \"\" \"\" \"..\" \"\" \"We.......\" \"\" \"\" \"..\" \"\" \"\" We.............. \"\" \"\" \"\" \".....\" \"\"....... \"\" \"\" \"\" \"\" We....................... \"\" \"\" \"\" \"\" \".............\" \"\" \"\" \"\" \"\" \"........\" \"\" \"\" \"......\" \"\" \"We.............................\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"..........................\" \"\" \"\" \"\"............................. \"\" \"\" \"\""}, {"heading": "2.2. Warmup", "text": "As we have discussed, for large minibatches (e.g. 8k), the linear scale rule breaks down when the network changes rapidly, which often occurs in the early stages of training. We find that this problem can be alleviated by a properly designed warmup [16], namely a strategy of using less aggressive learning rates at the beginning of training. The warmup strategy presented in [16] uses a low constant learning rate for the first few training periods. As we will show in \u00a7 5, we have found constant warmup particularly helpful for prototyping object recognition and segmentation methods [9, 30, 25, 14] which fine-tune pre-formed layers together with newly initialized layers. In our ImageNet experiments with a large minibatch of size kn, we have tried to train with the low learning rate of the first 5 epochs and then reheat up with the cell learning rate of the first 5 epochs."}, {"heading": "2.3. Batch Normalization with Large Minibatches", "text": "In the following, we will show that a commonly used \"linkage,\" which may be a practical consideration to avoid the need for communication, is actually necessary to obtain the loss function when changing the minibatch size. We note that (1) and (2) the loss per sample is l (x, w) independent of all other samples. This is not the case when BN is performed and activations are calculated from samples. We write lB (x, w) to indicate the loss of a single sample x depends on the statistics of all samples l (x, w). We refer to the loss via a single minibatch B of size n as L (B, w)."}, {"heading": "3. Subtleties and Pitfalls of Distributed SGD", "text": "In practice, there are many more subtle conversion errors that alter the definitions of hyperparameters, leading to models whose error rate may be higher than expected, and such problems can be difficult to spot. (1) However, it is important to consider the underlying solutions. (2) It is actually the result of the gradient of an L2 regulation term in the loss function. (1) It is a pattern-dependent term that can be written as l (x, w). (2) It is the result of an L2 regulation term in the loss function. (2) It is a pattern-dependent term such as cross-entropy loss. (2) The SGD update in (2) can be written as follows: wt + 1 = wt-dependent L2 regulation in the loss function. (2) It is a pattern-dependent term such as cross-entropy loss. (2) The GwD update may look like: SwD update in the loss function. (2)"}, {"heading": "4. Communication", "text": "In order to scale beyond the 8 GPUs in a single Big Basin server [24], gradient aggregation must be spread across multiple servers in a network. To achieve near-perfect linear scaling, aggregation must be performed in parallel to backprop. This is possible because there is no data dependency between gradients across layers. Therefore, once the gradient is calculated for one layer, it is aggregated across workers while the gradient calculation for the next layer is continued (as described in [5])."}, {"heading": "4.1. Gradient Aggregation", "text": "In fact, it is a way of hiding the cost of aggregation in the backpacker phase in order to overcome these effects, beyond the scope of this work (e.g. the number of parameters that are realized in the GPU). nbsp; It is about the way they are realized in the GPU. & nbsp; It is about the way it is realized in the GPU. & nbsp; It is about the way in which the GPU is realized. & nbsp; It is about the way in which the GPU is realized. & nbsp; It is about the way in which the GPH is realized."}, {"heading": "4.2. Software", "text": "The algorithms described are implemented in Gloo4, a library for collective communication, and they support deadt4https: / / github.com / facebookincubator / gloomultiple communication contexts, which means that no additional synchronization is required to run multiple allreduce instances in parallel. Local reduction and broadcasting (described as phases (1) and (3) are performed in parallel with interserver allreduce where possible. Caffe2 supports multi-hand execution of the computation graph, which is a training procedure. Whenever there is no data dependency between subgraphs, multiple threads can run these subgraphs in parallel. Applied to backprop, local gradients can be calculated one after the other without having to deal with all-reduced or weight updates, which means that during the backprop the set of runnable subgraphs can grow faster than we can execute them. If we select subgraphs to execute all of the same servers, we need to partially run one run over all of them."}, {"heading": "4.3. Hardware", "text": "We used Facebook's Big Basin [24] GPU servers for our experiments. Each server contains 8 NVIDIA Tesla P100 GPUs connected to NVIDIA NVLink. For local storage, each server has 3.2TB of NVMe SSDs. For network connectivity, according to the analysis below, the servers have a Mellanox ConnectX-4 50Gbit Ethernet network card and are connected to Wedge100 [1] Ethernet switches. We found that 50 Gbit network bandwidth for distributed synchronous SGD is sufficient for ResNet-50. ResNet-50 has approximately 25 million parameters. This means that the total size of the parameters is 25 \u00d7 106 \u00b7 size (float) = 100 MB. Backprop for ResNet-50 on a single NVIDIA Tesla P100 GPU requires 120 ms. Given the fact that just reducing 2 x bytes in the network compared to the value on which the network is operated, this leads to a maximum bandwidth requirement of 12 MB for a network."}, {"heading": "5. Main Results and Analysis", "text": "Our main result is that we can train ResNet-50 [16] on ImageNet [32] with 256 workers in one hour, while simultaneously adjusting the accuracy of small minibatch trainings. Applying the linear scale rule together with a warm-up strategy allows us to scale seamlessly between small and large minibatches (up to 8k images) without optimizing additional hyperparameters or affecting accuracy. In the following subsections we describe: (1) experimental settings, (2) determine the effectiveness of large minibatch trainings, (3) perform a deeper experimental analysis, (4) show our results generalizing to object recognition / segmentation, and (5) provide timings."}, {"heading": "5.1. Experimental Settings", "text": "The 1000-way ImageNet classification task [32] serves as our most important experimental benchmark = 9 average dynamics, which we perform with the help of models. Models are trained on the \u0445 1.28 million training images and evaluated by top1 errors on the 50,000 validation images. We use the ResNet-50 [16] variant of [12], but note that the standard dynamics used in [16] are equally effective. We use a weight loss of 0.0001 and following [16] we do not apply weight loss to the learnable BN coefficients (namely the \u03b2 and \u03b2 in [19]). To maintain the training target, which depends on the BN batch size n, we use n = 32 everywhere, regardless of the total minibatch size."}, {"heading": "5.2. Optimization or Generalization Issues?", "text": "The fact is that over the next five years we will be able to manoeuvre ourselves into a similar situation, in which we will be able to move to another world, in which we will be able to move, in which we will be able to move to another world, in which we will be able to move to another world, in which we will be able to move to another world, in which we will be able to move to another world, in which we will be able to move to another world, in which we will be able to move to another world. \""}, {"heading": "5.3. Analysis Experiments", "text": "It also shows that the learning curves for different minibatch sizes remain stable in a wide range of minibatch sizes, from 64 to 8k, after which it begins to increase. Beyond 64k training diverges when using the linear learning rate. 5Training curves for different minibatch sizes. Each of the nine plots in Figure 3 shows the top 1 training curve for the 256 minibatch baseline (orange) and a second curve that corresponds to different minibatch sizes."}, {"heading": "1 2 2.5 1,280,000 35.7 33.6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 4 5.0 640,000 35.7 33.7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 8 10.0 320,000 35.7 33.5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 16 20.0 160,000 35.6 33.6", "text": "(b) Linear scaling of the learning rate applied to Mask R-CNN. Using the single ResNet-50 model of [16] (so no default scaling is reported), we train Mask R-CNN using 1 to 8 GPUs that follow the linear scaling rule. Box and Mask AP are nearly identical in all configurations, demonstrating the successful generalization of the rule beyond classification. Table 3. Object detection on COCO with Mask R-CNN [14] is very similar to the ImageNet-1k curve, showing that it is unlikely for users that even a 5-fold increase in the dataset size will automatically result in a significant increase in usable minibatch size. Quantitatively, using an 8k minibatch increases the validation error by 0.26% from 25.83% for a 256 minibatch to 26.09%. Understanding the exact relationship between generalization error, minibatch-size and data input is open to future work."}, {"heading": "5.4. Generalization to Detection and Segmentation", "text": "A low failure rate on ImageNet is not typically an end goal. Instead, the benefit of the ImageNet training lies in learning miniature 256 512 1k 2k 4k 8k 11kmini-batch size0.20.220.240.260.3ti me pe rit e rati n (s e c s) 0.5124816ti me pe re p o c h (m ins) Figure 7: Distributed synchronous SGD timing. Time per iteration (seconds) and time per ImageNet epoch (minutes) for training with different minibatch sizes. The baseline (kn = 256) uses 8 GPUs in a single server, while all other training courses distribute training via (kn / 256) servers. With 352 GPUs (44 servers), our implementation completes a passport across all countries with 1.28 million ImageNet training images in about 30 seconds."}, {"heading": "5.5. Run Time", "text": "Figure 7 shows two visualizations of the runtime characteristics of our system: The blue curve is the time per iteration as a minibatch size varies from 256 to 11264 (11k). Remarkably, this curve is relatively flat and the time per iteration only increases by 12%, while the minibatch size is scaled by 44 x. In other words, the orange curve shows the near-linear decrease in time per epoch from over 16 minutes to just 30 seconds. Runtime performance can also be considered in terms of throughput (images / second), as shown in Figure 8. In terms of a perfectly efficient extrapolation of the 8 GPU baseline, our implementation achieves a scaling efficiency of 90%. We are known to thank Leon Bottou for helpful discussions about the theoretical background, Jerry Pan and Christian Puhrsch for discussions about efficient data loading, Andrew Dye for help with debugging distributed training, as well as Kevin Lee, Dodds, Brian Lee, and Brian J Volk for support."}], "references": [{"title": "Opening designs for 6-pack and Wedge 100", "author": ["J. Bagga", "H. Morsy", "Z. Yao"], "venue": " https: //code.facebook.com/posts/203733993317833/ opening-designs-for-6-pack-and-wedge-100", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "R", "author": ["M. Barnett", "L. Shuler"], "venue": "van De Geijn, S. Gupta, D. G. Payne, and J. Watts. Interprocessor collective communication library (intercom). In Scalable High-Performance Computing Conference", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Curiously fast convergence of some stochastic gradient descent algorithms", "author": ["L. Bottou"], "venue": "Unpublished open problem offered to the attendance of the SLDS 2009 conference", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Opt", "author": ["L. Bottou", "F.E. Curtis", "J. Nocedal"], "venue": "methods for large-scale machine learning. arXiv:1606.04838", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting Distributed Synchronous SGD", "author": ["J. Chen", "X. Pan", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "arXiv:1604.00981", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering", "author": ["K. Chen", "Q. Huo"], "venue": "ICASSP", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Using MPI: Portable Parallel Programming with the Message-Passing Interface", "author": ["W. Gropp", "E. Lusk", "A. Skjellum"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Training and investigating Residual Nets", "author": ["S. Gross", "M. Wilber"], "venue": "https://github.com/facebook/fb. resnet.torch", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Why random reshuffling beats stochastic gradient descent", "author": ["M. G\u00fcrb\u00fczbalaban", "A. Ozdaglar", "P. Parrilo"], "venue": "arXiv:1510.08560", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Mask R- CNN", "author": ["K. He", "G. Gkioxari", "P. Doll\u00e1r", "R. Girshick"], "venue": "arXiv:1703.06870", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2017}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["G. Hinton", "L. Deng", "D. Yu"], "venue": "E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["I. Hubara", "M. Courbariaux", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "arXiv:1510.08560", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"], "venue": "ICLR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "arXiv:1404.5997", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet classification with deep convolutional neural nets", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Introducing Big Basin: Our next-generation AI hardware", "author": ["K. Lee"], "venue": " https://code.facebook.com/posts/ 1835166200089399/introducing-big-basin", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Feature pyramid networks for object detection", "author": ["T.-Y. Lin", "P. Doll\u00e1r", "R. Girshick", "K. He", "B. Hariharan", "S. Belongie"], "venue": "CVPR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Microsoft COCO: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": "Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimization of collective reduction operations", "author": ["R. Rabenseifner"], "venue": "ICCS. Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1951}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "localization and detection using convolutional networks. In ICLR", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization of collective comm", "author": ["R. Thakur", "R. Rabenseifner", "W. Gropp"], "venue": "operations in MPICH. IJHPCA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "et al", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Aggregated residual transformations for deep neural networks", "author": ["S. Xie", "R. Girshick", "P. Doll\u00e1r", "Z. Tu", "K. He"], "venue": "CVPR", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "The Microsoft 2016 Conversational Speech Recognition System", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "arXiv:1609.03528", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 148, "endOffset": 172}, {"referenceID": 39, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 148, "endOffset": 172}, {"referenceID": 32, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 148, "endOffset": 172}, {"referenceID": 33, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 148, "endOffset": 172}, {"referenceID": 34, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 148, "endOffset": 172}, {"referenceID": 15, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 148, "endOffset": 172}, {"referenceID": 16, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 181, "endOffset": 189}, {"referenceID": 38, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 181, "endOffset": 189}, {"referenceID": 6, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 223, "endOffset": 230}, {"referenceID": 36, "context": "We are in an unprecedented era in AI research history in which the increasing data and model scale is rapidly improving accuracy in computer vision [22, 40, 33, 34, 35, 16], speech [17, 39], and natural language processing [7, 37].", "startOffset": 223, "endOffset": 230}, {"referenceID": 22, "context": "Take the profound impact in computer vision as an example: visual representations learned by deep convolutional neural networks [23, 22] show excellent performance on previously challenging tasks like ImageNet classification [32] and can be transferred to difficult perception problems such as object detection and seg64 128 256 512 1k 2k 4k 8k 16k 32k 64k", "startOffset": 128, "endOffset": 136}, {"referenceID": 21, "context": "Take the profound impact in computer vision as an example: visual representations learned by deep convolutional neural networks [23, 22] show excellent performance on previously challenging tasks like ImageNet classification [32] and can be transferred to difficult perception problems such as object detection and seg64 128 256 512 1k 2k 4k 8k 16k 32k 64k", "startOffset": 128, "endOffset": 136}, {"referenceID": 31, "context": "Take the profound impact in computer vision as an example: visual representations learned by deep convolutional neural networks [23, 22] show excellent performance on previously challenging tasks like ImageNet classification [32] and can be transferred to difficult perception problems such as object detection and seg64 128 256 512 1k 2k 4k 8k 16k 32k 64k", "startOffset": 225, "endOffset": 229}, {"referenceID": 7, "context": "mentation [8, 10, 27].", "startOffset": 10, "endOffset": 21}, {"referenceID": 9, "context": "mentation [8, 10, 27].", "startOffset": 10, "endOffset": 21}, {"referenceID": 26, "context": "mentation [8, 10, 27].", "startOffset": 10, "endOffset": 21}, {"referenceID": 21, "context": "Moreover, this pattern generalizes: larger datasets and network architectures consistently yield improved accuracy across all tasks that benefit from pretraining [22, 40, 33, 34, 35, 16].", "startOffset": 162, "endOffset": 186}, {"referenceID": 39, "context": "Moreover, this pattern generalizes: larger datasets and network architectures consistently yield improved accuracy across all tasks that benefit from pretraining [22, 40, 33, 34, 35, 16].", "startOffset": 162, "endOffset": 186}, {"referenceID": 32, "context": "Moreover, this pattern generalizes: larger datasets and network architectures consistently yield improved accuracy across all tasks that benefit from pretraining [22, 40, 33, 34, 35, 16].", "startOffset": 162, "endOffset": 186}, {"referenceID": 33, "context": "Moreover, this pattern generalizes: larger datasets and network architectures consistently yield improved accuracy across all tasks that benefit from pretraining [22, 40, 33, 34, 35, 16].", "startOffset": 162, "endOffset": 186}, {"referenceID": 34, "context": "Moreover, this pattern generalizes: larger datasets and network architectures consistently yield improved accuracy across all tasks that benefit from pretraining [22, 40, 33, 34, 35, 16].", "startOffset": 162, "endOffset": 186}, {"referenceID": 15, "context": "Moreover, this pattern generalizes: larger datasets and network architectures consistently yield improved accuracy across all tasks that benefit from pretraining [22, 40, 33, 34, 35, 16].", "startOffset": 162, "endOffset": 186}, {"referenceID": 15, "context": "As an example, we scale ResNet-50 [16] training, originally performed with a minibatch size of 256 images (using 8 Tesla P100 GPUs, training time is 29 hours), to larger minibatches (see Figure 1).", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "While this guideline is found in earlier work [21, 4], its empirical limits are not well understood and informally we have found that it is not widely known to the research community.", "startOffset": 46, "endOffset": 53}, {"referenceID": 3, "context": "While this guideline is found in earlier work [21, 4], its empirical limits are not well understood and informally we have found that it is not widely known to the research community.", "startOffset": 46, "endOffset": 53}, {"referenceID": 15, "context": ", a strategy of using lower learning rates at the start of training [16], to overcome early optimization difficulties.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "Our comprehensive experiments in \u00a75 show that optimization difficulty is the main issue with large minibatches, rather than poor generalization (at least on ImageNet), in contrast to some recent studies [20].", "startOffset": 203, "endOffset": 207}, {"referenceID": 8, "context": "Additionally, we show that the linear scaling rule and warmup generalize to more complex tasks including object detection and segmentation [9, 30, 14, 27], which we demonstrate via the recently developed Mask R-CNN [14].", "startOffset": 139, "endOffset": 154}, {"referenceID": 29, "context": "Additionally, we show that the linear scaling rule and warmup generalize to more complex tasks including object detection and segmentation [9, 30, 14, 27], which we demonstrate via the recently developed Mask R-CNN [14].", "startOffset": 139, "endOffset": 154}, {"referenceID": 13, "context": "Additionally, we show that the linear scaling rule and warmup generalize to more complex tasks including object detection and segmentation [9, 30, 14, 27], which we demonstrate via the recently developed Mask R-CNN [14].", "startOffset": 139, "endOffset": 154}, {"referenceID": 26, "context": "Additionally, we show that the linear scaling rule and warmup generalize to more complex tasks including object detection and segmentation [9, 30, 14, 27], which we demonstrate via the recently developed Mask R-CNN [14].", "startOffset": 139, "endOffset": 154}, {"referenceID": 13, "context": "Additionally, we show that the linear scaling rule and warmup generalize to more complex tasks including object detection and segmentation [9, 30, 14, 27], which we demonstrate via the recently developed Mask R-CNN [14].", "startOffset": 215, "endOffset": 219}, {"referenceID": 23, "context": "We use the recently open-sourced Caffe21 deep learning framework and Big Basin GPU servers [24], which operates efficiently using standard Ethernet networking (as opposed to specialized network interfaces).", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "in our experience migrating Faster R-CNN [30] and ResNets [16] from 1 to 8 GPUs.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "in our experience migrating Faster R-CNN [30] and ResNets [16] from 1 to 8 GPUs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "Minibatch Stochastic Gradient Descent [31], usually referred to as simply as SGD in recent literature even though it operates on minibatches, performs the following update:", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "The above linear scaling rule was adopted by Krizhevsky [21], if not earlier.", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "[5] presented a comparison of numerous distributed SGD variants, and although their work also employed the linear scaling rule, it did not establish a small minibatch baseline (the most related result is in v1 of [5] which reported a 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] presented a comparison of numerous distributed SGD variants, and although their work also employed the linear scaling rule, it did not establish a small minibatch baseline (the most related result is in v1 of [5] which reported a 0.", "startOffset": 213, "endOffset": 216}, {"referenceID": 3, "context": "[4] (section 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "We find that this issue can be alleviated by a properly designed warmup [16], namely, a strategy of using less aggressive learning rates at the start of training.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "The warmup strategy presented in [16] uses a low constant learning rate for the first few epochs of training.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "As we will show in \u00a75, we have found constant warmup particularly helpful for prototyping object detection and segmentation methods [9, 30, 25, 14] that fine-tune pre-trained layers together with newly initialized layers.", "startOffset": 132, "endOffset": 147}, {"referenceID": 29, "context": "As we will show in \u00a75, we have found constant warmup particularly helpful for prototyping object detection and segmentation methods [9, 30, 25, 14] that fine-tune pre-trained layers together with newly initialized layers.", "startOffset": 132, "endOffset": 147}, {"referenceID": 24, "context": "As we will show in \u00a75, we have found constant warmup particularly helpful for prototyping object detection and segmentation methods [9, 30, 25, 14] that fine-tune pre-trained layers together with newly initialized layers.", "startOffset": 132, "endOffset": 147}, {"referenceID": 13, "context": "As we will show in \u00a75, we have found constant warmup particularly helpful for prototyping object detection and segmentation methods [9, 30, 25, 14] that fine-tune pre-trained layers together with newly initialized layers.", "startOffset": 132, "endOffset": 147}, {"referenceID": 18, "context": "Batch Normalization (BN) [19] computes statistics along the minibatch dimension: this breaks the independence of each sample\u2019s loss, and changes in minibatch size change the underlying definition of the loss function being optimized.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "In this work, we use n = 32 which has performed well for a wide range of datasets and networks [19, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 15, "context": "In this work, we use n = 32 which has performed well for a wide range of datasets and networks [19, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 10, "context": "However, standard communication primitives like allreduce [11] perform summing, not averaging.", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": "In practice, common SGD implementations apply random shuffling of the training set during each SGD epoch, which can give better results [3, 13].", "startOffset": 136, "endOffset": 143}, {"referenceID": 12, "context": "In practice, common SGD implementations apply random shuffling of the training set during each SGD epoch, which can give better results [3, 13].", "startOffset": 136, "endOffset": 143}, {"referenceID": 15, "context": ", [16]), we ensure the samples in one epoch done by k workers are from a single consistent random shuffling of the training set.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "In order to scale beyond the 8 GPUs in a single Big Basin server [24], gradient aggregation has to span across servers on a network.", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "Therefore, as soon as the gradient for a layer is computed, it is aggregated across workers, while gradient computation for the next layer continues (as discussed in [5]).", "startOffset": 166, "endOffset": 169}, {"referenceID": 10, "context": "For every gradient, aggregation is done using an allreduce operation (similar to the MPI collective operation MPI Allreduce [11]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": ", quantized gradients [18], Block-Momentum SGD [6]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": ", quantized gradients [18], Block-Momentum SGD [6]).", "startOffset": 47, "endOffset": 50}, {"referenceID": 28, "context": "For interserver allreduce, we implemented two of the best algorithms for bandwidth-limited scenarios: the recursive halving and doubling algorithm [29, 36] and the bucket algorithm (also known as the ring algorithm) [2].", "startOffset": 147, "endOffset": 155}, {"referenceID": 35, "context": "For interserver allreduce, we implemented two of the best algorithms for bandwidth-limited scenarios: the recursive halving and doubling algorithm [29, 36] and the bucket algorithm (also known as the ring algorithm) [2].", "startOffset": 147, "endOffset": 155}, {"referenceID": 1, "context": "For interserver allreduce, we implemented two of the best algorithms for bandwidth-limited scenarios: the recursive halving and doubling algorithm [29, 36] and the bucket algorithm (also known as the ring algorithm) [2].", "startOffset": 216, "endOffset": 219}, {"referenceID": 28, "context": "To support non-power-of-two number of servers, we used the binary blocks algorithm [29].", "startOffset": 83, "endOffset": 87}, {"referenceID": 23, "context": "We used Facebook\u2019s Big Basin [24] GPU servers for our experiments.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "For network connectivity, the servers have a Mellanox ConnectX-4 50Gbit Ethernet network card and are connected to Wedge100 [1] Ethernet switches.", "startOffset": 124, "endOffset": 127}, {"referenceID": 15, "context": "Our main result is that we can train ResNet-50 [16] on ImageNet [32] using 256 workers in one hour, while matching the accuracy of small minibatch training.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "Our main result is that we can train ResNet-50 [16] on ImageNet [32] using 256 workers in one hour, while matching the accuracy of small minibatch training.", "startOffset": 64, "endOffset": 68}, {"referenceID": 31, "context": "The 1000-way ImageNet classification task [32] serves as our main experimental benchmark.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "We use the ResNet-50 [16] variant from [12], noting that the stride-2 convolutions are on 3\u00d73 layers instead of on 1\u00d71 layers as in [16].", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "We use the ResNet-50 [16] variant from [12], noting that the stride-2 convolutions are on 3\u00d73 layers instead of on 1\u00d71 layers as in [16].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "We use the ResNet-50 [16] variant from [12], noting that the stride-2 convolutions are on 3\u00d73 layers instead of on 1\u00d71 layers as in [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 27, "context": "We use Nesterov momentum [28] with m of 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "9 following [12] but note that standard momentum as was used in [16] is equally effective.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "9 following [12] but note that standard momentum as was used in [16] is equally effective.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "0001 and following [16] we do not apply weight decay on the learnable BN coefficients (namely, \u03b3 and \u03b2 in [19]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "0001 and following [16] we do not apply weight decay on the learnable BN coefficients (namely, \u03b3 and \u03b2 in [19]).", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "As in [12], we compute the BN statistics using running average (with momentum 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "1 as in [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "1 \u00b7 kn 256 ) the reference learning rate, and reduce it by 1/10 at the 30-th, 60-th, and 80-th epoch, similar to [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "We adopt the initialization of [15] for all convolutional layers.", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "We use scale and aspect ratio data augmentation [35] as in [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "We use scale and aspect ratio data augmentation [35] as in [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "The input image is normalized by the per-color mean and standard deviation, as in [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "Under these settings, we establish a ResNet-50 baseline using k = 8 (8 GPUs in one server) and n = 32 images per worker (minibatch size of kn = 256), as in [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "torch [12] has 24.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "01% error, and that of the original ResNet paper [16] has 24.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "1 \u00b7 \u221a 32 according to the square root scaling rule that was justified theoretically in [21] on grounds that it scales \u03b7 by the inverse amount of the reduction in the gradient estimator\u2019s standard deviation.", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "Results for ResNet-101 [16] are shown in Table 2c.", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "[38] that extends ImageNet-1k to 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "8 million images (roughly 5\u00d7 larger) by adding 4k additional categories from ImageNet22k [32].", "startOffset": 89, "endOffset": 93}, {"referenceID": 37, "context": "We evaluate the 1k-way classification error on the original ImageNet-1k validation set as in [38].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "Using the single ResNet-50 model from [16] (thus no std is reported), we train Mask R-CNN using using from 1 to 8 GPUs following the linear learning rate scaling rule.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "Object detection on COCO with Mask R-CNN [14].", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "A question of key importance is if the features learned with large minibatches generalize as well as the features learned with small minibatches? To test this, we adopt the object detection and instance segmentation tasks on COCO [26] as these advanced perception tasks benefit substantially from ImageNet pretraining [10].", "startOffset": 230, "endOffset": 234}, {"referenceID": 9, "context": "A question of key importance is if the features learned with large minibatches generalize as well as the features learned with small minibatches? To test this, we adopt the object detection and instance segmentation tasks on COCO [26] as these advanced perception tasks benefit substantially from ImageNet pretraining [10].", "startOffset": 318, "endOffset": 322}, {"referenceID": 13, "context": "We use the recently developed Mask R-CNN [14] system that is capable of learning to detect and segment object instances.", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "We follow all of the hyper-parameter settings used in [14] and only change the ResNet-50 model used to initialize Mask R-CNN training.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "We train Mask RCNN on the COCO trainval35k split and report results on the 5k image minival split used in [14].", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "As an extension of the image-centric Fast/Faster R-CNN [9, 30], Mask R-CNN exhibits different minibatch sizes for different layers: the network backbone uses two images (per GPU), but each image contributes 512 Regionsof-Interest for computing classification (multinomial crossentropy), bounding-box regression (smooth-L1/Huber), and pixel-wise mask (28 \u00d7 28 binomial cross-entropy) losses.", "startOffset": 55, "endOffset": 62}, {"referenceID": 29, "context": "As an extension of the image-centric Fast/Faster R-CNN [9, 30], Mask R-CNN exhibits different minibatch sizes for different layers: the network backbone uses two images (per GPU), but each image contributes 512 Regionsof-Interest for computing classification (multinomial crossentropy), bounding-box regression (smooth-L1/Huber), and pixel-wise mask (28 \u00d7 28 binomial cross-entropy) losses.", "startOffset": 55, "endOffset": 62}, {"referenceID": 15, "context": "In fact, this rule was already used without explicit discussion in [16] and was applied effectively as the default Mask R-CNN training scheme when using 8 GPUs.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "For these experiments, we initialize Mask R-CNN from the released MSRA ResNet-50 model, as was done in [14].", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \u223c90% scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internetscale data with high efficiency.", "creator": "LaTeX with hyperref package"}}}