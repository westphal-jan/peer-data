{"id": "1706.02684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Learning Local Receptive Fields and their Weight Sharing Scheme on Graphs", "abstract": "To achieve state-of-the-art results on challenges in vision, Convolutional Neural Networks learn stationary filters that take advantage of the underlying image structure. Our purpose is to propose an efficient layer formulation that extends this property to any domain described by a graph. Namely, we use the support of its adjacency matrix to design learnable weight sharing filters able to exploit the underlying structure of signals. The proposed formulation makes it possible to learn the weights of the filter as well as a scheme that controls how they are shared across the graph. We perform validation experiments with image datasets and show that these filters offer performances comparable with convolutional ones.", "histories": [["v1", "Thu, 8 Jun 2017 17:03:34 GMT  (48kb)", "http://arxiv.org/abs/1706.02684v1", null], ["v2", "Wed, 20 Sep 2017 14:56:59 GMT  (52kb)", "http://arxiv.org/abs/1706.02684v2", "Camera-ready version, 2017, 5th IEEE Global Conference on Signal and Information Processing, 5 pages, 3 figures, 3 tables"], ["v3", "Thu, 5 Oct 2017 16:32:20 GMT  (22kb)", "http://arxiv.org/abs/1706.02684v3", "To appear in 2017, 5th IEEE Global Conference on Signal and Information Processing, 5 pages, 3 figures, 3 tables"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["jean-charles vialatte", "vincent gripon", "gilles coppin"], "accepted": false, "id": "1706.02684"}, "pdf": {"name": "1706.02684.pdf", "metadata": {"source": "CRF", "title": "Learning Local Receptive Fields and their Weight Sharing Scheme on Graphs", "authors": ["Jean-Charles Vialatte", "Vincent Gripon", "Gilles Coppin"], "emails": ["jean-charles.vialatte@cityzendata.com", "name.surname@imt-atlantique.fr"], "sections": [{"heading": null, "text": "In this context, it should be noted that these filters provide services comparable to conventional systems; they are able to absorb huge amounts of data; they are the gold standard when a lot of data is available; they benefit from the ability to create stationary and multilateral structures, regardless of their position in educational balance sheets; and some authors draw a parallel between these two factors."}, {"heading": "II. RELATED WORK", "text": "Due to the effectiveness of CNNs on image datasets, many authors have proposed models to adapt them to other types of data, such as form datasets [3], molecular datasets [4], or graph datasets [5]. A review is taking place in [7]. In these adaptations of CNNs, the defined input coatings are designed to retain the attributes of a Convolutionary Operator. Specifically, CNNs have been adapted to signal datasets, as in [8], [9] where the conversion in the spectral domain of the graph is formalized as defined by its laplaker [10]. However, with this approach, the generated characteristics do not depend on the values signals locally. This has been mitigated in [11], which also suggest a fast approximate formulation. In [6], the authors define a confusion based on the power of the probability matrix."}, {"heading": "III. METHODOLOGY", "text": "We first recall the basic principles of Deep Neural Networks (DNNs) and CNNs and then present our proposed graph layer."}, {"heading": "A. Background", "text": "DNNs consist of a composition of layers, each of which is parameterized by a learnable weight core W, and a nonlinear function f: R \u2192 R. If the input of such a layer is x, the corresponding output is then: y = f (W \u00b7 x + b), where \u00b7 is the product operator of the matrix, f is applied componentally, and b is a learnable bias vector. The weight cores are learned by means of an optimization routine, usually based on gradient descent, so that the DNN is able to approximate an objective function. A DNN containing only this type of layer is called Multi-Layer Perceptron (MLP). In the case of CNNs, some of the layers have the special form of folding filters. In this case, the folding process can also be written as a product of the input signal with a matrix W, with a W having a toeplitz matrix that results in a very small number of works [16 being very precise]."}, {"heading": "B. Proposed Method", "text": "We propose to introduce a different type of layer, which we call the receptive graph layer. It is based on an adjacency matrix and aims to extend the principle of the revolutionary layers to any domain that can be described with the help of a graph. Let us consider an adjacency matrix A that fits well with the signals to be learned, in the sense that it describes an underlying graph structure between the intermediate characteristics. We define the receptive graph layer associated with A by using the product between a third tensor S and a weight core W. For the moment, the tensor W would be single-stage, containing the weights of the layer, and S is of the form n \u00b7 n \u00b7 \u03c9, where n \u00b7 n is the shape of the adjacence matrix and \u03c9 is the shape of W. At the first two ranks, the support of S must not exceed that of A, so that Aij = \u21d2, Sijk \u00b7 \u03c9 = S = S = S in total (where we get a S = S)."}, {"heading": "C. Training", "text": "The learning process requires learning S and W. We perform the two together. Learning W amounts to learning learning learning weights as in normal CNNs, while Learning S amounts to learning how to bind these weights to the receptive fields. We are also experimenting with a fine-tuning step that involves freezing S in the latter epochs. Based on our inspiration from CNNs, we propose limitations on the parameters of S., namely that we impose them between 0 and 1 and add them up to 1 in the third dimension. Therefore, the vectors in the third level of S can be interpreted to perform a weighted average of the parameters in W. We test two types of initialization: The first consists of distributing one-hot-bit vectors along the third level. We specify that for each receptive field a certain one-hot-bit vector can be distributed at most once more than any other."}, {"heading": "D. Discussion", "text": "For the sake of simplicity, we limited our explanation to square adjacence matrices. In the case of oriented graphs, one could remove the rows and columns of the zeros and obtain a receptive graph with a distinct number of neurons in the input (s) than in the output (m). Consequently, receptive graph layers expand common layers, as explained here: 1) To obtain a fully connected layer, one can choose \u03c9 to be the matrix of vectors nm and S to be the matrix of vectors containing all possible one-hot-bit vectors. 2) To obtain a revolutionary layer, one can choose how large the kernel is. S would be a hot-bit encoded along its third and circulating along its first two ranks. A step > 1 can be achieved by removing the corresponding one-hot-bit vectors. In our case, S is more similar to that layer obtained by taking into account convolutionary layers, which we do not order the neighboring layers with which we recognize."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Description", "text": "In the following experiments, we are interested in comparing different receptive graph layers with conventional ones. To this end, we use image data sets, but limit ourselves to the underlying structure. Note that we look at simple classifiers without pooling and only change the first layer. For each model, the first layer consists of 50 feature cards, followed by a fully connected layer of 300 neurons, terminated by a Softmax layer of 10 neurons. To activate, we use reflected linear units [18], and a dropout of 0.5 is applied to the fully connected layer. In other words, weights are randomly initialized [17]. Regularization with a weight of 10 \u2212 5 is applied to input layers. We present experiments on MNIST [19] first. It contains 10 classes of grayscale images (28x28 pixels) with 60,000 examples for testing."}, {"heading": "B. Experiments with MNIST", "text": "We look at a grid diagram that connects each pixel to itself and its 4 closest neighbors (or less at the boundaries); we also use the square of this diagram (pixels are connected to their 13 closest neighbors, including themselves); the cube of this diagram (25 closest neighbors); up to 10 powers (211 closest neighbors); in this subsection, we use an initialization with a hot bit; we test the model under two arrangements: Either the order of the node is unknown, and then the vectors with a hot bit are randomly distributed and modified during training; either a order of the node is known, and then the vectors with a hot bit are distributed in a circular way in the third order of S, which is frozen in that state; we use the number of the closest neighbors as a unit of measurement for the dimension of the third rank of S. We also compare it with a convolutionary layer of size 5x5, which contains as many weights as the result of the arithmetic diagram we get for the comparison values."}, {"heading": "C. Experiments with Scrambled MNIST", "text": "We select the threshold so that the number of remaining edges corresponds to a certain density p (5x5 turns roughly correspond to a density of p = 3%); we also conclude a graph based on the k nearest neighbors of the inverse of the values of this covariance matrix (k-NN); the latter two do not use predictions about the underlying signal structure; the pixels of the input images are mixed and the same reordering of pixels is used for each image; the dimension of the third level of S is chosen equal to k; the receptive graph layers are also compared with models obtained when replacing the first level with a fully connected or revolutionary plane; the results are shown in Table II; we find that the second receptive graph layer exceeds the CNN level on encrypted MNIST; this indicates that it was able to exploit information about the underlying structure because it was replaced by a fully connected or conventional one."}, {"heading": "D. Experiments with Cifar10", "text": "For the input level, we use the square of the grid diagram as support and consider the order known for initialization, which is similar to a folding in the initial state, except for the shape of the winding window. We are interested in whether the distribution of common weights would be modified by the learning process and whether it would improve performance. We used the same architecture as for MNIST. These are weak classifiers for Cifar10, but they are sufficient to analyze the usefulness of our proposed layer. Exploring better architectures is left for further work. The results are in Table III.The receptive graph layer is able to exceed the corresponding CNN in this configuration by a small amount, paving the way for more complex architectures."}, {"heading": "V. CONCLUSION", "text": "We have introduced a new type of layer for deep neural networks, which consists of using the support of a graph operator and distributing a pool of weights linearly over the defined edges.The linear distribution is learned together with the pool of weights. Thanks to these structural dependencies, we have shown that it is possible to divide weights in a similar way to that of Convolutionary Neural Networks (CNNs).We have conducted experiments with vision data sets where the receptive graph layer achieves a similar performance to the Convolutionary Layer, even if the underlying image structure is not at hand. We believe that with further work, the proposed layer could fully extend the performance of CNNs to many other areas described by a graph. Specifically, with more appropriate regulations for S, it may be possible for them to learn a stronger propagation structure.If a graph is not available, support for S must be closed to other areas."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partly financed by the CominLabs project Neural Communications and by the ANRT (Agence Nationale de la Recherche et de la Technologie) through a CIFRE (Convention Industrielle de Formation par la REcherche)."}], "references": [{"title": "Understanding deep convolutional networks", "author": ["S. Mallat"], "venue": "Philosophical Transactions of the Royal Society A, vol. 374, no. 2065, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Neighborhoodpreserving translations on graphs", "author": ["N. Grelier", "B. Pasdeloup", "J.-C. Vialatte", "V. Gripon"], "venue": "Proceedings of IEEE GlobalSIP, 2016, pp. 410\u2013414.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Shapenet: Convolutional neural networks on non-euclidean manifolds", "author": ["J. Masci", "D. Boscaini", "M. Bronstein", "P. Vandergheynst"], "venue": "2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["D.K. Duvenaud", "D. Maclaurin", "J. Iparraguirre", "R. Bombarell", "T. Hirzel", "A. Aspuru-Guzik", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2215\u20132223.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning, 2016, pp. 2014\u20132023.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Diffusion-convolutional neural networks", "author": ["J. Atwood", "D. Towsley"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 1993\u2013 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Geometric deep learning: going beyond euclidean data", "author": ["M.M. Bronstein", "J. Bruna", "Y. LeCun", "A. Szlam", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1611.08097, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6203, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv preprint arXiv:1506.05163, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine, vol. 30, pp. 83\u201398, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 3837\u20133845.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting semisupervised learning with graph embeddings", "author": ["Z. Yang", "W.W. Cohen", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1603.08861, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "arXiv preprint arXiv:1609.02907, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning the structure of deep convolutional networks", "author": ["J. Feng", "T. Darrell"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2749\u20132757.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Adanet: Adaptive structural learning of artificial neural networks", "author": ["C. Cortes", "X. Gonzalvo", "V. Kuznetsov", "M. Mohri", "S. Yang"], "venue": "arXiv preprint arXiv:1607.01097, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatially-sparse convolutional neural networks", "author": ["B. Graham"], "venue": "arXiv preprint arXiv:1409.6070, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 315\u2013323.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": "1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 2528\u2013 2536.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Some authors draw a parallel between these features and scattering transforms [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "A review is done in [7].", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "In particular, CNNs have been adapted to graph signal datasets, such as in [8], [9] where the convolution is formalized in the spectral domain of the graph defined by its Laplacian [10].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "In particular, CNNs have been adapted to graph signal datasets, such as in [8], [9] where the convolution is formalized in the spectral domain of the graph defined by its Laplacian [10].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "In particular, CNNs have been adapted to graph signal datasets, such as in [8], [9] where the convolution is formalized in the spectral domain of the graph defined by its Laplacian [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "This was alleviated in [11], which also propose a fast approximated formulation.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "In [6], the authors define a convolution based on multiplication with powers of the probability transition matrix.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [5], an ordering of the nodes is used.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Our model is first designed to be applied to the task of graph signal classification, but another common task related to signals on graph is the problem of node classification such as in [6], [12], [13].", "startOffset": 187, "endOffset": 190}, {"referenceID": 11, "context": "Our model is first designed to be applied to the task of graph signal classification, but another common task related to signals on graph is the problem of node classification such as in [6], [12], [13].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "Our model is first designed to be applied to the task of graph signal classification, but another common task related to signals on graph is the problem of node classification such as in [6], [12], [13].", "startOffset": 198, "endOffset": 202}, {"referenceID": 13, "context": "Models learning themselves or part of their structures as we do here have also been proposed, such as in [14], [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "Models learning themselves or part of their structures as we do here have also been proposed, such as in [14], [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Previous works [16] have shown that to obtain the best accuracy in vision challenges, it is better to use very small kernels, resulting in a sparse W .", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "Rectified Linear Units [18] are used for the activations and a dropout of 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "We first present experiments on MNIST [19].", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "Then we present experiments on Cifar10 [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "On MNIST, optimizations are done with ADAM [21] up to 100 epochs.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "For Cifar 10, they are done with ADADELTA [22] up to 350 epochs.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "One example is using gradient descent from the supervised task at hand [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 21, "context": "We can also notice that in our case, this amounts to select receptive fields, breeding another avenue [23].", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "To achieve state-of-the-art results on challenges in vision, Convolutional Neural Networks learn stationary filters that take advantage of the underlying image structure. Our purpose is to propose an efficient layer formulation that extends this property to any domain described by a graph. Namely, we use the support of its adjacency matrix to design learnable weight sharing filters able to exploit the underlying structure of signals. The proposed formulation makes it possible to learn the weights of the filter as well as a scheme that controls how they are shared across the graph. We perform validation experiments with image datasets and show that these filters offer performances comparable with convolutional ones.", "creator": "gnuplot 5.0 patchlevel 6"}}}