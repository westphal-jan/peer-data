{"id": "1706.02686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "What Does a Belief Function Believe In ?", "abstract": "The conditioning in the Dempster-Shafer Theory of Evidence has been defined (by Shafer \\cite{Shafer:90} as combination of a belief function and of an \"event\" via Dempster rule.", "histories": [["v1", "Thu, 8 Jun 2017 17:17:23 GMT  (14kb)", "http://arxiv.org/abs/1706.02686v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andrzej matuszewski", "mieczys{\\l}aw a k{\\l}opotek"], "accepted": false, "id": "1706.02686"}, "pdf": {"name": "1706.02686.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["lAW A. K lOPOTEK"], "emails": ["klopotek@ipipan.waw.pl"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.02 68Conditioning in the Dempster-Shafer evidence theory was defined (by Shafer [15] as a combination of a belief function and an \"event\" via the Dempster rule. Shafer [15], on the other hand, gives a \"probabilistic\" interpretation of a belief function (hence indirectly deriving it from a sample). In view of the fact that the conditional probability distribution of a sample-related probability distribution is a probability distribution derived from a subsample (selected on the basis of a conditioning event), the paper examines the empirical nature of the Dempster combination rule. It is shown that the so-called \"conditional\" belief function is not a belief function due to an event, but a belief function due to the manipulation of original empirical data. In light of this, a different interpretation of the belief function is proposed than the Shafers. Algorithms for the construction of faith networks are derived for this interpretation."}, {"heading": "1 Introduction", "text": "The Dempster-Shafer (DS) theory (DST) or the theory of evidence [14], [3] is considered by many researchers to be a suitable tool to present various aspects of human handling of uncertain knowledge, especially for the representation of partial ignorance [17], although this view has been challenged by various authors (compare the presentations and discussions in the International Journal of Approximate Reasoning (IJAR), specific topics in volume 1990: 4 no. 5 / 6 and volume 1992: 6 no. 3; see also [6], [4]. This paper aims to shed some light on the dispute over the adequacy of the DST from a technical viewpoint of view. The authors of this paper aim to implement an expert system dealing with uncertainty about the DST methodology mixed with the Bayesian approach [7]. Knowledge is represented by a belief network that supports the application of the thought system on the work of Shenoy Shafer [16] network to restore the Shayesian system."}, {"heading": "2 Nature of Conditioning in DST", "text": "Let us consider for a moment how \"empirical\" conditioning can be considered in probability theory. Let's define a probability distribution as relative frequency over a (large) population. Let's look at the apparent condition of an event, let's say that relative frequency is a predicate - a logical expression in variables that describe the population. Then, let's select all objects that correspond to the predicate alpha and the conditional distribution P (| \u03b1) will be the relative frequency within that subpopulation. Let's try to make the same expression in variables that describe the population. Following Shafer [15], we may be tempted to interpret an evaluation of an object that corresponds to a population with a specified population as a statement that our variable interest in that object takes one of the values mentioned in A, but we don't know which of them (and we're not even willing to choose an appropriate subset of A)."}, {"heading": "3 An Alternative View of DST", "text": "We present it informally here: instead of saying that sentence A is expressed causally that \"the variable of interest cannot be observed directly, but only through a particular measurement method (with some particular properties that ensure consistency),\" and sentence A expresses that \"the measurement method yields TRUE when tests are performed, when X = ai (the variable of interest) is performed for all the variable of interest.\" When we make conditioning, the measurement methods will take into account the labeling of objects by performing tests for variable values outside the label. \"In this way, the following is achieved: \u2022 Before and after each conditioning, the interpretation of obligation A is categorized, and the measurement method takes into account the labeling of objects by performing tests on variable values outside the label."}, {"heading": "4 Belief Networks from Data under New Interpreta-", "text": "On the basis of these purely theoretical considerations, an attempt was made to develop some practical algorithms for restoring the faith network structure from data for some limited classes of faith networks. It was started with the most successful structures of Bayesian networks: the tree and the polytree structures. In these efforts, the corresponding Bayesian algorithms were used as a general framework, although details had to be reworked. It is also worth noting that, unlike in the case of probability calculation, a randomized generation of a belief function is not a trivial task due to the data-changing nature of the DS combination. Let us briefly introduce these new algorithms: the algorithm of Chow and Liu [1] for restoring a probability distribution is well known and has been studied in depth, so we will omit its description in this work."}], "references": [{"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory, Vol. IT-14,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1968}, {"title": "Herskovits: A Bayesian method for the induction of probabilistic networks from data, Machine Learning", "author": ["E.G.F. Cooper"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Upper and lower probabilities induced by a multi-valued mapping", "author": ["A.P. Dempster"], "venue": "Ann. Math. Stat", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1967}, {"title": "Halpern: Uncertainty, belief, and probability", "author": ["J.Y.R. Fagin"], "venue": "Comput. Intell", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "Pearl: d-Separation: From theorems to algorithms, in : Uncertainty in Artificial Intelligence 5 (M.Henrion, R.D.Shachter, L.N.Kamal and J.F.Lemmer Eds)", "author": ["D. Geiger", "J.T. Verma"], "venue": "Elsevier Science Publishers B.V. (North-Holland),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Two views of belief: belief as generalized probability and belief as evidence,Artificial Intelligence", "author": ["J.Y. Halpern", "R. Fagin"], "venue": "WHAT DOES A BELIEF FUNCTION BELIEVE IN", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Bayesian and non-Bayesian evidential updating", "author": ["H.E. Kyburg Jr."], "venue": "Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1987}, {"title": "On random sets and belief functions", "author": ["H.T. Nguyen"], "venue": "J. Math. Anal. Appl", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1978}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Influence", "author": ["J. Pearl"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "Pearl: The recovery of causal poly-trees from statistical data, [in] Uncertainty in Artificial Intelligence", "author": ["J.G. Rebane"], "venue": "Elsevier Science Publishers B.V.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "A Mathematical Theory of Evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1976}, {"title": "Pearl eds: Readings in Uncertain Reasoning, (Morgan Kaufmann Publishers Inc", "author": ["G. Shafer"], "venue": "Belief Functions. Introduction,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Axioms for probability and belief-function propagation", "author": ["P.P. Shenoy", "G. Shafer"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Resolving misunderstandings about belief functions, International Journal of Approximate Reasoning 1992:6:321-344", "author": ["Ph. Smets"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Causality from probability, [w:] G.McKee (Ed.): Evolving knowledge in natural and artificial intelligence", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}], "referenceMentions": [{"referenceID": 11, "context": "The conditioning in the Dempster-Shafer Theory of Evidence has been defined (by Shafer [15] as combination of a belief function and of an \u201devent\u201d via Dempster rule.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "On the other hand Shafer [15] gives a \u201dprobabilistic\u201d interpretation of a belief function (hence indirectly its derivation from a sample).", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "The Dempster-Shafer (DS) Theory (DST) or the Theory of Evidence [14], [3] is considered by many researchers as an appropriate tool to represent various aspects of human dealing with uncertain knowledge, especially for representation of partial ignorance [17], though this view has been challenged by various authors (compare the presentations and discussions in International Journal of Approximate Reasoning (IJAR), special issues in Vol.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "The Dempster-Shafer (DS) Theory (DST) or the Theory of Evidence [14], [3] is considered by many researchers as an appropriate tool to represent various aspects of human dealing with uncertain knowledge, especially for representation of partial ignorance [17], though this view has been challenged by various authors (compare the presentations and discussions in International Journal of Approximate Reasoning (IJAR), special issues in Vol.", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "The Dempster-Shafer (DS) Theory (DST) or the Theory of Evidence [14], [3] is considered by many researchers as an appropriate tool to represent various aspects of human dealing with uncertain knowledge, especially for representation of partial ignorance [17], though this view has been challenged by various authors (compare the presentations and discussions in International Journal of Approximate Reasoning (IJAR), special issues in Vol.", "startOffset": 254, "endOffset": 258}, {"referenceID": 5, "context": "3; see also [6], [4].", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "3; see also [6], [4].", "startOffset": 17, "endOffset": 20}, {"referenceID": 12, "context": "Knowledge is represented by a belief network to enable application of the reasoning system based on the work of Shenoy and Shafer [16] (Shenoy-Shafer\u2019s axiomatic framework encompasses both DST and Bayesian-like reasoning scheme).", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 197, "endOffset": 200}, {"referenceID": 9, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 213, "endOffset": 217}, {"referenceID": 1, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 262, "endOffset": 265}, {"referenceID": 14, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 267, "endOffset": 271}, {"referenceID": 13, "context": "We agree with Smets [17] that fundamental deviation of DST from any probabilistic measure of uncertainty lies in the DS rule of combination (\u2295) which serves as a way of conditioning the overall DS distribution on some event (see [15]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "We agree with Smets [17] that fundamental deviation of DST from any probabilistic measure of uncertainty lies in the DS rule of combination (\u2295) which serves as a way of conditioning the overall DS distribution on some event (see [15]).", "startOffset": 229, "endOffset": 233}, {"referenceID": 10, "context": "In this paper we assume that DST notions like basic probability assignment or mass function m, belief function Bel, pseudo-mass and pseudo-belief functions, combination \u2295, marginalization \u2193 and empty extension \u2191 operations are well understood [14], [16].", "startOffset": 243, "endOffset": 247}, {"referenceID": 12, "context": "In this paper we assume that DST notions like basic probability assignment or mass function m, belief function Bel, pseudo-mass and pseudo-belief functions, combination \u2295, marginalization \u2193 and empty extension \u2191 operations are well understood [14], [16].", "startOffset": 249, "endOffset": 253}, {"referenceID": 11, "context": "Following Shafer [15] we may be tempted to interpret a valuation of an object \u03c9 of a population with a set A as a statement that our variable of interest takes for this object one of the values mentioned in A, but we do not know which one of them (and are not ready even to select any proper subset of A).", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "This is, in fact, the way as the \u201dgeneralized probability\u201d in [6], or families of probability distributions in [10] may be understood.", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "This is, in fact, the way as the \u201dgeneralized probability\u201d in [6], or families of probability distributions in [10] may be understood.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "Now let us go over to conditioning on an event {\u03c9|\u03b1(\u03c9)} (after [15]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "If BelB is the simple support function capturing the evidence of the set B (that is mB(B) = 1, mB(A) = 0 for any A 6= B, [15]) then by definition Bel(.", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "|B) = Bel \u2295 BelB is the belief of Bel conditioned on the event B [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "be a kind of random set interpretation [11]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "To demonstrate this a development of a method of a tree/polytree factorization of a joint DS belief distribution for purposes of Shenoy/Shafer uncertainty propagation [16] is briefly outlined.", "startOffset": 167, "endOffset": 171}, {"referenceID": 4, "context": "i=1 Beli\u03c0(i)\u03c0(i) Let, after [5] I(J,K|L)D denote d-separation of J from K by L in a directed acyclic graph D, where J,K and L are three disjoint sets of nodes in this dag D.", "startOffset": 28, "endOffset": 31}, {"referenceID": 14, "context": "We parallel here [18] in formulating the following principles, while understanding independence as defined above Let Vbe a set of random DS variables with a joint DS-belief distribution.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The algorithm of Chow and Liu [1] for recovery of tree structure of a probability distribution is well known and has been deeply investigated, so we will omit its description in this paper.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "The values of a in variable x with parameter p range:[0, 1] By the ternary joint distribution of the variables X1, X2 with background X3 we understand the function:", "startOffset": 53, "endOffset": 59}, {"referenceID": 8, "context": "A well known algorithm for recovery of polytree from data for probability distributions is that of Pearl [12], [13], we refrain from describing it here.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "A well known algorithm for recovery of polytree from data for probability distributions is that of Pearl [12], [13], we refrain from describing it here.", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "via Shenoy/Shafer algorithm [16], then the result will be exactly the same as is the valuation attached to the node n of the network.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "Clearly, the Shenoy/Shafer uncertainty propagation algorithm [16] is fully unaffected by the lack of identity between these two notions of conditioning,", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "But attachment of conditionality to a node of a belief network is important for understanding the contents of a belief network which was invented as a means of representing causal dependencies [18].", "startOffset": 193, "endOffset": 197}, {"referenceID": 12, "context": "Notably, this does not hold for the general view of belief networks (that is without reference to conditionality) presented by Shenoy and Shafer [16].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "To verify the validity of valuation of any node of a general form hypertree considered in [16] one may be forced to consider the entire hypertree at once.", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "The conditioning in the Dempster-Shafer Theory of Evidence has been defined (by Shafer [15] as combination of a belief function and of an \u201devent\u201d via Dempster rule. On the other hand Shafer [15] gives a \u201dprobabilistic\u201d interpretation of a belief function (hence indirectly its derivation from a sample). Given the fact that conditional probability distribution of a sample-derived probability distribution is a probability distribution derived from a subsample (selected on the grounds of a conditioning event), the paper investigates the empirical nature of the Dempsterrule of combination. It is demonstrated that the so-called \u201dconditional\u201d belief function is not a belief function given an event but rather a belief function given manipulation of original empirical data. Given this, an interpretation of belief function different from that of Shafer is proposed. Algorithms for construction of belief networks from data are derived for this interpretation. 2 ANDRZEJ MATUSZEWSKI, MIECZYS lAW A. K lOPOTEK", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}