{"id": "1706.02757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction", "abstract": "Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users' impressions of the robot after a conversation. We find that happiness in the user's recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learning based adaptive human-robot dialogue systems.", "histories": [["v1", "Thu, 8 Jun 2017 20:33:00 GMT  (694kb,D)", "http://arxiv.org/abs/1706.02757v1", "Robo-NLP workshop at ACL 2017. 9 pages, 5 figures, 6 tables"]], "COMMENTS": "Robo-NLP workshop at ACL 2017. 9 pages, 5 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.RO cs.CL cs.HC", "authors": ["jekaterina novikova", "christian dondrup", "ioannis papaioannou", "oliver lemon"], "accepted": false, "id": "1706.02757"}, "pdf": {"name": "1706.02757.pdf", "metadata": {"source": "CRF", "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction", "authors": ["Jekaterina Novikova", "Christian Dondrup", "Ioannis Papaioannou"], "emails": ["o.lemon}@hw.ac.uk"], "sections": [{"heading": null, "text": "Recognition of social signals, from human facial expressions or the prosody of language, is a popular research topic in human-robot interaction studies. There is also a long line of research in the community of spoken dialogues that examines user satisfaction with respect to dialog characteristics. In this paper, we show how different emotional facial expressions of human users, combined with prosodic features of human language and features of human-robot dialogue, correlate with the impressions that users of a robot have after a conversation. We find that happiness in the recognized facial expression of the user strongly correlates with the liking of a robot, while dialog-related characteristics (such as the number of human phrases or phrases per robot statement) correlate with the perception of a robot as intelligent."}, {"heading": "1 Introduction", "text": "Social signals, such as emotional expressions, play an important role in human-robot interaction, so they are increasingly recognized as an important factor to be taken into account both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in spoken dialogue systems (Herm et al., 2008; Meena et al., 2015). Human social signal recognition has become a popular topic in human-robot interaction (HRI) in recent years. Social signals are well recognized by human facial expressions or prosodic features of language (Ekman, 2004; Zeng et al., 2009) and have become the most popular methods for recognizing human affective signals in human-robot interaction (Ra'zuri et al al al al al., 2015; Devillers et al al al., 2015; Cid et al al., 2013).In human-robot interaction, human postcognition and improvement are mostly used."}, {"heading": "2 Experiment Setup and Evaluation", "text": "The human-robot dialogue system was evaluated by means of a user study in which human subjects interacted with a Pepper robots1, which acted autonomously with the system described in (Papaioannou and Lemon, 2017; Papaioannou et al., 2017).The dialog system used combines task-based with chat-based dialogue functions and decides the most appropriate measures based on a pre-trained Reinforcement Learning (RL) policy.When a task is detected, the robot decides within a pool of possible actions in which A = [PerformTask, Greet, Goodbye, Chat, GiveDirections, Wait, RequestTask, RequestShop].When a task is detected, 1http: / / doc.aldebaran.com / 2-5 / home _ pepper.htmlin synthesizes the user statements (e.g. \"where can I find Discounts\"), a response is predefined using database search and statements such as the example in 3."}, {"heading": "2.1 Experimental Scenario", "text": "The task chosen in the study and the setup were considered as the first steps to understand how a humanoid social robot should behave in the context of a shopping mall, while providing useful information for visitors to the mall. To this end, participants were asked to imagine entering a mall where they had never been before, where the robot was installed in the entrance area, and interacting with visitors. Participants were asked to complete as many of the following five tasks as possible: \u2022 receive information from the robot, where to get a coffee. \u2022 Get information from the robot, where to buy clothing. \u2022 get directions to the clothing store of their choice. \u2022 find out if there are current sales or discounts in the mall and try to get a voucher from the robot. \u2022 Take a selfie with the robot."}, {"heading": "2.2 Participants and Experimental Design", "text": "41 people (13 women, 28 men) participated in our study, who were between the ages of 18 and 38 (M = 24.46, SD = 4.72), most of whom were students (93% students and 7% employees) who had little or no prior experience with robots (56% with little or no experience, 39% with some experience and 5% with a lot of experience).Participants were initially given a briefing script describing the goal of the task and giving clues on how to communicate better with the roboti.2http: / / github.com / pandorabots / rosieon, such as \"wait until your turn to talk\" and \"Please remember that the robot only listens to you while its eyes flash blue\" 3. We assured our participants that we were testing the robot, not them, and controlled ambient distortions by avoiding non-task distractions during the experiment. During the experimental sessions, the participants would stand in front of the robot and the other one would need help in a corner (but the other was in the corner)."}, {"heading": "2.3 Measured Variables", "text": "We collected a set of objective metrics from the log files, video and audio recordings of the complex interactions and intelligent intelligence transcripts for dialogues. From the audio recordings, we collected a set of different prosodic and dialogue-related features. From the video recordings, we collected data on emotional intensities based on human facial expressions. From the dialogue transcripts, we collected a set of linguistic characteristics, such as lexical diversity, length of utterances, etc. In addition, we looked at a set of subjective metrics for a qualitative assessment, asking participants to complete a questionnaire after each interaction session to assess their perception of the robot. Emotions were detected and recognized using the Microsoft Emotion API for Video4. This API takes video frames as input (see Figure 2) and gives confidence about a set of emotions for the group of faces over a period of time."}, {"heading": "3 Multimodal Data Collection and Analysis", "text": "The data collected during the experiment required additional processing, alignment and annotation, as shown in Figure 3. Prosodic features of F0 and dialog-related features, showing the presence and absence of pauses and the presence / absence of language, were collected from audio recordings at a rate of 44,100 samples per second. Emotional intensity values were collected from video recordings at a rate of 25 frames per second. All data were aligned according to the recording, using averages of prosodic features per frame. Subsequently, the data were recorded in ELAN5 to detect associations between an utterance and its owners. Finally, the dialog texts were transcribed and linguistic features were aligned using R packets stringr, stringi, tidytext and qdap.A summary of the collected data is provided in Table 1. Specifically, the summary results show that the F0 value of human language varies as much as twice the average value during the course of a conversation."}, {"heading": "4 Linguistic Analysis of Dialogues", "text": "Following Gardent et al. (2017), we analyze the textual data of the dialogue in terms of the length of correspondences, lexical richness and syntactic variation. The results are summarized in tables 1 and 2 and grouped by a speaker, i.e. robot and human."}, {"heading": "4.1 Length of utterances", "text": "The results presented in Table 1 show that robot utterances are significantly longer than those of their human interlocutors, both in terms of words per utterance and sentences per utterance. This can be explained in part by the fact that a turning process was not very natural and therefore did not always succeed during the dialogue. It usually took some time for people to learn how to communicate properly with Pepper and to start talking to the robot only when the robot was listening. As a result, people were interrupted from time to time by the robot while never trying to interrupt the robot itself. Shorter average length of human utterances, on the other hand, is also due to the way in which humans tend to deal with inconsistencies in a dialogue, e.g. by reformulating and shortening their previous utterance to emphasize the most important keywords (see an example in Table 3). Robot utterances, however, have not been shortened or altered in other ways in which people deal with disfluid dialogues, e.g. by their most important utterances (see Table 3)."}, {"heading": "4.2 Lexical Richness", "text": "We used the Lexical Complexity Analyser (Lu, 2009) to measure various dimensions of lexical wealth, such as lexical complexity, lexical diversity, and the mean segment type-token ratio. We supplement the traditional measurement of lexical diversity type-token ratio (TTR) with the more robust measurement of the mean segment type-token ratio (MSTTR) (Lu, 2012), which splits all dialogs into consecutive segments of a given length and then calculates the average TTR of all segments. The higher the value of the MSTTR, the more varied is the measured text. We also measure the lexical complexity (LS), also known as lexical rarity, which is calculated as a proportion of lexical word types that are not included in the list of 2,000 common words that we measure per national lexical diversity (the national lexical D)."}, {"heading": "4.3 Syntactic Variation and Discourse Phenomena", "text": "We used the D-Level Analyser (Lu, 2009) to evaluate syntactical variations and complexity of human references against the revised D-Level Scale (Lu, 2014), which has eight levels of syntactical complexity, with levels 0 and 1 containing simple or incomplete sentences and higher levels containing more complex structures. Figure 5 shows a similar syntactical variation of human and robot language, although there are slight differences, for example, humans tend to use a higher percentage of both the simplest and most complex sentences. Generally, the majority of all sentences used by both humans and robots are simple sentences. This is because the subject of a human-robot conversation is quite simple and does not require a lot of complicated syntactical structures."}, {"heading": "5 Correlation between Robot Ratings and Multimodal Features of Human-Robot Dialogue", "text": "A summary of the correlation results is presented in Table 4. Results show that different groups of traits correlate with different groups of human assessments. For example, emotional traits, such as the intensity of happiness, correlate strongly with the perceived anthropomorphism of a robot, so that a person who perceives their happiness during a dialogue with the robot is likely to be kinder and nicer. Human assessments of robot anthropomorphism also correlate with a lexical diversity of human language: humans tend to talk to a robot that perceives them as conscious, natural and human (see an example in Table 5)."}, {"heading": "6 Predicting Perception of Robots in Human-Robot Dialogue", "text": "In order to develop a model that predicts potential human ratings for robotic likeability and perceived intelligence, we use the previously discussed prosodic features, dialogue-related features, and determined emotional intensities as predictive features of the model. For the prediction itself, we use ensemble learning (Random Forest, RF) (Breiman, 2001), which is a state-of-the-art algorithm that can be applied in a dynamic dialogue situation and is able to combine the respective strengths of different informative features in a single model. Setup: We use a 70 / 30% split for training and testing and 10-fold cross-validation of training data to match the optimal number of predictions for growing trees. 100 trees have been combined with 2 variables randomly selected as candidates for each split. We examine five different models that are used as predictors: 1) emotional intensities, 2) prosodic features, 3) dialog characteristics, 4) non-dialog characteristics."}, {"heading": "7 Discussion and Conclusions", "text": "In this paper, we show how dialog functions correlate with the user's perception of a robot (e.g. a strong correlation between a higher number of human phrases and the perceived intellect of the robot or between a higher number of sentences per robot utterance and the perceived ignorance of the robot), as well as correlations between emotional characteristics and robot likeability. Based on the results described in this article, a predictive model with emotional intensities (happiness, sadness and surprise) could be implemented to better predict the perception of the robot user. This model can provide valuable information on how to design more appealing dialogues between robots and humans. Combining these emotional characteristics, together with the dialog-related characteristics (both linguistic and non-linguistic) and the F0 value, can also provide better feedback in cases where, for example, a smile would generate ambiguity of the emotional representation of the perceived user (these can be used to create an important piece of work in 2012), in order to determine emotional characteristics."}], "references": [{"title": "Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots", "author": ["Christoph Bartneck", "Dana Kuli\u0107", "Elizabeth Croft", "Susana Zoghbi."], "venue": "International journal of social robotics 1(1):71\u201381.", "citeRegEx": "Bartneck et al\\.,? 2009", "shortCiteRegEx": "Bartneck et al\\.", "year": 2009}, {"title": "A real time and robust facial expression recognition and imitation approach for affective human-robot interaction using gabor filtering", "author": ["Felipe Cid", "Jos\u00e9 Augusto Prado", "Pablo Bustos", "Pedro Nunez."], "venue": "Intelligent Robots and Systems (IROS), 2013", "citeRegEx": "Cid et al\\.,? 2013", "shortCiteRegEx": "Cid et al\\.", "year": 2013}, {"title": "Inference of human beings emotional states from speech in human\u2013 robot interactions", "author": ["Laurence Devillers", "Marie Tahon", "Mohamed A Sehili", "Agnes Delaborde."], "venue": "International Journal of Social Robotics 7(4):451\u2013463.", "citeRegEx": "Devillers et al\\.,? 2015", "shortCiteRegEx": "Devillers et al\\.", "year": 2015}, {"title": "Emotional and conversational nonverbal signals", "author": ["Paul Ekman."], "venue": "Language, knowledge, and representation, Springer, pages 39\u201350.", "citeRegEx": "Ekman.,? 2004", "shortCiteRegEx": "Ekman.", "year": 2004}, {"title": "Creating training corpora for micro-planners", "author": ["C. Gardent", "A. Shimorina", "S. Narayan", "L. PerezBeltrachini."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). To appear.", "citeRegEx": "Gardent et al\\.,? 2017", "shortCiteRegEx": "Gardent et al\\.", "year": 2017}, {"title": "Ambiguous language and differences in beliefs", "author": ["Joseph Y. Halpern", "Willemien Kets."], "venue": "CoRR abs/1203.0699. http://arxiv.org/abs/1203.0699.", "citeRegEx": "Halpern and Kets.,? 2012", "shortCiteRegEx": "Halpern and Kets.", "year": 2012}, {"title": "When calls go wrong: How to detect problematic calls based on log-files and emotions", "author": ["Ota Herm", "Alexander Schmitt", "Jackson Liscombe"], "venue": "In Ninth Annual Conference of the International Speech Communication Association", "citeRegEx": "Herm et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Herm et al\\.", "year": 2008}, {"title": "Construction and evaluation of a user experience questionnaire", "author": ["Bettina Laugwitz", "Theo Held", "Martin Schrepp."], "venue": "Symposium of the Austrian HCI and Usability Engineering Group. Springer, pages 63\u201376.", "citeRegEx": "Laugwitz et al\\.,? 2008", "shortCiteRegEx": "Laugwitz et al\\.", "year": 2008}, {"title": "Automatic measurement of syntactic complexity in child language acquisition", "author": ["Xiaofei Lu."], "venue": "International Journal of Corpus Linguistics 14(1):3\u201328.", "citeRegEx": "Lu.,? 2009", "shortCiteRegEx": "Lu.", "year": 2009}, {"title": "The relationship of lexical richness to the quality of esl learners oral narratives", "author": ["Xiaofei Lu."], "venue": "The Modern Language Journal 96(2):190\u2013208.", "citeRegEx": "Lu.,? 2012", "shortCiteRegEx": "Lu.", "year": 2012}, {"title": "Computational methods for corpus annotation and analysis", "author": ["Xiaofei Lu."], "venue": "Springer.", "citeRegEx": "Lu.,? 2014", "shortCiteRegEx": "Lu.", "year": 2014}, {"title": "Automatic detection of miscommunication in spoken dialogue systems", "author": ["Raveesh Meena", "Jos\u00e9 Lopes Gabriel Skantze", "Joakim Gustafson."], "venue": "16th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 354.", "citeRegEx": "Meena et al\\.,? 2015", "shortCiteRegEx": "Meena et al\\.", "year": 2015}, {"title": "Emotionally expressive robot behavior improves human-robot collaboration", "author": ["Jekaterina Novikova", "Leon Watts", "Tetsunari Inamura."], "venue": "Robot and Human Interactive Communication (RO-MAN), 2015 24th IEEE International Symposium on. IEEE,", "citeRegEx": "Novikova et al\\.,? 2015", "shortCiteRegEx": "Novikova et al\\.", "year": 2015}, {"title": "Hybrid chat and task dialogue for more engaging hri using reinforcement learning", "author": ["Ioannis Papaioannou", "Christian Dondrup", "Jekaterina Novikova", "Oliver Lemon."], "venue": "Robot and Human Interactive Communication (RO-MAN), 2017 26th IEEE", "citeRegEx": "Papaioannou et al\\.,? 2017", "shortCiteRegEx": "Papaioannou et al\\.", "year": 2017}, {"title": "Combining Chat and Task-Based Multimodal Dialogue for More Engaging HRI: A Scalable Method Using Reinforcement Learning", "author": ["Ioannis Papaioannou", "Oliver Lemon."], "venue": "Proceedings of the Companion of the 2017 ACM/IEEE International", "citeRegEx": "Papaioannou and Lemon.,? 2017", "shortCiteRegEx": "Papaioannou and Lemon.", "year": 2017}, {"title": "Speech emotion recognition in emotional feedbackfor human-robot interaction", "author": ["Javier G R\u00e1zuri", "David Sundgren", "Rahim Rahmani", "Antonio Moran", "Isis Bonet", "Aron Larsson."], "venue": "International Journal of Advanced Research in Artificial Intelligence", "citeRegEx": "R\u00e1zuri et al\\.,? 2015", "shortCiteRegEx": "R\u00e1zuri et al\\.", "year": 2015}, {"title": "Modeling and predicting quality in spoken human-computer interaction", "author": ["Alexander Schmitt", "Benjamin Schatz", "Wolfgang Minker."], "venue": "Proceedings of the SIGDIAL 2011 Conference. Association for Computational Linguistics, pages 173\u2013184.", "citeRegEx": "Schmitt et al\\.,? 2011", "shortCiteRegEx": "Schmitt et al\\.", "year": 2011}, {"title": "Interaction quality: assessing the quality of ongoing spoken dialog interaction by expertsand how it relates to user satisfaction", "author": ["Alexander Schmitt", "Stefan Ultes."], "venue": "Speech Communication 74:12\u201336.", "citeRegEx": "Schmitt and Ultes.,? 2015", "shortCiteRegEx": "Schmitt and Ultes.", "year": 2015}, {"title": "Emulating empathy in socially assistive robotics", "author": ["Adriana Tapus", "Maja J Mataric."], "venue": "AAAI Spring Symposium: Multidisciplinary Collaboration for Socially Assistive Robotics. pages 93\u201396.", "citeRegEx": "Tapus and Mataric.,? 2007", "shortCiteRegEx": "Tapus and Mataric.", "year": 2007}, {"title": "Adaptive emotional expression in robot-child interaction", "author": ["Myrthe Tielman", "Mark Neerincx", "John-Jules Meyer", "Rosemarijn Looije."], "venue": "Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction. ACM, pages 407\u2013", "citeRegEx": "Tielman et al\\.,? 2014", "shortCiteRegEx": "Tielman et al\\.", "year": 2014}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Zhihong Zeng", "Maja Pantic", "Glenn I Roisman", "Thomas S Huang."], "venue": "IEEE transactions on pattern analysis and machine intelligence 31(1):39\u201358.", "citeRegEx": "Zeng et al\\.,? 2009", "shortCiteRegEx": "Zeng et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "tant factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al.", "startOffset": 70, "endOffset": 135}, {"referenceID": 12, "context": "tant factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al.", "startOffset": 70, "endOffset": 135}, {"referenceID": 2, "context": "tant factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al.", "startOffset": 70, "endOffset": 135}, {"referenceID": 6, "context": ", 2015) and in the area of spoken dialogue systems (Herm et al., 2008; Meena et al., 2015).", "startOffset": 51, "endOffset": 90}, {"referenceID": 11, "context": ", 2015) and in the area of spoken dialogue systems (Herm et al., 2008; Meena et al., 2015).", "startOffset": 51, "endOffset": 90}, {"referenceID": 3, "context": "Social signals are recognized well from human facial expressions or prosodic features of speech (Ekman, 2004; Zeng et al., 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al.", "startOffset": 96, "endOffset": 128}, {"referenceID": 20, "context": "Social signals are recognized well from human facial expressions or prosodic features of speech (Ekman, 2004; Zeng et al., 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al.", "startOffset": 96, "endOffset": 128}, {"referenceID": 15, "context": ", 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al., 2015; Devillers et al., 2015; Cid et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 2, "context": ", 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al., 2015; Devillers et al., 2015; Cid et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 1, "context": ", 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al., 2015; Devillers et al., 2015; Cid et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 19, "context": "In human-robot interaction, recognized human emotions are mostly used for mimicking human behaviour and enhancing the empathy towards a robot both in children (Tielman et al., 2014) and ar X iv :1 70 6.", "startOffset": 159, "endOffset": 181}, {"referenceID": 18, "context": "in adult users (Tapus and Mataric, 2007).", "startOffset": 15, "endOffset": 40}, {"referenceID": 6, "context": "In the area of spoken dialogue systems, signals recognised from linguistic cues and prosody have been used to detect problematic dialogues (Herm et al., 2008) and to assess dialogue quality as a whole (Schmitt and Ultes, 2015).", "startOffset": 139, "endOffset": 158}, {"referenceID": 17, "context": ", 2008) and to assess dialogue quality as a whole (Schmitt and Ultes, 2015).", "startOffset": 50, "endOffset": 75}, {"referenceID": 11, "context": "This type of dialogue-related signals has also been used to automatically detect miscommunication (Meena et al., 2015), or to predict the user satisfaction (Schmitt et al.", "startOffset": 98, "endOffset": 118}, {"referenceID": 16, "context": ", 2015), or to predict the user satisfaction (Schmitt et al., 2011).", "startOffset": 45, "endOffset": 67}, {"referenceID": 14, "context": "The human-robot dialogue system was evaluated via a user study in which human subjects interacted with a Pepper robot1 acting autonomously using the system described in (Papaioannou and Lemon, 2017; Papaioannou et al., 2017).", "startOffset": 169, "endOffset": 224}, {"referenceID": 13, "context": "The human-robot dialogue system was evaluated via a user study in which human subjects interacted with a Pepper robot1 acting autonomously using the system described in (Papaioannou and Lemon, 2017; Papaioannou et al., 2017).", "startOffset": 169, "endOffset": 224}, {"referenceID": 7, "context": "The questionnaire was based on a combination of the User Experience Questionnaire UEQ (Laugwitz et al., 2008) and the Godspeed Questionnaire (Bartneck et al.", "startOffset": 86, "endOffset": 109}, {"referenceID": 0, "context": ", 2008) and the Godspeed Questionnaire (Bartneck et al., 2009).", "startOffset": 39, "endOffset": 62}, {"referenceID": 4, "context": "Following Gardent et al. (2017), we analyse the dialogue textual data in terms of length of ut-", "startOffset": 10, "endOffset": 32}, {"referenceID": 8, "context": "We used the Lexical Complexity Analyser (Lu, 2009) to measure various dimensions of lexical richness, such as lexical sophistication, lexical diversity and mean segmental type-token ratio.", "startOffset": 40, "endOffset": 50}, {"referenceID": 9, "context": "We complement the traditional measure of lexical diversity type-token ratio (TTR) with the more robust measure of mean segmental type-token ratio (MSTTR) (Lu, 2012), which divides all the dialogues into successive segments of a given length and then calculates the average TTR of all segments.", "startOffset": 154, "endOffset": 164}, {"referenceID": 8, "context": "We used the D-Level Analyser (Lu, 2009) to evaluate syntactic variation and complexity of human references using the revised D-Level Scale (Lu, 2014).", "startOffset": 29, "endOffset": 39}, {"referenceID": 10, "context": "We used the D-Level Analyser (Lu, 2009) to evaluate syntactic variation and complexity of human references using the revised D-Level Scale (Lu, 2014).", "startOffset": 139, "endOffset": 149}, {"referenceID": 5, "context": "The combination of these emotional features, along with the dialogue-related features (both linguistic and nonlinguistic) and the F0 value can also provide better feedback in cases where, for instance, a smile can create ambiguity of the perceived user\u2019s emotional display (Halpern and Kets, 2012).", "startOffset": 273, "endOffset": 297}], "year": 2017, "abstractText": "Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users\u2019 impressions of the robot after a conversation. We find that happiness in the user\u2019s recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learningbased adaptive human-robot dialogue systems. Figure 1: Left: a live view of experimental setup showing a participant interacting with Pepper. Right: a diagram of experimental setup showing the participant (green) and the robot (white) positioned face to face. The scene was recorded by cameras (triangles C) from the robot\u2019s perspective focusing on the face of the participant and from the side, showing the whole scene. The experimenter (red) was seated behind a divider.", "creator": "LaTeX with hyperref package"}}}