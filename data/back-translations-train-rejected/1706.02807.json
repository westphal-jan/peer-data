{"id": "1706.02807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Learning to Embed Words in Context for Syntactic Tasks", "abstract": "We present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.", "histories": [["v1", "Fri, 9 Jun 2017 01:39:12 GMT  (1172kb)", "https://arxiv.org/abs/1706.02807v1", null], ["v2", "Mon, 12 Jun 2017 01:42:12 GMT  (1172kb)", "http://arxiv.org/abs/1706.02807v2", "Accepted by ACL 2017 Repl4NLP workshop"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lifu tu", "kevin gimpel", "karen livescu"], "accepted": false, "id": "1706.02807"}, "pdf": {"name": "1706.02807.pdf", "metadata": {"source": "CRF", "title": "Learning to Embed Words in Context for Syntactic Tasks", "authors": ["Lifu Tu Kevin Gimpel", "Karen Livescu"], "emails": ["lifu@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.02 807v 2 [cs.C L] June 12, 2017 in the context of surrounding words. Such models, which we call token embeddings, represent the properties of a word that are specific to a particular context, such as meaning of the word, syntactic category, and semantic role. We study simple, efficient token embeddings based on common neural network architectures. We learn token embeddings on a large amount of uncommented text and rate them as features for part-of-speech taggers and dependency savers trained on much smaller amounts of commented data."}, {"heading": "1 Introduction", "text": "Word embeddings have enjoyed a surge of popularity in natural language processing (NLP) due to the effectiveness of deep learning and the availability of pre-trained, downloadable models for embedding words. Many embedding models are developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been improved performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).The majority of this work has focused on a single embedding for each word type in a vocabulary ararary.1 We will refer to this as type - 1A word type is an entry in a vocabulary."}, {"heading": "2 Related Work", "text": "Most of them are able to get used to the work they are able to do, the work they are able to do."}, {"heading": "3 Token Embedding Models", "text": "For each word type x-W we define its embedding by vx-R. We define a word sequence x = < x1, x2,..., x | x | > in which each entry xj is a word type, i.e. xj-W. We define a word symbol as an element in a word sequence. We consider the class of functions f, which take a word sequence x and an index j of a particular symbol in x and output a vector of dimensionality d. We refer to decisions for f (x, j) as an encoder."}, {"heading": "3.1 Feedforward Encoders", "text": "Our first encoder is a basic feedback neural network that embeds the sequence of words contained in a text window around word j. We use a fixed-size window that contains word j, the w \u2032 words on the left, and the w \u2032 words on the right. We concatenate the vectors for each word type in that window and apply an affine transformation, followed by nonlinearity: fFF (x, j) = g (W (D) [vxj \u2212 w \u2032; vx (j \u2212 w \u2032) + 1;...; vxj + w \u2032] + b (D)), where g is an elementary non-linear function (e.g. tanh), W (D) a d \u2032 by d (2w \u2032 + 1) parameter matrix, semicolon (;) stands for vertical concatenation, and b (D) \u2022 Rd \u00b2 is a distortion vector. We assume that a sequence of x and end poles is generated."}, {"heading": "3.2 Recurrent Neural Network Encoders", "text": "We are therefore also looking at encoders based on recurrent neural networks (RNNs). RNNs have recently enjoyed great interest in the communities of deep learning, speech recognition and NLP (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), which are most commonly used with \"gated\" connections such as long-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000). We use an LSTM to encode the word sequence that contains the token, and take the last hidden vector as the d \u00b2 dimensional encoding. While we can use longer sequences, such as the sentence that contains the token (Kawakami and Dyer, 2015), we limit the input sequence to a well-defined context window around word j, so that input with the pre-chronized context we find not helpful for identifying the large window."}, {"heading": "3.3 Training", "text": "We assume that we get a corpus X = {x (i)} | X | i = 1 of uncommented word sequences. A common family of unattended criteria is the reconstruction error and its variants, which are used in the formation of autocoders that use an encoder f to convert the input x into a vector, followed by a decoder g that tries to reconstruct the input from the vector. Typical loss function is the square difference between the input and the reconstructed input. We use a generalization that is sensitive to the position of the elements. As our primary interest is to learn useful representations of a particular character in its context, we use a weighted reconstruction error: LlossWRE (f, g, x, j) = 1formang (f, j) -vector (2) -vector (1) -vector)."}, {"heading": "4 Qualitative Analysis", "text": "Before discussing downstream tasks, we perform a qualitative analysis to show what our token embedding models learn."}, {"heading": "4.1 Experimental Setup", "text": "We train a feedback DNN token embedding model on a body of 300,000 unlabeled English tweets. We use a window size w \u00b2 = 3 for the qualitative results given here; for downstream tasks, we will vary w \u00b2. For training, we use our weighted reconstruction error (Eq. 1). The encoder uses a hidden layer of size 512 followed by the token embedding layer of size d \u00b2 = 256. The decoder also uses a single hidden layer of size 512. We use ReLU activations except for the final encoder / decoder layers that use linear activations.In preliminary experiments, we compared 3 weighting schemes for the target: for the token index j, \"uniform\" weighting sets \u03c9i = 1 for all i; \"focused\" sets \u03c9j = 2 and perspeci = 1 for i 6 = j; and \"tapered\" sets \"= 1 for the skimming model."}, {"heading": "4.2 Nearest Neighbor Analysis", "text": "We test the encoder's ability to distinguish between different senses of ambiguous types. Table 1 shows query marks (Q) followed by their four closest adjacent marks (of the same type), all of which come from our cherished set of 3,000 tweets. We choose two ambiguous words common in tweets: \"2\" and \"so.\" As a query, we select tokens that express different senses. The word \"2\" can be both a number (left) and a synonym for \"to\" (right). The word \"so\" is both an amplifier (left) and a connecting (right). We find that, although generally different in context, the closest neighbors have the same meaning and the same POS tag. In Table 2, we consider the closest neighbors who may have different word types from the query type. For each query word, we allow searching for the closest neighbor, although they are generally different in context, as the same POS tag."}, {"heading": "4.3 Visualization", "text": "To gain a better qualitative understanding of the token embeddings, we visualize the learned token embeddings using tSNE (Maaten and Hinton, 2008). We learn token embeddings as above, except with w \u2032 = 1. Figure 1 shows a two-dimensional visualization of the token embeddings for the word type \"4.\" For this visualization, we embed tokens into the POSannotated Tweet datasets of Gimpel et al. (2011) and Owoputi et al. (2013), so that we have their gold standard POS tags. We show the left and right context words (with w \u2032 = 1) together with the token and its gold standard POS tag. We find that tokens of \"4\" with the same gold POS tag are close together in the embedded space, with prepositions appearing in the upper part of the plot and numbers in the lower part."}, {"heading": "5 Downstream Tasks", "text": "We evaluate our token embedding models based on two downstream tasks: POS tagging and dependency parsing. Given an input sequence x = < x1, x2,..., xn >, we want to predict their tag sequence and dependency parsing. We focus on Twitter because there is limited commented data, but plenty of unlabeled data for embedding training tokens."}, {"heading": "5.1 Part-of-Speech Tagging", "text": "It is a local classifier that predicts the tag for a token independently of any other predictions for the tweet. DNN contains two hidden layers followed by a softmax layer. Figure 2 (a) shows this architecture forw = 1 in predicting the tag of 4 in the tweet thanks to 4. We associate a 10-dimensional binary feature vector that is calculated for the word being tagged (Table 3). 2We train the tagger by minimizing the log loss (transverse entropy) on the training set by stopping the validation set early and reporting accuracy on the test set. We consider both learning the type embedding (\"upingsing\") and defining punctuation on the training set by specifying the embedding of the bed."}, {"heading": "5.2 Dependency Parser", "text": "That is, we use a local classifier that evaluates the parents for a word. To derive a parser at the test date, we independently select the highest-value parent for each word. This unit corresponds to a value S (xi, xj) that serves as a score for a dependency sheet with the child's word xi and the parent's word xj. Input to the DNN is the concatenation of type embeddings for xi and xj, the type embeddings of w words on both sides of xi and xj, the characteristics for a dependency sheet with the child's word xi and the parent's word xj. Input to the DNN is the concatenation of type embeddings for xi and xj, the type embeddings of w on both sides of xi and xj, the characteristics for xi and xj from Table 3, and the characteristics for the pair, including the relative positions, direction, and distance (shown in Table 4).3For a sentence of length of the function, we consider the loss of the root of the pair if the pair 3k is included."}, {"heading": "6 Experimental Setup", "text": "The only difference is that we train the symbol embedding models for 5 epochs, saving the model that achieves the best objective value back to a reserved set of 3,000 unlabeled tweets. We also experiment with multiple values for the context window size w \u00b2 and the hidden layer size reported below. 4We found that this works better than just adding the exposed values of an arc or arc for the pair < xi, xj >.i n j in an n-word sentence < xi, xj >."}, {"heading": "6.1 Part-of-Speech Tagging", "text": "We use the annotated Tweet datasets from Gimpel et al. (2011) and Owoputi et al. (2013). For training purposes, we combine the 1000 Tweet test set OCT27TRAIN and the 327 Tweet development set OCT27DEV. For validation, we use the 500 Tweet test set OCT27TEST and for the final test, the 547 Tweet test set DAILY547. The DNN tagger uses two hidden layers of size 512 with ReLU nonlinearity and a final Softmax layer of size 25 (one per day)."}, {"heading": "6.2 Dependency Parsing", "text": "We use data from Kong et al. (2014) and randomly divide their 717 training tweets into a 573-tweet train set and a 144-tweet validation set. We use their 201-tweet TEST-NEW as a test set. Kong et al. comment on whether certain tokens are included in the syntactic structure of each tweet (\"token selection\"). We use the same automatic token selection predictions (TS) as they are, which are 97.4% accurate. We use a pipeline architecture in which unselected tokens are not considered as possible parents when we do the summing in Equation 2 or the Argmax in Equation 4. Like Kong et al., we use gold standard POS tags and gold standard TS during training and tuning. For final testing on TEST-NEW, we automatically use predicted POS tags and automatic TS tags (using their same automatic predictions for both). Similarly, we use the annex F1 for the end speed of the XN (our DNS score)."}, {"heading": "7 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Part-of-Speech Tagging", "text": "We first train our basic data without using the basic data to collect the best possible number of training data and window sizes. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. We start with the results of our training. \"TokenEmbedding (w + w)\" refers to the token embedding with tagger context of tagger words and token embedding contexts. \"If we use only 10% of the training data, we will combine the basic data with the best context of the training."}, {"heading": "7.2 Dependency Parsing", "text": "The predictors with token embeddings are able to take advantage of a larger context: with DNN token embeddings, performance is best with w \"= 1, while with seq2seq token embeddings, performance is strong with w\" = 1 and 2. When using token embeddings, we actually found it beneficial to remove the central word type embeddings from input and use them only indirectly via the token embeddings functionality. We use w = \u2212 1 to display these settings.The upper part of Table 8 shows the results if we simply use our parser embeddings to output the parents with the highest score for each word in the test set. Token embeddings are more helpful for this task than type embeddings, improving performance from 73.0 to 75.8 for DNN token embeddings, and improving to 75.0 for the token embeddings in the test set."}, {"heading": "8 Conclusion", "text": "We presented a simple and efficient way to learn how to present words in their context using unmarked data, and how to use them to improve Twitter's syntactical analysis. Qualitatively, our token embedding encodes meaning and POS information, and groups tokens of different types with similar meanings in context. In quantitative terms, using token embedding in simple predictors continuously improves performance and even competes with the performance of highly structured predictive bases. Our code and trained token embedding models are publicly available on authors \"websites."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers Chris Dyer and Lingpeng Kong. We also thank the developers of Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) as well as the NVIDIA Corporation for donating GPUs used in this research."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. of ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Context-dependent word representation for neural machine translation", "author": ["Heeyoul Choi", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Choi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Text normalization in social media: progress, problems and applications for a pre-processing system of casual English", "author": ["Eleanor Clark", "Kenji Araki"], "venue": "Procedia-Social and Behavioral Sciences", "citeRegEx": "Clark and Araki.,? \\Q2011\\E", "shortCiteRegEx": "Clark and Araki.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le"], "venue": "In Advances in NIPS", "citeRegEx": "Dai and Le.,? \\Q2015\\E", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Lasagne: First release. http://dx.doi.org/10.5281/zenodo.27878", "author": ["Sander Dieleman", "Jan Schl\u00fcter", "Colin Raffel", "Eben Olson", "S\u00f8ren Kaae S\u00f8nderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg", "Jack Kelly"], "venue": null, "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural Computation", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Semi supervised preposition-sense disambiguation using multilingual data", "author": ["Hila Gonen", "Yoav Goldberg"], "venue": "In Proc. of COLING", "citeRegEx": "Gonen and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Gonen and Goldberg.", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Proc. of ICASSP", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of COLING", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric Huang", "Richard Socher", "Christopher D. Manning", "Andrew Ng"], "venue": "In Proc. of ACL", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Ontologically groundedmulti-sense representation learning for semantic vector space models", "author": ["Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Neural context embeddings for automatic discovery of word senses", "author": ["Mikael K\u00e5geb\u00e4ck", "Fredrik Johansson", "Richard Johansson", "Devdatt Dubhashi"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2015}, {"title": "Learning to represent words in context with multilingual supervision", "author": ["Kazuya Kawakami", "Chris Dyer"], "venue": "In Proc. of ICLR Workshop", "citeRegEx": "Kawakami and Dyer.,? \\Q2015\\E", "shortCiteRegEx": "Kawakami and Dyer.", "year": 2015}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "In Proc. of EMNLP", "citeRegEx": "Li and Jurafsky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky"], "venue": "In Proc. of ACL", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Proc. of AAAI", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "context2vec: Learning generic context embedding with bidirectional LSTM", "author": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan"], "venue": "In Proc. of CoNLL", "citeRegEx": "Melamud et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["TomasMikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in NIPS", "citeRegEx": "TomasMikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "TomasMikolov et al\\.", "year": 2013}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proc. of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Semi-supervised sequence tagging with bidirectional language models", "author": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power"], "venue": "In Proc. of ACL", "citeRegEx": "Peters et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2017}, {"title": "A simple and efficient method to generate word sense representations", "author": ["Luis Nieto Pi\u00f1a", "Richard Johansson"], "venue": "In Proc. of RANLP", "citeRegEx": "Pi\u00f1a and Johansson.,? \\Q2015\\E", "shortCiteRegEx": "Pi\u00f1a and Johansson.", "year": 2015}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["Lin Qiu", "Yong Cao", "Zaiqing Nie", "Yong Rui"], "venue": "In Proc. of AAAI", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Proc. of NAACL", "citeRegEx": "Reisinger and Mooney.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Proc. of Interspeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders", "author": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord"], "venue": "In Proc. of NAACL", "citeRegEx": "\u0160uster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "\u0160uster et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In Proc. of COLING", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "k-embeddings: Learning conceptual embeddings for words using context", "author": ["Thuy Vu", "D. Stott Parker"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Vu and Parker.,? \\Q2016\\E", "shortCiteRegEx": "Vu and Parker.", "year": 2016}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using Wikipedia", "author": ["Zhaohui Wu", "C. Lee Giles"], "venue": "In Proc. of AAAI", "citeRegEx": "Wu and Giles.,? \\Q2015\\E", "shortCiteRegEx": "Wu and Giles.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al.", "startOffset": 42, "endOffset": 113}, {"referenceID": 27, "context": "Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al.", "startOffset": 42, "endOffset": 113}, {"referenceID": 36, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 4, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 0, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 39, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 31, "context": "To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014).", "startOffset": 149, "endOffset": 216}, {"referenceID": 12, "context": "To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014).", "startOffset": 149, "endOffset": 216}, {"referenceID": 35, "context": "To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014).", "startOffset": 149, "endOffset": 216}, {"referenceID": 16, "context": "We add our token embeddings to Tweeboparser (Kong et al., 2014), improving its performance and establishing a new state of the art for Twitter dependency parsing.", "startOffset": 44, "endOffset": 63}, {"referenceID": 25, "context": "We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS tagger of Owoputi et al. (2013). We add our token embeddings to Tweeboparser (Kong et al.", "startOffset": 141, "endOffset": 163}, {"referenceID": 37, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 31, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 12, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 35, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 1, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 29, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 38, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 10, "context": "Some use bilingual information (Guo et al., 2014; \u0160uster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al.", "startOffset": 31, "endOffset": 96}, {"referenceID": 33, "context": "Some use bilingual information (Guo et al., 2014; \u0160uster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al.", "startOffset": 31, "endOffset": 96}, {"referenceID": 8, "context": "Some use bilingual information (Guo et al., 2014; \u0160uster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al.", "startOffset": 31, "endOffset": 96}, {"referenceID": 25, "context": ", 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al.", "startOffset": 100, "endOffset": 149}, {"referenceID": 17, "context": ", 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al.", "startOffset": 100, "endOffset": 149}, {"referenceID": 20, "context": ", 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al.", "startOffset": 45, "endOffset": 63}, {"referenceID": 13, "context": ", 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 30, "context": ", 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014).", "startOffset": 61, "endOffset": 79}, {"referenceID": 14, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 19, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 2, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 23, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 4, "context": ", 2016), convolutional networks (Collobert et al., 2011), or other architectures.", "startOffset": 32, "endOffset": 56}, {"referenceID": 15, "context": "Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al. (2016) and Peters et al.", "startOffset": 0, "endOffset": 151}, {"referenceID": 15, "context": "Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al. (2016) and Peters et al. (2017) use unsupervised learning with monolingual sentences.", "startOffset": 0, "endOffset": 176}, {"referenceID": 32, "context": "RNNs have recently enjoyed a great deal of interest in the deep learning, speech recognition, and NLP communities (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 114, "endOffset": 185}, {"referenceID": 9, "context": "RNNs have recently enjoyed a great deal of interest in the deep learning, speech recognition, and NLP communities (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 114, "endOffset": 185}, {"referenceID": 34, "context": "RNNs have recently enjoyed a great deal of interest in the deep learning, speech recognition, and NLP communities (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 114, "endOffset": 185}, {"referenceID": 11, "context": ", 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000).", "startOffset": 90, "endOffset": 143}, {"referenceID": 7, "context": ", 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000).", "startOffset": 90, "endOffset": 143}, {"referenceID": 15, "context": "While we can use longer sequences, such as the sentence containing the token (Kawakami and Dyer, 2015), we restrict the input sequence to a fixed-size context window around word j, so the input is identical to that of the feedforward encoder above.", "startOffset": 77, "endOffset": 102}, {"referenceID": 34, "context": "To train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (\u201cseq2seq\u201d) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015).", "startOffset": 105, "endOffset": 164}, {"referenceID": 18, "context": "To train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (\u201cseq2seq\u201d) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015).", "startOffset": 105, "endOffset": 164}, {"referenceID": 5, "context": "To train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (\u201cseq2seq\u201d) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015).", "startOffset": 105, "endOffset": 164}, {"referenceID": 3, "context": "The encoder appears to be doing a kind of canonicalization of nonstandard word uses, which suggests applications for token embeddings in normalization of social media text (Clark and Araki, 2011).", "startOffset": 172, "endOffset": 195}, {"referenceID": 21, "context": "In order to gain a better qualitative understanding of the token embeddings, we visualize the learned token embeddings using tSNE (Maaten and Hinton, 2008).", "startOffset": 130, "endOffset": 155}, {"referenceID": 26, "context": "(2011) and Owoputi et al. (2013), so we have their gold standard POS tags.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "We also use our classifier\u2019s scores as additional features in TweeboParser (Kong et al., 2014).", "startOffset": 75, "endOffset": 94}, {"referenceID": 26, "context": "(2011) and Owoputi et al. (2013). For training, we combine the 1000-tweet OCT27TRAIN set and the 327-tweet OCT27DEV development set.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "We use data from Kong et al. (2014), dividing their 717 training tweets randomly into a 573tweet train set and a 144-tweet validation set.", "startOffset": 17, "endOffset": 36}, {"referenceID": 26, "context": "Last row is best result from Owoputi et al. (2013).", "startOffset": 29, "endOffset": 51}, {"referenceID": 22, "context": "We add tag dictionary features constructed from the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993).", "startOffset": 101, "endOffset": 122}, {"referenceID": 25, "context": "Owoputi et al. (2013) achieve 92.", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "We use name list features, adding a binary feature for each name list used by Owoputi et al. (2013), where the feature indicates membership on the corresponding name list of the word being tagged.", "startOffset": 78, "endOffset": 100}, {"referenceID": 26, "context": "4 over the strongest baseline on the test set, matching the accuracy of Owoputi et al. (2013). This is notable since they used structured prediction while we use a simple local classifier, enabling fast and maximally-parallelizable test-time inference.", "startOffset": 72, "endOffset": 94}, {"referenceID": 16, "context": "We also use our head predictors to add a new feature to TweeboParser (Kong et al., 2014).", "startOffset": 69, "endOffset": 88}, {"referenceID": 6, "context": "We also thank the developers of Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) as well as NVIDIA Corporation for donating GPUs used in this research.", "startOffset": 83, "endOffset": 106}], "year": 2017, "abstractText": "We present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.", "creator": "LaTeX with hyperref package"}}}