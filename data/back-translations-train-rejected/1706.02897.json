{"id": "1706.02897", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Bandit Models of Human Behavior: Reward Processing in Mental Disorders", "abstract": "Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for multi-armed bandit problem, which extends the standard Thompson Sampling approach to incorporate reward processing biases associated with several neurological and psychiatric conditions, including Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. We demonstrate empirically that the proposed parametric approach can often outperform the baseline Thompson Sampling on a variety of datasets. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions.", "histories": [["v1", "Wed, 7 Jun 2017 18:36:12 GMT  (16kb)", "http://arxiv.org/abs/1706.02897v1", "Conference on Artificial General Intelligence, AGI-17"]], "COMMENTS": "Conference on Artificial General Intelligence, AGI-17", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["djallel bouneffouf", "irina rish", "guillermo a cecchi"], "accepted": false, "id": "1706.02897"}, "pdf": {"name": "1706.02897.pdf", "metadata": {"source": "CRF", "title": "Bandit Models of Human Behavior: Reward Processing in Mental Disorders", "authors": ["Djallel Bouneffouf", "Irina Rish", "Guillermo A. Cecchi"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 6.02 897v 1 [cs.A I] 7 Jun 2here propose a general parametric framework for the multi-armed bandit problem that extends the standard Thompson sampling approach to include distortions in reward processing associated with various neurological and psychiatric disorders such as Parkinson's and Alzheimer's disease, attention deficit / hyperactivity disorder (ADHD), addiction, and chronic pain. We demonstrate empirically that the proposed parametric approach can often exceed the baseline Thompson-Sampling for a variety of data sets. Furthermore, our parametric framework can be considered from a behavioral modeling perspective as a first step toward a unified computory model for capturing reward processing anomalies across multiple mental states."}, {"heading": "1 Introduction", "text": "In fact, it is often the case that people are confronted with classic exploration and exploitation, that they have to decide whether they want to pursue a good action that they have chosen beforehand (exploitation) or whether they receive more information about the environment, which may lead to better actions in the future, but can also be a bad choice (exploration and exploitation)."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reward Processing in Mental Disorders", "text": "The literature on processing abnormalities, particularly neurological and psychiatric disorders, is quite extensive; below we summarize some of the recent developments in this fast-growing field. Parkinson's disease (PD) is known to play a key role in strengthening learning processes through the neuromodulator dopamine. PD patients who are exhausted in the basal ganglia tend to impair their performance in tasks that require study and error learning, which is the most common cause of dementia in patients who make decisions that lead to negative results than when they expect positive results, while dopamine preparations are usually used to treat PD symptoms."}, {"heading": "2.2 Multi-Armed Bandit (MAB)", "text": "The Multi-Arm Bandit Problem (MAB) models a sequential decision-making process in which a player at any given time selects an action from a given finite set of possible actions and attempts to maximize cumulative reward over time. MAB is often used in affirmation of learning to study the trade-off between exploration and exploitation, and has been an active area of research since the 1950s. Optimal solutions have been provided using a stochastic formulation [1,2] or using a contrarian formulation [12,13,14]. Recently, there has been a sharp increase in interest in a Bayesian formulation [15] using the algorithm known as Thompson sampling [16]. Theoretical analysis in [17] shows that Thompson sampling for Bernoulli bandits asymptotically reaches the optimum performance limit. Empirical analysis of Thompson's sampling, including the study of more methodical Bandit decisions, indicates that the problems are highly complex than Bernoulli's."}, {"heading": "3 Background and Definitions", "text": "The stochastic multiarmed bandit. Faced with a slot machine with N arms representing potential actions, the player must select one of the weapons to play in each time step. MAB algorithm must decide which arm to play in each time step based on the results during the previous t \u2212 1 steps. Let's get the (unknown) expected reward for the arm i. The goal is to maximize the expected total reward during the T iterations, i.e., E-iteration Tt = 1 \u00b5i (t) is the arm played in step t, and the expectation is higher than the random decisions of i (t) made by the algorithm. We could also use the equivalent performance measure known as the expected total value of Bayerit."}, {"heading": "4 Proposed Approach: Human-Based Thompson Sampling", "text": "We will now introduce a more general rule for updating the parameters of the beta distribution in steps 10 and 11 of algorithm 1, which will make it possible to model a wide range of reward processing distortions associated with various disorders. More specifically, the proposed human-base algorithm 1: Thompson Sampling1: Foreach Arm i = 1,..., K 2: Set Si (t) = 1, Fi (t) = 1, Fi (t) = 1 3: End for 4: Foreach Arm t = 1, 2,..., T do 5: Foreach Arm i = 1, 2,..., K do 6: Sample Projecti (t) from beta (t), Fi (t))) 7: End for do8: Play Arm it = argmaxiblos (t) (t) (t), get reward r (t) 9: if r (t) \u2212 Fi (t) = Si (t).Enddoit: Play Arm (TS): (t) and then we get (t) algoritht (t): (t)."}, {"heading": "4.1 Reward Processing Models with Different Biases", "text": "In this section, we describe how specific restrictions on the model parameters in the proposed algorithm can cause different reward processing operations < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < > > > > > > > > > > > > > > > < < < > > > > > > > > > < < < > > > > > > > > > > > > > > > > > > > > < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < >; < >; < >; < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > < < < < < < < < < < < < < < < < < < >; < < < < < < < < < < < < < < >; < < < < >; < < < &lt"}, {"heading": "5 Empirical Evaluation", "text": "To comprehend and compare the proposed framework empirically, we have to deal with the following four categories: firstly with the way people live in the world, secondly with the way they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that"}, {"heading": "6 Conclusions", "text": "Our approach is based on extensive literature on decision-making in neurological and psychiatric disorders resulting from disorders in the reward processing system. It has been shown that the proposed model consistently outperforms the basic Thompson sampling method in all the data and experimental settings we examined. Our empirical results support several previous observations of reward processing distortions in a range of mental disorders, demonstrating the potential of the proposed model and its future extensions to capture reward processing aspects across different neurological and psychiatric states."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics 6(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning 47(2-3)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Reward processing in neurodegenerative disease", "author": ["D.C. Perry", "J.H. Kramer"], "venue": "Neurocase 21(1)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "By carrot or by stick: cognitive reinforcement learning in parkinsonism", "author": ["M.J. Frank", "L.C. Seeberger", "R.C. O\u2019reilly"], "venue": "Science 306(5703)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling", "author": ["A.D. Redish", "S. Jensen", "A. Johnson", "Z. Kurth-Nelson"], "venue": "Psychological review 114(3)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Does reward frequency or magnitude drive reinforcement-learning in attention-deficit/hyperactivity disorder? Psychiatry research", "author": ["M. Luman", "C.S. Van Meel", "J. Oosterlaan", "J.A. Sergeant", "H.M. Geurts"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Mesolimbic dopamine signaling in acute and chronic pain: implications for motivation, analgesia, and addiction", "author": ["A.M. Taylor", "S. Becker", "P. Schweinhardt", "C. Cahill"], "venue": "Pain 157(6)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Frontotemporal dementia: what can the behavioral variant teach us about human brain organization? The Neuroscientist", "author": ["W.W. Seeley", "J. Zhou", "E.J. Kim"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Computational psychiatry of adhd: neural gain impairments across marrian levels of analysis", "author": ["T.U. Hauser", "V.G. Fiore", "M. Moutoussis", "R.J. Dolan"], "venue": "Trends in neurosciences 39(2)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A neurocomputational model for cocaine addiction", "author": ["A. Dezfouli", "P. Piray", "M.M. Keramati", "H. Ekhtiari", "C. Lucas", "A. Mokri"], "venue": "Neural computation 21(10)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond pain: modeling decision-making deficits in chronic pain", "author": ["L.E. Hess", "A. Haimovici", "M.A. Mu\u00f1oz", "P. Montoya"], "venue": "Frontiers in behavioral neuroscience 8", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "On-line learning with malicious noise and the closure algorithm", "author": ["P. Auer", "N. Cesa-Bianchi"], "venue": "Ann. Math. Artif. Intell. 23(1-2)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput. 32(1)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-armed bandit problem with known trend", "author": ["D. Bouneffouf", "R. F\u00e9raud"], "venue": "Neurocomputing 205", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in neural information processing systems.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": "Biometrika 25", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1933}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scotland.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Freshness-aware thompson sampling", "author": ["D. Bouneffouf"], "venue": "Neural Information Processing - 21st International Conference, ICONIP 2014, Kuching, Malaysia, November 3-6, 2014. Proceedings, Part III.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and decisions in contextual multi-armed bandit tasks", "author": ["E. Schulz", "E. Konstantinidis", "M. Speekenbrink"], "venue": "In Proceedings of the 37th annual conference of the cognitive science society.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika 25(3/4)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1933}], "referenceMentions": [{"referenceID": 0, "context": "The exploration-exploitation trade-off is typically modeled as the multi-armed bandit (MAB) problem, stated as follows: given N possible actions (\u201carms\u201d), each associated with a fixed, unknown and independent reward probability distribution [1,2], an agent selects an action at each time point and receives a reward, drawn from the corresponding distribution, independently of the previous actions.", "startOffset": 241, "endOffset": 246}, {"referenceID": 1, "context": "The exploration-exploitation trade-off is typically modeled as the multi-armed bandit (MAB) problem, stated as follows: given N possible actions (\u201carms\u201d), each associated with a fixed, unknown and independent reward probability distribution [1,2], an agent selects an action at each time point and receives a reward, drawn from the corresponding distribution, independently of the previous actions.", "startOffset": 241, "endOffset": 246}, {"referenceID": 2, "context": "In order to better understand and model human decision-making behavior, scientists usually investigate reward processing mechanisms in healthy subjects [3].", "startOffset": 152, "endOffset": 155}, {"referenceID": 3, "context": "For example, it was shown that (unmedicated) patients with Parkinson\u2019s disease appear to learn better from negative rather than from positive rewards [4]; another example is addictive behaviors which may be associated with an inability to forget strong stimulus-response associations from the past, i.", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "to properly discount past rewards [5], and so on.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "For example, [4] demonstrate that off-medication PD patients are better at learning to avoid choices that lead to negative outcomes than they are at learning from positive outcomes, while dopamine medication typically used to treat PD symptoms reverses this bias.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "As discussed in [3], AD patients have decreased pursuit of rewarding behaviors, including loss of appetite; these changes are often secondary to apathy, associated with diminished reward system activity.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Frontotemporal dementia (bvFTD) typically involves a progressive change in personality and behavior including disinhibition, apathy, eating changes, repetitive or compulsive behaviors, and loss of empathy [3], and it is hypothesized that those changes are associated with abnormalities in reward processing.", "startOffset": 205, "endOffset": 208}, {"referenceID": 5, "context": "Authors in [6] suggest that the strength of the association between a stimulus and the corresponding response is more susceptible to degradation in ADHD patients, which suggests problems with storing the stimulus-response associations.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "In [5], it is demonstrated that patients suffering from addictive behavior are not able to forget the stimulus-response associations, which causes them to constantly seek the stimulus which generated such association.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], it is suggested that chronic pain results in a hypodopaminergic (low dopamine) state that impairs motivated behavior, resulting into a reduced drive in chronic pain patients to pursue the rewards.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].", "startOffset": 142, "endOffset": 157}, {"referenceID": 7, "context": "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].", "startOffset": 142, "endOffset": 157}, {"referenceID": 8, "context": "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].", "startOffset": 142, "endOffset": 157}, {"referenceID": 9, "context": "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].", "startOffset": 142, "endOffset": 157}, {"referenceID": 4, "context": "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].", "startOffset": 142, "endOffset": 157}, {"referenceID": 10, "context": "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].", "startOffset": 142, "endOffset": 157}, {"referenceID": 0, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].", "startOffset": 68, "endOffset": 73}, {"referenceID": 1, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].", "startOffset": 68, "endOffset": 73}, {"referenceID": 11, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].", "startOffset": 111, "endOffset": 121}, {"referenceID": 12, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].", "startOffset": 111, "endOffset": 121}, {"referenceID": 13, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].", "startOffset": 111, "endOffset": 121}, {"referenceID": 14, "context": "Recently, there has been a surge of interest in a Bayesian formulation [15], involving the algorithm known as Thompson sampling [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "Recently, there has been a surge of interest in a Bayesian formulation [15], involving the algorithm known as Thompson sampling [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "Theoretical analysis in [17] shows that Thompson sampling for Bernoulli bandits asymptotically achieves the optimal performance limit.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "Empirical analysis of Thompson sampling, including problems more complex than the Bernoulli bandit, demonstrates that its performance is highly competitive with other approaches [15,18].", "startOffset": 178, "endOffset": 185}, {"referenceID": 17, "context": "Empirical analysis of Thompson sampling, including problems more complex than the Bernoulli bandit, demonstrates that its performance is highly competitive with other approaches [15,18].", "startOffset": 178, "endOffset": 185}, {"referenceID": 18, "context": "Psychological study done in [19] shows that, instead of maximizing output by a deliberate mean-variance trade-off, participants approach dynamic decision-making problems by utilizing a probabilitymatching heuristic.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "Thus, their behavior is better described by the Thompson sampling choice rule than by the Upper Confidence Bound (UCB) approach [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "Choosing an arm i yields a random real-valued reward according to some fixed (unknown) distribution with support in [0, 1].", "startOffset": 116, "endOffset": 122}, {"referenceID": 19, "context": "Thompson sampling (TS) [20], also known as Basyesian posterior sampling, is a classical approach to multi-arm bandit problem, where the reward ri(t) for choosing an arm i at time t is assumed to follow a distribution Pr(rt|\u03bc\u0303) with the parameter \u03bc\u0303.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Given a prior Pr(\u03bc\u0303) on these parameters, their posterior distribution is given by the Bayes rule, Pr(\u03bc\u0303|rt) \u221d Pr(rt|\u03bc\u0303)Pr(\u03bc\u0303) [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 3, "context": "Recall that PD patients are typically better at learning to avoid negative outcomes than at learning to achieve positive outcomes [4]; one way to model this is to over-emphasize negative rewards, by placing a high weight on them, as compared to the reward processing in healthy individuals.", "startOffset": 130, "endOffset": 133}], "year": 2017, "abstractText": "Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for multi-armed bandit problem, which extends the standard Thompson Sampling approach to incorporate reward processing biases associated with several neurological and psychiatric conditions, including Parkinson\u2019s and Alzheimer\u2019s diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. We demonstrate empirically that the proposed parametric approach can often outperform the baseline Thompson Sampling on a variety of datasets. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions.", "creator": "LaTeX with hyperref package"}}}