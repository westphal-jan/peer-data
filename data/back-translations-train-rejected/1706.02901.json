{"id": "1706.02901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Characterizing Types of Convolution in Deep Convolutional Recurrent Neural Networks for Robust Speech Emotion Recognition", "abstract": "Deep convolutional neural networks are being actively investigated in a wide range of speech and audio processing applications including speech recognition, audio event detection and computational paralinguistics, owing to their ability to reduce factors of variations, such as speaker and environment information in signals, for speech recognition. However, studies have suggested to favor a certain type of convolutional operations when building a deep convolutional neural network for speech applications although there has been promising results using different types of convolutional operations. In this work, we study four types of convolutional operations on different input features for speech emotion recognition in order to derive a comprehensive understanding. Since affective behavioral information has been shown to reflect temporally varying of mental state and convolutional operation are applied locally in time, all deep neural networks share a deep recurrent sub-network architecture for further temporal modeling. We present detailed quantitative module-wise performance analysis to gain insights into information flows within the proposed architectures. In particular, we demonstrate the interplay of affective information and the other irrelevant information during the progression from one module to another. Finally we show that all of our deep neural networks provide state-of-the-art performance on the eNTERFACE'05 corpus.", "histories": [["v1", "Wed, 7 Jun 2017 15:17:21 GMT  (2024kb)", "http://arxiv.org/abs/1706.02901v1", "Submitted to IEEE transaction"]], "COMMENTS": "Submitted to IEEE transaction", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.MM cs.SD", "authors": ["che-wei huang", "shrikanth s narayanan"], "accepted": false, "id": "1706.02901"}, "pdf": {"name": "1706.02901.pdf", "metadata": {"source": "CRF", "title": "Characterizing Types of Convolution in Deep Convolutional Recurrent Neural Networks for Robust Speech Emotion Recognition", "authors": ["Che-Wei Huang"], "emails": [], "sections": [{"heading": null, "text": "We present detailed quantitative module-wise analyses to gain insights into the proposed evolution within the different evolutionary architectures. Finally, we show that all relevant evolutionary networks that we link together during the evolutionary process pass on the evolutionary architecture of another evolutionary network. 7J un2 017 are subject to IEEE TRANSACTIONS 1 applications, including speech recognition, audio event recognition and computational parallel linguistics, due to their ability to reduce variation factors such as loudspeaker and ambient information in speech recognition signals. However, studies have suggested to favor a certain type of evolutionary operations in the construction of a deep evolutionary neural network for speech applications, although there have been promising results that use different types of evolutionary operations in speech recognition signals. In this work, we are investigating four types of evolutionary operations on different input functions for speech emotions, in order to derive a comprehensive understanding of the evolutionary structure of the temporal structure."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most people are able to determine for themselves what they want and what they want. In fact, it is the case that most people are able to decide what they want and what they want. In fact, it is the case that most people are able to decide what they want and what they do not want."}, {"heading": "2 RELATED WORK", "text": "Before the current era of deep learning, speech recognition, was predominantly based on a two-step training approach, where feature engineering and classification training are performed separately. Frequently used features include pitch, MFCC, log mels and the recommended features from the INTERSPEECH spectrum. Support vector machine (SVM) and extreme learning machine (ELM) were two of the most competitive classification systems. For the ease with which models can be compared, Eyben et al. [3] summarized the performance of an SVM trained on the INTERSPEECH spectrum, challenging features across several public companies. Yan et al. [25] recently proposed an economical kernel for reduced regression for bi-modal emotions."}, {"heading": "3 DEEP CONVOLUTIONAL RECURRENT MODELS", "text": "In this section, we describe the proposed deep recursive volume networks and the details of structurally different convolutionary operations on log-mels and MFCCs. Fig. 1 illustrates the overview of the models we design for recognizing speech sensations. In the upper left part of Fig. 1, we define four types of convolutionary operations depending on the shape of their character maps. By dividing the Convolutionary operations into four types, we expect to understand their differences for a finer analysis after they have been optimized to learn from the spectral signals; in the upper right part of Fig. 1, we represent a deep recursive neural network called the LDNN model, as the usual sub-network architecture for each model. Since a Convolutionary layer is applied locally in time, the LDNN model is intended to model the long-term temporal relationship within an expression; in the lower part of Fig. 1, all of the models are presented for a comprehensive study only to represent the language in which we can relate to a single language."}, {"heading": "3.1 Types of Convolutional Operations", "text": "A convolutionary neural layer conv containing an input tensor X-RC \u00b7 H \u00b7 W 0 \u00b7 W 0 consists of a convolutionary function F\u0440: R \u00b7 H0 \u00b7 W 0 \u00b7 W 0 \u00b7 RK \u00b7 H 1 \u00b7 W 1 \u00b7 W 1 \u00b7 W 2 \u00b7 W 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 4 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S 2 \u00b7 S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S\" S. \"S\" S \"S.\" S \"S\" S. \"S\" S. \"S.\" S. \"S.\" S. \"S.\" S \"S.\" S \"S.\" S. \"S.\" S. \"S\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S\" S. \"S.\" S \"S.\" S. \"S.\" S. \"S\" S. \"S\" S. \"S\" S \"S.\" S. \"S.\" S. \"S\" S \"S\" S \"S.\" S \"S.\" S \"S\" S. \"S.\" S \"S\" S \"S\" S. \"S\" S \"S\" S. \"S.\" S. \"S\" S. \"S\" S. S. S \"S\" S \"S\" S \"S\" S \"S\" S. S. S \"S\" S \"S. S\" S \"S\" S. S \"S\" S. S \"S\" S. S \"S\" S. S \"S\" S \"S. S\" S \"S\" S \"S."}, {"heading": "3.2 Deep Recurrent Neural Network", "text": "Suppose the input is a sequence of vectors {xt}. The Elman type simple recursive neural network RNN = 6 is also defined by the following equations: ht = \u03c3h (Uhxxt + U hhht \u2212 1 + uh) (4) yt = \u03c3y (Uyhht + uy), (5) where ht as a nonlinear recursive transformation of all past history {xs} t s = 1 represents the system memory at time t, (Uba, ub) is an affine mapping from a space of type a to a type b, and \u03c3c is the activation function for type c. Here x, h and y denote the input, hidden and output vectors. However, the formation of a simple RNN with the backward propagation algorithm can cause the problems of the gradients to disappear or explode."}, {"heading": "3.3 CLDNN-based Models", "text": "Before defining a multitude of LSTD models that we have discussed in a shared sub-network architecture, it is necessary for us to be able to include the aforementioned LSD-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-DW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-DW-SW-SW-SW-SW-SW-SW-SW-SW-SW-DW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SDW-SW-SDW-SDW-SDW-SDW-W-SDW-SDW-SDW-SDW-SDW-SDW-"}, {"heading": "4 BASELINE MODELS", "text": "We evaluate our CLDNN-based models to understand wave operations by comparing them to three basic models for a speech emotion recognition task. The first of the basic models uses the low-level descriptors and their statistical functions within an utterance to train a support vector engine, the other two base models are based on BLSTM recursive neural networks and use log mels or MFCCs as input."}, {"heading": "4.1 Support vector machine with the Low-Level Descriptors and Their Statistical Functionals", "text": "Many linguistic studies have found empirically that emotions correlate with parameters known as low-level descriptors (LLDs) along various aspects of phonation and articulation in language, such as language rate in the time domain, fundamental frequency or formant frequency in the frequency domain, intensity or energy in the amplitude domain, or relative energy in different frequency bands in the spectral energy domain. In addition, statistical functionality of an entire emotional expression is derived from the LLDs to obtain global information, complementing local information gathered by frame-level LLDs. Popular selection criteria of these parameters for developing machine learning algorithms in practical applications often amount to several thousand features. For example, in the INTERSPEECH 2013 computational paralinguistics challenge, the recommended functionality includes 6,373 parameters of LDs and statistical functional algorithms in practical practice."}, {"heading": "4.2 LDNN with the log-Mels", "text": "As previous studies suggest [17], [18], [19], [20], explicit temporal modeling is advantageous for speech emotion recognition, where a relapsing neural network is a better choice than a hidden Markov model due to its outstanding ability to model long-term temporal relationships. In the meantime, to build a competitive and compatible base model with respect to the CLDNN-based model, we use the LDNN architecture defined in paragraph 3.3 as a second base model. In particular, we use log mels as input into the LDNN model as a \"raw\" functional environment without any temporal or spectral convolutionary operations. We call this model LDNN (log mels)."}, {"heading": "4.3 LDNN with the MFCCs", "text": "MFCCs are related to log-mels using a mathematical construct: the discrete cosine transformation (DCT). Specifically, the relationship is defined as follows: MFCC [k] = M \u2212 1 \u2211 m = 0log-mels [m] cos (k\u03c0M (m + 12))), (12) where MFCC [k] and log-Mel [m] are the kth and mth coefficients of MFCCs and log-mels, respectively, and M is the number of Mel-scaled filter banks. We can easily convert Eq. (12) into a revolutionary operation along the spectral direction in which all character maps are tensors of the form M \u00d7 1. For the kth character map hk, its mth component contains [m] = cos (k\u03c0M (m + 12) dentional operation) (13), the constellation Cconconconsor of the form M \u00d7 Cizze = its mth map contains the MCizze = its characteristic, its MCizk-skizze = its previous characteristic constellation."}, {"heading": "5 DATABASES DESCRIPTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 The Clean Set", "text": "Although the entire database contains language, facial expression and text, in this work we are conducting only experiments on audio modality, which includes 42 subjects from 14 different countries, 34 of whom were male and 8 female. Each subject was asked to carefully listen to 6 short stories, and each of them was designed to elicit a specific emotion among the 6 archetypal emotions defined by Ekman et al. [42] The subjects then responded to each of the scenarios to express their emotions in English according to a proposed script. Each subject was asked to speak five expressions per emotion class for 6 emotion classes (anger, disgust, fear, happiness, sadness and surprise).For each recorded emotional expression, there is a corresponding global label describing the information transmitted by the entire utterance. The resulting Correspondent for each emotion class was asked to speak for 6 emotion classes (anger, disgust, fear, happiness, sadness and surprise).For each recorded emotional expression, there is a corresponding global label describing the information transmitted through the entire utterance, the resulting Correspondent for 6 emotion classes (anger, happiness, happiness, sadness and surprise).For each recorded emotional expression, there is a corresponding label describing the information transmitted by a proposed script in English script."}, {"heading": "5.2 The Noisy Set", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "6 SPEECH EMOTION RECOGNITION EXPERIMENTS", "text": "In this section, we evaluate the proposed models based on the following experiments: 1) Base model) SVM with openSMILE features b) LDNN (MFCCs) c) LDNN (log-mels) 2) CLDNN-based model) T-CLDNN (MFCCs) b) FST-CLDNN (MFCCs) c) T-CLDNN (log-mels) d) S-CLDNN (log-mels) e) ST-CLDNN (log-mels) f) FST-CLDNN (log-mels) The purposes of these experiments are multi-faceted; the comparison between the base models and the CLDNN-based models aims to demonstrate the effectiveness of the convolutionary operations in learning affective information."}, {"heading": "6.1 SVM with openSMILE features", "text": "For the first set of basic experiments, we apply two evaluation strategies. In the first strategy, we perform a Leaveone Subject Out (LOSO) cross-validation. As we train our deep neural network models based on the TVT partitions, the second strategy evaluates the performance of SVM classifiers on the TVT partitions for fair comparison. In addition, we also use the regular pre-processing procedures, including the standardization of speakers to remove speaker characteristics and class weighting for slight class imbalances. We perform the basic experiments with SVM classifiers trained in the acoustic characteristics of the INTERSPEECH challenges in the past. SVM classifiers are trained in these handmade high-dimensional features using the Scikit Learn Machine Learn Toolkit [47] with linear, polynomic and radial base functions (RF) All experiments are conducted under clean SVM conditions."}, {"heading": "6.2 CLDNN-based Models with the MFCCs and the logMels", "text": "In both cases, the number of those who are able to reach more than 40% of the population is greater than the number of those who are able to comply with the rules. Indeed, the number of those who are able to cross borders is greater than the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders, the number of those who are able to cross borders."}, {"heading": "7 EXPERIMENTAL RESULTS", "text": "In this section, we present our experimental results on language sensation recognition. Although the class imbalance in the corpus is negligible, we use unweighted accuracy (UA) as a measure of performance throughout the section to avoid disadvantage compared to the larger classes."}, {"heading": "7.1 SVM with openSMILE features", "text": "Table 3 summarizes the results of the use of SVM classifiers to identify the emotion class of an emotional expression with one of the 6 archetypal emotions. Based on the LOSO evaluation strategy, an SVM with the STC feature set delivers the best baseline performance, while under the TVT evaluation strategy, an SVM with the ComParE feature set stands out from other feature sets. It is clear from these results that an SVM learns better from higher-dimensional feature sets such as the ComParE and the STC sets, which is also a consistent phenomenon observed in [3]. Yan et al. [25] recently published a baseline result on the eNTERFACE '05 corpus using the PC feature set. They trained an SVM classifier on the PC feature set with a loudspeaker-dependent five-fold cross-validation evaluation work as one of our baseline models and will also be comparable to their baseline work in our baseline."}, {"heading": "7.2 LDNN with the MFCCs and the log-Mels", "text": "The results of the LDNN-based models are presented in Table 4. Under the noisy condition, the LDNN models (MFCCs) and the LDNN models (log-mels) are able to accurately classify 75.51% and 78.87% of the test samples, respectively, and under the clean condition, they deliver a performance of 88.33% and 90.42%, respectively. As MFCCs are DCT-transformed log-mels, it is easy to determine that there is a 3.36% and 2.09% gap between LDNN (MFCCs) and LDNN (log-mels) under each condition, which may have removed a certain amount of affective information during the conversion of the log-mels into the MFCCs. Moreover, the increased gap under the noisy condition suggests that MFCCs are more sensitive to noise than log-mels, making learning MFCCs a more challenging task. Nevertheless, both LDNN models perform comparable results through the FS-25."}, {"heading": "7.3 CLDNN with the MFCCs and the log-Mels", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "7.4 Module-wise evaluations", "text": "This year, it is time for us to set out to find a solution that paves the way to the future."}, {"heading": "8 CONCLUSION", "text": "In fact, it is so that most people who are able to determine themselves, to determine themselves and to decide what they want and what they do not want. (...) It is not so that men are able to determine themselves. (...) It is so that men are able to determine themselves. (...) It is not so that men are able to determine themselves. (...) It is so that men are able to determine themselves. (...) It is so that men are able to determine what they want. (...) It is so. (...) It is so. (...) It. (...) It. (...). \"It. (...) It.\" It. (...). \"It.\" (...). \"It. (...).\" It. (...). \"It. (...).\" It. (...). \"It.\" It. (...). \"It. (...).\" It. (. \"It.\" It. (...). \"It.\" It. \"It. (...\" It. \"It.\" It. (...). \"It.\" It. \"It. (...\" It. \"It. (...).\" It. \"It.\" It. \"It. (...).\" It. (... \"It.\" It. \"It.\" It. \"It. (...\" It. (...). \"It.\" It. (...). \"It. (It.\" It. \"It.\" It. \"It. (...).\" It. \"It.\" It. (... \"It. (...).\" It. (It. \"It.\" It. (...). (It. \"It.\" It. \"It. (). (It. (It.\" It. \"It. (It.). (It. (It. (It.). (). (It. (It. (It.). (It.). (It.). (It. (It. (It. (It.) It.). (It. (It. (It.). (It. (). (It. (It.). (). (It. (It. (It.). (). (It.). (It. (It."}, {"heading": "APPENDIX A", "text": "VISUALIZATION OF ALL MODELS"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Deep convolutional neural networks are being actively investigated in a wide range of speech and audio processing applications including speech recognition, audio event detection and computational paralinguistics, owing to their ability to reduce factors of variations, such as speaker and environment information in signals, for speech recognition. However, studies have suggested to favor a certain type of convolutional operations when building a deep convolutional neural network for speech applications although there has been promising results using different types of convolutional operations. In this work, we study four types of convolutional operations on different input features for speech emotion recognition in order to derive a comprehensive understanding. Since affective behavioral information has been shown to reflect temporally varying of mental state and convolutional operation are applied locally in time, all deep neural networks share a deep recurrent sub-network architecture for further temporal modeling. We present detailed quantitative module-wise performance analysis to gain insights into information flows within the proposed architectures. In particular, we demonstrate the interplay of affective information and the other irrelevant information during the progression from one module to another. Finally we show that all of our deep neural networks provide state-of-the-art performance on the eNTERFACE\u201905 corpus.", "creator": "LaTeX with hyperref package"}}}