{"id": "1706.02909", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Deriving a Representative Vector for Ontology Classes with Instance Word Vector Embeddings", "abstract": "Selecting a representative vector for a set of vectors is a very common requirement in many algorithmic tasks. Traditionally, the mean or median vector is selected. Ontology classes are sets of homogeneous instance objects that can be converted to a vector space by word vector embeddings. This study proposes a methodology to derive a representative vector for ontology classes whose instances were converted to the vector space. We start by deriving five candidate vectors which are then used to train a machine learning model that would calculate a representative vector for the class. We show that our methodology out-performs the traditional mean and median vector representations.", "histories": [["v1", "Thu, 8 Jun 2017 03:01:37 GMT  (1398kb,D)", "http://arxiv.org/abs/1706.02909v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vindula jayawardana", "dimuthu lakmal", "nisansa de silva", "amal shehan perera", "keet sugathadasa", "buddhi ayesha"], "accepted": false, "id": "1706.02909"}, "pdf": {"name": "1706.02909.pdf", "metadata": {"source": "CRF", "title": "Deriving a Representative Vector for Ontology Classes with Instance Word Vector Embeddings", "authors": ["Vindula Jayawardana", "Dimuthu Lakmal", "Nisansa de Silva", "Amal Shehan Perera", "Keet Sugathadasa", "Buddhi Ayesha"], "emails": ["vindula.13@cse.mrt.ac.lk"], "sections": [{"heading": null, "text": "In fact, it is that we are able to assert ourselves, that we are able to comply, that we are able to comply with the rules, and that we are able to comply with the rules that we have set ourselves."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "This section discusses the background information on the techniques used in this study and related previous studies conducted by others in various areas relevant to this research. The following are key areas covered by this study."}, {"heading": "A. Ontologies", "text": "In many areas, ontologies are used to organize information as a form of knowledge representation; an ontology can model either the world or part of it as seen from the standpoint of this domain [4].ar Xiv: 170 6.02 909v 1 [cs.C L] 8J un2 017Individuals (instances) form the basic level of an ontology. They can be either concrete objects or abstract objects. Individuals are then grouped into structures called classes. Depending on the domain on which ontology is based, a class in an ontology can be called a concept, type, category or kind. More often, the definition of a class and its role is analogous to that of a collection of individuals with some additional properties that distinguish it from a mere set of objects. A class can be subordinated to another class or subsumed by another class."}, {"heading": "B. Word set expansion", "text": "The creation and maintenance of word lists is an integral part of many Natural Language Processing (NLP) tasks. These word lists usually contain words that are considered homogeneous in the degree of abstraction of the application. Thus, two words W1 and W2 can belong to a single word list in one application, but in another application to different word lists. This vague definition and use makes the creation and maintenance of these word lists a complex task. For the purpose of this study, we selected the algorithm presented in [4], which builds on the earlier algorithm described in [17]. The reason for this selection is that WordNet [5] based linguistic processes are reliable due to the fact that the WordNet lexicon is based on the knowledge of experienced linguists."}, {"heading": "C. Word Embedding", "text": "Word embedding systems are a set of natural language modeling and feature learning techniques in which words from a domain are mapped to vectors to create a model with a distributed representation of words that was first proposed by [14]. Each word in a text document is mapped to a vector space. In addition, word meanings and relationships between words are also mapped to the same vector space. word2vec1 [15], GloVe [18] and Latent Dirichlet Allocation (LDA) [19] are leading Word Vector embedding systems. Both Word2vec and GloVe use word on adjacent word mapping to learn condensed embedding. The difference is that Word2vec uses a neural network-based approach, while GloVe uses matrix factoring mechanisms. LDA also has an approach that uses matrices, but maps are mapped there with the relevant words."}, {"heading": "D. Clustering", "text": "Clustering is a fundamental part of exploratory data mining and statistical data analysis. The goal of clustering is to group a series of items into separate sub-sets (clusters), where the items in a given cluster are more similar to each other than all items in another cluster. The similarity measurement used and the desired number of clusters depend on the application. A cluster algorithm is naturally modeled as an iterative, multi-objective optimization problem that involves trial and error in moving toward a state that has the desired properties. Of all available cluster methods, we chose k-means clustering because of the ease of implementation and configuration. It is assumed that k-means clustering was first proposed by Stuart Lloyd [28] as a method for vector quantification for pulse code modulation in the field of signal processing. The goal is to distribute cluster observations in a cluster spacing from a cluster, where each means is measured by the observation in the middle of the cluster belongs to the cluster."}, {"heading": "E. Support Vector Machines", "text": "The SVM algorithm works on a series of training examples in which each example is marked as belonging to one of two classes. The goal is to find a hyperplane that divides the two classes in such a way that the examples of the two classes are divided by a clear gap that is mathematically as broad as possible. Thus, the process is a non-probable task for binary linear classifiers. However, the above-mentioned gap is limited by instances called support vectors. The idea of using SVMs for tasks that affect ontologies is rather rare. However, a study by Jie Liu et al. [30] defined a similarity cube consisting of similarity vectors that combine similar concepts, instances, and structures of two ontologies."}, {"heading": "III. METHODOLOGY", "text": "In this section, we discuss the methodology we have used to derive vector representation for ontology classes using instance vector embedding. Each of the following sections describes a step in our process. An overview of the methodology we propose in sections III-A and III-B is shown in Figure 1, and an overview of the methodology we propose from section III-C to section III-G is illustrated in Figure 2."}, {"heading": "A. Ontology Creation", "text": "After creating the hierarchy of ontology classes, we manually added seed instances for all classes of ontology based on a manual review of the content of consumer law cases in Findlaw. Next, we used the algorithm proposed in [4] to expand the instance sets, and then manually trimmed the extended lists to prevent conceptual drift. This entire process was conducted under the supervision and guidance of legal experts."}, {"heading": "B. Training word Embeddings", "text": "The formation of the word embedding was the process of building a word2vec model using a large legal text corpus obtained from Findlaw. [16] The text corpus consisted of legal cases under 78 legal categories. In creating the legal text corpus, we used Stanford CoreNLP to pre-process the text with tokenization, sentence splitting, part of the speech and lemmatization. The motive behind using a pipeline that pre-processes text up to and including lemmatization, instead of the traditional approach of training the word2vec model with only tokenized text [15] was to assign all the inflected forms of a given problem to a single entity. In the traditional approach, each inflected form of a problem is trained as a separate vector, diluting the values extracted from the context of this problem and causing us to derive all lectic contexts between all inflexible forms."}, {"heading": "C. Sub-Cluster Creation", "text": "By definition, the instances in a particular class are more semantically similar to an ontology than the instances in other classes. However, no matter how coherent a set of items is, as long as that set contains more than one element, it is possible to create non-empty subsets that are correct subsets of the original set. This is the main motivation behind this step in our methodology. As a result of this reasoning, it was decided that it is possible to find at least one main schism in the otherwise semantically coherent set of more than one instance, which means that we have decided to divide the instances in a single class into two subdivisions. To this end, we use K-Means, which are grouped relative to the instances in one class and the instances of another class. For example, we divide the \"judge class\" with k-Means to support the \"judges in two subdivisions,\" and then support the \"judges in two sub-groups.\""}, {"heading": "D. Support Vector Calculation", "text": "It is clear that the specification of the problem discussed in this study is more like a cluster task than a classification task. Furthermore, given that each time we would execute the proposed algorithm, it would have been a pointless effort to assign class names to the instances because there would be only one class. Therefore, a classifying algorithm such as Support Vector Machines (SVM) might seem strange as part of the process of this study at the outset. However, this problem is no longer a problem due to the steps taken in Section III-C. Instead of the single uniform cluster with the homogeneous class designation, the previous step resulted in two subclusters with two unique designations. Given that the entire premise of creating sub-clusters was based on the argument that there is a division in the individual vectors in the class."}, {"heading": "E. Candidate Matrix Calculation", "text": "Using the steps described above to derive the vector representation for the ontology classes, we have calculated a number of vector candidates for each class and then derived the candidate matrix from these candidate vectors. We describe the said candidate vectors at: \u2022 Average support vector (C1) instance, however, is an average instance (C2) instance {class median vector (C3) vector (C4, C5) instance, which is achieved by performing word vectorization on the selected class name and it has been designated as our desired output.1) -Average support vector (C1) vector of average support (C1): We identified the support vectors that mark the space around the hyperlevel, dividing the class into the two subclasses listed in Section III-D. We take the average of the support vectors as the first vector candidate vector."}, {"heading": "F. Optimization Goal", "text": "After calculating the candidate vectors, we proposed an optimal vector that represents the given class based on the optimization goal as follows: Y = M \u2211 i = 1 CiWiM \u2211 i = 1 Wi (4) Here, Y is the predicted class vector for the given class. M is the number of candidate vectors for a given class. Ci and Wi represent the ith candidate vector or the associated weight of that candidate vector. Here, the Wi is calculated using the method described in section III-G."}, {"heading": "G. Machine Learning implementation for weight calculation", "text": "The method of machine learning we use is a neural network.The data set is structured in the traditional (X, D) structure, where X is the set of inputs and D is the set of outputs. An input tuple x (so that x-X) has elements xi, j. The matching output tuple d (so that d-D) has a single element. X and D is populated as follows: Take the ith line of the candidate matrix Mcand the class as x and add it to X. Take the ith element of C0 of the class and add it to D. Once this is done for all classes, we get the (X, D) set of examples. It should be noted that the weights are learned universally and not on a class basis."}, {"heading": "IV. RESULTS", "text": "We used a number of legal ontology classes, seeded by legal experts and then expanded by the established expansion algorithm [4] under the guidance of the same legal experts. We report on our results below in Table I and in Fig.3 we show a visual comparison of the same data. We illustrate the results we have obtained in relation to ten prominent legal concept classes, as well as the mean result across all the classes we have considered. We compare the representative vector we propose with the traditional representative vectors: average vector and median vector. All the results shown in the table are euclidean distances between the representative vector in question and the respective C0 vector. It can be deduced from the results that the traditional approach of using the average vector as a representative vector of a class exceeds the other traditional approach of using the median vector."}, {"heading": "V. CONCLUSION AND FUTURE WORKS", "text": "In this paper, we have shown that the proposed method works as a better representation for a number of cases occupying an ontology class than the traditional methods of using the average vector or median vector. This discovery will mainly be useful for two important ontology tasks. The first is to populate an already seeded ontology. In this study, we used the algorithm proposed in [4] for this task to obtain a test dataset. However, this approach has the weakness of being dependent on the WordNet lexicon. A methodology based on the representative vector detection algorithm proposed in this paper will not exhibit this weakness because all the necessary vector representations are derived from word vector embeddings carried out using a corpus relevant to the given domain. Thus, all the unique jargon would be adequately covered without much of a threat of conceptual drift ontology. As a future work, we expect this idea to continue for the automated population."}], "references": [{"title": "A translation approach to portable ontology specifications", "author": ["T.R. Gruber"], "venue": "Knowledge Acquisition, 5(2):199-220, 1993.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "The application of latent semantic indexing and ontology in text classification", "author": ["X.-Q. Yang", "N. Sun", "T.-L. Sun", "X.-Y. Cao", "X.-J. Zheng"], "venue": "International Journal of Innovative Computing, Information and Control, vol. 5, no. 12, pp. 4491\u20134499, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Safs3 algorithm: Frequency statistic and semantic similarity based semantic classification use case", "author": ["N. de Silva"], "venue": "Advances in ICT for Emerging Regions (ICTer), 2015 Fifteenth International Conference on, pp. 77\u201383, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised algorithm for concept ontology based word set expansion", "author": ["N. De Silva", "A. Perera", "M. Maldeniya"], "venue": "Advances in ICT for Emerging Regions (ICTer), 2013 International Conference on, pp. 125\u2013 131, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Introduction to wordnet: An on-line lexical database", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross", "K.J. Miller"], "venue": "International journal of lexicography, vol. 3, no. 4, pp. 235\u2013244, 1990.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Nouns in wordnet: a lexical inheritance system", "author": ["G.A. Miller"], "venue": "International journal of Lexicography, vol. 3, no. 4, pp. 245\u2013264, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Building a wordnet for sinhala", "author": ["I. Wijesiri", "M. Gallage", "B. Gunathilaka", "M. Lakjeewa", "D.C. Wimalasuriya", "G. Dias", "R. Paranavithana", "N. De Silva"], "venue": "Volume editors, p. 100, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Omnisearch: a semantic search system based on the ontology for microrna target (omit) for microrna-target gene interaction data", "author": ["J. Huang", "F. Gutierrez", "H.J. Strachan", "D. Dou", "W. Huang", "B. Smith", "J.A. Blake", "K. Eilbeck", "D.A. Natale", "Y. Lin"], "venue": "Journal of biomedical semantics, vol. 7, no. 1, p. 1, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "The development of non-coding rna ontology", "author": ["J. Huang", "K. Eilbeck", "B. Smith", "J.A. Blake", "D. Dou", "W. Huang", "D.A. Natale", "A. Ruttenberg", "J. Huan", "M.T. Zimmermann"], "venue": "International journal of data mining and bioinformatics, vol. 15, no. 3, pp. 214\u2013232, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Ontology-based information extraction: An introduction and a survey of current approaches", "author": ["D.C. Wimalasuriya", "D. Dou"], "venue": "Journal of Information Science, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Discovering inconsistencies in pubmed abstracts through ontology-based information extraction", "author": ["N. de Silva", "D. Dou", "J. Huang"], "venue": "ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB), p. to appear, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "A survey on ontology mapping", "author": ["N. Choi", "I.-Y. Song", "H. Han"], "venue": "ACM Sigmod Record, vol. 35, no. 3, pp. 34\u201341, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pp. 3111\u20133119, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Semap-mapping dependency relationships into semantic frame relationships", "author": ["N. de Silva", "C. Fernando", "M. Maldeniya", "D. Wijeratne", "A. Perera", "B. Goertzel"], "venue": "17th ERU Research Symposium, vol. 17. Faculty of Engineering, University of Moratuwa, Sri Lanka, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Gaussian lda for topic models with word embeddings.", "author": ["R. Das", "M. Zaheer", "C. Dyer"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu", "B. Qin"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, vol. 1, pp. 1555\u20131565, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Study on sentiment computing and classification of sina weibo with word2vec", "author": ["B. Xue", "C. Fu", "Z. Shaobin"], "venue": "Big Data (BigData Congress), 2014 IEEE International Congress on. IEEE, pp. 358\u2013363, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Chinese comments sentiment classification based on word2vec and svm perf", "author": ["D. Zhang", "H. Xu", "Z. Su", "Y. Xu"], "venue": "Expert Systems with Applications, vol. 42, no. 4, pp. 1857\u20131863, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1857}, {"title": "Sentiment analysis of citations using word2vec", "author": ["H. Liu"], "venue": "arXiv preprint arXiv:1704.00177, 2017.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Support vector machines and word2vec for text classification with semantic features", "author": ["J. Lilleberg", "Y. Zhu", "Y. Zhang"], "venue": "Cognitive Informatics & Cognitive Computing (ICCI* CC), 2015 IEEE 14th International Conference on. IEEE, 2015, pp. 136\u2013140.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Using word2vec to build a simple ontology learning system", "author": ["G. Wohlgenannt", "F. Minic"], "venue": "Available at: http://ceur-ws.org/Vol-1690/ paper37.pdf", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "pp. 855\u2013864, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "IEEE transactions on information theory, vol. 28, no. 2, pp. 129\u2013137, 1982.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1982}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Svm-based ontology matching approach", "author": ["L. Liu", "F. Yang", "P. Zhang", "J.-Y. Wu", "L. Hu"], "venue": "International Journal of Automation and Computing, vol. 9, no. 3, pp. 306\u2013314, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "An ontology mapping method based on support vector machine", "author": ["J. Liu", "L. Qin", "H. Wang"], "venue": "pp. 225\u2013226, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "An improved training algorithm for support vector machines", "author": ["E. Osuna", "R. Freund", "F. Girosi"], "venue": "pp. 276\u2013285, 1997.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Gruber [1], an ontology is a \u201cformal and explicit specification of a shared conceptualization\u201d.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 233, "endOffset": 236}, {"referenceID": 2, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 238, "endOffset": 241}, {"referenceID": 3, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 263, "endOffset": 266}, {"referenceID": 4, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 302, "endOffset": 305}, {"referenceID": 6, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 306, "endOffset": 309}, {"referenceID": 7, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 342, "endOffset": 345}, {"referenceID": 8, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 347, "endOffset": 351}, {"referenceID": 9, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 380, "endOffset": 384}, {"referenceID": 10, "context": "The use of ontologies is becoming increasingly involved in various computational tasks given the fact that ontologies can overcome limitations in traditional natural language processing methods in domains such as text classification [2], [3], word set expansions [4], linguistic information management [5]\u2013[8], medical information management [9], [10], and Information Extraction [11], [12].", "startOffset": 386, "endOffset": 390}, {"referenceID": 11, "context": "ontology alignment and semi automated ontology population [13].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "We use a distributed representation of words in a vector space grouped together [14],", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "[15], which is a model architecture for computing continuous vector representations of words from very large data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "An ontology may model either the world or a part of it as seen by the said area\u2019s viewpoint [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "For the purpose of this study, we selected the algorithm presented in [4] which was built on the earlier algorithm described in [17].", "startOffset": 70, "endOffset": 73}, {"referenceID": 14, "context": "For the purpose of this study, we selected the algorithm presented in [4] which was built on the earlier algorithm described in [17].", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "The reason for this selection is: WordNet [5] based linguistic processes are reliable due to the fact that the WordNet lexicon was built on the knowledge of expert linguists.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "Word embedding systems, are a set of natural language modeling and feature learning techniques, where words from a domain are mapped to vectors to create a model that has a distributed representation of words, first proposed by [14].", "startOffset": 228, "endOffset": 232}, {"referenceID": 13, "context": "word2vec1 [15], GloVe [18], and Latent Dirichlet Allocation (LDA) [19] are leading Word Vector Embedding systems.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "word2vec1 [15], GloVe [18], and Latent Dirichlet Allocation (LDA) [19] are leading Word Vector Embedding systems.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "word2vec1 [15], GloVe [18], and Latent Dirichlet Allocation (LDA) [19] are leading Word Vector Embedding systems.", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "Word2vec is used in sentiment analysis [20]\u2013[23] and text classification [24].", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "Word2vec is used in sentiment analysis [20]\u2013[23] and text classification [24].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "Word2vec is used in sentiment analysis [20]\u2013[23] and text classification [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "[25]\u2019s approach to emulate a simple ontology using word2vec and Harmen Prins [26]\u2019s usage of word2vec exten-", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "sion: node2vec [27], to overcome the problems in vectorization of an ontology.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "Arguably, k-means clustering was first proposed by Stuart Lloyd [28] as a method of vector quantization for pulsecode modulation in the domain of signal processing.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Support Vector Machines [29] is a supervised learning model that is commonly used in machine learning tasks that analyze data for the propose of classification or regression analysis.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "[30] defined a", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31] has proposed a method of similarity aggregation using SVM, to classify weighted", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Next we used the algorithm proposed in [4] to expand the instance sets.", "startOffset": 39, "endOffset": 42}, {"referenceID": 13, "context": "The motive behind using a pipeline that pre-processes text up to and including lemmatization instead of the traditional approach of training the word2vec model with just tokenized text [15], was to map all inflected forms of a given lemma to a single entity.", "startOffset": 185, "endOffset": 189}, {"referenceID": 28, "context": "[32] in training support vector machines and then performed certain modifications to output the support vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "We used a set of legal ontology classes seeded by the legal experts and then expanded by the set expansion [4] algorithm under the guidance of the same legal experts.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "We, in this study used the algorithm proposed in [4] for this task to obtain a test dataset.", "startOffset": 49, "endOffset": 52}, {"referenceID": 16, "context": "That would be a topic modeling [19] task.", "startOffset": 31, "endOffset": 35}], "year": 2017, "abstractText": "Selecting a representative vector for a set of vectors is a very common requirement in many algorithmic tasks. Traditionally, the mean or median vector is selected. Ontology classes are sets of homogeneous instance objects that can be converted to a vector space by word vector embeddings. This study proposes a methodology to derive a representative vector for ontology classes whose instances were converted to the vector space. We start by deriving five candidate vectors which are then used to train a machine learning model that would calculate a representative vector for the class. We show that our methodology out-performs the traditional mean and median vector representations. keywords: Ontology, Word Embedding, Representative Vector, Neural Networks, word2vec", "creator": "LaTeX with hyperref package"}}}