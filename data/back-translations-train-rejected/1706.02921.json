{"id": "1706.02921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "End-to-End Musical Key Estimation Using a Convolutional Neural Network", "abstract": "We present an end-to-end system for musical key estimation, based on a convolutional neural network. The proposed system not only out-performs existing key estimation methods proposed in the academic literature; it is also capable of learning a unified model for diverse musical genres that performs comparably to existing systems specialised for specific genres. Our experiments confirm that different genres do differ in their interpretation of tonality, and thus a system tuned e.g. for pop music performs subpar on pieces of electronic music. They also reveal that such cross-genre setups evoke specific types of error (predicting the relative or parallel minor). However, using the data-driven approach proposed in this paper, we can train models that deal with multiple musical styles adequately, and without major losses in accuracy.", "histories": [["v1", "Fri, 9 Jun 2017 12:33:23 GMT  (242kb,D)", "http://arxiv.org/abs/1706.02921v1", "First published in the Proceedings of the 25th European Signal Processing Conference (EUSIPCO-2017) in 2017, published by EURASIP"]], "COMMENTS": "First published in the Proceedings of the 25th European Signal Processing Conference (EUSIPCO-2017) in 2017, published by EURASIP", "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["filip korzeniowski", "gerhard widmer"], "accepted": false, "id": "1706.02921"}, "pdf": {"name": "1706.02921.pdf", "metadata": {"source": "CRF", "title": "End-to-End Musical Key Estimation Using a Convolutional Neural Network", "authors": ["Filip Korzeniowski", "Gerhard Widmer"], "emails": ["filip.korzeniowski@jku.at"], "sections": [{"heading": null, "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "II. METHOD", "text": "Our system consists of two steps: First, we compute a logarithmically filtered log-magnitude spectrogram from the audio. This process is detailed in Sec. II-A. Then we feed this time frequency representation into the Convolutionary Neural Network, which is described in Sec. II-B for classification. Input ProcessingWe input a spectral representation of the audio to our models. Based on our earlier work on extracting harmonic information from audio [11], [12] we first compute the magnetic spectrogram from the audio | S | (image size of 8192 at 5 frames per second, at which the sampling rate is 44.1 kHz); then we apply a B4Log filter bank with logarithmically delimited triangular filters (24 bands per octave, from 65 Hz to 2100 Hz)."}, {"heading": "B. Model", "text": "The proposed neural network is designed to encompass all stages of the classical key estimation pipeline: a pre-processing phase of revolutionary layers, a dense layer that projects the characteristic maps into a short representation at the timeframe level, a global middle layer that aggregates this representation over time, and a Softmax classification layer that predicts the global key of a piece. Figure 1 shows the architecture of our model: five convolutionary layers with 8 characteristic maps calculated by 5 \u00d7 5 cores, followed by a dense layer with 48 framed units; this projection is then averaged over time and classified using a 24-fold Softmax layer. All layers (except the Softmax layer) use the exponential linear activation function [15]. Convolutionary layers represent the first part of the \"feature extraction,\" which in traditional key estimation systems should be equivalent to degrading or degrading factors."}, {"heading": "C. Training", "text": "We train the model with stochastic gradient descent with momentum by re-propagating the categorical cross entropy error between true key label yi and network output y-i through the network and applying the weight drop by a factor of 10 \u2212 4 to regularize it. Initial learning rate is 0.001, with an impulse factor of 0.9. If validation accuracy has not increased within 10 epochs, we halve the learning rate and continue training with the parameters that have yielded the best results until then. After 100 epochs, we select the model that has achieved the best validation accuracy."}, {"heading": "III. EXPERIMENTS", "text": "Our experiments are aimed at 1) comparing the proposed system with reference systems and 2) investigating the impact that the nature of the training data (in our case, the music genre) has on the results. Template-based algorithms require specific key templates for genres such as electronic dance music to perform well [4]. We want to see if and to what extent our system is affected by this. Below, we will discuss the data, evaluation metrics, reference systems and the various structures of our system that we have used in the experiments."}, {"heading": "A. Data", "text": "We use three sets of data in the course of our experiments: the GiantSteps key dataset [16], the GiantSteps-MTG key dataset, and a subset of the McGill Billboard dataset [17]. The GiantSteps key dataset3 dataset includes 604 two-minute audio previews from www.beatport.com, with important basic truths for each extract. It consists of various sub-genres of electronic music. We use this dataset for testing purposes only, and refer to it as GS.The GiantSteps key dataset4, collected by A \ufffd ngel Faraldo of the Music Technology Group at Pompeu Fabra University, includes 1486 two-minute audio previews from the same source. These excerpts differ from those in the GS dataset."}, {"heading": "B. Data Augmentation", "text": "The data sets provide little training data (1077 in GSMTG, 391 in BBTV) compared to data sets used in computer vision (e.g. 40000 in CIFAR-10 or 60000 in MNIST), but the generalization ability of deep neural networks depends on a large number of training samples, so we need to resort to data augmentation to artificially increase the number of training examples. Several audio input augmentation techniques have been researched [18], with pitch shifts being particularly popular for harmonic tasks [11], [19], [20]. Since pitch shifts are a costly time domain operation, most work manipulates the time frequency representation to mimic them. However, in this paper we found that using a pitch shifting algorithm from the time domain directly on the audio provides better results in terms of classification accuracy."}, {"heading": "C. Metrics", "text": "Evaluating the key results requires a more detailed quantitative analysis than calculating the results. In particular, we consider the task to be a simple 24-way classification problem in the design of the system, with some classes semantically closer to each other than others. MIREX evaluation is referred to as a single weighted measure that is only reflected in pitch, so it makes sense to consider some errors as such."}, {"heading": "D. Setups and Reference Systems", "text": "We train our method in three configurations: CK1, trained on GSMTG; CK2, trained on BBTV; and CK3, trained on GSMTG and BBTV. We evaluate each of the trained models on GS and BBTE. In this way, we observe the performance of the system when it is trained on the same genres, how it is tested when it is trained on a different genre (the cross-genre setup), and when it is trained on multiple genres (to see if it can learn a uniform model for different genres). We compare our method with the Queen Mary Key Detector (QM) [21] and three variations of the method presented in [4] (EDMA, EDMM, EDMT). Open source implementations are available for both systems. QM consists of a handmade pre-processing phase and correlates the obtained chromagrams with the key profiles based on Bach's Temperature Profile, EDMA systems are also handmade, but the three also use the EDMA Profiled system."}, {"heading": "E. Results", "text": "Table I shows the evaluation results of all the training configurations of our proposed model and the reference systems. We determine the statistical significance of the results using a signed ranking test by Wilcoxon, with the error types representing the ranking order. If we are trained to the right genre, our model clearly outperforms the reference systems: 74.3 vs. 70.1 (\u03b1 = 0.001) for the GiantSteps dataset and 83.9 vs. 78.7 (\u03b1 = 0.014) for the billboard datasets. However, when examining the cross-genre setups, we note a significant decrease in key accuracy: tested on GS (electronic music), a model trained on BBTV (pop / rock) scores a weighted score of only 57.3, compared to 74.3 when we are trained on GSMTG electronic music. However, we also see that the number of severe errors (category \"other\") that our reference systems in this setup includes is no higher than the 17.4% of the total 17.4% of the reference systems."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "We have presented a global key estimation system based on a Convolutionary Neural Network. Compared to previous work, this model can be trained end-to-end automatically without the need for expertise in feature design or specific pre-processing steps such as tuning correction or spectral whitening. We have experimentally demonstrated that the proposed model is state-of-the-art in electronic music and pop / rock data sets; in addition, we plan to evaluate the proposed model for other genres, such as classical music. Another feature of the proposed model is its ability to adapt to multiple types of music without changing the model itself; it only needs to be retrained with a training set broadened to the type of music of interest. While it still performed well in such a scenario, it did not reach the level of its specialized counterparts. A clear limitation of the proposed method is that it estimates only one global key for a complete piece. While we may consider this appropriate for certain types of music (other types of music are currently available), it is appropriate to increase the degree factor."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the European Research Council (ERC) under the EU's Horizon 2020 Framework Programme (ERC Grant Agreement number 670035, \"Con Espressione\" project), and the Tesla K40 used for this research was donated by NVIDIA Corporation."}], "references": [{"title": "Musical Key Extraction From Audio", "author": ["S. Pauws"], "venue": "Proceedings of the 5th International Society for Music Information Retrieval Conference (ISMIR), Barcelona, Spain, Oct. 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "What\u2019s Key for Key? The Krumhansl-Schmuckler Key- Finding Algorithm Reconsidered", "author": ["D. Temperley"], "venue": "Music Perception: An Interdisciplinary Journal, vol. 17, no. 1, pp. 65\u2013100, Oct. 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Signal Processing Parameters for Tonality Estimation", "author": ["K. Noland", "M. Sandler"], "venue": "Audio Engineering Society Convention 122. Audio Engineering Society, May 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Key Estimation in Electronic Dance Music", "author": ["\u00c1. Faraldo", "E. G\u00f3mez", "S. Jord\u00e0", "P. Herrera"], "venue": "Advances in Information Retrieval, N. Ferro, F. Crestani, M.-F. Moens, J. Mothe, F. Silvestri, G. M. Di Nunzio, C. Hauff, and G. Silvello, Eds. Cham: Springer International Publishing, 2016, vol. 9626, pp. 335\u2013347.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Simultaneous Estimation of Chords and Musical Context From Audio", "author": ["M. Mauch", "S. Dixon"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1280\u20131289, Aug. 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining Musicological Knowledge About Chords and Keys in a Simultaneous Chord and Local Key Estimation System", "author": ["J. Pauwels", "J.-P. Martens"], "venue": "Journal of New Music Research, vol. 43, no. 3, pp. 318\u2013330, Jul. 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "An End-to- End Machine Learning System for Harmonic Analysis of Music", "author": ["Y. Ni", "M. McVicar", "R. Santos-Rodriguez", "T. De Bie"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 6, pp. 1771\u20131783, Aug. 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony", "author": ["B. Di Giorgi", "M. Zanoni", "A. Sarti", "S. Tubaro"], "venue": "Proceedings of the 8th International Workshop on Multidimensional Systems, Erlangen, Germany, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Key detection through pitch class distribution model and ANN", "author": ["J. Sun", "H. Li", "L. Lei"], "venue": "2009 16th International Conference on Digital Signal Processing, Jul. 2009, pp. 1\u20136.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio-based music classification with a pretrained convolutional network", "author": ["S. Dieleman", "P. Brakel", "B. Schrauwen"], "venue": "12th International Society for Music Information Retrieval Conference (ISMIR-2011). University of Miami, 2011, pp. 669\u2013674.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A Fully Convolutional Deep Auditory Model for Musical Chord Recognition", "author": ["F. Korzeniowski", "G. Widmer"], "venue": "Proceedings of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP), Salerno, Italy, Sep. 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature Learning for Chord Recognition: The Deep Chroma Extractor", "author": ["\u2014\u2014"], "venue": "Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR), New York, USA, Aug. 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Potential of Simple Framewise Approaches to Piano Transcription", "author": ["R. Kelz", "M. Dorfer", "F. Korzeniowski", "S. B\u00f6ck", "A. Arzt", "G. Widmer"], "venue": "Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR), New York, USA, Aug. 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Madmom: A new Python Audio and Music Signal Processing Library", "author": ["S. B\u00f6ck", "F. Korzeniowski", "J. Schl\u00fcter", "F. Krebs", "G. Widmer"], "venue": "Proceedings of the 24th ACM International Conference on Multimedia (ACMMM), Amsterdam, Netherlands, Oct. 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "International Conference on Learning Representations (ICLR), arXiv:1511.07289, San Juan, Puerto Rico, Feb. 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections.", "author": ["P. Knees", "A. Faraldo", "P. Herrera", "R. Vogl", "S. B\u00f6ck", "F. H\u00f6rschl\u00e4ger", "M. Le Goff"], "venue": "Proceedings of the 16th", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.", "author": ["J.A. Burgoyne", "J. Wild", "I. Fujinaga"], "venue": "Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks.", "author": ["J. Schl\u00fcter", "T. Grill"], "venue": "Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Improved Techniques for Automatic Chord Recognition from Music Audio Signals", "author": ["T. Cho"], "venue": "Dissertation, New York University, New York, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Rethinking Automatic Chord Recognition with Convolutional Neural Networks", "author": ["E.J. Humphrey", "J.P. Bello"], "venue": "11th International Conference on Machine Learning and Applications (ICMLA). Boca Raton, USA: IEEE, Dec. 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "MIREX 2016 Entry: Vamp Plugins from the Centre for Digital Music", "author": ["C. Cannam", "M. Mauch", "M.E. Davies", "S. Dixon", "C. Landone", "K. Noland", "M. Levy", "M. Zanoni", "D. Stowell", "L.A. Figueira"], "venue": "MIREX, Tech. Rep., 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Examples of such systems are [1]\u2013[3], and, most recently, [4].", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "Examples of such systems are [1]\u2013[3], and, most recently, [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Examples of such systems are [1]\u2013[3], and, most recently, [4].", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "[5]\u2013[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5]\u2013[8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 8, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 171, "endOffset": 175}, {"referenceID": 10, "context": "Based on our previous work on extracting harmonic information from audio [11], [12], we first compute from the audio the magnitude spectrogram |S| (frame size of 8192 at 5 frames per second, where the sample rate is 44.", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "Based on our previous work on extracting harmonic information from audio [11], [12], we first compute from the audio the magnitude spectrogram |S| (frame size of 8192 at 5 frames per second, where the sample rate is 44.", "startOffset": 79, "endOffset": 83}, {"referenceID": 6, "context": "2One might argue that [7] is also an \u201cend-to-end\u201d system that detects both chords and keys.", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "All layers are followed by exponential-linear activations [15], except the last, which is followed by a softmax.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "Additionally, as shown in [13], the constant-q transform does not necessarily lead to better results in tasks relying on pitch information.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "All computations are done using the madmom library [14].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "softmax layer) use the exponential-linear activation function [15].", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "Template-based algorithms require specialised key templates for genres like electronic dance music in order to perform well [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 15, "context": "We use three datasets in the course of our experiments: the GiantSteps key dataset [16], the GiantSteps-MTG key dataset,", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "and a subset of the McGill Billboard dataset [17].", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 20, "context": "Detector (QM) [21] and three variations of the method presented in [4] (EDMA, EDMM, EDMT).", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Detector (QM) [21] and three variations of the method presented in [4] (EDMA, EDMM, EDMT).", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "CK* DENOTE THE PROPOSED MODEL TRAINED ON DIFFERENT DATA SETS, EDM* AND QM DENOTE REFERENCE SYSTEMS BY [4], [21].", "startOffset": 102, "endOffset": 105}, {"referenceID": 20, "context": "CK* DENOTE THE PROPOSED MODEL TRAINED ON DIFFERENT DATA SETS, EDM* AND QM DENOTE REFERENCE SYSTEMS BY [4], [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "The numbers presented for the EDM* systems for the GiantSteps dataset differ from the ones originally reported in [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "This is mainly because we applied a stricter criterion for the \u201cfifth\u201d category: we require the predicted mode to match the target mode, while [4] ignores the mode for this category.", "startOffset": 143, "endOffset": 146}], "year": 2017, "abstractText": "We present an end-to-end system for musical key estimation, based on a convolutional neural network. The proposed system not only out-performs existing key estimation methods proposed in the academic literature; it is also capable of learning a unified model for diverse musical genres that performs comparably to existing systems specialised for specific genres. Our experiments confirm that different genres do differ in their interpretation of tonality, and thus a system tuned e.g. for pop music performs subpar on pieces of electronic music. They also reveal that such cross-genre setups evoke specific types of error (predicting the relative or parallel minor). However, using the data-driven approach proposed in this paper, we can train models that deal with multiple musical styles adequately, and without major losses in accuracy.", "creator": "LaTeX with hyperref package"}}}