{"id": "1706.02949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "K+ Means : An Enhancement Over K-Means Clustering Algorithm", "abstract": "K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set to a predefined, say K number of clusters. Determination of K is a difficult job and it is not known that which value of K can partition the objects as per our intuition. To overcome this problem we proposed K+ Means algorithm. This algorithm is an enhancement over K-Means algorithm.", "histories": [["v1", "Thu, 8 Jun 2017 10:06:25 GMT  (109kb)", "http://arxiv.org/abs/1706.02949v1", null], ["v2", "Thu, 22 Jun 2017 13:25:51 GMT  (87kb)", "http://arxiv.org/abs/1706.02949v2", "Authors: Co-author's name added Section 3: Step (a) and (b) of K+Means algorithm are merged for simplicity. Section 3.1: K+ Means algorithm complexity rectified. Section 4.3: Figure-7 and Figure-8 modified for clarity"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["srikanta kolay", "kumar sankar ray", "abhoy chand mondal"], "accepted": false, "id": "1706.02949"}, "pdf": {"name": "1706.02949.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Srikanta Kolay"], "emails": ["kolaysrikanta@gmail.com", "ksray@isical.ac.in"], "sections": [{"heading": null, "text": "Determining K is a difficult task and it is not known which value of K can divide the objects according to our intuition. To solve this problem, we proposed the K + Means algorithm. This algorithm is an extension of the K-Means algorithm. Keywords: K-Means, K + Means, Clustering."}, {"heading": "1. Introduction", "text": "Hierarchical clustering [2] [3], partial clustering [4], Bayesian clustering [5] are the various cluster techniques. K-Means cluster algorithm is considered the most popular cluster algorithm due to its simplicity and efficiency. For this reason, we use K-Means algorithm for improvement. We extend the algorithm to K + Means by using the strengths of the algorithm and eliminating the weaknesses of the algorithm."}, {"heading": "2. K-Means Algorithm", "text": "(a) Place K-points in the space represented by the objects to be collected. These points represent initial group trioids. (b) Assign each object to the group that has the nearest centrioid. (c) When all objects have been assigned, recalculate the positions of the K-centrioids. (d) Repeat steps (b) and (c) until the centrioids stop moving, resulting in a division of the objects into groups from which the metric to be minimized can be calculated."}, {"heading": "2.1. Strengths of K-Means Algorithm", "text": "The strengths of the algorithm are the following: (a) Simple: easy to understand and implement. (b) Efficient: Time complexity: O (tkn), where n is the number of data points, k is the number of clusters and t is the number of iterations. Since both k and t are small K averages, it is considered a linear algorithm."}, {"heading": "2.2. Weaknesses of K-Means Algorithm", "text": "The weaknesses of the algorithm are as follows: (a) The user must specify K. (b) The algorithm is sensitive to outliers. Outliers are data points that are very far away from other data points."}, {"heading": "3. K+ Means Algorithm", "text": "(a) Place K-points in the space represented by the objects to be collected. These points represent initial group trioids. (b) Assign each object to the group that has the nearest centrioid. (c) Min, Max and Average cluster differences for each of the K clusters are calculated. (d) The average distance within the cluster will be approximately similar and preferably small for each of the K clusters. (e) If the average is greater for each cluster, its maximum and minimum values will be checked. If the maximum is high, an outlier will be detected because it has the maximum distance from its cluster representative. (f) Now, taking this outlier as another new representative, the algorithm is repeated to assign objects to the K + 1 cluster. (g) The algorithm repeats until no new representatives are formed and existing representatives do not change.The distance between the objects is assumed to be eudic distance."}, {"heading": "3.1. Strengths of K+ Means Algorithm", "text": "The strengths of the algorithm are as follows: (a) Simple: easy to understand and implement. (b) Efficient: Time complexity: O (tkn) where n is the number of data points, k is the number of clusters and t is the number of iterations. Here, the value of t is higher than the K-Means algorithm. (c) The user only needs to specify the initial value k. The actual number of clusters is determined from the data. (d) The algorithm is not sensitive to outliers. In the case of outliers, it defines a new cluster."}, {"heading": "4. An Worked Out Example", "text": "Consider an example data set from Table-1: Object / point x yp1 1 4 p2 1 3 p3 2 2 p4 7 2p5 8 3 p6 9 2 p7 5 6 p8 6 7 p9 7 6p10 8 7Based on the table above, the objects are drawn in 2D space as shown in Figure-1 below: Figure-1: Example data"}, {"heading": "4. 1. Application of K-Means Algorithm", "text": "Let us assume K = 2 and select p1 and p5 as the initial mean. If we now execute the K mean algorithm, we get cluster 1 with {p1, p2, p3} and cluster 2 with {P4, p5, p6, p7, p8, p9, p10}. The points (objects) and cluster centers according to the K mean algorithm are shown in Figure 2: Cluster according to the K mean algorithm"}, {"heading": "4. 2. Application of K+ Means Algorithm", "text": "Let's assume K = 2 and select p1 and p5 as initial centroids. Based on this, we get the initial clusters as shown in Figure-3 below: Figure-3: Initial clusters Now we calculate the new centroids and get the clusters as shown in Figure-4 below: Figure-4: New cluster centroids Now we calculate the cluster distances of c1 and c2. Consider c1: Min = 0.33, Max = 1.20, Avg = 0.86 Consider c2: Min = 1.30, Max = 3.29 Avg = 2.39 Here, the Avg value is comparatively high for c2. The max value is also high. Thus, we get the runaway object p6 as d (c2, p6) = 3.29. Let's take c3 (9.2) as new cluster centroids, we assign the objects in c1, c2 and c3 as clusters back as clusters, as shown in Figure-5: Now we calculate the Zroid = 3, beginning figure 5: 86."}, {"heading": "4.3. Comparison:", "text": "The K + Means algorithm provides better clusters than the K + Means algorithm. Since the number of iterations in the K + Means algorithm is higher than the number of iterations in the K + Means algorithm, the K + Means algorithm is less costly than the K + Means algorithm. The following example shows how two algorithms work for outlier objects. Figure 7 shows how K + Means algorithms group objects when we perform the algorithm under K = 2.Figure 7: Grouping by K + Means algorithm Figure 8 shows how K + Means algorithms group objects when we execute the algorithm under K = 2.Figure 8: Grouping by K + Means algorithm."}, {"heading": "9. Conclusion", "text": "In this work, we have expanded the K-Means algorithm to develop a new K + Means algorithm. In this K + Means algorithm, all the strengths of the K-Means algorithm are taken into account and the weaknesses of the K-Means algorithms are eliminated."}], "references": [{"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MCQUEEN"], "venue": "In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1967}, {"title": "Bayesian clustering using hidden Markov random fields in spatial population", "author": ["O Fran\u00e7ois", "S Ancelet", "G Guillot"], "venue": "genetics. Genetics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Abstract K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Introduction Hierarchical clustering[2][3], Partitional clustering[4], Bayesian clustering[5] are the different kind of clustering techniques.", "startOffset": 90, "endOffset": 93}], "year": 2017, "abstractText": "K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set to a predefined, say K number of clusters. Determination of K is a difficult job and it is not known that which value of K can partition the objects as per our intuition. To overcome this problem we proposed K+ Means algorithm. This algorithm is an enhancement over K-Means algorithm.", "creator": "PScript5.dll Version 5.2.2"}}}