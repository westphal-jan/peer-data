{"id": "1706.03059", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Depthwise Separable Convolutions for Neural Machine Translation", "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new \"super-separable\" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.", "histories": [["v1", "Fri, 9 Jun 2017 17:59:16 GMT  (227kb,D)", "http://arxiv.org/abs/1706.03059v1", null], ["v2", "Fri, 16 Jun 2017 02:35:48 GMT  (226kb,D)", "http://arxiv.org/abs/1706.03059v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lukasz kaiser", "aidan n gomez", "francois chollet"], "accepted": false, "id": "1706.03059"}, "pdf": {"name": "1706.03059.pdf", "metadata": {"source": "CRF", "title": "Depthwise Separable Convolutions for Neural Machine Translation", "authors": ["\u0141ukasz Kaiser", "Aidan N. Gomez", "Fran\u00e7ois Chollet"], "emails": ["lukaszkaiser@google.com", "aidan@cs.toronto.edu", "fchollet@google.com"], "sections": [{"heading": null, "text": "Independently separable windings reduce the number of parameters and calculations used in winding operations, while increasing display efficiency. It has been shown that they succeed both in image classification, by obtaining better models than previously possible for a certain number of parameters (the Xception architecture), and in significantly reducing the number of parameters required at a certain level (the MobileNets architecture family). Recently, revolutionary sequence-to-sequence networks have been applied to machine translation tasks with good results. As part of this work, we are investigating how independently separable windings can be applied to neural machine translation. We are introducing a new architecture inspired by Xception and ByteNet called SliceNet, which enables a significant reduction in the number of parameters and amount of computations needed to achieve and achieve results such as ByteNet, and with a similar number of parameters based on the latest technology."}, {"heading": "1 Introduction", "text": "In recent years, sequence-to-sequence recursive neural networks (RNNs) with long-term short-term memory cells (LSTM) [8] have proven successful in many tasks of natural language processing (NLP), including machine translation [19, 3, 5]. In fact, the results were so good that the gap between human translations and machine translation has significantly narrowed [23] and LSTM-based recursive neural networks have become standard in natural language processing [11, 7]. More recently, authoregressive revolutionary models have proved extremely effective when applied to audio [20], image [21] and text generation. Their success in sequence data competes or exceeds that of previous recursive models [11, 7]."}, {"heading": "2 Our contribution", "text": "We present a new sequence-to-sequence architecture called SliceNet and apply it to machine translation tasks, achieving state-of-the-art results. Our architecture has two key ideas: \u2022 Inspired by the Xception network [6], our model is a stack of separable folding layers with residual connections, an architecture that has already proven itself for classifying images. We also experimented with the use of grouped windings (or \"subseparable windings\") and added even more separation to our new super-separable windings. \u2022 We eliminate filtering in our architecture after studying the trade-off between filter dilation and larger folding windows. Filter dilation used to be a key component of successful 1D convolutions architectures for sequence-to-sequence tasks, such as ByteNet [11] and WaveNet [20], but thanks to separation we get better results without dilation."}, {"heading": "2.1 Separable convolutions and grouped convolutions", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "2.2 Super-separable convolutions", "text": "As can be seen above, the size (and cost) of a divisible folding with c channels and a receptive field of size k \u00b7 c + c2. If k is small compared to c (as is customary), the term c2 dominates, raising the question of how it could be reduced. We use the idea of group folding and the most recent divisible LSTM paper [12] to further reduce this size by including the final 1 \u00d7 1 folding in g groups, and call the result a super-divisible folding. We define a super-divisible folding (SuperSC noted) with g groups as follows. Applied to a tensor x, we first divide x to the depth dimension in g groups, then apply a divisible folding to each group individually, and then contact the results on the depth dimension.SuperSCg (Wp, Wd, x) = contact depth (W 1 d, x)."}, {"heading": "2.3 Filter dilation and convolution window size", "text": "It was presented in [11] and [20] as a key component of the preventive sequence-to-sequence of authoregressive architectures. If the preventive layers are stacked in such a way that the dilatation values of successive layers share common dividers, a problem similar to the chessboard artifact in the deconvolutions appears. Uneven filter cover leads to dead zones in which the filter cover is reduced (as is the case in the plaid-like representation of Figure 1)."}, {"heading": "3 SliceNet architecture", "text": "Our model follows the twisted auto-regressive structure introduced by ByteNet [11], WaveNet [20] and PixelCNN [21]. Inputs and outputs are embedded in the same depth of functionality, encoded and concatenated by two separate subnetworks, before being fed into a decoder that generates each element of output auto-regressively. At each step, the auto-regressive decoder generates a new output forecast based on the encoded inputs and encoding of existing predicted outputs. The encoders and decoders (described in Section 3.3) are built from stacks of conversion modules (described in Section 3.1) and attention (described in Section 3.2) is used to obtain information from the encoder from the decoder's decoder."}, {"heading": "3.1 Convolutional modules", "text": "To perform local calculations, we use modules of convolutions with ReLU nonlinearity and layer normalization. Each step in our module consists of three components: a ReLU activation of the inputs, followed by a divisible convolution SepConv, followed by a layer normalization. Layer normalization [2] acts on the hidden units of the underlying layer, computes layer-by-layer statistics and normalizes accordingly. These normalized units are then scaled and shifted by scalar learned parameters G or B, generating the final units to be activated by a non-linearity: LN (x) = G\u03c3 (x) (x \u2212 \u00b5 (x)) + B constellation (x) = constellation 1 h (x) and constellation (x) = constellation 1 (x).W = 1 h xi constellation (x)."}, {"heading": "3.2 Attention modules", "text": "The Attention Mechanism calculates the characteristic-vector similarities at each position and recalculates them according to the depth: Attend (source, target) = 1 \u221a Depth \u00b7 Softmax (target, source) \u00b7 Source To give attention access to position information, we add a signal that carries it. We call this signal Timing, it is a tensor of any shape [k, depth] defined by concatenation of sine and cosine functions of different frequencies, which are calculated up to k: Timing (t, 2d) = Sin (t / 100002d / depth) Timing (t, 2d + 1) Timing (t, 2d + 1) = cos (t / 100002d / depth) \u00b7 Our complete attention mechanism consists in adding the timing signal to the targets, performing two turn steps and then taking care of the source: Attrition1 (t, 2d + 1) = Conv1.1 (t / 100002d / depth) Attention mechanism consists of W (Attention mechanism) and then taking care of the target (1)."}, {"heading": "3.3 Autoregressive structure", "text": "As already discussed, the outputs of our model are generated in an autoregressive way. In contrast to RNNs, autoregressive sequence generation is not only dependent on the output previously generated, but potentially on all previously generated outputs. This notion of long-term dependencies has proven highly effective in the NMT in the past. By paying attention, it has been shown that creating long-term dependencies significantly increases the task performance of RNNs for NMT [4]. Similarly, a revolutionary autoregressive generation scheme provides large receptive fields about the inputs and past outputs that are capable of establishing these long-term dependencies. In the following, we will explain the structure of the InputEncoder, IOMixer, and Decoder. Output embedding simply performs a learning embedding. We name the concatenation of tensors a and b along the dth dimension (a-d) as [b]."}, {"heading": "4 Related Work", "text": "The basic sequence-to-sequence architecture consists of an RNN encoder that reads the source set at a time and converts it into a fixed state vector, followed by an RNN decoder that generates the target sentence at a time when the state vector becomes a pure sequence, whereas a pure sequence-to-sequence sequence-sequence-sequence-sequence-sequence-sequence-sequencer-sequencer-sequencer-sequencer-sequencer-sequencoder-sequencoder-sequencer-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencer-sequencer-sequencer-sequencer-sequence-sequencer-sequencoder-sequencer-sequencoder-sequencoder-sequencoder-sequencoder-sequence-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequencoder-sequence-sequence-sequencer-sequence-sequencer-sequence-sequencer-sequencer-sequencer-sequencer-sequence-sequencer-sequencer-sequencer-sequence-sequencer-sequencer-sequencer-sequencer-sequencer-sequence-sequencer-sequence-sequencer-sequencer-sequencer-sequencer-sequence-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequence-sequencer-sequence-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequ"}, {"heading": "5 Experiments", "text": "We are reinventing ourselves at a time when we are not yet ready to be able to find a solution that meets the needs of the individual."}, {"heading": "5.1 Conclusions", "text": "We have shown how this architecture achieves results that not only exceed ByteNet, but also state-of-the-art, while using more than two times fewer (non-embedded) parameters and floating-point operations than the ByteNet architecture. In addition, we have shown that filter dilation, which was previously considered a key component of successful Convolutionary Sequence-to-Sequence architectures, was not a requirement. Using separable windings makes much larger conversion window sizes possible, and we have found that we can achieve the best results by using larger windows instead of extended filters. We have also introduced a new type of separable convolution, the super separable convolution, which has incremental performance improvements over separable convolutions. Our work is another point on a significant dependence that starts with xcentric and percentage D models, both replaced by lower-cost convolutions."}], "references": [{"title": "Tensorflow: Largescale machine learning on heterogeneous distributed systems, 2015", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "URL http://download", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "CoRR, abs/1409.1259,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Xception: Deep learning with depthwise separable convolutions", "author": ["Fran\u00e7ois Chollet"], "venue": "arXiv preprint arXiv:1610.02357,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin"], "venue": "arXiv preprint arXiv:1705.03122,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "author": ["Andrew G Howard", "Menglong Zhu", "Bo Chen", "Dmitry Kalenichenko", "Weijun Wang", "Tobias Weyand", "Marco Andreetto", "Hartwig Adam"], "venue": "arXiv preprint arXiv:1704.04861,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings EMNLP", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.10099,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Factorization tricks for LSTM networks", "author": ["Oleksii Kuchaiev", "Boris Ginsburg"], "venue": "CoRR, abs/1703.10722,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Can active memory replace attention", "author": ["Samy Bengio \u0141ukasz Kaiser"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": "In ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "venue": "arXiv preprint arXiv:1701.06538,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Rotation, scaling and deformation invariant scattering for texture discrimination", "author": ["Laurent Sifre", "St\u00e9phane Mallat"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems, pages 3104\u20133112,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Wavenet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Lasse Espeholt", "Oriol Vinyals", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning visual representations at scale", "author": ["Vincent Vanhoucke"], "venue": "ICLR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "Lukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey 9  Dean"], "venue": "CoRR, abs/1609.08144,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "In recent years, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [8] have proven successful at many natural language processing (NLP) tasks, including machine translation [19, 3, 5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 16, "context": "In recent years, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [8] have proven successful at many natural language processing (NLP) tasks, including machine translation [19, 3, 5].", "startOffset": 218, "endOffset": 228}, {"referenceID": 1, "context": "In recent years, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [8] have proven successful at many natural language processing (NLP) tasks, including machine translation [19, 3, 5].", "startOffset": 218, "endOffset": 228}, {"referenceID": 3, "context": "In recent years, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [8] have proven successful at many natural language processing (NLP) tasks, including machine translation [19, 3, 5].", "startOffset": 218, "endOffset": 228}, {"referenceID": 20, "context": "In fact, the results they yielded have been so good that the gap between human translations and machine translations has narrowed significantly [23] and LSTM-based recurrent neural networks have become standard in natural language processing.", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "Even more recently, auto-regressive convolutional models have proven highly effective when applied to audio [20], image [21] and text generation [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "Even more recently, auto-regressive convolutional models have proven highly effective when applied to audio [20], image [21] and text generation [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Even more recently, auto-regressive convolutional models have proven highly effective when applied to audio [20], image [21] and text generation [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "Their success on sequence data in particular rivals or surpasses that of previous recurrent models [11, 7].", "startOffset": 99, "endOffset": 106}, {"referenceID": 5, "context": "Their success on sequence data in particular rivals or surpasses that of previous recurrent models [11, 7].", "startOffset": 99, "endOffset": 106}, {"referenceID": 4, "context": "These are the principal concerns addressed within this work: inspired by the efficiency of depthwise separable convolutions demonstrated in the domain of vision, in particular the Xception architecture [6] and MobileNets [9], we generalize these techniques and apply them to the language domain, with great success.", "startOffset": 202, "endOffset": 205}, {"referenceID": 7, "context": "These are the principal concerns addressed within this work: inspired by the efficiency of depthwise separable convolutions demonstrated in the domain of vision, in particular the Xception architecture [6] and MobileNets [9], we generalize these techniques and apply them to the language domain, with great success.", "startOffset": 221, "endOffset": 224}, {"referenceID": 4, "context": "\u2022 Inspired by the Xception network [6], our model is a stack of depthwise separable convolution layers with residual connections.", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "Filter dilation was previously a key component of successful 1D convolutional architectures for sequence-to-sequence tasks, such as ByteNet [11] and WaveNet [20], but we obtain better results without dilation thanks to separability.", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "Filter dilation was previously a key component of successful 1D convolutional architectures for sequence-to-sequence tasks, such as ByteNet [11] and WaveNet [20], but we obtain better results without dilation thanks to separability.", "startOffset": 157, "endOffset": 161}, {"referenceID": 4, "context": "The depthwise separable convolution operation can be understood as related to both grouped convolutions and the \"inception modules\" used by the Inception family of convolutional network architectures, a connection explored in Xception [6].", "startOffset": 235, "endOffset": 238}, {"referenceID": 4, "context": "Depthwise separable convolutions have been previously shown in Xception [6] to allow for image classification models that outperform similar architectures with the same number of parameters, by making more efficient use of the parameters available for representation learning.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "In MobileNets [9], depthwise separable convolutions allowed to create very small image classification models (e.", "startOffset": 14, "endOffset": 17}, {"referenceID": 10, "context": "We use the idea from group convolutions and the recent separable-LSTM paper [12] to further reduce this size by factoring the final 1\u00d7 1 convolution, and we call the result a super-separable convolution.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "Filter dilation, as introduced in [24], is a technique for aggregating multiscale information across considerably larger receptive fields in convolution operations, while avoiding an explosion in parameter count for the convolution kernels.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "It has been presented in [11] and [20] as a key component of convolutional sequence-to-sequence autoregressive architectures.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "It has been presented in [11] and [20] as a key component of convolutional sequence-to-sequence autoregressive architectures.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "Uneven filter coverage results in dead zones where filter coverage is reduced (as displayed in the plaid-like appearance of Figure 1 in [24]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "Our model follows the convolutional autoregressive structure introduced by ByteNet [11], WaveNet [20] and PixelCNN [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "Our model follows the convolutional autoregressive structure introduced by ByteNet [11], WaveNet [20] and PixelCNN [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "Our model follows the convolutional autoregressive structure introduced by ByteNet [11], WaveNet [20] and PixelCNN [21].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "By using attention, establishing long term dependencies has been shown to significantly boost task performance of RNNs for NMT [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 16, "context": "Machine translation using deep neural networks achieved great success with sequence-to-sequence models [19, 3, 5] that used recurrent neural networks (RNNs) with long short-term memory (LSTM, [8]) cells.", "startOffset": 103, "endOffset": 113}, {"referenceID": 1, "context": "Machine translation using deep neural networks achieved great success with sequence-to-sequence models [19, 3, 5] that used recurrent neural networks (RNNs) with long short-term memory (LSTM, [8]) cells.", "startOffset": 103, "endOffset": 113}, {"referenceID": 3, "context": "Machine translation using deep neural networks achieved great success with sequence-to-sequence models [19, 3, 5] that used recurrent neural networks (RNNs) with long short-term memory (LSTM, [8]) cells.", "startOffset": 103, "endOffset": 113}, {"referenceID": 6, "context": "Machine translation using deep neural networks achieved great success with sequence-to-sequence models [19, 3, 5] that used recurrent neural networks (RNNs) with long short-term memory (LSTM, [8]) cells.", "startOffset": 192, "endOffset": 195}, {"referenceID": 16, "context": "While a pure sequence-to-sequence recurrent neural network can already obtain good translation results [19, 5], it suffers from the fact that the whole input sentence needs to be encoded into a single fixed-size vector.", "startOffset": 103, "endOffset": 110}, {"referenceID": 3, "context": "While a pure sequence-to-sequence recurrent neural network can already obtain good translation results [19, 5], it suffers from the fact that the whole input sentence needs to be encoded into a single fixed-size vector.", "startOffset": 103, "endOffset": 110}, {"referenceID": 1, "context": "This clearly manifests itself in the degradation of translation quality on longer sentences and was overcome in [3] by using a neural model of attention.", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from [10] and later in [14].", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from [10] and later in [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "The state of this RNN has a fixed size, and in the first one the sentence representation generated by the convolutional network is also a fixed-size vector, which creates a bottleneck and hurts performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention [19, 5] discussed above.", "startOffset": 321, "endOffset": 328}, {"referenceID": 3, "context": "The state of this RNN has a fixed size, and in the first one the sentence representation generated by the convolutional network is also a fixed-size vector, which creates a bottleneck and hurts performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention [19, 5] discussed above.", "startOffset": 321, "endOffset": 328}, {"referenceID": 11, "context": "Fully convolutional neural machine translation without this bottleneck was first achieved in [13] and [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "Fully convolutional neural machine translation without this bottleneck was first achieved in [13] and [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "The model in [13] (Extended Neural GPU) used a recurrent stack of gated convolutional layers, while the model in [11] (ByteNet) did away with recursion and used left-padded convolutions in the decoder.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "The model in [13] (Extended Neural GPU) used a recurrent stack of gated convolutional layers, while the model in [11] (ByteNet) did away with recursion and used left-padded convolutions in the decoder.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "This idea, introduced in WaveNet [20], significantly improves efficiency of the model.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "The same technique is used in SliceNet as well, and it has been used in a number of neural translation models recently, most notably in [7] where it is combined with an attention mechanism in a way similar to SliceNet.", "startOffset": 136, "endOffset": 139}, {"referenceID": 15, "context": "Depthwise separable convolutions were first studied by Sifre [18] during a 2013 internship at Google Brain, and were first introduced in an ICLR 2014 presentation [22].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "Depthwise separable convolutions were first studied by Sifre [18] during a 2013 internship at Google Brain, and were first introduced in an ICLR 2014 presentation [22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 4, "context": "In 2016, they were demonstrated to yield strong results on large-scale image classification in Xception [6], and in 2017 they were shown to lead to small and parameter-efficient image classification models in MobileNets [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "In 2016, they were demonstrated to yield strong results on large-scale image classification in Xception [6], and in 2017 they were shown to lead to small and parameter-efficient image classification models in MobileNets [9].", "startOffset": 220, "endOffset": 223}, {"referenceID": 13, "context": "For tokenization, we use subword units, and follow the same tokenization process as Sennrich [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "All of our experiments are implemented using the TensorFlow framework [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "1 ByteNet [11] 23.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "8 GNMT [23] 24.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "6 ConvS2S [7] 25.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "1 GNMT+Mixture of Expert [17] 26.", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new \"super-separable\" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.", "creator": "LaTeX with hyperref package"}}}