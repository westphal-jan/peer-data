{"id": "1706.03146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Rethinking Skip-thought: A Neighborhood based Approach", "abstract": "We study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our model didn't aid our model to perform better, while it hurts the performance of the skip-thought model.", "histories": [["v1", "Fri, 9 Jun 2017 22:39:31 GMT  (201kb,D)", "http://arxiv.org/abs/1706.03146v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["shuai tang", "hailin jin", "chen fang", "zhaowen wang", "virginia r de sa"], "accepted": false, "id": "1706.03146"}, "pdf": {"name": "1706.03146.pdf", "metadata": {"source": "CRF", "title": "Rethinking Skip-thought: A Neighborhood based Approach", "authors": ["Shuai Tang", "Hailin Jin", "Chen Fang", "Zhaowen Wang", "Virginia R. de Sa"], "emails": ["shuaitang93@ucsd.edu,", "desa@ucsd.edu,", "hljin@adobe.com", "cfang@adobe.com", "zhawang@adobe.com"], "sections": [{"heading": "1 Introduction", "text": "We are interested in learning distributed sentence representation in an unattended manner. Previously, the skip-thinking model was introduced by Kiros et al. (2015), which learns to explore semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder; the skip model encodes the current sentence and then decodes the previous sentence and the next sentence instead of itself. Two independent decoders were used, since the previous sentence and the next sentence should intuitively be drawn from two different conditional distributions; by hypothesizing that the adjacent sentences provide the same neighborhood information for learning sentence representation, we first let one of the two decoders use only one decoder decoder to reconstruct the surrounding two sentences at the same time. Empirical results show that our skipped neighborhood model works as well as the covered model for 7 assessment tasks."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3 Approach", "text": "In this section, we present the model of leapfrogged thinking. We first briefly introduce the leapfrogged thinking model (Kiros et al., 2015) and then discuss how the decoders in the leapfrogged thinking model can be explicitly modified in order to obtain the leapfrogged neighbourhood model."}, {"heading": "3.1 Skip-thought Model", "text": "In the skipping model, the encoder calculates a vector of fixed dimensions as a representation zi for the sentence si, which learns a distribution p (si | si; \u03b8e), where \u03b8e stands for the set of parameters in the encoder. Subsequently, due to the representation zi, two separate decoders are used to reconstruct the preceding sentence si \u2212 1 and the next sentence si + 1, respectively. Since the two conditional distributions learned by the decoders are parameterized independently of each other, they implicitly use the information on the sentence sequence within the sentence. Intuitively, taking into account the current sentence si \u2212 1, it is assumed that the preceding sentence si \u2212 1 differs from the next sentence + si."}, {"heading": "3.2 Encoder: GRU", "text": "To make the comparison fair, we choose a recursive neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recursive unit used in Kiros et al. (2015). Since the comparison between different recursive units is not our main focus, we opted for GRU, a fast and stable recursive unit. Furthermore, Chung et al. (2014) shows that GRU works just as well in speech modeling tasks as short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).The assumed sentence si contains N-words, the w1i, w 2 i,..., w N i. At any time step t, the encoder generates a hidden state hti, and we consider it the representation of the previous sub-sequence through time. At the time N \u2212 the hidden state hNi stands for the given sentence si, which is zi."}, {"heading": "3.3 Decoder: Conditional GRU", "text": "The decoder must reconstruct the previous record si \u2212 1 and the next record si + 1. Specifically, the decoder is a recursive neural network with conditional GRU, and takes the representation zi as an additional input at each step."}, {"heading": "3.4 Skip-thought Neighbor Model", "text": "Our hypothesis is that even without the order information within a given sentence, the skipped model should behave similarly with respect to the reconstruction error and perform similar evaluation tasks. To modify the skipped model, we assume that skipping si \u2212 1 is the same as skipping si + 1. If we define {si \u2212 1, si + 1} as the two neighbors of si, then the skipped process can be called sj \u0445 p (s | zi; \u03b8d) for each j in the neighborhood of si. The conditional distribution learned by the decoder is parameterized by the decoder. In experiments, we directly drop one of the two decoders and use only one decoder to reconstruct the previous sentence si \u2212 1 and the next sentence si + 1 simultaneously, with the representation zi of si. Our skipped neighboring model can be viewed as part of the parameters between the previous decoder and the next decoder."}, {"heading": "3.5 Skip-thought Neighbor with Autoencoder", "text": "Previously, we defined {si \u2212 1, si + 1} as the two neighbors of si. Furthermore, we assume that si could also be a neighbor of itself. Therefore, the neighborhood from si becomes {si \u2212 1, si, si + 1}. By inserting an autoencoder path for each j in the neighborhood of si into our skipped neighbor model, the decoder in the model must reconstruct all three sets {si \u2212 1, si, si + 1} in the neighborhood of si simultaneously. The objective function is supplemented by the addition of an autoencoder path in its FastSent model. Their results show that the additional autoencoder path has slightly improved performance on the autoencoder path tasks."}, {"heading": "3.6 Skip-thought Neighbor with One Target", "text": "In our Skipped Neighbourhood Model, for a given sentence si, the decoder must reconstruct the sentences in its neighborhood {si \u2212 1, si + 1}, which are two goals. We refer to the inference process as si \u2192 {si \u2212 1, si + 1}. For the next sentence si + 1, the inference process is si + 1 \u2192 {si, si + 2}. In other words, for a given sentence pair {si, si + 1}, the inference process includes si \u2192 si + 1 and si + 1 \u2192 si. In our hypothesis, the model does not distinguish between sentences in a neighborhood. In this case, an inference process that includes si \u2192 si + 1 and si + 1 \u2192 si is equivalent to an inference process with only one of them. Therefore, we define a skipped neighborhood model with only one goal, and the goal is always the next sentence."}, {"heading": "4 Experiment Settings", "text": "The large corpus we used for unattended training is the data set BookCorpus (Zhu et al., 2015), which contains a total of 74 million sentences from 7000 books. All of our experiments were conducted in Torch7 (Collobert et al., 2011). To make the comparison fair, we have reimplemented the skip-thinking model under the same settings as Kiros et al. (2015) and the publicly available Theano-Code1. We use the ADAM algorithm (Kingma and Ba, 2014) for optimization. Instead of applying the gradient clipping standard used in Kiros et al. (2015), we have clipped the gradient clipping directly to make it stable within [\u2212 1, 1]."}, {"heading": "5 Quantitative Evaluation", "text": "We compared our proposed model with the model of skipped thinking on 7 evaluation tasks, which include semantic relationality, paraphrase recognition, question-type classification, and 4 benchmark sentiment and subjective datasets. After unattended training on the BookCorpus dataset, we fix the parameters in the encoder and apply it as a sentence extractor to the 7 tasks. For semantic relativity, we use the SICK dataset (Marelli et al., 2014), and we adopt the feature engineering idea proposed by Tai et al. (2015) The given sentence pair computes the encoder a pair of representations referred to as u and v, and the concatenation of the component-wise product u \u00b7 v and the absolute difference between the component-wise product u \u00b7 v and the absolute difference | u \u2212 v scripts Scripts is considered the scripts-scripts-pair model for the scripts-scripts."}, {"heading": "5.1 Skip-thought Neighbor vs. Skip-thought", "text": "Based on the results we show in Table 1, we can see that our neighborhood models with skipped thinking work just as well as models with skipped thinking, but with fewer parameters, which means that the neighborhood information is effective in helping the model capture sentential context information."}, {"heading": "5.2 Skip-thought Neighbor+AE vs. Skip-thought+AE", "text": "For our Skipped Thinking Neighborhood Model, the inclusion of an antooencoder (+ AE) means that, in addition to reconstructing the two neighbors si \u2212 1 and si + 1, the decoder must also reconstruct si. Since the implicit hypothesis in the model is that different decoders learn different conditional distributions, we add another decoder in the Skipped Thinking Model to reconstruct the si sentence. Results are also shown in Table 1. As we can see, our Skipped Thinking + AE models significantly outperform our Neighbourhood + AE models, especially in the Skipped Thinking Model, adding an auto encoder branch to the SICK, MR, CR, SUBJ and MPQAdataset. We find that the reconstruction error on the encoder branch drastically reduces the model, while the sum of reconstruction errors on the previous or decoder."}, {"heading": "5.3 Increasing the Number of Neighbors", "text": "In addition to using a decoder to predict the previous set and the next set, we expand the neighborhood to 4 sets, i.e. the previous 2 sets, and the next 2 sets. In this case, the decoder must reconstruct 4 sets at the same time. We conducted experiments with our model and evaluated the trained encoder based on 7 tasks. In our model, which was trained with 4 neighbors, there is no significant increase or loss in performance; it seems that increasing the number of neighbors does not improve performance, but does not harm either. Our hypothesis is that reconstructing four different sets in a neighborhood with only one set of parameters is a difficult task that could distract the model from collecting the contextual information."}, {"heading": "5.4 Skip-thought Neighbor with One Target", "text": "Compared to the Skipped Thinking model, our Skipped Thinking model has fewer parameters with one goal and runs faster during training, as our model only needs to reconstruct the next sentence for a given sentence, while the Skipped Thinking model needs to reconstruct its surrounding two sentences. In the third section of Table 1, the results of our model are presented with only one goal. Surprisingly, it performs as well overall as the Skipped Thinking models as all previous models."}, {"heading": "5.5 A Note on Normalizing the Representation", "text": "An interesting observation was made when we examined the publicly available code for Kiros et al. (2015), i.e. during the training, the representation generated by the encoder is sent directly to the two decoders, but after the training, the output of the encoder is normalized to maintain the l2 standard as 1, so that the sentence representation is a normalized vector. We conducted experiments on the effect of normalization during the evaluation and evaluated both our neighboring model with skipped thinking and our implemented model with skipped thinking. Generally, the normalization step slightly impairs performance in the semantic relation task SICK while improving performance in all other classification tasks. Table 1 presents the results with the normalization step, and Table 2 presents the results without normalization on the SICK dataset."}, {"heading": "6 Qualitative Investigation", "text": "We examined the decoder in our trained neighboring model, with which no thoughts arise."}, {"heading": "6.1 Sentence Retrieval", "text": "In the previous section it was mentioned that normalization improves the performance of the model, so the distance measure we used in the record query experiment is the cosinal distance. Most of the retrieved records look semantically related and can be considered as a contextual extension of the query records. Several examples can be found in Table 3."}, {"heading": "6.2 Conditional Sentence Generation", "text": "Since the models are trained to minimize the reconstruction error in the entire training corpus, it makes sense to analyze the behavior of the decoder based on conditional sentence generation. First, we randomly pick up sentences from the training corpus and calculate a representation for each of them. Then, we greedily decipher the representations of sentences. Table 4 presents the generated sentences. Several interesting observations that are worth mentioning here.The decoder in our skipped neighboring model aims to minimize the distance of the generated sentence to two goals, which makes us doubt whether the decoder is able to generate at least grammatically correct English sentences. However, the result shows that the sentences generated are both grammatically correct and generally significant. We also find that the generated sentences have similar initial words and generally have negative expressions, such as i 't', i'm' not, i 'n', etc. After we have examined the training corpus, we find that they are infrequently associated with the majority observation caused by this data corpus."}, {"heading": "7 Conclusion", "text": "We hypothesized that neighborhood information is effective in learning sentence representation, and empirically tested our hypothesis. Our neighborhood models with skipped thinking were trained unsupervised and evaluated using 7 tasks. Results showed that our models work just as well as those with skipped thinking. Furthermore, our model performs better with only one goal than the model with skipped thinking. Future work could examine the neighborhood model with skipped thinking with only one goal more closely and see if the proposed model is able to generalize to even larger corpora or another body that does not come from books."}, {"heading": "Acknowledgments", "text": "We thank Jeffrey L. Elman, Benjamin K. Bergen, Seana Coulson and Marta Kutas for engaging conversations and Thomas Andy Keller, Thomas Donoghue, Larry Muhlstein and Reina Mizrahi for engaging chats. We also thank Adobe Research Lab for supporting the GPUs and NVIDIA for the DGX-1 study as well as the NSF IIS 1528214 and NSF SMA 1041755."}], "references": [{"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "ACL.", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet."], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["William B. Dolan", "Chris Quirk", "Chris Brockett."], "venue": "COLING.", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word 10(2-3):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "HLT-NAACL.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Juergen Schmidhuber."], "venue": "Neural Computation 9:1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "KDD.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Siamese cbow: Optimizing word embeddings for sentence representations", "author": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke."], "venue": "ACL.", "citeRegEx": "Kenter et al\\.,? 2016", "shortCiteRegEx": "Kenter et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Jamie Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "NIPS.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "ICML.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "COLING.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "LREC.", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "ACL.", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "ACL.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Towards generalizable sentence embeddings", "author": ["Eleni Triantafillou", "Jamie Ryan Kiros", "Raquel Urtasun", "Richard Zemel."], "venue": "RepL4NLP, ACL Workshop.", "citeRegEx": "Triantafillou et al\\.,? 2016", "shortCiteRegEx": "Triantafillou et al\\.", "year": 2016}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."], "venue": "Language Resources and Evaluation 39:165\u2013210.", "citeRegEx": "Wiebe et al\\.,? 2005", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "ICCV pages 19\u201327.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "We study the skip-thought model proposed by Kiros et al. (2015) with neighborhood information as weak supervision.", "startOffset": 44, "endOffset": 64}, {"referenceID": 7, "context": "(2015), which learns to explore the semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder.", "startOffset": 82, "endOffset": 96}, {"referenceID": 11, "context": "Previously, the skip-thought model was introduced by Kiros et al. (2015), which learns to explore the semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder.", "startOffset": 53, "endOffset": 73}, {"referenceID": 7, "context": "(2015), which learns to explore the semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder. The skip-thought model encodes the current sentence and then decodes the previous sentence and the next one, instead of itself. Two independent decoders were applied, since intuitively, the previous sentence and the next sentence should be drawn from 2 different conditional distributions, respectively. By posing a hypothesis that the adjacent sentences provide the same neighborhood information for learning sentence representation, we first drop one of the 2 decoders, and use only one decoder to reconstruct the surrounding 2 sentences at the same time. The empirical results show that our skip-thought neighbor model performs as well as the skip-thought model on 7 evaluation tasks. Then, inspired by Hill et al. (2016), as they tested the effect of incorporating an autoencoder branch in their proposed FastSent model, we also conduct experiments to explore reconstructing the input sentence itself as well in both our skip-thought neighbor model and the skip-thought model.", "startOffset": 83, "endOffset": 878}, {"referenceID": 16, "context": "Previously, Mikolov et al. (2013b) proposed a method for distributed representation learning for words by predicting surrounding words, and empirically showed that the additive composition of the learned word representations successfully captures contextual information of phrases and sentences.", "startOffset": 12, "endOffset": 35}, {"referenceID": 14, "context": "Similarly, Le and Mikolov (2014) proposed a method that learns a fixed-dimension vector for each sentence, by predicting the words within the given sentence.", "startOffset": 11, "endOffset": 33}, {"referenceID": 5, "context": "representation learning was proposed by Dai and Le (2015). The model combines an LSTM encoder, and an LSTM decoder to learn language representation in an unsupervised fashion on the supervised evaluation datasets, and then finetunes the LSTM encoder for supervised tasks on the same datasets.", "startOffset": 40, "endOffset": 58}, {"referenceID": 19, "context": "Tai et al. (2015) modified the plain LSTM network to a tree-structured LSTM network, which helps the model to address the long-term dependency problem.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Bowman et al. (2016) proposed a model that learns to parse the sentence at the same time as the RNN is processing the input sentence.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Inspired by the skip-gram model (Mikolov et al., 2013a), and the sentence-level distributional hypothesis (Harris, 1954), the skip-thought model encodes the current sentence as a fixed-dimension vector, and instead of predicting the input sentence itself, the decoders predict the previous sentence and the next sentence independently.", "startOffset": 32, "endOffset": 55}, {"referenceID": 7, "context": ", 2013a), and the sentence-level distributional hypothesis (Harris, 1954), the skip-thought model encodes the current sentence as a fixed-dimension vector, and instead of predicting the input sentence itself, the decoders predict the previous sentence and the next sentence independently.", "startOffset": 59, "endOffset": 73}, {"referenceID": 0, "context": "(2016), they finetuned the skip-thought models on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.", "startOffset": 104, "endOffset": 125}, {"referenceID": 10, "context": "Instead of learning to compose a sentence representation from the word representations, the skip-thought model Kiros et al. (2015) utilizes the structure and relationship of the adjacent sentences in the large unlabelled corpus.", "startOffset": 111, "endOffset": 131}, {"referenceID": 5, "context": ", 2013a), and the sentence-level distributional hypothesis (Harris, 1954), the skip-thought model encodes the current sentence as a fixed-dimension vector, and instead of predicting the input sentence itself, the decoders predict the previous sentence and the next sentence independently. The skip-thought model provides an alternative way for unsupervised sentence representation learning, and has shown great success. The learned sentence representation encoder outperforms previous unsupervised pretrained models on 8 evaluation tasks with no finetuning, and the results are comparable to supervised trained models. In Triantafillou et al. (2016), they finetuned the skip-thought models on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al.", "startOffset": 60, "endOffset": 650}, {"referenceID": 11, "context": "Later, Siamese CBOW (Kenter et al., 2016) aimed to learn the word representations to make the cosine similarity of adjacent sentences in the representation space larger than that of sentences which are not adjacent.", "startOffset": 20, "endOffset": 41}, {"referenceID": 8, "context": "In Hill et al. (2016), the proposed FastSent model takes summation of the word representations to compose a sentence representation, and predicts the words in both the previous sentence and the next sentence.", "startOffset": 3, "endOffset": 22}, {"referenceID": 13, "context": "We first briefly introduce the skipthought model (Kiros et al., 2015), and then discuss how to explicitly modify the decoders in the skip-thought model to get the skip-thought neighbor model.", "startOffset": 49, "endOffset": 69}, {"referenceID": 2, "context": "In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recurrent unit used in Kiros et al.", "startOffset": 118, "endOffset": 136}, {"referenceID": 9, "context": "(2014) shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 105, "endOffset": 139}, {"referenceID": 2, "context": "In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recurrent unit used in Kiros et al. (2015). Since the comparison among different recurrent units is not our main focus, we decided to use GRU, which is a fast and stable recurrent unit.", "startOffset": 119, "endOffset": 199}, {"referenceID": 2, "context": "In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recurrent unit used in Kiros et al. (2015). Since the comparison among different recurrent units is not our main focus, we decided to use GRU, which is a fast and stable recurrent unit. In addition, Chung et al. (2014) shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 119, "endOffset": 375}, {"referenceID": 8, "context": "Previously Hill et al. (2016) tested adding an autoencoder path into their FastSent model.", "startOffset": 11, "endOffset": 30}, {"referenceID": 24, "context": "The large corpus that we used for unsupervised training is the BookCorpus dataset (Zhu et al., 2015), which contains 74 million sentences from 7000 books in total.", "startOffset": 82, "endOffset": 100}, {"referenceID": 4, "context": "All of our experiments were conducted in Torch7 (Collobert et al., 2011).", "startOffset": 48, "endOffset": 72}, {"referenceID": 4, "context": "All of our experiments were conducted in Torch7 (Collobert et al., 2011). To make the comparison fair, we reimplemented the skip-thought model under the same settings, according to Kiros et al. (2015), and the publicly available theano code1.", "startOffset": 49, "endOffset": 201}, {"referenceID": 12, "context": "We use the ADAM (Kingma and Ba, 2014) algorithm for optimization.", "startOffset": 16, "endOffset": 37}, {"referenceID": 12, "context": "We use the ADAM (Kingma and Ba, 2014) algorithm for optimization. Instead of applying the gradient clipping according to the norm of the gradient, which was used in Kiros et al. (2015), we directly cut off the gradient to make it within [\u22121, 1] for stable training.", "startOffset": 17, "endOffset": 185}, {"referenceID": 16, "context": "For semantic relatedness, we use the SICK dataset (Marelli et al., 2014), and we adopt the feature engineering idea proposed by Tai et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 16, "context": "For semantic relatedness, we use the SICK dataset (Marelli et al., 2014), and we adopt the feature engineering idea proposed by Tai et al. (2015). For a given sentence pair, the encoder computes a pair of representations, denoted as u and v, and the concatenation of the component-wise product u \u00b7v and the absolute difference |u \u2212 v| is regarded as the feature for the given sentence pair.", "startOffset": 51, "endOffset": 146}, {"referenceID": 6, "context": "The dataset we use for the paraphrase detection is the Microsoft Paraphrase Detection Corpus (Dolan et al., 2004).", "startOffset": 93, "endOffset": 113}, {"referenceID": 6, "context": "The dataset we use for the paraphrase detection is the Microsoft Paraphrase Detection Corpus (Dolan et al., 2004). We follow the same feature engineering idea from Tai et al. (2015) to compute a single feature for each sentence pair.", "startOffset": 94, "endOffset": 182}, {"referenceID": 15, "context": "The 5 classification tasks are question-type classification (TREC) (Li and Roth, 2002), movie review sentiment (MR) (Pang and Lee, 2005), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification", "startOffset": 67, "endOffset": 86}, {"referenceID": 20, "context": "The 5 classification tasks are question-type classification (TREC) (Li and Roth, 2002), movie review sentiment (MR) (Pang and Lee, 2005), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification", "startOffset": 116, "endOffset": 136}, {"referenceID": 10, "context": "The 5 classification tasks are question-type classification (TREC) (Li and Roth, 2002), movie review sentiment (MR) (Pang and Lee, 2005), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification", "startOffset": 168, "endOffset": 186}, {"referenceID": 19, "context": "torch (SUBJ) (Pang and Lee, 2004), and opinion polarity (MPQA) (Wiebe et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 23, "context": "torch (SUBJ) (Pang and Lee, 2004), and opinion polarity (MPQA) (Wiebe et al., 2005).", "startOffset": 63, "endOffset": 83}, {"referenceID": 13, "context": "In order to deal with more words besides the words used for training, the same word expansion method, which was introduced by Kiros et al. (2015), is applied after training on the BookCorpus dataset.", "startOffset": 126, "endOffset": 146}, {"referenceID": 13, "context": "An interesting observation was found when we were investigating the publicly available code for Kiros et al. (2015), which is, during training, the representation produced from the encoder will be directly sent to the two decoders, however, after training, the output from the encoder will be normalized to keep the l2-norm as 1, so the sentence representation is a normalized vector.", "startOffset": 96, "endOffset": 116}], "year": 2017, "abstractText": "We study the skip-thought model proposed by Kiros et al. (2015) with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our model didn\u2019t aid our model to perform better, while it hurts the performance of the skip-thought model.", "creator": "LaTeX with hyperref package"}}}