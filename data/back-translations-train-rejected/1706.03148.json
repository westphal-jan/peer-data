{"id": "1706.03148", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Trimming and Improving Skip-thought Vectors", "abstract": "The skip-thought model has been proven to be effective at learning sentence representations and capturing sentence semantics. In this paper, we propose a suite of techniques to trim and improve it. First, we validate a hypothesis that, given a current sentence, inferring the previous and inferring the next sentence provide similar supervision power, therefore only one decoder for predicting the next sentence is preserved in our trimmed skip-thought model. Second, we present a connection layer between encoder and decoder to help the model to generalize better on semantic relatedness tasks. Third, we found that a good word embedding initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus with contiguous sentences, and then evaluate the trained model on 7 supervised tasks, which includes semantic relatedness, paraphrase detection, and text classification benchmarks. We empirically show that, our proposed model is a faster, lighter-weight and equally powerful alternative to the original skip-thought model.", "histories": [["v1", "Fri, 9 Jun 2017 22:44:31 GMT  (511kb,D)", "http://arxiv.org/abs/1706.03148v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shuai tang", "hailin jin", "chen fang", "zhaowen wang", "virginia r de sa"], "accepted": false, "id": "1706.03148"}, "pdf": {"name": "1706.03148.pdf", "metadata": {"source": "CRF", "title": "Trimming and Improving Skip-thought Vectors", "authors": ["Shuai Tang", "Hailin Jin Chen", "Fang Zhaowen Wang"], "emails": ["shuaitang93@ucsd.edu,", "desa@ucsd.edu,", "zhawang}@adobe.com"], "sections": [{"heading": null, "text": "In this paper, we propose a number of techniques to trim and improve the next set. First, we confirm a hypothesis that in a current set, we present a connecting layer between encoder and decoder to help the model better generalize semantic kinship tasks. Third, we found that a good word to embed initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus of contiguous sentences, and then evaluate the trained model against 7 supervised tasks, which include semantic kinship, paraphrase recognition, and text classification benchmarks. Empirically, we show that our proposed model is a faster, lighter, and equally powerful model for skipping."}, {"heading": "1 Introduction", "text": "The fact is that we are in a time in which we are able, in a time in which we are not yet able to change the world, in a time in which we are able to change the world, in a time in which we are able to change the world, \"he said."}, {"heading": "2 Approach", "text": "In this section, we present the trimmed model of the skipping thought. It contains some simple but effective modifications of the previously proposed skipping thought model [6]. We will first briefly introduce the model of the skipping thought and then discuss how it can be explicitly modified by incorporating our three proposed techniques."}, {"heading": "2.1 Skip-thought Model", "text": "In a model where a set of tuples (si \u2212 1, si, si + 1) is given, the encoder calculates a fixed-dimensional vector as a representation zi for the sentence si, which learns a distribution p (zi | si; \u03b8e), referring to the set of parameters in the encoder. Depending on the representation zi, two separate decoders are then used to reconstruct the preceding sentence si \u2212 1 and the next sentence si + 1, respectively. We refer to them as the previous decoder p (si \u2212 1 | zi; \u03b8p) and the next decoder p (si + 1 | zi; \u03b8n), in which the previous sentence si \u2212 1 is reconstructed. An illustration is shown in Figure 1a. Since the two conditional distributions learned from the decoders are parameterized independently, they implicitly use the sentence order information within the sentence."}, {"heading": "2.2 Trimming Skip-thought Model by Neighborhood Hypothesis", "text": "The Neighbourhood Hypothesis was first introduced in [7] and pointed out that in view of the current proposition, the inference from the previous proposition and the inference from the next proposition provide the same surveillance power. In order to include the Neighbourhood Hypothesis in the model, we must modify the model of skip. Given si, we assume that the inference from si \u2212 1 is the same as the inference from si + 1. If we define si \u2212 1, si + 1 are two neighbors of si, then the process of skipping can be called sj \u0445 p (s | zi; \u03b8d) for each j in the neighborhood of si. The conditional distribution learned from the decoder is parameterized by \u03b8d. Figure 1b illustrates the Neighbourhood Hypothesis. Moreover, in our trimmed skip model, for a given proposition, the decoder of the decoder of the decoder that are the decoders in its propositions {+ 1, si, si \u2212 two objectives."}, {"heading": "2.3 Average+Max Connection", "text": "In skipped thought models [6], only the hidden state in the last step, which emerges from the RNN encoder, is considered a vector representation for a given sentence and serves as conditional information for the decoder to reconstruct the neighboring 2 sentences. Recently, [9] a large corpus, the SNLI, has been assembled for the detection of textual discharges. In the face of a sentence pair that includes premise and hypothesis, the task is to classify the relationship of the sentence pair, discharge, contradiction, or neutrality. [11] It is proposed to summarize the hidden states from all time steps calculated from an RNN encoder as sentence representation, while [8] it is proposed to bundle the results from an average pooling function and a maximum pooling function, both running over all time steps, to serve as sentence representation."}, {"heading": "2.4 Word Embeddings Initialization", "text": "Distributed word embedding plays a role in the deep learning models dealing with NLP-related tasks. The proposed training methods, such as continuous word pouches and skip programs [12], always serve as strong basic models for the monitored tasks in NLP. Preschooled word embedding, including word2vec [13] and GloVe [14], also increase model performance in the monitored tasks. We assume that the initialization of deep models with preschooled word embedding is useful in order to transfer knowledge from unsupervised learning to the monitored tasks. We opt for the word embedding matrix in the model with word2vec [13], GloVe [14] and the original method of [6], which uses samples from a uniform distribution."}, {"heading": "3 Experiment Settings", "text": "The mentioned hsci-eaJnlhsrdcnlhSrdc\u00fce nvo edn nlrcnheeaeaeVnlrdhsrteeu-eaJnlhsrteeSrlhsci-eaJnlhsrdcnlhsrteeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaermnllrrrrrrrrrrrrrrrrrrrrrrrrrgn-eaiuiuiuiuiuiuiuiuiueaeaeaeaeaeaeaeaeaeaeaos \"eaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaos\" eaeaeaeaeaeaeaeaeaeaeaeaeaeaeaos \"nrrrrrrrrrrrrrmnlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "4 Quantitative Evaluation", "text": "We compared our proposed trimmed skip thought model with the trimmed skip thought model for 7 evaluation tasks, which include semantic relationality (SICK) [18], paraphrase recognition (MSRP) [19], question-type classification (TREC) [20] and 4 benchmark sensations and subjective datasets, including film evaluation mood (MR) [21], customer ratings (CR) [22], subjectivity / objectivity classification (SUBJ) [23] and opinion polarity (MPQA) [24]. After unattended training on the BookCorpus dataset, we set the parameters in the encoder and apply it as a sentence representation extractor to the 7 tasks. For SICK and MSRP tasks, we take over the feature engineering idea for the model proposed by [25]."}, {"heading": "4.1 Trimmed skip-thought vs. Skip-thought", "text": "In this comparison, all models use the simple link, which means that the sentence representation at the last step is the hidden state. Table 2 presents the results of 3 trimmed models for skipping thinking and 3 of our implemented models for skipping. The table shows that our trimmed models for skipping are slightly better than the models for skipping overall, but not significant, but the performance on the TREC dataset is worse than the models for skipping thinking. The general performance comparison between our trimmed model for skipping and the model for skipping proves that the neighborhood hypothesis is reasonable, which means that the neighborhood information is effective for distributed sentence representation. In addition, our trimmed model for skipping includes faster training, as our model only needs to reconstruct its next sentence, while the model for skipping thinking needs to reconstruct its two surrounding sentences."}, {"heading": "4.2 Plain Connection vs. Average+Max Connection", "text": "The results are presented in Table 2. As expected, our proposed trimmed model benefits from the Average + Max Connection in assessing the relationship of a sentence pair. Performance in the SICK task improves. However, performance on 2 classification benchmarks, MR and CR, slightly decreases compared to our single-coupled model. Overall performance in the evaluation tasks reaches the results shown in [6] with the exception of TREC, which shows that our model with Average + Max Connection could be a quick, easier alternative to the skipped-thought model. See Table 3 for detailed parameter counting."}, {"heading": "4.3 Word Embedding Initialization", "text": "The second section in Table 2 presents the comparison between 3 different embeddings of GloVe. After the training, we learn a linear mapping from the word2vec 3 embeddings to the RNN word embeddings, regardless of which initialization was used in the model as in [6]. 3https: / / code.google.com / archive / p / word2vec / In general, models with pre-formed word embeddings perform better as initialization in the evaluation tasks than those with random embeddings, which shows that a good initialization for word embeddings helps the model to better transfer knowledge from unattended training. It is worth noting here that we also trained a linear projection of GloVe word embeddings on the RNN word embeddings in the models initialized with GloVe 4."}, {"heading": "4.4 Doubling Encoder\u2019s Dimension", "text": "In our experiments above, the encoder is either a bidirectional GRU with 300 dimensions each or a unidirectional GRU with 600 dimensions. With the average + max connection, the dimension of a sentence representation is 1200. We assumed that a model with a larger encoder size could also improve performance in evaluation tasks. Therefore, we double the dimension of the encoder, which is now either a bi-GRU with 600 dimensions each or a uni-GRU with 1200 dimensions. As a result, the sentence representation is a 2400-dimensional vector corresponding to the dimensionality of the representation reported in [6]. Table 2 presents the results. Our trimmed models with double encoder perform better than the models with skipped thinking in [6] via SICK and MSRP and have comparable results with 4 classification benchmarks. The performance is worse than the original model with skipped thinking on TREC."}, {"heading": "5 Qualitative Investigation", "text": "We carry out qualitative studies on our trimmed skip thinking model. The model examined here contains bi-GRU as encoder with 300 dimensions each, single-layer GRU as decoder with 600 dimensions and average + max connection."}, {"heading": "5.1 Sentence Retrieval", "text": "For this task, 1000 records were selected as a query record and 1 million records as a database. All records are taken from the training corpus. Cosine width is measured at 4https: / / nlp.stanford.edu / projects / GloVe / to measure the distance in the display space. See Table 4 for some examples. Most retrieved records look semantically related and can be considered as a contextual extension of the query records."}, {"heading": "5.2 Conditional Sentence Generation", "text": "The decoder in our tailored model of skipping thought has been trained in the way of speech modeling, so it makes sense to analyze the sentences generated by the decoder after training. As the process of sentence generation depends on the representations generated by the encoder, we first randomly pick up sentences from the training corpus and forward the model to obtain the output of the decoder for each of them. Greedy decoding is used in sentence generation. Table 5 presents the sentences generated. We observe that the sentences generated tend to start with i'm and I do not. It could cause the corpus distortion, as there are a large number of sentences that do not begin with i'm, I do not, etc. Furthermore, the decoder is trained to reconstruct the next sentence in the model, which could be imagined as a contextual extension of the input sentence, while the sentences generated rarely relate to the associated input sentences, which is unrelated to the case of the models."}, {"heading": "6 Related Work", "text": "In fact, it is so that it will be able to fix and correct the mentioned bugs."}, {"heading": "7 Conclusion", "text": "We proposed three techniques to trim and also improve the skip-thinking model [6], including disconnecting a decoder, incorporating nonlinear nonparametric connections, and initializing with pre-formed word vectors. We demonstrated empirically the effectiveness of our proposed techniques. In addition, our proposed trimmed skip-thinking model contains far fewer parameters, which is much faster than the skip-thinking model. In addition, our model could be facilitated and benefit from a larger model size by various connecting methods between the encoder and the decoder. Future research could use proposed techniques for unattended imaging learning and generalize to more complex model types."}, {"heading": "Acknowledgments", "text": "We thank Jeffrey L. Elman, Benjamin K. Bergen and Seana Coulson for the insightful discussion and Thomas Donoghue and Reina Mizrahi for the stimulating chat. We also thank Adobe Research Lab for supporting the GPUs and NVIDIA for the DGX-1 study as well as NSF IIS 1528214 and NSF SMA 1041755."}], "references": [{"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, vol. 14, pp. 179\u2013211, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "NIPS, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["J.R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Rethinking skip-thought: A neighborhood based approach", "author": ["S. Tang", "H. Jin", "C. Fang", "Z. Wang", "V.R. de Sa"], "venue": "RepL4NLP, ACL Workshop, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Enhancing and combining sequential and tree lstm for natural language inference", "author": ["Q. Chen", "X. Zhu", "Z. Ling", "S. Wei", "H. Jiang"], "venue": "arXiv preprint arXiv:1609.06038, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A large annotated corpus for learning natural language inference", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": "EMNLP, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014. 9", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A decomposable attention model for natural language inference", "author": ["A.P. Parikh", "O. Tackstrom", "D. Das", "J. Uszkoreit"], "venue": "EMNLP, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R.S. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "ICCV, pp. 19\u201327, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["M. Marelli", "S. Menini", "M. Baroni", "L. Bentivogli", "R. Bernardi", "R. Zamparelli"], "venue": "LREC, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["W.B. Dolan", "C. Quirk", "C. Brockett"], "venue": "COLING, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "COLING, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "ACL, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "KDD, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "ACL, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["J. Wiebe", "T. Wilson", "C. Cardie"], "venue": "Language Resources and Evaluation, vol. 39, pp. 165\u2013210, 2005.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "ACL, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards generalizable sentence embeddings", "author": ["E. Triantafillou", "J.R. Kiros", "R. Urtasun", "R. Zemel"], "venue": "RepL4NLP, ACL Workshop, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "HLT-NAACL, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, vol. 10, no. 2-3, pp. 146\u2013162, 1954.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1954}, {"title": "An exploration of discourse-based sentence spaces for compositional distributional semantics", "author": ["T. Polajnar", "L. Rimell", "S. Clark"], "venue": "Workshop on LSDSem, p. 1, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Siamese cbow: Optimizing word embeddings for sentence representations", "author": ["T. Kenter", "A. Borisov", "M. de Rijke"], "venue": "ACL, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Discourse-based objectives for fast unsupervised sentence representation learning", "author": ["Y. Jernite", "S.R. Bowman", "D. Sontag"], "venue": "arXiv preprint arXiv:1705.00557, 2017. 10", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "With the development of deep learning techniques, recurrent neural networks (RNNs) [1, 2, 3] have shown encouraging results on natural language processing (NLP) tasks, and become the dominant methods in processing sequential data.", "startOffset": 83, "endOffset": 92}, {"referenceID": 1, "context": "With the development of deep learning techniques, recurrent neural networks (RNNs) [1, 2, 3] have shown encouraging results on natural language processing (NLP) tasks, and become the dominant methods in processing sequential data.", "startOffset": 83, "endOffset": 92}, {"referenceID": 2, "context": "With the development of deep learning techniques, recurrent neural networks (RNNs) [1, 2, 3] have shown encouraging results on natural language processing (NLP) tasks, and become the dominant methods in processing sequential data.", "startOffset": 83, "endOffset": 92}, {"referenceID": 3, "context": "[4] proposed LSTM-based sequence to sequence learning (seq2seq) model for machine translation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Later [5] applied the seq2seq model for unsupervised representation learning on language, and then finetuned the model for supervised tasks.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "[6] proposed the skip-thought model, which is an encoder-decoder model for unsupervised distributed sentence representation learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "First, given the neighborhood hypothesis first proposed in [7], we directly abandon one of the decoders in the skip-thought model, and leave only one encoder and one decoder for learning from inferring the next sentence given the current one.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "Second, we replace the plain connection used between the encoder and decoder with the Average+Max Connection, which is a non-linear non-parametric feature engineering method proposed by [8] for Stanford Natural Language Inference (SNLI) [9] challenge, and enhances the model to capture more complex interactions among the hidden states.", "startOffset": 186, "endOffset": 189}, {"referenceID": 8, "context": "Second, we replace the plain connection used between the encoder and decoder with the Average+Max Connection, which is a non-linear non-parametric feature engineering method proposed by [8] for Stanford Natural Language Inference (SNLI) [9] challenge, and enhances the model to capture more complex interactions among the hidden states.", "startOffset": 237, "endOffset": 240}, {"referenceID": 5, "context": "Figure 1: The comparison of the previously proposed skip-thought model [6], and our proposed trimmed skip-thought model.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "It includes a few simple yet effective modifications from the previously proposed skip-thought model [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "Encoder: The encoder is a recurrent neural network, which is composed of bi-directional gated recurrent unit (GRU) [10], or uni-directional GRU.", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "Table 1: Here presents the Gated Recurrent Unit (GRU) [3] and Conditional GRU, omitting the subscript i.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "The neighborhood hypothesis was first introduced in [7], and it pointed out that given the current sentence, inferring the previous sentence and inferring the next sentence both provide same supervision power.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "In the neighborhood hypothesis [7], the model doesn\u2019t distinguish between the sentences in a neighborhood.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "In skip-thought models [6], only the hidden state at the last time step produced from the RNN encoder is regarded as the vector representation for a given sentence, and serves as the conditional information for the decoder to reconstruct the adjacent 2 sentences.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "Recently, [9] collected a large corpus, which is SNLI, for textual entailment recognition.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "[11] proposed to summarize the hidden states from all time steps computed from a RNN encoder as a sentence representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "While [8] proposed to concatenate the outputs from an average pooling function and a max pooling function, which both run over all time steps, to serve as a sentence representation, and showed a performance boost on the SNLI dataset.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Here, since our goal is to simplify and accelerate the skip-thought model, and also get comparable results on the evaluation tasks, we consider comparing the 2 different composition functions, which are the original one used in the skip-thought model [6], denoted as Plain Connection, and the function proposed by [8], denoted as Average+Max Connection.", "startOffset": 251, "endOffset": 254}, {"referenceID": 7, "context": "Here, since our goal is to simplify and accelerate the skip-thought model, and also get comparable results on the evaluation tasks, we consider comparing the 2 different composition functions, which are the original one used in the skip-thought model [6], denoted as Plain Connection, and the function proposed by [8], denoted as Average+Max Connection.", "startOffset": 314, "endOffset": 317}, {"referenceID": 11, "context": "The proposed training methods, such as continuous bag-of-words and skip-gram [12], always serves as strong baseline models for the supervised tasks in NLP.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "The pretrained word embeddings, including word2vec [13] and GloVe [14], also boosts the model performance on the supervised tasks.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "The pretrained word embeddings, including word2vec [13] and GloVe [14], also boosts the model performance on the supervised tasks.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "We choose to initialize the word embedding matrix in the model with word2vec [13], GloVe [14], and the original method of [6] that uses random samples from a uniform distribution, respectively.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "We choose to initialize the word embedding matrix in the model with word2vec [13], GloVe [14], and the original method of [6] that uses random samples from a uniform distribution, respectively.", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "We choose to initialize the word embedding matrix in the model with word2vec [13], GloVe [14], and the original method of [6] that uses random samples from a uniform distribution, respectively.", "startOffset": 122, "endOffset": 125}, {"referenceID": 14, "context": "The large corpus that we used for unsupervised training is the BookCorpus dataset [15], which contains 74 million sentences from 7000 books in total.", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "All of our experiments were conducted in Torch7 [16].", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "To make the comparison fair, we follow the encoder design by [6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "In addition, [3] shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) [2].", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "In addition, [3] shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "We also reimplemented the skip-thought model under the same settings, according to [6], and the publicly available theano code 1.", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "The experiments with bi-directional encoder and unidirectional encoder were both conducted in [6], and we follow the design of these experiments.", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "In [6], for bi-skip model, the encoder contains a bi-directional GRU with 1200 dimension of each, for uni-skip model, the encoder contains a uni-directional GRU with 2400 dimension, and the decoder is a one-layer with 2400 dimension.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "For stable training, we use ADAM [17] algorithm for optimization.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "In order to generalize the model trained with relatively small, fixed vocabulary to a large amount of English words, [6] proposed a word expansion method that learns a linear projection from the pretrained word embeddings word2vec [13] to the learned RNN word embeddings.", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "In order to generalize the model trained with relatively small, fixed vocabulary to a large amount of English words, [6] proposed a word expansion method that learns a linear projection from the pretrained word embeddings word2vec [13] to the learned RNN word embeddings.", "startOffset": 231, "endOffset": 235}, {"referenceID": 5, "context": "Results reported by [6] bi-T-skip word2vec 0.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "4 bi-skip [6] random 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "4 uni-skip [6] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "4 C-skip [6] 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 179, "endOffset": 183}, {"referenceID": 19, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 220, "endOffset": 224}, {"referenceID": 20, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 320, "endOffset": 324}, {"referenceID": 21, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 356, "endOffset": 360}, {"referenceID": 22, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 409, "endOffset": 413}, {"referenceID": 23, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 443, "endOffset": 447}, {"referenceID": 24, "context": "For SICK and MSRP tasks, we adopt the feature engineering idea proposed by [25].", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Unlike the results in [7], these models presented in this paper contain word embeddings with lower dimension, which is half of that in [7], and GRUs with much smaller size.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "Unlike the results in [7], these models presented in this paper contain word embeddings with lower dimension, which is half of that in [7], and GRUs with much smaller size.", "startOffset": 135, "endOffset": 138}, {"referenceID": 12, "context": "Also, our models presented here use word2vec [13] as word embeddings initialization, which is different from random initialization applied in [7].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Also, our models presented here use word2vec [13] as word embeddings initialization, which is different from random initialization applied in [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "The results of our implemented skip-thought model differ from those presented in [6], (also presented here in the last section in Table 2), since our implementation contains much fewer parameters than the original skip-thought model, and it has word2vec [13] initialization.", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "The results of our implemented skip-thought model differ from those presented in [6], (also presented here in the last section in Table 2), since our implementation contains much fewer parameters than the original skip-thought model, and it has word2vec [13] initialization.", "startOffset": 254, "endOffset": 258}, {"referenceID": 5, "context": "The overall performance on the evaluation tasks reaches the results reported in [6] except TREC, which shows that our model with Average+Max Connection could be a fast, lighter-weight alternative to the skip-thought model.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "uni-skip [6] 69.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "4M 48M bi-skip [6] 51.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "After training, we learn a linear mapping from the word2vec 3 embedding space to RNN word embedding space, regardless of what initialization was applied in the model as in [6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "As a result, the sentence representation is a 2400-dimension vector, which matches the dimensionality of the representation reported in [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Our trimmed skip-thought models with doubled encoder performs better than the skip-thought models report in [6] on SICK and MSRP, and have comparable results on 4 classification benchmarks.", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "The cut down on the training time comes from the neighborhood hypothesis[7] and many fewer parameters in the model.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "Previously, [12] proposed the continuous bag-of-words (CBOW) model and the skip-gram model for distributed word representation learning.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "[13] improved the skip-gram model, and empirically showed that additive composition of the learned word representations successfully captures contextual information of phrases and sentences, which is a strong baseline model for NLP tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Similarly, [26] proposed a method to learn a fixed-dimension vector for each sentence by predicting the words within the given sentence.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "LSTM-based autoencoder model for language representation learning was proposed by [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "The skip-thought model was proposed by [6] for learning a generic, distributed sentence encoder, and its key idea was inspired by the skip-gram model [12].", "startOffset": 39, "endOffset": 42}, {"referenceID": 11, "context": "The skip-thought model was proposed by [6] for learning a generic, distributed sentence encoder, and its key idea was inspired by the skip-gram model [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 26, "context": "In [27], they finetuned the skip-thought models on the SNLI corpus [9], which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [27], they finetuned the skip-thought models on the SNLI corpus [9], which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.", "startOffset": 67, "endOffset": 70}, {"referenceID": 27, "context": "[28] pointed out that the skip-thought model made use of the sentence-level distributional hypothesis [29, 30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[28] pointed out that the skip-thought model made use of the sentence-level distributional hypothesis [29, 30].", "startOffset": 102, "endOffset": 110}, {"referenceID": 29, "context": "[28] pointed out that the skip-thought model made use of the sentence-level distributional hypothesis [29, 30].", "startOffset": 102, "endOffset": 110}, {"referenceID": 27, "context": "Following the same hypothesis, [28] proposed FastSent model.", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "Later, Siamese CBOW [31] aimed to learn the word representations to make the cosine similarity of adjacent sentences in the representation space larger than that of sentences which are not adjacent.", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "Instead of learning to reconstruct the sentences which are adjacent to the current sentence, [32] proposed a model that learns to categorize the manually defined relationships of two input sentences.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "We proposed 3 techniques for trimming and also improving the skip-thought model[6], which includes dropping off one decoder, incorporating non-linear non-parametric connection, and initializing with pretrained word vectors.", "startOffset": 79, "endOffset": 82}], "year": 2017, "abstractText": "The skip-thought model has been proven to be effective at learning sentence representations and capturing sentence semantics. In this paper, we propose a suite of techniques to trim and improve it. First, we validate a hypothesis that, given a current sentence, inferring the previous and inferring the next sentence provide similar supervision power, therefore only one decoder for predicting the next sentence is preserved in our trimmed skip-thought model. Second, we present a connection layer between encoder and decoder to help the model to generalize better on semantic relatedness tasks. Third, we found that a good word embedding initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus with contiguous sentences, and then evaluate the trained model on 7 supervised tasks, which includes semantic relatedness, paraphrase detection, and text classification benchmarks. We empirically show that, our proposed model is a faster, lighter-weight and equally powerful alternative to the original skip-thought model.", "creator": "LaTeX with hyperref package"}}}