{"id": "1706.03196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Online Learning for Neural Machine Translation Post-editing", "abstract": "Neural machine translation has meant a revolution of the field. Nevertheless, post-editing the outputs of the system is mandatory for tasks requiring high translation quality. Post-editing offers a unique opportunity for improving neural machine translation systems, using online learning techniques and treating the post-edited translations as new, fresh training data. We review classical learning methods and propose a new optimization algorithm. We thoroughly compare online learning algorithms in a post-editing scenario. Results show significant improvements in translation quality and effort reduction.", "histories": [["v1", "Sat, 10 Jun 2017 07:41:22 GMT  (192kb,D)", "http://arxiv.org/abs/1706.03196v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["\\'alvaro peris", "luis cebri\\'an", "francisco casacuberta"], "accepted": false, "id": "1706.03196"}, "pdf": {"name": "1706.03196.pdf", "metadata": {"source": "CRF", "title": "Online Learning for Neural Machine Translation Post-editing", "authors": ["\u00c1lvaro Peris", "Luis Cebri\u00e1n", "Francisco Casacuberta"], "emails": ["fcn}@prhlt.upv.es", "luicebch@inf.upv.es"], "sections": [{"heading": null, "text": "1 Online Learning for Neural Machine Translation Post-editingNeural machine translation has revolutionized the field, but post-editing the results of the system is still mandatory for tasks requiring high translation quality. Post-editing offers a unique opportunity to improve neural machine translation systems by using online learning techniques and treating post-edited translations as new, fresh training data. We review classic learning methods and propose a new optimization algorithm. We thoroughly compare online learning algorithms in a post-editing scenario, and the results show significant improvements in translation quality and effort reduction."}, {"heading": "1 Introduction", "text": "This year it is so far that it is only a matter of time before it will be so far, until it is so far, until it is so far."}, {"heading": "2 Related work", "text": "In the field of machine learning, the OL techniques 1https: / / github.com / lvapeab / nmt-keras / tree / interactive _ NMT are mainly used to align the weights of the log-linear model (Zens et al., 2002; Och and Ney, 2002) from classic phrase-based SMT systems (Koehn, 2010). Specifically, PA-based techniques, such as the marginal infusion relaxed algorithm (MIRA) (Crammer and Singer, 2001), are particularly well suited for handling a large number of features. Therefore, MIRA is commonly applied to models that deal with many sparse features (Watanabe et al., 2007; Chiang, 2012). Emulation of model functions - in addition to the weights of the log-linear model - is a less researched application of OL techniques. Most of these were developed under the Maat project, we based on the alabat system ()."}, {"heading": "3 Background: NMT", "text": "In this paper, we use an attentive NMT system similar to that described by Bahdanau et al. (2015), but using LSTM networks; the input is a sequence of words x in the source language; each word is projected linearly into continuous space using an embedding matrix; the sequence of word embedding feeds a bi-directional LSTM that analyzes the input sequence in both directions, from left to right and vice versa; this network computes a sequence of notes by concatenating the hidden states from the forward and backward layers; for each decoding, an attention mechanism weights each element from the sequence of annotations according to the previous decoding state and computes a common context vector; the decoder is another LSTM network that uses the context vector, the previously generated word and its previous hidden state to calculate a passive state (2014)."}, {"heading": "3.1 Training", "text": "In order to estimate the model parameters, i.e. the weight matrices, the training goal is to minimize a loss function L\u0443, usually the minus log probability over a bilingual parallel corpus T = {(xt, yt)} Tt = 1, consisting of T source target pairs (xt or yt), in the case that it is the length of the T target set. SGD techniques are the predominant method for optimizing neural networks. The goal is to minimize a predetermined loss function parameterized by the network parameters. SGD updates the parameters in the opposite direction of the gradient of L\u043e. The update quantity is controlled by a learning rate. Depending on when these updates occur, SGD techniques can be duplicated or applied in large batch (2011, SGD variant, online mode)."}, {"heading": "4 Online gradient descent", "text": "Online SGD updates the model parameters after each sample. \u2212 Therefore, for a single sample (xt, yt), the parameter updates are calculated as follows (\u03b2, yt): Most optimizers used in deep learning are variants of this vanilla SGD. Therefore, they can also be applied within the scope of the OL framework. In this work, we compare Adagrad, Adadelta and Adam, whose update rules are briefly commented below. Adagrad (Duchi et al., 2011) aims to perform major updates for rare parameters, whereby the update rule is defined as follows: \u0445 t = \u2212 quadratic Gt + quadratic Gt + quadratic Gt (xt, yt) quadratic and quadratic quadratic quarters and quadratic quarters and quadratic quarters and quadratic quarters and quadratic quarters"}, {"heading": "5 Passive-aggressive via subgradient techniques", "text": "We propose a new version of SGD, inspired by PA techniques such as MIRA (Crammer and Singer, 2001), which aims to perform the minimum modification of the model parameters in order to convince them with a correctness criterion. Our criterion is based directly on the target loss function (Eq. (1)). Nevertheless, we could use other loss functions such as BLEU, although this could be costly. We rejected this loss for efficiency reasons. Let's apply the hypothesis generated by the NMT (using the current parameters) of the source set xt. We consider this to be flawed if the model assigns a lower probability for the target yt than for ht."}, {"heading": "5.1 Projected subgradient", "text": "An extension of the PAS method is the projected subgradient method (PPAS), in which the optimization problem is reformulated (Boyd et al., 2003). We define Gt (xt, yt, ht) as max (0, (xt, yt, ht). Then, equation (5) can be rewritten as follows: + 0 = arg min-Gt (xt, yt, ht). With this expression, we can calculate the intermediate weight actualization: + 1 = static actualization, static actualization, static actualization, static actualization, static actualization, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static, static"}, {"heading": "6 Experiments and results", "text": "In this section we describe the experimental setup used to carry out the experiment. We define the evaluation parameters, describe the corpora and explain the configuration of the NMT systems."}, {"heading": "6.1 Experimental framework", "text": "We tested our OL methods in a follow-up task. Within this framework, we define three different scenarios: 1. We process samples from a different area than the one in which our system was trained. 2. We process the results of a system that is trained in the same area of our task, but has data available outside the scope. 3. We only have the data collected in this area, either for training or testing purposes. For each scenario, we adapt the system on our own, applying OL to the reworked sentences from the test set. The final goal is to reduce the postification effort required by coming samples. The main difference is that in the first case, we rely exclusively on the OL techniques to adapt the system to the translation domain; while in the later cases, the NMT systems are already adapted. We measure the extent to which OL can refine this adapted system. Note that we work under a pure training framework, i.e. each sentence is an OL sentence."}, {"heading": "6.2 Metrics and evaluation", "text": "To approximate the post-processing effort, we used the translation processing rate (TER) (Snover et al., 2006). This metric calculates the processing distance between hypotheses and reference sentences. The processing operations considered are addition, deletion, replacement and displacement. In addition, we used BLEU (Papineni et al., 2002) and Meteor (Lavie and Denkowski, 2009) to evaluate the translation quality of the systems. BLEU compares the ratio of n-gram structures that are divided between the system hypotheses and reference sentences, while Meteor calculates the accuracy and recall value F1 value between hypotheses and references, additionally taking into account exact, native synonyms and paraphrase correspondences. For all results shown in this work, we calculate 95% confidence intervals by bootstrap repetition (Koehnen, 2004), the sentence is rewritten as follows."}, {"heading": "6.3 Corpora", "text": "As out-of-domain data (scenarios 1 and 2) we use the well-known, publicly available Europarl corpus (Koehn, 2005), using the newstest2013 partition as a development set. As in-domain corpus (scenarios 2 and 3) we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al., 2009) and TED (Federico et al., 2011) corpus. Their domains are medicine, printer manuals and TED conversations. We use the standard partitions for all corpus. We used the English-French language pair for all experiments. Table 1 shows the most important numbers of the corpus. We tokenized the text using the script by Moses, keeping sentences truecase. For all tasks we use the common byte-pair encoding (BPE) for translation on a subword level, as by Sennet al. (2016)."}, {"heading": "6.4 NMT systems", "text": "The NMT2 systems and OL algorithms were implemented with the libraries Keras and Theano (Theano Development Team, 2016) \u2212 \u03b299. Therefore, as described in Section 3, the system consists of an encoder decoder LSTM network equipped with the attention mechanism described by Bahdanau et al. (2015). For practical reasons, we used single-layer LSTMs. The size of each LSTM is 512, as well as the word size and the attention mechanism layer. As regulatory methods, we applied the layer normalization (Ba et al., 2016) and Gaussian noise to the weights (Ali = 0.01) (Graves, 2011). We also stopped training early if the BLEU did not improve the hyperparameters of development."}, {"heading": "7 Results and discussion", "text": "In this section we show and analyze the experiments carried out. We applied OL to the NMT system with the post-processing samples from the test set, with the aim of reducing the post-processing effort of the following samples. As described in Section 6.3, we present three different scenarios: 1. We have a general corpus and no in-domain data, so in order to reduce the post-processing effort, we need to focus exclusively on the in-domain samples using OL. 2. We have the general corpus, but we also have enough in-domain data to train a system. In this case, we train first with the out-of-domain corpus and then with the in-domain corpus. OL can also refine the resulting system in order to obtain a more tailored system and thus reduce the effort for post-processing a system. 3. We only have in-domain corpus. This situation is more artificial than the previous system, but can be illustrated in what extent."}, {"heading": "7.1 Adapting without in-domain", "text": "In this scenario, we assume that we only have an out-of-domain corpus. The text to be translated comes from an unknown domain. Therefore, we have to adapt the system on-the-fly according to the posted sentences: as soon as a sentence is translated and corrected, we change our system to take this sample into account. As a basis, we take an offline system that remains static throughout the translation process. First, we study the development of the various learning algorithms."}, {"heading": "7.2 Adapting with an in-domain", "text": "In this case, we assume that we have a collection of in-domain data, in addition to our out-of-domain corpus. Therefore, we first have a general system on out-of-domain data and fine-tune it with the in-domain training. Next, we investigate whether OL can still improve the model, although it can still be observed in the early stages of the process. How we apply the OL techniques to the test, the BLEU shows differences between the OL algorithms. In this case, the behavior was more8stable for all algorithms. Although some deviations can be observed in the early stages of the process, the system learned faster and better than in previous cases."}, {"heading": "7.3 Refining an NMT system", "text": "Fig. 5 shows the evolution of BLEU values along the online learning process. Here, too, algorithms with adaptive learning rates are better than the rest of them. Nevertheless, such differences are smaller than with other tasks. The PAS algorithm is particularly effective in the first iterations of the process. 0 200 400 600 1000 0 5 10 sentences% B L E d iff er en Emea-Test SGD Adagrad Adadelta Adam PPASFigure 5: Evolution of a system that has been trained on Europarl and Emea with its own Emea test set. We show the BLEU differences (averaged to the ninth sentence) in relation to the offline system. All OL algorithms significantly perform an offline baseline, which is shown as a Fig system."}, {"heading": "8 Conclusions and future work", "text": "In this paper, we examined the implementation of OL strategies for NMT. We empirically demonstrated the ability of NMT to adapt to new domains through gradient descent. In addition, we discussed and compared a wide range of gradient descent algorithms. In addition, we proposed two new methods, inspired by a PA strategy and addressed using subgradient optimization methods. PA-based methods offer competitive performance. In almost every case, they performed better than Vanilla SGD. Nevertheless, adaptive SGD algorithms generally performed better than PA. We found two cases where the performance of the PA method outperformed adaptive SGD algorithms, although the differences were small. One of the main disadvantages with respect to NMT is the independence (up to a point) of the network objective function (probability of the target given the source)."}, {"heading": "Acknowledgments", "text": "The research that led to these results was funded by the Generalitat Valenciana within the framework of the grant PROMETEOII / 2014 / 030. We also acknowledge NVIDIA for donating a GPU used in this work.10"}], "references": [{"title": "CASMACAT: An open source workbench for advanced computer aided translation", "author": ["Trilles", "Chara Tsoukala."], "venue": "The Prague Bulletin of Mathematical Linguistics 100:101\u2013112.", "citeRegEx": "Trilles and Tsoukala.,? 2013", "shortCiteRegEx": "Trilles and Tsoukala.", "year": 2013}, {"title": "Productivity and quality in the post-editing of outputs from translation memories and machine translation", "author": ["Ana Guerberof Arenas."], "venue": "Localisation Focus 7(1):11\u201321.", "citeRegEx": "Arenas.,? 2008", "shortCiteRegEx": "Arenas.", "year": 2008}, {"title": "Layer normalization", "author": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey Hinton."], "venue": "arXiv:1607.06450.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Statistical approaches to computer-assisted translation", "author": ["Sergio Barrachina", "Oliver Bender", "Francisco Casacuberta", "Jorge Civera", "Elsa Cubel", "Shahram Khadivi", "Antonio Lagarda", "Hermann Ney", "Jes\u00fas Tom\u00e1s", "Enrique Vidal", "Juan-Miguel Vilar."], "venue": "Com-", "citeRegEx": "Barrachina et al\\.,? 2009", "shortCiteRegEx": "Barrachina et al\\.", "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE transactions on neural networks 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Subgradient methods", "author": ["Stephen Boyd", "Lin Xiao", "Almir Mutapcic."], "venue": "lecture notes of EE392o, Stanford University, Autumn Quarter.", "citeRegEx": "Boyd et al\\.,? 2003", "shortCiteRegEx": "Boyd et al\\.", "year": 2003}, {"title": "Massive exploration of neural machine translation architectures", "author": ["Denny Britz", "Anna Goldie", "Thang Luong", "Quoc Le."], "venue": "arXiv:1703.03906.", "citeRegEx": "Britz et al\\.,? 2017", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "Hope and fear for discriminative training of statistical translation models", "author": ["David Chiang."], "venue": "Journal of Machine Learning Research 13(Apr):1159\u20131187.", "citeRegEx": "Chiang.,? 2012", "shortCiteRegEx": "Chiang.", "year": 2012}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of the Workshop on Syntax, Semantic and Structure in Statistical Transla-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Online passive-aggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer."], "venue": "Journal of Machine Learning Research 7:551\u2013585.", "citeRegEx": "Crammer et al\\.,? 2006", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "Proceedings of the Annual Conference on Computational Learning Theory. pages 99\u2013115.", "citeRegEx": "Crammer and Singer.,? 2001", "shortCiteRegEx": "Crammer and Singer.", "year": 2001}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Overview of the IWSLT evaluation campaign", "author": ["Marcello Federico", "Luisa Bentivogli", "Michael Paul", "Sebastian St\u00fcker."], "venue": "Proceedings of the International Workshop on Spoken Language Translation. pages 11\u201327.", "citeRegEx": "Federico et al\\.,? 2011", "shortCiteRegEx": "Federico et al\\.", "year": 2011}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves."], "venue": "Advances in Neural Information Processing Systems. pages 2348\u20132356.", "citeRegEx": "Graves.,? 2011", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the Interna-", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Neural interactive translation prediction", "author": ["Rebecca Knowles", "Philipp Koehn."], "venue": "Proceedings of the Association for Machine Translation in the Americas. pages 107\u2013120.", "citeRegEx": "Knowles and Koehn.,? 2016", "shortCiteRegEx": "Knowles and Koehn.", "year": 2016}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Machine Translation Summit. pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "A process study of computeraided translation", "author": ["Philipp Koehn."], "venue": "Machine Translation 23(4):241\u2013 263.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "The METEOR metric for automatic evaluation of machine translation", "author": ["Alon Lavie", "Michael J Denkowski."], "venue": "Machine translation 23(2-3):105\u2013115.", "citeRegEx": "Lavie and Denkowski.,? 2009", "shortCiteRegEx": "Lavie and Denkowski.", "year": 2009}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Online adaptation strategies for statistical machine translation in postediting scenarios", "author": ["Pascual Mart\u0131\u0301nez-G\u00f3mez", "Germ\u00e1n Sanchis-Trilles", "Francisco Casacuberta"], "venue": "Pattern Recognition", "citeRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.", "year": 2012}, {"title": "Online learning approaches in computer assisted translation", "author": ["Prashant Mathur", "Mauro Cettolo", "Marcello Federico", "FBK-Fondazione Bruno Kessler."], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation. pages 301\u2013308.", "citeRegEx": "Mathur et al\\.,? 2013", "shortCiteRegEx": "Mathur et al\\.", "year": 2013}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 295\u2013302.", "citeRegEx": "Och and Ney.,? 2002", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "Online learning for statistical machine translation", "author": ["Daniel Ortiz-Mart\u0131\u0301nez"], "venue": "Computational Linguistics", "citeRegEx": "Ortiz.Mart\u0131\u0301nez.,? \\Q2016\\E", "shortCiteRegEx": "Ortiz.Mart\u0131\u0301nez.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1312.6026.", "citeRegEx": "Pascanu et al\\.,? 2014", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "arXiv:1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Interactive neural machine translation", "author": ["\u00c1lvaro Peris", "Miguel Domingo", "Francisco Casacuberta."], "venue": "Computer Speech & Language 45:201\u2013220.", "citeRegEx": "Peris et al\\.,? 2017", "shortCiteRegEx": "Peris et al\\.", "year": 2017}, {"title": "A productivity test of statistical machine translation post-editing in a typical localisation context", "author": ["Mirko Plitt", "Fran\u00e7ois Masselot."], "venue": "The Prague bulletin of mathematical linguistics 93:7\u201316.", "citeRegEx": "Plitt and Masselot.,? 2010", "shortCiteRegEx": "Plitt and Masselot.", "year": 2010}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro."], "venue": "The Annals of Mathematical Statistics pages 400\u2013407.", "citeRegEx": "Robbins and Monro.,? 1951", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Algorithms of nondifferentiable optimization: Development and application", "author": ["N.Z. Shor", "N.G. Zhurbenko", "A.P. Likhovid", "P.I. Stetsyuk."], "venue": "Cybernetics and Systems Analysis 39(4):537\u2013548.", "citeRegEx": "Shor et al\\.,? 2003", "shortCiteRegEx": "Shor et al\\.", "year": 2003}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "Proceedings of the Association for Machine Translation in the Americas. pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the Advances in Neural Information Processing Systems, volume 27, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv:1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "News from OPUS - A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent Advances in Natural Language Processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Online large-margin training for statistical machine translation", "author": ["Taro Watanabe", "Jun Suzuki", "Hajime Tsukada", "Hideki Isozaki."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Watanabe et al\\.,? 2007", "shortCiteRegEx": "Watanabe et al\\.", "year": 2007}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["O. Vinyals", "G. Corrado", "M. Hughes", "J. Dean."], "venue": "arXiv:1609.08144.", "citeRegEx": "Vinyals et al\\.,? 2016", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Phrase-based statistical machine translation", "author": ["Richard Zens", "Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the Annual German Conference on Advances in Artificial Intelligence. volume 2479, pages 18\u201332.", "citeRegEx": "Zens et al\\.,? 2002", "shortCiteRegEx": "Zens et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 1, "context": "In the machine translation (MT) industry, the reduction of the post-editing effort has a great interest, as it leads to larger productivity (Arenas, 2008; Plitt and Masselot, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 36, "context": "In the machine translation (MT) industry, the reduction of the post-editing effort has a great interest, as it leads to larger productivity (Arenas, 2008; Plitt and Masselot, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 31, "context": "The application of OL to the classical phrasebased SMT systems has been thoroughly studied (Ortiz-Mart\u0131\u0301nez, 2016).", "startOffset": 91, "endOffset": 114}, {"referenceID": 42, "context": "Nevertheless, over the last years, the new neural machine translation (NMT) technology has shaken the machine translation field (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 128, "endOffset": 192}, {"referenceID": 3, "context": "Nevertheless, over the last years, the new neural machine translation (NMT) technology has shaken the machine translation field (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 128, "endOffset": 192}, {"referenceID": 5, "context": "In order to cope with the vanishing gradient problem (Bengio et al., 1994), such RNNs feature gated units, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 16, "context": ", 1994), such RNNs feature gated units, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al.", "startOffset": 78, "endOffset": 112}, {"referenceID": 9, "context": ", 1994), such RNNs feature gated units, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al., 2014).", "startOffset": 144, "endOffset": 162}, {"referenceID": 3, "context": "The inclusion of attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015a) overcame the problem of processing long source sentences.", "startOffset": 38, "endOffset": 82}, {"referenceID": 25, "context": "The inclusion of attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015a) overcame the problem of processing long source sentences.", "startOffset": 38, "endOffset": 82}, {"referenceID": 17, "context": "The management of out-ofvocabulary words has also been explored (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 64, "endOffset": 104}, {"referenceID": 26, "context": "The management of out-ofvocabulary words has also been explored (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 64, "endOffset": 104}, {"referenceID": 17, "context": "Closely related to this, the NMT vocabulary size limitation has been tackled either developing strategies for taking into account large vocabularies (Jean et al., 2015) or shifting from word-level translations to character (Chung et al.", "startOffset": 149, "endOffset": 168}, {"referenceID": 10, "context": ", 2015) or shifting from word-level translations to character (Chung et al., 2016) or sub-word-level (Sennrich et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 38, "context": ", 2016) or sub-word-level (Sennrich et al., 2016) translations.", "startOffset": 26, "endOffset": 49}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al.", "startOffset": 8, "endOffset": 278}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al. (2014) and Sutskever et al.", "startOffset": 8, "endOffset": 365}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al. (2014) and Sutskever et al. (2014). From there to now, NMT has surpassed the classical phrase-based systems in many tasks and currently is one the most active research topics (e.", "startOffset": 8, "endOffset": 393}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al. (2014) and Sutskever et al. (2014). From there to now, NMT has surpassed the classical phrase-based systems in many tasks and currently is one the most active research topics (e.g. Erk and Smith (2016)).", "startOffset": 8, "endOffset": 560}, {"referenceID": 37, "context": "2 Such NMT systems are usually trained from parallel corpora by means of stochastic gradient descent (SGD) (Robbins and Monro, 1951).", "startOffset": 107, "endOffset": 132}, {"referenceID": 11, "context": "Moreover, we propose a new SGD variant for NMT, based on passive-aggressive (PA) techniques (Crammer et al., 2006).", "startOffset": 92, "endOffset": 114}, {"referenceID": 48, "context": "com/lvapeab/nmt-keras/ tree/interactive_NMT are mainly employed for tuning the weights of the log-linear model (Zens et al., 2002; Och and Ney, 2002) from classical phrase-based SMT systems (Koehn, 2010).", "startOffset": 111, "endOffset": 149}, {"referenceID": 30, "context": "com/lvapeab/nmt-keras/ tree/interactive_NMT are mainly employed for tuning the weights of the log-linear model (Zens et al., 2002; Och and Ney, 2002) from classical phrase-based SMT systems (Koehn, 2010).", "startOffset": 111, "endOffset": 149}, {"referenceID": 23, "context": ", 2002; Och and Ney, 2002) from classical phrase-based SMT systems (Koehn, 2010).", "startOffset": 67, "endOffset": 80}, {"referenceID": 12, "context": "More specifically, PA-based techniques, such as the margin infuse relaxed algorithm (MIRA) (Crammer and Singer, 2001) are especially well-suited when dealing with a large number of features.", "startOffset": 91, "endOffset": 117}, {"referenceID": 45, "context": "Therefore, MIRA is usually applied to models dealing with lots of sparse features (Watanabe et al., 2007; Chiang, 2012).", "startOffset": 82, "endOffset": 119}, {"referenceID": 8, "context": "Therefore, MIRA is usually applied to models dealing with lots of sparse features (Watanabe et al., 2007; Chiang, 2012).", "startOffset": 82, "endOffset": 119}, {"referenceID": 31, "context": "As colophon of it, Ortiz-Mart\u0131\u0301nez (2016) developed an incremental derivation of the EM algorithm.", "startOffset": 19, "endOffset": 42}, {"referenceID": 27, "context": "Mart\u0131\u0301nez-G\u00f3mez et al. (2012) and Mathur et al.", "startOffset": 0, "endOffset": 30}, {"referenceID": 27, "context": "Mart\u0131\u0301nez-G\u00f3mez et al. (2012) and Mathur et al. (2013) used OL for adapting a SMT under a CAT framework.", "startOffset": 0, "endOffset": 55}, {"referenceID": 3, "context": "In this work, we use an attentional NMT system similar to the one described by Bahdanau et al. (2015), but using LSTM networks.", "startOffset": 79, "endOffset": 102}, {"referenceID": 33, "context": "Finally, a deep output layer (Pascanu et al., 2014) is employed to compute a distribution over the target language vocabulary.", "startOffset": 29, "endOffset": 51}, {"referenceID": 42, "context": "3 method (Sutskever et al., 2014).", "startOffset": 9, "endOffset": 33}, {"referenceID": 13, "context": "Moreover, depending on how the gradient is computed and applied, a large variety of SGD variants have been proposed (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2014).", "startOffset": 116, "endOffset": 171}, {"referenceID": 47, "context": "Moreover, depending on how the gradient is computed and applied, a large variety of SGD variants have been proposed (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2014).", "startOffset": 116, "endOffset": 171}, {"referenceID": 19, "context": "Moreover, depending on how the gradient is computed and applied, a large variety of SGD variants have been proposed (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2014).", "startOffset": 116, "endOffset": 171}, {"referenceID": 13, "context": "Adagrad (Duchi et al., 2011) aims to perform larger updates for infrequent parameters, defining its update rule as:", "startOffset": 8, "endOffset": 28}, {"referenceID": 47, "context": "Adadelta (Zeiler, 2012) is a less aggressive variant of Adadelta, which avoids its constant learning rate decay.", "startOffset": 9, "endOffset": 23}, {"referenceID": 19, "context": "Finally, Adam (Kingma and Ba, 2014) computes decaying averages for the past gradients (vt) and the past squared gradients (mt).", "startOffset": 14, "endOffset": 35}, {"referenceID": 12, "context": "We propose a new version of SGD, inspired by PA techniques such as MIRA (Crammer and Singer, 2001).", "startOffset": 72, "endOffset": 98}, {"referenceID": 11, "context": "where C is a parameter that controls the aggressiveness of the algorithm (Crammer et al., 2006).", "startOffset": 73, "endOffset": 95}, {"referenceID": 40, "context": "For obtaining \u0398\u0302, we use a subgradient method (Shor et al., 2003).", "startOffset": 46, "endOffset": 65}, {"referenceID": 6, "context": "An extension of the PAS method is the projected subgradient method (PPAS), in which the optimization problem is reformulated (Boyd et al., 2003).", "startOffset": 125, "endOffset": 144}, {"referenceID": 41, "context": "For approximating the post-editing effort, we used the translation edit rate (TER) (Snover et al., 2006).", "startOffset": 83, "endOffset": 104}, {"referenceID": 32, "context": "Moreover, we used BLEU (Papineni et al., 2002) and Meteor (Lavie and Denkowski, 2009) for assessing the translation quality of the systems.", "startOffset": 23, "endOffset": 46}, {"referenceID": 24, "context": ", 2002) and Meteor (Lavie and Denkowski, 2009) for assessing the translation quality of the systems.", "startOffset": 19, "endOffset": 46}, {"referenceID": 21, "context": "For all results shown in this work, we compute 95% confidence intervals by means of bootstrap resampling (Koehn, 2004).", "startOffset": 105, "endOffset": 118}, {"referenceID": 22, "context": "As out-of-domain data (scenarios 1 and 2), we use the well-known, publicly available Europarl (Koehn, 2005) corpus, using the partition newstest2013 as development set.", "startOffset": 94, "endOffset": 107}, {"referenceID": 44, "context": "As in-domain corpora (scenarios 2 and 3), we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al.", "startOffset": 58, "endOffset": 75}, {"referenceID": 4, "context": "As in-domain corpora (scenarios 2 and 3), we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al., 2009) and TED (Federico et al.", "startOffset": 82, "endOffset": 107}, {"referenceID": 14, "context": ", 2009) and TED (Federico et al., 2011) corpora.", "startOffset": 16, "endOffset": 39}, {"referenceID": 4, "context": "As in-domain corpora (scenarios 2 and 3), we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al., 2009) and TED (Federico et al., 2011) corpora. The domains of them are medical, printer manuals and TED talks, respectively. We used the standard partitions for all corpora. We used the English\u2013French language pair for all experiments. Table 1 shows the main figures of the corpora. We tokenized the text using the script from Moses, keeping sentences truecase. For all tasks, we use the joint byte pair encoding (BPE) algorithm for translating at a sub-word level, as described by Sennrich et al. (2016). We learned 32,000 merge operations.", "startOffset": 83, "endOffset": 607}, {"referenceID": 2, "context": "As regularization methods, we applied layer normalization (Ba et al., 2016) and Gaussian noise to the weights (\u03c3 = 0.", "startOffset": 58, "endOffset": 75}, {"referenceID": 15, "context": "01) (Graves, 2011).", "startOffset": 4, "endOffset": 18}, {"referenceID": 47, "context": "The original NMT systems were trained using Adadelta (Zeiler, 2012), with the default parameters.", "startOffset": 53, "endOffset": 67}, {"referenceID": 34, "context": "The norm of the gradients was clipped to 1 in order to avoid the exploding gradient problem (Pascanu et al., 2012).", "startOffset": 92, "endOffset": 114}, {"referenceID": 5, "context": "For choosing the main hyperparameters of the system, we take advantage of the vast exploration made by Britz et al. (2017). Therefore, as described in Section 3, the system consists in an encoder-decoder LSTM network equipped with the attention mechanism described by Bahdanau et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 2, "context": "Therefore, as described in Section 3, the system consists in an encoder-decoder LSTM network equipped with the attention mechanism described by Bahdanau et al. (2015). For practical reasons, we used single-layered LSTMs.", "startOffset": 144, "endOffset": 167}, {"referenceID": 29, "context": "Minimum risk training aims to overcome this gap and it has successfully been applied to classical SMT (Och, 2003; Chiang, 2012) and recently to NMT (Shen et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 8, "context": "Minimum risk training aims to overcome this gap and it has successfully been applied to classical SMT (Och, 2003; Chiang, 2012) and recently to NMT (Shen et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 39, "context": "Minimum risk training aims to overcome this gap and it has successfully been applied to classical SMT (Och, 2003; Chiang, 2012) and recently to NMT (Shen et al., 2016).", "startOffset": 148, "endOffset": 167}, {"referenceID": 27, "context": "As future work, we intend to directly optimize the evaluation metric, integrating it into the online learning framework (Mart\u0131\u0301nez-G\u00f3mez et al., 2012).", "startOffset": 120, "endOffset": 150}, {"referenceID": 20, "context": "The development of interactive NMT systems has been recently addressed (Knowles and Koehn, 2016; Peris et al., 2017).", "startOffset": 71, "endOffset": 116}, {"referenceID": 35, "context": "The development of interactive NMT systems has been recently addressed (Knowles and Koehn, 2016; Peris et al., 2017).", "startOffset": 71, "endOffset": 116}], "year": 2017, "abstractText": "Neural machine translation has meant a revolution of the field. Nevertheless, postediting the outputs of the system is mandatory for tasks requiring high translation quality. Post-editing offers a unique opportunity for improving neural machine translation systems, using online learning techniques and treating the post-edited translations as new, fresh training data. We review classical learning methods and propose a new optimization algorithm. We thoroughly compare online learning algorithms in a post-editing scenario. Results show significant improvements in translation quality and effort reduction.", "creator": "LaTeX with hyperref package"}}}