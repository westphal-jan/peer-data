{"id": "1706.03199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Toward Optimal Run Racing: Application to Deep Learning Calibration", "abstract": "This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem - selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art with no extra hyper-parameter.", "histories": [["v1", "Sat, 10 Jun 2017 07:55:38 GMT  (974kb,D)", "https://arxiv.org/abs/1706.03199v1", null], ["v2", "Tue, 20 Jun 2017 11:38:25 GMT  (974kb,D)", "http://arxiv.org/abs/1706.03199v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["olivier bousquet", "sylvain gelly", "karol kurach", "marc schoenauer", "michele sebag", "olivier teytaud", "damien vincent"], "accepted": false, "id": "1706.03199"}, "pdf": {"name": "1706.03199.pdf", "metadata": {"source": "CRF", "title": "Toward Optimal Run Racing: Application to Deep Learning Calibration", "authors": ["Olivier Bousquet", "Sylvain Gelly", "Karol Kurach", "Marc Schoenauer", "Mich\u00e8le Sebag", "Olivier Teytaud", "Damien Vincent"], "emails": ["oteytaud@google.com"], "sections": [{"heading": null, "text": "Parallel adjustment is seen as a solution to the calibration problem of the algorithm \u2212 selecting the best neural architecture and learning hyperparameter values depending on the available dataset; the notoriously expensive calibration problem is optimally reduced by the detection and early termination of sub-optimal runs; the theoretical contribution refers to the guarantees of optimality within the multiple hypotheses testing framework; experiments with the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement over the state of the art [7] without additional hyperparameters."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Related work", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "3 Overview of PaRR", "text": "The approach presented is based on performance modeling and closely follows the approach of [7].The same 11 basic models are noted conservatively [7] 1. All attempts to reduce the number of models resulted in lower performance. Therefore, each learning curve (validation error) is used by MCMC to derive an estimate of the validation error. \"> t, together with the confidence that the overall accounting of the total budget is more finite, with the result that there is a finite, uninformative procedur.The general computational budgeting is finite, with t < T For simplicity and through misuse of language, we will point to asymptotic characteristics that are applicable in epoch T. Criteria for the posture of a run are presented below to make the decision on the posture of a learning curve."}, {"heading": "4 Experiments", "text": "In this section, the experimental framework and the applied experimental methodology for empirically comparing the proposed PaRR cross-sectional criteria are presented. Initial experimental results (Section 4.3) suggest some simple heuristic improvements (Section 4.4)."}, {"heading": "4.1 Experimental setting", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "4.2 Experimental methodology", "text": "The main components for the PaRR decisions are: i) the confidence threshold required to trim a run; ii) the comparative threshold: The trimming decision is made when the predictive performance of a run is below the comparative threshold, with reliability being at least 1, and iii) the entire calculation budget. The sensitivity of the confidence value is discussed in Section 4.3. The effects of the total calculation budget (here is the number of simultaneous runs) are dramatic, as shown in Figure 1 in the case of the bit-level PTB modeling task. 50 runs are started in parallel, with a permissible number of eras on 30, 15 and 7. Empirically, about 30 independent parallel runs are needed to ultimately provide a \"reasonably optimal\" performance. Note that this experimental setting is typical, with very high computational costs."}, {"heading": "4.3 Pruning with confidence \u03b4 = .5", "text": "In Table 1, the results of the criteria (a) (baseline [7]); (c), (e) and (f) in all experimental cases for \u03b4 =.5.Although confidence is very low (which at first glance might imply that the probability of FAIL is about 50%), these results are very good. A cautious interpretation of this fact is that, although confidence is low, the comparison threshold is set at the best validation error to date, which is quite a conservative threshold. At a low confidence threshold, more aggressive comparison thresholds lead to errors. Criterion (c) fails in view of the predicted error threshold. Criterion (e) fails even more often in view of the best predicted validation error. Overall, the basic method (a) and method (e) do not fail. Calculation savings are such that method (e) wins in 5 experimental cases and (a) fails even more frequently in 1 experimental case (e)."}, {"heading": "4.4 Conservative pruning rules", "text": "A natural question arising from the empirical results (Table 1) is whether undesirable errors could be prevented using simple heuristic conservative rules, such as: i) never cutting the current best; ii) discarding all predictions of negative loss; iii) discarding predictions with correlation data / observations below 0.5. Accordingly, the experiments are repeated by enriching all intersectional criteria with the conservative rules (Table 2). Although these simple rules save some errors in methods (c) and (e), the overall conclusions remain the same as in Table 1: methods (a) and (f) are the only safe ones, with (e) slightly better than (a) in terms of computational savings. As expected, the overall advantage is undermined as the aggressive (c) and (e) methods no longer fail on 4 of the 15 problems."}, {"heading": "4.5 Pruning with low confidence \u03b4 and conservative rules", "text": "Assuming that conservative heuristic rules prevent aggressive intersection criteria from most failures, the question arises whether better savings can be achieved by lowering the confidence threshold. However, as shown in Table 3 (supplement material), a lower \u03b4 =.01 improves the results of criterion (c) (although dominated by the results of criterion (f) for \u03b4 =.5). However, errors are still observed for criterion (e); the interpretation of this fact (in accordance with the multiple hypotheses framework, in fact) is that an increase in the number of tests is a strong factor of failure. Interestingly, overall results are worse globally than for g = 0.5. Other confidence levels (\u03b4 = 0.1, \u03b4 = 0.3, \u03b4 = 0.05, \u03b4 = 0.01) have been taken into account (results in supplement material). Figure 3 graphically shows the performance of the intersection criterion (f) compared to baseline (7) [7]."}, {"heading": "5 Conclusions", "text": "A first contribution of the presented paper is to confirm the relevance of the cutting method proposed by [7], with the arithmetical savings often exceeding 85% (especially for image applications) and applicable to both levels of randomized hyperparameter optimization and model selection (the problem of model selection itself, which includes a hyperparameter selection problem). Cumulative gains from both levels can reduce the computational costs by more than one order of magnitude, as demonstrated by the Cifar and Metacifar experiments. Interestingly, the sensitivity of the approach to the confidence threshold proves to be low. Set to [7] at 0.05, we show that it can be increased to 0.5; this stability is explained by conservative modeling of performance in the settings under consideration. Our second contribution is a new and more principled cutting method that makes the previous method small but significantly superior to all confidence thresholds, with a reference to the stability in the preceding 3."}, {"heading": "A Experiments with other values of \u03b4", "text": "A.1 Comparison between the criteria with DPKP = 0.1Testbed Budget (number of runs) -27.5% TB cost saved by method (a) (equal to Domhan et al) C.c. saved by method (c): prediction-hold operator. C.c. saved by method (e): best-predictionhalt operator. C.c. saved by method \"Cleverhalt\" Cifar-adagrad 22 -50.7% -50.7% -57.7% -50.7% -50.7% Cifar-adam 22 -87.0% -92.4% FAIL by.797 \u2192 Method \"Cleverhalt\" -95.2% FAIL \"0.797% higher than the bond of 0.933 -87.0% Cifar-gradient 22 -79.0% -79.0% -79.0% -79.0% Cifar-adam 22% -92.4% FAIL.\" 797 \"We have 0.923 -923 -923% -95.2% AI81.3% FAIL."}], "references": [{"title": "Collaborative hyperparameter tuning", "author": ["R. Bardenet", "M. Brendel", "B. K\u00e9gl", "M. Sebag"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML), volume 28, pages 199\u2013207,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for hyperparameter optimization", "author": ["J. Bergstra", "R. Bardenet", "Y. Bengio", "B. K\u00e9gl"], "venue": "P. B. F. P. K. W. J. Shawe-Taylor, R.S. Zemel, editor, Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS), volume 24 of Advances in Neural Information Processing Systems, Granada, Spain,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparison of ranking methods for classification algorithm selection", "author": ["P. Brazdil", "C. Soares"], "venue": "R. L. de M\u00e1ntaras and E. Plaza, editors, the 11th European Conference on Machine Learning (ECML), volume 1810 of LNCS, pages 63\u201374. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep recurrent neural networks for acoustic modelling", "author": ["W. Chan", "I. Lane"], "venue": "CoRR, abs/1504.01482,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "New types of deep neural network learning for speech recognition and related applications: an overview", "author": ["L. Deng", "G.E. Hinton", "B. Kingsbury"], "venue": "ICASSP, pages 8599\u20138603. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves", "author": ["T. Domhan", "J.T. Springenberg", "F. Hutter"], "venue": "IJCAI, pages 3460\u20133468. AAAI Press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML, volume 32 of JMLR Workshop and Conference Proceedings, pages 647\u2013655. JMLR.org,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-stochastic Best Arm Identification and Hyperparameter Optimization, volume 51 of JMLR Workshop and Conference Proceedings", "author": ["A. Gretton", "C.C. Robert", "editors"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Hyperparameter optimization of deep neural networks using non-probabilistic RBF surrogate model", "author": ["I. Ilievski", "T. Akhtar", "J. Feng", "C.A. Shoemaker"], "venue": "CoRR, abs/1607.08316,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, NIPS 12, pages 1106\u20131114,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian hyperparameter optimization for ensemble learning", "author": ["J. Levesque", "C. Gagn\u00e9", "R. Sabourin"], "venue": "CoRR, abs/1605.06394,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient hyperparameter optimization and infinitely many armed bandits", "author": ["L. Li", "K.G. Jamieson", "G. DeSalvo", "A. Rostamizadeh", "A. Talwalkar"], "venue": "CoRR, abs/1603.06560,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Comput. Linguist., 19(2):313\u2013 330, June", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "al"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Bayesian heuristic approach to global optimization and examples", "author": ["J. Mockus"], "venue": "J. Global Optimization, 22(1-4):191\u2013203,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Using meta-mining to support data mining workflow planning and optimization", "author": ["P. Nguyen", "M. Hilario", "A. Kalousis"], "venue": "J. Artif. Intell. Res. (JAIR), 51:605\u2013644,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A clearing procedure as a niching method for genetic algorithms", "author": ["A. P\u00e9trowski"], "venue": "International Conference on Evolutionary Computation, pages 798\u2013803,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "The algorithm selection problem", "author": ["J. Rice"], "venue": "Advances in computers, 15:65\u2013118,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1976}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "al"], "venue": "search. Nature,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, NIPS12, pages 2960\u20132968,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Pairwise meta-rules for better meta-learningbased algorithm ranking", "author": ["Q. Sun", "B. Pfahringer"], "venue": "Machine Learning, 93(1):141\u2013161,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Freeze-thaw bayesian optimization", "author": ["K. Swersky", "J. Snoek", "R.P. Adams"], "venue": "CoRR, abs/1406.3896,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 847\u2013855. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-weka: combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "I. S. Dhillon, Y. Koren, R. Ghani, T. E. Senator, P. Bradley, R. Parekh, J. He, R. L. Grossman, and R. Uthurusamy, editors, The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, (KDD), pages 847\u2013855,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art [7] with no extra hyper-parameter.", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 1, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 22, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 0, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 20, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 15, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 9, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 6, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 4, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 13, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 18, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 5, "context": "manual algorithm calibration, as noted by [7].", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "The challenge lies in making the stopping decision with little and censored evidence: as all runs are simultaneous, only prior information about the learning curves behavior is available, as in [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 21, "context": "ensuring that the best run lives until the end of the period; ii) the optimization consists of minimizing the computational budget subject to the optimality guarantee, by stopping any run (with no possible resuming of the run, as opposed to [26]) as early as possible.", "startOffset": 241, "endOffset": 245}, {"referenceID": 5, "context": "The present paper, building upon the current best approach [7], makes theoretical and empirical contributions.", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "On the empirical side, experimentations on the Cifar [13], PTB [17], and MiniWiki [11] benchmarks show a consistent improvement compared to [7], for each and every hyper-parameter setting.", "startOffset": 63, "endOffset": 67}, {"referenceID": 5, "context": "On the empirical side, experimentations on the Cifar [13], PTB [17], and MiniWiki [11] benchmarks show a consistent improvement compared to [7], for each and every hyper-parameter setting.", "startOffset": 140, "endOffset": 143}, {"referenceID": 17, "context": "In the domain of algorithm selection and calibration, a usual approach is to build a performance model [22], predicting the eventual performance of the algorithm based on its hyper-parameter configuration and on the description of the problem instance at hand.", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 23, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 102, "endOffset": 109}, {"referenceID": 19, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 102, "endOffset": 109}, {"referenceID": 23, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 157, "endOffset": 161}, {"referenceID": 1, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 14, "endOffset": 25}, {"referenceID": 19, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 14, "endOffset": 25}, {"referenceID": 23, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 14, "endOffset": 25}, {"referenceID": 14, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "In [12], coordinate-based optimization reports good results, particularly so in high-dimensional hyper-parameter space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "A first extension overcoming the sequential issue is proposed by [26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "Overall, [26] maintains a basket of runs, typically involving 10 alive (non-frozen) runs and 3 new ones, where the decision is based on the maximum \"asymptotic\" performance reached on this learning curve according to Expected Improvement.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "Another approach is that of [7], with two differences compared to [26].", "startOffset": 28, "endOffset": 31}, {"referenceID": 21, "context": "Another approach is that of [7], with two differences compared to [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "Firstly, the domain knowledge is leveraged to select 11 models best reflecting the usual learning curves (ranging from vapor-pressure to Weibull law; see [7] for more detail), and referred to as basic models in the following.", "startOffset": 154, "endOffset": 157}, {"referenceID": 11, "context": "In [16], the problem of hyperparameter optimization is formulated as a pure exploration adaptive resource allocation problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "The approach builds upon the Successive-Halving process proposed by [9], which most simply prunes 50% of the runs with lowest current performance, until a single configuration remains; each run corresponds to a (uniformly sampled) configuration.", "startOffset": 68, "endOffset": 71}, {"referenceID": 11, "context": "The Hyperband approach [16] addresses this limitation using a infinitely-many arm bandit approach on the space of number n of configurations", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Typically, whenever several runs are very similar and close from the best-so-far one, the parallel approach proposed by [7] is bound to keep them all.", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "The presented approach relies on performance modelling and closely follows the approach of [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "These criteria are parameterized by a confidence threshold \u03b4 \u2208 [0, 1], as in [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "This is the criterion used by [7]).", "startOffset": 30, "endOffset": 33}, {"referenceID": 12, "context": "PTB [17] aims at language modeling at the bit, byte and word levels.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "02, 1]), the learning rate (in [5, 100]), the dropout keep probability (in [0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 8, "context": "01, 10]), the number of epochs before learning rate decay (in [12, 198]), the learning rate decay (in [0.", "startOffset": 62, "endOffset": 71}, {"referenceID": 3, "context": "143, 30 epochs, cell clipping [5] between 1 and 10000.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "Table 1 reports the results of criteria (a) (the baseline [7]); (c), (e) and (f) on all experimental cases, for \u03b4 = .", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "3 graphically displays the performance of the pruning criterion (f) compared to the baseline (a) [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "A first contribution of the presented work is to confirm the relevance of the pruning method proposed by [7], with computational savings often above 85% (particularly so for image applications), and applicable at both levels of randomized hyper-parameter optimization, and model selection (the model selection problem itself embedding a hyper-parameter selection problem).", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "05 in [7], we show that it can be increased up to 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "A first research perspective is to introduce diversity-based pruning for ensemble methods [15], taking inspiration from the clearing methods", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "in multi-modeal optimization [21].", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "A second perspective will investigate whether the quantile-based proposed approach can make the inference simpler (as opposed to the Metropolis-Hasting method used in [7]).", "startOffset": 167, "endOffset": 170}], "year": 2017, "abstractText": "This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem \u2212 selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art [7] with no extra hyper-parameter.", "creator": "LaTeX with hyperref package"}}}