{"id": "1706.03200", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Critical Hyper-Parameters: No Random, No Cry", "abstract": "The selection of hyper-parameters is critical in Deep Learning. Because of the long training time of complex models and the availability of compute resources in the cloud, \"one-shot\" optimization schemes - where the sets of hyper-parameters are selected in advance (e.g. on a grid or in a random manner) and the training is executed in parallel - are commonly used. It is known that grid search is sub-optimal, especially when only a few critical parameters matter, and suggest to use random search instead. Yet, random search can be \"unlucky\" and produce sets of values that leave some part of the domain unexplored. Quasi-random methods, such as Low Discrepancy Sequences (LDS) avoid these issues. We show that such methods have theoretical properties that make them appealing for performing hyperparameter search, and demonstrate that, when applied to the selection of hyperparameters of complex Deep Learning models (such as state-of-the-art LSTM language models and image classification models), they yield suitable hyperparameters values with much fewer runs than random search. We propose a particularly simple LDS method which can be used as a drop-in replacement for grid or random search in any Deep Learning pipeline, both as a fully one-shot hyperparameter search or as an initializer in iterative batch optimization.", "histories": [["v1", "Sat, 10 Jun 2017 08:02:34 GMT  (4508kb,D)", "http://arxiv.org/abs/1706.03200v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["olivier bousquet", "sylvain gelly", "karol kurach", "olivier teytaud", "damien vincent"], "accepted": false, "id": "1706.03200"}, "pdf": {"name": "1706.03200.pdf", "metadata": {"source": "CRF", "title": "Critical Hyper-Parameters: No Random, No Cry", "authors": ["Olivier Bousquet", "Sylvain Gelly", "Karol Kurach", "Olivier Teytaud", "Damien Vincent"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Hyperparameter (HP) optimization can be interpreted as the black box search of an x, so that for a certain function f: S-Rd \u2192 R, the value f (x) is small, where the function f can be stochastical. This captures the situation in which one is looking for the best setting of the hyperparameters of a (possibly randomized) machine learning algorithm by testing several values of these parameters and selecting the value that yields the best validation error. Nonlinear optimization in general is a well-developed area [18]. However, hyperparameter optimization in the context of deep learning has several specific features that need to be taken into account in order to develop appropriate optimization techniques, calculations can be paralleled, derivatives are not easily accessible, and there is a discrepancy between training, validation and test errors."}, {"heading": "2 One-shot optimization: formalization & algorithms", "text": "In this paper, we focus on so-called one-shot optimization methods, where a priori one selects the points at which the function is evaluated and does not modify this choice based on the observed values of the function, reflecting the parallel coordination of HP, where one exercises a learning algorithm with several choices of HP and simply selects the best choice (based on the validation error).We do not consider the possibility of saving computing resources by stopping some runs early based on an estimate of their final performance, as is the case with methods such as sequential halving [11] and hyperband [14] or modeling validation curves as in [5]."}, {"heading": "2.1 Formalization", "text": "We consider a function f defined to [0, 1] d and are interested in recognizing its improbability. To simplify the analysis, we assume that the inflation rate of f is reached at a certain point. [0, 1] d and, given the performance of such an algorithm, the value mini f (xi) is measured. Since the choice of sequence is independent of the function f and its values, we can simply use such an algorithm as a production of a distribution of elements of [0, 1] d and, given the performance of such an algorithm measured by the optimization error: \"mini f (xi) \u2212 infx [0,1] d (x) d (xi) [mini f] f (x) \u2212 f [x] f (x) f (x). This is a random variable and we are therefore interested in its quantities."}, {"heading": "2.2 Sampling Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Grid and Random", "text": "Suppose n = kd. Then the scanning in the grid consists in selecting k values for each axis and then taking all the kd points that can be achieved with these k values per axis. The value k does not have to be the same for all axes; this does not affect the current discussion. There are various tools for selecting k values per axis, evenly distributed or purely random or in a stratification. Random scanning consists in selecting n independently and uniformly scanned vectors in [0, 1] d."}, {"heading": "2.2.2 Low Discrepancy Sequences", "text": "In the case of numerical integration, there is a close link between the integration error and a measure of the \"spread\" of the points referred to as the discrepancy. Definition 4 (discrepancy) The discrepancy of a set S = {x1,.., xn} [0, 1] d in relation to a random sequence of [0, 1] d is defined as a disc (S, R): = supR | \u00b5S (R) \u2212 \u00b5 (R) is the random extension of [0, 1] d in relation to a random sequence of [0, 1] d is the fraction of elements of S belonging to R.LDS, algorithms constructing such a disc (S, H0) = O (log (n) d / n)."}, {"heading": "2.2.3 Latin Hypercube Sampling (LHS)", "text": "LHS [6, 16] construct a sequence as follows: Consider for a given n the division of [0, 1] d into a regular grid of nd cells and then define the index of a cell as its position in the corresponding nd grid; select d random permutations \u03c31,..., \u03c3d of {1,..., n} and choose for each k = 1,..., n, the cell whose index \u03c31 (k),..., \u03c3d (k), and choose xk uniformly randomly in that cell. LHS ensures that all edge projections on an axis have a point in each of the n regular intervals [0, 1]. On the other hand, LHS can be unlucky; if \u03c3j (i) = i for each j level {1, 2,..., d} and i level {1, 2,..., n}, then all points on the diagonal cells are located."}, {"heading": "3 Theoretical Analysis", "text": "Due to length constraints, all evidence is reported on the complementary material realm. Desirable properties. Here are some properties that are particularly relevant for building parallel HP optimization: No bad sequence: When using randomized algorithms to generate sequences, a desirable property is that the probability of obtaining a bad sequence (and thus missing the optimal setting of HP by a large amount) should be as low as possible. In other words, you want to have a way to avoid bad luck. Robustness w.r.t. Irrelevant parameters: the performance of the search process should not be affected by the addition of irrelevant parameters. In fact, it is often the case that you develop algorithms with many HPs, some of which do not affect the performance of the algorithm or only slightly affect it. Otherwise, it is stated that the optimization is projected on a subset of critical variables, we get a point with similar properties in this sub-dimensional space."}, {"heading": "3.1 Dispersion of Sampling Algorithms & Projection to Critical Variables", "text": "The first observation is that when we compare (shifted) lattices and (shifted) LDS as viewed in Theorem 1, their stochastic dispersion is of the same asymptotic order 1 / n1 / d.Theorem 2 (Asymptotic rate) Consider random, (shifted) lattices, or a shifted LDS as in Theorem 1. For each fixed sequence, there is a constant sequence of such deviations that for n large enough, sdisp (P, \u03b4) \u2264 K (c\u03b4 n) 1 / d.Remark 1 A natural question is whether the dependence on \u03b4 is different for different sequences. We can prove that stochastic dispersion is more likely for shifted LDS or shifted lattices than for random. In particular, we can show that for all 0 < c < 12, for both close enough to1, sdisp. \""}, {"heading": "3.2 Pathological Functions", "text": "As we have seen above, dispersion can give a characterization of the optimization error in cases where the continuity module is well behaved around the optimum. Therefore, convergence to zero of the dispersion is a sufficient condition for consistency. However, one can construct pathological functions for which the considered algorithms do not have low optimization errors. Deterministic sequences are inconsistent: In particular, if the sequence is deterministic, one can always construct a function on which the optimization will not converge at all. In fact, a sequence x1 is given,., xn,., one can construct a function such that f (xi) = 1 for each i and f (x) = 0 otherwise except in the neighborhood of the xi. Obviously, the optimization error will be 1 for each n, which means that the optimization procedure is not even consistent (i.e. the convergence of the function within the boundary of n)."}, {"heading": "4 Experiments", "text": "We perform extensive empirical evaluations of LDS. First, we validated the theory using a series of simple optimization problems in toys. These rapid problems provide comprehensive validation with negligible p-values. The results (given in Section C) illustrate each claim - the performance of LDS for one-shot optimization (compared to Random and LHS), the positive effect of Hammersley (compared to Halton) and scrambling, the additional improvement in the ranking of variables due to decreasing importance, and the presence of counter-examples. We conduct some real deep learning experiments to further confirm the good properties of S-SH for hyperparameter optimization. Finally, we expand the use of LDS as initialization in a first step in the context of Bayesian optimization. Some additional experiments are performed in Section E, the additional results coincide with the assertions."}, {"heading": "4.1 Deep Learning tasks", "text": "We need to have a consistent and robust way of comparing the scanning algorithms that are inherently stochastical. To this end, we can simply measure the probability that a scanning algorithm S is better than a random search if both use the same budget b of hyperparameters. From this \"win rate\" probability, we can simply define a speed, usually in the form of 2p \u2212 11 \u2212 p.Given two examples of random search R1 and R2, this refers to the additional budget needed for R1 to be better than R2 with the probability p. That is, if b1 + s) b2 + s2 + s2 + s2 s2 s2 s2 s2 s1 s2 s2 s2 s2 s2 s2 s2 s1 \u2212 p.In addition to the speed we need to be better than R2 with the probability p."}, {"heading": "4.2 LDS as a first-step in a Bayesian optimization framework.", "text": "LDS (here S-Ha) is also successful in the context of sampling the first batch in a Bayesian optimization run (Table 1), LDS exceeds (a) random sampling, (b) LHS, and (c) methods based on pessimistic fantasy about the objective value of sampled vectors [8]. In rare cases, Bayesian optimization on LHS or pessimistic fantasies outperforms our LDS-based Bayesian optimization even for one batch - but in dimension 12, LDS-BO (LDS-initialized Bayesian optimization) still performs best for most batches in {1, 3, 5} and most functions in {Branin, Beale, SixHump, Rastrigin, Ellipsoidal, Sphere}."}, {"heading": "5 Conclusion", "text": "We analyzed theoretically and experimentally the performance of LDS for hyperparameter optimization. Results of the convergence rate show that LDS is strictly better than random search for both high and low confidence quantities (Theorem 3 and Remark 1). Specifically, LDS with enough points can find the optimum within range with the probability of exactly 1, due to the absence of unfortunate cases, as opposed to random search. In interim quantitatives, the theoretical quantity boundaries between LDS and random search are equivalent, but we could not prove that the constant is better except in dimension 1. In general, where parameters have different effects, and few are really important, we show that parameters should be ranked (Remark 2). Furthermore, we prove that LDS exceeds the grid search (if some variables are crucial) and the best LDS sequences are robust against bad parameter evaluation (Theorem 4), we confirmed the results of our experiments are only small, though our LDS confirms and our experiments are a small one."}, {"heading": "A Proofs", "text": "Proof [Lemma 2] A sphere with radius has volume Vd, so if v is the volume of a sphere, its radius (v / Vd) is 1 / d. If v is the largest volume of a sphere that does not contain a point of S, then the largest hypercube contained in this sphere, which has volume of v.Kd, is also empty, showing that vdisp (S, B) \u2264 Kdvdisp (S, H). Finally, it is easy to see that the volume scattering is limited upwards by the discrepancy as it results from its definition. Proof [Theorem 2] for lattices, this is a known result [20]. By chance, the probability that n randomly selected spheres with radius x contain a certain point that is limited upwards by 1 \u2212 (1 \u2212 V \u2032 d) n, therefore this results in an upper limit of (log 1 / \u03b4 n) for n large sequences."}, {"heading": "B Detailed experiment setups", "text": "Setup A language model with 3 layers LSTM with 250 units for 6 epochs. In this setup we have 5 hyperparameters: Weight inite scale [0.02, 1], Adams parameter [0.001, 2], Clipping gradient standard [0.02, 1], Learning rate [0.5, 30], Drop-out keep probability [0.6, 1]. Setup B image classification model with 3 Convolutionary layers, with filter size 7x7, Max pooling, increment 1, 512 channels; then a Convolutionary layer with filter size 5x5 and 64 channels; then two fully connected layers with 384 and 192 units, respectively. We apply stochastic gradient departure with batch size 64, Weight inite scale in [1, 30] for feedforward layers and [0.005, 0.03] for convolutionary layers, Learning rate [0.000.5, 0.002], Fracture rate with fraction rate, 9001, 0.001]."}, {"heading": "C Toy optimization problems", "text": "We performed a series of experiments on multiple toy problems to quickly confirm our assumptions. Compared methods are one-shot optimization algorithms based on the following samples: Random, LHS, Sobol, Hammersley, Halton, S-SH. We loop through dimensions 2, 4, 8 and 16; we check three objective functions (l2-norm f (x) = | x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x). The budget is n = 37 in all cases."}, {"heading": "D Artificial datasets", "text": "Our artificial data sets are indexed with a string (AN or ANBN or.N or other irregularly repeated numbers) and 4 numbers. For the artificial data set C, the 4 numbers are the vocabulary size (the number of letters), the maximum word size (n or N), the vocabulary growth and the depth - unless otherwise stated, the vocabulary growth is 10 and the depth is 3. There are also four parameters for the maximum word sequence (ANBN, AN,.N and anbn; but the last two parameters are different: they are \"fixed size\" (true for fixed size, false different) and \"size gaps\" (effects of size gaps equal to True detailed below). For example, artificial (anbn, 26,10,0,1) the letters are randomly drawn between 1 and 10, and that there are size gaps; whereas artificial (anbn, 26,10,1,0) means that we are equal. \""}, {"heading": "E Additional experiments", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The selection of hyper-parameters is critical in Deep Learning. Because of the long training time of complex models and the availability of compute resources in the cloud, \u201cone-shot\u201d optimization schemes \u2013 where the sets of hyper-parameters are selected in advance (e.g. on a grid or in a random manner) and the training is executed in parallel \u2013 are commonly used. [1] show that grid search is sub-optimal, especially when only a few critical parameters matter, and suggest to use random search instead. Yet, random search can be \u201cunlucky\u201d and produce sets of values that leave some part of the domain unexplored. Quasi-random methods, such as Low Discrepancy Sequences (LDS) avoid these issues. We show that such methods have theoretical properties that make them appealing for performing hyperparameter search, and demonstrate that, when applied to the selection of hyperparameters of complex Deep Learning models (such as state-of-the-art LSTM language models and image classification models), they yield suitable hyperparameters values with much fewer runs than random search. We propose a particularly simple LDS method which can be used as a drop-in replacement for grid/random search in any Deep Learning pipeline, both as a fully one-shot hyperparameter search or as an initializer in iterative batch optimization.", "creator": "LaTeX with hyperref package"}}}