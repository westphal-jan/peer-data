{"id": "1706.03235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "ACCNet: Actor-Coordinator-Critic Net for \"Learning-to-Communicate\" with Deep Multi-agent Reinforcement Learning", "abstract": "Communication is a critical factor for the big multi-agent world to stay organized and productive. Typically, most previous multi-agent \"learning-to-communicate\" studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm, which can not generalize to changing environment or large collection of agents.", "histories": [["v1", "Sat, 10 Jun 2017 13:50:23 GMT  (1157kb,D)", "https://arxiv.org/abs/1706.03235v1", "Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments"], ["v2", "Tue, 13 Jun 2017 02:00:14 GMT  (1158kb,D)", "http://arxiv.org/abs/1706.03235v2", "Version-2 of original submission. Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments"], ["v3", "Sun, 29 Oct 2017 05:09:39 GMT  (2089kb,D)", "http://arxiv.org/abs/1706.03235v3", "V3 of original submission. Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments"]], "COMMENTS": "Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["hangyu mao", "zhibo gong", "yan ni", "xiangyu liu", "quanbin wang", "weichen ke", "chao ma", "yiping song", "zhen xiao"], "accepted": false, "id": "1706.03235"}, "pdf": {"name": "1706.03235.pdf", "metadata": {"source": "CRF", "title": "ACCNet: Actor-Coordinator-Critic Net for \u201cLearning-to-Communicate\u201d with Deep Multi-agent Reinforcement Learning", "authors": ["Hangyu Mao", "Zhibo Gong", "Yan Ni", "Zhen Xiao"], "emails": [], "sections": [{"heading": null, "text": "Typically, most previous studies on multi-agents, learning-to-communication, predefining communication protocols, or using technologies such as tabular reinforcement learning and evolutionary algorithms that cannot be directly applied to the changing environment or large aggregate of agents. In this paper, we propose a framework for solving the multi-agent learning-to-communication problem. ACCNet naturally combines the powerful amplification technology between actors and critics with deep learning technology. It can even learn communication protocols from scratch in partially observable environments. We show that ACCNet can achieve better results than multiple baselines in continuous and discrete action environments. We also analyze the protocols learned and discuss some design considerations."}, {"heading": "Introduction", "text": "In fact, it is in such a way that we are able to put ourselves in a situation in which we see ourselves in a position, in which we are able to claim that we are in a position, in which we are located, in which we are located, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live in which we live, in which we live, in which we live, in which we live in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we, in which we live, in which we, in which we live, in which we live, in which we live,"}, {"heading": "Background", "text": "Reinforcement learning (RL) [13] is a machine learning approach to solving sequential decision-making problems. At each time step Q = Q | Q, the agent observes a state st and takes action, and then receives a feedback reward from the environment and observes a new state st + 1. The goal of RL is to learn a policy approach \u03c0 (a | s), i.e., a mapping from the state to action that can maximize the expected discount cumulative future reward E [R] = E [\u2211 T = 0 trt]. Model-free RL algorithms can be divided into three groups [14, 15]. (1) Actor-only methods directly learn the parameterized policy approach (a | s; \u03b8). They can generate continuous action but suffer from high variance in the estimation of policy development. (2) Criticonly methods use a small time difference to estimate the time difference."}, {"heading": "Related Work", "text": "How communication protocols can be learned efficiently is critical to the success of multi-agent systems. However, most previous work defines the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithms [22] that do not extend directly to the changing environment and the large collection of agents as [13, 4] points. Recently, the differentiated communication channel embedded in a deep neural network has proven useful for learning communication protocols. However, in general, the protocols can be optimized at the same time as the network is being optimized. Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].CommNet is a single network designed for all agents. The input is the concatenation of current states of all agents that are relevant for all network layers."}, {"heading": "Actor-Coordinator-Critic Net Framework", "text": "In this section, we present two paradigms of the ACCNet framework for learning communication protocols based on models developed by stakeholder critics."}, {"heading": "AC-CNet", "text": "The simplest approach is to build a channel of communication between actors and to maintain the independence of the critics. As shown in Figure 2 (b), a channel of communication of the coordinator is used to coordinate the actors to generate coordinated actions, hence we call this paradigm AC-CNet. Specifically, each agent encodes its local state into a local message and sends it to the coordinator, who, taking into account the messages of all other actors, generates the global communication signal for that agent. As the global signal is a coding of all local messages, we expect that it can capture the global information of the system. The integrated state is the concatenation of local state and global signal, which is fed into the input into the actor-critic model. Then, the entire AC-CNet is trained as the original actor-critic model. However, the AC-CNet necessarily needs the communication between actor and coordinator to maintain the global information during execution, which is impractical in some situations [45]."}, {"heading": "A-CCNet", "text": "Fortunately, machine learning has the fascinating property that we can make predictions after a model is trained and the auxiliary data on which the model is trained no longer needs to be stored. We wonder if we can turn communication between actors into criticism so that actors can act independently of their specific states during execution, and auxiliary critics no longer need to be stored during training. In fact, it is possible that methods between actors and critics are unsuitable for this task, since the training and execution mechanisms of these methods are exactly the same. As shown in Figure 2 (c), a communication channel for coordinators is used to coordinate the critics to generate better estimated Q values, so we call this paradigm A-CCNet. Specifically, the actor in A-CCNet can generate the same code as the actor in Figure 2 (c), using the coordinator's communication channel to coordinate the critics to generate better estimated values between the two actors, generating better values during communication."}, {"heading": "Formal Formulation of ACCNet", "text": "In AC-CNet, since critics are independent, we can update each agent based on the equation (1-4), just as in updating individual actors-critics-agents. A major difference is that we have to move the gradients of the actors into the communication channels of the coordinator so that the communication protocols can be optimized at the same time. In A-CCNet, since critics communicate with each other, the critical network of the i-agent is now V i (si, sg; wi) or Qi (si, ai, sg; wi), where sg = f (s1,..., sN, a1,..., aN) is the global communication signal2. We can then speak the equation (1-4) in multiagent2Generally speaking, f is an injective function. In addition, the use of V i (si, sg; wi) and Qi (si, a1, sg; wi; wi) for discrete and continuous action is natural."}, {"heading": "Some Comparisons", "text": "Before the comparison, we will first present the two simultaneous studies mentioned in footnote 1, i.e. COMA [34] from Oxford and MADDPG [33] from OpenAI.COMA, MADDPG and A-CCNet share a similar idea: to accelerate training with the help of critics and to execute it in real-world environments only on the basis of actors, but the research purposes are different. COMA aims at solving the credit allocation problem in cooperative environments with multiple actors. MADDPG wants to examine both collaboration and competition between actors. ACCNet proposes to create a general framework to facilitate the learning of communication protocols among actors even from the ground up. Specifically, COMA relies on the stochastic policy gradient theorem [13] and the REINFORCE [35] algorithm, which uses a counterfactual baseline and a centralized critic to solve the problem of lending between actors."}, {"heading": "Experiments", "text": "In this section, we will test the proposed ACCNet under both continuous and discrete action environments. These environments are partially observable with multiple distributed and fully cooperative actors."}, {"heading": "Continuous Action Space Environment", "text": "In fact, most of us are able to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live, to live in, to live, to live in, to live, to live in, to live, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live, to live in, to live in, to live in, to live, to live in, to live in, to live, to live in, to live in, to live, to live in, in, to live in, in, to live in, to live, to live in, to live in, in, to live, in, in, in, to live, in, in, in, in, in, in, in, in, to live in, to live, to live, in, in, to live, in, to live, to live, in, in, to live, in, in, in, to live, in, to live, in, in, in, to live, in, to live, in, to live, in, in, to live, to live, in, to live, in, in, to live, to live, to live, to live, in, to live, to live, in, to live, to live, in, to live, to live, to live, to live, in"}, {"heading": "Discrete Action Space Environment", "text": "We consider the problem of traffic junctions modified from [28, 23]. As shown in Figure 6, four cars are derived on the four-lane road. New car is generated when a car reaches its destination at the edge of the grid. Simulation is classified as failure if location overlaps have occurred in 40 periods. Our goal is to learn an auto driving policy, so we have a low failure rate (FR).Setting. We use the same RL elements as in CommNet. State. All cars can only know their position and driving direction. You can not see other cars. So we represent the local state as a single-lane vector rate {location, direction}.Action A car has two possible actions: gas itself through a cell on its route or braking to stay at its current location. Reward. A collision causes a reward rcoll = -10.0, and each car gets reward from the speed time = 0.01 percent."}, {"heading": "Design Discussion of ACCNet", "text": "As we know, some design decisions are very important for the success of DRL in real-world applications. For example, experience repetition, frame skipping, target network, reward clipping, asynchronous training, auxiliary task, and even methods of DL such as batch normalization, attention mechanism, and skip connection are widely used [24, 29, 30, 31, 18, 32, 23]. In this section, we briefly present a few design decisions used by ACCNet, in the hope that other researchers can confirm their usefulness in new environments. Note that all of these design decisions need to be studied further. (1) The embedded communication channel. We suggest using deep neural networks to encode the communication message so that the final message dimension is controlled to be independent of the dimension of the original information. And most importantly, the communication channel is embedded in deep neural networks, which communication protocols can even be learned from scratch."}, {"heading": "Conclusion", "text": "The proposed ACCNet, born with the combined capabilities of deep models and actuator critics who amplify models, is a general framework for learning communication protocols from the ground up for fully cooperative, partially observable MARL problems, regardless of whether the action space is continuous or discrete. In particular, A-CCNet, a concrete implementation of ACCNet, can make the formation of MARL systems more stationary than previous methods supported by both mathematical equation (11) and experimental results from different environments. Another attractive advantage of A-CCNet is that it does not require communication during execution while retaining good generalization capability. For future work, we will focus our efforts on the following important and challenging problems. (1) How to make the training of discrete action space MARL systems more stationary. Specific experiential replay method can be a powerful tool for this problem. (2) How to design communication signals more sparingly."}, {"heading": "Acknowledgments", "text": "The authors thank Xiangyu Liu, Weichen Ke, Chao Ma, Quanbin Wang, Yiping Song and the anonymous reviewers for their insightful comments. This work was supported by the National Natural Science Foundation of China under grant number 61572044. Contact author is Zhen Xiao."}], "references": [{"title": "TCP-like congestion control for layered multicast data transfer[C]//INFOCOM\u201998", "author": ["L Vicisano", "J Crowcroft", "L. Rizzo"], "venue": "Seventeenth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "CODA: congestion detection and avoidance in sensor networks[C]//Proceedings of the 1st international conference on Embedded networked sensor systems", "author": ["Y Wan C", "B Eisenman S", "T. Campbell A"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A multiagent approach to managing air traffic flow[J", "author": ["K Agogino A", "K. Tumer"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning to communicate with deep multi-agent reinforcement learning[C]//Advances in Neural Information Processing Systems", "author": ["J Foerster", "M Assael Y", "N de Freitas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search[J", "author": ["D Silver", "A Huang", "J Maddison C"], "venue": "Nature,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Multiagent cooperation and competition with deep reinforcement learning[J", "author": ["A Tampuu", "T Matiisen", "D Kodelja"], "venue": "PloS one, 2017,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games[J", "author": ["P Peng", "Q Yuan", "Y Wen"], "venue": "arXiv preprint arXiv:1703.10069,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Optimizing information exchange in cooperative multi-agent systems[C]// 2003:137-144", "author": ["V Goldman C", "S. Zilberstein"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Decentralized control of cooperative systems: categorization and complexity analysis[M", "author": ["V Goldman C", "S. Zilberstein"], "venue": "AI Access Foundation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Decentralized POMDPs[M]// Reinforcement Learning", "author": ["A. Oliehoek F"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A concise introduction to decentralized POMDPs[M", "author": ["A Oliehoek F", "C. Amato"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning[J", "author": ["F Chen Y", "M Liu", "M Everett"], "venue": "arXiv preprint arXiv:1609.07845,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Introduction to reinforcement learning[M", "author": ["S Sutton R", "G. Barto A"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Onactor-critic algorithms[J", "author": ["R Konda V", "N. Tsitsiklis J"], "venue": "SIAM journal on Control and Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "A survey of actor-critic reinforcement learning: Standard and natural policy gradients[J", "author": ["I Grondman", "L Busoniu", "D Lopes G A"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Deterministic policy gradient algorithms[J", "author": ["G. Lever"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A survey of actor-critic reinforcement learning: Standard and natural policy gradients[J", "author": ["I Grondman", "L Busoniu", "D Lopes G A"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Continuous control with deep reinforcement learning[J", "author": ["P Lillicrap T", "J Hunt J", "A Pritzel"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents[C]//Proceedings of the tenth international conference on machine learning", "author": ["M. Tan"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Learning communication for multi-agent systems[C]//Workshop on Radical Agent Concepts", "author": ["L Giles C", "C. Jim K"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Learning multiagent communication with backpropagation[C]//Advances in Neural Information", "author": ["S Sukhbaatar", "R. Fergus"], "venue": "Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Human-level control through deep reinforcement learning[J", "author": ["V Mnih", "K Kavukcuoglu", "D Silver"], "venue": "Nature,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "MATE: MPLS adaptive traffic engineering[C]//INFOCOM", "author": ["A Elwalid", "C Jin", "S Low"], "venue": "Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Walking the tightrope: Responsive yet stable traffic engineering[C]//ACM SIGCOMM Computer Communication Review", "author": ["S Kandula", "D Katabi", "B Davie"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Mazebase: A sandbox for learning from games[J", "author": ["S Sukhbaatar", "A Szlam", "G Synnaeve"], "venue": "arXiv preprint arXiv:1511.07401,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Prioritized experience replay[J", "author": ["T Schaul", "J Quan", "I Antonoglou"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning[C]//International", "author": ["V Mnih", "P Badia A", "M Mirza"], "venue": "Conference on Machine Learning", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1937}, {"title": "Reinforcement learning with unsupervised auxiliary tasks[J", "author": ["M Jaderberg", "V Mnih", "M Czarnecki W"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Deep attention recurrent Q-network[J", "author": ["I Sorokin", "A Seleznev", "M Pavlov"], "venue": "arXiv preprint arXiv:1512.01693,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "author": ["Lowe", "Ryan"], "venue": "arXiv preprint arXiv:1706.02275", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Counterfactual Multi-Agent Policy Gradients", "author": ["Foerster", "Jakob"], "venue": "arXiv preprint arXiv:1705.08926", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1992}, {"title": "Actor-critic algorithms. Advances in neural information processing systems", "author": ["Konda", "Vijay R", "John N. Tsitsiklis"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Emergence of Grounded Compositional Language in Multi-Agent Populations", "author": ["Mordatch", "Igor", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1703.04908", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "author": ["Das", "Abhishek"], "venue": "arXiv preprint arXiv:1703.06585", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "author": ["Havrylov", "Serhii", "Ivan Titov"], "venue": "arXiv preprint arXiv:1705.11192", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "Deep decentralized multi-task multi-agent reinforcement learning under partial observability", "author": ["Omidshafiei", "Shayegan"], "venue": "International Conference on Machine Learning", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob"], "venue": "arXiv preprint arXiv:1702.08887", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2017}, {"title": "Sparse deep belief net model for visual area V2", "author": ["Lee", "Honglak", "Chaitanya Ekanadham", "Andrew Y. Ng"], "venue": "Advances in neural information processing systems", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "Winner-take-all autoencoders", "author": ["Makhzani", "Alireza", "Brendan J. Frey"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach[J", "author": ["R Dobbe", "D Fridovich-Keil", "C. Tomlin"], "venue": "arXiv preprint arXiv:1707.06334,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "For applications where individual agent has limited capability, it is particularly critical for multiple agents to learn communication protocols to work in a collaborative way, for example: data routing [1], congestion detection [2] and air traffic management [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 1, "context": "For applications where individual agent has limited capability, it is particularly critical for multiple agents to learn communication protocols to work in a collaborative way, for example: data routing [1], congestion detection [2] and air traffic management [3].", "startOffset": 229, "endOffset": 232}, {"referenceID": 2, "context": "For applications where individual agent has limited capability, it is particularly critical for multiple agents to learn communication protocols to work in a collaborative way, for example: data routing [1], congestion detection [2] and air traffic management [3].", "startOffset": 260, "endOffset": 263}, {"referenceID": 4, "context": ", the combination of deep learning (DL) and multi-agent reinforcement learning (MARL), in many applications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": ", the combination of deep learning (DL) and multi-agent reinforcement learning (MARL), in many applications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": ", the combination of deep learning (DL) and multi-agent reinforcement learning (MARL), in many applications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft [7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 7, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 70, "endOffset": 76}, {"referenceID": 8, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 70, "endOffset": 76}, {"referenceID": 9, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "The limited communication bandwidth is a common setting for recent \u201clearningto-communicate\u201d studies [20, 12, 23, 4].", "startOffset": 100, "endOffset": 115}, {"referenceID": 20, "context": "The limited communication bandwidth is a common setting for recent \u201clearningto-communicate\u201d studies [20, 12, 23, 4].", "startOffset": 100, "endOffset": 115}, {"referenceID": 3, "context": "The limited communication bandwidth is a common setting for recent \u201clearningto-communicate\u201d studies [20, 12, 23, 4].", "startOffset": 100, "endOffset": 115}, {"referenceID": 18, "context": "Traditional cooperative agents can share sensations, learned policies or even training episodes [19, 20], which is not suitable for real-world applications because communication itself takes up much bandwidth.", "startOffset": 96, "endOffset": 104}, {"referenceID": 11, "context": "However, the actors of AC-CNet inevitably need communication even during execution, which is impractical under some special situations [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "Reinforcement learning (RL) [13] is a machine learning approach to solve sequential decision making problem.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Model-free RL algorithms can be divided into three groups [14, 15].", "startOffset": 58, "endOffset": 66}, {"referenceID": 14, "context": "Model-free RL algorithms can be divided into three groups [14, 15].", "startOffset": 58, "endOffset": 66}, {"referenceID": 29, "context": "This mutual reinforcement behavior helps actor-critic methods avoid bad local minima and 1One similar work [33] from OpenAI is released at the same time.", "startOffset": 107, "endOffset": 111}, {"referenceID": 30, "context": "Another concurrent work [34] from Oxford also uses a similar idea.", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "converge faster, in particular for on-policy methods that follow the very recent policy to sample trajectory during training [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 12, "context": "Specifically, if actor uses stochastic policy for action selection, the actor and critic are updated based on the following TD-error and Stochastic Policy Gradient Theorem [13]:", "startOffset": 172, "endOffset": 176}, {"referenceID": 15, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 226, "endOffset": 230}, {"referenceID": 31, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 426, "endOffset": 430}, {"referenceID": 12, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 432, "endOffset": 436}, {"referenceID": 32, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 438, "endOffset": 442}, {"referenceID": 15, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 444, "endOffset": 448}, {"referenceID": 17, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 453, "endOffset": 457}, {"referenceID": 18, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 57, "endOffset": 65}, {"referenceID": 19, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 250, "endOffset": 257}, {"referenceID": 3, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 250, "endOffset": 257}, {"referenceID": 20, "context": "Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 21, "context": "Both CommNet and DIAL are based on DQN [24] for discrete action.", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "Other relevant excellent studies include but not limited to [37, 38, 39].", "startOffset": 60, "endOffset": 72}, {"referenceID": 34, "context": "Other relevant excellent studies include but not limited to [37, 38, 39].", "startOffset": 60, "endOffset": 72}, {"referenceID": 35, "context": "Other relevant excellent studies include but not limited to [37, 38, 39].", "startOffset": 60, "endOffset": 72}, {"referenceID": 11, "context": "However, the AC-CNet inevitably needs communication between actor and coordinator to get the global information even during execution, which is impractical under some special situations [12, 45].", "startOffset": 186, "endOffset": 194}, {"referenceID": 40, "context": "However, the AC-CNet inevitably needs communication between actor and coordinator to get the global information even during execution, which is impractical under some special situations [12, 45].", "startOffset": 186, "endOffset": 194}, {"referenceID": 29, "context": "More formally, Equation (11) always keeps true for any agent indexed by i with any changing policies \u03c0 6= \u03c0i [33]:", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": ", COMA [34] from Oxford and MADDPG [33] from OpenAI.", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": ", COMA [34] from Oxford and MADDPG [33] from OpenAI.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Specifically, COMA is based on Stochastic Policy Gradient Theorem [13] and REINFORCE [35] algorithm.", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "Specifically, COMA is based on Stochastic Policy Gradient Theorem [13] and REINFORCE [35] algorithm.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "MADDPG extends DDPG [16, 18] into multi-agent environments.", "startOffset": 20, "endOffset": 28}, {"referenceID": 17, "context": "MADDPG extends DDPG [16, 18] into multi-agent environments.", "startOffset": 20, "endOffset": 28}, {"referenceID": 22, "context": "Figure 3: TwoIE and ThreeIE topologies for network flow control studies [26, 27].", "startOffset": 72, "endOffset": 80}, {"referenceID": 23, "context": "Figure 3: TwoIE and ThreeIE topologies for network flow control studies [26, 27].", "startOffset": 72, "endOffset": 80}, {"referenceID": 24, "context": "We consider the Traffic Junction problem modified from [28, 23].", "startOffset": 55, "endOffset": 63}, {"referenceID": 20, "context": "We consider the Traffic Junction problem modified from [28, 23].", "startOffset": 55, "endOffset": 63}, {"referenceID": 20, "context": "The results of CommNet and Discrete-CN are directly cited from [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 25, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 26, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 27, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 17, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 28, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 20, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 3, "context": "However, as [4] point out, it is necessary to disable experience replay for MARL due to the non-concurrent property of local experiences when sampled independently for each agent.", "startOffset": 12, "endOffset": 15}, {"referenceID": 36, "context": "In fact, [40] use the same replay method and name it CER.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "[29] introduce prioritized experience replay based on the magnitude of TD-error to accelerate learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Footnote 2 and [4, 41] explain this phenomenon in some extent, but further research is needed.", "startOffset": 15, "endOffset": 22}, {"referenceID": 37, "context": "Footnote 2 and [4, 41] explain this phenomenon in some extent, but further research is needed.", "startOffset": 15, "endOffset": 22}, {"referenceID": 3, "context": "In addition, the A-CCNet does not need the coordinator during execution, and centralized training is a common setting for MARL systems [4, 34, 33].", "startOffset": 135, "endOffset": 146}, {"referenceID": 30, "context": "In addition, the A-CCNet does not need the coordinator during execution, and centralized training is a common setting for MARL systems [4, 34, 33].", "startOffset": 135, "endOffset": 146}, {"referenceID": 29, "context": "In addition, the A-CCNet does not need the coordinator during execution, and centralized training is a common setting for MARL systems [4, 34, 33].", "startOffset": 135, "endOffset": 146}, {"referenceID": 38, "context": "Although those concepts are borrowed from sparse autoencoder [42, 43, 44], they are very useful in the real-world distributed MARL systems.", "startOffset": 61, "endOffset": 73}, {"referenceID": 39, "context": "Although those concepts are borrowed from sparse autoencoder [42, 43, 44], they are very useful in the real-world distributed MARL systems.", "startOffset": 61, "endOffset": 73}], "year": 2017, "abstractText": "Communication is a critical factor for the big multi-agent world to stay organized and productive. Typically, most previous multi-agent \u201clearning-to-communicate\u201d studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm, which cannot generalize to the changing environment or large collection of agents directly. In this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) framework for solving multi-agent \u201clearning-to-communicate\u201d problem. The ACCNet naturally combines the powerful actor-critic reinforcement learning technology with deep learning technology. It can learn the communication protocols even from scratch under partially observable environments. We demonstrate that the ACCNet can achieve better results than several baselines under both continuous and discrete action space environments. We also analyse the learned protocols and discuss some design considerations.", "creator": "LaTeX with hyperref package"}}}