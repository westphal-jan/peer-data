{"id": "1706.03254", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "On Hash-Based Work Distribution Methods for Parallel Best-First Search", "abstract": "Parallel best-first search algorithms such as Hash Distributed A* (HDA*) distribute work among the processes using a global hash function. We analyze the search and communication overheads of state-of-the-art hash-based parallel best-first search algorithms, and show that although Zobrist hashing, the standard hash function used by HDA*, achieves good load balance for many domains, it incurs significant communication overhead since almost all generated nodes are transferred to a different processor than their parents. We propose Abstract Zobrist hashing, a new work distribution method for parallel search which, instead of computing a hash value based on the raw features of a state, uses a feature projection function to generate a set of abstract features which results in a higher locality, resulting in reduced communications overhead. We show that Abstract Zobrist hashing outperforms previous methods on search domains using hand-coded, domain specific feature projection functions. We then propose GRAZHDA*, a graph-partitioning based approach to automatically generating feature projection functions. GRAZHDA* seeks to approximate the partitioning of the actual search space graph by partitioning the domain transition graph, an abstraction of the state space graph. We show that GRAZHDA* outperforms previous methods on domain-independent planning.", "histories": [["v1", "Sat, 10 Jun 2017 17:05:46 GMT  (6133kb,D)", "http://arxiv.org/abs/1706.03254v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["yuu jinnai", "alex fukunaga"], "accepted": false, "id": "1706.03254"}, "pdf": {"name": "1706.03254.pdf", "metadata": {"source": "CRF", "title": "On Hash-Based Work Distribution Methods for Parallel Best-First Search", "authors": ["Yuu Jinnai", "Alex Fukunaga"], "emails": ["DDYUUDD@GMAIL.COM", "FUKUNAGA@IDEA.C.U-TOKYO.AC.JP"], "sections": [{"heading": null, "text": "We analyze the search and communication overheads of modern hash-based best-first parallel search algorithms and show that, although Zobrist hashing, the standard hash function used by HDA *, achieves a good load distribution for many domains, a significant communication effort occurs, since almost all generated nodes are transferred to a different processor than their parents. We propose Abstract Zobrist hashing, a new work distribution method for parallel search that, instead of calculating a hash value based on the raw features of a state, uses a feature projection function to generate a set of abstract features that result in higher locality, resulting in reduced communication overhead. We show that Abstract Zobrist hashing outperforms previous methods in search domains by using hand-coded, domain-specific feature projection functions. Then, we project GRAADA's projection based approach to determine the actual RAADA partition space *."}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of them are able to surpass themselves by going in search of themselves. (...) Most of them are not able to go in search of themselves. (...) Most of them are not able to go in search of themselves. (...) Most of them are not able to go in search of themselves. (...) Most of them are not able to go in search of themselves. (...) Most of them are not able to go out in search of themselves. (...) Most of them are not able to go out in search of themselves. (...) Most of them are not able to go out in search of themselves. (...) Most of them are not able to go out in search of themselves. (...) Most of themselves are not able to go out. (...) Most of others are not able to go out in search of themselves. (...)"}, {"heading": "2. Preliminaries and Background", "text": "In this section, we first define the three main categories of overheads that pose a parallel search challenge (Section 2.1), then examine parallel best-first search algorithms (Section 2.2) and review the HDA * framework (Section 2.3), and then review the two earlier approaches proposed for the HDA * framework: Zobrist hashing (Section 2.4) and abstraction (Section 2.5)."}, {"heading": "2.1 Parallel Overheads", "text": "Although an ideal best-first parallel search algorithm would achieve n-fold acceleration of n threads, multiple overheads can prevent linear acceleration.Communication Overhead (CO): 1 Communication Overhead refers to the cost of exchanging information between threads. In this essay, we define communication overhead as the ratio of nodes transferred to other threads: CO: = # Nodes sent to other threads # Nodes generated. CO is detrimental to performance due to delays due to message transmission (e.g. network communication), as well as access to data structures such as message queues. In general, CO increases with the number of threads. If nodes are randomly assigned to the threads, CO becomes proportional to 1 \u2212 1 # Thread. Search Overhead (SO): Parallel search usually expands more nodes than sequential A *."}, {"heading": "2.2 Parallel Best-First Search Algorithms", "text": "The key to success is that it is about a way in which one sees oneself in a position to surpass oneself by letting oneself in a way in which one puts oneself in the center. (...) In fact, it is not that one is able to put oneself in the center. (...) It is not that one is able to put oneself in the center. (...) It is not that one is able to put oneself in the center. (...) It is not that one puts oneself in the center. (...) It is not that one puts oneself in the center, that one puts oneself in the center. (...) It is not that one puts oneself in the center. (...) It is that one is not in oneself. (...) It is that one does not place oneself in the center. (...) It is that one does not place oneself in the center. (...) It is that one does not place oneself in the center."}, {"heading": "2.3 Hash Distributed A* (HDA*)", "text": "Hash Distributed A * (HDA *) (Kishimoto et al., 2013) is a parallel A * algorithm that takes up the idea of a hash-based work distribution of PRA * (Evett et al., 1995) and asynchronous communication of TDS (Romein et al., 1999). In HDA *, each processor has its own open / closed lists. A global hash function assigns a unique owner thread to each search node. Each thread T repeatedly performs the following: 1. T checks its message queue for new nodes. For each new node n in T's message queue, if it is not in the open list (no duplicate), place n in the open list. 2. Expect the node n with the highest priority in the open list. For each generated node c, calculate the hash value H (c) and send c to the thread that has H (c). Make the open list of HDA * features in the two of the open list."}, {"heading": "2.4 Zobrist Hashing (HDA\u2217[Z ]) and Operator-Based Zobrist Hashing (HDA\u2217[Zoperator ])", "text": "Since work distribution in HDA * is entirely determined by a global hash function, the choice of hash function is critical to its performance. Kishimoto et al. (2009, 2013) notes that it is desirable to use a hash function that is uniformly distributed among the processors, and uses the Zobrist hash function (1970) described below. Zobrist hash value of a state containing random bit strings (algorithm 2).Z (s) is calculated as follows: = R [x0] xor R [x1] xor \u00b7 xor R [xn] (1) Algorithm 1: HDA [Z] Input: s = (x0, xn) 1 hash."}, {"heading": "3. Analysis of Parallel Overheads in Multicore Best-First Search", "text": "As discussed in Section 2.1, there are three broad classes of parallel overheads in parallel search: overheads for search (SDA), overheads for communication (CO), and coordination (synchronization) overheads. Since state-of-the-art parallel search algorithms such as HDA * and PBNF have successfully eliminated coordination effort, the remaining overheads are SO and CO. Previous work focused on quantitative evaluation of SO because SO is fundamental overheads for the algorithm itself, whereas CO is due to the difficult-to-evaluate and-control machine environment. Thus, in this section, we first evaluate the SO of HDA work [Z] and the SafePBNF.Kishimoto et al. previously analyzed overheads for HDA search [Z] (2013), measuring R < R =, andR >, the share of extended nodes in f < f > that relate to the HDA search."}, {"heading": "3.1 Search Overhead and the Order of Node Expansion on Combinatorial Search", "text": "In fact, it is not as if it were a matter of a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and in which it is not in which it is not in which it is about which it is about a way in which it is about a way in which it is about which it is not in which it is not in which it is not in which it is not in which it is about which it is about which it is about which it is not in which it is about which it is about which it is about which it is not in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is not in which it is"}, {"heading": "3.1.1 BAND EFFECT", "text": "The order in which the states of HDA \u043c [Z] are widened is fairly consistent with sequential A *. However, there are some deviations from the strict A * order, within a \"band\" that is symmetrical around the strict A * order. For example, in Figure 2a we have highlighted a band that shows that the (approximately) 5,000th state widened by HDA * corresponds to a strict A * order between 4500-5500 (i.e., a bandwidth of approximately 1000 at this point in the search) The width of the band tends to increase the number of threads (see the bands bands in Figure 2a, 2b, 3a). Although the bandwidth tends to increase as the search progresses, the growth rate is relatively small. Also, the harder instance (i.e., the greater number of nodes widened by A *) is widened by the narrower tendencies of A *."}, {"heading": "3.1.2 BURST EFFECT", "text": "However, whether this is a problem that is not yet complete, it is possible that some threads will be extended into poor regions of the search space because good nodes have not yet been sent into their open lists of threads that have not yet completed their initialization. For example, n1 is assumed to be a child of the root node n0, and n1 has a significantly inferior f -value than other descendants of n0. Sequential A * will not expand until all nodes with lower f -values have been expanded. However, at the beginning of the search n1 is assigned a thread t1 whose queue q1 is empty, in which case the children of n1 may also have significantly inferior values."}, {"heading": "3.1.3 NODE REEXPANSIONS", "text": "With consistent heuristics, A * never expands a node once it is stored in the closed list, because the first time a node expands, it is guaranteed that we have reached that node through a cheapest path. However, the parallel best-first search may require nodes to expand again, even if they are in the closed list. In HDA *, for example, each processor in its local open list selects the best (lowest f-cost) node, but the selected node may not currently have the lowest f-value in the world. As a result, although HDA * tends to find the shortest paths to a node, the paths may not be the cheapest, and some nodes extended by a thread in HDA * may have been reached by a suboptimal path and need to be expanded later after being reached by a more cost-effective path."}, {"heading": "3.1.4 THE IMPACT OF WORK DISTRIBUTION METHOD ON THE ORDER OF NODE EXPANSION", "text": "In contrast to the original HDA * by Kishimoto et al. (2009), the Zobrist hashing, HDA, used [P] a perfect hashing scheme, which permutations (Tiles positions) leads to lexicographic indicators. (2009), the Zobrist hashing, HDA, HDA, HDA, HDA, HDA, HDA. (2009), the Zobrist hashing, HDA, HDA, HDA, HDA, HDA, HDA. (2009), the Zobrist hashing, HDA, HDA, HDA, HDA. (2009), the Zobrist hashing, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, [P, [P], HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, [P, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, HDA, H"}, {"heading": "3.3 The Effect of Communication Overhead on Speedup", "text": "Although the HDA system competes with abstraction-based methods (HDA-based methods) (HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-HDA-"}, {"heading": "4. Abstract Zobrist Hashing(AZH)", "text": "As we discussed in Section 3, both search and communication have overarching effects on the performance of HDA *, and methods that address only one of these overheads are insufficient. [HDA], which uses Zobrist hashing, assigns nodes evenly to the processors by trying to keep generated states at the same processor level as they were generated, but this results in significant search results because all productive searches can be performed on 1 node, while all other nodes are looking for unproductive nodes that would not be augmented by A *. Therefore, we need a more balanced approach that overlays both search and communication."}, {"heading": "4.1 Evaluation of Work Distribution Methods on Domain-Specific Solvers", "text": "We evaluated the performance of the following HDA * variants on several standard benchmark domains with different properties. \u2022 HDA * [Z, Afeature]: HDA * with AZH \u2022 HDA * [Z]: HDA * with Zobrist hashing (Kishimoto et al., 2009) \u2022 HDA * [P, Astate]: HDA * with Abstraction based work distribution (Burns et al., 2010) \u2022 HDA * with a perfect hash function (Burns et al., 2010) The experiments were executed on an Intel Xeon E5-2650 v2 2.60 GHz CPU with 128 GB of RAM, with up to 16 cores.The 15 puzzle experiments in Section 4.1.1 included improvements from the recent work of Burns et al., as. the code in Section 3.1 based on the code of Burns et al. (2010).The 15 puzzle experiments in Section 4.1.1 included improvements from the more recent work of Burns et al."}, {"heading": "4.1.1 15-PUZZLE", "text": "These are not the same cases as the 100 cases used in Section 3.1, because the solver used in this experiment was faster than the solver used in Section 3.16, and some of the instances used in Section 3.1 were too simple to evaluate parallel efficiency. 7 We selected cases that were sufficiently difficult to avoid the results dominated by the initial burst effect startup overhead (Section 3.1.2) - sequential A * took an average of 52.3 seconds to resolve these instances. In addition to these instances, we have [Z, Afeature], HDA and HDA. We also evaluated SafePBNF (Burns et al., 2010) and HDA. The projections A (xi) (abstract features) we used for AZH in HDA."}, {"heading": "4.1.2 24-PUZZLE", "text": "For the same reason as with the 15 puzzle experiments in Section 4.1.1 above, they differ from the 24 puzzle instances used in Section 3.2. we chose the most difficult instances to solve given the memory limitation (128GB).The average runtime of sequential A * in these instances was 219.0 seconds.The average solving length of our 24 puzzle instances was 92.9 (the average solving length in the work of Korf and Felner (2002) was 100.8).We used a disjoint pattern database heuristician (Korf & Felner, 2002).The disjoint pattern heuristician database heuristician is much more efficient than Manhattan distance, so the average wall time of 24 puzzles with disjoint pattern database heuristician H1 is much faster than that of 15 puzzle Z."}, {"heading": "4.1.3 MULTIPLE SEQUENCE ALIGNMENT", "text": "We have used 60 benchmark instances that consist of 10 actual amino acid sequences of BAliBASE instances that we have used are: BB12021, BB12036, BBS11010, BBS11026, BBS11035, BBS11037, BBS12016, BBS12032, BBS12032, BBS12032, BBS12032, BBS12032."}, {"heading": "4.2 Automated, Domain Independent Abstract Feature Generation", "text": "In Section 4.1, we evaluated handmade, domain-specific feature projection functions for instances of the HDA * framework (HDA \u0445 [Z], HDA \u043d [P], HDA \u0445 [P, Astate], HDA \u0445 [Z, Afeature]) and showed that AZH surpasses previous methods. Next, we will turn to fully automated, domain-independent methods for generating feature projection functions that can be used when a formal model of a domain (such as PDDL / SAS + for classical planning) is available. From now on, we will discuss domain-independent methods for distributing work. Table 5 summarizes the previously proposed methods and their abbreviations. For HDA *, automated domain-independent feature generation for classical planning problems that we have represented in the SAS + representation (Ba \ufffd ckstro \ufffd m & Nebbel, 1995) is straightforward."}, {"heading": "4.2.1 GREEDY ABSTRACT FEATURE GENERATION (GAZHDA*)", "text": "The greedy abstract feature generation (GreedyAFG) is a simple, domain-independent feature generation method that divides each feature into 2 abstract features (Jinnai & Fukunaga, 2016a).GreedyAFG initially identifies atomic groups (Edelkamp, 2001) and their domain transition graph (DTG).The Atom group is a set of mutually exclusive suggestions from which exactly one of each attainable state applies, e.g. the values of an SAS + multi-value variable (Ba \ufffd ckstro \ufffd m & Nebula, 1995).GreedyAFG classifies each atomic group X into 2 abstract features S1 and S2, based on the undirected DTG (nodes are values, edges are transitions), as follows: (1) assigns the minimum degree node (nodes with the least number of edges between other nodes) S1; snodes 2 snodes S1; (2) greedily add unweighted nodes to the S1."}, {"heading": "4.2.2 FLUENCY-DEPENDENT ABSTRACT FEATURE GENERATION (FAZHDA*)", "text": "Because the hash value of the state changes when the value of an abstract attribute changes, GreedyAFG does not prevent high CO if an abstract attribute changes its value very frequently, for example in the block domain, each operator in the domain changes the value of the SAS + variable representing the state of the robot hand. Fluctuation of a variable v is the number of ground actions that change the value of v divided by the total number of basic actions of the problem. By ignoring variables with high fluctuation, FluencyAFG has been shown to be quite successful in reducing CO and increasing acceleration compared to GreedyAFG.One problem with fluctuation is that within the AZHDA * framework CO is associated with a change in the value of an abstract attribute, not with the attribute itself."}, {"heading": "5. A Graph Partitioning-Based Model for Work Distribution", "text": "Although GAZHDA * and FAZHDA *, the domain-independent methods for generating abstract characteristics discussed in Section 4.2, aim to reduce the communication effort compared to HDA \u0445 [Z], they are not based on an explicit model that enables the prediction of the actual communication effort achieved during the search. Furthermore, the impact of these methods on the search effort is completely unclear, and therefore it is not possible to predict the parallel efficiency achieved during the search. Previous work relied on ad hoc coordination of control parameters to achieve good performance (Jinnai & Fukunaga, 2016b). In this section we will first show that a work distribution method can be modelled as a division of the search space diagram, and that communication effort and load balancing can be understood as the number of intersections and balance of the partition. With this model, we introduce a metric, estimated efficiency, and experimentally demonstrate that efficiency has a strong correlation to actual correlation."}, {"heading": "5.1 Work Distribution as Graph Partitioning", "text": "The goal is to design a work distribution method that maximizes efficiency by reducing CO, SO and load balance (LB). We propose an approach that is based on optimizing a priority estimate of CO, SO and LB. However, in our approach we seek a space of hash functions by using these estimates of CO, SO, LB as the basis for a (cheap) evaluation function for this search in the space of hash functions. To make this possible, we first develop a model for estimating the performance of algorithms based on the notion of a workload graph. To guarantee the optimality of a solution, a parallel search method must be used to expand a target node and all nodes with f < f (relevant nodes) Searching for a workload graph."}, {"heading": "5.2 Parallel Efficiency and Graph Partitioning", "text": "In this section, we develop a metaphor for assessing the effectiveness of nodes; secondly, we need to increase the effectiveness of nodes in terms of the effectiveness of nodes; secondly, we need to increase the effectiveness of nodes in terms of the effectiveness of nodes in terms of the effectiveness of nodes; and thirdly, we need to focus on the effectiveness of nodes in terms of the effectiveness of nodes; secondly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fourthly, fourth-, fourth-, fourth-, fourth-, fourth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fifth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, fourth-, and, fourth-, and, fourth-, we need, we need to, we need to, we need to assess the effectiveness, we need to assess the effectiveness, the effectiveness of the effectiveness of, the effectiveness of, the effectiveness of the effectiveness of nodes; secondly, the effectiveness of nodes; secondly; secondly, we need to, we need to increase, we need to increase the effectiveness of the effectiveness of"}, {"heading": "6. Graph Partitioning-Based Abstract Feature Generation (GRAZHDA*)", "text": "A standard approach to workload distribution in parallel scientific calculation is a workload problem where the workload is plotted as a graph, and partitioning the graph by a specific target (usually the cut-edge ratio is metric) represents the distribution of workload among processors (Hendrickson & Kolda, 2000; Buluc, Meyerhenke, Safro, Sanders, & Schulz, 2015).In Section 5, we showed that workload distributions for parallel search on an implicit graph can be modeled as partitions of a workload chart that is isomorphic to the search space, and that this workload chart can be used to estimate the CO and LB of a work distribution. If we were given a workload graph, then by defining a graph-cut target such as partitioning the nodes in the search space (f) to maximize efficiency, we would have a method of generating a more optimal workload distribution."}, {"heading": "6.1 Previous Methods and Their Relationship to GRAZHDA*", "text": "In this section, we show that previously proposed methods for the HDA framework are theoretically always possible if they can be considered examples of GRAZHDA's *. First, we define a DTG division as follows: Given that the DTG division into an abstract state (A0 [v0], A1 [v1]), where Ai [vi] is defined by a graph that ignores the objective function of a state while optimizing objective functions. However, DTG division corresponds to AF / DTG for an abstraction strategy. Then, in order to model non-DTG-based methods, we refer to all other methods that map a state space to an abstract 11. In HDA *, the owner of a state is compiled as a processor (s) hash value (s) mod p, so it is possible to assign states with different hash values."}, {"heading": "6.2 Effective Objective Functions for GRAZHDA*", "text": "In the previous section, we have shown that earlier variants of HDA * can be considered examples of GRAZHDA * that divided the workload chart based on ad hoc criteria. However, since the GRAZHDA * framework formulates the workload distribution as a problem of graphical distribution, it is a natural idea to design an objective function for the distribution that leads directly to a desired trade-off between search and communication costs, resulting in good overall efficiency. Fortunately, there is a metric available that can be used as a basis for such a goal: effesti. In Section 5.2.1, we have shown that effesti, based on the workload, is an effective predictor of the actual efficiency of a work distribution strategy. In this section, we propose approximations of effesti that can be used as objective functions for the DTG distribution in GRAZHDA *. In principle, to maximize the workload performance of ZRAHDA *, it is only undesirable to have a result of ZRAHDA *."}, {"heading": "6.2.1 SPARSEST CUT OBJECTIVE FUNCTION (GRAZHDA*/SPARSITY)", "text": "This year, there is less than a year to go before an agreement can be reached."}, {"heading": "6.2.3 PARTITIONING THE DTGS", "text": "Given an objective function such as Sparsity, GRAZHDA * divides each DDG into two abstract features, as in Section 6 above. Since each domain transition curve typically has less than 10 nodes, we calculate the optimal partition for both objective functions using a simple, depth-oriented branch-and-bound method. It is possible that branch-and-bound becomes impractical when a domain has very large DTGs, or we develop a more complicated objective function for partitioning the DTGs. In such cases, we can use heuristic partitioning methods such as the FM algorithm (Fiduccia & Mattheyses, 1982), but so far, branch-and-bound has sufficed - in all standard IPC benchmark domains we evaluate, the process of generating abstract features (which includes partitioning all DTGs) takes less than 4 seconds (most cases last 1 second) for each instance tested."}, {"heading": "6.3 Evaluation of Automated, Domain-Independent Work Distribution Methods", "text": "In addition to the methods in Section 5.2.1, we evaluated the performance of GRAZHDA * / sparsity. We used CGL-B (CausalGraph-Goal-Level & Bisimulation) to merge and shrink heuristic methods (Helmert et al., 2014) that are more efficient than LFPA-Merge & shrink (Helmert et al., 2007) used in a previous conference paper, GAZHDA * and FAZHDA * (Jinnai & Fukunaga, 2016b), for example in Block10-1, CGL-B expands 11,065,451 nodes, while LFPA expands 51,781,104 nodes. We set the abstraction size for Merge & shrink to 1000. Choosing the heurists influences the behavior of parallel searching when the hayrists have a different expansion rate because they affect the relative costs of communication."}, {"heading": "6.3.1 THE EFFECT OF THE NUMBER OF CORES ON SPEEDUP", "text": "Figure 18 shows the acceleration of the algorithms, when the number of cores increased from 8 to 48. GRAZHDA * / sparseness consistently outperformed the other methods. The performance gap between the better methods (GRAZHDA * / sparseness, FAZHDA *, OZHDA *, DAHDA *) and the baseline value ZHDA * widens with the number of cores. Because as the number of cores increases, the communication effort increases with the number of cores, and our new work distribution method successfully reduces the communication effort."}, {"heading": "6.3.2 CLOUD ENVIRONMENT RESULTS", "text": "In addition to the 48-core cluster, we evaluated GRAZHDA * / sparsity on an Amazon EC2 cloud cluster with 128 virtual cores (vCPUs) and 480 GB of aggregated RAM (a cluster with 32 m1.xlarge EC2 instances, each with 4 vCPUs, 3.75 GB RAM / core).This is a less favorable environment for parallel searching compared to a bare metal cluster, since physical processors are shared with other users and network performance is inconsistent (Iosup, Ostermann, Yigitbasi, Prodan, Fahringer, & Epema, 2011).We deliberately chose this configuration to evaluate work distribution methods in an environment that is significantly different from our other experiments."}, {"heading": "6.3.3 24-PUZZLE EXPERIMENTS", "text": "We examined GRAZHDA * / Sparsity on the 24 puzzle with the same configuration as Section 4.1.2. Abstract feature generated by GRAZHDA * / Sparsity is shown in Figure 19d. We compared GRAZHDA * / Sparsity (automated abstract feature generation) vs. AZHDA * with manual work distribution (HDA * [Z, Afeature]) (Figure 8d) and HDA * [Z]. For 8 cores, the acceleration was 7.84 (GRAZHDA * / Sparsity), 7.85 (HDA * [Z, Afeature]) and 5.95 (HDA \u0445 [Z]). Thus, the fully automated GRAZHDA * / Sparsity competes with a carefully hand-crafted work distribution method. For the 15 puzzle, the partition generated by GRAZHDA * / Sparsity is exactly the handmade hash function of Figure 8b, so the performance is identical."}, {"heading": "6.3.4 EVALUATION OF PARALLEL SEARCH OVERHEADS AND PERFORMANCE IN LOW COMMUNICATIONS-COST ENVIRONMENTS", "text": "In recent years, the number of those who are able to survive has multiplied. (...) In recent years, the number of those who are able to survive has doubled. (...) The number of those who are able to survive has multiplied. (...) The number of those who are able to survive is very high. (...) The number of those who are able to survive themselves is very high. (...) The number of those who are able to survive themselves is very high. (...) The number of those who are able to survive themselves is very low. (...) The number of those who are able to survive themselves is low. (...) The number of those who are able to survive themselves is low. (...) The number of those who are able to survive themselves is very low. (...) The number of those who are able to survive themselves is very low."}, {"heading": "7. Conclusions", "text": "We investigated the distribution methods for HDA *, and showed that previous methods suffered from high communication costs (HDA * Z), high distribution costs (HDA * P, Astate]), or both (HDA * P]), which limited their efficiency. We suggested that a new distribution method that combines the strengths of Zobrist hashing and abstraction, and that it achieves a successful trade-off between communication and search results, which leads to better performance than previous work distribution methods with handmade abstract skills. We then expanded the investigation to automated, domain-independent approaches to generated work distribution. We formulated work distribution as Graphical Partitioning and validated effects."}, {"heading": "Appendix A. Dynamic AHDA* (DAHDA*), an improvement to AHDA* for distributed memory systems", "text": "This section represents an improvement on AHDA * (Burns et al., 2010), while we used AHDA * as one of the bases for evaluating our new AZHDA * Strategies.The baseline implementation of AHDA * (Z, Astate / SDD) is based on the greedy abstraction algorithm described in (Zhou & Hansen, 2006b) and selects a subset of DTGs (atomic groups).The greedy abstraction algorithm adds to the abstract graph (G) at a time when the selection of DTG that minimizes the maximum degree of delimitation of the abstract graph until the graph of the nodes is reached."}, {"heading": "Appendix B. Experimental results with standard deviations", "text": "0.000 (0.000) 0,00 (0.000) 0,00 (0.000) 0,00 (0.000) 0,00 (0.000) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.000) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.000) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) 0,00 (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.001) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006 (0.006) (0.006) (0.006) (0.006) (0.006 (0.006) (0.006) (0.006) (0.006) (0.006) (0.006 (0.006) (0.006) (0.006 (0.006) (0.006) (0.006) (0.006) (0.006) (0.006) (0.006 (0.006) (0.006) (0.006) (0.006 (0.006) (0.006) (0.0"}], "references": [{"title": "Tiebreaking strategies for a* search: How to explore the final frontier", "author": ["M. Asai", "A. Fukunaga"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Asai and Fukunaga,? \\Q2016\\E", "shortCiteRegEx": "Asai and Fukunaga", "year": 2016}, {"title": "Complexity results for SAS+ planning", "author": ["C. B\u00e4ckstr\u00f6m", "B. Nebel"], "venue": "Computational Intelligence,", "citeRegEx": "B\u00e4ckstr\u00f6m and Nebel,? \\Q1995\\E", "shortCiteRegEx": "B\u00e4ckstr\u00f6m and Nebel", "year": 1995}, {"title": "Recent advances in graph partitioning", "author": ["A. Buluc", "H. Meyerhenke", "I. Safro", "P. Sanders", "C. Schulz"], "venue": "arXiv preprint arXiv:1311.3144", "citeRegEx": "Buluc et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Buluc et al\\.", "year": 2015}, {"title": "Best-first heuristic search for multicore machines", "author": ["E. Burns", "S. Lemons", "W. Ruml", "R. Zhou"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Burns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Burns et al\\.", "year": 2010}, {"title": "Implementing fast heuristic search code", "author": ["E.A. Burns", "M. Hatem", "M.J. Leighton", "W. Ruml"], "venue": "In Proceedings of the Annual Symposium on Combinatorial Search,", "citeRegEx": "Burns et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Burns et al\\.", "year": 2012}, {"title": "Planning with pattern databases", "author": ["S. Edelkamp"], "venue": "In European Conference on Planning (ECP),", "citeRegEx": "Edelkamp,? \\Q2001\\E", "shortCiteRegEx": "Edelkamp", "year": 2001}, {"title": "A scalable concurrent malloc (3) implementation for FreeBSD", "author": ["J. Evans"], "venue": "In Proc. BSDCan Conference", "citeRegEx": "Evans,? \\Q2006\\E", "shortCiteRegEx": "Evans", "year": 2006}, {"title": "PRA*: Massively parallel heuristic search", "author": ["M. Evett", "J. Hendler", "A. Mahanti", "D. Nau"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "Evett et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Evett et al\\.", "year": 1995}, {"title": "A linear-time heuristic for improving network partitions", "author": ["C.M. Fiduccia", "R.M. Mattheyses"], "venue": "In Conference on Design Automation,", "citeRegEx": "Fiduccia and Mattheyses,? \\Q1982\\E", "shortCiteRegEx": "Fiduccia and Mattheyses", "year": 1982}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P.E. Hart", "N.J. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "Admissible heuristics for optimal planning", "author": ["P. Haslum", "H. Geffner"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Haslum and Geffner,? \\Q2000\\E", "shortCiteRegEx": "Haslum and Geffner", "year": 2000}, {"title": "The Fast Downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Helmert,? \\Q2006\\E", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Helmert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2007}, {"title": "Merge-and-shrink abstraction: A method for generating lower bounds in factored state spaces", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann", "R. Nissim"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Helmert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2014}, {"title": "Graph partitioning models for parallel computing", "author": ["B. Hendrickson", "T.G. Kolda"], "venue": "Parallel computing,", "citeRegEx": "Hendrickson and Kolda,? \\Q2000\\E", "shortCiteRegEx": "Hendrickson and Kolda", "year": 2000}, {"title": "A stack-slicing algorithm for multi-core model checking", "author": ["G.J. Holzmann"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "Holzmann,? \\Q2008\\E", "shortCiteRegEx": "Holzmann", "year": 2008}, {"title": "The design of a multicore extension of the SPIN model checker", "author": ["G.J. Holzmann", "D. Bo\u015dna\u0109ki"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "Holzmann and Bo\u015dna\u0109ki,? \\Q2007\\E", "shortCiteRegEx": "Holzmann and Bo\u015dna\u0109ki", "year": 2007}, {"title": "On a practical, integer-linear programming model for delete-free tasks and its use as a heuristic for cost-optimal planning", "author": ["T. Imai", "A. Fukunaga"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Imai and Fukunaga,? \\Q2015\\E", "shortCiteRegEx": "Imai and Fukunaga", "year": 2015}, {"title": "Performance analysis of cloud computing services for many-tasks scientific computing", "author": ["A. Iosup", "S. Ostermann", "M.N. Yigitbasi", "R. Prodan", "T. Fahringer", "D.H. Epema"], "venue": "IEEE Transactions on Parallel and Distributed Systems,", "citeRegEx": "Iosup et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Iosup et al\\.", "year": 2011}, {"title": "Parallel A* and AO* algorithms: An optimality criterion and performance evaluation", "author": ["K. Irani", "Y. Shih"], "venue": "In International Conference on Parallel Processing,", "citeRegEx": "Irani and Shih,? \\Q1986\\E", "shortCiteRegEx": "Irani and Shih", "year": 1986}, {"title": "Parallel external directed model checking with linear I/O. In Verification, Model Checking, and Abstract Interpretation", "author": ["S. Jabbar", "S. Edelkamp"], "venue": "7th International Conference,", "citeRegEx": "Jabbar and Edelkamp,? \\Q2006\\E", "shortCiteRegEx": "Jabbar and Edelkamp", "year": 2006}, {"title": "Abstract Zobrist hash: An efficient work distribution method for parallel best-first search", "author": ["Y. Jinnai", "A. Fukunaga"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Jinnai and Fukunaga,? \\Q2016\\E", "shortCiteRegEx": "Jinnai and Fukunaga", "year": 2016}, {"title": "Automated creation of efficient work distribution functions for parallel best-first search", "author": ["Y. Jinnai", "A. Fukunaga"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Jinnai and Fukunaga,? \\Q2016\\E", "shortCiteRegEx": "Jinnai and Fukunaga", "year": 2016}, {"title": "State-variable planning under structural restrictions: Algorithms and complexity", "author": ["P. Jonsson", "C. B\u00e4ckstr\u00f6m"], "venue": "Artificial Intelligence,", "citeRegEx": "Jonsson and B\u00e4ckstr\u00f6m,? \\Q1998\\E", "shortCiteRegEx": "Jonsson and B\u00e4ckstr\u00f6m", "year": 1998}, {"title": "Measuring and understanding throughput of network topologies", "author": ["S.A. Jyothi", "A. Singla", "P. Godfrey", "A. Kolla"], "venue": "arXiv preprint arXiv:1402.2531", "citeRegEx": "Jyothi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jyothi et al\\.", "year": 2014}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on scientific Computing,", "citeRegEx": "Karypis and Kumar,? \\Q1998\\E", "shortCiteRegEx": "Karypis and Kumar", "year": 1998}, {"title": "Evaluation of a simple, scalable, parallel bestfirst search strategy", "author": ["A. Kishimoto", "A. Fukunaga", "A. Botea"], "venue": "Artificial Intelligence,", "citeRegEx": "Kishimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kishimoto et al\\.", "year": 2013}, {"title": "Scalable, parallel best-first search for optimal sequential planning", "author": ["A. Kishimoto", "A.S. Fukunaga", "A. Botea"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Kishimoto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kishimoto et al\\.", "year": 2009}, {"title": "Evaluations of Hash Distributed A* in optimal sequence alignment", "author": ["Y. Kobayashi", "A. Kishimoto", "O. Watanabe"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Kobayashi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2011}, {"title": "Depth-first iterative deepening: An optimal admissible tree search", "author": ["R. Korf"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf,? \\Q1985\\E", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Disjoint pattern database heuristics", "author": ["R.E. Korf", "A. Felner"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf and Felner,? \\Q2002\\E", "shortCiteRegEx": "Korf and Felner", "year": 2002}, {"title": "Large-scale parallel breadth-first search", "author": ["R.E. Korf", "P. Schultze"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Korf and Schultze,? \\Q2005\\E", "shortCiteRegEx": "Korf and Schultze", "year": 2005}, {"title": "Parallel best-first search of state-space graphs: A summary of results", "author": ["V. Kumar", "K. Ramesh", "V.N. Rao"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Kumar et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 1988}, {"title": "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms", "author": ["T. Leighton", "S. Rao"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Leighton and Rao,? \\Q1999\\E", "shortCiteRegEx": "Leighton and Rao", "year": 1999}, {"title": "Scalable global and local hashing strategies for duplicate pruning in parallel A* graph search", "author": ["N.R. Mahapatra", "S. Dutt"], "venue": "IEEE Transactions on Parallel and Distributed Systems,", "citeRegEx": "Mahapatra and Dutt,? \\Q1997\\E", "shortCiteRegEx": "Mahapatra and Dutt", "year": 1997}, {"title": "Sequential and parallel algorithms for frontier A* with delayed duplicate detection", "author": ["R. Niewiadomski", "J.N. Amaral", "R.C. Holte"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Niewiadomski et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Niewiadomski et al\\.", "year": 2006}, {"title": "Heuristics - Intelligent Search Strategies for Computer Problem Solving. Addison\u2013 Wesley", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1984\\E", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "Rapid and sensitive sequence comparison with FASTP and FASTA", "author": ["W.R. Pearson"], "venue": "Methods in enzymology,", "citeRegEx": "Pearson,? \\Q1990\\E", "shortCiteRegEx": "Pearson", "year": 1990}, {"title": "PA*SE: Parallel A* for slow expansions", "author": ["M. Phillips", "M. Likhachev", "S. Koenig"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Phillips et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Phillips et al\\.", "year": 2014}, {"title": "LP-based heuristics for costoptimal planning", "author": ["F. Pommerening", "G. R\u00f6ger", "M. Helmert", "B. Bonet"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Pommerening et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pommerening et al\\.", "year": 2014}, {"title": "Transposition table driven work scheduling in distributed search", "author": ["J.W. Romein", "A. Plaat", "H.E. Bal", "J. Schaeffer"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Romein et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Romein et al\\.", "year": 1999}, {"title": "BAliBASE 3.0: Latest developments of the multiple sequence alignment benchmark. Proteins: Structure", "author": ["J.D. Thompson", "P. Koehl", "R. Ripp", "O. Poch"], "venue": "Function and Genetics (PROTEINS),", "citeRegEx": "Thompson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Thompson et al\\.", "year": 2005}, {"title": "Parallel AI planning on the SCC", "author": ["V. Vidal", "S. Vernhes", "G. Infantes"], "venue": "In 4th Many-core Applications Research Community (MARC) Symposium,", "citeRegEx": "Vidal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2012}, {"title": "Structured duplicate detection in external-memory graph search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Hansen,? \\Q2004\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2004}, {"title": "Breadth-first heuristic search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Artificial Intelligence,", "citeRegEx": "Zhou and Hansen,? \\Q2006\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2006}, {"title": "Domain-independent structured duplicate detection", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Hansen,? \\Q2006\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2006}, {"title": "Parallel structured duplicate detection", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Hansen,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2007}, {"title": "Dynamic state-space partitioning in external-memory graph search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Zhou and Hansen,? \\Q2011\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2011}, {"title": "Massively parallel A* search on a GPU", "author": ["Y. Zhou", "J. Zeng"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Zeng,? \\Q2015\\E", "shortCiteRegEx": "Zhou and Zeng", "year": 2015}, {"title": "A new hashing method with application for game playing", "author": ["YUU", "JINNAI", "ALEX", "A.L. FUKUNAGA Zobrist"], "venue": "reprinted in International Computer Chess Association Journal (ICCA),", "citeRegEx": "YUU et al\\.,? \\Q1970\\E", "shortCiteRegEx": "YUU et al\\.", "year": 1970}], "referenceMentions": [{"referenceID": 3, "context": "Zobrist hashing incurs a heavy communication overhead because many nodes are assigned to processes that are different from their parents, and proposed AHDA*, which used an abstraction-based hash function originally designed for use with PSDD (Zhou & Hansen, 2007) and PBNF (Burns et al., 2010).", "startOffset": 273, "endOffset": 293}, {"referenceID": 3, "context": "Finally, Table 10 in Appendix A shows new experimental results comparing DAHDA* vs AHDA* (Burns et al., 2010) which were not included in the conference paper which introduced DAHDA* (Jinnai & Fukunaga, 2016b).", "startOffset": 89, "endOffset": 109}, {"referenceID": 3, "context": "Even when a parallel search itself does not require synchronization, coordination overhead can be incurred due to contention for the memory bus (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 144, "endOffset": 188}, {"referenceID": 26, "context": "Even when a parallel search itself does not require synchronization, coordination overhead can be incurred due to contention for the memory bus (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 144, "endOffset": 188}, {"referenceID": 3, "context": "However, concurrent access to the shared open list becomes a bottleneck and inherently limits the scalability of this approach unless the cost of expanding each node is extremely expensive, even if lock-free data structures are used (Burns et al., 2010).", "startOffset": 233, "endOffset": 253}, {"referenceID": 32, "context": "The simplest approach is a randomized strategy which sends generated states to a randomly selected neighbor processes (Kumar et al., 1988).", "startOffset": 118, "endOffset": 138}, {"referenceID": 3, "context": "However, synchronous node sending was shown to degrade performance on domains with fast node expansion, such as grid pathfinding and sliding-tile puzzle (Burns et al., 2010).", "startOffset": 153, "endOffset": 173}, {"referenceID": 36, "context": "While the use of abstractions as the basis for heuristic functions has a long history (Pearl, 1984), the use of abstractions as a mechanism for partitioning search states originated in Structured Duplicate Detection (SDD), an external memory search which stores explored states on disk (Zhou & Hansen, 2004).", "startOffset": 86, "endOffset": 99}, {"referenceID": 3, "context": "first heuristic search (Zhou & Hansen, 2006a), Parallel Best-NBlocks First (PBNF) (Burns et al., 2010) extends PSDD to best-first search on multicore machine by ensuring that n-blocks with the best current f -values are assigned to processors.", "startOffset": 82, "endOffset": 102}, {"referenceID": 15, "context": "Stack-slicing projects states to their path costs to achieve efficient communication in depth-first search (Holzmann, 2008), and is useful in domains with levelled graphs, where each state can be reached only by a unique path cost, such as model checking (Holzmann & Bo\u015dna\u0109ki, 2007) (thus enabling dupicate detection).", "startOffset": 107, "endOffset": 123}, {"referenceID": 3, "context": "first heuristic search (Zhou & Hansen, 2006a), Parallel Best-NBlocks First (PBNF) (Burns et al., 2010) extends PSDD to best-first search on multicore machine by ensuring that n-blocks with the best current f -values are assigned to processors. Since livelock is possible in PBNF on domains with infinite state spaces, Burns et al proposed SafePBNF, a livelock-free version of PBNF (2010). Burns et al (2010) also proposed AHDA*, a variant of HDA* which uses an abstraction-based node distribution function.", "startOffset": 83, "endOffset": 388}, {"referenceID": 3, "context": "first heuristic search (Zhou & Hansen, 2006a), Parallel Best-NBlocks First (PBNF) (Burns et al., 2010) extends PSDD to best-first search on multicore machine by ensuring that n-blocks with the best current f -values are assigned to processors. Since livelock is possible in PBNF on domains with infinite state spaces, Burns et al proposed SafePBNF, a livelock-free version of PBNF (2010). Burns et al (2010) also proposed AHDA*, a variant of HDA* which uses an abstraction-based node distribution function.", "startOffset": 83, "endOffset": 408}, {"referenceID": 26, "context": "Hash Distributed A* (HDA*) (Kishimoto et al., 2013) is a parallel A* algorithm which incorporates the idea of hash-based work distribution from PRA* (Evett et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 7, "context": ", 2013) is a parallel A* algorithm which incorporates the idea of hash-based work distribution from PRA* (Evett et al., 1995) and asynchronous communication from TDS (Romein et al.", "startOffset": 105, "endOffset": 125}, {"referenceID": 40, "context": ", 1995) and asynchronous communication from TDS (Romein et al., 1999).", "startOffset": 48, "endOffset": 69}, {"referenceID": 26, "context": "Kishimoto et al. (2009, 2013) noted that it was desirable to use a hash function which uniformly distributed nodes among processors, and used the Zobrist hash function (1970), described below.", "startOffset": 0, "endOffset": 175}, {"referenceID": 21, "context": "Although Jinnai and Fukunaga showed that OZHDA* reduces communication overhead compared to Zobrist hashing (2016b), it may result in increased search overhead compared to HDA\u2217[Z ](the extent of which is unpredictable).", "startOffset": 9, "endOffset": 115}, {"referenceID": 3, "context": "The abstraction strategy in AHDA* applies the state space partitioning technique used in PBNF (Burns et al., 2010) and PSDD (Zhou & Hansen, 2007), which projects nodes in the state space to abstract states.", "startOffset": 94, "endOffset": 114}, {"referenceID": 3, "context": "In order to minimize communication overhead in HDA*, Burns et al. (2010) proposed AHDA*, which uses abstraction based node assignment.", "startOffset": 53, "endOffset": 73}, {"referenceID": 3, "context": "In order to minimize communication overhead in HDA*, Burns et al. (2010) proposed AHDA*, which uses abstraction based node assignment. The abstraction strategy in AHDA* applies the state space partitioning technique used in PBNF (Burns et al., 2010) and PSDD (Zhou & Hansen, 2007), which projects nodes in the state space to abstract states. After mapping states to abstract states, the AHDA* implementation by Burns et al. (2010) assigns abstract states to processors using a perfect hashing and a modulus operator.", "startOffset": 53, "endOffset": 431}, {"referenceID": 3, "context": "The AHDA* implementation by Burns et al. (2010) implemented the hashing strategy using a perfect hashing and a modulus operator, and an abstraction strategy following the construction for SDD (Zhou & Hansen, 2006b) (for domain-independent planning), or a hand-crafted abstraction (for the sliding tiles puzzle and grid path-finding domains).", "startOffset": 28, "endOffset": 48}, {"referenceID": 3, "context": "The AHDA* implementation by Burns et al. (2010) implemented the hashing strategy using a perfect hashing and a modulus operator, and an abstraction strategy following the construction for SDD (Zhou & Hansen, 2006b) (for domain-independent planning), or a hand-crafted abstraction (for the sliding tiles puzzle and grid path-finding domains). Note that an abstraction strategy can itself be seen as a type of hashing strategy, but in this paper, we make the distinction between the method used to project states onto some cluster of states (abstraction) and methods which are used to map states (or abstract states) to processors (hashing). Jinnai and Fukunaga (2016b) showed that AHDA* with a static Nmax threshold performed poorly for a benchmark set with varying difficulty because a fixed size abstract graph results in very poor load balance, and implemented Dynamic AHDA* (DAHDA*) which dynamically sets the size of the abstract graph according to the number of features (the state space size is exponential in the number of features).", "startOffset": 28, "endOffset": 668}, {"referenceID": 3, "context": "For example, we denote AHDA* (Burns et al., 2010) using a perfect hashing and a hand-crafted abstraction as HDA\u2217[P ,Astate ], and AHDA* using a perfect hashing and a SDD abstraction as HDA\u2217[P ,Astate/SDD ].", "startOffset": 29, "endOffset": 49}, {"referenceID": 26, "context": "Kishimoto et al. previously analyzed search overhead for HDA\u2217[Z ] (2013). They measuredR<, R=, andR>, the fraction of expanded nodes with f < f\u2217, f = f\u2217, and f > f\u2217 (where f\u2217 is optimal cost), respectively.", "startOffset": 0, "endOffset": 73}, {"referenceID": 27, "context": "4] (Kishimoto et al., 2009)", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "4] (Burns et al., 2010) HDA\u2217[P ,Astate ] AHDA* with perfect hashing and state-based abstraction [Sec 2.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "5] (Burns et al., 2010)", "startOffset": 3, "endOffset": 23}, {"referenceID": 28, "context": "3) (Kobayashi et al., 2011)", "startOffset": 3, "endOffset": 27}, {"referenceID": 27, "context": "2] (Kishimoto et al., 2009)", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "4] trivial variant of HDA\u2217[P ,Astate/SDD ], which was ussed for classical planning in (Burns et al., 2010); uses Zobristbased hashing instead of perfect hashing.", "startOffset": 86, "endOffset": 106}, {"referenceID": 3, "context": "Burns et al. analyzed the quality of nodes expanded by SafePBNF and HDA\u2217[P ,Astate ] by comparing the number of nodes expanded according to their f values, and showed that HDA\u2217[P ,Astate ] expands nodes with larger f value (lower quality nodes) compared to SafePBNF (2010).", "startOffset": 0, "endOffset": 273}, {"referenceID": 3, "context": "In addition, previous work has not directly compared HDA\u2217[Z ] and SafePBNF, as Burns et al. (2010) compared SafePBNF to \u2014HDA\u2217[P ,Astate ]", "startOffset": 79, "endOffset": 99}, {"referenceID": 9, "context": "Although the traditional definition of A* (Hart et al., 1968) specifies that nodes are expanded in order of nondecreasing f -value (i.", "startOffset": 42, "endOffset": 61}, {"referenceID": 3, "context": "33 GHz CPU with 16 GB RAM, using a 15-puzzle solver based on the solver code used in the work of Burns et al. (2010). We recorded the order in which states were expanded.", "startOffset": 97, "endOffset": 117}, {"referenceID": 3, "context": "This requires communication and coordination overhead, which increases the walltime by about <10% of the time on the 15-puzzle (Burns et al., 2010).", "startOffset": 127, "endOffset": 147}, {"referenceID": 28, "context": "(Kobayashi et al., 2011) analyzed node reexpansion on multiple sequence alignment which HDA\u2217[Z ] suffers from high node duplication rate.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "For SafePBNF, we used the configuration used in (Burns et al., 2010).", "startOffset": 48, "endOffset": 68}, {"referenceID": 3, "context": "HDA\u2217[P ] is an instance of HDA* which is called \u201cHDA*\u201d in the work of Burns et al. (2010). Unlike the original HDA* by Kishimoto et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 3, "context": "HDA\u2217[P ] is an instance of HDA* which is called \u201cHDA*\u201d in the work of Burns et al. (2010). Unlike the original HDA* by Kishimoto et al. (2009), which uses Zobrist hashing, HDA\u2217[P ] uses a perfect hashing scheme which maps permutations (tile positions) to lexicographic indices (thread IDs) by Korf and Schultze (2005).", "startOffset": 70, "endOffset": 143}, {"referenceID": 3, "context": "HDA\u2217[P ] is an instance of HDA* which is called \u201cHDA*\u201d in the work of Burns et al. (2010). Unlike the original HDA* by Kishimoto et al. (2009), which uses Zobrist hashing, HDA\u2217[P ] uses a perfect hashing scheme which maps permutations (tile positions) to lexicographic indices (thread IDs) by Korf and Schultze (2005). A perfect hashing scheme computes a unique mapping from permutations (abstract state encoding) to lexicographic indices (thread ID)4.", "startOffset": 70, "endOffset": 318}, {"referenceID": 3, "context": "Previous work compared HDA\u2217[P ], HDA\u2217[P ,Astate ], and SafePBNF on the 15-puzzle and grid pathfinding problems (Burns et al., 2010).", "startOffset": 111, "endOffset": 131}, {"referenceID": 4, "context": "However, it has been shown that a bucket implementation (O(1) for all operations) results in significantly faster performance on state-of-the-art A* implementations (Burns et al., 2012).", "startOffset": 165, "endOffset": 185}, {"referenceID": 3, "context": "For SafePBNF we used the same configuration used in previous work (Burns et al., 2010).", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": "However, it has been shown that a bucket implementation (O(1) for all operations) results in significantly faster performance on state-of-the-art A* implementations (Burns et al., 2012). Therefore, we revisit the comparison of HDA* and SafePBNF by (1) using Zobrist hashing for HDA* (i.e., HDA\u2217[Z ]) in order to minimize search overhead (2) using both easy instances (solvable in < 1 second) and hard instances (requiring up to 1000 seconds to solve with sequential A*) of the sliding tiles and grid path-finding domains in order to isolate the startup costs associated with the burst effect, and (3) using both bucket and heap implementations of the open list in order to isolate the effect of data structure efficiency (as opposed to search efficiency). For the 15-puzzle, we used the standard set of 100 instances by Korf (1985), and used the Manhattan Distance heuristic.", "startOffset": 166, "endOffset": 832}, {"referenceID": 3, "context": "In addition to the original implementation of AHDA* (Burns et al., 2010), which distributes abstract states using a perfect hashing (HDA\u2217[P ,Astate ]), we implemented HDA\u2217[Z ,Astate ] which uses Zobrist hashing to distribute.", "startOffset": 52, "endOffset": 72}, {"referenceID": 27, "context": "\u2022 HDA\u2217[Z ,Afeature ]: HDA* using AZH \u2022 HDA\u2217[Z ]: HDA* using Zobrist hashing (Kishimoto et al., 2009)", "startOffset": 76, "endOffset": 100}, {"referenceID": 3, "context": "\u2022 HDA\u2217[P ,Astate ]: HDA* using Abstraction based work distribution (Burns et al., 2010) \u2022 HDA\u2217[P ]: HDA* using a perfect hash function (Burns et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 3, "context": ", 2010) \u2022 HDA\u2217[P ]: HDA* using a perfect hash function (Burns et al., 2010)", "startOffset": 55, "endOffset": 75}, {"referenceID": 6, "context": "2), using the Pthreads library, try lock for asynchronous communication, and the Jemalloc memory allocator (Evans, 2006).", "startOffset": 107, "endOffset": 120}, {"referenceID": 4, "context": "We implemented the open list as a 2-level bucket (Burns et al., 2012) for the 15-puzzle and 24puzzle, and a binary heap for MSA (binary heap was faster for MSA).", "startOffset": 49, "endOffset": 69}, {"referenceID": 3, "context": "In addition to HDA\u2217[Z ,Afeature ], HDA\u2217[Z ], and HDA\u2217[P ,Astate ], we also evaluated SafePBNF (Burns et al., 2010) and HDA\u2217[P ].", "startOffset": 94, "endOffset": 114}, {"referenceID": 4, "context": "(2010), while the code used in this section incorporated all of the enhancements from their more recent work on efficient sliding tile solver code (Burns et al., 2012) 7.", "startOffset": 147, "endOffset": 167}, {"referenceID": 3, "context": "1, the code is based on the code used in the work of Burns et al. (2010), while the code used in this section incorporated all of the enhancements from their more recent work on efficient sliding tile solver code (Burns et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 29, "context": "9 (the average solution length in th epreious work by Korf and Felner (2002) was 100.", "startOffset": 54, "endOffset": 77}, {"referenceID": 37, "context": "Edge costs are based on the PAM250 matrix score with gap penalty 8 (Pearson, 1990).", "startOffset": 67, "endOffset": 82}, {"referenceID": 28, "context": "We also evaluated the performance of Hyperplane Work Distribution (Kobayashi et al., 2011).", "startOffset": 66, "endOffset": 90}, {"referenceID": 28, "context": "We also evaluated the performance of Hyperplane Work Distribution (Kobayashi et al., 2011). HDA\u2217[Z ] suffers from node reexpansion in non-unit cost domains such as MSA. Hyperplane work distribution seeks to reduce node reexpansions by mapping the n-dimension grid to hyperplanes (denoted as HDA\u2217[Hyperplane]). For HDA\u2217[Hyperplane], we determined the plane thickness d using the tuning method by Kobayashi et al. (2011) where \u03bb = 0.", "startOffset": 67, "endOffset": 419}, {"referenceID": 3, "context": "5) (Burns et al., 2010) addressed ZHDA* HDA\u2217[Z ] not optimized (Sec.", "startOffset": 3, "endOffset": 23}, {"referenceID": 27, "context": "4) (Kishimoto et al., 2009) addressed", "startOffset": 3, "endOffset": 27}, {"referenceID": 26, "context": "For HDA\u2217[Z ], automated domain-independent feature generation for classical planning problems represented in the SAS+ representation (B\u00e4ckstr\u00f6m & Nebel, 1995) is straightforward (Kishimoto et al., 2013).", "startOffset": 178, "endOffset": 202}, {"referenceID": 3, "context": "Burns et al. used the greedy abstraction algorithm by Zhou and Hansen (2006b) to select the subset of features, which we refer to as SDD abstraction.", "startOffset": 0, "endOffset": 78}, {"referenceID": 5, "context": "GreedyAFG first identifies atom groups (Edelkamp, 2001) and its domain transition graph (DTG).", "startOffset": 39, "endOffset": 55}, {"referenceID": 3, "context": "memory bus contention) that affect performance (Burns et al., 2010; Kishimoto et al., 2013), but we assume that CO and SO are the dominant factors in determining efficiency.", "startOffset": 47, "endOffset": 91}, {"referenceID": 26, "context": "memory bus contention) that affect performance (Burns et al., 2010; Kishimoto et al., 2013), but we assume that CO and SO are the dominant factors in determining efficiency.", "startOffset": 47, "endOffset": 91}, {"referenceID": 3, "context": "\u2022 DAHDA*: HDA\u2217[Z ,Astate/SDDdynamic ], AHDA* (Burns et al., 2010) with dynamic abstraction size threshold (Appendix A).", "startOffset": 45, "endOffset": 65}, {"referenceID": 26, "context": "\u2022 ZHDA*: HDA\u2217[Z ], HDA* using Zobrist hashing (Kishimoto et al., 2013) (Sec.", "startOffset": 46, "endOffset": 70}, {"referenceID": 13, "context": "We implemented these HDA* variants on top of the Fast Downward classical planner using the merge&shrink heuristic (Helmert et al., 2014) (abstraction size =1000).", "startOffset": 114, "endOffset": 136}, {"referenceID": 40, "context": "We packed 100 states per MPI message in order to reduce the number of messages (Romein et al., 1999).", "startOffset": 79, "endOffset": 100}, {"referenceID": 3, "context": "memory bus contention) which affect performance (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 48, "endOffset": 92}, {"referenceID": 26, "context": "memory bus contention) which affect performance (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 48, "endOffset": 92}, {"referenceID": 3, "context": "AHDA* (Burns et al., 2010) (Section 2.", "startOffset": 6, "endOffset": 26}, {"referenceID": 13, "context": "We used CGL-B (CausalGraph-Goal-Level&Bisimulation) merge&shrink heuristic (Helmert et al., 2014), which is more efficient and recently proposed than LFPA merge&shrink (Helmert et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 12, "context": ", 2014), which is more efficient and recently proposed than LFPA merge&shrink (Helmert et al., 2007) used in a previous conference paper which evaluated GAZHDA* and FAZHDA* (Jinnai & Fukunaga, 2016b).", "startOffset": 78, "endOffset": 100}, {"referenceID": 9, "context": "While there is no dominance relationship among planners using cheap heuristics such as merge&shrink heuristics (which require only a table lookup during search) and expensive heuristics such as LM-cut, recent work in forward-search based planning has focused on heuristics which tend to be slow, such as heuristics that require the solution of a linear program at every search node (Pommerening, R\u00f6ger, Helmert, & Bonet, 2014; Imai & Fukunaga, 2015), so parallel strategies that focus on minimizing search overheads is of practical importance. Previous evaluations of parallel work distribution strategies in domain-independent planning used relatively fast heuristics. Kishimoto et al. (2013), as well as Jinnai and Fukunaga (2016a, 2016b) used merge&shrink abstraction based heuristics.", "startOffset": 403, "endOffset": 694}, {"referenceID": 9, "context": "While there is no dominance relationship among planners using cheap heuristics such as merge&shrink heuristics (which require only a table lookup during search) and expensive heuristics such as LM-cut, recent work in forward-search based planning has focused on heuristics which tend to be slow, such as heuristics that require the solution of a linear program at every search node (Pommerening, R\u00f6ger, Helmert, & Bonet, 2014; Imai & Fukunaga, 2015), so parallel strategies that focus on minimizing search overheads is of practical importance. Previous evaluations of parallel work distribution strategies in domain-independent planning used relatively fast heuristics. Kishimoto et al. (2013), as well as Jinnai and Fukunaga (2016a, 2016b) used merge&shrink abstraction based heuristics. Zhou and Hansen (2007) and Burns et al.", "startOffset": 403, "endOffset": 812}, {"referenceID": 3, "context": "Zhou and Hansen (2007) and Burns et al. (2010)used the max-pair heuristic (Haslum & Geffner, 2000).", "startOffset": 27, "endOffset": 47}, {"referenceID": 13, "context": "This approach is similar to merge-and-shrink heuristic (Helmert et al., 2014) which merging multiple DTGs into abstract state space to better estimate the state-space graph.", "startOffset": 55, "endOffset": 77}, {"referenceID": 3, "context": "This section presents an improvement to AHDA* (Burns et al., 2010).", "startOffset": 46, "endOffset": 66}, {"referenceID": 11, "context": "The AHDA* results in Table 10 are for a 48-core cluster, 2GB/core, and uses Nmax = 102, 103, 104, 105, 106 nodes based on Fast-Downward (Helmert, 2006) using merge&shrink heuristic (Helmert et al.", "startOffset": 136, "endOffset": 151}, {"referenceID": 13, "context": "The AHDA* results in Table 10 are for a 48-core cluster, 2GB/core, and uses Nmax = 102, 103, 104, 105, 106 nodes based on Fast-Downward (Helmert, 2006) using merge&shrink heuristic (Helmert et al., 2014).", "startOffset": 181, "endOffset": 203}], "year": 2017, "abstractText": "Parallel best-first search algorithms such as Hash Distributed A* (HDA*) distribute work among the processes using a global hash function. We analyze the search and communication overheads of state-of-the-art hash-based parallel best-first search algorithms, and show that although Zobrist hashing, the standard hash function used by HDA*, achieves good load balance for many domains, it incurs significant communication overhead since almost all generated nodes are transferred to a different processor than their parents. We propose Abstract Zobrist hashing, a new work distribution method for parallel search which, instead of computing a hash value based on the raw features of a state, uses a feature projection function to generate a set of abstract features which results in a higher locality, resulting in reduced communications overhead. We show that Abstract Zobrist hashing outperforms previous methods on search domains using hand-coded, domain specific feature projection functions. We then propose GRAZHDA*, a graph-partitioning based approach to automatically generating feature projection functions. GRAZHDA* seeks to approximate the partitioning of the actual search space graph by partitioning the domain transition graph, an abstraction of the state space graph. We show that GRAZHDA* outperforms previous methods on domain-independent planning.", "creator": "TeX"}}}