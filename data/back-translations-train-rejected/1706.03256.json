{"id": "1706.03256", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Progressive Neural Networks for Transfer Learning in Emotion Recognition", "abstract": "Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems.", "histories": [["v1", "Sat, 10 Jun 2017 17:26:20 GMT  (1076kb,D)", "http://arxiv.org/abs/1706.03256v1", "5 pages, 4 figures, to appear in the proceedings of Interspeech 2017"]], "COMMENTS": "5 pages, 4 figures, to appear in the proceedings of Interspeech 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john gideon", "soheil khorram", "zakaria aldeneh", "dimitrios dimitriadis", "emily mower provost"], "accepted": false, "id": "1706.03256"}, "pdf": {"name": "1706.03256.pdf", "metadata": {"source": "CRF", "title": "Progressive Neural Networks for Transfer Learning in Emotion Recognition", "authors": ["John Gideon", "Soheil Khorram", "Zakaria Aldeneh", "Dimitrios Dimitriadis", "Emily Mower Provost"], "emails": ["emilykmp}@umich.edu,", "dbdimitr@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before a solution is found, until an agreement is reached."}, {"heading": "2. Datasets and Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Datasets", "text": "In our study, we use expressions from two sets of data: IEMOCAP [14] and MSP-IMPROV [15], two of the most commonly used sets of data for classifying emotions. Both sets of data were collected to simulate natural dyadic interactions between actors, and have similar identification schemes. We use expressions with majority agreement and truth denomination. We only look at expressions with happy, sad, angry and neutral labels. IEMOCAP: The IEMOCAP data set contains expressions from ten speakers in five sessions. Each session contains a male and a female speaker. We combine expressions of excitement and joy into a happy category, as in [14]. The final data set contains 5531 expressions (1103 angry, 1708 neutral, 1084 sad, 1636 happy).MSP-IMPROV: The MSP-IMPROV data set contains expressions of excitement and joy into a happy category, as in [13]. The final data set contains 5531 expressions (1103 angry, 1708 neutral, 1084 sad, 1636 happy).MSP-IMPROV: The MSP-IMPROV data set contains expressions from excitement and joy into a happy category, as in a happy category."}, {"heading": "2.2. Features", "text": "The eGeMAPS feature set contains a total of 88 features, including frequency, energy, spectral, ceptral, and dynamic information. The final feature vectors for each utterance are determined by applying the following statistics: mean, coefficient of variation, 20th, 50th, and 80th percentile, range from 20th to 80th percentile, mean and standard deviation of the slope of rising / falling signal portions, alpha ratio, Hammarberg index, and spectral slope of 0-500 Hz, and 500-1500 Hz. We perform a data set-specific global z normalization for all features."}, {"heading": "3. Methods", "text": "We compare three methods in the context of transfer learning. As a baseline method, we consider the performance of a DNN to the target task without additional knowledge. In addition, we use the common transfer approach of pretraining a DNN to the source task and fine-tuning to the target task (PT / FT). The underlying assumption of PT / FT is that the target model uses the knowledge available in the source task. This approach is effective in many applications, including ASR [8] and natural language processing. Both methods are compared with the recently introduced progressive neural networks (ProgNets)."}, {"heading": "4. Paralinguistic Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setup", "text": "In the first experiments, we will investigate the effectiveness of knowledge transfer from speech or gender recognition to emotion recognition using the three methods mentioned above. In this section, we will first report on the UAR of the systems on both IEMOCAP and MSP-IMPROV. We will analyze the learning curves to compare the convergence behavior of the systems. Previous work has shown that using the weights of a pre-trained model to initialize a new model to be trained on a similar task can increase the convergence rate [11]."}, {"heading": "4.2. Results", "text": "When transferring from speaker detection to emotion detection, ProgNets significantly outperform both standard DNN (p = 2,6E-3) and PT / FT (p = 2,0E-2) for IEMOCAP and both standard DNN (p = 8,8E-4) and PT / FT (p = 1,2E-2) for MSP-IMPROV. PT / FT system slightly outperforms standard DNN, but the improvement is not significant. This suggests that ProgNets can efficiently integrate the representations acquired by the speaker detection systems into emotion detection systems, but PT / FT cannot leverage this knowledge effectively. ProgNets gender information transmission performance is not consistent between IEMOCAP and MSPIMPROV."}, {"heading": "5. Cross-Dataset Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experimental Setup", "text": "In this experiment, the model is trained on the source data set and then adapted (PT / FT and ProgNet) to the target dataset. Standard DNN is trained only on the target dataset. We investigate the effects of transfer learning when the target data size is small by using different subsets of training folds: 8, 4, 2 and 1. Previous work has shown that transferring knowledge from a large source dataset to a smaller target dataset can be useful [8]. The source model is always trained using the full source dataset (all eight folds). We perform transfer learning by first treating IEMOCAP as a source and MSP-IMPROV as a target and reversing the source / target designations."}, {"heading": "5.2. Results", "text": "Figure 4 shows a summary of the transfer results across different corporations. ProgNet exceeds the standard DNN when transferring from MSP-IMPROV to IEMOCAP for training wrinkle sizes of 1 (p = 2.0E-3), 2 (p = 1.1E-2) and 4 (p = 1.6E-2) and when transferring from IEMOCAP to MSP-IMPROV for training wrinkle sizes of 1 (p = 5.0E-3), 2 (p = 3.2E-2) and 8 (p = 3.6E-2). PT / FT only achieves a significant improvement over the standard DNN baseline when transferring from MSP-IMPROV to IEMOCAP for training wrinkle sizes of 1 (p = 7.1E-4) and 2 (p = 1.0E-2). Since ProgNet has a greater number of weights to the knowledge transfer FT, AP is most advantageous when the target data is larger than the PFT in some cases."}, {"heading": "6. Conclusion", "text": "This paper demonstrates the usefulness of progressive neural networks for this task. Whereas previously a DNN was used on a source dataset for knowledge transfer between tasks, progressive neural networks provide an alternative way to avoid the oblivion effect by allowing the network to maintain learned representations for solving the original task. ProgNets significantly exceeded the standard DNN and PT / FT networks for knowledge transfer between speaker identity and emotion, indicating the usefulness of ProgNets. We have also shown that ProgNets can offer significant improvements for transferring tasks for emotion transfer and tasks for data transfer, compared to systems that do not use source information. In this paper, we focused on transferring knowledge between paralinguistic tasks and datasets. However, future work will examine the usefulness of transferring knowledge about both at the same time, even if the data is not source-specific, for example, this data may be used by highly classified persons (as opposed to)."}, {"heading": "7. Acknowledgement", "text": "This work was partially supported by IBM within the Sapphire Project. We would like to thank Dr. David Nahamoo and Dr. Lazaros Polymenakos, IBM Research, Yorktown Heights, for their support."}, {"heading": "8. References", "text": "In recent years, it has been shown that most of them are people who are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. \"(...) Most of them are able to survive themselves.\" (...) Most of them are not able to survive themselves. \"(...) Most of them are able to survive themselves.\" (...) Most of them are able to survive themselves. \"(...) Most of them are not able to survive themselves.\" (...) Most of them are not able to survive themselves. \"(...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves."}], "references": [{"title": "Automatic speech classification to five emotional states based on gender information", "author": ["D. Ververidis", "C. Kotropoulos"], "venue": "Signal Processing Conference, 2004 12th European. IEEE, 2004, pp. 341\u2013344.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Toward detecting emotions in spoken dialogs", "author": ["C.M. Lee", "S.S. Narayanan"], "venue": "IEEE transactions on speech and audio processing, vol. 13, no. 2, pp. 293\u2013303, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Improving automatic emotion recognition from speech via gender differentiation", "author": ["T. Vogt", "E. Andr\u00e9"], "venue": "Proc. Language Resources and Evaluation Conference (LREC 2006), Genoa, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Comparison of gender-and speaker-adaptive emotion recognition.", "author": ["M. Sidorov", "S. Ultes", "A. Schmitt"], "venue": "LREC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Paralinguistics in speech and languagestate-of-the-art and the challenge", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "F. Burkhardt", "L. Devillers", "C. M\u00fcLler", "S. Narayanan"], "venue": "Computer Speech & Language, vol. 27, no. 1, pp. 4\u201339, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-corpus acoustic emotion recognition from singing and speaking: A multi-task learning approach", "author": ["B. Zhang", "E.M. Provost", "G. Essi"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5805\u20135809.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-task learning framework for emotion recognition using 2d continuous space", "author": ["R. Xia", "Y. Liu"], "venue": "IEEE Transactions on Affective Computing, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-lingual transfer learning during supervised training in low resource scenarios.", "author": ["A. Das", "M. Hasegawa-Johnson"], "venue": "TERSPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition", "author": ["J. Deng", "Z. Zhang", "E. Marchi", "B. Schuller"], "venue": "Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on. IEEE, 2013, pp. 511\u2013516.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning for emotion recognition on small datasets using transfer learning", "author": ["H.-W. Ng", "V.D. Nguyen", "V. Vonikakis", "S. Winkler"], "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction. ACM, 2015, pp. 443\u2013449.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Progressive neural networks", "author": ["A.A. Rusu", "N.C. Rabinowitz", "G. Desjardins", "H. Soyer", "J. Kirkpatrick", "K. Kavukcuoglu", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1606.04671, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "The cascade-correlation learning architecture", "author": ["S.E. Fahlman", "C. Lebiere"], "venue": "1990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1610.04286, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Iemocap: Interactive emotional dyadic motion capture database", "author": ["C. Busso", "M. Bulut", "C.-C. Lee", "A. Kazemzadeh", "E. Mower", "S. Kim", "J.N. Chang", "S. Lee", "S.S. Narayanan"], "venue": "Language resources and evaluation, vol. 42, no. 4, p. 335, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception", "author": ["C. Busso", "S. Parthasarathy", "A. Burmania", "M. Abdel-Wahab", "N. Sadoughi", "E.M. Provost"], "venue": "IEEE Transactions on Affective Computing, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Emotion spotting: Discovering regions of evidence in audio-visual emotion expressions", "author": ["Y. Kim", "E.M. Provost"], "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction. ACM, 2016, pp. 92\u201399.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Retrieving categorical emotions using a probabilistic framework to define preference learning samples", "author": ["R. Lotfian", "C. Busso"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing", "author": ["F. Eyben", "K.R. Scherer", "B.W. Schuller", "J. Sundberg", "E. Andr\u00e9", "C. Busso", "L.Y. Devillers", "J. Epps", "P. Laukka", "S.S. Narayanan"], "venue": "IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "How transferable are neural networks in nlp applications?", "author": ["L. Mou", "Z. Meng", "R. Yan", "G. Li", "Y. Xu", "L. Zhang", "Z. Jin"], "venue": "arXiv preprint arXiv:1603.06111,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "The interspeech 2009 emotion challenge.", "author": ["B.W. Schuller", "S. Steidl", "A. Batliner"], "venue": "in Interspeech,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Evaluating the replicability of significance tests for comparing learning algorithms", "author": ["R.R. Bouckaert", "E. Frank"], "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 2004, pp. 3\u201312.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Previous work demonstrated that multi-task learning techniques (MTL) could be used to jointly model both gender and emotion [1, 2, 3], resulting in consistent performance increases compared to gender-agnostic models.", "startOffset": 124, "endOffset": 133}, {"referenceID": 1, "context": "Previous work demonstrated that multi-task learning techniques (MTL) could be used to jointly model both gender and emotion [1, 2, 3], resulting in consistent performance increases compared to gender-agnostic models.", "startOffset": 124, "endOffset": 133}, {"referenceID": 2, "context": "Previous work demonstrated that multi-task learning techniques (MTL) could be used to jointly model both gender and emotion [1, 2, 3], resulting in consistent performance increases compared to gender-agnostic models.", "startOffset": 124, "endOffset": 133}, {"referenceID": 3, "context": "Previous work also showed that emotion recognition systems could be improved by incorporating speaker identity as a feature, along with the other emotion-related features [4].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] explored MTL frameworks for leveraging data from different domains (speech and", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] treated dimensional and categorical labels as two different tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The most common approach to transfer learning is to train a model in one domain and fine-tune it in a related domain [8, 9, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 8, "context": "The most common approach to transfer learning is to train a model in one domain and fine-tune it in a related domain [8, 9, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 9, "context": "The most common approach to transfer learning is to train a model in one domain and fine-tune it in a related domain [8, 9, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 7, "context": "For example, in ASR, transfer learning is used to transfer knowledge from a richly-resourced language to an under-resourced language [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "[9] presented a sparse auto-encoder method for transferring knowledge between six emotion recognition datasets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] studied transfer learning in the context of facial expression recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "First, it is unclear how to initialize a model given learned weights from a sequence of related tasks [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "Second, when a model is fine-tuned using initial weights learned from a source task, the end-model loses its ability to solve the source task, a phenomenon termed the \u201cforgetting effect\u201d [12].", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "Another recent approach for transfer learning is progressive neural networks (ProgNets) [11, 13].", "startOffset": 88, "endOffset": 96}, {"referenceID": 12, "context": "Another recent approach for transfer learning is progressive neural networks (ProgNets) [11, 13].", "startOffset": 88, "endOffset": 96}, {"referenceID": 13, "context": "plied between two emotion datasets: IEMOCAP [14] and MSPIMPROV [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "plied between two emotion datasets: IEMOCAP [14] and MSPIMPROV [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "We use speech utterances from two datasets in our study: IEMOCAP [14] and MSP-IMPROV [15], two of the most commonly used datasets in emotion classification [16, 17].", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "We use speech utterances from two datasets in our study: IEMOCAP [14] and MSP-IMPROV [15], two of the most commonly used datasets in emotion classification [16, 17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "We use speech utterances from two datasets in our study: IEMOCAP [14] and MSP-IMPROV [15], two of the most commonly used datasets in emotion classification [16, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 16, "context": "We use speech utterances from two datasets in our study: IEMOCAP [14] and MSP-IMPROV [15], two of the most commonly used datasets in emotion classification [16, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 13, "context": "We combine excitement and happiness utterances to form the happy category, as in [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "We use the eGeMAPS [18] feature set designed to standardize features used in affective computing.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "This approach has been effective in many applications, including ASR [8] and natural language processing [19].", "startOffset": 69, "endOffset": 72}, {"referenceID": 18, "context": "This approach has been effective in many applications, including ASR [8] and natural language processing [19].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Both these methods are compared to the recently introduced progressive neural networks (ProgNets) [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "ProgNets do not disrupt the learned information in existing source tasks, which avoids the forgetting effect present in PT/FT [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "UAR is an unweighted accuracy that gives the same weights to different classes and is a popular metric for emotion recognition, used to account for unbalanced datasets [20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "We evaluate the performance of the methods using a repeated ten-fold cross-validation scheme, as used in [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "We perform significance tests using a repeated cross-validation paired t-test with ten degrees of freedom, as shown in [21], and note significance when p < 0.", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "Prior work demonstrated that using the weights of a pre-trained model to initialize a new model to be trained on a related task can increase convergence speed [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 7, "context": "Previous work has shown that transferring knowledge from a large source data set to a smaller target datasets can be beneficial [8].", "startOffset": 128, "endOffset": 131}], "year": 2017, "abstractText": "Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems.", "creator": "LaTeX with hyperref package"}}}