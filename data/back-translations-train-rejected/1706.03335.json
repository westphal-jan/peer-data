{"id": "1706.03335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "Exploring Automated Essay Scoring for Nonnative English Speakers", "abstract": "Automated Essay Scoring (AES) has been quite popular and is being widely used. However, lack of appropriate methodology for rating nonnative English speakers' essays has meant a lopsided advancement in this field. In this paper, we report initial results of our experiments with nonnative AES that learns from manual evaluation of nonnative essays. For this purpose, we conducted an exercise in which essays written by nonnative English speakers in test environment were rated both manually and by the automated system designed for the experiment. In the process, we experimented with a few features to learn about nuances linked to nonnative evaluation. The proposed methodology of automated essay evaluation has yielded a correlation coefficient of 0.750 with the manual evaluation.", "histories": [["v1", "Sun, 11 Jun 2017 10:18:46 GMT  (428kb)", "http://arxiv.org/abs/1706.03335v1", null], ["v2", "Wed, 14 Jun 2017 09:52:35 GMT  (428kb)", "http://arxiv.org/abs/1706.03335v2", null], ["v3", "Fri, 29 Sep 2017 07:08:50 GMT  (652kb)", "http://arxiv.org/abs/1706.03335v3", "Accepted for publication at EUROPHRAS 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amber nigam", "vibhore goyal"], "accepted": false, "id": "1706.03335"}, "pdf": {"name": "1706.03335.pdf", "metadata": {"source": "CRF", "title": "Exploring Automated Essay Scoring for Nonnative English Speakers", "authors": ["Amber Nigam"], "emails": [], "sections": [{"heading": null, "text": "Automated Essay Scoring (AES) has been very popular and widely used. However, the lack of an appropriate methodology for the evaluation of essays by native English speakers has meant one-sided progress in this area. In this paper, we report on the initial results of our experiments with native AES, which learn from the manual evaluation of native English speakers. To this end, we conducted an exercise in which essays written by native English speakers in a test environment were evaluated both manually and by the automated system developed for the experiment, experimenting with some features to learn nuances associated with nonnative evaluation. The proposed methodology of automated essay evaluation has yielded a correlation coefficient of 0.750 with manual evaluation."}, {"heading": "1. Introduction", "text": "There are different versions of Automated Essay Scoring (AES) and the lack of generalisability between different analyses and bodies raises the question of the validity of a quantity suitable for all AES. In addition, nonnative analysis differs in some aspects from native analysis. For example, it is difficult to detect contexts in essays that have errors specific to some nonnative uses. In addition, some nonnative uses such as Qutub Minar and Karur are not part of standard English. However, this does not make the use of these words erroneous. In this paper, we discussed our methodology for nonnative essay evaluation. First, we described the scope of functionality we used for our experiments. Second, we discussed various adjustments that have been made to the system so that it learns from manual evaluation and takes account of it. Finally, we discussed the results of our experiments and the intention of broader analysis that this experiment belongs to."}, {"heading": "2. Related Work", "text": "Although automated essay scoring (AES) is widely used in many of the real-world applications, there is very limited published work on the evaluation of nonnative speakers. The following analyses deal with nonnative speakers in one way or another. (1999), reported that even if 75% of the essays used in modeling were written by nonnative English speakers, the characteristics selected by the regression process were largely the same as those in the models in which the majority of subjects spoke English. Correlations between E-Rater systems and those of a single human reader were event.73 As mentioned in the paper, there were significant differences between the endpoints of human readers and E-Rater."}, {"heading": "3. Experiment", "text": "The current experiment involved building a model by selecting features, followed by manual and automated essay evaluation (as shown in Figure 1). In total, there were more than 900 essays ranging in length from 150 to 400 words on 7 unique topics. Essay topics were carefully selected to belong to commonly known topics, making them easy to understand and write. Test participants were all candidates whose native language is Hindi to keep the analysis independent of the test participant's native language. Each essay was manually rated on a scale of 1-10 by two evaluators who had one between their values.81 Cohen's Kappa statistics (Cohen J, 1968).The split between training, validation, and test kits was about 60: 20: 20. We used LanguageTool (Daniel Naber, 2003; LanguageTool, 2012) to detect grammatical errors, and the system was designed to detect fraud attempts such as repetition of content, writing from context, and over-use."}, {"heading": "4. Feature Set Selection", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to assert themselves."}, {"heading": "5. Self-correction Mechanism", "text": "One of the unique features of the engine is that it displays unknown spellings / phrases as soon as their cumulative frequency exceeds a pre-determined threshold, which has helped us to include many nonnative uses in our repository, such as proper names like Kaushambi and Ranganathan being some of the recent additions."}, {"heading": "6. Results", "text": "Various machine learning algorithms were used to predict the score of the essays, and the predictions of the Random Forest algorithm came closest to manual scoring, as in Table 2. Key predictive features of the analysis include some grammatical error densities, lexical density, readability, and spelling error density.We used WEKA toolkit (Hall et al., 2009) for the score prediction."}, {"heading": "7. Conclusion and Future Work", "text": "In this paper, we have outlined a methodology for evaluating English essays by nonnative speakers that limited itself to selecting a relevant set of characteristics for evaluation, categorized grammatical errors into finer types to learn about their meaning from their distinctive treatment in the contribution to manual evaluation in nonnative context, and treated essays for typical nonnative grammatical errors (Alla Rozovskaya and Dan Roth, 2016), which enabled improved context matching for essays with nonnative errors, and developed a self-correction mechanism to learn them and respond promptly to nonnative spellings and styles. Our results also show that these incremental adjustments have contributed cumulatively to better aligning automated evaluation with manual evaluation. We plan to use the Long Short Term Memory (LSTM) network to further explore this area. We also intend to expand our research by examining the effects of birth and learning from English birth."}], "references": [{"title": "Grammatical Error Correction: Machine Translation and Classifiers ACL (2016", "author": ["Alla Rozovskaya", "Dan Roth"], "venue": null, "citeRegEx": "Rozovskaya and Roth.,? \\Q2016\\E", "shortCiteRegEx": "Rozovskaya and Roth.", "year": 2016}, {"title": "Weighted Kappa: Nominal Scale Agreement with Provision for Scaled Disagreement or Partial Credit", "author": ["J. Cohen"], "venue": "Psychol. Bull. 70,", "citeRegEx": "Cohen,? \\Q1968\\E", "shortCiteRegEx": "Cohen", "year": 1968}, {"title": "A Rule-Based Style and Grammar Checker", "author": ["Daniel Naber"], "venue": "Diploma Thesis, University of Bielefeld,", "citeRegEx": "Naber.,? \\Q2003\\E", "shortCiteRegEx": "Naber.", "year": 2003}, {"title": "Automatic Text Scoring Using Neural Networks", "author": ["Dimitrios Alikaniotis", "Helen Yannakoudakis", "Marek Rei"], "venue": "In Proceedings of ACL,", "citeRegEx": "Alikaniotis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alikaniotis et al\\.", "year": 2016}, {"title": "A Neural Approach to Automated Essay Scoring", "author": ["Kaveh Taghipour", "Hwee Tou Ng"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Taghipour and Ng.,? \\Q2016\\E", "shortCiteRegEx": "Taghipour and Ng.", "year": 2016}, {"title": "The measurement of textual coherence with Latent Semantic Analysis", "author": ["P.W. Foltz", "W. Kintsch", "T.K. Landauer"], "venue": "Discourse Processes,", "citeRegEx": "Foltz et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "The weka data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "In The SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel", "author": ["JP Kincaid", "RP Fishburne", "RL Rogers", "BS. Chissom"], "venue": "Memphis, Tenn: Naval Air Station;", "citeRegEx": "Kincaid et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Automatic evaluation of text coherence: Models and representations", "author": ["Mirella Lapata", "Regina Barzilay."], "venue": "IJCAI, volume 5, pages 1085\u2013 1090.", "citeRegEx": "Lapata and Barzilay.,? 2005", "shortCiteRegEx": "Lapata and Barzilay.", "year": 2005}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics, 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Lexical density and register differentiation", "author": ["J Ure"], "venue": "In G. Perren and J.L.M. Trim (eds), Applications of Linguistics,", "citeRegEx": "Ure,? \\Q1971\\E", "shortCiteRegEx": "Ure", "year": 1971}], "referenceMentions": [{"referenceID": 8, "context": "We use Grid model (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008) to detect coherence.", "startOffset": 18, "endOffset": 72}, {"referenceID": 9, "context": "We use Grid model (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008) to detect coherence.", "startOffset": 18, "endOffset": 72}, {"referenceID": 6, "context": "We used WEKA toolkit (Hall et al., 2009) for score prediction.", "startOffset": 21, "endOffset": 40}], "year": 2017, "abstractText": "Automated Essay Scoring (AES) has been quite popular and is being widely used. However, lack of appropriate methodology for rating nonnative English speakers\u2019 essays has meant a lopsided advancement in this field. In this paper, we report initial results of our experiments with nonnative AES that learns from manual evaluation of nonnative essays. For this purpose, we conducted an exercise in which essays written by nonnative English speakers in test environment were rated both manually and by the automated system designed for the experiment. In the process, we experimented with a few features to learn about nuances linked to nonnative evaluation. The proposed methodology of automated essay evaluation has yielded a correlation coefficient of 0.750 with the manual evaluation.", "creator": "Microsoft\u00ae Word 2013"}}}