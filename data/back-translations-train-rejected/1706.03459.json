{"id": "1706.03459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Optimal Auctions through Deep Learning", "abstract": "Designing an auction that maximizes expected revenue is an intricate task. Indeed, as of today--despite major efforts and impressive progress over the past few years--only the single-item case is fully understood. In this work, we initiate the exploration of the use of tools from deep learning on this topic. The design objective is revenue optimal, dominant-strategy incentive compatible auctions. We show that multi-layer neural networks can learn almost-optimal auctions for settings for which there are analytical solutions, such as Myerson's auction for a single item, Manelli and Vincent's mechanism for a single bidder with additive preferences over two items, or Yao's auction for two additive bidders with binary support distributions and multiple items, even if no prior knowledge about the form of optimal auctions is encoded in the network and the only feedback during training is revenue and regret. We further show how characterization results, even rather implicit ones such as Rochet's characterization through induced utilities and their gradients, can be leveraged to obtain more precise fits to the optimal design. We conclude by demonstrating the potential of deep learning for deriving optimal auctions with high revenue for poorly understood problems.", "histories": [["v1", "Mon, 12 Jun 2017 04:03:15 GMT  (1475kb,D)", "http://arxiv.org/abs/1706.03459v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.LG", "authors": ["paul d\\\"utting", "zhe feng", "harikrishna narasimhan", "david c parkes"], "accepted": false, "id": "1706.03459"}, "pdf": {"name": "1706.03459.pdf", "metadata": {"source": "CRF", "title": "Optimal Auctions through Deep Learning\u2217", "authors": ["Paul D\u00fctting", "Zhe Feng", "Harikrishna Narasimhan", "David C. Parkes"], "emails": ["p.d.duetting@lse.ac.uk.", "feng@g.harvard.edu.", "hnarasimhan@g.harvard.edu.", "parkes@g.harvard.edu."], "sections": [{"heading": "1 Introduction", "text": "Optimal auction design is one of the cornerstones of economic theory and has also received much attention in computer science in recent years. It is of great practical importance, since auctions are used within the industry and the public sector to organize the sale of many products and services. A fundamental question is that a protocol is designed for the sale of one or more items to maximize expected revenues. Myerson's [45] groundbreaking work solved the problem of optimal auction design for a single item. The problem of designing optimal auctions for more than one item is much more difficult and defies a thorough theoretical understanding for several decades. Indeed, despite a flurry of recent breakthroughs [1, 22, 24, 11, 23, 31] many important issues remain unresolved."}, {"heading": "1.1 Optimal Auctions: Structure and Challenges", "text": "In the standard model, there is a virtual revenue increase (V1) and a virtual revenue increase (Vn). The auctioneer knows the distribution mechanisms, but not the valuation functions of the bidders. The auctioneer carries out a mechanismM = (g, p), consisting of a collection of allocation rules gi: V \u2192 2M and payment rules pi: V \u2192 R \u2265 0. The auction collects bids bi \u2212 Vi from the bidders and then calculates an allocation g (b) and payments p (b), with bid profile = (b1, bn). A feasible mechanism is one that calculates each element to at most one bidder."}, {"heading": "1.2 Our Approach and Results", "text": "In fact, it is that most people are able to help themselves if they are not able to help themselves. (...) It is not that they are able to help themselves. (...) It is as if they were not able to help themselves. (...) It is as if they were able to help themselves. (...) It is as if they were able to help themselves. (...) It is as if they were able to help themselves. (...) It is as if they were not able. (...) It is as if they were. (...) It is as if they were. (...) It is as if they were able to help themselves. (...) It is as if they were. (...) It is as if they were. (...). (...). (...). (...). (...). (...). (...). (). (It. (...). (). (It. (.). (It.). (It. (...). (). (It. (). (It. (). (It. (.). (It. (). (It.). (It. (It.). (It. (.). (It. (). (It.). (It. (.). (It.). (It. (It. (.). (It.). (It. (.). (It. (It.). (It.). (It.). (. (It.). (It. (It.). (It. (.). (It.). (. (It.). (It. (.). (It.). (It. (.). (It.). (It.). (It. (. (It.). (. (It.).). (It. (It. (.). (It.). (It.). (It. (.). (It. (.).). ((. (It.).). (It.). ((It.). (It. (It.). (It. (.). (((.).). (It. (.).). (It.). (It is."}, {"heading": "1.3 Related work", "text": "Considering that these two are two different types who live in very different countries in the world, it's not surprising that most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves."}, {"heading": "2 Characterization Based Approach", "text": "We start with auction design settings where there are known characterizations of DSIC mechanisms. We use these characterization results to construct neural networks that represent DSIC mechanisms for all decisions of network parameters. In this context, we optimize network parameters with expected negated revenue as a loss function. We illustrate this approach in two settings: (i) single auctions, where we use Myerson's monotonicity condition to model the allocation and payment rules as neural networks; and (ii) a single bidder environment with additive preferences over multiple properties, using Rochet's implicit characterization by induced utilities and their gradients to restore the optimal auction. We call the first architecture MyersonNet and the second architecture RochetNet."}, {"heading": "2.1 Single-item Auctions", "text": "In the United States, we have appended the payment rule pi over a price (b) to the bidder (b) so that the bidder (b) indicates the probability that the bidder (b) ti (b) ti (b) ti (b) ti (b) ti (b) ti (b) ti (b) ti (b) ti (b) b).For the regular distributions of which the optimal auction can be described (strictly monotonous) virtual value transformations 1,."}, {"heading": "2.2 Multi-item Auction with Single Additive Bidder", "text": "It is well known that the optimal auction in this environment may require a random evaluation. We consider a mechanism (g, p) with the result rule g: Rm (0, 1) m, which issues a bid for a single bidder who makes a bid for a multiple bidder. (b) m, where the optimal auction requires a random selection. (b) m, where the bidder requires a random selection. (b) m, where the bidder requires a random selection. (b) m, where the bidder requires a random selection. (b) m, where the bidder requires a random selection. (b) The payment rule p: Rm (0)."}, {"heading": "3 Fully Agnostic Approach", "text": "In the previous section, we have developed a framework for the use of neural networks in order to design a new profile in which we have a new profile. (...) We are now developing a framework for the use of in-depth learning methods, which we strive to achieve the most important advances in DSIC. (...) In this case, we have developed a mechanism that does not quite fit the existing analytical results in the theoretical literature, and provide feedback on the learning algorithms through the revenue and regret from the network. We point to these as the most important advances in the DSIC RegretNet architecture. (...) We describe our approach to a general setting with n bidders N and m elements M. For the ease of exposure, we consider additional bidders, although the approach slightly extends to more general bidder evaluations. (...) We are considering a randomized allocation rule. (...)"}, {"heading": "4 Experimental Results", "text": "We present experimental results that show that deep learning can be used to restore almostop-optimal mechanisms for a variety of distributions, as well as to find new mechanisms for settings where there is no analytical solution for optimal design.Setup. We use the TensorFlow Deep Learning library to implement learning algorithms for neural networks. We integrate the L2 regularization, with the regularization parameter set to 0.001. The learning rate for the solvers is set to 0.001, and the solvers will run for a maximum of 400,000 iterations.The batch size in the mini-batch stochastic gradient solver is set to 16 for RegretNet and 100 for the other neural networks.In some cases, the parameters have been set differently to achieve faster convergence (details in Appendix C.) Most of the experiments have been distributed on a DIVA net test result cluster."}, {"heading": "4.1 Characterization Based Approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Single-item Auction", "text": "We rate MyersonNet for the design of single auctions on three regular distributions: (a) symmetrical uniform distribution with 3 bidders and each vi \u0445 U [0, 1] (b) asymmetrical uniform distribution with 5 bidders and each vi \u0445 U [0, i] (c) exponential distribution with 3 bidders and each vi \u0445 Exp (3).We examine auctions with a small number of bidders because this is where the revenue-optimized auctions significantly differ from efficient auctions. Optimal auctions for these distributions include virtual valuations that are strictly monotonous. We also consider an irregular distribution Firregular: (d) each vi is rated by U [0, 3] with the probability of 3 / 4 and by U [3, 8] with the probability of 1 / 4. In this irregular case, the optimal auction uses virtual valuations that are not strictly monotonous."}, {"heading": "4.1.2 Multi-item Setting with Single Additive Bidder", "text": "(...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...)."}, {"heading": "4.2 Fully Agnostic Approach", "text": "Next, we apply the regret-based approach to the distributions analysed above, as well as to new distributions for which there are no characterisation results, and show that we can restore a mechanism where revenues are close to the optimum and where regrets are very low.5"}, {"heading": "4.2.1 Single-item Auctions", "text": "For one-time auctions, the RegretNet allocation rule leads each bid through a monotonous transformation (consisting of 5 groups of 10 linear functions) followed by a softmax activation function to calculate the allocation probabilities (see Figure 7).5 The payment rule uses a general virtual revenue network with two hidden layers of 10 nodes each (T = 2), followed by an issue layer that calculates the payment to each bidder (conditioned by allocation).5 The RegretNet training problem in our experiments (see (26) uses a stricter definition of regret than the Myrill layer shown in (24), which evaluates regret for the agent i as maximum benefit-gain across all training profiles, including the mean benefit-gain i: r (g, p) = max \"[L] maxv.\" S \"ui The optimal distribution response to the distribution response to the net is distributionality."}, {"heading": "4.2.2 Multi-item Setting with a Single Additive Bidder", "text": "We also apply the regret-based approach to the hiring of individual additive bidders. Here, the RegretNet allocation and payment networks each use two hidden layers of 10 nodes (R = T = 2 in Figure 8), followed by an output layer. Results for the two-part additive bidder distributions are shown in Table 2. Even without the help of characterization results, RegretNet is able to identify mechanisms that generate revenue close to optimum revenue, and with little regret and infringement of IR. Results for the distribution of ten items are shown in Table 4, where we can see that RegretNet identifies a new mechanism that generates higher revenue than baselines, but produces very little regret."}, {"heading": "4.2.3 Multi-item Setting with Multiple Bidders", "text": "Finally, we apply the regret-based approach to settings for which there are no characterization results. We consider a setting that contains two bidders with additive preferences over two items, with the following apportionments: (a) Discrete Uniform I: the item values for each bidder are drawn from identical uniform revenue distributions over two values {0.5, 1.0} (b) Discrete Uniform II: the item values for each bidder are drawn from identical uniform distribution mechanisms over three values {0.5, 1.0, 1.5} (c) Continuous Uniform: the item values for each bidder are i.i.d. drawn from identical uniform distributions over [0, 1]. Even for these simple apportions, this setting is analytically difficult to solve. In fact, the optimal mechanism for this setting is known only for the first apportionment [57]. For the discrete distributions, we provide the optimal distributions of the trained mechanisms against a Myerson auction to each third item and for an optimal distribution to each third item."}, {"heading": "5 Conclusion", "text": "Our work shows that machine learning tools can rediscover theoretical outcomes such as the optimal single auction of Myerson [45] or the mechanism of Manelli and Vincent [42] for a single additive bidder and two objects, and provides some evidence of the helpfulness of deep learning techniques beyond these frameworks. We believe that this is the starting point of a very fruitful research agenda of machine-based mechanism design that could lead to new theoretical insights as well as new practical mechanisms. In general, it would be interesting to see if our framework can be used to gain additional insights into the structure of optimal DSIC mechanisms. One particularly promising direction is auction environments with n > 1 bidders and two objects where Yao [57] provides partial characterization. More generally, the power of deep learning is that through imaging learning it can achieve good or close to optimal designs in complex environments for which clean analytical characteristics are hard to imagine."}, {"heading": "A Augmented Lagrangian Method for Constrained Optimization", "text": "We give a brief description of the advanced Lagrange method for solving problems with restricted optimization [7]. We use this method to solve neural network training problems with equality limitations. Let us consider the following optimization problem with s equality limitations: min w, Rd C (w) (27) s.t. gj (w) = 0,.j = 1,... The advanced Lagrange method formulates an unqualified goal that includes the Lagrange method for the above problem, supplemented by additional square penalties that punish violations of equality limitations: C\u03c1 (w, \u03bb) = C (w) + s, which applies equality, j = 1 (w) s, j (w) 2 s, j (w) 2, where we punish the optimization of equality limitations, which punish violations in equality limitations punish punishments punishing punishments punishing punishments for punishments (w, \u03bb) = C (w) + s, which applies equality violations."}, {"heading": "B Proof for Theorem 2.3", "text": "The proof. The convexity of u\u03b1, \u03b2 results from the fact that it is a \"max\" of linear functions. We now show that u\u03b1, \u03b2 is not decreasing monotonously. Let's leave hj (v) = wj \u00b7 v + \u03b2j. Since wj is not negative in all entries, we have for all vi \u2264 v \"i\" i \"i\" i \"i\" M \"hj (v\") \u2264 hj (v \"). Thenu\u03b1, \u03b2 (v\") = max j \"hj\" (v) = hj \"(v\") \u2264 hj \"(v\") \u2264 \"J\" (v \") \u2264,\" \u03b2 \"(v\"), where j \"argminj\" [J] hj \"(v). It remains to show that u\u03b1, \u03b2\" 1 \"Lipschitz\" (v \") \u2264\" Rm \"(v\") = u\u03b1, \u03b2 \"(v\"), \u03b2 \"\u03b2\" (v \"), \u03b2\" J \"hj\" (hj) = hj."}, {"heading": "C Supplementary Experimental Details", "text": "The learning rate in RochetNet is 0.001 for all item distributions. The learning rate in RochetNet is 0.001 for the distribution of 2 items and 0.001 for the distribution of 10 items. The learning rate in RegretNet is 0.001 for all item distributions except exponential distribution, 0.001 for the exponential distribution of 1 item, 0.001 for the uniform distribution of 2 items for 1 bidder, 0.005 for the uniform distribution of 2 items for 2 bidders, 0.005 for the uniform distribution of 2 items for 1 bidder, 0.001 for the uniform distribution of 10 items for 1 bidder, 0.005 for the uniform distribution of 2 items for 2 bidders, 0.005 for the uniform distribution of 2 items for 2 bidders, 0.005 for the uniform distribution of 2 items for 2 items and 0.002 for the uniform distribution of 2 items. We have sometimes been able to convert ourselves to a mechanism with higher revenues (and lower remorse) by reducing the learning rate to 0.005."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Designing an auction that maximizes expected revenue is an intricate task. Indeed, as of<lb>today\u2014despite major efforts and impressive progress over the past few years\u2014only the single-<lb>item case is fully understood. In this work, we initiate the exploration of the use of tools<lb>from deep learning on this topic. The design objective is revenue optimal, dominant-strategy<lb>incentive compatible auctions. We show that multi-layer neural networks can learn almost-<lb>optimal auctions for settings for which there are analytical solutions, such as Myerson\u2019s auction<lb>for a single item, Manelli and Vincent\u2019s mechanism for a single bidder with additive preferences<lb>over two items, or Yao\u2019s auction for two additive bidders with binary support distributions and<lb>multiple items, even if no prior knowledge about the form of optimal auctions is encoded in<lb>the network and the only feedback during training is revenue and regret. We further show how<lb>characterization results, even rather implicit ones such as Rochet\u2019s characterization through<lb>induced utilities and their gradients, can be leveraged to obtain more precise fits to the optimal<lb>design. We conclude by demonstrating the potential of deep learning for deriving optimal<lb>auctions with high revenue for poorly understood problems.", "creator": "LaTeX with hyperref package"}}}