{"id": "1706.03499", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models", "abstract": "This paper describes the Stockholm University/University of Groningen (SU-RUG) system for the SIGMORPHON 2017 shared task on morphological inflection. Our system is based on an attentional sequence-to-sequence neural network model using Long Short-Term Memory (LSTM) cells, with joint training of morphological inflection and the inverse transformation, i.e. lemmatization and morphological analysis. Our system outperforms the baseline with a large margin, and our submission ranks as the 4th best team for the track we participate in (task 1, high-resource).", "histories": [["v1", "Mon, 12 Jun 2017 08:08:00 GMT  (275kb,D)", "http://arxiv.org/abs/1706.03499v1", "4 pages, to appear at CoNLL-SIGMORPHON 2017"]], "COMMENTS": "4 pages, to appear at CoNLL-SIGMORPHON 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert \\\"ostling", "johannes bjerva"], "accepted": false, "id": "1706.03499"}, "pdf": {"name": "1706.03499.pdf", "metadata": {"source": "CRF", "title": "SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models", "authors": ["Robert \u00d6stling", "Johannes Bjerva"], "emails": ["robert@ling.su.se", "j.bjerva@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "We focus on task 1 of the joint task SIGMORPHON 2017 (Cotterell et al., 2017), the morphological diffraction. The task is to learn to assign a lemma and a morphological description to the corresponding bent form. Thus, for example, the English verb torment with the characteristics 3.SG.PRS should be mapped to torment. As our model is poorly suited for resource-poor conditions, we have submitted results only for the 51 languages with high training data available in the joint task (i.e. without Scottish Gaelic)."}, {"heading": "2 Background", "text": "The results of the joint task of SIGMORPHON 2016 (Cotterell et al., 2016) suggest that the attention-oriented sequence-to-sequence model of Bahdanau et al. (2014) is very suitable for this task (Can and Protects, 2016), so we are using this model as the basis for our model. This work was carried out during a visit by the second author to the Institute of Linguistics at Stockholm University. A current trend in neural machine translation is to use back-translated text (Sennrich et al., 2016) as a way to benefit from additional monolingual data in the target language. There is also work on translation models with reconstruction loss that promote solutions that can be translated back into their original (Tu et al., 2016). These developments are technically similar to our subsequent semi-monitored training courses."}, {"heading": "3 Method", "text": "Our system is based on the attention-oriented sequenceto-sequence model by Bahdanau et al. (2014) with the exception of Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the variational dropout Gal and Ghahramani (2016). The most important innovation is that our diffraction model is trained along with the reverse process, i.e. lemmatization and morphological analysis. This can be done in two ways: 1. Fully monitored, simply sharing the forward (kink) and backward (lemmatization and morphological analysis) model along with common character embeddings. 2. Semi-monitored, mixing monitored examples with examples using only the inflected target form. This form is passed on first by the backward model, a greedy quest to obtain a unique problem, and finally by the forward model to reconstruct the inflected form."}, {"heading": "4 Model configuration", "text": "For official submission, we use 128 LSTM cells for the (unidirectional) encoder, decoder, attention mechanism, character embedding, and fully connected layers for encoding / predicting morphological characteristics. We use a failure factor of 0.5 across the network, including recurring parts. For optimization, we use Adam (Kingma and Ba, 2015) with standard parameters. Each model is trained on a single CPU for 48 hours, using a batch size of 64, and the model parameters during this time, which produce the lowest amount of development, mean that Levensht is stored at a distance."}, {"heading": "5 Results and Analysis", "text": "The system generally performs well, with a macro-average accuracy of 93.6% and a machining accuracy of 0.14%, which is substantially higher than the baseline (77.8% accuracy and 0.5 machining distance) and ranks as the ninth best running and fourth best team in this joint task set by SIGMORPHON 2017. Furthermore, the difference in results between our run and the best run overall is small (1.75% accuracy and 0.04 machining distance).Table 1 provides a detailed version of the official results of our joint task system, in the high setting of task 1.It is noteworthy that the system has a 100% accuracy in both Basque and Quechua, indicating that it is capable of fully learning the rules of very regular morphological systems. The relatively high accuracy of Semitic languages (Arabic: 89.8%, Hebrew: 99.0%) and the accuracy of systemic lines reaffirm the ability to revisit the decoding or non-Italian encoding models."}, {"heading": "6 Conclusions", "text": "We implemented a system with an attention-oriented sequence-to-sequence model using Long ShortTerm Memory (LSTM) cells. As our model is poorly suited for low-resource conditions, we only participated in the high-resource setting. Our diffraction model is trained together with the reverse process, i.e. lemmatization and morphological analysis. The system clearly outperforms the basic system and performs well compared to other submitted systems, which shows that this approach is very suitable for morphological diffraction given sufficient data volumes."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the reviewers and Johan Sjons for their comments on earlier versions of this manuscript, which was partly funded by the NWO-VICI fellowship \"Lost in Translation - Found in Meaning\" (288-89-003), which was carried out at the Abel Cluster, owned by the University of Oslo and the Norwegian Meta-Centre for Supercomputing (NOTUR), and operated by the Department of Research Computing of the USIT, the IT department of the University of Oslo."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "The sigmorphon 2016 shared task: Morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Mor-", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton."], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "CoRR abs/1611.01874.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "The results of the SIGMORPHON 2016 shared task (Cotterell et al., 2016) indicated that the atten-", "startOffset": 47, "endOffset": 71}, {"referenceID": 4, "context": "(2014) is very suitable for this task (Kann and Sch\u00fctze, 2016), so we use this framework as the basis of our model.", "startOffset": 38, "endOffset": 62}, {"referenceID": 0, "context": "tional sequence-to-sequence model of Bahdanau et al. (2014) is very suitable for this task (Kann and Sch\u00fctze, 2016), so we use this framework as the basis of our model.", "startOffset": 37, "endOffset": 60}, {"referenceID": 6, "context": "A recent trend in neural machine translation is to use back-translated text (Sennrich et al., 2016) as a way to benefit from additional monolingual data in the target language.", "startOffset": 76, "endOffset": 99}, {"referenceID": 8, "context": "encourages solutions that can be translated back to their original (Tu et al., 2016).", "startOffset": 67, "endOffset": 84}, {"referenceID": 0, "context": "Our system is based on the attentional sequenceto-sequence model of Bahdanau et al. (2014) with Long Short-Term Memory (LSTM) cells", "startOffset": 68, "endOffset": 91}, {"referenceID": 3, "context": "(Hochreiter and Schmidhuber, 1997) and variational dropout Gal and Ghahramani (2016).", "startOffset": 0, "endOffset": 34}, {"referenceID": 2, "context": "(Hochreiter and Schmidhuber, 1997) and variational dropout Gal and Ghahramani (2016). The main innovation is that our inflection model is trained jointly with the reverse process, that is, lemmatization and morphological analysis.", "startOffset": 59, "endOffset": 85}, {"referenceID": 7, "context": "Our implementation is based on the Chainer library (Tokui et al., 2015) and available at github.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": "For optimization, we use Adam (Kingma and Ba, 2015) with default parameters.", "startOffset": 30, "endOffset": 51}], "year": 2017, "abstractText": "This paper describes the Stockholm University/University of Groningen (SURUG) system for the SIGMORPHON 2017 shared task on morphological inflection. Our system is based on an attentional sequence-to-sequence neural network model using Long Short-Term Memory (LSTM) cells, with joint training of morphological inflection and the inverse transformation, i.e. lemmatization and morphological analysis. Our system outperforms the baseline with a large margin, and our submission ranks as the 4th best team for the track we participate in (task 1, high-resource).", "creator": "LaTeX with hyperref package"}}}