{"id": "1706.03542", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Exploring the Syntactic Abilities of RNNs with Multi-task Learning", "abstract": "Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models.", "histories": [["v1", "Mon, 12 Jun 2017 10:00:47 GMT  (91kb,D)", "http://arxiv.org/abs/1706.03542v1", "To appear in CoNLL 2017"]], "COMMENTS": "To appear in CoNLL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["emile enguehard", "yoav goldberg", "tal linzen"], "accepted": false, "id": "1706.03542"}, "pdf": {"name": "1706.03542.pdf", "metadata": {"source": "CRF", "title": "Exploring the Syntactic Abilities of RNNs with Multi-task Learning", "authors": ["\u00c9mile Enguehard", "Yoav Goldberg", "Tal Linzen"], "emails": ["emile.enguehard@ens.fr", "tal.linzen@ens.fr", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "2 Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Agreement Prediction", "text": "Contemporary English verbs of the third person are numerically identical to their subject: singular subjects require singular verbs (the boy smiles) and plural subjects require plural verbs (the boys smiles); subjects in English are not marked openly, and complex sentences often have multiple subjects that correspond to different verbs; determining that the subject of the verb in bold face is a banner and not the singular adjector in sentences that have multiple nouns; (1) The only championship banners currently displayed within the building are for national or NCAA champions; determining that the subject of the verb in bold face is a banner and not the singular adjector and building requires an understanding of the structure of the sentence; in the match task, the learner is given the words that lead to a verb (a \"preamble\"), and it is instructed to predict whether this object in the bold face is the banner group, not the adjector and the adjector structure of the building."}, {"heading": "2.2 CCG Supertagging", "text": "Combinatorial Categorial Grammar (CCG) is a syntactic formalism based on a large inventory of lexical categories (Steedman, 2000), known as supertags, which can be considered a fine-grained extension of common speech building blocks. Transitive verbs (smiles), transitive verbs (structure), and survey verbs (appear) have different names: S\\ NP, (S\\ NP) / NP, and (S\\ NP) / (S\\ NP). CCG parsers typically rely on a supertagging step, where each word in a sentence is associated with a matching tag. In fact, supertagging is almost as difficult to find as the complete CCG parse of the sentence: once the supertags are determined, only a small number of parses are possible. At the same time, supertagging is easy to set up as a machine learning problem, since it amounts to a simple classification problem for each word."}, {"heading": "2.3 Language Modeling", "text": "The aim of a language model is to learn the distribution of the j \u2212 1 word in a previous sentence. We try to minimize the mean negative log probability of all sentences si = wi, 1.. wi, ni in our data: L (p) = \u2212 1 Z N \u00b2 i = 1 N = 1 logp \u00b2 (wi, j | wi, 1: j \u2212 1) (1), where Z = 1 Ni = 1 Ni. Voice modeling performance is often quantified by the perplexity 2L (p). The effectiveness of RNNs in speech modeling, especially LSTMs, has been demonstrated in numerous studies (Mikolov et al., 2010; Sundermeyer et al., 2012; Jozefowicz et al., 2016)."}, {"heading": "2.4 Multitask Learning", "text": "The benefits of multi-task learning in neural networks are obvious; neural networks often require a large amount of training data to achieve good performance on a task, and even with a significant amount of training data, the signal may be too sparse to pick up given its weak inductive distortions. By training a network on a simple task for which large amounts of data are available, we can encourage it to develop representations that would support its performance in the primary task (Caruana, 1998; Bakker and Heskes, 2003).This logic has been applied to various NLP tasks, with generally encouraging results (Collobert and Weston, 2008; Hashimoto et al., 2016; S\u00f8gaardand Goldberg, 2016; Mart\u00ednez Alonso and Plank, 2017; Bingel and S\u00f8gaard, 2017)."}, {"heading": "3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Datasets", "text": "We used two training datasets, the first being the corpus of about 1.5 million sentences from Wikipedia, compiled by Linzen et al. (2016). All sentences contained no more than 50 words and contained at least one third-person concurrence dependency. According to Linzen et al. (2016), we replaced rare words with their part-of-speech tag sentence using the Penn Treebank tag sentence (Marcus et al., 1993).1The second data set we used was the CCG Bank (Hockenmaier and Steedman, 2007), a CCG version of Penn Treebank. This corpus contained 48,934 English sentences, of which 27299 contain a concurrent- dependent third-person verb congruence dependency. A negligible number of sentences longer than 90 words was removed. We applied the traditional split, in which sections 2-21 are used for training and section 23 for tests (out of 12407 and 1224.132 respectively, the supertags we tried to replace only the fewest of the supertags)."}, {"heading": "3.2 Model", "text": "The model in all our experiments was a standard single layer LSTM.3 The first layer was a vector embedding of word marks into the D-dimensional space. The second layer was a D-dimensional LSTM. The following layers depended on the task. The initial levels consisted of a linear layer with a one-dimensional output and a sigma activation. The second layer was a linear layer with a N-dimensional output, where N is the size of the lexicon and a softmax activation. The third layer with an S-dimensional activation is a linear layer with an S-dimensional activation."}, {"heading": "3.3 Training", "text": "All neural networks have been implemented in Keras (Chollet, 2015) and Theano (Theano Development Team, 2016). We use the AdaGrad Optimizer. We use batch training with batch sizes 128 for speech modeling experiments and 256 for supertagging experiments for supertagging."}, {"heading": "4 Agreement and Supertagging", "text": "For the supertagging experiments, we used the full CCG corpus and 30% of the Wikipedia corpus for the match task (20% for training and 10% for testing), we trained the model for 20 epochs, the accuracy figures we report are averaged over three runs, we set the size of Network D to 500 hidden units, 4 we conducted a single talk experiment in each direction, as well as four joint training experiments, setting the weight r of the match task at 0.1, 1, 10, or 100. We considered two basic lines for the match task: the last noun predicts the number of verbs based on the number of the most recent noun, and the majority ground line always predicts a singular verb (singular verbs are more common than plural verbs in our corpus). Our base line for the supertagging was a majority line predicting the most common supertagural for each word, and the most common unification task (the singular verbs are more common)."}, {"heading": "4.1 Overall Results", "text": "Figure 1 shows the overall results of the experiment. Multi-task training with supertagging significantly improved the overall accuracy of the agreement task (from 2.04% to 1.24%), either through pre-training or joint training: relative to the individual task, the error rate decreased by up to 40% (from 2.04% to 1.24%). Conversely, multi-task training with agreement did not improve supertagging accuracy either in the pre-training system or in the joint training regime; supertagging accuracy reduced the higher weight of the agreement task (Figure 1b). Comparing the two multi-task tutorials, both the pre-training results and the joint training setup resulted in the optimal r. In the subsequent supertagging experiments, we did without the joint training setup, which is time-dependent."}, {"heading": "4.2 Effect of Corpus Size", "text": "To further investigate the relative contribution of the two supervision signals, we conducted a series of follow-up experiments in the pre-training setup, using subsets of different sizes of both corpora. Since POS tags contain less syntactical information than CCG supertags, we expect them to be less helpful as an auxiliary task. Penn Treebank POS tags distinguish singular and plural terms, but CCG supertags do not. To treat the two tasks in the same way, we removed them as auxiliary tasks. Penn Treebank labels distinguish singular and verbal terms, whereby CCG supertags do not matter."}, {"heading": "4.3 Attraction Errors", "text": "Most sentences are syntactically simple and do not pose any particular challenges to the models: the accuracy of the last noun in Figure 1a was close to 95%. To examine the behavior of the model in more difficult sentences, we next break down our test sentences by the number of match attractors (see Section 2.1). Our results shown in Figure 3 confirm that attractors make the match task more difficult and that pre-training helps to overcome this difficulty. This effect is compounded when we use only a small part of the match corpus. In this scenario, the accuracy of the one-task model for sentences with four attractors is only 20.4%. Pre-training makes it possible to overcome this difficulty substantially (though not completely) by increasing the accuracy to 40.1% in the case of POS tagging and to 51.2% in the case of supertagging, suggesting that a network that has developed sophisticated syntactical representations can transfer its knowledge to a new set of only moderate tactical tasks."}, {"heading": "4.4 Relative Clauses", "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move, to move and to move."}, {"heading": "5 Agreement and Language Modeling", "text": "We now turn to the task of language modeling. Previous experiments confirmed that the agreement in sentences without attractors is easily predictable. Therefore, in the language modeling experiments we limited ourselves to sentences with potential attractors. Specifically, we trained our language model within the subset of 30% of the Wikipedia corpus only on sentences with at least one noun (any number) between subject and verb. There were 60680 sentences in the training set. We obtained our results over three runs. The training was interrupted after 10 epochs, and the number of hidden units was set to D = 50."}, {"heading": "5.1 Overall Results", "text": "The overall results are shown in Figure 6. Joint training with the LM task significantly improves the performance of the match task by increasing the accuracy from 90.2% to 92.6% (a relative reduction of the error rate by 25%), possibly due to the higher quality of the word representations that can be learned from the speech modeling signal, which in turn helps the model to make more accurate syntactical predictions. In the other direction, we do not see any clear improvements in helplessness when we train the LM with consent. Surprisingly, the visual inspection of Figure 6b suggests that the jointly trained LM can perform slightly better than the simple task basis for small values of r (i.e. if the match task has a small effect on the overall training loss).To assess the statistical significance of this difference, we repeated the standard experiment with r = 0.01 with 20 random standard initializations, which is an average of 0.0005 M, which is about the difference of 0.11."}, {"heading": "5.2 Grammaticality of LM Predictions", "text": "To evaluate the syntactical abilities of a language model trained RNN, Linzen et al. (2016) proposed to perform the match task by comparing the probability under the learned LM of correct and false verb forms, assuming that all other things that are equal to a grammatical sequence should have a higher probability than an ungrammatic one (Lau et al., 2016; Le Godais et al.). For example, if the sentence begins with the dogs, we calculate: p-correct = p-2 = are the dogs) p-1 p-2 =. + p-3 The prediction for the match task is derived from p-3."}, {"heading": "6 Conclusions", "text": "Previous work has shown that the syntactic representations developed by the RNNs trained on the complex prediction tasks are sufficient for the majority of the sentences but break down into more complex sentences (Linzen et al., 2016, 2017). Alternatively, they may be due to insufficient control signals in the prediction task, for example, because relative clauses with similarities in a natural corpus are rare. We have shown that additional oversight of syntactical meeting tasks such as CCG meetings can contribute to the agreement being incapable of prediction task because relative clauses with similarities are rare."}, {"heading": "Acknowledgments", "text": "We thank Emmanuel Dupoux for the discussion. This research was supported by the European Research Council (ERC-2011-AdG 295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-IDEX-0001-02 PSL and ANR10-LABX-0087 IEC) and the Israeli Science Foundation (grant number 1555 / 15)."}, {"heading": "A Appendix", "text": "Figure 8 follows the word-by-word predictions made by the single problem model and the pre-trained model for three sample sentences; the basic grammatical truth is marked with a dotted black line. Overall, the pre-trained model is closer to the basic truth than the single problem model, even in cases where both models ultimately make the correct prediction (Figure 8b). Figure 8a and 8c show cases where an attractor in an embedded sentence misleads the single problem, but not the single problem model. Finally, Figure 9 shows a sample of four units that seem to reconstruct interpretable aspects of the sentence."}], "references": [{"title": "Task clustering and gating for Bayesian multitask learning", "author": ["Bart Bakker", "Tom Heskes."], "venue": "Journal of Machine Learning Research 4:83\u201399.", "citeRegEx": "Bakker and Heskes.,? 2003", "shortCiteRegEx": "Bakker and Heskes.", "year": 2003}, {"title": "Supertagging: An approach to almost parsing", "author": ["Srinivas Bangalore", "Aravind K. Joshi."], "venue": "Computational Linguistics 25(2):237\u2013265.", "citeRegEx": "Bangalore and Joshi.,? 1999", "shortCiteRegEx": "Bangalore and Joshi.", "year": 1999}, {"title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "author": ["Joachim Bingel", "Anders S\u00f8gaard."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short", "citeRegEx": "Bingel and S\u00f8gaard.,? 2017", "shortCiteRegEx": "Bingel and S\u00f8gaard.", "year": 2017}, {"title": "Regulating mental energy: Performance units in language production", "author": ["Kathryn Bock", "J. Cooper Cutting."], "venue": "Journal of Memory and Language 31(1):99\u2013127.", "citeRegEx": "Bock and Cutting.,? 1992", "shortCiteRegEx": "Bock and Cutting.", "year": 1992}, {"title": "Reaching agreement", "author": ["Kathryn Bock", "Erica L. Middleton."], "venue": "Natural Language & Linguistic Theory 29(4):1033\u20131069.", "citeRegEx": "Bock and Middleton.,? 2011", "shortCiteRegEx": "Bock and Middleton.", "year": 2011}, {"title": "Broken agreement", "author": ["Kathryn Bock", "Carol A. Miller."], "venue": "Cognitive Psychology 23(1):45\u201393.", "citeRegEx": "Bock and Miller.,? 1991", "shortCiteRegEx": "Bock and Miller.", "year": 1991}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Sebastian Thrun and Lorien Pratt, editors, Learning to learn, Kluwer Academic Publishers, Boston, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning. New York, NY, USA, pages", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "A. Noah Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L. Elman."], "venue": "Machine Learning 7(2-3):195\u2013225.", "citeRegEx": "Elman.,? 1991", "shortCiteRegEx": "Elman.", "year": 1991}, {"title": "Subject-verb agreement errors in French and English: The role of syntactic hierarchy", "author": ["Julie Franck", "Gabriella Vigliocco", "Janet Nicol."], "venue": "Language and Cognitive Processes 17(4):371\u2013404.", "citeRegEx": "Franck et al\\.,? 2002", "shortCiteRegEx": "Franck et al\\.", "year": 2002}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems 28. pages 1828\u20131836.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "A joint many-task model: Growing a neural network for multiple NLP tasks", "author": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher."], "venue": "NIPS 2016 Continual Learning and Deep Networks Workshop.", "citeRegEx": "Hashimoto et al\\.,? 2016", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2016}, {"title": "CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank", "author": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics 33(3):355\u2013396.", "citeRegEx": "Hockenmaier and Steedman.,? 2007", "shortCiteRegEx": "Hockenmaier and Steedman.", "year": 2007}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Grammaticality, acceptability, and probability: A probabilistic view of linguistic knowledge", "author": ["Jey Han Lau", "Alexander Clark", "Shalom Lappin."], "venue": "Cognitive Science .", "citeRegEx": "Lau et al\\.,? 2016", "shortCiteRegEx": "Lau et al\\.", "year": 2016}, {"title": "Comparing character-level neural language models using a lexical decision task", "author": ["Ga\u00ebl Le Godais", "Tal Linzen", "Emmanuel Dupoux."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Vol-", "citeRegEx": "Godais et al\\.,? 2017", "shortCiteRegEx": "Godais et al\\.", "year": 2017}, {"title": "LSTM CCG parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pages 221\u2013231.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:521\u2013 535.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Agreement attraction errors in neural networks", "author": ["Tal Linzen", "Yoav Goldberg", "Emmanuel Dupoux."], "venue": "Proceedings of the CUNY Conference on Human Sentence Processing.", "citeRegEx": "Linzen et al\\.,? 2017", "shortCiteRegEx": "Linzen et al\\.", "year": 2017}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational Linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "When is multitask learning effective? Semantic sequence prediction under varying data conditions", "author": ["H\u00e9ctor Mart\u00ednez Alonso", "Barbara Plank."], "venue": "Proceedings of the Conference of the European Chapter of the Association for Computationl Lin-", "citeRegEx": "Alonso and Plank.,? 2017", "shortCiteRegEx": "Alonso and Plank.", "year": 2017}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Proceedings of Interspeech.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Recursive Deep Learning for Natural Language Processing and Computer Vision", "author": ["Richard Socher."], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Socher.,? 2014", "shortCiteRegEx": "Socher.", "year": 2014}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "On the interpretation of the number attraction effect: Response time evidence", "author": ["Adrian Staub."], "venue": "Journal of Memory and Language 60(2):308\u2013327.", "citeRegEx": "Staub.,? 2009", "shortCiteRegEx": "Staub.", "year": 2009}, {"title": "The syntactic process", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Proceedings of the 13th Annual Conference of the International Speech Communication Association (INTERSPEECH). pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Supertagging with LSTMs", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of NAACL-HLT . pages 232\u2013237.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Agreement attraction in comprehension: Representations and processes", "author": ["Matthew W. Wagers", "Ellen F. Lau", "Colin Phillips."], "venue": "Journal of Memory and Language 61(2):206\u2013237.", "citeRegEx": "Wagers et al\\.,? 2009", "shortCiteRegEx": "Wagers et al\\.", "year": 2009}, {"title": "CCG supertagging with a recurrent neural network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Probability of a plural prediction after each word in the sentence for three sample sentences. The black dotted line indicates the grammatical ground truth", "author": ["Wagers"], "venue": null, "citeRegEx": "Wagers,? \\Q2009\\E", "shortCiteRegEx": "Wagers", "year": 2009}], "referenceMentions": [{"referenceID": 19, "context": "RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016).", "startOffset": 81, "endOffset": 102}, {"referenceID": 5, "context": "(2016) have recently applied this methodology to evaluate how well a trained RNN captures sentence structure, using the agreement prediction task (Bock and Miller, 1991; Elman, 1991).", "startOffset": 146, "endOffset": 182}, {"referenceID": 10, "context": "(2016) have recently applied this methodology to evaluate how well a trained RNN captures sentence structure, using the agreement prediction task (Bock and Miller, 1991; Elman, 1991).", "startOffset": 146, "endOffset": 182}, {"referenceID": 1, "context": "To provide the RNN with an incentive to develop more sophisticated representations, we trained it to perform one of two tasks: the first is combinatory categorical grammar (CCG) supertagging (Bangalore and Joshi, 1999), a sequence labeling task likely to require robust syntactic representations; the second task is language modeling.", "startOffset": 191, "endOffset": 218}, {"referenceID": 16, "context": "Linzen et al. (2016) have recently applied this methodology to evaluate how well a trained RNN captures sentence structure, using the agreement prediction task (Bock and Miller, 1991; Elman, 1991).", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": "This suggests that RNNs are in principle capable of acquiring much better syntactic representations than those they learned from the corpus in Linzen et al. (2016). In the other direction, joint training on the agreement prediction task did not improve overall language model perplexity, but made the model more syntax-aware: grammatically appropriate verb forms had higher probability than grammatically inappropriate ones.", "startOffset": 143, "endOffset": 164}, {"referenceID": 5, "context": "This task is modeled after a standard psycholinguistic task, which is used to study syntactic representations in humans (Bock and Miller, 1991; Franck et al., 2002; Staub, 2009; Bock and Middleton, 2011).", "startOffset": 120, "endOffset": 203}, {"referenceID": 11, "context": "This task is modeled after a standard psycholinguistic task, which is used to study syntactic representations in humans (Bock and Miller, 1991; Franck et al., 2002; Staub, 2009; Bock and Middleton, 2011).", "startOffset": 120, "endOffset": 203}, {"referenceID": 26, "context": "This task is modeled after a standard psycholinguistic task, which is used to study syntactic representations in humans (Bock and Miller, 1991; Franck et al., 2002; Staub, 2009; Bock and Middleton, 2011).", "startOffset": 120, "endOffset": 203}, {"referenceID": 4, "context": "This task is modeled after a standard psycholinguistic task, which is used to study syntactic representations in humans (Bock and Miller, 1991; Franck et al., 2002; Staub, 2009; Bock and Middleton, 2011).", "startOffset": 120, "endOffset": 203}, {"referenceID": 27, "context": "Combinatory Categorial Grammar (CCG) is a syntactic formalism that relies on a large inventory of lexical categories (Steedman, 2000).", "startOffset": 117, "endOffset": 133}, {"referenceID": 1, "context": "At the same time, supertagging is simple to set up as a machine learning problem, since at each word it amounts to a straightforward classification problem (Bangalore and Joshi, 1999).", "startOffset": 156, "endOffset": 183}, {"referenceID": 32, "context": "RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016).", "startOffset": 72, "endOffset": 131}, {"referenceID": 18, "context": "RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016).", "startOffset": 72, "endOffset": 131}, {"referenceID": 30, "context": "RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016).", "startOffset": 72, "endOffset": 131}, {"referenceID": 14, "context": "In contrast with the agreement task, training data for supertagging needs to be obtained from parsed sentences which require expert annotation (Hockenmaier and Steedman, 2007); the amount of training data is therefore limited even in English, and much more sparse in other languages.", "startOffset": 143, "endOffset": 175}, {"referenceID": 23, "context": "The effectiveness of RNNs in language modeling, in particular LSTMs, has been demonstrated in numerous studies (Mikolov et al., 2010; Sundermeyer et al., 2012; Jozefowicz et al., 2016).", "startOffset": 111, "endOffset": 184}, {"referenceID": 28, "context": "The effectiveness of RNNs in language modeling, in particular LSTMs, has been demonstrated in numerous studies (Mikolov et al., 2010; Sundermeyer et al., 2012; Jozefowicz et al., 2016).", "startOffset": 111, "endOffset": 184}, {"referenceID": 15, "context": "The effectiveness of RNNs in language modeling, in particular LSTMs, has been demonstrated in numerous studies (Mikolov et al., 2010; Sundermeyer et al., 2012; Jozefowicz et al., 2016).", "startOffset": 111, "endOffset": 184}, {"referenceID": 6, "context": "By training a network on a simple task for which large quantities of data are available, we can encourage it to evolve representations that would help its performance on the primary task (Caruana, 1998; Bakker and Heskes, 2003).", "startOffset": 187, "endOffset": 227}, {"referenceID": 0, "context": "By training a network on a simple task for which large quantities of data are available, we can encourage it to evolve representations that would help its performance on the primary task (Caruana, 1998; Bakker and Heskes, 2003).", "startOffset": 187, "endOffset": 227}, {"referenceID": 8, "context": "This logic has been applied to various NLP tasks, with generally encouraging results (Collobert and Weston, 2008; Hashimoto et al., 2016; S\u00f8gaard and Goldberg, 2016; Mart\u00ednez Alonso and Plank, 2017; Bingel and S\u00f8gaard, 2017).", "startOffset": 85, "endOffset": 224}, {"referenceID": 13, "context": "This logic has been applied to various NLP tasks, with generally encouraging results (Collobert and Weston, 2008; Hashimoto et al., 2016; S\u00f8gaard and Goldberg, 2016; Mart\u00ednez Alonso and Plank, 2017; Bingel and S\u00f8gaard, 2017).", "startOffset": 85, "endOffset": 224}, {"referenceID": 25, "context": "This logic has been applied to various NLP tasks, with generally encouraging results (Collobert and Weston, 2008; Hashimoto et al., 2016; S\u00f8gaard and Goldberg, 2016; Mart\u00ednez Alonso and Plank, 2017; Bingel and S\u00f8gaard, 2017).", "startOffset": 85, "endOffset": 224}, {"referenceID": 2, "context": "This logic has been applied to various NLP tasks, with generally encouraging results (Collobert and Weston, 2008; Hashimoto et al., 2016; S\u00f8gaard and Goldberg, 2016; Mart\u00ednez Alonso and Plank, 2017; Bingel and S\u00f8gaard, 2017).", "startOffset": 85, "endOffset": 224}, {"referenceID": 21, "context": "(2016), we replaced rare words by their part-ofspeech tags, using the Penn Treebank tag set (Marcus et al., 1993).", "startOffset": 92, "endOffset": 113}, {"referenceID": 19, "context": "5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had at most 50 words and contained at least one third-person present-tense agreement dependency.", "startOffset": 59, "endOffset": 80}, {"referenceID": 19, "context": "5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had at most 50 words and contained at least one third-person present-tense agreement dependency. Following Linzen et al. (2016), we replaced rare words by their part-ofspeech tags, using the Penn Treebank tag set (Marcus et al.", "startOffset": 59, "endOffset": 223}, {"referenceID": 14, "context": "The second data set we used is the CCG-Bank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank.", "startOffset": 44, "endOffset": 76}, {"referenceID": 7, "context": "All neural networks were implemented in Keras (Chollet, 2015) and Theano (Theano Development Team, 2016).", "startOffset": 46, "endOffset": 61}, {"referenceID": 32, "context": "Since our model did not have access to the right context of a word when determining its supertag, we could not expect to compete with stateof-the-art taggers that use right-context lookahead (Xu et al., 2015) or even bidirectional RNNs that read the entire sentence from right to left (Vaswani et al.", "startOffset": 191, "endOffset": 208}, {"referenceID": 30, "context": ", 2015) or even bidirectional RNNs that read the entire sentence from right to left (Vaswani et al., 2016; Lewis et al., 2016); we therefore did not compare our accuracy to these taggers.", "startOffset": 84, "endOffset": 126}, {"referenceID": 18, "context": ", 2015) or even bidirectional RNNs that read the entire sentence from right to left (Vaswani et al., 2016; Lewis et al., 2016); we therefore did not compare our accuracy to these taggers.", "startOffset": 84, "endOffset": 126}, {"referenceID": 19, "context": "In Linzen et al. (2016), attraction errors were particularly severe when the attractor was inside a relPrositional / SP Prositional / PS Rlative / SP Rlative / PS 0.", "startOffset": 3, "endOffset": 24}, {"referenceID": 3, "context": "Figure 4: Accuracy on sentences from Bock and Cutting (1992). Error bars indicate standard deviation across runs.", "startOffset": 37, "endOffset": 61}, {"referenceID": 20, "context": "To gain a more precise understanding of the errors and the extent to which pre-training can mitigate them, we turn to two sets of carefully constructed sentences from the psycholinguistic literature (Linzen et al., 2017).", "startOffset": 199, "endOffset": 220}, {"referenceID": 3, "context": "Bock and Cutting (1992) compared preambles with prepositional phrase modifiers to closely matched relative clause modifiers:", "startOffset": 0, "endOffset": 24}, {"referenceID": 31, "context": "Our second set of sentences was based on the experimental materials of Wagers et al. (2009). We adapted them by deleting the relativizer and creating two preambles from each sentence in the original experiment:", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "The relative improvement of the pre-trained model compared to the single-task one is more modest in these sentences, possibly because the single-task model does better to begin with on these sentences than on the Bock and Cutting (1992) ones.", "startOffset": 213, "endOffset": 237}, {"referenceID": 3, "context": "The relative improvement of the pre-trained model compared to the single-task one is more modest in these sentences, possibly because the single-task model does better to begin with on these sentences than on the Bock and Cutting (1992) ones. This in turn may be because the attractor immediately precedes the verb in Bock and Cutting (1992) but not in Wagers et al.", "startOffset": 213, "endOffset": 342}, {"referenceID": 3, "context": "The relative improvement of the pre-trained model compared to the single-task one is more modest in these sentences, possibly because the single-task model does better to begin with on these sentences than on the Bock and Cutting (1992) ones. This in turn may be because the attractor immediately precedes the verb in Bock and Cutting (1992) but not in Wagers et al. (2009), and an immediately adjacent noun may be a stronger attractor.", "startOffset": 213, "endOffset": 374}, {"referenceID": 31, "context": "Figure 5: Accuracy on sentences based on Wagers et al. (2009). Error bars indicate standard deviation across runs.", "startOffset": 41, "endOffset": 62}, {"referenceID": 16, "context": "(2016) proposed to perform the agreement task by comparing the probability under the learned LM of the correct and incorrect verb forms, under the assumption that all other things being equal a grammatical sequence should have a higher probability than an ungrammatical one (Lau et al., 2016; Le Godais et al., 2017).", "startOffset": 274, "endOffset": 316}, {"referenceID": 17, "context": "To evaluate the syntactic abilities of an RNN trained as a language model, Linzen et al. (2016) proposed to perform the agreement task by comparing the probability under the learned LM of the correct and incorrect verb forms, under the assumption that all other things being equal a grammatical sequence should have a higher probability than an ungrammatical one (Lau et al.", "startOffset": 75, "endOffset": 96}, {"referenceID": 24, "context": "tectures (Socher, 2014; Grefenstette et al., 2015; Dyer et al., 2016).", "startOffset": 9, "endOffset": 69}, {"referenceID": 12, "context": "tectures (Socher, 2014; Grefenstette et al., 2015; Dyer et al., 2016).", "startOffset": 9, "endOffset": 69}, {"referenceID": 9, "context": "tectures (Socher, 2014; Grefenstette et al., 2015; Dyer et al., 2016).", "startOffset": 9, "endOffset": 69}], "year": 2017, "abstractText": "Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multitask training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models.", "creator": "LaTeX with hyperref package"}}}