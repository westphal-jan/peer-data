{"id": "1706.03607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Clustering over Multi-Objective Samples: The one2all Sample", "abstract": "Clustering is a fundamental technique in data analysis. Consider data points $X$ that lie in a (relaxed) metric space (where the triangle inequality can be relaxed by a constant factor). Each set of points $Q$ ({\\em centers}) defines a clustering of $X$ according to the closest center with {\\em cost} $V(Q)=\\sum_{x\\in X} d_{xQ}$. This formulation generalizes classic $k$-means clustering, which uses squared distances. Two basic tasks, parametrized by $k \\geq 1$, are {\\em cost estimation}, which returns (approximate) $V(Q)$ for queries $Q$ such that $|Q|=k$ and {\\em clustering}, which returns an (approximate) minimizer of $V(Q)$ of size $|Q|=k$. With very large data sets $X$, we seek efficient constructions of small summaries that allow us to efficiently approximate clustering costs over the full data.", "histories": [["v1", "Mon, 12 Jun 2017 13:05:46 GMT  (19kb,D)", "http://arxiv.org/abs/1706.03607v1", "10 pages, 1 figure"], ["v2", "Sun, 29 Oct 2017 10:27:20 GMT  (76kb,D)", "http://arxiv.org/abs/1706.03607v2", "17 pages, 2 figure"]], "COMMENTS": "10 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["edith cohen", "shiri chechik", "haim kaplan"], "accepted": false, "id": "1706.03607"}, "pdf": {"name": "1706.03607.pdf", "metadata": {"source": "CRF", "title": "Clustering over Multi-Objective Samples: The one2all Sample", "authors": ["Edith Cohen", "Shiri Chechik", "Haim Kaplan"], "emails": [], "sections": [{"heading": null, "text": "Each set of points Q (centers) defines a clustering of X according to the nearest center with cost V (Q) = \u2211 x-X dxQ. This formulation generalizes classic k-mean clustering using square distances. Two basic tasks parameterized by k \u2265 1 are cost estimates that provide (approximate) V (Q) for queries Q. We present a novel tool for data reduction based on multi-objective probability ratios relative to size (Q) of size | Q | = k. With very large data sets X, we seek efficient constructions of small summaries that allow us to efficiently approximate cluster costs across the full data."}, {"heading": "1 Introduction", "text": "We have a set X of data points that are in a (relaxed) Q-Q cost question (Q = Q = Q = Q = Q Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "2 Multi-objective pps samples for clustering", "text": "Consider the framework of the weighted and multi-objectively weighted sample in our context of cluster costs. Consider the approximation of cluster costs V (Q | X, w) from a sample S of X. For probabilities px > 0 for x-X and a sample S drawn according to these probabilities, we have the unbiased reverse probability estimator [19] of V (Q | X, w): V-V (Q | X, w) = \u2211 x-S wx dxQ px = V (Q | S, {wx / px}). (3) Note that the estimate corresponds to the cluster costs of S with the weights wx / px of Q."}, {"heading": "2.1 Probability proportional to size (pps) sampling", "text": "[17] To obtain guarantees for the estimated quality of cluster costs according to Q, we must use weighted samples. (4) The pp base probabilities for a sample with the size parameter r > 1 are x-x (Q-X, w) = wxdxQ-y-X-wydyQ. (4) The pp probabilities for a sample with the size parameter r > 1 are x-x (Q-X, w) = min {1, rB-x (Q-X, w). For the pp sample, we obtain the following guarantees: Theorem 2.1 Consider a sample S where each x-X (independent or with VarOpt-dependent sample [7, 12]) is recorded with the probability of px-X (Q-X, w). Then, the estimate (3) has the following statistical guarantees: \u2022 The coefficient of variation (CV), defined as the ratio of the standard deviation to the mean, (measure of the \"relative error\") x-w (Q-X, w) is maximum."}, {"heading": "2.2 Multi-objective pps sampling", "text": "If we aim for estimates with statistical guarantees for a number of Q-queries (for example, all sets of K-points in metric space M), we use multi-objective samples [13, 11]. The multi-objective (MO) pp base sample probabilities are defined as the maximum of pp base probabilities over Q-Q: x (Q-X, w) = max Q-Q-Q-x (Q-X, w). (5) For a size parameter r, the multi-objective ps probabilities are defined as the maximum (r) x (Q-X, w) = min {1, rp x (Q-X, w)} = max Q-Q-Q-Q-Q-x (Q-X, w)."}, {"heading": "2.3 Optimization over multi-objective pps samples", "text": "We are now looking at clustering, with an approximate or bicriteria-approximate goal. We would like to perform this optimization by using a clustering algorithm A on sample S with weights wx / px. A multi-objective sample for Q with the size parameter r = \u2212 2 provides ForEach warranties. This warranty is not sufficient to ensure that Q of A meets the quality requirements of A, even approximately, above X, but it allows us to test this using the following samples: V (Q | X, w) \u2264 (1 +) V (Q | S, {wx / px}). (6) In order to perform the test, we can calculate the exact cost V (Q | X, w). Alternatively, we can simply draw another independent sample S \u00b2 using the same distribution and calculate the estimate V (Q | S, {wx / px}). If the test is met, we know (within the warranties and approximate quantity of Q-S, Q-Q) that a Q / S approximate solution results in Q (Q)."}, {"heading": "3 The one2all Theorem statement and implications", "text": "Consider a relaxed metric spaceM, in which distances satisfy all the properties of a metric space, except that the triangular inequality is loosened using a parameter. (7) For v > 0, we use the notation Q (v) = [Q | V] = [Q | X, w] for the set of all subsets Q-M that cost at least v. We assume that Xq = [x] X | dxq = dxM} represent the points in X that come closest to q-M. In the case of equilibriums, we apply arbitrary equations to ensure that Xq-M is a partition of X. We assume that Xq-X is not empty for all q-M, otherwise we can remove the point q from M without affecting the grouping costs of X by M. For the set M, we define the probabilities of M."}, {"heading": "4 Proof of the one2all Theorem", "text": "To prove this, we must show that Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "5 Conclusion", "text": "We presented the one2all construction for cluster costs: From each group of centers M with cost V (M), we obtain a summary structure of size O (| M |) in the form of a multi-objective sample, with which we can estimate cluster costs based on any set Q with costs that are at least a fraction of V (M). In the future, we point to some potential applications that go beyond estimating and optimizing cluster costs. First, we find that the set of distances from Q to the one2all sample S is essentially a sketch of the full (weighted) distance vector from Q to X [13]. Sketches of different sets Q allow us to estimate the relationships between the respective full vectors, such as distance standards, weighted Jaccard similarity, quantity storage aggregates, and more, which can be useful building blocks in other applications. Second, recent work has led clustering of Euclidean averages to a limited technical approximation problem [which could lead to a limited approximation problem]."}], "references": [{"title": "Geometric approximation via coresets", "author": ["P.K. Agarwal", "S. Har-Peled", "K.R. Varadarajan"], "venue": "Combinatorial and computational geometry, MSRI. University Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "NP-hardness of Euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Mach. Learn., 75(2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "The hardness of approximation of Euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "SoCG", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "New frameworks for offline and streaming coreset constructions", "author": ["V. Braverman", "D. Feldman", "H. Lang"], "venue": "CoRR, abs/1612.00889", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Selecting several samples from a single population", "author": ["K.R.W. Brewer", "L.J. Early", "S.F. Joyce"], "venue": "Australian Journal of Statistics, 14(3):231\u2013239", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1972}, {"title": "A general purpose unequal probability sampling plan", "author": ["M.T. Chao"], "venue": "Biometrika, 69(3):653\u2013656", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1982}, {"title": "Average distance queries through weighted samples in graphs and metric spaces: High scalability with tight statistical guarantees", "author": ["S. Chechik", "E. Cohen", "H. Kaplan"], "venue": "RANDOM. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "On coresets for k-median and k-means clustering in metric and Euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM J. Comput., 39(3)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Size-estimation framework with applications to transitive closure and reachability", "author": ["E. Cohen"], "venue": "J. Comput. System Sci., 55:441\u2013453", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Multi-objective weighted sampling", "author": ["E. Cohen"], "venue": "HotWeb. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient stream sampling for variance-optimal estimation of subset sums", "author": ["E. Cohen", "N. Duffield", "C. Lund", "M. Thorup", "H. Kaplan"], "venue": "SIAM J. Comput., 40(5)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Coordinated weighted sampling for estimating aggregates over multiple weight assignments", "author": ["E. Cohen", "H. Kaplan", "S. Sen"], "venue": "VLDB, 2(1\u20132)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["M.B. Cohen", "S. Elder", "C. Musco", "C. Musco", "M. Persu"], "venue": "STOC. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified framework for approximating and clustering data", "author": ["D. Feldman", "M. Langberg"], "venue": "STOC. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means", "author": ["D. Feldman", "M. Schmidt", "C. Sohler"], "venue": "PCA and projective clustering. In SODA. ACM-SIAM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "On the theory of sampling from finite populations", "author": ["M.H. Hansen", "W.N. Hurwitz"], "venue": "Ann. Math. Statist., 14(4)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1943}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "STOC. ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["D.G. Horvitz", "D.J. Thompson"], "venue": "Journal of the American Statistical Association, 47(260):663\u2013685", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1952}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Computational Geometry, 28(2)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Retaining units after changing strata and probabilities", "author": ["L. Kish", "A. Scott"], "venue": "Journal of the American Statistical Association, 66(335):pp. 461\u2013470", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1971}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Trans. Inf. Theor.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1982}, {"title": "Optimal time bounds for approximate clustering", "author": ["R.R. Mettu", "C.G. Plaxton"], "venue": "Mach. Learn., 56(1-3)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Fixed sample size pps approximations with a permanent random number", "author": ["P.J. Saavedra"], "venue": "Proc. of the Section on Survey Research Methods, pages 697\u2013700, Alexandria, VA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Model Assisted Survey Sampling", "author": ["C-E. S\u00e4rndal", "B. Swensson", "J. Wretman"], "venue": "Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}, {"title": "Sampling Algorithms", "author": ["Y. Till\u00e9"], "venue": "Springer-Verlag, New York", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "A constant-factor bi-criteria approximation guarantee for k-means++", "author": ["D. Wei"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Clustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4].", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Clustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 19, "context": "There is a local search polynomial algorithm with 9 + approximation ratio [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "In practice, clustering is solved using heuristics, most notably Lloyd\u2019s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers.", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "In practice, clustering is solved using heuristics, most notably Lloyd\u2019s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers.", "startOffset": 144, "endOffset": 147}, {"referenceID": 26, "context": "A recent result [27] established that kmeans++ provides constant factors bi-criteria guarantees: When applied to obtain \u03b2k centers for some constant \u03b2 > 1, the clustering cost is within a constant factor of the optimum k-means cost.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets.", "startOffset": 94, "endOffset": 101}, {"referenceID": 17, "context": "The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets.", "startOffset": 94, "endOffset": 101}, {"referenceID": 22, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 8, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 14, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 15, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 4, "context": "Initial coresets constructions had bounds with high (exponential or high polynomial) dependence on some parameters (dimension, , k) and poly logarithmic dependence on n, with the best current asymptotic bound of O(k \u22122 log k log n) claimed in [5].", "startOffset": 243, "endOffset": 246}, {"referenceID": 14, "context": "While a notion of \u201cweak coresets\u201d that aim to only support optimization was considered in the coreset literature [15], the constructions are also worst-case.", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 145, "endOffset": 153}, {"referenceID": 10, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 145, "endOffset": 153}, {"referenceID": 20, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 5, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 23, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 9, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 24, "context": "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample \u22122 points with probabilities px \u221d wxdQx proportional to their contribution to the sum [17].", "startOffset": 32, "endOffset": 40}, {"referenceID": 25, "context": "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample \u22122 points with probabilities px \u221d wxdQx proportional to their contribution to the sum [17].", "startOffset": 32, "endOffset": 40}, {"referenceID": 16, "context": "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample \u22122 points with probabilities px \u221d wxdQx proportional to their contribution to the sum [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "The inverse-probability [19] estimate obtained from the sample S, V\u0302 (Q | X,w) \u2261 V (Q | S, {wx/px}) , is an unbiased estimate of V (Q | X,w) with well-concentrated normalized squared error of .", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "Our result generalizes previous work [8] that only applied to the case where k = 1, where clustering cost reduces to inverse classic closeness centrality (sum of distances from a single point Q).", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant \u03b1 [27].", "startOffset": 59, "endOffset": 62}, {"referenceID": 26, "context": "To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant \u03b1 [27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "We use an optimization framework over multi-objective samples [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "For probabilities px > 0 for x \u2208 X and a sample S drawn according to these probabilities, we have the unbiased inverse probability estimator [19] of V (Q | X,w):", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "1 Probability proportional to size (pps) sampling [17] To obtain guarantees on the estimate quality of the clustering cost by Q, we need to use weighted sampling.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "1 Consider a sample S where each x \u2208 X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px \u2265 \u03c8 x (Q | X,w).", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": "1 Consider a sample S where each x \u2208 X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px \u2265 \u03c8 x (Q | X,w).", "startOffset": 101, "endOffset": 108}, {"referenceID": 12, "context": "2 Multi-objective pps sampling When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11].", "startOffset": 194, "endOffset": 202}, {"referenceID": 10, "context": "2 Multi-objective pps sampling When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11].", "startOffset": 194, "endOffset": 202}, {"referenceID": 0, "context": "// Initialization foreach x \u2208 X do // for sampling ux \u223c U [0, 1] r \u2190 \u22122 // Start with ForEach guarantee // Main Loop repeat S \u2190\u22a5 // Initialize sample foreach x \u2208 X such that ux \u2264 r\u03c0x do // sample points S \u2190 S \u222a {x} // Cluster the sample S foreach x \u2208 S do // weights for sampled points w\u2032 x \u2190 wx/min{1, r\u03c0x} Q\u2190 A(S,w) // Apply approximate clustering algorithm A to sample r \u2190 2r // Double the sample size parameter until V (Q | X,w)) \u2264 (1 + )V (Q | S,w\u2032) // Exact or approx using a validation sample", "startOffset": 58, "endOffset": 64}, {"referenceID": 12, "context": "First, we note that the set of distances of Q to the one2all sample S is essentially a sketch of the full (weighted) distance vector of Q to X [13].", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Second, recent work casted Euclidean k-means clustering as a constrained rank-k approximation problem [14].", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "Clustering is a fundamental technique in data analysis. Consider data points X that lie in a (relaxed) metric space (where the triangle inequality can be relaxed by a constant factor). Each set of points Q (centers) defines a clustering of X according to the closest center with cost V (Q) = \u2211 x\u2208X dxQ. This formulation generalizes classic k-means clustering, which uses squared distances. Two basic tasks, parametrized by k \u2265 1, are cost estimation, which returns (approximate) V (Q) for queries Q such that |Q| = k and clustering, which returns an (approximate) minimizer of V (Q) of size |Q| = k. With very large data sets X, we seek efficient constructions of small summaries that allow us to efficiently approximate clustering costs over the full data. We present a novel data reduction tool based on multi-objective probability-proportional-to-size (pps) sampling: Our one2all construction inputs any set of centers M and efficiently computes a sample of size O(|M |) from which we can tightly estimate the clustering cost V (Q) for any Q that has at least a fraction of the clustering cost of M . For cost queries, we apply one2all to a bicriteria approximation to obtain a sample of size O(k \u22122) for all |Q| = k. For clustering, we propose a wrapper that applies a black-box algorithm to a sample and tests clustering quality over X, adaptively increasing the sample size. Our approach exploits the structure of the data to provide quality guarantees through small samples, without the use of typically much larger worst-case-size summaries.", "creator": "LaTeX with hyperref package"}}}