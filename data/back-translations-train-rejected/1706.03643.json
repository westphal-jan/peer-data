{"id": "1706.03643", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Tackling Over-pruning in Variational Autoencoders", "abstract": "Variational autoencoders (VAE) are directed generative models that learn factorial latent variables. As noted by Burda et al. (2015), these models exhibit the problem of factor over-pruning where a significant number of stochastic factors fail to learn anything and become inactive. This can limit their modeling power and their ability to learn diverse and meaningful latent representations. In this paper, we evaluate several methods to address this problem and propose a more effective model-based approach called the epitomic variational autoencoder (eVAE). The so-called epitomes of this model are groups of mutually exclusive latent factors that compete to explain the data. This approach helps prevent inactive units since each group is pressured to explain the data. We compare the approaches with qualitative and quantitative results on MNIST and TFD datasets. Our results show that eVAE makes efficient use of model capacity and generalizes better than VAE.", "histories": [["v1", "Fri, 9 Jun 2017 10:13:00 GMT  (2271kb,D)", "http://arxiv.org/abs/1706.03643v1", null], ["v2", "Mon, 7 Aug 2017 01:13:29 GMT  (2271kb,D)", "http://arxiv.org/abs/1706.03643v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["serena yeung", "anitha kannan", "yann dauphin", "li fei-fei"], "accepted": false, "id": "1706.03643"}, "pdf": {"name": "1706.03643.pdf", "metadata": {"source": "META", "title": "Tackling Over-pruning in Variational Autoencoders", "authors": ["Serena Yeung", "Anitha Kannan", "Yann Dauphin", "Li Fei-Fei"], "emails": ["ena@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Variational Autoencoders", "text": "The generative model of a VAE consists in first generating a sample from a d-dimensional stochastic variable z distributed according to a standard Gaussian concept. Given z, the n-dimensional observation x is generated from a parametric family of distributions, such as a Gaussian: pTB (x | z) = N (x; f1 (z), exp (f2 (z))))) (2), where f1 and f2 are non-linear deterministic functions of z modeled using neural networks, and \u03b8 characterizes the parameters of the generative model. The model is trained by optimizing the probability p (X | \u03b8), with the square p (z) resulting from a dataset X of T i.i.d. samples. Since p (z | x) is impracticable, VAE is the exact square recognition of a quadratic network using the quadratic (x)."}, {"heading": "3. Over-pruning", "text": "The first term promotes proper reconstruction, while the second term captures the divergence between the rear q (zd) and its Gaussian components distributed before p (zd), independently for each component. The simplest way to minimize the sum is for a large number of components to collapse on the previous p (zd) in order to compensate for a few highly non-Gaussian components that help rebuild. However, this is achieved by eliminating the corresponding component.1 This behavior is noticeable in the early iterations of training when the log p (x | z) model is fairly impoverished and an improvement in loss can easily be achieved by optimizing this KL term. However, once the units have become inactive, it is almost impossible to resurrect them. A quantity useful for understanding this effect is the activity level of a unit."}, {"heading": "3.1. Weighting the KL term", "text": "One approach to reducing the overlap is to introduce a trade-off between the two terms using a parameter \u03bb: \u2212 Eq\u03c6 (z | x) [log p (x | z)] + \u03bb D \u2211 i = 1 KL (q\u03c6 (zi | x) \u0445 p (zi))) \u03bb controls the importance of encoding the information in z close to the prior. \u03bb = 0 corresponds to a vanilla autocoder and \u03bb = 1 to the correct VAE target. Fig. 2 shows the effect of \u03bb on the activity and generation of units. While the number of active units is increased, the samples generated from the model are still deficient. This is the reason for small values of \u03bb, the model approaches a vanilla autocoder and therefore consumes its capacity to ensure that the reconstruction of the training set is optimised (sharper reconstructions as a function of which are shown in Appendix 9.1), at the cost of generating capacity."}, {"heading": "3.2. Dropout VAE", "text": "Another approach is to add dropout to the latent variable z of the UAE (Dropout UAE), which increases the number of active units (Fig. 3), but generalizes poorly because it uses the dropout layers only to replicate the representation, resulting in blur in both generation and reconstruction, and demonstrating that the mere use of additional units is not sufficient to model additional variation factors, as shown in Fig. 4."}, {"heading": "4. eVAE: A model-based approach", "text": "It is about the question of whether it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about which it is about which it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it"}, {"heading": "4.1. Overcoming over-pruning", "text": "Following (Kingma & Welling, 2014) we use a recognition network q (z, y | x) for approximate posterior conclusions (z, y | x) = q (y | x) q (z | y, x) = q (y | x) N (z; my \u00b5, exp (my \u03c6)))) (6), where \u00b5 = h1 (x) and \u03c6 = h2 (x) are neural networks mapping x to D-dimensional space. We use a similar masking operation as a generative model. Unlike the generative model (eq. 5), the masking operation defined by y can operate directly on the outputs of the recognition network mapping the parameters of q (z | y, x). Similar to VAE, the lower limit of the probability of a dataset can be derived, leading to the cost function (negative limit): Cevae."}, {"heading": "4.2. Training", "text": "7. For the stochastic continuous variable z, we use the repair parameterization trick as in UAE, which repairs the detection distribution in relation to auxiliary variables with fixed distributions. This enables an efficient scanning of the rear distribution, since it results in a deterministic function of input and auxiliary variables.For the discrete variable y, we cannot apply the repair parameterization trick. We therefore approach q (y | x) for a point estimate, so that q (y | x) = \u03b4 (y \u2212 y \u0445), where it only evaluates 1 if y = y and the best y = argmin Cevae. We also researched the modeling q (y | x) = Mult (h (x)) = g (y \u2212 y \u0445), evaluating only y = \u00b2 and the best y = Cevae."}, {"heading": "5. Experiments", "text": "We present experimental results to two sets of data, MNIST (LeCun et al., 1998) and Toronto Faces Database (TFD) (Susskind et al., 2010). We use standard splits for both MNIST and TFD. In our experiments, encoders and decoders are fully networked networks and show results for different depths and units per layer. We use ReLU nonlinearities, and models are trained using the Adam updating rule (Kingma & Ba, 2014) for 200 epochs (MNIST) and 250 epochs (TFD), using base algorithm 1 Learning Epitomic VAE 1: Processing, Medicare initialization parameters 2: up to the convergence of parameters (Medicare, Ba) 3: Means each x of their best y \u00b2 = argmin Cevae 4: Randomize and Partition of data in minibatches, each having a 0.0000.000.000.05: 0.000.000.000.000.05: 1: 0.000.000.000.000.000.000.05: 1: 0.000.000.000.000.000.05: 0.000.000.000.000.000.05: 0.000.000.000.000.000.000.05: 0.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000"}, {"heading": "5.1. Qualitative results: Reconstruction vs. Generation", "text": "In Fig. 6, the ability of eVAE to overcome excessive circumcision and to use latent capacities to model greater variability in the data is first demonstrated in qualitative terms. In Fig. 6, the generation results for UAE and eVAE are compared for various dimensions D of the latent variables z. With D = 2, VAE generates realistic digits but suffers from a lack of diversity. If D is increased to 5, the generation shows greater variability, but also begins to deteriorate in quality. As D is further increased to 10 and 20, the degradation continues. Compare this with the performance of eVAE in the generation: As the dimension D of z is increased, while inclusions of size K = 2 are retained, eVAE is able to model greater variability in the data. Highlighted digits in the 20-d eVAE show multiple styles such as crossed, uncrossed, round, and thin in both models."}, {"heading": "5.2. Choice of epitome size", "text": "Next, we examine how the choice of the epitome size K influences generation power. We measure sample quality using the Parzen Window Estimator (Rifai et al., 2012). Fig. 7 shows the parcel log density for various choices of the epitome size at the MNIST, where the encoder and decoder consist of a single deterministic layer of 500 units. Epitomas do not overlap, and the results are grouped according to the overall dimension D of the latent variable z. mVAE can also be viewed as a mixture of independent VAE models with the same dimension D and for the mixture VAE (mVAE), an ablative version of eVAE where parameters are not divided. mVAE can also be regarded as a mixture of independent VAEs formed in the same way as eVAE."}, {"heading": "5.3. Increasing complexity of encoder and decoder", "text": "Here we examine the effects of encoder and decoder architectures in terms of overlap and generation performance. We vary model complexity by the number of layers of L deterministic hidden units and the number of hidden units H in each deterministic layer. Table 1 shows the parcel log densities of UAE, mVAE and eVAE models trained on TFD with different latent dimension D (see Appendix 9.3 for MNIST). All terms are non-overlapping and of size K = 5. We observe that in VAE, increasing the number of hidden units H (e.g. from 500 to 1000) for a fixed network depth L has a negligible effect on the number of active units and performance. On the other hand, by increasing the depth of the encoder and decoder L, the number of active units in VAE decreases, although performance can be improved."}, {"heading": "5.4. Log-likelihood evaluation", "text": "Table 2 shows meaning-weighted estimates as an average of L5000 for UAE and eUAE on MNIST, with different dimensions of D latent variable. All models have 2 deterministic hidden layers of 200 units each and are trained in 8 phases with a learning rate of 0.001 \u00b7 10 \u2212 17 for 3i epochs, for each level i = 0... 7 according to Burda et al. (2015). The UAE model has a total of 20 active units and is therefore unable to improve performance with increasing D. On the other hand, eVAE is able to use the additional latent capacity to improve protocol probability. Note that these results can be improved by the narrower lower limit of IWAE (Burda et al., 2015), but this is an orthogonal consideration as epitomic training can also improve IWAE."}, {"heading": "5.5. Sample-based evaluation", "text": "In Table 3, we compare the generative performance of eVAE with other models based on their samples. Encoders and decoders have L = 2 layers H = 1000 deterministic units. D = 8 for MNIST and D = 15 for TFD. UAE, mVAE and eVAE refer to the most powerful models across all architectures in Table 1. For MNIST, the VAE model is (L, H, D) = (3, 500, 8), mVAE is (3, 1000, 24) and eVAE is (3, 500, 48). For TFD, the VAE model is (3, 500, 15), mVAE is (3, 1000, 50) and eVAE is (3, 500, 25)."}, {"heading": "6. Related Work", "text": "A number of applications use variational autoencoders as building blocks. In (Gregor et al., 2015) a generative model for images is proposed in which the generator of the UAE is an attention-based recurring model conditioned on the previously drawn canvas. (Eslami et al., 2016) proposes a UAE-based recurring generative model that describes images formed by sequentially selecting an object to draw and add to a canvas that is updated over time. In (Kulkarni et al., 2015), VAEs are used to reproduce 3D objects. Conditional variants of VAE are also used for attribute-specific image generation (Yan et al., 2015) and future frame synthesis (Xue et al., 2016). All of these applications suffer from the problem of model reshaping and have adopted strategies that take away from the clean mathematical formulation of VE."}, {"heading": "7. Conclusion", "text": "This paper introduces Epitomic UAE, an extension of variational autoencoders to address the problem of model overlap that has limited the generation capacity of VAEs in high-dimensional spaces. Based on the intuition that sub-concepts can be modeled with less dimensions than the entire latent space, VAE models latent space as several common sub-spaces that have specialized. We show how this model approaches the problem of model overlap in principle, and present qualitative and quantitative analyses of how eVAE allows increased use of model capacity to model greater data variability. We believe that modeling latent space as multiple structured sub-spaces is a promising direction of work and enables increased effective capacity that can potentially be combined with methods to increase the flexibility of posterior inference."}, {"heading": "8. Acknowledgments", "text": "We thank Marc'Aurelio Ranzato, Joost van Amersfoort and Ross Girshick for helpful discussions and have adopted the term \"epitome\" from an earlier work by (Jojic et al., 2003)."}, {"heading": "9. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1. Effect of KL weight \u03bb on reconstruction", "text": "We visualize UAE reconstructions by setting the KL-term weight \u03bb so that latent units remain active. The upper half of each figure are the original digits, and the lower half are the corresponding reconstructions. While the reconstruction performance is good, the generation is poor (Fig. 2). This shows that UAE learns to model only regions of the posterior manifold well in the vicinity of training samples, instead of generalizing the entire posterior manifold well."}, {"heading": "9.2. Effect of increasing latent dimension on reconstruction", "text": "In Fig. 6 in Fig. 5.1, the effect of increasing latent dimensions on the generation of VAE and eVAE models is shown. Here, we show the effect of the same factor on the reconstruction quality of the models (Fig. 10). The upper half of each figure are the original digits, and the lower half are the corresponding reconstructions. As the dimension of the latent variable z increases from 2-d to 20-d, the VAE reconstruction becomes very sharp (the best model), but the generation deteriorates (Fig. 6). On the other hand, eVAE is able to achieve both good reconstruction and generation."}, {"heading": "9.3. MNIST encoder and decoder complexity", "text": "Table 4 shows the effects of increasing complexity of encoder and decoder complexity on MNIST. We vary model complexity by the number of layers L of deterministic hidden units and the hidden units H in each deterministic layer. Parzen log densities are provided for VAE, mVAE, and eVAE models trained at MNIST with different latent dimensions D."}], "references": [{"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R Jozefowicz", "Bengio"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger B", "Salakhutdinov", "Ruslan"], "venue": "ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["Eslami", "S.M. Ali", "Heess", "Nicolas", "Weber", "Theophane", "Tassa", "Yuval", "Kavukcuoglu", "Koray", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Eslami et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eslami et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Structured sparse coding via lateral inhibition", "author": ["Gregor", "Karol", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "In Proceedings of the 24th International Conference on Neural Information Processing Systems,", "citeRegEx": "Gregor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2011}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.046239,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Epitomic analysis of appearance and shape", "author": ["Jojic", "Nebojsa", "Frey", "Brendan J", "Kannan", "Anitha"], "venue": "In Proceedings of International Conference on Computer Vision,", "citeRegEx": "Jojic et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jojic et al\\.", "year": 2003}, {"title": "How to train deep variational autoencoders and probabilistic ladder networks", "author": ["C. Kaae Sonderby", "T. Raiko", "L. Maale", "S. Kaae Snderby", "O. Winther"], "venue": "arXiv preprint arXiv:1602.02282,", "citeRegEx": "Sonderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sonderby et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Deep convolutional inverse graphics", "author": ["T.D. Kulkarni", "W. Whitney", "P. Kohli", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Rich"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1206.6434,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["T. Salimans", "D.P. Kingma", "M. Welling"], "venue": null, "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "The toronto face database", "author": ["Susskind", "Josh M", "Anderson", "Adam K", "Hinton", "Geoffrey E"], "venue": "Department of Computer Science,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Thibodeau-Laufer", "Eric", "Alain", "Guillaume", "Yosinski", "Jason"], "venue": null, "citeRegEx": "Thibodeau.Laufer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thibodeau.Laufer et al\\.", "year": 2014}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["Xue", "Tianfan", "Wu", "Jiajun", "Bouman", "Katherine L", "Freeman", "William T"], "venue": "arXiv preprint arXiv:1607.02586,", "citeRegEx": "Xue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2016}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Yan", "Xinchen", "Yang", "Jimei", "Sohn", "Kihyuk", "Lee", "Honglak"], "venue": "CoRR, abs/1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "As noted by Burda et al. (2015), these models exhibit the problem of factor overpruning where a significant number of stochastic factors fail to learn anything and become inactive.", "startOffset": 12, "endOffset": 32}, {"referenceID": 5, "context": "A number of recent works use VAE as a modeling framework, including iterative conditional generation of images (Gregor et al., 2015) and conditional future frame prediction (Xue et al.", "startOffset": 111, "endOffset": 132}, {"referenceID": 21, "context": ", 2015) and conditional future frame prediction (Xue et al., 2016).", "startOffset": 48, "endOffset": 66}, {"referenceID": 0, "context": "However, a number of studies (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016) have noted that straightforward implementations that optimize the variational bound on the probability of observations converge to a solution in which only a small subset of the stochastic latent units are active.", "startOffset": 29, "endOffset": 99}, {"referenceID": 11, "context": "However, a number of studies (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016) have noted that straightforward implementations that optimize the variational bound on the probability of observations converge to a solution in which only a small subset of the stochastic latent units are active.", "startOffset": 29, "endOffset": 99}, {"referenceID": 0, "context": "Existing methods propose training schemes to tackle the over-pruning problem that arises due to pre-maturely deactivating units (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016).", "startOffset": 128, "endOffset": 198}, {"referenceID": 11, "context": "Existing methods propose training schemes to tackle the over-pruning problem that arises due to pre-maturely deactivating units (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016).", "startOffset": 128, "endOffset": 198}, {"referenceID": 11, "context": "For instance, (Kingma et al., 2016) enforces minimum KL contribution from subsets of latent units while (Bowman et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": ", 2016) enforces minimum KL contribution from subsets of latent units while (Bowman et al., 2015) use KL cost annealing.", "startOffset": 76, "endOffset": 97}, {"referenceID": 1, "context": "Following (Burda et al., 2015), we define a unit to be used, or \u201cactive\u201d, if Au = Covx(Eu\u223cq(u|x)[u]) > 0.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "We present experimental results on two datasets, MNIST (LeCun et al., 1998) and Toronto Faces Database (TFD) (Susskind et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 19, "context": ", 1998) and Toronto Faces Database (TFD) (Susskind et al., 2010).", "startOffset": 41, "endOffset": 64}, {"referenceID": 17, "context": "We measure sample quality using the Parzen window estimator (Rifai et al., 2012).", "startOffset": 60, "endOffset": 80}, {"referenceID": 1, "context": "7, following Burda et al. (2015). The VAE model has 20 active units at all D, so is not able", "startOffset": 13, "endOffset": 33}, {"referenceID": 1, "context": "Note that these results can be improved through the tighter lower bound of IWAE (Burda et al., 2015), but this is an orthogonal consideration since epitomic training can also improve IWAE.", "startOffset": 80, "endOffset": 100}, {"referenceID": 5, "context": "In (Gregor et al., 2015), a generative model for images is proposed in which the generator of the VAE is an attention-based recurrent model that is conditioned on the canvas drawn so far.", "startOffset": 3, "endOffset": 24}, {"referenceID": 2, "context": "(Eslami et al., 2016) proposes a VAE-based recurrent generative model that describes images as formed by sequentially choosing an object to draw and adding it to a canvas that is updated over time.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "In (Kulkarni et al., 2015), VAEs are used for rendering 3D objects.", "startOffset": 3, "endOffset": 26}, {"referenceID": 22, "context": "Conditional variants of VAE are also used for attribute specific image generation (Yan et al., 2015) and future frame synthesis (Xue et al.", "startOffset": 82, "endOffset": 100}, {"referenceID": 21, "context": ", 2015) and future frame synthesis (Xue et al., 2016).", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": "A complementary approach to the problem of model pruning in VAE was proposed in (Burda et al., 2015); the idea is to improve the variational bound by using multiple weighted posterior samples.", "startOffset": 80, "endOffset": 100}, {"referenceID": 18, "context": "Methods to increase the flexibility of posterior inference are proposed in (Salimans et al., 2015; Rezende & Mohamed, 2016; Kingma et al., 2016).", "startOffset": 75, "endOffset": 144}, {"referenceID": 11, "context": "Methods to increase the flexibility of posterior inference are proposed in (Salimans et al., 2015; Rezende & Mohamed, 2016; Kingma et al., 2016).", "startOffset": 75, "endOffset": 144}, {"referenceID": 6, "context": "DBN (Hinton et al., 2006) 138\u00b1 2 1909\u00b1 66 Deep CAE (Bengio et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 20, "context": ", 2013) 121\u00b1 1 2110\u00b1 50 Deep GSN (Thibodeau-Laufer et al., 2014) 214\u00b1 1 1890\u00b1 29 GAN (Goodfellow et al.", "startOffset": 33, "endOffset": 64}, {"referenceID": 3, "context": ", 2014) 214\u00b1 1 1890\u00b1 29 GAN (Goodfellow et al., 2014) 225\u00b1 2 2057\u00b1 26 GMMN + AE (Li et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 15, "context": ", 2014) 225\u00b1 2 2057\u00b1 26 GMMN + AE (Li et al., 2015) 282\u00b1 2 2204\u00b1 20 Adversarial AE (Makhzani et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 11, "context": "(Kingma et al., 2016) augments the flexibility of the posterior through autoregression over projections of stochastic latent variables.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "However, the problem of over-pruning still persists: for instance, (Kingma et al., 2016) enforces a minimum information constraint to ensure all units are used.", "startOffset": 67, "endOffset": 88}, {"referenceID": 4, "context": "(Gregor et al., 2011; Jenatton et al., 2011).", "startOffset": 0, "endOffset": 44}, {"referenceID": 7, "context": "(Gregor et al., 2011; Jenatton et al., 2011).", "startOffset": 0, "endOffset": 44}, {"referenceID": 8, "context": "We also borrowed the term \u2018epitome\u2019 from an earlier work of (Jojic et al., 2003).", "startOffset": 60, "endOffset": 80}], "year": 2017, "abstractText": "Variational autoencoders (VAE) are directed generative models that learn factorial latent variables. As noted by Burda et al. (2015), these models exhibit the problem of factor overpruning where a significant number of stochastic factors fail to learn anything and become inactive. This can limit their modeling power and their ability to learn diverse and meaningful latent representations. In this paper, we evaluate several methods to address this problem and propose a more effective model-based approach called the epitomic variational autoencoder (eVAE). The so-called epitomes of this model are groups of mutually exclusive latent factors that compete to explain the data. This approach helps prevent inactive units since each group is pressured to explain the data. We compare the approaches with qualitative and quantitative results on MNIST and TFD datasets. Our results show that eVAE makes efficient use of model capacity and generalizes better than VAE.", "creator": "LaTeX with hyperref package"}}}