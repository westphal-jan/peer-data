{"id": "1706.03729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Channel-Recurrent Variational Autoencoders", "abstract": "Variational Autoencoder (VAE) is an efficient framework in modeling natural images with probabilistic latent spaces. However, when the input spaces become complex, VAE becomes less effective, potentially due to the oversimplification of its latent space construction. In this paper, we propose to integrate recurrent connections across channels to both inference and generation steps of VAE. Sequentially building up the complexity of high-level features in this way allows us to capture global-to-local and coarse-to-fine structures of the input data spaces. We show that our channel-recurrent VAE improves existing approaches in multiple aspects: (1) it attains lower negative log-likelihood than standard VAE on MNIST; when trained adversarially, (2) it generates face and bird images with substantially higher visual quality than the state-of-the-art VAE-GAN and (3) channel-recurrency allows learning more interpretable representations; finally (4) it achieves competitive classification results on STL-10 in a semi-supervised setup.", "histories": [["v1", "Mon, 12 Jun 2017 17:01:34 GMT  (7356kb,D)", "http://arxiv.org/abs/1706.03729v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wenling shang", "kihyuk sohn", "zeynep akata", "yuandong tian"], "accepted": false, "id": "1706.03729"}, "pdf": {"name": "1706.03729.pdf", "metadata": {"source": "CRF", "title": "Channel-Recurrent Variational Autoencoders", "authors": ["Wenling Shang", "Kihyuk Sohn", "Zeynep Akata", "Yuandong Tian"], "emails": ["shangw@fb.com", "ksohn@nec-labs.com", "z.akata@uva.nl", "yuandong@fb.com"], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for only one person to be able to take care of another person."}, {"heading": "2 Related Works", "text": "The improvement of variable inference in deep generative models has recently aroused interest in the machine learning community. To allow for flexible approximation, the family of normalizing flows [3] must iterate an initial estimated posterior density through a sequence of invertable mappings to eventually achieve a complex distribution performance that may be even more faithful than the actual one. A specific form of normalization is proposed and achieved in [4] on the basis of a series of inverted auto-regressive transformations. Furthermore, the variable power loss of automobile traffic must be transformed."}, {"heading": "3 Channel-Recurrent VAE", "text": "In this section, we develop our proposed crVAE logbook (1) based on standard UAE models (1), followed by discussions and experiments at the cVAE level to explore spatially correlated latent spaces. (1) Finally, we build our crVAE model by establishing recurring connections to bcVAE.UAE [1]. (2) Variational autoencoders (VAE) approach the intractable posterior structure of directed models with deep neural networks. (1) The variable inhibition (VLB) of the training objects results in the training objective: LVAE = \u2212 EqB (z). (x) + DKL (q2)."}, {"heading": "4 Experiments", "text": "In this section, we first present data sets and describe important implementation details of our facility. Next, we evaluate various draft models of ablation studies on MNIST. Then, we provide results on faces and bird generations, image completion, dislocation factors of variation, and semi-monitored object recognition tasks. MNIST [20] consists of 60,000 images of 28 x 28 handwritten digits for training and 10,000 for testing. CelebA [16] contains 162, 770 training and 19, 867 validation examples, which are cropped and scaled to 64 x 64 or 128 x 128. In addition, we combine three large bird data sets, Birdsnap [21], NABirds [22], and Caltech-UCSD-V\u00f6gel-200-2011 (CUB), to 64 x 64 x 128 x 128."}, {"heading": "4.1 Ablation Studies on MNIST", "text": "The negative log likelihood (NLL) of the MNIST was used as a benchmark to evaluate the probable generative models. the NLL is approximated by the importance test [26], i.e. for each image from the statistically binarized MNIST test set. [27] We try out the approximate posterior 10K times. We keep the same latent dimensions for all models. The results are summarized in Table 1, i.e. for each image from the statistically binarized MNIST test set [27]. As expected, cVAE performs worse (83.12) because it cannot detect coherent patterns well. The baseline, standard VAE [1], receives 82.04 NLL."}, {"heading": "4.2 Generating Images of Faces and Birds", "text": "As explained in [15], the KL divergence concept in the UAE lens impedes realistic image generation (Figure 2) because it focuses on stretching the latent space over the entire training scenario if it is assigned a too low probability. On the other hand, the Jensen-Shannon divergence of GAN [29] focuses on a limited but highly probable region, resulting in crystal-clear images. To maintain the expressivity of latent space while creating realistic images, we extend crVAE in crVAE-GAN examples by regulating generation output with an adversal discriminator."}, {"heading": "4.3 Image Completion on CelebA", "text": "To verify how faithfully the latent diversity of crVAE-GAN reflects the semantic meaning of the input space, we perform image completion experiments on 64 x 64 CelebA datasets. Specifically, we close parts such as the right face, eye, or mouth region of each validation image and optimize their latent representations z to fill in the missing parts [30]: min z [gen (z) m \u2212 x m \u00b2 22 + \u03b3 logN (z; 0, I) + \u03c4 log (1 \u2212 D (z)]]], (5) where gen (\u00b7) refers to the output of the generation network, m \u00b2 {0, 1} 3 \u00d7 64 \u00d7 64 is a mask whose entries are 0 when corresponding pixel positions are hidden and 1 otherwise, and elementary represents multiplication. The basic truth x, concealed x, and the final outputs (z) are + one, whose entries are xelated and 1."}, {"heading": "4.4 Disentangling Factors of Variation on CelebA", "text": "Another interesting aspect of crVAE is that the latent variables are processed sequentially, allowing variables at each step in time to learn a global to local and coarse to fine visual concepts of how time develops. For example, in Figure 5 (a), we set all latent variables to zero at t = 0 and continuously add content to the latent blocks that arise at each step in time by randomly pulling samples from the standard MVN, while observing that latent variables detect different semantic factors of variation in different time steps, such as the global face structure and the expression of changes later. To further verify the untangling of the property of our model, we first draw random samples from standard MVN over 8 time steps, ranging from z = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 z8] we select a new sample."}, {"heading": "4.5 Semi-Supervised Object Recognition on STL-10", "text": "Inspired by current work on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the UAE-GAN framework to the STL-10 object recognition task. Our experimental setup, such as architecture, training / evaluation protocol, hyperparameters and data preprocessing, closely follows [32], where the discriminator classifies not only real images, but also the category. To simplify the pipeline, our inputs are not randomly removed and the discriminator takes the fully reconstructed instead of the unpainted images. In this context, we replace the generator with our proposed crVAE and compare it with models that have deterministic auto encoders [32] or standard VAE as generators (Table 2)."}, {"heading": "5 Conclusion", "text": "We propose a Channel Recurrent Scheme, i.e. crVAE, to learn flexible latent representations within the UAE framework. Channel Recurrency improves over standard UAE on a variety of tasks. In addition, we examine the latent diversity that our crVAE shows that it is able to naturally weave explanatory factors into latent subspaces, allowing richer semantic variations for each type of attribute."}, {"heading": "Acknowledgments", "text": "We thank Justin Chiu for his helpful comments."}, {"heading": "C More Image Completion Examples.", "text": "(a) right half (b) mouth (c) eye image S3: Each block from left to right: ground truth; closed; UAE-GAN; crVAE-GAN."}, {"heading": "D More Subspace Interpolation Examples", "text": "(a) hair (b) smile (c) azimuth (d) face shape (e) illumination (f) facial hairFigure S4: Interpolations between z = [z1, \u00b7 \u00b7 \u00b7, zt, \u00b7 \u00b7, z8] and z't = [z1, \u00b7 \u00b7, z't, \u00b7 \u00b7 \u00b7, z8]. All numbers are pre-generated from standard MVN; all numbers in each block share the same z-t, i.e. they end at the same latent representation for the time step. (a) (b) (c) are additional samples of these appearing in the main text. We can see how training along one direction shifts a certain variation factor at a given time. Although due to the high dimensionality of the subspaces we manipulate, highly correlated factors can interpret the wine. (f) shows that the beard correlates with sex."}, {"heading": "G Details of Network Architecture", "text": "This section describes the building blocks used to construct the networks in our experiments. \u2022 In this section the following descriptions are customized: \u2022 Csn refers to a convolutional layer with n filters and steps. \u2022 Dn refers to a deconvolutional layer with n filters; in our networks are deconv layers of stride 2 and kernel size 4 + 4. \u2022 FCn refers to a fully connected layer with n filters. \u2022 P k refers to k \u00d7 k pooling. \u2022 LSTMtn refers to a T -time-step LSTM with n hidden units. \u2022 BN refers to batch normalization. \u2022 ReLU refers to ReLU refers to ReLU activation, CReLU Reaky Leaky Leaky Reactivation with 0.1 slope.G.1 Inference Feature Extractor Architecture The following models are used to extract the configuration: MNIST."}], "references": [{"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Tim Salimans", "Diederik Kingma P", "Max Welling"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Rezende", "Shakir Mohamed"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik Kingma", "Tim Salimans", "Rafal Jozefowicz", "Xi Chen", "Ilya Sutskever", "Max Welling"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Variational lossy autoencoder", "author": ["Xi Chen", "Diederik P Kingma", "Tim Salimans", "Yan Duan", "Prafulla Dhariwal", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1611.02731,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "ICLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Dai Andrew", "Rafal Jozefowicz", "Samy Bengio"], "venue": "In arXiv,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Ladder variational autoencoders", "author": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Epitomic variational autoencoder", "author": ["Serena Yeung", "Anitha Kannan", "Yann Dauphin", "Li Fei-fei"], "venue": "In ICLR 2017 workshop submission,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "Soren Kaae Sonderby", "Hugo Larochelle", "Ole Winther"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "author": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "venue": "In ICLR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "Aaron van den Oord", "Matthias Bethge"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In ICCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Derivations for linear algebra and optimization", "author": ["John Duchi"], "venue": "Berkeley, California,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Birdsnap: Large-scale fine-grained visual categorization of birds", "author": ["Thomas Berg", "Jiongxin Liu", "Seung Woo Lee", "Michelle L Alexander", "David W Jacobs", "Peter N Belhumeur"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection", "author": ["Grant Van Horn", "Steve Branson", "Ryan Farrell", "Scott Haber", "Jessie Barry", "Panos Ipeirotis", "Pietro Perona", "Serge Belongie"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The caltech-ucsd birds-200-2011 dataset", "author": ["Catherine Wah", "Steve Branson", "Peter Welinder", "Pietro Perona", "Serge Belongie"], "venue": "In Report,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Honglak Lee", "Andrew Y Ng"], "venue": "In AISTATS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Binarized mnist dataset", "author": ["Hugo Larochelle"], "venue": "http://www.cs.toronto.edu/~larocheh/ public/datasets/binarized_mnist,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["Jost Tobias Springenberg"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Semi-supervised learning with context-conditional generative adversarial networks", "author": ["Emily Denton", "Sam Gross", "Rob Fergus"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Stacked what-where autoencoders", "author": ["Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann Lecun"], "venue": "In ICLR Workshop,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["Tom Le Paine", "Pooya Khorrami", "Wei Han", "Thomas S Huang"], "venue": "ICLR Workshop,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Density estimation using real nvp", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gregoire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "In ICML,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In Variational Autoencoder (VAE) [1], a bottom-up inference network and a top-down generation network parameterized by deep neural networks are jointly trained to maximize the variational lower-bound (VLB) of the data log-likelihood.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 96, "endOffset": 105}, {"referenceID": 2, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 96, "endOffset": 105}, {"referenceID": 3, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 96, "endOffset": 105}, {"referenceID": 4, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 6, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 216, "endOffset": 225}, {"referenceID": 7, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 216, "endOffset": 225}, {"referenceID": 8, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 216, "endOffset": 225}, {"referenceID": 0, "context": "A predominant architecture [1] models the latent space using the activation of a fully-connected (FC) layer, which is too rigid to model rich local details well due to excessive global constraints.", "startOffset": 27, "endOffset": 30}, {"referenceID": 9, "context": "VAE with convolutional latent space [10] or convolutional VAE (cVAE), with fewer global constraints on the latent space, better preserves the visual details of its reconstructed samples (Figure 2(c)), however its generated samples (Figure 2(g)) appear to be chaotic due to lack of high-level consistency.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "(a) standard VAE [1] Sampler", "startOffset": 17, "endOffset": 20}, {"referenceID": 10, "context": "We then integrate crVAE into the state-of-the-art image generation framework VAE-GAN [11], which we refer to as crVAE-GAN, and present several qualitative studies, such as image completion and synthesis of high-resolution (128\u00d7128) faces and birds.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "Finally, our model can be further enhanced by combining with other advanced techniques in training deep latent Gaussian models (DLGMs), such as a training protocol of importance weighted autoencoder [6] (Table 1).", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "To enable flexible approximated posterior or prior, the family of normalizing flows [3] iterates an initial estimated posterior density through a sequence of invertible mappings to eventually reach a complex distribution, potentially more faithful to the true posterior.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "A specific form of normalizing flow is proposed in [4] based on a series of inverse autoregressive transformations and has attained close to state-of-the-art density estimation performance.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Furthermore, variational lossy autoencoder [5] transforms a simple standard MVN prior to some more complex distribution via similar autoregressive flow.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "The channel-recurrent construction also shares the spirit of deep autoregressive networks (DARN) [12] in the sense of building lateral latent variable connections.", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "For example, InfoGAN [13] shows unsupervised discovery of basic visual concepts by maximizing mutual information between the inputs and the predicted latent variables.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "\u03b2-VAE [14] shows similar behaviors with VAEs by applying stronger regularizations on the KL-divergence term to enforce individual latent variable to account for a specific factor of variation.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "We start from standard VAE model of [1], followed by discussions and experiments on cVAE to explore spatially-correlated latent spaces.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "VAE [1].", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "Thus, the associated prior does not necessarily reflect the complicated structure of the true input space [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "As a consequence, the content from VAE samples lacks in variety, resulting in overly smoothed reconstruction or generation of images (see Figure 2(b) and Figure 2(f), trained on CelebA faces [16]).", "startOffset": 191, "endOffset": 195}, {"referenceID": 9, "context": "To preserve more spatial information, we explore convolutional VAE [10] (cVAE, Figure 1(b)), whose inference and generation networks are fully convolutional [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "To preserve more spatial information, we explore convolutional VAE [10] (cVAE, Figure 1(b)), whose inference and generation networks are fully convolutional [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "Although appealing, training a model with MVN prior or by adding direct lateral connection between latent variables can significantly complicate the optimization of the DLGMs [18, 12].", "startOffset": 175, "endOffset": 183}, {"referenceID": 11, "context": "Although appealing, training a model with MVN prior or by adding direct lateral connection between latent variables can significantly complicate the optimization of the DLGMs [18, 12].", "startOffset": 175, "endOffset": 183}, {"referenceID": 18, "context": "Specifically, we propose to connect the blocks via LSTMs [19], both in the inference and generation networks of VAE to encourage channel-level interaction, as shown in Figure 1(d).", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Model specifications NLL standard VAE [1] \u2013 82.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "02 IWAE [6]\u2020 1 stochastic layer 84.", "startOffset": 8, "endOffset": 11}, {"referenceID": 11, "context": "78 DARN [12]\u2020 1 hidden layer 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "13 VAE-IAF [4]\u2020 1 hidden layer 79.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "10 Model trained and evaluated on binarized MNIST DLGM-NF [3]\u2020 planar normalizing flow (k=80) 85.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "10 DLGM-HVI [2]\u2020 8 leapfrog steps 85.", "startOffset": 12, "endOffset": 15}, {"referenceID": 25, "context": "Negative log-likelihood (NLL) is estimated using importance weighted sampling [26], i.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "MNIST [20] is composed of 60, 000 images of 28\u00d728 handwritten digits for training and 10, 000 for testing.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "CelebA [16] contains 162, 770 training and 19, 867 validation examples which are cropped and scaled to 64\u00d764 or 128\u00d7128.", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "In addition, we combine three large scale bird datasets, namely Birdsnap [21], NABirds [22] and Caltech-UCSD birds-200-2011 (CUB) [23], to one congregated dataset referred to as Birds, in total of 106, 474 images.", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "In addition, we combine three large scale bird datasets, namely Birdsnap [21], NABirds [22] and Caltech-UCSD birds-200-2011 (CUB) [23], to one congregated dataset referred to as Birds, in total of 106, 474 images.", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "In addition, we combine three large scale bird datasets, namely Birdsnap [21], NABirds [22] and Caltech-UCSD birds-200-2011 (CUB) [23], to one congregated dataset referred to as Birds, in total of 106, 474 images.", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "Finally, STL-10 [24] consists of 100, 000 unlabeled examples, 5000 labeled training examples split into 10-fold cross validations, and 8000 labeled testing examples from 10 classes.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "All models share the standard VAE work flow and are optimized with SGD using ADAM [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 25, "context": "NLL is approximated via importance sampling [26], i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 26, "context": "image from the statistically binarized MNIST test set [27], we sample from its approximated posterior 10K times.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "The baseline, standard VAE [1], obtains 82.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "For example, by integrating k-sample importance weighted estimation [6], we can improve the NLL to 80.", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 126, "endOffset": 135}, {"referenceID": 1, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 126, "endOffset": 135}, {"referenceID": 3, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 126, "endOffset": 135}, {"referenceID": 27, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 211, "endOffset": 215}, {"referenceID": 11, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 225, "endOffset": 229}, {"referenceID": 14, "context": "As pointed out in [15], the KL divergence term in VAE objective hampers realistic image generation (Figure 2) since it focuses on stretching the latent space over the entire training set in case of assigning too low probability to any of them.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "On the other hand, the Jensen-Shannon divergence of GAN [29] concentrates on a limited, but high probability regions, resulting in crispier images.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "We compare our crVAE-GAN against the VAE-GAN [11] for generating 128\u00d7128 images in Figure 3.", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "Specifically, we occlude parts such as right-half of the face, eye or mouth regions of each validation image and optimize their latent representations z\u2019s to fill in the missing parts [30]:", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "In addition, we note that, unlike other unsupervised disentangling methods such as \u03b2-VAE [14] or InfoGAN [13], our model associates a latent subspace instead of a single latent unit to each factor, which grants more freedom for semantic variations.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "In addition, we note that, unlike other unsupervised disentangling methods such as \u03b2-VAE [14] or InfoGAN [13], our model associates a latent subspace instead of a single latent unit to each factor, which grants more freedom for semantic variations.", "startOffset": 105, "endOffset": 109}, {"referenceID": 30, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 31, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 32, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 69, "endOffset": 77}, {"referenceID": 33, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 69, "endOffset": 77}, {"referenceID": 31, "context": "Our experimental setup, such as architecture, training/evaluation protocol, hyperparameters and data preprocessing, closely follows [32], where the discriminator not only classifies real-fake images but also the category.", "startOffset": 132, "endOffset": 136}, {"referenceID": 31, "context": "We replace the generator in this framework with our proposed crVAE, comparing against models that have deterministic autoencoders [32] or standard VAE as generators (Table 2).", "startOffset": 130, "endOffset": 134}, {"referenceID": 34, "context": "40% from Exemplar CNNs [35].", "startOffset": 23, "endOffset": 27}, {"referenceID": 31, "context": "Model Accuracy Model Accuracy Deterministic AE [32]\u2020 73.", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "81 Stacked What-Where AE [34]\u2020 74.", "startOffset": 25, "endOffset": 29}, {"referenceID": 34, "context": "91 Exemplar CNN [35]\u2020 75.", "startOffset": 16, "endOffset": 20}, {"referenceID": 35, "context": "86 Unsupervised Pre-train [36]\u2020 70.", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "Variational Autoencoder (VAE) is an efficient framework in modeling natural images with probabilistic latent spaces. However, when the input spaces become complex, VAE becomes less effective, potentially due to the oversimplification of its latent space construction. In this paper, we propose to integrate recurrent connections across channels to both inference and generation steps of VAE. Sequentially building up the complexity of high-level features in this way allows us to capture global-to-local and coarse-to-fine structures of the input data spaces. We show that our channel-recurrent VAE improves existing approaches in multiple aspects: (1) it attains lower negative log-likelihood than standard VAE on MNIST; when trained adversarially, (2) it generates face and bird images with substantially higher visual quality than the state-of-the-art VAE-GAN and (3) channel-recurrency allows learning more interpretable representations; finally (4) it achieves competitive classification results on STL-10 in a semi-supervised setup.", "creator": "LaTeX with hyperref package"}}}