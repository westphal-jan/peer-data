{"id": "1706.03747", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Acoustic data-driven lexicon learning based on a greedy pronunciation selection framework", "abstract": "Speech recognition systems for irregularly-spelled languages like English normally require hand-written pronunciations. In this paper, we describe a system for automatically obtaining pronunciations of words for which pronunciations are not available, but for which transcribed data exists. Our method integrates information from the letter sequence and from the acoustic evidence. The novel aspect of the problem that we address is the problem of how to prune entries from such a lexicon (since, empirically, lexicons with too many entries do not tend to be good for ASR performance). Experiments on various ASR tasks show that, with the proposed framework, starting with an initial lexicon of several thousand words, we are able to learn a lexicon which performs close to a full expert lexicon in terms of WER performance on test data, and is better than lexicons built using G2P alone or with a pruning criterion based on pronunciation probability.", "histories": [["v1", "Mon, 12 Jun 2017 17:35:41 GMT  (60kb,D)", "http://arxiv.org/abs/1706.03747v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaohui zhang", "vimal manohar", "daniel povey", "sanjeev khudanpur"], "accepted": false, "id": "1706.03747"}, "pdf": {"name": "1706.03747.pdf", "metadata": {"source": "CRF", "title": "Acoustic data-driven lexicon learning based on a greedy pronunciation selection framework", "authors": ["Xiaohui Zhang", "Vimal Manohar", "Daniel Povey", "Sanjeev Khudanpur"], "emails": ["xiaohui@jhu.edu,", "vimal.manohar91@gmail.com,", "danielpovey@gmail.com,", "khudanpur@jhu.edu"], "sections": [{"heading": "1. Introduction", "text": "In recent years, we have had an increasing interest in the research of acoustic data sets, which are necessary for continuous speech recognition. (...) It has been shown that it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about and in which way in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about which it is about which it is about a way in which it is about a way in which it is about a way in which it is about and in which it is about which it is about a way in which it is about a way in which it is about and in which it is about which it is about a way in which it is about which it is about which it is about which it is about a way in which it is about a way"}, {"heading": "2. Collecting pronunciation candidates from multiple sources", "text": "In our framework, such as [10], we first expand the seed lexicon to include OOV words in the training data, using a G2P model trained on the seed lexicon, and then train an acoustic model (AM) using the G2P-enhanced lexicon. Then, we generate alignments for all the training data, on the basis of which we then build a Bigram speech model (LM). With this phone LM and the AM, we construct a phonetic decoder and use it to generate phonetic transcription of training data. For each word in the transcript, we can align it with a telephone sequence using time information from the alignments and phonetic transcriptions. Then, for each specific word w, we can calculate the icon Xiv: 170 6.03 747v 1 [cs.C L] 12 Jun 2017 relative frequency of each phone sequence to which it is aligned, by calculating the telephone sequence by the individual pronunciation of the phone sequence."}, {"heading": "3. Acoustic evidence collection", "text": "First, we introduce some notations. Let O = {O1, O2,..., OM} denote multiple acoustic sequences; Mw denote the number of utterances in O that contain the word w 1; then we further define \u03b8wb, p (w, b) as the pronunciation probability of a pronunciation b for a word w (p, b) as the conditional data probability given the pronunciation of b determined by the acoustic model. This is the \"acoustic proof\" we want to derive from grid statistics required by our pronunciation selection algorithms. Using the combined lexicon and an existing AM (which we used for phonetic decoding in the candidate collection phase) we generate lattices for each distinction phase."}, {"heading": "4. Data-likelihood-reduction based greedy pronunciation selection", "text": "We formulate the pronunciation selection process as a greedy selection process, with data probability reduction being the selection criterion. In this section, we first define how to calculate the optimal data probability for a number of pronunciation candidates using EM and propose a pronunciation selection criterion based on probability reduction, then use a graphic example to compare the proposed selection criterion with other criteria. Finally, we talk about some practical problems in our algorithm and summarize the entire iterative framework of pronunciation selection."}, {"heading": "4.1. A pronunciation selection criterion based on perutterance likelihood reduction", "text": "Given a number of pronunciation candidates for a particular word w, and the conditional probability \u03c4uwb (acoustic evidence) for each utterance Ou, we want to maximize the overall probability of data over the pronunciation model \u03b8w 3: L (\u03b8w) = \u2211 u log (\u2211 b \u03c4uwb\u03b8wb) (1), where the sum probability u is taken only over pronunciations where the word w actually appears. Since the maximization of this goal has no closed form solution, such as [8], we use EM, which maximizes the following auxiliary function, instead (n stands for the iteration index, \u03bbnuwb, p (w, b | Ou, \u03b8nw) the pronunciation is calculated posterior at the next iteration Q (prevn + 1w, up n w w) = empirical pronunciation wb, empirical pronunciation wb, isamic pronunciation wb, isamic hwb, isamic pronunciation."}, {"heading": "4.2. An illustrative example", "text": "Here we show an example illustrating the advantage of pronunciation selection based on the per-pronunciation log probability reduction \u0445 Lb over the learned pronunciation probabilities \u03b8 \u0445 w, in terms of dealing with the confusion of pronunciation variants. In Table 1, we listed the pronunciation candidates, average pronunciation posteriors, learned pronunciation probabilities, and the per-pronunciation log probability reduction of two English words \"machine\" and \"us\" from the TED-LIUM [15] training corpus. Note that the two pronunciations of \"machine\" differ only in one vowel, while the two pronunciations of \"us\" represent two different meanings. \"We want a selection criterion under which it is possible to set a threshold to exclude the reduction of\" M IH SH SH IY IY N \"(generated from phonetic decoding) in the\" machine, \"while we cannot exclude the pronunciation of\" H, \"SH is possible for pronunciation."}, {"heading": "4.3. Refining the pronunciation selection criterion \u2206Lb", "text": "One difficulty with the direct use of \u2206 Lb in a selection frame for iterative pronunciation is that we must develop an interpretable threshold T to decide when the pronunciation should no longer be removed. However, we note that the upper limit of \u0445 Lb can be reached in extreme cases if we remove an absolutely dominant pronunciation p (meaning: satisfy the observed conditional probabilities: \u03c4uwp = 1, \u03c4uwb = \u03b2). Before removing p, it is obvious that the maximum threshold of candidates (1) can be removed that the maximum threshold of candidates w can be reached, where W is a single vector s.t. After removing p, with the constraint b, B\\ p, the threshold of p, the log probability is a constant: L (\u03b8) w) = 0, Mw, the protocol of persons who do not have pronunciation of persons who do not have pronunciation of persons."}, {"heading": "4.4. Summary: an iterative framework", "text": "The proposed pronunciation selection algorithm, which iteratively cuts pronunciations from the original candidate group B, is summarized as algorithm 1 (Bt stands for the selected subset of pronunciation candidates in iteration t)."}, {"heading": "5. Experiments", "text": "In this paper, we focus on a G2P model equipped with a small seed icon to generate most G2P systems. \"In order to evaluate the performance of the proposed lexicon learning framework, a small seed lexicon is built for most G2P systems, in which we can only learn a small portion (5%) of words from the vocabulary of each lexicon task. (4) A baseline system called G2P-ext is built using a G2P-ext that uses a G2P-extended lexicon with the optimal number of variants per word, and another baseline system called G2P-1best is built using a G2P-enhanced lexicon in which we only use the top G2P pronouns."}, {"heading": "6. Conclusion and future work", "text": "In this article, we propose an acoustic data-based lexicon learning system that uses a probability reduction-based criterion for selecting pronunciation candidates from multiple sources, i.e. G2P and phonetic decoding. With the proposed criterion, the pronunciation candidates are greedily iteratively trimmed based on the acoustic data probability reduction caused by the removal of each candidate. This approach allows us to construct a compact but informative lexicon. Experiments on various ASR tasks show that within the proposed framework, starting with a small expert lexicon (containing 0.88K to 10K words), we are able to learn a lexicon more similar to a complete expert lexicon in terms of the WHO performance of test data than lexicon constructed solely with G2P or a criterion pronunciation probabilities."}, {"heading": "7. References", "text": "[1] M. J. Gales, K. M. Knill, and A. Ragni, \"Unicode-basedgraphemic systems for limited resource languages,\" in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5186-5190. [2] D. F. Harwath and J. R. Glass, \"Speech recognition without a lexicon-bridging the gap between graphemic and phonetic systems.\" in INTERSPEECH, 2014, pp. 2655-2659. [3] C.-y. Lee, T. J. O'Donnell, and J. Glass, \"Unsupervised lexicon discovery from acoustic input,\" Transactions of the Association for Computational Linguistics, pp. 389-403, 2015. [4] C.-y. Lee, Y. Glass, and J. R. Glass, \"Joint learning of phonetic units and pronwords for EMaseme.\""}], "references": [{"title": "Unicode-based graphemic systems for limited resource languages", "author": ["M.J. Gales", "K.M. Knill", "A. Ragni"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5186\u20135190.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition without a lexicon-bridging the gap between graphemic and phonetic systems.", "author": ["D.F. Harwath", "J.R. Glass"], "venue": "INTERSPEECH,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T.J. O\u2019Donnell", "J. Glass"], "venue": "Transactions of the Association for Computational Linguistics, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint learning of phonetic units and word pronunciations for asr.", "author": ["C.-y. Lee", "Y. Zhang", "J.R. Glass"], "venue": "EMNLp,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Joint-sequence models for grapheme-tophoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech communication, vol. 50, no. 5, pp. 434\u2013451, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition", "author": ["L. Lu", "A. Ghoshal", "S. Renals"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 374\u2013379.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Approaches to automatic lexicon learning with limited training examples", "author": ["N. Goel", "S. Thomas", "M. Agarwal", "P. Akyazi", "L. Burget", "K. Feng", "A. Ghoshal", "O. Glembek", "M. Karafi\u00e1t", "D. Povey"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 5094\u2013 5097.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning lexicons from speech using a pronunciation mixture model", "author": ["I. McGraw", "I. Badr", "J.R. Glass"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining acoustic data driven g2p and letter-to-sound rules for under resource lexicon generation", "author": ["R. Rasipuram"], "venue": "Proceedings of INTERSPEECH, no. EPFL-CONF-192596, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1925}, {"title": "Sp\u00e9cinov- Tr\u00e9laz\u00e9, \u201cAcoustics-based phonetic transcription method for proper nouns.", "author": ["A. Laurent", "S. Meignier", "T. Merlin", "P. Del\u00e9glise"], "venue": "INTERSPEECH,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Acoustic data-driven pronunciation lexicon generation for logographic languages", "author": ["G. Chen", "D. Povey", "S. Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5350\u20135354.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised joint estimation of grapheme-to-phoneme conversion systems and acoustic model adaptation for non-native speech recognition", "author": ["S. Tsujioka", "S. Sakti", "K. Yoshino", "G. Neubig", "S. Nakamura"], "venue": "Interspeech 2016, pp. 3091\u20133095, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Implicit pronunciation modelling in asr", "author": ["T. Hain"], "venue": "ISCA Tutorial and Research Workshop (ITRW) on Pronunciation Modeling and Lexicon Adaptation for Spoken Language Technology, 2002.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Generating exact lattices in the wfst framework", "author": ["D. Povey", "M. Hannemann", "G. Boulianne", "L. Burget", "A. Ghoshal", "M. Janda", "M. Karafi\u00e1t", "S. Kombrink", "P. Motl\u0131\u0301\u010dek", "Y. Qian"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 4213\u20134216.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131\u0301\u010dek", "Y. Qian", "P. Schwarz"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5206\u20135210.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts.", "author": ["V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "INTERSPEECH,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["D. Povey", "V. Peddinti", "D. Galvez", "P. Ghahrmani", "V. Manohar", "X. Na", "Y. Wang", "S. Khudanpur"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 152, "endOffset": 158}, {"referenceID": 4, "context": "Given a small expert lexicon, the most straightforward way to generate pronunciation candidates for OOV words is to train a Grapheme-to-Phoneme (G2P) [5] model using the seed lexicon", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "and apply it to these OOV words [6, 7, 8].", "startOffset": 32, "endOffset": 41}, {"referenceID": 6, "context": "and apply it to these OOV words [6, 7, 8].", "startOffset": 32, "endOffset": 41}, {"referenceID": 7, "context": "and apply it to these OOV words [6, 7, 8].", "startOffset": 32, "endOffset": 41}, {"referenceID": 8, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 67, "endOffset": 74}, {"referenceID": 9, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 67, "endOffset": 74}, {"referenceID": 6, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 112, "endOffset": 123}, {"referenceID": 10, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 112, "endOffset": 123}, {"referenceID": 11, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 112, "endOffset": 123}, {"referenceID": 12, "context": "That is, given a set of pronunciation candidates from G2P and phonetic decoding (and maybe some from a manually created lexicon), which subset should we keep? Keeping all the pronunciations is impractical because it would make decoding slow, and also because too many pronunciations tend to hurt ASR performance, even when pronunciation probabilities are used [13].", "startOffset": 360, "endOffset": 364}, {"referenceID": 10, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 5, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 7, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 6, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 11, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 9, "context": "In our framework, like [10], we first extend the seed lexicon to include OOV words in the training data, using a G2P model trained on the seed lexicon, and then train an acoustic model (AM) using the G2P-extended lexicon.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "This lattice generation treats distinct pronunciations of words as distinct symbols for the purposes of lattice determinization, unlike our standard procedure described in [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "doesn\u2019t have a closed form solution, like [8], we use EM which maximizes the following auxiliary function instead (n stands for the iteration index, \u03bbuwb , p(w, b|Ou,\u03b8 w) is the pronunciation posterior computed at the nth iteration)", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "According to this, we scale this upper bound by a scalar \u03b1 between [0, 1] to get an interpretable threshold: T = \u2212\u03b1 log \u03b4 , where \u03b1 = 1 corresponds to the above extreme case, which means, for a pronunciation to be not removed, it would have to be present with probability 1 in 100% instances of the word, and \u03b1 = 0 means we will never remove any pronunciation candidates.", "startOffset": 67, "endOffset": 73}, {"referenceID": 4, "context": "With the seed lexicon, we train a G2P model using Sequitur [5] and apply it to all OOV (w.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "All experiments were done with Kaldi [16].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "We conduct experiments on the Librispeech-460 task [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "For each lexicon condition, we use the 460h training data subset to build speaker-adaptive trained GMM (SAT) models (the same AM training recipe as the \u201dSAT 460\u201d from [17]), on top of which we then train sub-sampled time-delay neural networks (TDNNs) [18] with the lattice-free MMI (LF-MMI) [19] criterion.", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "For each lexicon condition, we use the 460h training data subset to build speaker-adaptive trained GMM (SAT) models (the same AM training recipe as the \u201dSAT 460\u201d from [17]), on top of which we then train sub-sampled time-delay neural networks (TDNNs) [18] with the lattice-free MMI (LF-MMI) [19] criterion.", "startOffset": 251, "endOffset": 255}, {"referenceID": 17, "context": "For each lexicon condition, we use the 460h training data subset to build speaker-adaptive trained GMM (SAT) models (the same AM training recipe as the \u201dSAT 460\u201d from [17]), on top of which we then train sub-sampled time-delay neural networks (TDNNs) [18] with the lattice-free MMI (LF-MMI) [19] criterion.", "startOffset": 291, "endOffset": 295}, {"referenceID": 10, "context": "\u201cpp-based selection on G2P candidates\u201d means, we first align acoustic training data with a large G2P-extended lexicon containing all G2P generated candidates (up to 10 candidates per word), and then use max-normalized pronunciation probabilities [11] to prune those candidates for each OOV word, with a tuned threshold (0.", "startOffset": 246, "endOffset": 250}], "year": 2017, "abstractText": "Speech recognition systems for irregularly-spelled languages like English normally require hand-written pronunciations. In this paper, we describe a system for automatically obtaining pronunciations of words for which pronunciations are not available, but for which transcribed data exists. Our method integrates information from the letter sequence and from the acoustic evidence. The novel aspect of the problem that we address is the problem of how to prune entries from such a lexicon (since, empirically, lexicons with too many entries do not tend to be good for ASR performance). Experiments on various ASR tasks show that, with the proposed framework, starting with an initial lexicon of several thousand words, we are able to learn a lexicon which performs close to a full expert lexicon in terms of WER performance on test data, and is better than lexicons built using G2P alone or with a pruning criterion based on pronunciation probability.", "creator": "LaTeX with hyperref package"}}}