{"id": "1706.03757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Semantic Entity Retrieval Toolkit", "abstract": "Unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained attention. In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models. The toolkit provides a unified interface to different representation learning algorithms, fine-grained parsing configuration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. After model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation.", "histories": [["v1", "Mon, 12 Jun 2017 17:51:05 GMT  (916kb,D)", "http://arxiv.org/abs/1706.03757v1", "Submitted to Neu-IR '17 SIGIR Workshop on Neural Information Retrieval"], ["v2", "Mon, 17 Jul 2017 14:30:49 GMT  (920kb,D)", "http://arxiv.org/abs/1706.03757v2", "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17). 2017"]], "COMMENTS": "Submitted to Neu-IR '17 SIGIR Workshop on Neural Information Retrieval", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["christophe van gysel", "maarten de rijke", "evangelos kanoulas"], "accepted": false, "id": "1706.03757"}, "pdf": {"name": "1706.03757.pdf", "metadata": {"source": "META", "title": "Semantic Entity Retrieval Toolkit", "authors": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "emails": ["cvangysel@uva.nl", "derijke@uva.nl", "e.kanoulas@uva.nl"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Information Systems \u2192 Content Analysis and Feature Selection; KEYWORDS Neural Information Search; Entity Search; Toolkit"}, {"heading": "1 INTRODUCTION", "text": "Unattended learning of low-dimensional, semantic representations of words and entities has lately taken on greater importance for the enterprise-oriented tasks of expert discovery [10] and product search [8]. Representations are learned from a collection of documents and domain-specific associations between documents and entities. Expert discovery is the task of identifying the right person with the appropriate skills or knowledge [1], and an association indicates the authorship of documents (e.g. scientific papers) or participation in a project (e.g. annual progress reports). In the case of product search, an associated document is a product description or review [8]. In this paper, we describe the Semantic Entity Retrieval Toolkit (SERT), which implements our previously published entity representation models [8, 10]. Beyond a single interface that combines different models, the toolkit can be used as a source kit."}, {"heading": "2 THE TOOLKIT", "text": "SERT is organized as a pipeline of utilities as shown in Figure 1. First, a collection of documents and entity associations is processed and packaged in a numerical format (\u00a7 2.1), then low-dimensional representations of words and entities are learned (\u00a7 2.2), and upward representations can be used to draw conclusions (\u00a7 2.3)."}, {"heading": "2.1 Collection parsing and preparation", "text": "To begin with, SERT constructs a vocabulary that is used to symbolize the document collection. Words that do not appear in the dictionary are ignored. Word sequences are extracted from the documents and stored together with associated entities in numerical format provided by NumPy [7]. Word sequences can be extracted sequentially or a strip can be used to extract non-consecutive windows. In addition, a hierarchy of word sequence extractors can be used to extract # skip grams, i.e. word sequences can be extracted sequentially or a strip to extract non-consecutive windows."}, {"heading": "2.2 Representation learning", "text": "As soon as the collection has been processed and packaged in a machine-friendly manner, representations of words and units can be learned. e Toolkit includes implementations of modern representation learning models applied to expert building [10] and product search [8]. Users of the toolkit can use these implementations to learn out-of-the-box representations or adapt the algorithms to their needs. In addition, users can implement their own models by extending an interface provided by the framework. Code snippet 1 shows an example of a model implemented in the SERT toolkit, where the user can decode a symbolic cost function that is optimized with eano [6]. Due to the composed organization of the toolkit (Figure 1), modeling and word processing are separated from each other. Consequently, researchers can focus only on modeling and representation. In addition, improvements to the collection processing (Section 2.1) are treated collectively in all models."}, {"heading": "2.3 Entity ranking & other uses of the representations", "text": "Once a model has been trained, SERT can be used to classify entities based on a text request. Specific implementation used to classify entities depends on the model that has been trained. In the most general case, a matching score is calculated for each entity and entities are classified in descending order of their score. However, in the specific case when the model is interpreted as a metric vector space [2, 8], SERT maps the entity ranking as the closest neighboring problem and uses special data structures to retrieve it [5]. For better ranking, SERT outputs the entity ranking as a TREC-compatible le that can be used as an input into the Trec evaluation usage1. 1h ps: / / github.com / usnistgov / trec evalIn addition to the entity ranking, the learned representations and the modelc-specific interface can be used as the convenient interface for the SERT to extract the parameters from the Darth."}, {"heading": "3 CONCLUSIONS", "text": "In this paper, we have described the Semantic Entity Retrieval Toolkit, a toolkit that learns latent representations of words and entities. the toolkit contains implementations of state-of-the-art entity representation algorithms [8, 10] and consists of three components: word processing, representation learning, and conclusion. e-toolkit users can easily make changes to existing models or contribute their own models by extending an interface provided by the SERT framework. Future work includes integration with Pyndri [11] so that document collections indexed with Indri can be transparently used to train entity representations. In addition, integration with machine learning frames alongside eano, such as TensorFlow and PyTorch, will facilitate integration of existing models into the SERT [11]."}], "references": [{"title": "O\u0082 the Beaten Path: Let\u2019s Replace Term-Based Retrieval with k-NN Search", "author": ["Leonid Boytsov", "David Novak", "Yury Malkov", "Nyberg Eric"], "venue": "In CIKM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A closer look at skip-gram modelling", "author": ["David Guthrie", "Ben Allison", "Wei Liu", "Louise Guthrie", "Yorick Wilks"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "An empirical comparison of exact nearest neighbour algorithms", "author": ["Ashraf M Kibriya", "Eibe Frank"], "venue": "In ECMLPKDD. Springer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "\u008ce NumPy Array: A Structure for E\u0081cient Numerical Computation", "author": ["St\u00e9fan van der Walt", "S. Chris Colbert", "Ga\u00ebl Varoquaux"], "venue": "Computing in Science & Engineering 13,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Learning Latent Vector Spaces for Product Search", "author": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "venue": "In CIKM. ACM", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Structural Regularities in Expert Vector Spaces", "author": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "venue": "In ICTIR. ACM", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Unsupervised, E\u0081cient and Semantic Expertise Retrieval", "author": ["Christophe Van Gysel", "Maarten de Rijke", "Marcel Worring"], "venue": "In WWW. ACM", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Pyndri: a Python Interface to the Indri Search Engine", "author": ["Christophe Van Gysel", "Evangelos Kanoulas", "Maarten de Rijke"], "venue": "In ECIR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "\u008ce unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a\u008aention for the entity-oriented tasks of expert \u0080nding [10] and product search [8].", "startOffset": 168, "endOffset": 172}, {"referenceID": 4, "context": "\u008ce unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a\u008aention for the entity-oriented tasks of expert \u0080nding [10] and product search [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 4, "context": "In the case of product search, an associated document is a product description or review [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [8, 10].", "startOffset": 158, "endOffset": 165}, {"referenceID": 6, "context": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [8, 10].", "startOffset": 158, "endOffset": 165}, {"referenceID": 5, "context": "extracted and used as feature vectors in entity clustering or recommendation tasks [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "A\u0089erwards, word sequences are extracted from the documents and stored together with the associated entities in the numerical format provided by NumPy [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": ", word sequences where a number of tokens are skipped a\u0089er selecting a token [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "For example, in the case of expert \u0080nding [10], this weight is the reciprocal of the document length of the document where the sequence was extracted from.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [8].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "\u008ce toolkit includes implementations of state-of-the-art representation learning models that were applied to expert \u0080nding [10] and product search [8].", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "\u008ce toolkit includes implementations of state-of-the-art representation learning models that were applied to expert \u0080nding [10] and product search [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "However, in the special case when the model is interpreted as a metric vector space [2, 8], SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5].", "startOffset": 84, "endOffset": 90}, {"referenceID": 4, "context": "However, in the special case when the model is interpreted as a metric vector space [2, 8], SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5].", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "However, in the special case when the model is interpreted as a metric vector space [2, 8], SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5].", "startOffset": 201, "endOffset": 204}, {"referenceID": 5, "context": "com/usnistgov/trec eval Apart from entity ranking, the learned representations and modelspeci\u0080c parameters can be extracted conveniently from the models through the interface (get representations in Snippet 1) and used for down-stream tasks such as clustering, recommendation and determining entity importance as shown in [9].", "startOffset": 322, "endOffset": 325}, {"referenceID": 4, "context": "\u008ce toolkit contains implementations of state-of-the-art entity representations algorithms [8, 10] and consists of three components: text processing, representation learning and inference.", "startOffset": 90, "endOffset": 97}, {"referenceID": 6, "context": "\u008ce toolkit contains implementations of state-of-the-art entity representations algorithms [8, 10] and consists of three components: text processing, representation learning and inference.", "startOffset": 90, "endOffset": 97}, {"referenceID": 7, "context": "Future work includes integration with Pyndri [11] such that document collections indexed with Indri can transparently be used to train entity representations.", "startOffset": 45, "endOffset": 49}], "year": 2017, "abstractText": "Unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a\u008aention. In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models. \u008ce toolkit provides a uni\u0080ed interface to di\u0082erent representation learning algorithms, \u0080ne-grained parsing con\u0080guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A\u0089er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation.", "creator": "LaTeX with hyperref package"}}}