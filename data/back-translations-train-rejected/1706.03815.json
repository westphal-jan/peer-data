{"id": "1706.03815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Encoding of phonology in a recurrent neural model of grounded speech", "abstract": "We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.", "histories": [["v1", "Mon, 12 Jun 2017 19:07:02 GMT  (265kb,D)", "https://arxiv.org/abs/1706.03815v1", "Accepted at CoNLL 2017"], ["v2", "Fri, 16 Jun 2017 08:35:44 GMT  (265kb,D)", "http://arxiv.org/abs/1706.03815v2", "Accepted at CoNLL 2017"]], "COMMENTS": "Accepted at CoNLL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["afra alishahi", "marie barking", "grzegorz chrupa{\\l}a"], "accepted": false, "id": "1706.03815"}, "pdf": {"name": "1706.03815.pdf", "metadata": {"source": "META", "title": "Encoding of phonology in a recurrent neural model of grounded speech", "authors": ["Afra Alishahi", "Marie Barking"], "emails": ["a.alishahi@uvt.nl", "m.barking@uvt.nl", "g.chrupala@uvt.nl"], "sections": [{"heading": "1 Introduction", "text": "In the field of neuroscience, there has long been an interest in understanding neural representations of linguistic input in human brains, mostly through the analysis of neuro-imaging data from participants exposed to simplified, highly controlled input. More recently, naturalistic data has been used and patterns in the brain correlate with patterns in input (e.g. Wehbe et al., 2014; Khalighinejad et al., 2017). This type of approach is also relevant if the goal is to understand the dynamics in complex neural network models of language comprehension. Firstly, because similar techniques are commonly applicable, but more important because knowledge of how the work of artificial and biological neural networks is similar or otherwise valuable for the general functioning of cognitive sciences."}, {"heading": "2 Related Work", "text": "The research on coding phonology was carried out from both a psycholinguistic and a computer-aided modeling perspective."}, {"heading": "2.1 Phoneme perception", "text": "In fact, most of us are able to survive ourselves if we do not put ourselves in a position to survive ourselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world. \""}, {"heading": "2.2 Computational models", "text": "There are a number of approaches that have changed in the past few years, such as the introduction of the electronic ankle cuff, which are in the USA and in Europe, but also in the USA, in Europe and in the USA, in the USA and in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in China, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3 Model", "text": "As our model of speech acquisition of grounded speech signals, we adopt the model of Chrupa\u0142a et al. (2017a) based on the Recurrent Highway Network. This model has two desirable properties: firstly, thanks to the analyses performed in this paper, we roughly understand how the hidden layers differ in terms of the level of linguistic representation they encode. Secondly, the model is trained on clean synthetic language, which makes it applicable to the controlled experiments in Section 5.2. We refer the reader to Chrupa\u0142a et al. (2017a) for a detailed description of the model architecture. Here we give a brief overview. The model uses its correlations between two modalities, i.e. language and vision, as a source of weak monitoring to understand speech; in other words, it implements speech capture from the speech signal grounded in visual perception."}, {"heading": "4 Experimental data and setup", "text": "The phoneme representations in each level are calculated as the activations averaged over the duration of the phoneme occurrence in the input. Average input vectors are calculated similarly to the MFCC vectors averaged over the course of the articulation of the phoneme occurrence. If we need to represent a phoneme type, we do this by averaging the vectors of all its occurrences in the validation set. Table 1 shows the phoneme inventory we work with; this is also the inventory used by Gentle / Kaldi (see Section 4.3)."}, {"heading": "4.1 Model settings", "text": "We use the version of the COCO Speech model presented by Chrupa\u0142a et al. (2017a), which was implemented in Theano (Bastien et al., 2012). 1 The details of the model configuration are as follows: coil layer with length 6, size 64, increment 3, 5 layers of recursive road network with 512 dimensions and 2 micro-steps, attention multi-layer perceptron with 512 hidden units, Adam optimizer, initial learning rate 0.0002. 4096-dimensional image characteristic vectors originate from the last fully connected layer of VGG-16 (Simonyan and Zisserman, 2014), which was pre-trained on Imagenet (Russakovsky et al., 2014), and represent average values of characteristic vectors for ten sections of each image. The total number of learnable parameters is 9,784,193. Table 2 sketches the architecture of the model code."}, {"heading": "4.2 Synthetically Spoken COCO", "text": "The Speech COCO model was trained using the Synthetically Spoken COCO dataset (Chrupa\u0142a et al., 2017b), a version of the MS COCO1code, data and pre-trained models available at https: / / github.com / gchrupala / visuell-grounded-speech.dataset (Lin et al., 2014), with language synthesized for the original image descriptions using a high-quality speech synthesis from gTTS.2."}, {"heading": "4.3 Forced alignment", "text": "We aligned the speech signal to the corresponding phonemic transcription with the Gentle Toolkit, which in turn is based on Kaldi (Povey et al., 2011). It uses an English speech recognition model to transcribe the input tone signal, and then finds the optimal alignment of the transcription to the signal. This fails due to a small number of expressions we remove from the data. In the next step, we extract MFCC features from the audio signal and guide them through the COCO Speech encoder and record the activations for the folding layer and all recurring layers. For each utterance, the representations (i.e. MFCC features and activations) are stored in a tr \u00d7 Dr matrix, with tr and Dr indicating the number of time steps or dimensions for each representation r. Given the orientation of each phoneme token to the underlying audio, we then return the slice of the corresponding matrix."}, {"heading": "5 Experiments", "text": "In this section, we report on four experiments we have designed to clarify the extent to which information about phonology is represented in the activation of the layers of the COCO language model. In Section 5.1, we quantify how easy it is to decode phoneme identity from activations. In Section 5.2, we determine phoneme discriminability in a controlled task with minimal pair stimuli. Section 5.3 shows how the phoneme inventory is available at https: / / github.com / pndurette / gTTS. 3Available at https: / / github.com / lowerquality / Gentle.organized in the activation space of the model. Finally, in Section 5.4, we address the general question of the representation of phonological form versus meaning with the controlled task of synonym discrimination."}, {"heading": "5.1 Phoneme decoding", "text": "In this section, we quantify the extent to which phoneme identity can be decoded from the input functions of the MFCC, compared to the representations extracted from the COCO language. As explained in Section 4.3, we use phonemic transcriptions aligned with the corresponding audio to segment the signal into chunks corresponding to the individual phonemes. We take a sample of 5000 expressions from the validation set of synthetically spoken COCO and extract the forcibly aligned representations from the COCO model. We divide this data into 23 training portions and 13 heldout portions, and use monitored classifications to quantify the reproducibility of phoneme identities from the representations. Each phoneme disk is averaged over time so that it becomes a Dr-dimensional vector. For each representation, we then form L2-punished logistic regression (with the fixed weight of the helmet) on the basis of poorly measured training fixes."}, {"heading": "5.2 Phoneme discrimination", "text": "In fact, it is not as if it were a matter of a way in which people move in the most different areas of the world. (...) It is not as if people live in the most different areas of the world, in the most different areas of the world, in the most different areas of the world, in which people live in the most different areas of the world. (...) It is not as if people live in the most different areas of the world, in which people live in the most different areas of the world, in the most different areas of the world. (...) It is as if people live in the most different areas of the world, in the most different areas of life. (...) It is not as if people live in the most different areas of the world, in the most different areas of the world, in the most different cultures, in the most different areas of the world, in different cultures, in the most different areas of the world, in different cultures, in different cultures, different cultures, in the most different areas of the most different areas of the world, in different cultures, different cultures, different cultures, different cultures, in the most different areas of the most different areas of the world."}, {"heading": "5.3 Organization of phonemes", "text": "In this section, we take a closer look at the underlying organization of phonemes in the model. Our experiment is inspired by Khalighinejad et al. (2017), which investigates how the speech signal is represented in the brain at different stages of the auditory pathway, collecting and analyzing the hidden activation vectors of our model in response to each phoneme in the input, and showing that brain responses to different phoneme categories turn out to be organized by phonetic characteristics. We conduct an analogous experiment by analyzing the hidden layer activations of our model in response to each phoneme in the input. First, we generate a distance matrix for each phoneme category pair by analyzing the distance between the phoneme categories by calculating the euclidean distance between the activation vectors for each layer individually, as well as a distance matrix for all phoneme categories in the input."}, {"heading": "5.4 Synonym discrimination", "text": "Next, we simulate the task of distinguishing between synonyms, i.e. words with different acoustic forms but the same meaning. With a representation that encodes phonological forms, our expectation is that the task would be simple; in contrast, with a representation that is invariably tophonological to encode the meaning, the task would be difficult. We create a list of synonyms for each noun, verb and adjective in the validation data that Wordt (Miller, 1995) uses as a criterion. From these generated word pairs, we select synonyms for the experiment based on the following criteria: \u2022 Both forms are clearly synonyms in the sense that one word can be replaced by the other without changing the meaning of a sentence. \u2022 Both forms appear more than 20 times in the validation data that clearly differ in form (i.e. they are not simply variable)."}, {"heading": "6 Discussion", "text": "In this work, we focus on making progress on this problem for a specific area: representations of phonology in a multi-layered, recurring neural network trained on grounded speech signals. We believe it is important to perform multiple analyses using different methods: each individual experiment can be misleading as it depends on analytical decisions, such as the type of supervised model used for decoding, the algorithm used for clustering, or the similarity metric for representative similarity analysis. To the extent that more than one experiment points to the same conclusion, our confidence in the reliability of the knowledge gained is increased. Previous work (Chrupa\u0142a et al., 2017a) shows that the coding of semantics in our RNN model of grounded language becomes stronger, while the coding of the form weakens."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.", "creator": "LaTeX with hyperref package"}}}