{"id": "1706.03824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Attention-based Vocabulary Selection for NMT Decoding", "abstract": "Neural Machine Translation (NMT) models usually use large target vocabulary sizes to capture most of the words in the target language. The vocabulary size is a big factor when decoding new sentences as the final softmax layer normalizes over all possible target words. To address this problem, it is widely common to restrict the target vocabulary with candidate lists based on the source sentence. Usually, the candidate lists are a combination of external word-to-word aligner, phrase table entries or most frequent words. In this work, we propose a simple and yet novel approach to learn candidate lists directly from the attention layer during NMT training. The candidate lists are highly optimized for the current NMT model and do not need any external computation of the candidate pool. We show significant decoding speedup compared with using the entire vocabulary, without losing any translation quality for two language pairs.", "histories": [["v1", "Mon, 12 Jun 2017 19:51:00 GMT  (783kb,D)", "http://arxiv.org/abs/1706.03824v1", "Submitted to Second Conference on Machine Translation (WMT-17); 7 pages"]], "COMMENTS": "Submitted to Second Conference on Machine Translation (WMT-17); 7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["baskaran sankaran", "markus freitag", "yaser al-onaizan"], "accepted": false, "id": "1706.03824"}, "pdf": {"name": "1706.03824.pdf", "metadata": {"source": "CRF", "title": "Attention-based Vocabulary Selection for NMT Decoding", "authors": ["Baskaran Sankaran"], "emails": ["bsankara@us.ibm.com", "freitagm@us.ibm.com", "onaizan@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Due to the fact that Neural Machine Translation (NMT) achieves comparable or even better results compared to statistical machine translation (SMT) models (Jean et al., 2015; Luong et al., 2015), it has become very popular in recent years (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). With the recent success of NMT, attention has shifted towards a more practical translation. Compared to traditional phrase-based machine translation machines, NMT decryption tends to be much slower. One of the most expensive parts in NMT is the Softmax calculation of the full target vocabulary. Recent work shows that we can limit Softmax translation to a subset of likely candidates to whom the source is given. Candidates are based on a dictionary constructed from Viterbi word alignments, or we reconstruct phrases from a phrase or explain the most common phrases in a candidate's system by adding them to it completely."}, {"heading": "2 Neural Machine Translation", "text": "The attention-based NMT (Bahdanau et al., 2014) is an encoder decoder network. The encoder uses a bidirectional RNN to encode the source set x = (x1,..., xl) into a sequence of hidden states q = (h1,..., hl), where l is the length of the source set. Each hi is a concatenation of a left-to-right \u2212 hi and a right-to-left area \u2212 hi RNN: hi = [\u2190 \u2212 h \u2212 h i] = [\u2190 \u2212 f (xi, h i + 1) \u2212 f (xi, \u2212 h \u2212 1) \u2212 f are two gated recurrent units (GRU) introduced by (Cho et al., 2014)."}, {"heading": "3 Our Approach", "text": "In this section, we describe our approach to learning alignments from attention, as well as the GRU decoder introduced in session2 of the dl4mt tutorial: https: / / github.com / nyu-dl / dl4mttutorial / tree / master / session2."}, {"heading": "3.1 Learning Alignments from Attention", "text": "This attention implicitly captures the alignment between the target word generated at this point in time and the source words. We formalize this implicit notion into soft alignments by aligning the generated target word yt with the source word. The strength of the alignment is determined by the weight of the attention weights. While the attention weights are probabilities, we treat them as fractions of the target words. Our method simply accumulates these (normalized) attention weights into a matrix while the na\u00efve implementations of dimensions."}, {"heading": "3.2 Vocabulary Selection for Decoding", "text": "As already mentioned, vocabulary selection to accelerate decoding has become widely used in the NMT (Jean et al., 2015; Mi et al., 2016, etc.). In this work, we use the alignments learned during the training for vocabulary selection. It should be noted that the accumulated attention weights are fractions and not probabilities. Secondly, these values characterize the alignment from target to source. During decoding, we are interested in obtaining the target vocabulary based on the initial words. Therefore, we first normalize the distribution matrix along the target axis and then use the normalized distribution to obtain top-n targets for each source pattern. This eliminates the need for external tools to generate alignments and also simplifies the decoding pipeline. Following the results of L'Hostis et al. (2016), we rely only on learned alignments and no other top-1 resources."}, {"heading": "4 Experiments", "text": "We test our vocabulary selection approach on two language pairs: German \u2192 English and Italian \u2192 English. The votes from which we extract the candidate lists are either learned during the entire training (from scratch) or only during the last period (advanced training)."}, {"heading": "4.1 Setup", "text": "For the German \u2192 English translation task, we train an NMT system based on the WMT 2016 training data (Bojar et al., 2016) (3.9 parallel sentences) and use newstest-2014 and newstest-2015 as our dev / test sets. For the Italian \u2192 English translation task, we train our system on a large data set consisting of 20 million parallel sentences. Sentences come from different resources such as Europarl, News Comments, Wikipedia, openSubti-1It is typical to be 2000 (Jean et al., 2015; Mi et al., 2016).tles As a test set, we use Newstestestestestest2009.In all our experiments, we use our in-house NMT implementation, which is similar (Bahdanau et al., 2014; we use subword units deciphered by byte pairs).tles, which we use the vowels, the Newstestestestestestest 2009.In which we base our attention on the Bahdanau-based NT implementation (we use the NMT implementation)."}, {"heading": "4.2 German\u2192English", "text": "The results for the German \u2192 English translation task are presented in Table 1. Applying the vocabulary selection during decoding speeds up decoding by up to 7x compared to baseline decoding without vocabulary selection. However, in our experiments in both languages, the list of the 100 best target candidates per word is a good trade-off between the target words used in the baseline. One of the interesting trends is that continuing education is proving extremely competitive to apply language alignments across the entire NMT education across multiple eras. It should be emphasised again that we only conducted the training facility for one epoch to learn alignments, which suggests that attention weights are very stable once the model is properly formed. While the word-based models are slightly inferior to the BPE models, we note that we are observing previous trends."}, {"heading": "4.3 Italian\u2192English", "text": "Empirical results for the Italian \u2192 English translation task are in Table 3. We can speed up the decoding speed by a factor of 3.6x to 3.9x using a candidate list that comes from the attention of our NMT model. Sweet Spot candidate list is 100. We can speed up decoding by a factor of 3.7, while in BLEU we lose only 0.1 point. Continuing education (where we learn the candidate list only in the last epoch) works as well as the fully trained candidate list. The average candidate per word is even smaller compared to the fully trained candidate list, which makes decoding even faster."}, {"heading": "4.4 Dynamic Vocabulary Selection during Training", "text": "Since we accumulate the attention weights in a sparse alignment matrix, we could also use them to dynamically select the target vocabulary during the training, which would be exactly the same as the large vocabulary NMT; but unlike other approaches, we would not rely on external resources / tools such as Model 1 alignments, phrase tables, etc. We will explain the recipe for this. We will first normalize the sparse matrix and obtain the top n target marks for each source word, as explained in Section 3.2.2. We will begin the NMT training without vocabulary selection and train the entire target vocabulary in the initial stages. We will switch to vocabulary selection mode as soon as the alignment matrix2To avoid outdated probabilities, we normalize / trim the alignment variants at the beginning of each epoch for dynamic vocabulary selection."}, {"heading": "5 Related Works", "text": "Vocabulary selection has been extensively studied in the context of the decoding of NMT (Luong et al., 2015; Mi et al., 2016; L'Hostis et al., 2016), all of which are inspired by the early work of Jean et al. (2015) and use a kind of external strategy (based on word alignments or phrase tables or coexistence counts, etc.) to make vocabulary selection. In contrast, we use the alignments learned from the attention weights in early training to select the target vocabulary. The other difference is that throughout the training, the selected vocabulary is disregarded under these earlier approaches. However, the alignments learned in the previous epoch could be used to select target word vocabularies for the next epoch. Hierarchical Softmax (Morin and Bengio, 2005; Mnih and Hinton, 2009) is known to reduce the number of target words by structuring them."}, {"heading": "6 Summary", "text": "We presented a simple approach to learning source-target alignment directly from the attention layer in Neural Machine Translation. We demonstrated that alignment for vocabulary selection can be used in decoding without using external resources such as aligners, phrase tables, etc. We recommend setting \u03b1thr = 0.1 and top-n candidates to 100 to achieve good performance and faster decoding for most language pairs / records. Our experiments showed decoding acceleration of up to a factor of 7 for different settings. We also demonstrated how this can be used for dynamic vocabulary selection during training."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Neural Machine Translation (NMT) models usually use large target vocabulary sizes to capture most of the words in the target language. The vocabulary size is a big factor when decoding new sentences as the final softmax layer normalizes over all possible target words. To address this problem, it is widely common to restrict the target vocabulary with candidate lists based on the source sentence. Usually, the candidate lists are a combination of external word-to-word aligner, phrase table entries or most frequent words. In this work, we propose a simple and yet novel approach to learn candidate lists directly from the attention layer during NMT training. The candidate lists are highly optimized for the current NMT model and do not need any external computation of the candidate pool. We show significant decoding speedup compared with using the entire vocabulary, without losing any translation quality for two language pairs.", "creator": "LaTeX with hyperref package"}}}