{"id": "1706.03847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Recurrent Neural Networks with Top-k Gains for Session-based Recommendations", "abstract": "RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly.", "histories": [["v1", "Mon, 12 Jun 2017 20:49:23 GMT  (288kb,D)", "http://arxiv.org/abs/1706.03847v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bal\\'azs hidasi", "alexandros karatzoglou"], "accepted": false, "id": "1706.03847"}, "pdf": {"name": "1706.03847.pdf", "metadata": {"source": "META", "title": "Recurrent Neural Networks with Top-k Gains for Session-based Recommendations ", "authors": ["Bal\u00e1zs Hidasi", "Alexandros Karatzoglou"], "emails": ["balazs.hidasi@gravityrd.com", "alexk@tid.es", "Recall@20"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Information Systems \u2192 Collaborative Ltering; \u2022 Computer Methods \u2192 Neural Networks; KEYWORDS relapsing neural networks, loss function, ranking, session-based recommendation"}, {"heading": "1 INTRODUCTION", "text": "In this context, it should be noted that the measures in question are primarily aimed at achieving objectives and objectives."}, {"heading": "1.1 Related Work", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 SAMPLING THE OUTPUT", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "3 LOSS FUNCTION DESIGN", "text": "In this section, we examine the loss functions implemented in GRU4Rec and identify their weaknesses. We propose two ways to stabilize the numerical instability of cross entropy loss, we show how learning deteriorates with the paired losses of TOP1 and BPR when we insert more samples into the output, and we propose a family of loss functions based on the paired losses that alleviate this problem. We note that while our goal is to improve GRU4Rec, the loss functions are still evaluated per example, as the item sequences on which they are based are based on the scores of these samples. The functions proposed in this section can also be used with other models such as matrix factorization."}, {"heading": "3.1 Categorical cross-entropy", "text": "The categorical cross-entropy measures the distance of a proposed (discrete) probability distribution q from the target distribution p as from (1).H (p, q) = \u2212 N \u2211 j = 1 pj logqj (1) This loss is often used in machine learning and deep learning, especially for multi-class class problems. The next item recommendation can be interpreted as a class category, where the class names are the items in the system and item sequences to be associated with the label of the following item. In a single-class scenario - like the next item recommendation - the target distribution is a uniform hot vector over the item phase, with the coordinate corresponding to the target point set to 1. The proposed distribution consists of the items assigned by the algorithm. Output scores must be transformed to form a distribution."}, {"heading": "3.2 Ranking losses: TOP1 & BPR", "text": "The loss is high if the target \"score\" is higher than that of the negative sample. GRU4Rec calculates results for several negative samples per target, and therefore the loss function consists of the average of each pair-wise loss, resulting in a list-wise loss function consisting of pair-wise losses. One of the loss functions is characterized by TOP1 (4). It is a relevant sample function consisting of two parts. The first part aims to push the target score over the score of the samples, while the second part lowers the score of negative samples towards zero. The latter acts as a regulator, but rather than restricting the model weights directly to the negative examples. As all items act as a negative score in a training example or another, it generally reduces the score of negative samples towards zero."}, {"heading": "3.3 Ranking-max loss function family", "text": "To overcome the disappearance of gradients as the number of samples increases, we propose a new family of listwise loss functions based on individual pair-wise losses. The idea is to compare the target number with the most relevant samples, which is the maximum score among the samples. (8) The general structure of the loss is determined by (8).Lpairwise \u2212 max (ri, {r} NSj = 1) = Lpairwise (ri, maxj r j r j) (8) The maximum selection is non-reproducible and therefore cannot be used with gradient descent. Therefore, we use the softmax results to preserve di erentiability. Here, the softmax transformation is used only on the negative examples (i.e. ri is excluded as we are looking for results from the maximum score among the negative examples. This, of course, results in loss functions where each negative sample is considered proportional to the maximum score."}, {"heading": "4 EXPERIMENTS", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "4.1 Using additional samples", "text": "The first series of experiments examines the impact of additional negative samples on the accuracy of the recommendations. The experiments were conducted on the basis of CLASS and VIDEO data sets. As the results are very similar, we exclude the VIDEO results to save some space. Figure 3 shows the performance of the network after additional samples with TOP1, cross-entropy, TOP1-max and BPR-max losses. As this latter scenario is more theoretical because it is not scalable, as theory suggests (see Section 3), TOP1 loss does not cope well with many samples. There is a slight increase in performance with a few additional samples as the probability of relevant samples increases; but performance increases rapidly as many irrelevant samples grow."}, {"heading": "4.2 Loss-functions", "text": "The big improvement in accuracy is due to the combination of additional samples and the loss functions (xedcross-entropy, TOP1-max and BPR-max). Table 2 shows our most important results. In addition to the original RGU4Rec and the itemkNN, we have included results with cross-entropy (XE) loss without additional sample in the assessment that the fixed cross-entropy loss is still only slightly better than TOP1. The increase in the sample and the correct loss function is breathtaking, as the best results exceeded the accuracy of the original GRU4Rec by 15-35% and that of item-kNN by up to 51%. Basically, the best results on RSC15 are the same as those obtained through very expensive data augmentation and the additional ne-tuning by [18]. In contrast to our method, where the increase in training time makes little to nothing difference, the data augmentation could be applied simultaneously with our max of 4% improvement (or max of 4%)."}, {"heading": "5 CONCLUSION", "text": "We have introduced a new class of loss capabilities that, together with an improved sampling strategy, have yielded impressive top-k gains for session-based recommendations to the RNNs. We believe these new losses could be more universally applicable and, together with the corresponding sampling strategies, also offer top-k gains for recommendation settings and algorithms such as matrix factorization or autoencoder. It is also conceivable that these techniques may provide a domain with significant similarities to the recommendation domain in terms of machine learning (e.g. ranking, retrieval) and data structure (e.g. little large input and output space)."}], "references": [{"title": "Precision-oriented Evaluation of Recommender Systems: An Algorithmic Comparison", "author": ["Alejandro Bellogin", "Pablo Castells", "Ivan Cantador"], "venue": "In RecSys\u201911: 5th ACM Conf. on Recommender Systems. 333\u2013336", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "From RankNet to LambdaRank to LambdaMART: An Overview", "author": ["Chris J.C. Burges"], "venue": "Technical Report. https://www.microsoft.com/en-us/research/ publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In SSST-8: 8th Workshop on Syntax, Semantics and Structure in Statistical Translation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Collaborative ltering with recurrent neural networks", "author": ["Robin Devooght", "Hugues Bersini"], "venue": "arXiv preprint arXiv:1608.07400", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Session-based Recommendations with Recurrent Neural Networks", "author": ["Bal\u00e1zs Hidasi", "Alexandros Karatzoglou", "Linas Baltrunas", "Domonkos Tikk"], "venue": "International Conference on Learning Representations (2016)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations", "author": ["Bal\u00e1zs Hidasi", "Massimo Quadrana", "Alexandros Karatzoglou", "Domonkos Tikk"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys \u201916)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback", "author": ["B. Hidasi", "D. Tikk"], "venue": "In ECML-PKDD\u201912, Part II. Number 7524 in LNCS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Shihao Ji", "SVN Vishwanathan", "Nadathur Satish", "Michael J Anderson", "Pradeep Dubey"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "OrdRec: An Ordinal Model for Predicting Personalized Item Rating Distributions", "author": ["Yehuda Koren", "Joe Sill"], "venue": "In Proceedings of the Fifth ACMConference on Recommender Systems (RecSys \u201911)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Amazon.com recommendations: Item-toitem collaborative ltering", "author": ["G. Linden", "B. Smith", "J. York"], "venue": "Internet Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Enlister: Baidu\u2019s Recommender System for the Biggest Chinese Q&A Website", "author": ["Qiwen Liu", "Tianjian Chen", "Jing Cai", "Dianhai Yu"], "venue": "In RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "author": ["Ste en Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt- Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Arti cial Intelligence (UAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In UAI\u201909: 25th Conf. on Uncertainty in Arti cial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Item-based collaborative ltering recommendation algorithms", "author": ["Badrul Sarwar", "George Karypis", "Joseph Konstan", "John Riedl"], "venue": "Int. Conf. on World Wide Web", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "CLiMF: Learning to Maximize Reciprocal Rank with Collaborative Less-is-more Filtering", "author": ["Yue Shi", "Alexandros Karatzoglou", "Linas Baltrunas", "Martha Larson", "Nuria Oliver", "Alan Hanjalic"], "venue": "In Proceedings of the Sixth ACM Conference on Recommender Systems (RecSys", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Improved Recurrent Neural Networks for Session-based Recommendations. In Proceedings of the 1stWorkshop on Deep Learning for Recommender Systems (DLRS 2016)", "author": ["Yong Kiam Tan", "Xinxing Xu", "Yong Liu"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "COFIRANK Maximum Margin Matrix Factorization for Collaborative Ranking", "author": ["Markus Weimer", "Alexandros Karatzoglou", "Quoc Viet Le", "Alex Smola"], "venue": "In Proceedings of the 20th International Conference on Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Recurrent Recommender Networks", "author": ["Chao-Yuan Wu", "Amr Ahmed", "Alex Beutel", "Alexander J. Smola", "How Jing"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM \u201917)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}], "referenceMentions": [{"referenceID": 14, "context": "Until recently many of these recommendations tasks were tackled mainly using relatively simple methods such as item-based collaborative ltering [16] or content-based methods.", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "In recommender systems RNNs have been recently applied to the session-based recommendation setting with impressive results [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "[3]) and in particular ranking objectives and loss functions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user pro le is the item-to-item recommendation approach [12, 16].", "startOffset": 183, "endOffset": 191}, {"referenceID": 14, "context": "One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user pro le is the item-to-item recommendation approach [12, 16].", "startOffset": 183, "endOffset": 191}, {"referenceID": 7, "context": "Long Short-Term Memory (LSTM) [9] networks are a type of RNNs that have been shown to solve the optimization issues the plague vanilla-type RNNs.", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "A slightly simpli ed version of LSTM \u2013 that still maintains all their properties \u2013 are Gated Recurrent Units (GRUs) [4], which we use in this work.", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Recurrent Neural Networks have been used with success in the area of session-based recommendations; [6] proposed a Recurrent Neural Network with a pairwise ranking loss for this task, [18] proposed data augmentation techniques to improve the performance of the RNN for sessionbased recommendations; these techniques have though the side e ect of increasing training times as a single session is split into several sub-sessions for training.", "startOffset": 100, "endOffset": 103}, {"referenceID": 16, "context": "Recurrent Neural Networks have been used with success in the area of session-based recommendations; [6] proposed a Recurrent Neural Network with a pairwise ranking loss for this task, [18] proposed data augmentation techniques to improve the performance of the RNN for sessionbased recommendations; these techniques have though the side e ect of increasing training times as a single session is split into several sub-sessions for training.", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "Session-based RNNs have been augmented [7] with feature information, such as text and images from the clicked/consumed items, showing improved performance over the plain models.", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "RNNs have also been used in more standard user-item collaborative ltering settings where the aim is to model the evolution of the user and items factors [20],[5] where the results are less striking, with the proposed methods barely outperforming standard matrix factorization methods.", "startOffset": 153, "endOffset": 157}, {"referenceID": 3, "context": "RNNs have also been used in more standard user-item collaborative ltering settings where the aim is to model the evolution of the user and items factors [20],[5] where the results are less striking, with the proposed methods barely outperforming standard matrix factorization methods.", "startOffset": 158, "endOffset": 161}, {"referenceID": 17, "context": "One of the rst learning to rank techniques for collaborative ltering was introduced in [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "Further ranking loss function for collaborative ltering were introduced in [17] [14] and [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Further ranking loss function for collaborative ltering were introduced in [17] [14] and [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Further ranking loss function for collaborative ltering were introduced in [17] [14] and [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Another work related to sampling large output spaces in deep networks for e cient loss computations for language models is the \u2019blackout\u2019 method [10], where essentially a sampling procedure similar to the one used in [6] is applied in order to e ciently compute the categorical cross-entropy loss.", "startOffset": 145, "endOffset": 149}, {"referenceID": 4, "context": "Another work related to sampling large output spaces in deep networks for e cient loss computations for language models is the \u2019blackout\u2019 method [10], where essentially a sampling procedure similar to the one used in [6] is applied in order to e ciently compute the categorical cross-entropy loss.", "startOffset": 217, "endOffset": 220}, {"referenceID": 4, "context": "In the remainder of the paper we will refer to the RNN algorithm implemented in [6] as GRU4Rec, the name of the implementation published by the authors on github 1.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "GRU4Rec introduced mini-batch based sampling [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "[6] reported slightly better results than with other losses, but deemed the loss to be unstable for a large fraction of the hyperparameter space and thus advised against its use.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "The other loss function (5) is based on the popular Bayesian Personalized Ranking (BPR) [15] loss.", "startOffset": 88, "endOffset": 92}, {"referenceID": 4, "context": "Even though we showed that the heuristic TOP1 loss is sensitive to relevant samples with very high scores, it was found to be performing better than BPR in [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "The datsets and the split are exactly the same for RSC15 as in [6]; and for VIDXL and CLASS as in [7].", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "The datsets and the split are exactly the same for RSC15 as in [6]; and for VIDXL and CLASS as in [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "VIDEO is of the same source as in [6], but a slightly di erent subset.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Since the size of the VIDXL test set is large, we compare the target item\u2019s score to that of the 50,000 most popular items during testing, similarly to [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "While this evaluation for VIDXL overestimates the performance, the comparison of algorithms remain fair [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)[8, 13].", "startOffset": 96, "endOffset": 103}, {"referenceID": 11, "context": "Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)[8, 13].", "startOffset": 96, "endOffset": 103}, {"referenceID": 4, "context": "Results for RSC15, VIDXL and CLASS are taken directly from corresponding papers [6, 7] and measured with the optimal hyperparameters in [6] for VIDEO.", "startOffset": 80, "endOffset": 86}, {"referenceID": 5, "context": "Results for RSC15, VIDXL and CLASS are taken directly from corresponding papers [6, 7] and measured with the optimal hyperparameters in [6] for VIDEO.", "startOffset": 80, "endOffset": 86}, {"referenceID": 4, "context": "Results for RSC15, VIDXL and CLASS are taken directly from corresponding papers [6, 7] and measured with the optimal hyperparameters in [6] for VIDEO.", "startOffset": 136, "endOffset": 139}, {"referenceID": 16, "context": "The best results on RSC15 are basically the same what was achieved by very costly data augmentation and additional ne tuning by [18].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance bene ts over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative ltering approaches. Unlike data augmentation-based improvements, our method does not increase training times signi cantly.", "creator": "LaTeX with hyperref package"}}}