{"id": "1706.03880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "MNL-Bandit: A Dynamic Learning Approach to Assortment Selection", "abstract": "We consider a dynamic assortment selection problem, where in every round the retailer offers a subset (assortment) of $N$ substitutable products to a consumer, who selects one of these products according to a multinomial logit (MNL) choice model. The retailer observes this choice and the objective is to dynamically learn the model parameters, while optimizing cumulative revenues over a selling horizon of length $T$. We refer to this exploration-exploitation formulation as the MNL-Bandit problem. Existing methods for this problem follow an \"explore-then-exploit\" approach, which estimate parameters to a desired accuracy and then, treating these estimates as if they are the correct parameter values, offers the optimal assortment based on these estimates. These approaches require certain a priori knowledge of \"separability\", determined by the true parameters of the underlying MNL model, and this in turn is critical in determining the length of the exploration period. (Separability refers to the distinguishability of the true optimal assortment from the other sub-optimal alternatives.) In this paper, we give an efficient algorithm that simultaneously explores and exploits, achieving performance independent of the underlying parameters. The algorithm can be implemented in a fully online manner, without knowledge of the horizon length $T$. Furthermore, the algorithm is adaptive in the sense that its performance is near-optimal in both the \"well separated\" case, as well as the general parameter setting where this separation need not hold.", "histories": [["v1", "Tue, 13 Jun 2017 00:54:47 GMT  (737kb,D)", "http://arxiv.org/abs/1706.03880v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shipra agrawal", "vashist avadhanula", "vineet goyal", "assaf zeevi"], "accepted": false, "id": "1706.03880"}, "pdf": {"name": "1706.03880.pdf", "metadata": {"source": "CRF", "title": "MNL-Bandit: A Dynamic Learning Approach to Assortment Selection *", "authors": ["Shipra Agrawal", "Vashist Avadhanula", "Vineet Goyal", "Assaf Zeevi"], "emails": ["sa3305@columbia.edu", "vavadhanula18@gsb.columbia.edu", "vg2277@columbia.edu", "assaf@gsb.columbia.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves; most of them are able to survive themselves if they aren't able to survive themselves; most of them are able to survive themselves; most of them are able to survive themselves; most of them are able to survive themselves; most of them are not able to survive themselves; most of them are not able to survive themselves; and most of them are not able to survive themselves, and most of them are not able to survive themselves, and most of them are not able to survive themselves."}, {"heading": "1. Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Overview of the problem.", "text": "In many industries, including retail and online advertising, it is impractical to solve such a problem, where the seller must select a subset of products that he can offer from a universe of replaceable items to maximize expected revenue. Selection models capture substitution effects between products by indicating the likelihood that a consumer will select a product based on the set offered. Traditionally, assortment decisions are made at the beginning of the sales period based on the estimated selection model from historical data; see Kok and Fisher (2007) for a detailed review. In this work, we focus on the dynamic version of the problem where the retailer must consider consumer preferences and maximized revenue at the same time. In many business applications, such as fast fashion and online retailing, new products can be imported or removed from the assortments offered, and the sales horizon for a particular product may be short."}, {"heading": "1.2. Contributions", "text": "We provide an efficient online algorithm that reasonably balances our regrets. (This regret is the most common \"result.\" Our algorithm is online in the sense that it does not require prior knowledge of the instance parameters or the time horizon. Furthermore, the regret we present for this algorithm is not asymptotic or parameter-independent. The \"big-oh\" notation is used for Brevity and contains only absolute constants. To achieve the best of our knowledge, this is the first such policy that has a parameter-independent regret for the MNL voting model. The assumption regarding no-purchase is fairly natural and is a norm in online retailing, for example."}, {"heading": "2. Problem formulation", "text": "The basic range problem is independent of C1, we assume that the seller selects a range list = 1,., N \"and observes the purchasing decision of the customer (1). As already mentioned, we assume that consumer preferences are modelled using a multinomial model (MNL). Under this model, the probability that a consumer buys a product when offering a range list is fully taken into account. (2), if it is a range list. (2), is conditioned by, pi (S): = P (Ct = i) St = S = vi v0 + vj, if i (0)., otherwise, the attraction parameter for the product is i in the MNL model. The random variables {Ct = 1.2,.} are conditioned conditionally independent, namely, Ct on the event {St = S}."}, {"heading": "3. The proposed policy", "text": "In this section, we describe our proposed policy for the MNL bandit problem. The policy was designed using the characteristics of the MNL model, which is based on the principle of optimism under uncertainty."}, {"heading": "3.1. Challenges and overview", "text": "A key difficulty in applying standard multi-arm bandit techniques to this problem is that the response observed when offering a product that i is not independent of other products in the S range cannot, therefore, be treated directly as N-independent weapons. As already mentioned, a naive extension of the MAB algorithms for this problem would treat each of the viable ranges as an arm, resulting in a mathematically inefficient algorithm with exponential regret. Our policy uses the specific properties of the dependency structure in the MNL model to obtain an efficient algorithm with order. Our policy is based on a non-trivial extension of the UCB algorithm Auer et al. (2002) It uses previous observations to maintain increasingly precise ceilings of trust for the MNL parameters {vi, i = 1, N}, and uses these to (implicitly) obtain an estimate of expected revenues (R) for each viable S range."}, {"heading": "3.2. Details of the policy", "text": "We divide the time horizon into epochs, in which we offer an answer to each epoch until a purchase result has occurred. (D) We divide the time horizon into epochs, in which we achieve a purchase result. (D) E \"contains all time steps after the end of the epoch.\" (D) E \"includes all time steps after the end of the epoch.\" (D) E \"is a geometric random variable with probability of success, which is defined as the probability of a no-purchase in S.\" The total number of epochs L in time T is implicitly defined as the minimum number for which L. \"(D) -E.\" At the end of each epoch, we update our estimates for the parameters of the MNL used in the epoch."}, {"heading": "4. Main results", "text": "Assumption 4.1 We assume the following assumptions: 1. The MNL parameter corresponding to each product fulfils vi \u2264 v0 = 1. 2. The S range is such that S-S and Q-S imply that Q-S is equivalent to the assumption that \"no option to buy\" is preferable to any other product. We note that this is true in many realistic environments, especially in online retail and online advertising. The second assumption implies that removing a product from a feasible range preserves feasibility, including most of the substantive constraints that arise in practice, including cardinality and matroid limitations in general. We would like to point out that the initial assumption is made to facilitate the presentation of the most important results and is not central to deduce limits from the regret. In Section 5, we loosen this assumption and deduce limits of regret that apply to all parameters. Our main result is the following problem: the upper instance is set from 1.0 to 1.0, whereby T is defined."}, {"heading": "4.1. Proof Outline", "text": "The first step in our repentance analysis is to prove the following two properties of the vUCBi estimates \"converted in the vUCBi estimates,\" which are converted in the vUCBi estimates, \"in order to achieve the true value with high probability.\" We provide the exact statements for the properties mentioned above, \"such as the upper safety the actual parameters vi, akin which the upper safety is used for the upper safety.\" We provide the exact statements for the properties mentioned above in Lemma 4.1. These properties follow from an observation that corresponds to the IIA (Independence of Irrelevant Alternatives)."}, {"heading": "4.2. Upper confidence bounds", "text": "In this section, we will show that the upper confidence limits vUCBi, \"converge with the true parameters vi from above. Specifically, we have the following result. Lemma 4.1 For each\" we have: 1. vUCBi, \"\u2265 vi with a probability of at least 1 \u2212 5\" for all i = 1,..., n. 2. There are constants C1 and C2 like this vUCBi, \"\u2212 vi \u2264 C1 \u221a vi log ('+ 1) Ti (') + C2log (') Ti (') with a probability of at least 1 \u2212 5.\" We first state that the estimates v-i, \"\u2264 L are unbiased i.i.d estimates of the true parameter vi for all products. It is not immediately a priori clear whether the estimates v-i,\", \"\u2264 L,\" are independent. In our setting it is possible that the distribution of the estimate v-i, \"depends on the offered sorting S,\" i.e., that we depend on the previous estimate in this moment, i.e. that v-1."}, {"heading": "4.3. Optimistic estimate and convergence rates", "text": "In this section we show that the estimated revenues approach the optimal expected revenues from above. First, we show that the estimated revenues represent an upper confidence limit to the optimal revenues. In particular, we have the following result.Lemma 4.2 Suppose it is the range with the highest expected revenues, and Algorithm 1 offers S 'in each epoch. In Lemma A.3 we show that the optimal expected revenues are monotonous in the MNL parameters. It is important to note that we do not claim that the expected revenues are generally a monotonous function, but only the value of the expected revenues corresponding to the optimal assortment increases. (In Lemma A.3 we show that the optimal expected revenues are monotonous in the MNL parameters.) The result is derived from Lemma 4.1, where we have determined that vUCBi, > vi is highly likely."}, {"heading": "5. Extensions", "text": "In this section, we expand our approach (algorithm 1) to the setting in which the assumption that vi \u2264 v0 is loosened for all i. The main ideas in the extension remain the same as our previous approach, in particular optimism under uncertainty, and our policy is structurally similar to algorithm 1. The modified policy requires a small but mandatory initial exploration period. Contrary to the work of Rusmevichientong et al. (2010) and Acid \u0301 and Zeevi (2013), the exploration phase is not dependent on specific instance parameters and is constant for all problem situations. Therefore, our algorithm is parameter independent and remains relevant for practical applications. In addition, our approach simultaneously explores and utilizes the initial exploration phase. In particular, the initial exploration phase consists of ensuring that the estimates converge with the true parameters from the above areas, especially in cases where the attraction parameter, vi (frequency of purchase) is large."}, {"heading": "6. Parameter dependent regret bounds", "text": "In this section, we derive an O (logT) remorse for algorithm 1, which is parameter-dependent (sections 4 and 5). In sections 4 and 5, we have set worst-case remorse limits for algorithm 1, which apply to all problem cases. Although our algorithm ensures that the exploration exploitation trade-off is balanced at all times, for problem cases that are \"well separated,\" our algorithm quickly approaches the optimal solution, which leads to better remorse limits. Specifically, we consider problem cases where the optimal selection and \"second-best\" sorting are sufficiently \"separated and derive an O (logT) remorse from it, which depends on the parameters of the instance. In contrast to theAlgorithm 2 exploration exploitation algorithm for MNL bandit, general parameters 1: vUCBi, 0 = 1 for all i = 1,."}, {"heading": "6.1. Proof outline.", "text": "In this context, we analyze regret by looking separately at the epochs that fulfill certain desirable characteristics and those that do not. Specifically, we refer to the epoch as a \"good\" epoch when the parameters, vUCBi \"satisfy the following property, 0 \u2264 vUCBi,\" \u2212 vi \u2264 C1 \"the epoch is a good epoch with a high probability (1 \u2212 5\") and we show that regret due to bad epochs is limited by a constant (see Appendix C). Therefore, we focus on good epochs and show that there is a constant epoch, so that after each product at least good epochs are offered."}, {"heading": "7. Lower bounds and near-optimality of the proposed policy", "text": "In this section, we consider the special case of TU restrictions, namely the problem of cardinality limitation in assortment optimization, and note that each policy must bear a regret of the burden (\u221a NT / K). Specifically, we prove the following result.Theorem 4 There is a (random) example of the MNL bandit problem with v0 \u2265 vi, i = 1,..., N, so that for each N, K < N, T \u2265 N and any policy that offers assortments S\u03c0t, | \u2264 K in due course t, we have regulation (T, v): = E\u03c0 (T \u0445 t = 1 R (S \u0445, v) \u2212 R (S\u0432t, v))) \u2265 C \u221a NT Kwhere the assortments of K cardinality are (at best) with the maximum expected revenues, and C is an absolute constant."}, {"heading": "7.1. Proof overview", "text": "We prove theorem 4 by reducing it to a parametric multi-armed bandit (MAB) problem for which a lower limit is known. Definition 7.1 (MAB example IMAB) We define IMAB as a (random) instance of the MAB problem with N \u2265 2 Bernoulli weapons and the following parameters (probability of reward 1) \u00b5i = {\u03b1, if i 6 = j, if i = j, for all i = 1,.., N, where j is uniformly randomly selected by {1,.., N}, \u03b1 < 1 and = 1 100 \u221a N\u03b1, T. Throughout this section we will use the terms algorithm and politics interchangeably. An algorithm A is called online if it adaptively selects a historical dependency."}, {"heading": "7.2. Construction of the MAB algorithm using the MNL algorithm", "text": "Algorithm 3 provides the exact construction of AMAB, which simulates the AMNL algorithm as a \"black box.\" Note that AMAB pulls the arms in time steps t = 1,..., T. These arm pulls are linked together by simulations of AMNL steps (Call AMNL, Feedback to AMNL). If Step \"is simulated by AMNL, it uses the feedback of 1,...\" \u2212 1 to suggest a range of S \"; and recalls feedback from AMNL on which product (or product) has been purchased from those offered in S, with the probability of purchase of product i,. \u2212 S'vi. Before it is shown that AMAB actually delivers the correct feedback to AMNL in the\" th step for each product, \"we introduce some comments."}, {"heading": "7.3. Proof of Theorem 4.", "text": "We prove the result by specifying three key results. First, we include regret for the MAB algorithm = 7.7. Then we demonstrate a lower limit for regret for the MNL algorithm, AMNL. Finally, we refer to the regret of AMAB and AMNL and use the established lower and upper limits to show a contradiction. For the rest of this evidence, we assume that L is the total number of calls to AMNL in AMNL in AMNL. Let S be the optimal selection for IMNL. For each instantiation of IMNL, it is easy to see that the optimal selection of K contains items that are all equipped with parameters, i.e. it contains all i such that d i K e = j. Therefore V (S) = K (\u03b1 +) = K\u00b5j.Upper limit for regretting the MAB algorithms. The first step in our analysis is to prove an upper limit for the regret algorithm."}, {"heading": "8. Computational study", "text": "In this section, we present insights from numerical experiments that test the empirical performance of our policy and highlight some of its most prominent features. We examine the performance of Algorithm 1 from the perspective of robustness in terms of the \"separability parameter\" of the underlying instance. In particular, we look at different degrees of separation between the optimal and the second-best turnover and perform a numerical regret analysis. We compare the performance of Algorithm 1 with the approach in Acid \ufffd and Zeevi (2013) for different degrees of separation between the optimal and the suboptimal turnover. We observe that if the separation between the optimal range and the second-best range is sufficiently small, the approach collapses in Acid \ufffd and Zeevi (2013), i.e. generates linear regret, while the regret of Algorithm 1 only increases sublinearly in terms of the sales horizon. We also present results from a simulated study of a real data set."}, {"heading": "8.1. Robustness of Algorithm 1.", "text": "We will consider a parametric instance (see Example 8.1), in which the separation between the revenues of the optimal range and the next best range is determined by the parameter and compare the performance of algorithm 1 for different values of the experimental setup. We will consider the parametric MNL setting with N = 10, K = 4, ri = 1 for all i and the usage parameters v0 = 1 and for i = 1.,.., N, vi = {0.25 +, if i, {1,2,9,10} 0.25, otherwise (8.1), where 0 < < 0.25; remember the difference between revenues that correspond to the optimal range and the next best range. Note that this problem has a unique optimal sorting, {1,2,9,10} with an expected turnover of 1 + 4 + 4 and the next best turnover of 1 / 3 + 0.3 values that we consider for different 0.25."}, {"heading": "8.2. Comparison with existing approaches.", "text": "In this section, we present a calculation study comparing the performance of our algorithm with that of Sour \u0301 and Zeevi (2013). (To our knowledge, Sour \u0301 and Zeevi (2013) are currently the best existing approach to our problem.) In order to be implemented, their approach requires certain a priori information about a \"separability parameter\"; roughly speaking, the measurement of the degree to which the optimal and next-best ranges are distinguished from a revenue point of view. More specifically, their algorithm follows an exploration and exploit approach in which each product must initially be offered for a minimum period of time determined by an estimate of the said \"separability parameter.\" After this mandatory exploration phase, the parameters of the selection model are estimated on the basis of past observations and the optimal sorting is offered according to the estimated parameters for subsequent consumers."}, {"heading": "8.3. Performance of Algorithm 1 on real data.", "text": "We look at the \"UCI Car Evaluation Database\" (see Lichman (2013), which contains attributes based on information from N = 1728 cars and consumer ratings for each car. Exact details of the attributes are given in Table 1. Evaluation for each car is also available. Specifically, each car is associated with one of the following four ratings, unacceptable, acceptable and very good. We assume that consumer choice is modelled by the MNL model, in which the median benefit of a product is linear in the values of the attributes. Morespecifically, we will convert the categorical attributes in Table 1 to attributes with binary values by adding stupid attributes (e.g. \"price very high,\" \"price low\" are considered as two different attributes that can take values)."}, {"heading": "Acknowledgments", "text": "V. Goyal is partially supported by NSF grants CMMI-1351838 (CAREER) and CMMI-1636046. A. Zeevi is partially supported by NSF grants NetSE-0964170 and BSF-2010466."}, {"heading": "A. Proof of Theorem 1", "text": "In the rest of this section, we provide detailed proof of theorem 1 following the event discussed in section 4.1. The proof is organized as follows: In section A.1, we perform the proof of Lemma 4.1 and in section A.2, we perform similar properties for estimates R (S), \"and then we expand these properties to complete the necessary properties of vUCBi.\" Unbiased Estimates.Lemma A.1 The moment we have conditioned the function of the estimate is determined by \"E\u03c0 i\" and v, \"and then we expand these properties to establish the necessary properties of vUCBi.\" Lemma A.1 The moment we have conditioned the function of the estimate is given by \"Ectuv.\" We (ectuv) have these properties to establish the necessary properties of vUCBi. \""}, {"heading": "B. Proof of Theorem 2", "text": "The proof for Theorem 2 is very similar to the proof for Theorem 1, and we assume that the initial exploration phase is actually limited, and then follow the proof of Theorem 1 to determine the correctness of confidence intervals, optimistic sorting, and finally to derive convergence rates and repentance limits. It is easy to see that the number of exploration periods is limited by 48N logT, where T is the selling horizon under consideration. We then use the observation that the length of any epoch is a geometric variable limiting the total expected duration of the exploration phase. Lemma B.1 Let's limit the total number of epochs in Algorithm 2 and the total number of exploration phases."}, {"heading": "C. Parameter dependent bounds", "text": "The proof of quandary 6.2 Let's assume, for the sake of contradiction, that algorithm 1 offered suboptimal assortments in more than N (N \u2212 1) 2 \u03c4 epochs. Let's be the epoch by which we offered the first N\u03c4 sub-optimal assortments and S1 bethe subset of products offered at least once per epoch (otherwise the total number of offers would not be more than 1, i.e.S1 = {i | Ti (\"1) may be the epoch through which we offered the next (N \u2212 1) Lemgeon suboptimal assortments (otherwise the total number of offers would not be more than 1), i.e., S1 | 2 are the epoch through which we offered the next (N \u2212 1) suboptimal assortments and S2 we are the subset of products offered in at least two epochs."}, {"heading": "D. Multiplicative Chernoff Bounds", "text": "We will extend the Chernoff boundaries as in Mitzenmacher and Upfal (2005) 1 to geometric random variables and establish the following concentration inequality. (Theorem 5) Let us consider n i.i.d geometric random variables X1, \u00b7 \u00b7, Xn with parameter p, i.e. for all iPr (Xi = m) = (1 \u2212 p) mp \u00b2 m = {0,1,2, \u00b7 \u00b7 \u2212 \u2212 and let us leave \u00b5 = E (Xi) = 1 \u2212 pp."}, {"heading": "E. Proof of Lemma 7.1", "text": "We follow the evidence for a lower limit for the Bernoulli instance with parameters 12. We first determine a limit for the KL divergence which will be useful for us.Lemma E.1 Lass p and q = 2 Bernoulli distributions with parameters \u03b1 + and \u03b1. Then the KL divergence between the distributions p and q is limited by 4K 2, KL (p \u00b2 q) \u2264 4 \u03b1 2.KL (p \u00b2 q) = 1 \u00b7 Log \u03b1 \u03b1 + (1 \u2212 \u03b1) Log 1 \u2212 \u03b1 \u2212 \u03b1 \u2212 \u03b1 1 \u2212 \u03b1 Log 1 \u2212 \u03b1 E.2 Log (1 \u2212 \u03b1) = 1 \u2212 Log (p \u00b2 q) \u2264 4 \u2264 KL (p \u00b2 q) = 1 \u2212 \u03b1 2.KL (p \u00b2 q)) Log (1 \u2212 x) Log 1 \u2212 \u03b1 e \u2212 x and the Taylor series for \u2212 Log 1 \u2212 x x2 for the supports."}], "references": [{"title": "Fast probabilistic algorithms for hamiltonian circuits and matchings", "author": ["D. Angluin", "L.G. Valiant"], "venue": null, "citeRegEx": "Angluin and Valiant.,? \\Q1977\\E", "shortCiteRegEx": "Angluin and Valiant.", "year": 1977}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Auer", "Peter", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Proceedings of the Ninth Annual ACM Symposium on Theory of Computing . STOC", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Dynamic pricing with limited supply", "author": ["M. Babaioff", "S. Dughmi", "R. Kleinberg", "A. Slivkins"], "venue": null, "citeRegEx": "Babaioff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Babaioff et al\\.", "year": 2015}, {"title": "Discrete choice analysis: theory and application to travel demand", "author": ["M. Ben-Akiva", "S. Lerman"], "venue": "Transactions on Economics and Computation", "citeRegEx": "Ben.Akiva and Lerman.,? \\Q1985\\E", "shortCiteRegEx": "Ben.Akiva and Lerman.", "year": 1985}, {"title": "A markov chain approximation to choice modeling", "author": ["press. Blanchet", "Jose", "Guillermo Gallego", "Vineet Goyal"], "venue": null, "citeRegEx": "Blanchet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchet et al\\.", "year": 2016}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": null, "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Assortment planning under the multinomial logit model with", "author": ["J. Davis", "G. Gallego", "H. Topaloglu"], "venue": null, "citeRegEx": "Davis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2013}, {"title": "Near-optimal algorithms for capacity constrained assortment optimization", "author": ["A. D\u00e9sir", "V. Goyal"], "venue": null, "citeRegEx": "D\u00e9sir and Goyal.,? \\Q2014\\E", "shortCiteRegEx": "D\u00e9sir and Goyal.", "year": 2014}, {"title": "Capacity constrained assortment optimization under the markov", "author": ["A. SSRN . D\u00e9sir", "V. Goyal", "D. Segev", "C. Ye"], "venue": null, "citeRegEx": "D\u00e9sir et al\\.,? \\Q2015\\E", "shortCiteRegEx": "D\u00e9sir et al\\.", "year": 2015}, {"title": "A nonparametric approach to modeling choice with limited data", "author": ["V. Farias", "S. Jagabathula", "D. Shah"], "venue": null, "citeRegEx": "Farias et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Farias et al\\.", "year": 2013}, {"title": "Parametric bandits: The generalized linear case", "author": ["S. Filippi", "O. Cappe", "A. Garivier", "C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "Constrained assortment optimization for the nested logit model", "author": ["G. Gallego", "H. Topaloglu"], "venue": null, "citeRegEx": "Gallego and Topaloglu.,? \\Q2014\\E", "shortCiteRegEx": "Gallego and Topaloglu.", "year": 2014}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "Proceedings of the Fortieth", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "ology and application. Operations Research", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Individual choice behavior: A theoretical analysis", "author": ["M. Lichman"], "venue": "UCI machine learning repository. URL http://archive.ics.uci.edu/ml. Luce, R.D", "citeRegEx": "Lichman,? \\Q2013\\E", "shortCiteRegEx": "Lichman", "year": 2013}, {"title": "logit choice model and capacity constraint. Operations research", "author": ["P. Rusmevichientong", "J.N. Tsitsiklis"], "venue": "Linearly parameterized bandits. Math. Oper. Res", "citeRegEx": "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong and Tsitsiklis.", "year": 2010}, {"title": "Optimal dynamic assortment planning with demand learning", "author": ["D. Saur\u00e9", "A. Zeevi"], "venue": null, "citeRegEx": "Saur\u00e9 and Zeevi.,? \\Q2013\\E", "shortCiteRegEx": "Saur\u00e9 and Zeevi.", "year": 2013}, {"title": "Revenue management under a general discrete choice model of consumer", "author": ["K. Talluri", "G. van Ryzin"], "venue": "Service Operations Management", "citeRegEx": "Talluri and Ryzin.,? \\Q2004\\E", "shortCiteRegEx": "Talluri and Ryzin.", "year": 2004}, {"title": "Concentration Bounds. From Corollary A.1, it follows that v\u0302i,\u03c4 , \u03c4 \u2208 Ti(`) are i.i.d geometric random variables with mean vi. We will use this observation and extend the multiplicative ChernoffHoeffding bounds discussed in Mitzenmacher and Upfal (2005) and Babaioff et al. (2015) to geometric random variables and derive the result", "author": ["E\u03c0 (v\u0302i", "`) = vi"], "venue": null, "citeRegEx": ".v\u0302i et al\\.,? \\Q2015\\E", "shortCiteRegEx": ".v\u0302i et al\\.", "year": 2015}, {"title": "2015) to geometric variables and prove the above result. Lemma 4.1 follows directly from Lemma A.2. The proof of Lemma A.2 is long and tedious and in the interest of continuity, we complete the proof in Appendix D. Following the proof", "author": ["Mitzenmacher", "Upfal", "Babaioff"], "venue": null, "citeRegEx": "Mitzenmacher et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitzenmacher et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "(The model was introduced independently by Luce (1959) and Plackett (1975), see also Train (2009), McFadden (1978), Ben-Akiva and Lerman (1985) for further discussion and survey of other commonly used choice models.", "startOffset": 116, "endOffset": 144}, {"referenceID": 3, "context": "(The model was introduced independently by Luce (1959) and Plackett (1975), see also Train (2009), McFadden (1978), Ben-Akiva and Lerman (1985) for further discussion and survey of other commonly used choice models.) If the consumer preferences (MNL parameters in our setting) are known a priori, then the problem of computing the optimal assortment, which we refer to as the static assortment optimization problem, is well studied. Talluri and van Ryzin (2004) consider the unconstrained assortment planning problem under the MNL model and present a greedy approach to obtain the optimal", "startOffset": 116, "endOffset": 462}, {"referenceID": 5, "context": "Recent works of Davis et al. (2013) and D\u00e9sir and Goyal (2014) consider assortment planning problems under MNL with various constraints.", "startOffset": 16, "endOffset": 36}, {"referenceID": 5, "context": "Recent works of Davis et al. (2013) and D\u00e9sir and Goyal (2014) consider assortment planning problems under MNL with various constraints.", "startOffset": 16, "endOffset": 63}, {"referenceID": 5, "context": "Recent works of Davis et al. (2013) and D\u00e9sir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al.", "startOffset": 16, "endOffset": 195}, {"referenceID": 5, "context": "Recent works of Davis et al. (2013) and D\u00e9sir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al. (2011), Gallego and Topaloglu (2014) and Li et al.", "startOffset": 16, "endOffset": 216}, {"referenceID": 5, "context": "Recent works of Davis et al. (2013) and D\u00e9sir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al. (2011), Gallego and Topaloglu (2014) and Li et al.", "startOffset": 16, "endOffset": 246}, {"referenceID": 5, "context": "Recent works of Davis et al. (2013) and D\u00e9sir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al. (2011), Gallego and Topaloglu (2014) and Li et al. (2015)), Markov Chain (Blanchet et al.", "startOffset": 16, "endOffset": 267}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al.", "startOffset": 23, "endOffset": 70}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al.", "startOffset": 23, "endOffset": 117}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature.", "startOffset": 23, "endOffset": 143}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al.", "startOffset": 23, "endOffset": 258}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon.", "startOffset": 23, "endOffset": 290}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon.", "startOffset": 23, "endOffset": 317}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other.", "startOffset": 23, "endOffset": 461}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an \u201cexplore first and exploit later\u201d approach.", "startOffset": 23, "endOffset": 575}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an \u201cexplore first and exploit later\u201d approach.", "startOffset": 23, "endOffset": 602}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an \u201cexplore first and exploit later\u201d approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are \u201cwell separated,\u201d they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al.", "startOffset": 23, "endOffset": 1929}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an \u201cexplore first and exploit later\u201d approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are \u201cwell separated,\u201d they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al.", "startOffset": 23, "endOffset": 1969}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an \u201cexplore first and exploit later\u201d approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are \u201cwell separated,\u201d they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al. (2010)) problems.", "startOffset": 23, "endOffset": 2023}, {"referenceID": 4, "context": "(2015)), Markov Chain (Blanchet et al. (2016) and D\u00e9sir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Saur\u00e9 and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an \u201cexplore first and exploit later\u201d approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are \u201cwell separated,\u201d they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al. (2010)) problems. However, these do not apply directly to our problem, since the revenue corresponding to an assortment is nonlinear in problem parameters. Other works (see Chen et al. (2013)) have considered versions of MAB where one can play a subset of arms in each round and the expected reward is a function of rewards for the arms played.", "startOffset": 23, "endOffset": 2208}, {"referenceID": 13, "context": "This is comparable to the bounds established in Saur\u00e9 and Zeevi (2013) and Rusmevichientong et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 13, "context": "This is comparable to the bounds established in Saur\u00e9 and Zeevi (2013) and Rusmevichientong et al. (2010), even though we do not require any prior information on \u2206 unlike the aforementioned work.", "startOffset": 48, "endOffset": 106}, {"referenceID": 5, "context": ", we can handle matroid constraints such as assignment, partition and more general totally unimodular constraints (see Davis et al. (2013)).", "startOffset": 119, "endOffset": 139}, {"referenceID": 5, "context": ", we can handle matroid constraints such as assignment, partition and more general totally unimodular constraints (see Davis et al. (2013)). Our algorithm is predicated on upper confidence bound (UCB) type logic, originally developed to balance the aforementioned exploration-exploitation trade-off in the context of the multi-armed bandit (MAB) problem (cf. Lai and Robbins (1985) and Auer et al.", "startOffset": 119, "endOffset": 382}, {"referenceID": 1, "context": "Lai and Robbins (1985) and Auer et al. (2002)).", "startOffset": 27, "endOffset": 46}, {"referenceID": 6, "context": "We refer the reader to Davis et al. (2013) for a detailed discussion on assortment and pricing optimization problems that can be formulated under the TU constraints.", "startOffset": 23, "endOffset": 43}, {"referenceID": 1, "context": "Our policy is based on a non-trivial extension of the UCB algorithm Auer et al. (2002). It uses the past observations to maintain increasingly accurate upper confidence bounds for the MNL parameters {vi, i = 1, .", "startOffset": 68, "endOffset": 87}, {"referenceID": 6, "context": "There are efficient polynomial time algorithms to solve the static assortment optimization problem under MNL model with known parameters (see Davis et al. (2013), Rusmevichientong et al.", "startOffset": 142, "endOffset": 162}, {"referenceID": 6, "context": "There are efficient polynomial time algorithms to solve the static assortment optimization problem under MNL model with known parameters (see Davis et al. (2013), Rusmevichientong et al. (2010)).", "startOffset": 142, "endOffset": 194}, {"referenceID": 1, "context": "Intuitively, these properties establish v UCB i,` as upper confidence bounds converging to actual parameters vi, akin to the upper confidence bounds used in the UCB algorithm for MAB in Auer et al. (2002). We provide the precise statements for the above mentioned properties in Lemma 4.", "startOffset": 186, "endOffset": 205}, {"referenceID": 2, "context": "We will use this observation and extend the multiplicative Chernoff-Hoeffding bounds discussed in Mitzenmacher and Upfal (2005) and Babaioff et al. (2015) to geometric random variables and derive the result.", "startOffset": 132, "endOffset": 155}, {"referenceID": 16, "context": "(2010) and Saur\u00e9 and Zeevi (2013), the exploratory period does not depend on the specific instance parameters and is constant for all problem instances.", "startOffset": 11, "endOffset": 34}, {"referenceID": 16, "context": "(2010) and Saur\u00e9 and Zeevi (2013) for the cardinality constrained problem.", "startOffset": 11, "endOffset": 34}, {"referenceID": 16, "context": "(2010) and Saur\u00e9 and Zeevi (2013) for the cardinality constrained problem. (In fact our algorithm also has improved regret bounds compared to the O(N 2 log T ) bound established by Rusmevichientong et al. (2010)).", "startOffset": 11, "endOffset": 212}, {"referenceID": 5, "context": "1 is a simple extension of the proof of the \u03a9( \u221a NT ) lower bound for the Bernoulli instance with parameters 1 2 and 1 2 + (for example, see Bubeck and Cesa-Bianchi (2012)), and has been provided in Appendix E for the sake of completeness.", "startOffset": 141, "endOffset": 172}, {"referenceID": 16, "context": "We contrast the performance of Algorithm 1 with the approach in Saur\u00e9 and Zeevi (2013) for different levels of separation between the optimal and sub-optimal revenues.", "startOffset": 64, "endOffset": 87}, {"referenceID": 16, "context": "We contrast the performance of Algorithm 1 with the approach in Saur\u00e9 and Zeevi (2013) for different levels of separation between the optimal and sub-optimal revenues. We observe that when the separation between the optimal assortment and second best assortment is sufficiently small, the approach in Saur\u00e9 and Zeevi (2013) breaks down, i.", "startOffset": 64, "endOffset": 324}, {"referenceID": 16, "context": "In this section, we present a computational study comparing the performance of our algorithm to that of Saur\u00e9 and Zeevi (2013). (To the best of our knowledge, Saur\u00e9 and Zeevi (2013) is currently the best existing approach for our problem setting.", "startOffset": 104, "endOffset": 127}, {"referenceID": 16, "context": "In this section, we present a computational study comparing the performance of our algorithm to that of Saur\u00e9 and Zeevi (2013). (To the best of our knowledge, Saur\u00e9 and Zeevi (2013) is currently the best existing approach for our problem setting.", "startOffset": 104, "endOffset": 182}, {"referenceID": 16, "context": "In this section, we present a computational study comparing the performance of our algorithm to that of Saur\u00e9 and Zeevi (2013). (To the best of our knowledge, Saur\u00e9 and Zeevi (2013) is currently the best existing approach for our problem setting.) To be implemented, their approach requires certain a priori information of a \u201cseparability parameter\u201d; roughly speaking, measuring the degree to which the optimal and next-best assortments are distinct from a revenue standpoint. More specifically, their algorithm follows an explore-then-exploit approach, where every product is first required to be offered for a minimum duration of time that is determined by an estimate of said \u201cseparability parameter.\u201d After this mandatory exploration phase, the parameters of the choice model are estimated based on the past observations and the optimal assortment corresponding to the estimated parameters is offered for the subsequent consumers. If the optimal assortment and the next best assortment are \u201cwell separated,\u201d then the offered assortment is optimal with high probability, otherwise, the algorithm could potentially incur linear regret. Therefore, the knowledge of this \u201cseparability parameter\u201d is crucial. For our comparison, we consider the exploration period suggested by Saur\u00e9 and Zeevi (2013) and compare it with the performance of Algorithm 1 for different values of separation ( .", "startOffset": 104, "endOffset": 1299}, {"referenceID": 16, "context": "In this section, we present a computational study comparing the performance of our algorithm to that of Saur\u00e9 and Zeevi (2013). (To the best of our knowledge, Saur\u00e9 and Zeevi (2013) is currently the best existing approach for our problem setting.) To be implemented, their approach requires certain a priori information of a \u201cseparability parameter\u201d; roughly speaking, measuring the degree to which the optimal and next-best assortments are distinct from a revenue standpoint. More specifically, their algorithm follows an explore-then-exploit approach, where every product is first required to be offered for a minimum duration of time that is determined by an estimate of said \u201cseparability parameter.\u201d After this mandatory exploration phase, the parameters of the choice model are estimated based on the past observations and the optimal assortment corresponding to the estimated parameters is offered for the subsequent consumers. If the optimal assortment and the next best assortment are \u201cwell separated,\u201d then the offered assortment is optimal with high probability, otherwise, the algorithm could potentially incur linear regret. Therefore, the knowledge of this \u201cseparability parameter\u201d is crucial. For our comparison, we consider the exploration period suggested by Saur\u00e9 and Zeevi (2013) and compare it with the performance of Algorithm 1 for different values of separation ( .) We will show that for any given exploration period, there is an instance where the approach in Saur\u00e9 and Zeevi (2013) \u201cbreaks down\u201d or in other words incurs", "startOffset": 104, "endOffset": 1508}, {"referenceID": 16, "context": "Since the implementation of the policy in Saur\u00e9 and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Saur\u00e9 and Zeevi (2013) and the selling horizon as T = 10.", "startOffset": 42, "endOffset": 65}, {"referenceID": 16, "context": "Since the implementation of the policy in Saur\u00e9 and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Saur\u00e9 and Zeevi (2013) and the selling horizon as T = 10.", "startOffset": 42, "endOffset": 236}, {"referenceID": 16, "context": "Since the implementation of the policy in Saur\u00e9 and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Saur\u00e9 and Zeevi (2013) and the selling horizon as T = 10. Figure 3 compares the regret of Algorithm 1 with that of Saur\u00e9 and Zeevi (2013). The results are based on running 100 independent simulations with standard error of 2%.", "startOffset": 42, "endOffset": 351}, {"referenceID": 16, "context": "Since the implementation of the policy in Saur\u00e9 and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Saur\u00e9 and Zeevi (2013) and the selling horizon as T = 10. Figure 3 compares the regret of Algorithm 1 with that of Saur\u00e9 and Zeevi (2013). The results are based on running 100 independent simulations with standard error of 2%. We observe that the regret of the Saur\u00e9 and Zeevi (2013) is better than the regret of Algorithm 1 when = 0.", "startOffset": 42, "endOffset": 497}, {"referenceID": 16, "context": "Since the implementation of the policy in Saur\u00e9 and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Saur\u00e9 and Zeevi (2013) and the selling horizon as T = 10. Figure 3 compares the regret of Algorithm 1 with that of Saur\u00e9 and Zeevi (2013). The results are based on running 100 independent simulations with standard error of 2%. We observe that the regret of the Saur\u00e9 and Zeevi (2013) is better than the regret of Algorithm 1 when = 0.25 but is worse for other values of . This can be attributed to the fact that for the assumed exploration period, Their algorithm fails to identify the optimal assortment within the exploration phase with sufficient probability and hence incurs a linear regret for = 0.05,0.1 and 0.15. Specifically, among the 100 simulations we tested, the algorithm of Saur\u00e9 and Zeevi (2013) identified the optimal assortment for only 7%,40%,61% and 97% cases, when = 0.", "startOffset": 42, "endOffset": 924}, {"referenceID": 14, "context": "We consider the \u201cUCI Car Evaluation Database\u201d (see Lichman (2013)) which contains attributes based information of N = 1728 cars and consumer ratings for each car.", "startOffset": 51, "endOffset": 66}, {"referenceID": 16, "context": "Figure 3 Comparison with the algorithm of Saur\u00e9 and Zeevi (2013). The graphs (a), (b), (c) and (d) compares the performance of Algorithm 1 to that of Saur\u00e9 and Zeevi (2013) on problem instance (8.", "startOffset": 42, "endOffset": 65}, {"referenceID": 16, "context": "Figure 3 Comparison with the algorithm of Saur\u00e9 and Zeevi (2013). The graphs (a), (b), (c) and (d) compares the performance of Algorithm 1 to that of Saur\u00e9 and Zeevi (2013) on problem instance (8.", "startOffset": 42, "endOffset": 173}, {"referenceID": 16, "context": "It should be noted that Algorithm 1 did not require any a priori knowledge on the parameters unlike the other existing approaches such as Saur\u00e9 and Zeevi (2013) and therefore can be applied to a wide range of other settings.", "startOffset": 138, "endOffset": 161}], "year": 2017, "abstractText": "We consider a dynamic assortment selection problem, where in every round the retailer offers a subset (assortment) of N substitutable products to a consumer, who selects one of these products according to a multinomial logit (MNL) choice model. The retailer observes this choice and the objective is to dynamically learn the model parameters, while optimizing cumulative revenues over a selling horizon of length T . We refer to this exploration-exploitation formulation as the MNL-Bandit problem. Existing methods for this problem follow an explore-then-exploit approach, which estimate parameters to a desired accuracy and then, treating these estimates as if they are the correct parameter values, offers the optimal assortment based on these estimates. These approaches require certain a priori knowledge of \u201cseparability,\u201d determined by the true parameters of the underlying MNL model, and this in turn is critical in determining the length of the exploration period. (Separability refers to the distinguishability of the true optimal assortment from the other sub-optimal alternatives.) In this paper, we give an efficient algorithm that simultaneously explores and exploits, achieving performance independent of the underlying parameters. The algorithm can be implemented in a fully online manner, without knowledge of the horizon length T . Furthermore, the algorithm is adaptive in the sense that its performance is near-optimal in both the \u201cwell separated\u201d case, as well as the general parameter setting where this separation need not hold.", "creator": "TeX"}}}