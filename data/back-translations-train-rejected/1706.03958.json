{"id": "1706.03958", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Accelerated Dual Learning by Homotopic Initialization", "abstract": "Gradient descent and coordinate descent are well understood in terms of their asymptotic behavior, but less so in a transient regime often used for approximations in machine learning. We investigate how proper initialization can have a profound effect on finding near-optimal solutions quickly. We show that a certain property of a data set, namely the boundedness of the correlations between eigenfeatures and the response variable, can lead to faster initial progress than expected by commonplace analysis. Convex optimization problems can tacitly benefit from that, but this automatism does not apply to their dual formulation. We analyze this phenomenon and devise provably good initialization strategies for dual optimization as well as heuristics for the non-convex case, relevant for deep learning. We find our predictions and methods to be experimentally well-supported.", "histories": [["v1", "Tue, 13 Jun 2017 08:59:27 GMT  (2531kb,D)", "http://arxiv.org/abs/1706.03958v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hadi daneshmand", "hamed hassani", "thomas hofmann"], "accepted": false, "id": "1706.03958"}, "pdf": {"name": "1706.03958.pdf", "metadata": {"source": "CRF", "title": "Accelerated Dual Learning by Homotopic Initialization", "authors": ["Hadi Daneshmand", "Hamed Hassani", "Thomas Hofmann"], "emails": ["hadi.daneshmand@inf.ethz.ch", "hamed@inf.ethz.ch", "thomas.hofmann@inf.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "The standard approach to supervised learning is to view it as a problem with a proper loss function and a regulator, and then to minimize regulatory education risk through a chosen parametric model family. However, this view obstructs the fact that the ultimate goal is to minimize invisible risk, and that regulatory risk merely serves as a proxy for the former. Machine learning is optimized by designing an approximate solution."}, {"heading": "2 Ridge Regression", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Primal Formulation", "text": "For concreteness, we perform a detailed analysis of the burr regression [12]. In view of a formation set of n observations (xi, yi) with inputs xi-Rd and reponses yi-R, we use X-Rn-d to denote the data matrix and y-Rn to denote the response vector. Let us assume that y is the mean zero and the variance of the unit and that the data are centred. The burr regression target with regression \u00b5 > 0 can be expressed with Q (\u03b2) = 1-2 \u03b2-H\u03b2, H: = 1 n X-X + \u00b5I-Rd, b: = 1 n X-y-Rd (1) The optimal solution is explicitly given by the normal equations: \u03b2Q! = 0-0 \u03b2-E = H-1b = (X + n\u00b5I) \u2212 1 X-y."}, {"heading": "2.2 Gradient Descent Analysis", "text": "(4) For the diagonalized square target, the iterate sequence is explicitly stated as follows: Lemma 1. GD initialized with \u03b20, the iterate sequence generated by GD guarantees convergence with \u03b2 \u043a + (I \u2212 GP) t (\u03b20 \u2212 GP) (5).The rate of parameter convergence of \u03b2d is lower limited by 1 \u2212 GP > 1 \u2212 2 \u2212 GP / GP. Our interest lies in the convergence of the objective value. We can easily (and accurately) compare the distance in the parameter space with the sensitivity of \u03b2d and the rate of sensitivity with the sensitivity of 1 \u2212 GP > 1 \u2212 2 / GP."}, {"heading": "2.3 Analysis of Initialization", "text": "We examine a single coordinate in the diagonalized problem. Q (\u03b2tj) \u2212 Q \u0445 = (1 \u2212 \u03b3\u03bbj) 2t \u00b7 \u03bbj 2 (\u03b20j \u2212 \u0445 j) 2 (7) There are two factors here, one that decreases exponentially with t and a constant that depends on the initialization. In an asymptotic environment, only the first property would play a role that would lead to the known slowdown in the convergence of GD for disease-related problems in directions in which GD becomes very small. But what if we can tolerate some suboptimism? Then we could try to find an initialization that is less than the second term. There could be two cases: (i) we develop an intelligent strategy to find a good \u03b20 that is sufficiently close to \u03b2, (ii) we set \u03b20 = 0, but can reasonably assume that \u03b2 \u00b2 is small."}, {"heading": "3 Dual Ridge Regression", "text": "It turns out that the \u03b20 = 0 initialization, which we analyzed for the primary in the previous section, is not advantageous for the dual. Nevertheless, we can use the insight of data freedom to derive an effective initialization for the dual problem."}, {"heading": "3.1 Dual Objective", "text": "It is easy to derive dual targets for convex optimization problems by performing fennel duality analyses, e.g. see [8]. For ridge regression, we get the following dual target with dual parameters. (1) In the dual space, where typically n \u2212 d has n \u2212 d dimensional subspace size (X >), so that for the corresponding dual vectors v, v > Gv = 1n. This means that we have at least (n \u2212 d) orthogonal guidelines with a variance of 1 / n computing power for a total variance of 1 / n. (n \u2212 d) / n While in the primary case there are most d terms bound and typically \u00b5 1 / n, in the dual system we can get a sub-optimal contribution, even assuming a complete variance of Q."}, {"heading": "3.2 Accelerated RCDM by Homotopic Initialization", "text": "In the large scale setting (when the sample size n is large), the gradient step is mathematically expensive. (RCDM) [19, 20] is mathematically more attractive than GD and offers competitive convergence. An RCD step is achieved by a coordinate approach to the gradient (). More precisely, it randomly selects a random coordinate where the coordinate size of RCDM 1,. n) and updates it using the corresponding coordinate of the gradient (denoted by Q). (as\u03b1 + r = \u03b1r \u2212 r). (\u03b1), (17), where the coordinate size of RCDM is high. We ask whether homotopic initialization also accelerates convergence - to a suboptimal solution. The convergence dependence on the initialization is more subtle in a stochastic setting where each optimization step is altered by the noise of the gradient approximation."}, {"heading": "4 Generalized Linear Model", "text": "Accelerated convergence for an approximate solution also applies to generalized linear models. Faced with a convex differentiable smooth function, a generalized linear model (GLM) will aim to minimize the non-square target [9] R (z) = E (x > i z) \u2212 yi (x > i z)], (22) where the expectation is higher than the population distribution of the data. Let (k) (\u00b7) denote the k-th derivative profile of area data (\u00b7). The above formulation receives a logistic regression where (a) = log (1 + exp (a) and a ridge regression where (a) = a2."}, {"heading": "5 Initialization for Deep Neural Networks", "text": "So far, we have seen that if a dataset has the limiting property, then initialization with the all-zero vector speeds up optimization on the primal lens, up to a suboptimal solution. We have proven this for both burr regression and generalized linear models in a simplified environment. Our experiments show that nonlinear features of a trained neural network provide such a representation of the data (see Figure 4). This observation justifies the effectiveness of one of the most common initialization schemes of deep neural networks: use the resulting weights of a trained flat network to initialize a deep neural network [10]. We attribute the gain of such an initialization to the nature of characteristics obtained by a smaller network. Suppose that we use a trained network with N \u2212 1 layer to initialize the first N \u2212 1 layer to initialize the first N \u2212 1 layers of a larger network (we assume that the number of plugged-in units is the same)."}, {"heading": "6 Experimental Results", "text": "6.1 Data Sets and ProtocolsIn this section, we present our empirical results on real data sets selected from the LIBSVM library [5] (see Table 1 for more details).We have measured the limit constant \u03c4 for these data sets, which is used in our analysis.The regulator is \u00b5 = 10 \u2212 6 for all data sets except GISETTE, which has relatively fewer samples, and therefore we have used the regulator \u00b5 = 10 \u2212 3."}, {"heading": "6.2 Initialization for Dual Objective", "text": "We conducted an experiment to evaluate the advantage of homotopic initialization over dual programming. Although our experiment was based on initializing the Random Coordinate Descent Method (RCDM) method to optimize the dual goal of burr regression, we used the homotopic parameter \u03bd = 0.25 \u221a \u00b5 in all experiments, which is mathematically a favorable choice. We used coordinate-by-coordinate increments \u03b3r = G \u2212 1r, r, which are in fact coordinate-by-coordinate Lipchitz constants and their common choice for RCDM [19]. Our sampling scheme of coordinates is a random permutation in each epoch. Figure 3 shows the dual suboptimality, primary suboptimality, and test errors by optimization. To calculate the primary suboptimality, we mapped the dual parameter with the primary initial parameter."}, {"heading": "6.3 Initialization for neural networks", "text": "In this experiment, we trained a multilayer perceptron (MLP) with 10 hidden layers and 100 hidden units in each layer. We used two sets of data, MNIST and CIFAR, each containing 50,000 and 20,000 samples, respectively. Here, we compared the limitations of features obtained from the last layer of the network before and after training. Our observations show that a trained neural network provides quality (see Figures 4.a and 4.b). Based on this observation, we used a layer-by-layer initialization strategy. We trained an MLP with 5 layers and the same number of hidden units. Then, we initialized the first 5 layers of the main network (with 10 layers) through the trained weights of the smaller network and set the rest of the weights to zero. We compared the convergence of GD with size 0.1 using these two different initialization schemes."}, {"heading": "Acknowledgments", "text": "We would like to thank Aurelien Lucchi, Octavian Ganea and D\u00fcnner Celestine for the helpful conversations."}, {"heading": "7 Appendix", "text": "Here we present the proof of the lemmas and theorems of the manuscript with our experiments on the SVM problem. We start with the analysis of the method of random coordinate descend, then we provide the proof for the claim to a generalized linear model. Finally, we present the experimental result."}, {"heading": "7.1 Analysis of RCDM", "text": "We have proven that homotopic initialization accelerates the convergence of gradient lineage (up to a suboptimal solution).In this section, we expand the result to the random coordinate lineage method. For gradient lineage, we can divide the convergence of RCDM into two terms: a term that depends on the initial sub-optimality of dual lineage, the second term is determined by the initial euclidean distance to the dual minimizer. Later, we will use this result to prove that homotopic initialization provides an acceleration to a suboptimal level."}, {"heading": "7.2 Generalized Linear Model", "text": "The course of GLM (2) + E (3) (3) (3) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2)) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) ("}], "references": [{"title": "Numerical continuation methods: an introduction, volume 13", "author": ["Eugene L Allgower", "Kurt Georg"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Venkat Chandrasekaran", "Michael I Jordan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Starting small-learning with adaptive sample sizes", "author": ["Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Primal-dual rates and certificates", "author": ["Celestine Dunner", "Simone Forte", "Martin Takac", "Martin Jaggi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Scaled least squares estimator for glms in large-scale problems", "author": ["Murat A Erdogdu", "Lee H Dicker", "Mohsen Bayati"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jurgen Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["Arthur E Hoerl", "Robert W Kennard"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1970}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Takac", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Data-dependent initializations of convolutional neural networks", "author": ["Philipp Kr\u00e4henb\u00fchl", "Carl Doersch", "Jeff Donahue", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1511.06856,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Gradient descent only converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Adaptive newton method for empirical risk minimization to statistical accuracy", "author": ["Aryan Mokhtari", "Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann", "Alejandro Ribeiro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "author": ["Daniel Soudry", "Yair Carmon"], "venue": "arXiv preprint arXiv:1605.08361,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "On early stopping in gradient descent learning", "author": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto"], "venue": "Constructive Approximation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "How transferable are features in deep neural networks? In Advances in neural information processing", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].", "startOffset": 122, "endOffset": 126}, {"referenceID": 3, "context": "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].", "startOffset": 169, "endOffset": 172}, {"referenceID": 5, "context": "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].", "startOffset": 233, "endOffset": 240}, {"referenceID": 20, "context": "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].", "startOffset": 233, "endOffset": 240}, {"referenceID": 22, "context": "There are many examples that clearly demonstrate that initialization matters, for instance in deep learning via weight transfer [23], data-dependent initialization [15], or in avoidance of saddle points [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 14, "context": "There are many examples that clearly demonstrate that initialization matters, for instance in deep learning via weight transfer [23], data-dependent initialization [15], or in avoidance of saddle points [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 15, "context": "There are many examples that clearly demonstrate that initialization matters, for instance in deep learning via weight transfer [23], data-dependent initialization [15], or in avoidance of saddle points [16].", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "The same is true for unsupervised learning problems like matrix completion [14] orK-means [2].", "startOffset": 75, "endOffset": 79}, {"referenceID": 1, "context": "The same is true for unsupervised learning problems like matrix completion [14] orK-means [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "In this vein, the current paper provides a detailed analysis of convex learning, specifically of ridge regression and generalized linear models, that suggests to pre-train models with artificially increased regularization and to use this as an initialization in the spirit of homotopy or continuation methods [1, 18].", "startOffset": 309, "endOffset": 316}, {"referenceID": 17, "context": "In this vein, the current paper provides a detailed analysis of convex learning, specifically of ridge regression and generalized linear models, that suggests to pre-train models with artificially increased regularization and to use this as an initialization in the spirit of homotopy or continuation methods [1, 18].", "startOffset": 309, "endOffset": 316}, {"referenceID": 19, "context": "through the use of kernels), allows for fast algorithms like stochastic coordinate descent [20] that exhibit linear convergence and is ar X iv :1 70 6.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "also amenable to data sharding and communication-efficient distributed implementations [13].", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "1 Primal Formulation For concreteness, we perform an in-depth analysis of ridge regression [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 21, "context": "This seems to be related to a similar assumption made in early stopping [22] and in general when using norm-based regularization: we do not want to trust features that need to be amplified a lot (low variance, but high output covariance).", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "see [8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 18, "context": "Random coordinate descent method (RCDM) [19, 20] is computationally more attractive than GD and offers a competitive convergence.", "startOffset": 40, "endOffset": 48}, {"referenceID": 19, "context": "Random coordinate descent method (RCDM) [19, 20] is computationally more attractive than GD and offers a competitive convergence.", "startOffset": 40, "endOffset": 48}, {"referenceID": 18, "context": "(18), the classical analysis of RCDM [19] suggests the rate 1\u2212 (n+ 2B/\u03bc)\u22121, while our analysis improves the rate by a factor of \u03c1/n > 1 to 1\u2212 (\u03c1/n)(n+ 2B/\u03bc)\u22121.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": ", as in the catalyst method [17]), but also with respect to the sample", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "In our experiments, we used larger step sizes \u03b3r = G\u22121 r,r , which is equal to the coordinate-wise Lipschitz constants [19] of the dual ridge objective.", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Given a convex differentiable smooth function \u03c6, a generalized linear model (GLM) aims at minimizing the non-quadratic objective [9] R(z) = E [ \u03c6(xi z)\u2212 yi ( xi z )] , (22) where the expectation is over the population distribution of the data.", "startOffset": 129, "endOffset": 132}, {"referenceID": 8, "context": "Although the data is assumed to be generated from a normal distribution in the last lemma, we believe that this result can be extended to an arbitrary distribution using zero-bias transformations (see [9] for more details).", "startOffset": 201, "endOffset": 204}, {"referenceID": 9, "context": "This observation justifies the effectiveness of one of the most common initialization schemes of deep neural networks: Use resulting weights of a trained shallow network to initialize a deep neural network [10].", "startOffset": 206, "endOffset": 210}, {"referenceID": 10, "context": "In practice, we do not need to freeze weights of early layers because back propagation naturally causes small changes in early layers due to the vanishing gradient phenomenon [11].", "startOffset": 175, "endOffset": 179}, {"referenceID": 2, "context": "Our argument can also be extended to the layer-wise training for deep neural networks [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "2 In this section, we present our empirical results on real datasets, selected from LIBSVM library [5] (see Table 1 for more details).", "startOffset": 99, "endOffset": 102}, {"referenceID": 18, "context": "We used coordinate-wise step sizes \u03b3r = G\u22121 r,r , which are equal coordinate-wise Lipchitz constants and its a common choice for RCDM [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "To compute the primal suboptimality, we mapped the dual parameter to the primal one using the mapping \u03b2 = (n\u03bc)\u22121X\u03b1(t) (used in [20]).", "startOffset": 127, "endOffset": 131}], "year": 2017, "abstractText": "Gradient descent and coordinate descent are well understood in terms of their asymptotic behavior, but less so in a transient regime often used for approximations in machine learning. We investigate how proper initialization can have a profound effect on finding near-optimal solutions quickly. We show that a certain property of a data set, namely the boundedness of the correlations between eigenfeatures and the response variable, can lead to faster initial progress than expected by commonplace analysis. Convex optimization problems can tacitly benefit from that, but this automatism does not apply to their dual formulation. We analyze this phenomenon and devise provably good initialization strategies for dual optimization as well as heuristics for the non-convex case, relevant for deep learning. We find our predictions and methods to be experimentally well-supported.", "creator": "LaTeX with hyperref package"}}}