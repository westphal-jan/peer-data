{"id": "1706.03993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Getting deep recommenders fit: Bloom embeddings for sparse binary input/output networks", "abstract": "Recommendation algorithms that incorporate techniques from deep learning are becoming increasingly popular. Due to the structure of the data coming from recommendation domains (i.e., one-hot-encoded vectors of item preferences), these algorithms tend to have large input and output dimensionalities that dominate their overall size. This makes them difficult to train, due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "histories": [["v1", "Tue, 13 Jun 2017 10:50:25 GMT  (65kb,D)", "http://arxiv.org/abs/1706.03993v1", "Accepted for publication at ACM RecSys 2017; previous version submitted to ICLR 2016"]], "COMMENTS": "Accepted for publication at ACM RecSys 2017; previous version submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IR cs.NE", "authors": ["joan serr\\`a", "alexandros karatzoglou"], "accepted": false, "id": "1706.03993"}, "pdf": {"name": "1706.03993.pdf", "metadata": {"source": "META", "title": "Getting Deep Recommenders Fit: Bloom Embeddings for Sparse Binary Input/Output Networks", "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "emails": ["rstname.lastname@telefonica.com"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computer Methods \u2192 Neural Networks; Learning Latent Representations; \u2022 Information Systems \u2192 Document Representation; KEYWORDS Deep Recommendations, sparse Input / Output, Bloom lters, Neural Networks, Embedding."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a way in which people are able to put themselves at the centre, without having to put themselves at the centre."}, {"heading": "2 RELATEDWORK", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "3 BLOOM EMBEDDINGS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Bloom lters", "text": "Bloom lters [7] are a compact probable data structure, which is used to represent items and to check whether an item is part of a set [38]. Since the cases we are dealing with represent sets of single-hot items, bloom lters are an interesting option to embed these items in a compact room with good recovery (or check) guarantees. Essentially, bloom lters project each item of a set to k di erent positions of a binary item of size -. However, predictions are made using a series of k-independent hash functions H = {Hi} ki = 1, each of which, with a bandwidth of 1 tom, distributes the projected items to k di erent positions of a binary item of size [38]. Appropriate independent hash functions can be performed using improved double hash functions or triple hash functions H = 1, each of which is hashjik with a range of 1 tom to randomly distribute the projected items [38]. If hash is an incorrect hash function or triple hash function, the hash is a hash hash, or triple hash functions {Hashjik = 1}, the hash is a hash number of hash hash = 1."}, {"heading": "3.2 Embedding and recovery", "text": "In the following, we describe the use of such elements in x. For each of these components, we create a separate, high-dimensional instance, and the recovery or mapping of such instances from these instances. We describe the use of such elements in x. we pursue the idea that we want to embed both inputs and outputs and perform training in the embedding space. To do this, we only need a probability-based output function that is suitable for such activation. Let x be an input or output instance with dimensionality d, so that x = [x1],. xd], xi {0, 1}. Instances x are assumed to be economical, that is, we can more conveniently (and compactly) represent x as a set p = {pi} ci = 1, pi, where c is the number of such elements in x."}, {"heading": "3.3 Suitability", "text": "Note that BE is designed for both inputs and outputs and provides rank-based mapping between the original instances and the embedded vectors. BE provides a more compact representation of the original instance and does not require disk or disk space (at most marginal RAM memory, no GPU memory). In addition, BE can be performed on-the- y, without training and in constant time. Below, we show the remaining desirable qualities using a comprehensive setup: We show that the accuracy of the model remains stable, or even increases in the face of a reasonable embedding dimension, that no changes in the model architecture or configuration are required, that training times are faster thanks to the reduction in the number of parameters of the model, that evaluation times are not greater than those required by a rule of higher performance, and that the number of alternatives is greater."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 General considerations", "text": "We show that BE works under multiple conditions and that it can be applied to multiple tasks. In particular, we focus on recommendations and collaborative ltering, but also show the validity of the approach to a natural language processing task. We look at a number of data sets, network architectures, constructions and evaluation measures. In total, we look at 7 di-erent setups that we describe in Sec. 4.2. We also show that BE is competitive in terms of the available alternatives. To this end, we look at 4 di-erent state-of-the-art approaches that we outline in Sec. 4.3 The data sets are formed by inputs without instances that are either individual instances (or a hotly encoded user per-les) or sequences of instances (or per-le lists). Outputs, even from n-instances, correspond to individual instances."}, {"heading": "4.2 Tasks", "text": "We give a brief summary of the 7 tasks considered to be tasks (Tables 1 and 2). All data sets are publicly available and for all tasks we ensure that the network architecture and experimental setup is designed to achieve a state-of-the-art result. Thus, we use max. outputs and categorical cross-entropy losses in all experiments. (1) Movielens (ML): Film recommendations using the Movielens 20M data set1 [25]. Ratings were discarded with a threshold of 3.5 and films with less than 5 ratings and users with less than two films. Inputs and outputs were built by users using a uniform time span to ensure a minimum of one film in both input and output. To carry out recommendations, we build on the top of Wu et al. [49] and employ a 3-shift feed-forward neural model with 150 reactivated linear units in [21] the hidden layers."}, {"heading": "4.3 Alternative approaches", "text": "In order to compare performance, we need to adapt to reality."}, {"heading": "5 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Compression and performance", "text": "We start with reporting the performance of Si / S0 as a function of the embedding dimension. As mentioned above, to facilitate comparisons, we report in relative terms using score ratios Si / S0 and dimensionality ratios. If we look at the former as a function of the baseline, we see several things that are worth mentioning (Fig. 1). First, we observe that for most tasks the score ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio"}, {"heading": "5.2 Training and retrieval time", "text": "For this purpose, we record the time ratios Ti / T0 as a function of the dimensionality ratio m / d (Fig. 3). In terms of training times, we essentially observe a linear decrease with m / d (Fig. 3, le). ML is an exception to the trend, and CADE and AMZ experiment with almost no decrease at very low dimensionality ratios m / d < 0.2. In general, we achieve faster training times thanks to the reduction in the number of parameters of the model, dominated by input / output matrices (the output dimension also determines the time for calculating the loss function). We obtain a 2x acceleration for 2x input / output compression and, roughly speaking, a single bit for 5x input / output compression."}, {"heading": "5.3 Comparison with alternatives", "text": "Finally, we compare the performance of BE with that of one of the considered alternative methods. We do this by determining a dimensionality ratio Si / d and calculating the corresponding score ratio Si / S0 for a given task (Table 3). We see that BE is better than the alternative methods in 5 of 7 tasks (10 of 14 test points considered). PMI is better in one of the tasks (CADE) and CCA is also better in one of the tasks (AMZ). It is important to note that when BE wins, it always does so by a relatively large distance (see, for example, the ML or YC tasks). Otherwise, when an alternative approach wins, it is generally by a smaller distance (see, for example, the AMZ task). These results become more relevant when we realize that PMI and CCA are both SVD-based approaches and introduce a separate level of verified learning for the task, with paired item emitters and correlations being evaluated (see 4.3)."}, {"heading": "6 GOING ONE STEP FURTHERWITH CO-OCCURRENCE-BASED COLLISIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Co-occurrence-based embedding", "text": "We have also seen that some alternative approaches induce embedding by exploiting co-occurrence information (Secs. 2 and 4.3). We refer to this approach by co-occurrence information (Secs. 2 and 4.3). Here, we examine a variant of co-occurrence information that uses co-occurrence information to adjust the collisions that will inevitably occur when performing the embedding. We refer to this approach by co-occurrence sign embedding (CBE).Algorithm 1 pseudo-code for CBE.Input: input and / or output instance instance X (n \u00d7 d sparse binary matrix), embedding dimensional alityms, number of projections k, and pre-calculated hash matrix H (d \u00d7 k integer matrix). Output: co-occurrence-precombination-based matrix matrix matrix matrix matrix matrix matrix matrix matrix."}, {"heading": "6.2 CBE results", "text": "With the exception of the BC task, the performance of the CBE is always higher than that of the BE. However, one possible explanation for these moderate performance increases is the low occurrence in the data considered (Table 4, le). As can be seen, typically less than 3% of all possible pairs show an occurrence. Furthermore, the average frequency of such concurrent events is very low, with ratios in relation to the total number of occurrences in the order of 10 \u2212 5 or 10 \u2212 6. Although they are moderate on average, we observed that the increases provided by the CBE were more pronounced at low dimensional ratios than those for which the CBE ratios are m / d. Regarding the best approaches resulting from the previous comparison of Table 3, we believe that the BE ratios are sometimes more significant than the CBE ratios in the order of magnitude 5."}, {"heading": "7 CONCLUSION", "text": "We have shown that a compact representation can be achieved without compromising the performance of the original neural network model or, in some cases, even increasing it by a significant factor. Due to the compact representation, the loss function and the input and output layers deal with fewer parameters, resulting in faster training times. e approach is advantageous in terms of the alternatives considered and offers a number of other advantages such as on-the- operation or zero-space requirements, all without any changes to the core network architecture, task setting or loss function. In the future, the proposed approach could be improved in addition to further exploiting coexistence by considering further extensions of the bloom approach, such as the counting of bloom latters [9]. Theoretically, these extensions could provide a more compact representation by eliminating the binary character of embedding by examining the process in detail by comparing the process of the EQS function with the models."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the curators of the records used in this study for their public accessibility and Santi Pascual for his comments on an earlier version of the paper."}], "references": [{"title": "Label-embedding for image classi\u0080cation", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "IEEE Trans. on Pa\u0088ern Analysis and Machine Intelligence 38,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Uncovering shared structures in multiclass classi\u0080cation", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Error-correcting output codes for multi-label text categorization", "author": ["G. Armano", "C. Chira", "N. Hatami"], "venue": "In Proc. of the Italian Information Retrieval Conf. (IIR)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "\u008ce million song dataset", "author": ["T. Bertin-Mahieux", "D.P.W. Ellis", "B. Whitman", "P. Lamere"], "venue": "In Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Space/time trade-o\u0082s in hash coding with allowable errors", "author": ["B.H. Bloom"], "venue": "Commun. ACM 13,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1970}, {"title": "Bloom \u0080lters \u2013 a tutorial, analysis, and survey", "author": ["J. Blustein", "A. El-Maazawi"], "venue": "Technical Report", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "An improved construction for counting Bloom \u0080lters", "author": ["F. Bonomi", "M. Mitzenmacher", "R. Panigrahy", "S. Singh", "G. Varghese"], "venue": "In European Symposium on Algorithms (ESA), Y. Azar and T. Erlebach (Eds.). Lecture Notes in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Improving methods for single-label text categorization", "author": ["A. Cardoso-Cachopo"], "venue": "Ph.D. Dissertation. Instituto Superior Tecnico, Universidade Tecnica de Lisboa", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J. Wilson", "S. Tyree", "K. Weinberger", "Y. Chen"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Wide & deep learning for recommender systems", "author": ["H.-T. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir", "R. Anil", "Z. Haque", "L. Hong", "V. Jain", "X. Liu", "H. Shah"], "venue": "In Proc. of the Workshop on Deep Learning for Recommender Systems (DLRS)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "On the properties of neural machine translation: encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "In Proc. of the Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Information-theoretic label embeddings for large-scale image classi\u0080cation", "author": ["F. Chollet"], "venue": "ArXiv: 1607.05691", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Robust Bloom \u0080lters for large multilabel classi\u0080cation tasks", "author": ["M. Ciss\u00e9", "N. Usunier", "T. Arti\u00e8res", "P. Gallinari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "BinaryConnect: training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Die\u008aerich", "G. Bakiri"], "venue": "Journal of Arti\u0080cial Intelligence Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Bloom \u0080lters in probabilistic veri\u0080cation", "author": ["P.C. Dillinger", "P. Manolios"], "venue": "In Proc. of the Int. Conf. on Formal Methods in Computer-Aided Design (FMCAD)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Small statistical models by random feature mixing", "author": ["K. Ganchev", "M. Dredze"], "venue": "In ACL Workshop on Mobile Language Processing (MLP)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Deep sparse recti\u0080er neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of the Int. Conf. on Arti\u0080cial Intelligence and Statistics (AISTATS)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "E\u0081cient so\u0089max approximation for GPUs", "author": ["E. Grave", "A. Joulin", "M. Ciss\u00e9", "D. Grangier", "H. J\u00e9gou"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Generating sequences with recurrent neural networks. ArXiv: 1308.0850", "author": ["A. Graves"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "\u008ce MovieLens datasets: history and context", "author": ["F.M. Harper", "J.K. Konstan"], "venue": "ACM Trans. on Interactive Intelligent Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Long short-term memory networks", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika 28,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1936}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "J. Comput. System Sci. 78,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["D.J. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "In Proc. of the Int. Conf. on Learning Representations (ICLR)", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Vowpal wabbit online learning project", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": "Technical Report", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "In Proc. of the ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD)", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Ph.D. Dissertation. Brno University of Technology", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "E\u0081cient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Probability and computing: randomized algorithms and probabilistic analysis", "author": ["M. Mitzenmacher", "E. Upfal"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Proc. of the Int. Workshop on Arti\u0080cial Intelligence and Statistics (AISTATS)", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Pe\u008aerson", "G. Dror", "J. Langford", "A. Smola", "S.V.N. Vishwanathan"], "venue": "Journal of Machine Learning Research", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Hybrid recommender system based on autoencoders", "author": ["F. Strub", "R. Gaudel", "J. Mary"], "venue": "In Proc. of theWorkshop on Deep Learning for Recommender Systems (DLRS)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Lecture 6.5-RMSprop: divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine", "author": ["T. Tieleman", "G. Hinton"], "venue": "Learning 4,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "E\u0081cient exact gradient update for training deep networks with very large sparse targets", "author": ["P. Vincent", "A. Br\u00e9bisson", "X. Bouthilier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. A\u008aenberg", "J. Langford", "A. Smola"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML)", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Machine Learning 81,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Kernel dependency estimation", "author": ["J. Weston", "O. Chapelle", "A. Elissee", "B. Sch\u00f6lkopf", "V. Vapnik"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2002}, {"title": "Collaborative denoising auto-Encoders for top-N recommender systems", "author": ["Y. Wu", "C. DuBois", "A.X. Zheng", "M. Ester"], "venue": "In Proc. of the ACM Int. Conf. on Web Search and Data Mining (WSDM)", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Improving recommendation lists through topic diversi\u0080cation", "author": ["C.-N. Ziegler", "S.M. McNee", "J.A. Konstan", "G. Lausen"], "venue": "In Proc. of the Int. World Wide Web Conf. (WWW)", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": ", [12, 26, 42, 49]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 37, "context": ", [12, 26, 42, 49]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 44, "context": ", [12, 26, 42, 49]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 4, "context": ", [5, 37, 44]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 33, "context": ", [5, 37, 44]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 39, "context": ", [5, 37, 44]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 0, "context": "[1, 4, 48]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "[1, 4, 48]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 43, "context": "[1, 4, 48]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 6, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 8, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 17, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 34, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 29, "context": "A common approach to embed high-dimensional inputs is the hashing trick [33, 41, 46].", "startOffset": 72, "endOffset": 84}, {"referenceID": 36, "context": "A common approach to embed high-dimensional inputs is the hashing trick [33, 41, 46].", "startOffset": 72, "endOffset": 84}, {"referenceID": 41, "context": "A common approach to embed high-dimensional inputs is the hashing trick [33, 41, 46].", "startOffset": 72, "endOffset": 84}, {"referenceID": 19, "context": "A more elementary version of the hashing trick [20] can be used at the outputs by considering it as a special case of the Bloom-based methodology proposed here.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "A framework providing both encoding and decoding strategies is the error-correcting output codes (ECOC) framework [17].", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "Originally designed for single-class outputs, it can be also applied to class sets [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 43, "context": "Another example of a framework o\u0082ering recovery capabilities is kernel dependency estimation [48].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "[30] builds on top of ECOC to reduce multi-label regression to binary regression problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] use Bloom \u0080lters to reduce multi-label classi\u0080cation to binary classi\u0080cation problems and improve the robustness of individual binary classi\u0080ers\u2019 errors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Nonetheless, recently, Chollet [14] has successfully applied a K-nearest neighbors (KNN) algorithm to perform such a mapping and to derive a ranking of the items in the original", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "Canonical correlation analysis is an example that considers both inputs and outputs at the same time [28].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "regularized learning [2], label embedding trees [4], or the WSABIE algorithm [47].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "regularized learning [2], label embedding trees [4], or the WSABIE algorithm [47].", "startOffset": 48, "endOffset": 51}, {"referenceID": 42, "context": "regularized learning [2], label embedding trees [4], or the WSABIE algorithm [47].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "[1] provide a comprehensive list.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "A common approach is to reduce the size of already trained models by some quantization and/or pruning of the connections in dense layers [16, 24, 31].", "startOffset": 137, "endOffset": 149}, {"referenceID": 10, "context": "A less frequent approach is to reduce the model size before training [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "\u008ce hierarchical so\u0089max approach [39] or the more recent adaptive so\u0089max [22] are two examples of those.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "\u008ce hierarchical so\u0089max approach [39] or the more recent adaptive so\u0089max [22] are two examples of those.", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": "[45] focuses on both aspects of very large sparse outputs but, to the best of our knowledge, cannot be directly applied to common so\u0089max outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Bloom \u0080lters [7] are a compact probabilistic data structure that is used to represent sets of items, and to e\u0081ciently check whether an item is a member of a set [38].", "startOffset": 13, "endOffset": 16}, {"referenceID": 34, "context": "Bloom \u0080lters [7] are a compact probabilistic data structure that is used to represent sets of items, and to e\u0081ciently check whether an item is a member of a set [38].", "startOffset": 161, "endOffset": 165}, {"referenceID": 34, "context": "a set of k independent hash functions H = {Hi }i=1, each of which with a range from 1 tom, ideally distributing the projected items uniformly at random [38].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "Proper independent hash functions can be derived using enhanced double hashing or triple hashing [18].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "\u008cus, item checks return no false negatives, meaning that the structure gives an answer with 100% recall [38].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "\u008cis implies that false positives are possible, due to collisions between projections of di\u0082erent items [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "less than 1% false positive probability [9].", "startOffset": 40, "endOffset": 43}, {"referenceID": 30, "context": "For the sake of comparison, we also choose appropriate and well-known evaluation measures [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "(1) Movielens (ML): movie recommendation with the Movielens 20M data set1 [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 44, "context": "[49] and employ a 3-layer feed-forward neural network model with 150 recti-", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "\u0080ed linear units [21] in the hidden layers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 28, "context": "We optimize the weights of the network using cross-entropy and Adam [32],", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "(2) Million song data set (MSD): song recommendation with the Million song data set2 [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 31, "context": "(3) Amazon book reviews (AMZ): book recommendation with the Amazon book reviews data set3 [35].", "startOffset": 90, "endOffset": 94}, {"referenceID": 45, "context": "(4) Book crossing (BC): book recommendation with the book crossing data set4 [50].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "[26] and consider a GRU model [13].", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "We set the inner dimensionality to 100 and train the network with Adagrad [19], using a learning rate of 0.", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "(6) Penn treebank (PTB): next word prediction with the Penn treebank data set [36].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Inspired by Graves [23], we perform next word prediction with an LSTM network [27].", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "Inspired by Graves [23], we perform next word prediction with an LSTM network [27].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "99, and clip gradients to have a maximum norm of 1 [23].", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "(7) CADE web directory (CADE): text categorization with the CADE web directory of Brazilian web pages6 [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 38, "context": "We train the network with RMSprop [43], a learning rate of 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "We \u0080rst consider the popular hashing trick for classi\u0080er and recommender inputs [33, 46].", "startOffset": 80, "endOffset": 88}, {"referenceID": 41, "context": "We \u0080rst consider the popular hashing trick for classi\u0080er and recommender inputs [33, 46].", "startOffset": 80, "endOffset": 88}, {"referenceID": 19, "context": "Nonetheless, in the case of binary outputs, variants like the one used by Ganchev and Dredze [20] can be adapted to map to the original items using Eqs.", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "Originally designed for single-class targets [17], ECOC can be applied to class sets (inputs and outputs), with its corresponding encoding and decoding strategies [3].", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "Originally designed for single-class targets [17], ECOC can be applied to class sets (inputs and outputs), with its corresponding encoding and decoding strategies [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 16, "context": "ECOC matrix with the randomized hill-climbing method of [17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Recently, Chollet [14]", "startOffset": 18, "endOffset": 22}, {"referenceID": 25, "context": "CCA is a common way to learn a joint dense, real-valued embedding for both inputs and outputs at the same time [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "computed using SVD on a correlation matrix [29] and, similarly to PMI, we can use the KNN trick to rank items or labels at prediction time.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "\u008ce fact that an embedding performs be\u008aer than the original Baseline has been also observed in some other methods for speci\u0080c data sets [14, 33, 48].", "startOffset": 135, "endOffset": 147}, {"referenceID": 29, "context": "\u008ce fact that an embedding performs be\u008aer than the original Baseline has been also observed in some other methods for speci\u0080c data sets [14, 33, 48].", "startOffset": 135, "endOffset": 147}, {"referenceID": 43, "context": "\u008ce fact that an embedding performs be\u008aer than the original Baseline has been also observed in some other methods for speci\u0080c data sets [14, 33, 48].", "startOffset": 135, "endOffset": 147}, {"referenceID": 13, "context": "For instance, Chollet [14] has reported increases up to 7% using", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "7Note that the ML data is essentially collected through a survey-type method [25].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "In the future, besides continuing to exploit co-occurrences, one could enhance the proposed approach by considering further extensions of Bloom \u0080lters such as counting Bloom \u0080lters [9].", "startOffset": 181, "endOffset": 184}], "year": 2017, "abstractText": "Recommendation algorithms that incorporate techniques from deep learning are becoming increasingly popular. Due to the structure of the data coming from recommendation domains (i.e., one-hotencoded vectors of item preferences), these algorithms tend to have large input and output dimensionalities that dominate their overall size. \u008cis makes them di\u0081cult to train, due to the limited memory of graphical processing units, and di\u0081cult to deploy on mobile devices with limited hardware. To address these di\u0081culties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally e\u0081cient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as \u2018on-the-\u0083y\u2019 constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training con\u0080guration.", "creator": "LaTeX with hyperref package"}}}