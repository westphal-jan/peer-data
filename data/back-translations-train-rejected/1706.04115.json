{"id": "1706.04115", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "histories": [["v1", "Tue, 13 Jun 2017 15:17:42 GMT  (180kb,D)", "http://arxiv.org/abs/1706.04115v1", "CoNLL 2017"]], "COMMENTS": "CoNLL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["omer levy", "minjoon seo", "eunsol choi", "luke zettlemoyer"], "accepted": false, "id": "1706.04115"}, "pdf": {"name": "1706.04115.pdf", "metadata": {"source": "CRF", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "authors": ["Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer"], "emails": ["omerlevy@cs.washington.edu", "minjoon@cs.washington.edu", "eunsol@cs.washington.edu", "lsz@cs.washington.edu"], "sections": [{"heading": null, "text": "We show that relation extraction can be reduced to answering simple reading comprehension questions by linking one or more natural language questions to each relation slot. This reduction has several advantages: we can (1) learn relationship extraction models by extending newer neural reading comprehension techniques, (2) build very large training sets for these models by combining relationship-specific crowdsourcing questions with remote monitoring, and even (3) engage in zero-shot learning by extracting new relation types specified only at the test date for which we do not have labeled training examples. Experiments on a Wikipedia slot fill task show that the approach can generalize new questions for known relationship types with high accuracy, and that zero-shot generalization is possible to invisible relation types at lower levels of accuracy, setting the bar for future work on this task."}, {"heading": "1 Introduction", "text": "In fact, it is true that most people who are able to outdo themselves are unable to outdo themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 Related Work", "text": "We are interested in a particularly persistent zero-point learning scenario: given examples of N relation types described during training, we extract relationships of a new type RN + 1. The only information we have about RN + 1 is parameterized questions.This setting differs from the state of the art in terms of extraction. Bronstein et al. (2015) investigate a similar zero-point setting for event trigger identification, in which RN + 1 is specified by a series of trigger words in the test phase. They generalize the similarity between potential triggers and the seed methods given. Instead, we focus on filling in slots where questions are more appropriate than trigger words. (Open IE)"}, {"heading": "3 Approach", "text": "In fact, the fact is that most of them will be able to move to another world in which they are able to integrate, and in which they are able, in which they are able, in which they are in."}, {"heading": "4 Dataset", "text": "In fact, it is the case that most of them are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) It is the case that they are able to abide by the rules. (...) It is not the case that they are able to break the rules. (...) It is not the case that they are able to break the rules. (...) It is the case that they are able to break the rules. (...) It is the case that they are not able to break the rules. (...) It is not the case that they are able to break the rules. (...) It is not the case. (...) It is the case that they are able to break the rules. (...) (...) () (() (()) ((()) (() () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () () () (() (() () () () () (() () () () (() (() () ((() (() () () () (() (() (() () () (() ((() (() ((() () (() () ((() () () (() () () (((() () (((() () (((() () () () (((() (() (() () (((()) (((("}, {"heading": "5 Model", "text": "The question we have to ask ourselves is whether we really take seriously the answer to the question we have asked ourselves. (...) The answer to the question we have asked ourselves is that the answer to the question we have asked ourselves is very likely. (...) The answer to the question we have asked ourselves is that the answer to the question we have asked ourselves is very likely. (...) The answer to the question we have asked ourselves is that the answer to the question we have asked ourselves is very likely. (...) The answer to the question we have asked ourselves is very likely. (...) The answer to the question we have asked ourselves is that the answer to the question we have asked ourselves is very likely. (...) The answer to the question we have asked ourselves is very likely. (...)"}, {"heading": "6 Experiments", "text": "To understand how well our method can generalize to invisible data, we design experiments for invisible entities (Section 6.1), invisible question templates (Section 6.2), and invisible relationships (Section 6.3).Evaluation Metrics Each instance is a non-zero response by comparing the tokens in the labeled answer with those of the predicted answer.6 Precision is the true positive number divided by the number of times the system has returned. Recall is the true positive number divided by the number of instances that have an answer. Hyperparameters In our experiments, we initialized word embedding with GloVe (Pennington et al., 2014) and do not fine-tune with them. Typical training was a sequence of 1 million examples for which 3 epochs were sufficient for convergence."}, {"heading": "6.1 Unseen Entities", "text": "We show that our approach to reading comprehension works well in a typical setting of relation extraction by testing it on invisible entities and texts. Setup We divided our data set along entities in the question, randomly dividing each entity into one of three groups: train, developer, or test. For example, Alan Turing examples only appear in training, while Steve Jobs examples are used exclusively for testing. We randomly assign 1,000,000 train examples, 1,000 for developers, and 10,000 for testing. This partition also ensures that the sentences at test time are different from those on the train, as the sentences from each entity's Wikipedia article are compiled. Result Table 1 shows that our model generalizes well to new entities and texts, with slight variations in performance between KB relation, NL relation, multiple templates, and question sets that perform significantly worse than these variants."}, {"heading": "6.2 Unseen Question Templates", "text": "We tested the ability of our method to generalize to new descriptions of the same relationship by submitting one question template for each relationship during the training, and another for the development template. For example, \"What did x do for a living?\" can only appear in the training set, while \"What is x's job?\" is exclusive to the test set. Each split was layered by scanning N examples per question set (N = 1000, 10, 50 for the turn, dev or test).This process created 10 training sets of 966,000 examples with matching development and test sets of 940 and 4,700 examples respectively. We trained and tested several templates on each of the folds, yielding performance on invisible templates. We then replicated the existing test sets and replaced the invisible question templates with templates from the training set that provide a performance that converts our previously observed templates to 3.5 points. \""}, {"heading": "6.3 Unseen Relations", "text": "We investigated a pure zero-shot setting in which test time relationships are not observed during the training.Setup We created 10 folds of traction / development / test samples divided along relationships: 84 relationships for the train, 12 development and 24 test scenarios. For example, if a school is training for tests, no examples will appear on the train. Using stratified samples of relationships, we created 10 training sets, each with 840,000 examples, with matching development and test scenarios of 600 and 12,000 examples per fold. Table 3 shows the performance of each system; Figure 4 extends these results for variants of our model by applying a global threshold to the trust values of the answers to generate precision / recall curves (see Section 5). As expected, the presentation of knowledge base relationships as indicators (KB Relation and Miwa & Bansal) in a zero-shot setting is based on the types of answers we use to generate precision / recall curves (see Section 5)."}, {"heading": "7 Analysis", "text": "To understand how our method extracts invisible relationships, we analyzed 100 random examples, of which 60 had answers in the sentence and 40 did not (negative examples).For negative examples, we checked whether a diversion - a wrong answer to the right answer type - appears in the sentence. For example, the question \"Who is John McCain married?\" has no answer in \"John McCain chose Sarah Palin as his running partner,\" but \"Sarah Palin\" is the right answer type. We noted that 14 negative examples (35%) diversions with the results from the invisible relationships in Section 6.3, we found that our method 2 / 14 of diversion examples is wrong compared to the simpler examples. It appears that most of the negative examples are simple, a significant part of them that we do not correlate. For positive examples, we have observed that some instances can be solved by diversion maneuvers."}, {"heading": "8 Conclusion", "text": "We have shown that the extraction of relationships can be reduced to a reading comprehension problem, which allows us to generalize to invisible relationships that are spontaneously defined in natural language. However, the problem of extracting zero-point relationships is far from solved and poses an interesting challenge for both information extraction and machine reading communities. As we continue to explore machine reading, we may find that other tasks may benefit from a similar approach. To support future work in this way, we are making our code and data publicly available.7"}, {"heading": "Acknowledgements", "text": "The research was supported in part by DARPA through the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google, Tencent and Nvidia, and an Allen Distinguished Investigator Award. We also thank Mandar Joshi, Victoria Lin and the UW NLP Group for helpful conversations and comments on the work."}], "references": [{"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Seed-based event trigger labeling: How far can event descriptions get us", "author": ["Ofer Bronstein", "Ido Dagan", "Qi Li", "Heng Ji", "Anette Frank"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Bronstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bronstein et al\\.", "year": 2015}, {"title": "Lifted rule injection for relation embeddings", "author": ["Thomas Demeester", "Tim Rockt\u00e4schel", "Sebastian Riedel."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Demeester et al\\.,? 2016", "shortCiteRegEx": "Demeester et al\\.", "year": 2016}, {"title": "Question-answer driven semantic role labeling: Using natural language to annotate natural language", "author": ["Luheng He", "Mike Lewis", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Wikireading: A novel large-scale language understanding task over wikipedia", "author": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot."], "venue": "Proceedings of the Conference of the", "citeRegEx": "Hewlett et al\\.,? 2016", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computa-", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das."], "venue": "arXiv preprint arXiv:1611.01436 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Effective crowd annotation for relation extraction", "author": ["Angli Liu", "Stephen Soderland", "Jonathan Bragg", "Christopher H. Lin", "Xiao Ling", "Daniel S. Weld."], "venue": "Proceedings of the 2016 Conference", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "End-to-end relation extraction using lstms on sequences and tree structures", "author": ["Makoto Miwa", "Mohit Bansal."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Compu-", "citeRegEx": "Miwa and Bansal.,? 2016", "shortCiteRegEx": "Miwa and Bansal.", "year": 2016}, {"title": "Learning to answer questions from wikipedia infoboxes", "author": ["Alvaro Morales", "Varot Premtoon", "Cordelia Avery", "Sue Felshin", "Boris Katz."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Morales et al\\.,? 2016", "shortCiteRegEx": "Morales et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the As-", "citeRegEx": "Riedel et al\\.,? 2013", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Tim Rockt\u00e4schel", "Sameer Singh", "Sebastian Riedel."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Associa-", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Representing text for joint embedding", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Generalizing to unseen entities and entity pairs with row-less universal schema", "author": ["Patrick Verga", "Arvind Neelakantan", "Andrew McCallum."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-", "citeRegEx": "Verga et al\\.,? 2017", "shortCiteRegEx": "Verga et al\\.", "year": 2017}, {"title": "Wikidata: A new platform for collaborative data collection", "author": ["Denny Vrande\u010di\u0107."], "venue": "Proceedings of the 21st international conference companion on World Wide Web. ACM, pages 1063\u20131064.", "citeRegEx": "Vrande\u010di\u0107.,? 2012", "shortCiteRegEx": "Vrande\u010di\u0107.", "year": 2012}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "When the type of facts (relations) are predefined, one can use crowdsourcing (Liu et al., 2016) or distant supervision (Hoffmann et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 6, "context": ", 2016) or distant supervision (Hoffmann et al., 2011) to collect examples and train an extraction model for", "startOffset": 31, "endOffset": 54}, {"referenceID": 18, "context": "for a relatively large number of relations (120) from Wikidata (Vrande\u010di\u0107, 2012), which are easily gathered in practice via the WikiReading dataset (Hewlett et al.", "startOffset": 63, "endOffset": 80}, {"referenceID": 5, "context": "for a relatively large number of relations (120) from Wikidata (Vrande\u010di\u0107, 2012), which are easily gathered in practice via the WikiReading dataset (Hewlett et al., 2016).", "startOffset": 148, "endOffset": 170}, {"referenceID": 15, "context": "We show that a recent state-of-the-art neural approach for reading comprehension (Seo et al., 2016) can be directly extended to model answerability and trained on our new dataset.", "startOffset": 81, "endOffset": 99}, {"referenceID": 1, "context": "Bronstein et al. (2015) explore a similar zero-shot setting for event-trigger identification, in which RN+1 is specified by a set of trigger words at test time.", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "Universal schema (Riedel et al., 2013) rep-", "startOffset": 17, "endOffset": 38}, {"referenceID": 17, "context": "Verga et al. (2017) predict facts for entity pairs that were not observed in the original matrix; this is equivalent to extracting seen relation types with unseen entities (see Sec-", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Rockt\u00e4schel et al. (2015) and Demeester et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "(2015) and Demeester et al. (2016) use inference rules to predict hidden knowledge-base relations from observed naturallanguage relations.", "startOffset": 11, "endOffset": 35}, {"referenceID": 16, "context": "Toutanova et al. (2015) proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "Slot-Filling Data We use the WikiReading dataset (Hewlett et al., 2016) to collect labeled", "startOffset": 49, "endOffset": 71}, {"referenceID": 18, "context": "WikiReading was collected by aligning each Wikidata (Vrande\u010di\u0107, 2012) relation R(e, a) with the corresponding Wikipedia article D for the entity e, under the reasonable assumption that the relation can be derived from the", "startOffset": 52, "endOffset": 69}, {"referenceID": 4, "context": "Negative Examples To support relation extraction, our dataset deviates from recent reading comprehension formulations (Hermann et al., 2015; Rajpurkar et al., 2016), and introduces negative", "startOffset": 118, "endOffset": 164}, {"referenceID": 12, "context": "Negative Examples To support relation extraction, our dataset deviates from recent reading comprehension formulations (Hermann et al., 2015; Rajpurkar et al., 2016), and introduces negative", "startOffset": 118, "endOffset": 164}, {"referenceID": 10, "context": "Following the methodology of InfoboxQA (Morales et al., 2016), we generate negative examples by matching (for the same entity e) a question q that pertains to one relation", "startOffset": 39, "endOffset": 61}, {"referenceID": 0, "context": "The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.", "startOffset": 22, "endOffset": 43}, {"referenceID": 3, "context": "For the task of identifying predicate-argument structures, QASRL (He et al., 2015) was proposed as an open", "startOffset": 65, "endOffset": 82}, {"referenceID": 12, "context": "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).", "startOffset": 111, "endOffset": 192}, {"referenceID": 20, "context": "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).", "startOffset": 111, "endOffset": 192}, {"referenceID": 7, "context": "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).", "startOffset": 111, "endOffset": 192}, {"referenceID": 19, "context": "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).", "startOffset": 111, "endOffset": 192}, {"referenceID": 15, "context": "We start from the BiDAF model (Seo et al., 2016), whose input is two sequences of words: a sentence s and a question q.", "startOffset": 30, "endOffset": 48}, {"referenceID": 15, "context": "Seo et al. (2016) obtain the span with the highest probability during post-processing.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "Hyperparameters In our experiments, we initialized word embeddings with GloVe (Pennington et al., 2014), and did not fine-tune them.", "startOffset": 78, "endOffset": 103}, {"referenceID": 5, "context": "shown to have good results on the extractive portion of WikiReading (Hewlett et al., 2016).", "startOffset": 68, "endOffset": 90}, {"referenceID": 9, "context": "Lastly, we retrain an off-the-shelf relation extraction system (Miwa and Bansal, 2016), which has shown promising results on a number of benchmarks.", "startOffset": 63, "endOffset": 86}], "year": 2017, "abstractText": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relationextraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "creator": "LaTeX with hyperref package"}}}