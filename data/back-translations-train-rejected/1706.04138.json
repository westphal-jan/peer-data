{"id": "1706.04138", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing", "abstract": "In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs $mt$ and $src$ in a single neural architecture, modeling $\\{mt, src\\} \\rightarrow pe$ directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT 2016 shared task on automatic post-editing and can demonstrate that double-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Double-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.", "histories": [["v1", "Tue, 13 Jun 2017 15:55:02 GMT  (204kb,D)", "http://arxiv.org/abs/1706.04138v1", null], ["v2", "Sat, 30 Sep 2017 13:03:33 GMT  (233kb,D)", "http://arxiv.org/abs/1706.04138v2", "Accepted for presentation at IJCNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "roman grundkiewicz"], "accepted": false, "id": "1706.04138"}, "pdf": {"name": "1706.04138.pdf", "metadata": {"source": "META", "title": "An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing", "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz"], "emails": ["junczys@amu.edu.pl", "rgrundki@exseed.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Faced with the raw output of a (possibly unknown) machine translation system from src language to mt language, we focus directly on the results of a single system (APE) is the process of automatic correction of raw MT output (mt), so that a closer resemblance to human postedited MT output (pe) is achieved. While APE systems that generate only one model produce good results, the field has always sought methods that also integrate src into various formulas. Neural encoder decoder models and multi-source models can now achieve this in a more natural way than for previously popular phrase dreamer-based machine translation (PB-SMT) systems. Nevertheless, the results of formulti source models in APE scenarios are incomplete or unsatisfactory in terms of performance. In this work, we are examining a number of singlesource and dual source systems that are based-neural."}, {"heading": "2 Previous work", "text": "Prior to the application of neural sequence models to APE, most APE systems were based on phrase-based SMTs using a monolingual approach initially proposed by Simard et al. (2007). B\u00e9chara et al. (2011) proposed a \"source context-conscious\" variant of this approach: automatically generated word alignments are used to create a new source language consisting of associated MT output and source code pairs. Incorporating source language information in this form will prove useful to improve automatic post-processing of results (B\u00e9chara et al., 2012; Chatterjee et al., 2015) The quality of word alignments plays an important role in these methods, as demonstrated by Pal et al al al. (2015) During the 2016 WMT APE, two systems relied on neural models, the CUNI system (Livibock\u00fd et al al al., 2016) and the shared task were the winners."}, {"heading": "3 Attentional Encoder-Decoder", "text": "The model differs from the standard model provided by Bahdanau et al. (2014) in several aspects, the most important of which is the condition provided in this section. The summary is based on the description in Sennrich et al. (2017).Given the raw MT output sequence (x1,., xTx) of length Tx and its manually processed equivalents (y1,., yTy) of length Ty, we construct the encoder decoder model using the following formulations., xTx) of length Tx and its manually processed equivalents (y1,., yTy) of length Ty."}, {"heading": "4 Encoder-Decoder Models with APE-specific Attention Models", "text": "The following models use most parts of the architecture described above wherever possible, most differences occur in the decoder RNN cell and the attention mechanism."}, {"heading": "4.1 Hard Monotonic Attention", "text": "In fact, it is the case that they will be able to be in a position to be in the position they are in, and that they will be able to be in a position to be in a position to be in the position in which they are able to be in, in the situation in which they are able to be in, in the situation in which they are able to be in, in the situation in which they are able to be in, in the situation in which they are able to hide. \""}, {"heading": "4.2 Hard and Soft Attention", "text": "While the hard attention model can be used to force fidelity to the original input, we also want the model to be able to view information anywhere in the source sequence, which is a property of the soft attention model. By reintroducing the conditional GRU cell with soft attention into the ENCDEC GRU HARD model while entering the heavily visited encoder state haj, we can try to exploit both attention mechanisms. By combining Equivalent 2 and Equivalent 3, we obtain: sj = cGRUatt (sj \u2212 1, [E [yj \u2212 1]; haj], C). (4) The rest of the model remains unchanged; the translation process is the same as before and we use the same target step / token sequence for training. This model is called ENCDEC-CGRU-HARD."}, {"heading": "4.3 Soft Double-Attention", "text": "Neural multi-source models (Zoph and Knight, 2016) seem to be suitable for the APE task, of course, since raw MT results and original source language inputs are available. Although applications for the APE problem have been reported (Libovick\u00fd and Helcl, 2017), state-of-the-art MT results seem to fail. In this section, we give details of our implementation of the doublesource model. We rename the existing C encoder to Cmt to signal that the first encoder consumes the raw MT output, and we perform a structurally identical second encoder Csrc = {hsrc1,.. hsrcTsrc} via the source language. To3Similar to GNU wdiff.compute the start state s0 for the multi-encoder model, we calculate the averaged second encoder Csrc context Cscrc {schcrc = WincrsT1, GRsrrrrt = hatten.rt. Context = GRrrrt: GRcrrc = WincrT1, GRrrrrrrrrt = ratten.T."}, {"heading": "4.4 Hard Attention with Soft Double-Attention", "text": "Similar to the approach described in Section 4.2, we can extend the doubly attentive cGRU to use the high-frequency encoder context as additional input: sj = cGRU2-att (sj \u2212 1, [E [yj \u2212 1]; hmtaj], Cmt, Csrc).4Calixto et al. (2017) combine their two attention models by modifying their GRU cell to contain another set of parameters, which is multiplied by the additional context vector and summed into the GRU components. Formally, both approaches yield identical results, since the original parameters for the concatenation must be adapted to the now longer input vector dimensions. The GRU cell itself does not need to be modified. In this formulation, only the first encoder context Cmt is used by the hard monotonic attention mechanism. The sequence data for the attention intraction consists of the previous step in- / step mechanism."}, {"heading": "5 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Training, Development, and Test Data", "text": "The training data consists of a small set of 12,000 post-processed triplets (src, mt, pe), where src is the original English text, mt is the raw MT output generated by an English-German system, and pe is the human post-processed MT output. The MT system used to produce the raw MT output is unknown, as is the original training data. The task is to automatically correct the MT output so that it resembles human manipulated data. The main task metric is TER (Snover et al., 2006) - the lower the better - with BLEU (Papineni et al., 2002) as a secondary metric. To overcome the problem that there is too little training data available, Junczys-Dowkent and Grundkiewicz (2016) - the authors of the best WMT16 AP have provided large amounts of training data."}, {"heading": "5.2 Training parameters", "text": "All models are trained on the basis of the same training data. \u2022 Models with single input encoders only take the raw MT output (mt) as input, double encoder models use the raw MT output (mt) and the original source (pe). The training procedures and model settings are, whenever possible, the same: \u2022 All embedding vectors consist of 512 units, the RNN states use 1024 units. We select a vocabulary size of 40,000 for all inputs and outputs. When hard attention models are trained, the maximum sentence length is 100 to accommodate the additional step symbols, otherwise 50. \u2022 To avoid overtaking, we use ubiquitous downtime steps (Gal, 2015) via GRU steps and input embedding, with failure probabilities 0.2 and via source and target words with probabilities 0.2. \u2022 We use Adam (Kingma and Ba, 2014) as our Adam, Adam Gynge, with a total of one mini-size Gynga-4, and all models with four."}, {"heading": "5.3 Evaluation", "text": "Table 2 contains a selection of the most relevant results for the WMT16 APE common task - during the task and after. WMT 2016 BASELINE-1 is the raw uncorrected MT output. BASELINE-2 is the result of a vanilla phrase-based Moses system (Koehn et al., 2007) trained only on the official 12,000 sentences. Junczys-Dowmunt and Grundkiewicz (2016) is the best system in the common task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test group for a single neural model (single source), while Pal et al al al al al al. (2017) RERANKING - the overall best reported result on the test group - is a system combination of Pal et al. (2017) SYMMETRIC with phrase-based models re-ranking the n-best list. In Table 3 we present the results for the work discussed in these models."}, {"heading": "5.4 On Faithfulness", "text": "One question remains: We postulated several times that the hard-attention models could have a potential for higher accuracy. Since the APE task is predominantly a monolingual task, we can verify this by comparing the TER values in terms of reference post output (TER-pe) and the TER values in terms of raw APE output (TER-mt). The lower the TER-mt value, the less changes were made to the input in order to arrive at the output, resulting in higher accuracy. Table 4 provides this comparison for the APE test set of the WMT 2016. We can actually verify that the hard-attention models stick to the input and make fewer changes than their soft-attention counterparts. This difference is particularly dramatic for ENCDEC-M-CGRU and ENCDEC-M-CGRU-HARD, where only slight differences in the TER-pe occur, but a gap of more than two TER-TER points for TER-mt."}, {"heading": "6 Conclusions and Future Work", "text": "In this paper, we presented several neural APE models that are equipped with non-standard attention mechanisms and combinations, including, for the first time, applying hard attention models to APE, while previously proposing double soft attention models for APE tasks, but with inconclusive results. This is the first paper to present current results for double attention models that integrate complete post-processing triplets into a single end-to-end model. Ensembles of double attention models provide more than 1.52 TER points improvement over the best WMT-2016 system and 0.7 TER improvements over the best reported system combination for the same testset.We also demonstrated that hard attention models for APE provide results similar to those of pure soft attention models, but do so by making fewer changes to the input. This could be a useful feature in scenarios where conservative processing quality for APE should be used to evaluate the impact of the attention-green directions on the HER."}, {"heading": "Acknowledgments", "text": "This research was funded by the Amazon Academic Research Awards program."}], "references": [{"title": "Sequence to sequence transduction with hard monotonic attention", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1611.01487 .", "citeRegEx": "Aharoni and Goldberg.,? 2016", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Statistical post-editing for a statistical MT system", "author": ["Hanna B\u00e9chara", "Yanjun Ma", "Josef van Genabith."], "venue": "Proceedings of the 13th Machine Translation Summit. Xiamen, China, pages 308\u2013315.", "citeRegEx": "B\u00e9chara et al\\.,? 2011", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2011}, {"title": "An evaluation of statistical post-editing systems applied to RBMT and SMT systems", "author": ["Hanna B\u00e9chara", "Rapha\u00ebl Rubino", "Yifan He", "Yanjun Ma", "Josef van Genabith."], "venue": "Proceedings of COLING 2012. Mumbai, India, pages 215\u2013230.", "citeRegEx": "B\u00e9chara et al\\.,? 2012", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2012}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Doubly-attentive decoder for multi-modal neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1702.01287. http://arxiv.org/abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Exploring the planet of the APEs: a comparative study of state-of-the-art methods for MT automatic post-editing", "author": ["Rajen Chatterjee", "Marion Weller", "Matteo Negri", "Marco Turchi."], "venue": "Proceedings of the 53rd Annual Meeting of the As-", "citeRegEx": "Chatterjee et al\\.,? 2015", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2015}, {"title": "Learning Phrase Representations Using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal."], "venue": "ArXiv e-prints .", "citeRegEx": "Gal.,? 2015", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Algorithms for the longest common subsequence problem", "author": ["Daniel S. Hirschberg."], "venue": "J. ACM 24(4):664\u2013 675. https://doi.org/10.1145/322033.322044.", "citeRegEx": "Hirschberg.,? 1977", "shortCiteRegEx": "Hirschberg.", "year": 1977}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "arXiv preprint arXiv:1610.01108 http://arxiv.org/abs/1610.01108.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the First Conference on Machine Translation. pages 751\u2013", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Attention strategies for multi-source sequenceto-sequence learning", "author": ["Jindrich Libovick\u00fd", "Jindrich Helcl."], "venue": "CoRR abs/1704.06567. http://arxiv.org/abs/1704.06567.", "citeRegEx": "Libovick\u00fd and Helcl.,? 2017", "shortCiteRegEx": "Libovick\u00fd and Helcl.", "year": 2017}, {"title": "CUNI system for WMT16 automatic post-editing and multimodal translation tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina."], "venue": "Proceedings of the First Conference on Machine", "citeRegEx": "Libovick\u00fd et al\\.,? 2016", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proc. of the Annual Meeting on Association for Computational Linguistics. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Neural automatic post-editing using prior alignment and reranking", "author": ["Santanu Pal", "Sudip Kumar Naskar", "Mihaela Vela", "Qun Liu", "Josef van Genabith."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics. pages", "citeRegEx": "Pal et al\\.,? 2017", "shortCiteRegEx": "Pal et al\\.", "year": 2017}, {"title": "USAAR-SAPE: An English\u2013Spanish statistical automatic post-editing system", "author": ["Santanu Pal", "Mihaela Vela", "Sudip Kumar Naskar", "Josef van Genabith."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. Association for", "citeRegEx": "Pal et al\\.,? 2015", "shortCiteRegEx": "Pal et al\\.", "year": 2015}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Asso-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Nematus: a toolkit", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Statistical phrase-based post-editing", "author": ["Michel Simard", "Cyril Goutte", "Pierre Isabelle."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Simard et al\\.,? 2007", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "A Study", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": null, "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Multisource neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "CoRR abs/1601.00710. http://arxiv.org/abs/1601.00710.", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": ", 2016) and compare our performance against the shared task winner, the system submitted by the Adam Mickiewicz University (AMU) team (Junczys-Dowmunt and Grundkiewicz, 2016), and a more recent system by Pal et al.", "startOffset": 134, "endOffset": 174}, {"referenceID": 11, "context": ", 2016) and compare our performance against the shared task winner, the system submitted by the Adam Mickiewicz University (AMU) team (Junczys-Dowmunt and Grundkiewicz, 2016), and a more recent system by Pal et al. (2017) with the previously best published results on the same test set.", "startOffset": 135, "endOffset": 222}, {"referenceID": 3, "context": "The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015).", "startOffset": 126, "endOffset": 173}, {"referenceID": 6, "context": "The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015).", "startOffset": 126, "endOffset": 173}, {"referenceID": 16, "context": "Before the application of neural sequence-tosequence models to APE, most APE systems would rely on phrase-based SMT following a monolingual approach first proposed by Simard et al. (2007). B\u00e9chara et al.", "startOffset": 167, "endOffset": 188}, {"referenceID": 2, "context": "B\u00e9chara et al. (2011) proposed a \u201csource-context aware\u201d variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT output and source token pairs.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "B\u00e9chara et al. (2011) proposed a \u201csource-context aware\u201d variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT output and source token pairs. The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015). The quality of the word alignments plays an important role for this methods, as shown for instance by Pal et al. (2015).", "startOffset": 0, "endOffset": 515}, {"referenceID": 15, "context": "During the WMT 2016 APE two systems relied on neural models, the CUNI system (Libovick\u00fd et al., 2016) and the shared task winner, the system submitted by the AMU team (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 77, "endOffset": 101}, {"referenceID": 11, "context": ", 2016) and the shared task winner, the system submitted by the AMU team (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 73, "endOffset": 113}, {"referenceID": 16, "context": "The influence of the components on the final result was tuned with Minimum Error Rate Training (Och, 2003) with regard to the task metric TER.", "startOffset": 95, "endOffset": 106}, {"referenceID": 14, "context": "\u2022 and finally, the large amounts of artificial data provided by the authors make it feasible to explore the influence of model choices on post-editing quality without worrying about data scarcity \u2013 a problem suffered from by other neural approaches to the same task, see for instance Libovick\u00fd and Helcl (2017).", "startOffset": 284, "endOffset": 311}, {"referenceID": 17, "context": "Following the WMT 2016 APE shared task, Pal et al. (2017) published work on another neural APE system that integrates precomputed wordalignment features into the neural structure and enforces symmetric attention during the neural training process.", "startOffset": 40, "endOffset": 58}, {"referenceID": 20, "context": "The attentional encoder-decoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017).", "startOffset": 99, "endOffset": 122}, {"referenceID": 1, "context": "The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention.", "startOffset": 56, "endOffset": 79}, {"referenceID": 1, "context": "The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention. The summary provided in this section is based on the description in Sennrich et al. (2017).", "startOffset": 56, "endOffset": 251}, {"referenceID": 7, "context": "The GRU RNN cell (Cho et al., 2014) is defined as:", "startOffset": 17, "endOffset": 35}, {"referenceID": 0, "context": "Following the description by Aharoni and Goldberg (2016) for their LSTM-based model, we now adapt the previously described encoder-decoder model to incorporate hard attention.", "startOffset": 29, "endOffset": 57}, {"referenceID": 9, "context": "For the described APE task, using the Longest Common Subsequence algorithm (Hirschberg, 1977), we first generate a sequence of match, delete and insert operations which transform the raw MT output (x1, \u00b7 \u00b7 \u00b7xTx) into the corrected post-edited sequence (y1, \u00b7 \u00b7 \u00b7 yTy).", "startOffset": 75, "endOffset": 93}, {"referenceID": 23, "context": "Neural multi-source models (Zoph and Knight, 2016) seem to be natural fit for the APE task, as raw MT output and original source language input are available.", "startOffset": 27, "endOffset": 50}, {"referenceID": 14, "context": "Although applications to the APE problem have been reported (Libovick\u00fd and Helcl, 2017), state-of-the-art results seem to be missing.", "startOffset": 60, "endOffset": 87}, {"referenceID": 5, "context": "In the decoder, we replace the conditional GRU with attention, with a doubly-attentive cGRU cell (Calixto et al., 2017) over contexts C and C:", "startOffset": 97, "endOffset": 119}, {"referenceID": 11, "context": "Adapted from Junczys-Dowmunt and Grundkiewicz (2016).", "startOffset": 13, "endOffset": 53}, {"referenceID": 22, "context": "The main task metric is TER (Snover et al., 2006) \u2014 the lower the better \u2014 with BLEU (Papineni et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 19, "context": ", 2006) \u2014 the lower the better \u2014 with BLEU (Papineni et al., 2002) as a secondary metric.", "startOffset": 43, "endOffset": 66}, {"referenceID": 11, "context": "To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) \u2014 the authors of the best WMT16-APE shared task system \u2014 generated large amounts of artificial data via round-trip translations.", "startOffset": 53, "endOffset": 93}, {"referenceID": 8, "context": "\u2022 To avoid overfitting, we use pervasive dropout (Gal, 2015) over GRU steps and input embeddings, with dropout probabilities 0.", "startOffset": 49, "endOffset": 60}, {"referenceID": 12, "context": "\u2022 We use Adam (Kingma and Ba, 2014) as our optimizer, with a mini-batch size of 64.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "devset cross-entropy of each training run are averaged element-wise (Junczys-Dowmunt et al., 2016) resulting in new single models with generally improved performance.", "startOffset": 68, "endOffset": 98}, {"referenceID": 13, "context": "BASELINE-2 is the results of a vanilla phrase-based Moses system (Koehn et al., 2007) trained only on the official 12,000 sentences.", "startOffset": 65, "endOffset": 85}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task.", "startOffset": 0, "endOffset": 40}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al.", "startOffset": 0, "endOffset": 97}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al. (2017) RERANKING \u2014 the overall best reported result on the test set \u2014 is a system combination of Pal et al.", "startOffset": 0, "endOffset": 240}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al. (2017) RERANKING \u2014 the overall best reported result on the test set \u2014 is a system combination of Pal et al. (2017) SYMMETRIC with phrase-based models via n-best list re-ranking.", "startOffset": 0, "endOffset": 348}, {"referenceID": 17, "context": "The double-attention models, however, each outperform the best WMT16 system and the currently reported best single-model Pal et al. (2017) SYMMETRIC.", "startOffset": 121, "endOffset": 139}, {"referenceID": 17, "context": "The double-attention models, however, each outperform the best WMT16 system and the currently reported best single-model Pal et al. (2017) SYMMETRIC. The ensembles also beat the system combination Pal et al. (2017) RERANKING in terms of TER (not in terms of BLEU though).", "startOffset": 121, "endOffset": 215}, {"referenceID": 11, "context": "47 Junczys-Dowmunt and Grundkiewicz (2016) 21.", "startOffset": 3, "endOffset": 43}], "year": 2017, "abstractText": "In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs mt and src in a single neural architecture, modeling {mt, src} \u2192 pe directly. Apart from that, we investigate the influence of hardattention models which seem to be wellsuited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT 2016 shared task on automatic postediting and can demonstrate that doubleattention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Double-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.", "creator": "LaTeX with hyperref package"}}}