{"id": "1706.04215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Identifying Spatial Relations in Images using Convolutional Neural Networks", "abstract": "Traditional approaches to building a large scale knowledge graph have usually relied on extracting information (entities, their properties, and relations between them) from unstructured text (e.g. Dbpedia). Recent advances in Convolutional Neural Networks (CNN) allow us to shift our focus to learning entities and relations from images, as they build robust models that require little or no pre-processing of the images. In this paper, we present an approach to identify and extract spatial relations (e.g., The girl is standing behind the table) from images using CNNs. Our research addresses two specific challenges: providing insight into how spatial relations are learned by the network and which parts of the image are used to predict these relations. We use the pre-trained network VGGNet to extract features from an image and train a Multi-layer Perceptron (MLP) on a set of synthetic images and the sun09 dataset to extract spatial relations. The MLP predicts spatial relations without a bounding box around the objects or the space in the image depicting the relation. To understand how the spatial relations are represented in the network, a heatmap is overlayed on the image to show the regions that are deemed important by the network. Also, we analyze the MLP to show the relationship between the activation of consistent groups of nodes and the prediction of a spatial relation. We show how the loss of these groups affects the networks ability to identify relations.", "histories": [["v1", "Tue, 13 Jun 2017 18:24:11 GMT  (3634kb,D)", "http://arxiv.org/abs/1706.04215v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CV cs.NE", "authors": ["mandar haldekar", "ashwinkumar ganesan", "tim oates"], "accepted": false, "id": "1706.04215"}, "pdf": {"name": "1706.04215.pdf", "metadata": {"source": "CRF", "title": "Identifying Spatial Relations in Images using Convolutional Neural Networks", "authors": ["Mandar Haldekar", "Ashwinkumar Ganesan", "Tim Oates"], "emails": ["gashwin1@umbc.edu", "oates@cs.umbc.edu"], "sections": [{"heading": null, "text": "In fact, most people who are able to survive on their own are still alive even if they are not able to survive on their own."}, {"heading": "II. RELATED WORK", "text": "A Convolutionary Neural Network (CNN) is a feed-forward network that uses a combination of revolutionary layers, pooling layers, and fully connected layers. It is common practice to train with a stack of input images (determined during the fine-tuning of the network) and the gradient is calculated for the average loss across the stack. There are several CNN architectures that have been suggested to classify an object in images such as AlexNet [15], VGGNet [27], and Inception [30]. One of the main reasons why CNNs are effective is that the network is trained on large data sets (such as Imagenet) and can be applied directly (with fine-tuning) to use object recognition on images from different domains (domain adaptation) as well as for various visual recognition tasks [6]. VGNetLike many CNN architectures, VGNet [27] is used for output of images from the Imagenet field (SVILC) of network layers, which are fully connected to the small network layer used."}, {"heading": "B. Spatial Relations", "text": "Especially in the USA, where the number of unemployed has increased by more than a third in the last ten years, the number of unemployed has fallen by more than half in the last ten years. Unemployment in the USA has doubled in the last ten years. Unemployment in the USA has tripled in the last ten years. Unemployment in the USA has tripled in the last ten years, unemployment in the USA has increased by more than 20%. Unemployment in the USA has tripled, while unemployment in the USA has increased by more than 20%. Unemployment in the USA has tripled in the last ten years, while unemployment in the USA has increased by more than 20%. Unemployment in the USA has tripled."}, {"heading": "III. PRELIMINARIES & ANALYSIS METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Cross Entropy", "text": "Consider a trained neural network A. In view of an image X (i) of size MxN and the caption of the spatial relation y1... yz, the softmax function of the cross entropy is: C (i) = \u2212 z \u2211 y = 1Pi (y) log (Qi (y)) (1), where C (i) is the cross entropy, Qi (y) is the softmax probability of the label and Pi (y) is the actual value of the label. An input image can be modified for a number of reasons, such as identification of the important regions of an image, isolation of a particular object in the image, etc. Let X (i) j be the modified version of the original image X (i). Using Equation 1, the cross entropy of the new image C (i) j (with the caption of the original image) can be calculated (C (i))).The entropy difference resulting from the measurement (during the measurement) is used to (E)."}, {"heading": "B. Characterizing the Cross Entropy Difference", "text": "Once a network is trained, the size of E (i) indicates the meaning of the image change. The larger the size of E (i), the higher the likely meaning. We call this the influence of the change introduced (in the image or in the network) on the ability of the network to make a prediction. There are three possible conditions: \u2022 E (i) > 0: impact is positive, i.e. the change introduced increases the probability that the image will be identified as a label. \u2022 E (i) < 0: impact is negative, i.e. the change introduced reduces the probability that the image will be identified as a label. \u2022 E (i) = 0: No impact."}, {"heading": "C. Ablating Nodes in Layers", "text": "Consider a fully connected layer li of size n. A fully connected layer li \u2212 1 is size h, the resulting output is Oi \u2212 1 (size 1 \u00b7 h). The weight vector is Wh \u00b7 n and b1 \u00b7 n is the preload. A fully connected layer generates an output: Oi = max (0, W \u00b7 Oi \u2212 1 + b), where a rectified linear unit (ReLU) is the activation function. We perform an elementary multiplication with a vector A1 \u00b7 n.AOi = Oi AA is the ablation vector. The vector contains at each index position 1 except the nodes that must be removed and have a value of 0. AOi is the final output of the layer that is transferred to the next layer. Since we use a ReLU, the node may have a final output of 0 or a value greater than 0. Therefore, the method will only remove nodes if the output is greater than 0."}, {"heading": "D. Extract Important Regions of Images", "text": "The key idea is to artificially mask small regions of the image and measure its influence (eq. 2).The mask is a spot placed on the specific part of the image that renders the region unrecognizable, like a gray rectangular mask for images with non-gray colored backgrounds. Below is the method of measuring the influence: Consider an image X (i) with cross-entropy C (i), create a gray mask for size AxB. The mask is moved sequentially onto the image to create a series of images X (i) S = {X (i) k}, where each image has a single region that is masked. The number of regions k depends on the size of the mask and the incremental level at which the mask is next applied to the image."}, {"heading": "E. Analysis of effect of nodes on spatial relations", "text": "Another way to understand how the network learns spatial relations is to analyze the internals of the architecture and isolated groups of nodes that have a positive influence on the classification of a particular spatial relationship, which is calculated by measuring the change in the entropy of the cross when a particular node or group of nodes is removed (i.e. set to zero). Consider a trained MLP A and its fully connected (FC) layer. The size of the l is p. The nodes in the layer are N = {n1... np} Let the relations be Y = {y1.. yz}. Considering an image set X, we first find the cross entropy of the baseline: C (Y) = {C (i) (y) | i-X, y-Y} We tap a single node at a time and measure the change in entropy. Thus, each image in the test data set has an influence value E in relation to the desired node. The overall node has an impact on the relationship (the) of the corresponding relationship."}, {"heading": "IV. SYSTEM ARCHITECTURE & DATASETS", "text": "Figure 3 shows the overall architecture of our system. It consists of two parts: \u2022 Pre-processing: Images of the size 224x224 are transformed into a feature vector using VGGNet (output of layer FC-7), which has a size of 4096. \u2022 Model generation: Once the images are pre-processed, a two-layer MLP is trained to detect the spatial relationship. The type of predicted relationship depends on the method by which the labels are encoded. Image pre-processing A pre-trained model of a 16-layer VGGNet [27] is used. Input requires a default size of 224 x 224 pixels. Therefore, before the image is transformed, it is scaled (original images have a different number of pixels). The process is completed in a single forward pass. While VGGNet performs object detection, the feature vector from the FC-7 layer is used as the second layer, which is not completely connected to the network layer."}, {"heading": "B. Model Generation", "text": "The extracted image attributes are transmitted as inputs to a multi-layer perceptron, as shown in Figure 3. The MLP contains two fully connected layers. The hidden layers of the MLP are ReLU enabled. The failure rate is 0.5 and there is a Softmax classifier in the output layer. Category cross entropy is the loss function. An Adam Optimizer calculates the gradient after each stack (size 10). The learning rate is 0.001. There are two types of data sets to evaluate the system. A modified version of the SUN09 dataset [22] and a synthetic dataset containing a limited number of objects and relationships in the image."}, {"heading": "C. SUN09 Dataset", "text": "The original SUN09 dataset consists of approximately 12,000 images. Malinowski et. al. [22] takes a subset of them and comments on them with spatial prepositions such as above, below, behind and before. Images were randomly selected to comment on them. The final dataset consists of 53 structured queries (i.e. tuples defined as subject-relation objects) and 11 unique spatial relationships. There are 4468 images for training and 4955 images for testing."}, {"heading": "D. Synthetic Dataset", "text": "This dataset consists of 10 object types in the training dataset and 5 object types in the test dataset. The objects are: \u2022 Training: dog, tiger, table, lamp, television, sofa, ball, hat, vase and vacuum cleaner. \u2022 Test: deer, lion, drawer, bag, car. Each object above has a variety of images in the dataset. For example, there are 7 different dog images and 5 different TV images. The images contain any two objects that are either rotated or translated to different locations in a template with different background colors. They are then combined into a single image. Figure 4 shows a sample set of images. The training dataset contains 2628 images, while the test contains 432 images. Each object is commented as an object relation object.Background colors used in the test are completely different from those used, while the differentiating object between the individual objects is not significant (while the synthetic objects are related to each other)."}, {"heading": "V. EXPERIMENTAL RESULTS & DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Extracting Regions", "text": "It shows the accuracy of the network in terms of training and test data. We use images of training data (synthetic data sets) correctly classified by the network for our analysis. We analyzed 50 training images for each of the 3 spatial planning classes correctly classified by the MLP. The objects used for the test are completely separated from the objects in the synthetic data set used for training. As they are new in size of 224 x 224 pixels, a total of 196 images are generated once each region has an image manipulation mask of size 16 x 16 pixels."}, {"heading": "B. Measuring Influence of Layer Nodes", "text": "We use the method discussed in Section III-E. For this experiment, we analyzed the training data set (not the test), as there is a probability of extracting node groups to avoid negative influences from the images in which the network is misclassified. For each class label, we consider the top 25% of the nodes from the total size of the hidden layer so that their decomposition will result in a maximum increase in cross entropy. We group these into three groups, one per class label. In this way, we obtain three groups of nodes in which each group of nodes influences a particular spatial relationship more than others. Figures 5 (a) and (b) show the differences in cross entropy compared to the group being broken up for the FC-0 and FC-1 layers. Each group has 3 bars, i.e. absolute cost difference, positive cost difference, and negative cost difference (made positive) of the nodes, so that any change can be described as positive."}, {"heading": "VI. CONCLUSIONS & FUTURE WORK", "text": "We have analyzed how an MLP (with a pre-trained CNN) learns spatial relations between objects in an image. We extract image characteristics using CNNs and train an MLP in a controlled manner to classify spatial relations on two sets of data: SUN09 and a simplified synthetic dataset. We show that the network pays attention to specific parts of the image while classifying them for spatial relations, and that the network searches for relative positions of objects to identify the relationship. We also show the existence of nodes that represent a particular relationship by conducting extensive testing to isolate individual nodes that affect a particular relationship and how the group behaves when it is removed. In the future, we want to visualize visual data in symbolic form by getting the network to identify more relationships (spatial and other) and objects by designing the network in such a way that both subject and object labels are predicted, as well as the binary images and the relational labels."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Dbpedia-a crystallization point for the web of data. Web Semantics: science, services and agents on the world wide web", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "CoRR, abs/1310.1531,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "On the relationship between visual attributes and convolutional networks", "author": ["V. Escorcia", "J.C. Niebles", "B. Ghanem"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "The ecological approach to visual perception: classic edition", "author": ["J.J. Gibson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "R-cnns for pose estimation and action detection", "author": ["G. Gkioxari", "B. Hariharan", "R. Girshick", "J. Malik"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "CoRR, abs/1412.1058,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F. Li"], "venue": "CoRR, abs/1406.5679,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Image retrieval with structured object queries using latent ranking svm", "author": ["T. Lan", "W. Yang", "Y. Wang", "G. Mori"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A computational analysis of the apprehension of spatial relations", "author": ["G.D. Logan", "D.D. Sadler"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Do Convnets Learn Correspondence", "author": ["J.L. Long", "N. Zhang", "T. Darrell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A pooling approach to modelling spatial relations for image retrieval and annotation", "author": ["M. Malinowski", "M. Fritz"], "venue": "[cs.CV],", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "CoRR, abs/1403.6382,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases", "author": ["F. Sadeghi", "S.K. Divvala", "A. Farhadi"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Deconvolutional Networks for Feature Learning", "author": ["M. Zeiler", "D. Krishnan", "G. Taylor", "R. Fergus"], "venue": "In Cvpr,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Learning Deep Features for Discriminative Localization", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Reasoning about object affordances in a knowledge base representation", "author": ["Y. Zhu", "A. Fathi", "L. Fei-Fei"], "venue": "Vision\u2013ECCV", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Knowledge graphs (KG) such DBpedia [3] and Yago [29] store entities and their relations in the form of RDF triples.", "startOffset": 35, "endOffset": 38}, {"referenceID": 27, "context": "Knowledge graphs (KG) such DBpedia [3] and Yago [29] store entities and their relations in the form of RDF triples.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "perform tasks such as image search, visual verification [25] (i.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": ", to verify the validity of a fact by looking for proof in pictures) and visual question answering [2] (i.", "startOffset": 99, "endOffset": 102}, {"referenceID": 32, "context": "al [34] use a knowledge graph representation to reason about object affordances [8], where the system tries to learn", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "al [34] use a knowledge graph representation to reason about object affordances [8], where the system tries to learn", "startOffset": 80, "endOffset": 83}, {"referenceID": 19, "context": "Object detection, scene description, pose estimation and other such tasks were commonly solved using SIFT [21] and HOG [4] features.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "Object detection, scene description, pose estimation and other such tasks were commonly solved using SIFT [21] and HOG [4] features.", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "But in recent years, deep neural networks have had tremendous success in diverse fields such as computer vision [16] [9], natural language processing [12] [28] and audio [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": "But in recent years, deep neural networks have had tremendous success in diverse fields such as computer vision [16] [9], natural language processing [12] [28] and audio [11].", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "But in recent years, deep neural networks have had tremendous success in diverse fields such as computer vision [16] [9], natural language processing [12] [28] and audio [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 26, "context": "But in recent years, deep neural networks have had tremendous success in diverse fields such as computer vision [16] [9], natural language processing [12] [28] and audio [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 9, "context": "But in recent years, deep neural networks have had tremendous success in diverse fields such as computer vision [16] [9], natural language processing [12] [28] and audio [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "in 2012 [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "CNNs are also able to achieve state-of-the-art performance in many of the aforementioned tasks [9] [10] [33].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "CNNs are also able to achieve state-of-the-art performance in many of the aforementioned tasks [9] [10] [33].", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "CNNs are also able to achieve state-of-the-art performance in many of the aforementioned tasks [9] [10] [33].", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "intrinsic [19].", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "There are multiple CNN architectures that have been proposed to classify an object in images such AlexNet [15], VGGNet [27] and Inception [30].", "startOffset": 106, "endOffset": 110}, {"referenceID": 25, "context": "There are multiple CNN architectures that have been proposed to classify an object in images such AlexNet [15], VGGNet [27] and Inception [30].", "startOffset": 119, "endOffset": 123}, {"referenceID": 28, "context": "There are multiple CNN architectures that have been proposed to classify an object in images such AlexNet [15], VGGNet [27] and Inception [30].", "startOffset": 138, "endOffset": 142}, {"referenceID": 4, "context": "One of the key reasons why CNNs are effective is because the network is trained on large datasets (such as imagenet) and can be directly applied (with fine tuning) to perform object recognition on images from different domains (domain adaptation) as well as used for various visual recognition tasks [6].", "startOffset": 300, "endOffset": 303}, {"referenceID": 25, "context": "Like many CNN architectures, VGGNet [27] is trained on the imagenet (ILSVRC) dataset [5].", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "Like many CNN architectures, VGGNet [27] is trained on the imagenet (ILSVRC) dataset [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "Output of specific layers are used to describe images [13] (layer FC-7).", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "objects in images relies on constructing handcrafted features by manually inspecting the dataset [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": ", to identify the relation [19].", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "al [22] create spatial templates for each relation and then use a bi-directional fragment embedding framework to associate the template with text triplets [14].", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "al [22] create spatial templates for each relation and then use a bi-directional fragment embedding framework to associate the template with text triplets [14].", "startOffset": 155, "endOffset": 159}, {"referenceID": 29, "context": "al [31] create a deconvolutional network to show the mid-level representations in a convolutional", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "[26] backpropagate the gradient of the class score with respect to the image pixels to generate an artificial image which is representative of what is learned for that particular class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] prove that despite using large receptive fields, CNNs learn the correspondence between object parts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] provide empirical evidence of attribute centric nodes (ACNs) inside a CNN that are trained to recognize specific objects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "Although this can be achieved by a class activation mapping technique which finds the regions in a single forward pass of the network [32], a fully convolutional neural network without any fully connected layers is required.", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "A pre-trained model of a 16-layer VGGNet [27] is used.", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "the multi-layer perceptron (figure 3) because features extracted are robust in terms of the kinds of objects detected, and are translation and rotation invariant [24].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "A modified version of the SUN09 dataset [22] and a synthetic dataset containing a limited number of objects and relations in the image.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "[22] take a subset of them and", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We plan to use image captioning datasets such as MS COCO [18] and Flickr30k [23] and annotate the images with spatial relations between object pairs by parsing the image descriptions.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "We plan to use image captioning datasets such as MS COCO [18] and Flickr30k [23] and annotate the images with spatial relations between object pairs by parsing the image descriptions.", "startOffset": 76, "endOffset": 80}], "year": 2017, "abstractText": "Traditional approaches to building a large scale knowledge graph have usually relied on extracting information (entities, their properties, and relations between them) from unstructured text (e.g. Dbpedia). Recent advances in Convolutional Neural Networks (CNN) allow us to shift our focus to learning entities and relations from images, as they build robust models that require little or no pre-processing of the images. In this paper, we present an approach to identify and extract spatial relations (e.g., The girl is standing behind the table) from images using CNNs. Our research addresses two specific challenges: providing insight into how spatial relations are learned by the network and which parts of the image are used to predict these relations. We use the pre-trained network VGGNet to extract features from an image and train a Multi-layer Perceptron (MLP) on a set of synthetic images and the sun09 dataset to extract spatial relations. The MLP predicts spatial relations without a bounding box around the objects or the space in the image depicting the relation. To understand how the spatial relations are represented in the network, a heatmap is overlayed on the image to show the regions that are deemed important by the network. Also, we analyze the MLP to show the relationship between the activation of consistent groups of nodes and the prediction of a spatial relation. We show how the loss of these groups affects the network\u2019s ability to identify relations.", "creator": "LaTeX with hyperref package"}}}