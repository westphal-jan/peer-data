{"id": "1706.04223", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Adversarially Regularized Autoencoders for Generating Discrete Structures", "abstract": "Generative adversarial networks are an effective approach for learning rich latent representations of continuous data, but have proven difficult to apply directly to discrete structured data, such as text sequences or discretized images. Ideally we could encode discrete structures in a continuous code space to avoid this problem, but it is difficult to learn an appropriate general-purpose encoder. In this work, we consider a simple approach for handling these two challenges jointly, employing a discrete structure autoencoder with a code space regularized by generative adversarial training. The model learns a smooth regularized code space while still being able to model the underlying data, and can be used as a discrete GAN with the ability to generate coherent discrete outputs from continuous samples. We demonstrate empirically how key properties of the data are captured in the model's latent space, and evaluate the model itself on the tasks of discrete image generation, text generation, and semi-supervised learning.", "histories": [["v1", "Tue, 13 Jun 2017 19:00:53 GMT  (146kb,D)", "http://arxiv.org/abs/1706.04223v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["junbo zhao", "yoon kim", "kelly zhang", "alexander m rush", "yann lecun"], "accepted": false, "id": "1706.04223"}, "pdf": {"name": "1706.04223.pdf", "metadata": {"source": "CRF", "title": "Adversarially Regularized Autoencoders for Generating Discrete Structures", "authors": ["Junbo (Jake"], "emails": ["jakezhao@cs.nyu.edu", "yoonkim@seas.harvard.edu", "kz918@nyu.edu", "srush@seas.harvard.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 Related Work", "text": "The success of GANs on images has led to GANS being applied discreetly, e.g. to texts. Political graduation methods are a natural way to deal with the resulting undifferentiated generators. [5] This means that there will be a latent encryption of the sentence, and there will also be a potential disadvantage of the existing models. [6]"}, {"heading": "3 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Generative Adversarial Networks", "text": "Generative Adversarial Networks (GANs) are a class of parametrized implicit generative models [9]. The method approaches the collection of samples from a real distribution c \u0445 Pr, using instead a latent variable z and a parametrized deterministic generator function c (z) to generate generated samples c (p). The goal is to characterize the complex data described by the unknown Pr within the latent space. GAN training uses two separate models: a generator g (z) forms a latent vector from an easy-to-evaluate source distribution to a value, and a critic / discriminator f (c) aims to distinguish real data from fake samples generated by g. The generator is trained to deceive the critic, and the critic to separate real from generated ones. In this work, we use the recently proposed Waterstone GAN (WAN) and the real distribution (GAN)."}, {"heading": "3.2 Discrete Structure Autoencoders", "text": "Ideally, the code represents important abstract features of the original input (although this is difficult to formalize), rather than simply learning to copy any arbitrary code. We are interested in probabilistic autoencoders for discrete structures. Define X = Vn as a discrete set of structures where V is a vocabulary of symbols. For example, a discrete structure autoencoder consists of two parameterized functions: a deterministic encoder function: X 7 \u2192 C with the number of pixels or for sentences V = {1,.., # words} and n with the sentence length. A discrete structure autoencoder consists of two parameterized functions: a deterministic encoder function encodes: X 7 \u2192 C with the parameters \u03c6 and a decoder distribution p\u0445 (x) with the parameters that give a distribution over the structures. \u2212 The model is trained on transverse entropy reconstruction, which we learn a problem with clicking the input function (the incoder)."}, {"heading": "4 Model", "text": "The goal is to take advantage of the GAN's ability to learn the latent structure of code data while using an autoencoder to abstract the coding and generation of discrete structures to support GAN training. The main difference to WGANs as described above is that we no longer have access to observed data samples for GAN. Instead, we have access to discrete structure x-Px, where Px is the distribution of interest. (Working with this space would require direct backpropagation through non-differentiable operations and form the basis for political gradient methods for GAN training.) We address this problem by integrating an encoder into the process that first defines a part x to a continuous code. (We would require backward propagation through non-differentiable operations and is the basis for political gradient methods for GAN training.) We address this problem by integrating an encoder into the process."}, {"heading": "5 Architectures", "text": "We look at two different instantiations of ARAEs = c, one for discrete images and the other for text sequences. (For both models we use the same WGAN architecture, but they only exist in different forms.) The structure of deterministic encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder"}, {"heading": "6 Methods and Data", "text": "We look at two different settings for testing the ARAE: (1) images, using the binary version of MNIST, and (2) text, using the Stanford Natural Language Inference Corpus [3]. This corpus provides a useful test bed, as it consists of sentences with a relatively simple structure; the corpus is also provided with pairs of sentence classifications that allow us to experiment with semi-supervised learning in a controlled setting. For this task, the model is presented with two sentences - premise and hypothesis - and must predict their relationship: dissociation, contradiction, or neutrality. For the training, we used a subset of the corpus consisting of sentences of less than 15 words, although preliminary results suggest that this approach works up to 30 words. We look at several different empirical tasks to assess the performance of the model as both autoencoding (ARAE) and latent-variable (GAN-space)."}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Code Space Structure", "text": "Since the code space we use by definition is not capable of representing the entire discrete input space, the autoencoder would ideally learn to maintain valid representations only for real inputs that exist roughly along a low-dimensional multiplicity determined by the space of natural images or natural language sentences, a property that is difficult to maintain in standard auto-encoders, which often learn partial identity mapping but ideally should be improved by code space regulation. We test this property by passing two sets of samples through ARAE, one of real samples and the other of explicitly noisy examples that exist outside of this multiplicity. Figure 2 (left) shows these examples and their reconstruction from the discredited MNIST, where the noisy examples come from the addition of noise to the original image. For images, we observe that a regular input simply copies the data, regardless of whether the input is based on the input."}, {"heading": "7.2 Semi-Supervised Training", "text": "Next, we use ARAE for the semi-supervised training on a natural language sequence task shown in Table 2 (right). We experiment with the use of 22.2%, 10.8% and 5.25% of the originally labeled training data, and use the rest of the training for the unlabeled AE training. The labeled set is randomly selected. The complete SNLI training contains 543k set pairs, and we use the subset of unlabeled set pairs of 120k, 59k and 28k for the three sets, respectively. As a starting point, we use an AU trained on the additional data, similar to the settings examined in [7]. For ARAE, we use the subset of unsupervised set sets of length < 15, which roughly contains 655k individual sets (due to the length restriction, this is a subset of 715k sets used for the AE training)."}, {"heading": "AE Samples", "text": "A man clutches a woman walking past a barbecue, a couple of motorcycles loading my bicycle, the actor is walking in a small dog area, no dog is a young man walking outside on a dirt road, sitting in the dock, a large group of people are taking a photo of themselves playing a football match, the man and the woman are in an empty stadium pointing to a mountain, a man in a blue shirt is sitting in the dock."}, {"heading": "7.3 Sample Generation", "text": "A common test of a GAN's ability to generate realistic samples covering the original data space is to learn a simple model from the samples from the GAN itself. Recognizing the pitfalls of such quantitative evaluations [31], we can do this for text GANs by generating a large set of sampled sentences and training a simple language model across the generations. For these experiments, we generate 100k samples from (i) ARAE-GAN, (ii) an AU, (iii) an RNN-LM trained on the same data, and (iv) the real training set. To enable an \"example\" from an AU, we match a multivariate Gaussian to the code space (the training data) and generate code vectors from that Gaussian and decode back into the set space. All models are of the same size to allow a fair comparison. Samples from the models will then be reconstructed in the data space (the training data data data data model) We will train a large language model based on the xARN-2, and the NN-filled standard."}, {"heading": "7.4 Interpolation and Vector Arithmetic", "text": "A widely observed property of GANs (and VAEs) is that the Gaussian before p (z) elicits the ability to smooth interpolation between results by exploiting the structure of latent space. While language models provide a better estimate of the underlying probability space, constructing this style of interpolation would require a combinatorial search that makes this a useful feature of the text. We experiment with this property by making two points z0 and z1 from p (z) and constructing intermediate points z1 + (1) z0. For each, we generate the argmax output x-x. Samples are shown in Figure 4 for text and in Figure 2."}, {"heading": "8 Conclusion", "text": "We present hostile autoencoders as a simple approach to training an autoencoder for discrete structures together with a code-space generative adversarial network. The model learns an improved autoencoder, as demonstrated by semi-supervised experiments and analysis of the manifold structure of text and images. It also learns a useful generative model for text that has a robust latent space, as demonstrated by natural interpolations and vector arithmetic. However, we note that (as often observed in the training of GANs) our model seemed to be quite sensitive to hyperparameters. Finally, although many useful models for text generation already exist, text GANs offer a qualitatively different approach that is influenced by the underlying latent variable structure. We imagine that such a framework could be extended to a conditional setting, combined with other existing decoding schemes, or could be used to provide a more interpretable language model."}, {"heading": "Acknowledgment", "text": "We thank Sam Wiseman, Kyunghyun Cho, Sam Bowman, Joan Bruna, Yacine Jernite, Mart\u00edn Arjovsky, Mikael Henaff and Michael Mathieu for fruitful discussions. Yoon Kim is supported by a SYSTRAN research award and we thank NVIDIA Corporation for donating a Titan X Pascal GPU used for this research."}, {"heading": "Appendix: Experiments details", "text": "This year it has come to the point where you are able to put yourself at the top, in the way that you put yourself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}], "references": [{"title": "Began: Boundary equilibrium generative adversarial networks", "author": ["David Berthelot", "Tom Schumm", "Luke Metz"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Categorical Reparameterization with Gumbel-Softmax", "author": ["Samuel R. Bowman", "Luke Vilnis", "Andrew M. Dai Oriol Vinyal", "Rafal Jozefowicz", "Samy Bengio"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Maximum-Likelihood Augment Discrete Generative Adversarial Networks", "author": ["Tong Che", "Yanran Li", "Ruixiang Zhang", "R Devon Hjelm", "Wenjie Li", "Yangqui Song", "Yoshua Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Variational Lossy Autoencoder", "author": ["Xi Chen", "Diederik P. Kingma", "Tim Salimans", "Yan Duan", "Prafulla Dhariwal", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "In Proceedings of ICLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Likelihood Ratio Gradient Estimation: An Overview", "author": ["Peter Glynn"], "venue": "In Proceedings of Winter Simulation Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Improved Training of Wasserstein GANs", "author": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Aaron Courville Vincent Dumoulin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen"], "venue": "In Proceedings of NAACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Athul Paul Jacob", "author": ["R Devon Hjelm"], "venue": "Tong Che, Kyunghyun Cho, and Yoshua Bengio. Boundary-Seeking Generative Adversarial Networks. arXiv:1702.08431", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Categorical Reparameterization with Gumbel-Softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "venue": "In Proceedings of ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Improving Variational Inference with Autoregressive Flow", "author": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Auto-Encoding Variational Bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "In Proceedings of ICLR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "GANs for Sequences of Discrete Elements with the Gumbel-Softmax Distribution", "author": ["Matt Kusner", "Jose Miguel Hernandez-Lobato"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Adversarial Learning for Neural Dialogue Generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "S\u00e9bastien Jean", "Alan Ritter", "Dan Jurafsky"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "author": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": "In Proceedings of ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Adversarial Variational Bayes", "author": ["Lars Mescheder", "Sebastian Nowozin", "Andreas Geiger"], "venue": "Unifying Variational Autoencoders and Generative Adversarial Networks", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Scott Wen tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "author": ["Ofir Press", "Amir Bar", "Ben Bogin", "Jonathan Berant", "Lior Wolf"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In Proceedings of ICLR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Variational Inference with Normalizing Flows", "author": ["Danilo J. Rezende", "Shakir Mohamed"], "venue": "In Proceedings of ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In Proceedings of ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "author": ["Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "Aaron van den Oord", "Matthias Bethge"], "venue": "In Proceedings of ICLR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of ICML,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Simple Statistical Gradient-following Algorithms for Connectionist Reinforcement Learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1992}, {"title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions", "author": ["Zichao Yang", "Zhiting Hu", "Ruslan Salakhutdinov", "Taylor Berg-Kirkpatrick"], "venue": "In Proceedings of ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": "In Proceedings of AAAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Recent work on generative adversarial networks (GANs) [9] and other deep latent variable models has shown significant progress in learning smooth latent variable representations of complex, highdimensional continuous data such as images [1, 2, 25, 37].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "Recent work on generative adversarial networks (GANs) [9] and other deep latent variable models has shown significant progress in learning smooth latent variable representations of complex, highdimensional continuous data such as images [1, 2, 25, 37].", "startOffset": 237, "endOffset": 251}, {"referenceID": 21, "context": "Recent work on generative adversarial networks (GANs) [9] and other deep latent variable models has shown significant progress in learning smooth latent variable representations of complex, highdimensional continuous data such as images [1, 2, 25, 37].", "startOffset": 237, "endOffset": 251}, {"referenceID": 3, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 10, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 30, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 15, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "A similar property was observed in image GANs [25] and word representations [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "A similar property was observed in image GANs [25] and word representations [23].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "Policy gradient methods are a natural way to deal with the resulting non-differentiable generator objective when training directly in discrete space [8, 34].", "startOffset": 149, "endOffset": 156}, {"referenceID": 28, "context": "Policy gradient methods are a natural way to deal with the resulting non-differentiable generator objective when training directly in discrete space [8, 34].", "startOffset": 149, "endOffset": 156}, {"referenceID": 3, "context": "language modeling) objective [5, 36, 18].", "startOffset": 29, "endOffset": 40}, {"referenceID": 30, "context": "language modeling) objective [5, 36, 18].", "startOffset": 29, "endOffset": 40}, {"referenceID": 16, "context": "language modeling) objective [5, 36, 18].", "startOffset": 29, "endOffset": 40}, {"referenceID": 12, "context": "Another direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick [14, 19]\u2014while initial experiments were encouraging on a synthetic task [17], scaling them to work on natural language is a challenging open problem.", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Another direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick [14, 19]\u2014while initial experiments were encouraging on a synthetic task [17], scaling them to work on natural language is a challenging open problem.", "startOffset": 119, "endOffset": 127}, {"referenceID": 15, "context": "Another direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick [14, 19]\u2014while initial experiments were encouraging on a synthetic task [17], scaling them to work on natural language is a challenging open problem.", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "There has also been a flurry of recent, related approaches that work directly with the soft outputs from a generator [10, 28, 30, 24].", "startOffset": 117, "endOffset": 133}, {"referenceID": 25, "context": "There has also been a flurry of recent, related approaches that work directly with the soft outputs from a generator [10, 28, 30, 24].", "startOffset": 117, "endOffset": 133}, {"referenceID": 20, "context": "There has also been a flurry of recent, related approaches that work directly with the soft outputs from a generator [10, 28, 30, 24].", "startOffset": 117, "endOffset": 133}, {"referenceID": 25, "context": "[30] train with adversarial loss for unaligned style transfer between text by having the discriminator act on the RNN hidden states and using the soft outputs at each step as input to an RNN generator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "One way\u2014among others\u2014to regularize the code space is through having an explicit prior on the code space and using a variational approximation to the posterior, leading to a family of models called variational autoencoders (VAE) [16, 27].", "startOffset": 228, "endOffset": 236}, {"referenceID": 23, "context": "One way\u2014among others\u2014to regularize the code space is through having an explicit prior on the code space and using a variational approximation to the posterior, leading to a family of models called variational autoencoders (VAE) [16, 27].", "startOffset": 228, "endOffset": 236}, {"referenceID": 2, "context": "Unfortunately VAEs for text can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing [4], the decoder simply becomes a language model and ignores the latent code (although there has been some recent successes with convolutional models [29, 35]).", "startOffset": 169, "endOffset": 172}, {"referenceID": 24, "context": "Unfortunately VAEs for text can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing [4], the decoder simply becomes a language model and ignores the latent code (although there has been some recent successes with convolutional models [29, 35]).", "startOffset": 319, "endOffset": 327}, {"referenceID": 29, "context": "Unfortunately VAEs for text can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing [4], the decoder simply becomes a language model and ignores the latent code (although there has been some recent successes with convolutional models [29, 35]).", "startOffset": 319, "endOffset": 327}, {"referenceID": 22, "context": "the prior/posterior more flexible through explicit parameterization [26, 15, 6].", "startOffset": 68, "endOffset": 79}, {"referenceID": 13, "context": "the prior/posterior more flexible through explicit parameterization [26, 15, 6].", "startOffset": 68, "endOffset": 79}, {"referenceID": 4, "context": "the prior/posterior more flexible through explicit parameterization [26, 15, 6].", "startOffset": 68, "endOffset": 79}, {"referenceID": 18, "context": "Nonetheless, this view (which has been observed by various researchers [32, 22, 20]) provides an interesting connection between VAEs and GANs.", "startOffset": 71, "endOffset": 83}, {"referenceID": 7, "context": "Generative Adversarial Networks (GANs) are a class of parameterized implicit generative models [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "In our experiments we use an LSTM architecture [13] for both the encoder/decoder, and train with teacher-forcing.", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "We consider two different settings for testing the ARAE: (1) images, utilizing the binarized version of MNIST, and (2) text, using the Stanford Natural Language Inference corpus [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "We note that unlike denoising autoencoders which require a domain-specific noising function [11, 33], the ARAE is not explicitly trained to denoise an input, but learns to do so as a byproduct of adversarial regularization.", "startOffset": 92, "endOffset": 100}, {"referenceID": 27, "context": "We note that unlike denoising autoencoders which require a domain-specific noising function [11, 33], the ARAE is not explicitly trained to denoise an input, but learns to do so as a byproduct of adversarial regularization.", "startOffset": 92, "endOffset": 100}, {"referenceID": 5, "context": "As a baseline we use an AE trained on the additional data, similar to the setting explored in [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "As observed by Dai and Le [7], training on unlabeled data with an AE objective improves upon a model just trained on labeled data.", "startOffset": 26, "endOffset": 29}, {"referenceID": 26, "context": "Acknowledging the pitfalls of such quantitative evaluations [31], for text GANs we can do this by producing a large set of sampled sentences, and training a simple language model over the generations.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Another intriguing property of image GANs is the ability to move in the latent space via offset vectors (similar to the case with word vectors [23]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "[25] observe that when the mean latent vector for \u201cmen with glasses\u201d is subtracted from the mean latent vector for \u201cmen without glasses\u201d and applied to an image of a \u201cwoman without glasses\u201d, the resulting image is that of a \u201cwoman with glasses\u201d.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Generative adversarial networks are an effective approach for learning rich latent representations of continuous data, but have proven difficult to apply directly to discrete structured data, such as text sequences or discretized images. Ideally we could encode discrete structures in a continuous code space to avoid this problem, but it is difficult to learn an appropriate general-purpose encoder. In this work, we consider a simple approach for handling these two challenges jointly, employing a discrete structure autoencoder with a code space regularized by generative adversarial training. The model learns a smooth regularized code space while still being able to model the underlying data, and can be used as a discrete GAN with the ability to generate coherent discrete outputs from continuous samples. We demonstrate empirically how key properties of the data are captured in the model\u2019s latent space, and evaluate the model itself on the tasks of discrete image generation, text generation, and semi-supervised learning.", "creator": "LaTeX with hyperref package"}}}