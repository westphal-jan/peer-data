{"id": "1706.04313", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Teaching Compositionality to CNNs", "abstract": "Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via application-specific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.", "histories": [["v1", "Wed, 14 Jun 2017 04:34:59 GMT  (9418kb,D)", "http://arxiv.org/abs/1706.04313v1", "Preprint appearing in CVPR 2017"]], "COMMENTS": "Preprint appearing in CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["austin stone", "huayan wang", "michael stark", "yi liu", "d scott phoenix", "dileep george"], "accepted": false, "id": "1706.04313"}, "pdf": {"name": "1706.04313.pdf", "metadata": {"source": "CRF", "title": "Teaching Compositionality to CNNs\u2217", "authors": ["Austin Stone", "Yi Liu", "Huayan Wang", "D. Scott Phoenix", "Michael Stark", "Dileep George"], "emails": ["dileep}@vicarious.com"], "sections": [{"heading": "1. Introduction", "text": "Convolutionary neural networks (CNNs) have performed remarkably in many computer vision tasks [21, 20, 42, 37, 35], including image classification [20], object class capture [41, 12], instance segmentation [13], caption [18, 44], and scene understanding [6]. Their success is typically attributed to two factors; they have sufficient capacity to effectively utilize the increasing amount of image training data available today, while managing the number of free parameters through the use of inductive biases from neuroscience. Specifically, the overlay of locally connected filter and collecting layers from visual layers [15] is similar to the visual cortex, which has intertwined simple cells that have localized receptive fields, and complex cells that have broader receptive fields and greater local inventory."}, {"heading": "2. Related work", "text": "Our work mainly refers to three major lines of research: compositional models, inductive biases, and the role of context in visual recognition. Compositional models have existed since the early days of computer vision [24] and appear mainly in two different contexts. The first variant focuses on creating hierarchical representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45]. The second variant designs neural network-based representations in the form of recursive neural networks [38], which imply hierarchical priorities in Deep Boltzman Machines, or the introduction of parametric network units, which are themselves compositional units. The basis for our work is an idea of compositional compositivity (Sect. 3.1) that differs from all of these approaches."}, {"heading": "3. Teaching compositionality to CNNs", "text": "This section describes our approach to encouraging CNNs to learn compositional representation, from introducing our concept of compositivity (Section 3.1) to describing the network architecture (Section 3.2) and the training process (Section 3.3) to the technical details of our implementation (Section 3.4)."}, {"heading": "3.1. Compositionality notion", "text": "The aim of our concept of compositivity is to promote the representation of a part of an image in such a way that it resembles the corresponding part of the representation of that image. Formally, X should be an image, m a binary mask identifying a part of X (i.e., m is a tensor of the same shape as X with 1s indicating partial affiliation), \u03c6 a mapping of an image to any attribute level of a CNN and p the projection operator on the attribute map represented by \u03c6. We define \u03c6 as a compositional iff, the following applies: \u03c6 (m \u00b7 X) = p (m) \u00b7 \u03c6 (X). (1) Here, the \u00b7 operator represents an elementary multiplication. The projection operator p scans the object mask down to the size of the output of \u03c6. For example, if \u03c6 (X) is the activation of a folding layer with the size (h, w, c) (the first two dimensions c are spatial and the number of the object is applied to the number of the object), then we apply the number of the mask to the other parts of the object."}, {"heading": "3.2. Enhanced network architecture", "text": "To encourage a network to fulfill the property of compositivity of Eq. (1) (Fig. 3.1), we design an advanced architecture and corresponding objective function. Note that this improvement is not destructive and leaves the original network completely intact; only virtual copies of the original network are created, Fig. 2.If there is only one object in the input image, compositionality in class takes the form that the activations within the region of that object remain unchangeable, regardless of the background on which the object appears. Furthermore, with multiple objects we explicitly ensure that the activations of each object remain the same as if that object were presented in isolation (i.e. activations should be invariant on the other objects within the respective object mask).To implement this notion, we create K + 1 weight distribution of CNNs, where K is the number of objects shown in the scene."}, {"heading": "3.3. Training procedure", "text": "To promote correct discrimination, we add separate discriminatory loss selections for both the K-masked and the unmasked CNN terms Lmk and Lu. Their relative contributions are controlled by the hyperparameter \u03b3 [0, 1] to obtain activation choices for CNN. (1 \u2212 \u03b3) Lu. (2) Compositional losses. To promote composiality, we add K-N terms that create dependencies between the reactions of the corresponding layers of masked and unmasked CNNs, to activate Ld = 1K (KN-Lmk) + (1 \u2212 \u03b3) Lu. (2) Compositional losses. To promote composiality, we add K-N terms that create dependencies between the corresponding layers of masked and unmasked CNNs. Specifically, on all layers on which an object mask is applied, we add the terms 2-masked between the CNN activations and the CNN masked."}, {"heading": "3.4. Implementation details", "text": "Our experiments (sparkling wine 4) use the following network architectures: MS-COCO-sub (sparkling wine 4.4): Conv1-Conv3 (224 x 224 x 64), Pool1, Conv4-Conv6 (128 x 128 x 128), Pool2, Conv7-Conv9 (64 x 64 x 256), Pool3, Conv10-Conv12 (32 x 32 x 512), Pool4, Conv7-Conv9 (131072 x 20), CNN-Single 3D (sparkling wine 4.3): Conv1-Conv3 (128 x 128 x 64), Pool1, Conv4-Conv6 (64 x 64 x 128), Pool2, Conv7-Conv9 (32 x 256), Pool3, Conv10Conv12 (16 x 512), Conv3-Conv4, fc1 (32768 x 14)."}, {"heading": "4. Experiments", "text": "In this section, we provide a detailed experimental assessment of our approach to imparting compositionality to CNNs, highlighting its ability to improve performance over standard CNN training on both synthetic (Section 4.3) and real images (MS-COCO [23], Section 4.4), focusing on an in-depth analysis of the contributions of various components of our composition lens, as well as quantifying the impact of the object context on performance."}, {"heading": "4.1. Datasets and metrics", "text": "This year, the time has come for us to be able to get to grips with the problems that have been mentioned in order to get to grips with them."}, {"heading": "4.2. Methods", "text": "In this section we evaluate the following baselines and variations of our composition techniques (see Section 3.3). For a clean comparison, we always train all networks from the ground up (i.e. we do not use any pre-training of any kind). COMP-FULL. Our main architecture, in which m \u2032 k is chosen so that it is equal to a block of all 1s, but with the positions of objects other than the cheapest object on 0s. COMP-OBJ-ONLY. Like COMP-FULL, but with m \u2032 k equal to mk (this punishes any shifting of activations within the object region, but does not discourage from background activations). COMP-NO-MASK. Like COMP-FULL, except that the first 20 classes in the original MS-COCO arrangement w / o person. 3http: / / mscoco.org / dataset / # detections-evalmasked CNNs mk do not apply to their object, but to the original MS-COW-Person / o-COCO arrangement."}, {"heading": "4.3. Diagnostic experiments on synthetic data", "text": "We start by comparing the performance of different variants of our compositional target and the corresponding baseline (Fig. 4.2) in a diagnostic environment on synthetic data. In order to assess both the best case performance and the convergence behaviour, we record the test performance compared to the training periods in Fig. 3a to 3f. The respective best performance per curve is given in brackets in plot legends. Fig. 5 and 6 provide qualitative results. In Fig. 3, we observe that all variants of compositional CNNs (blue curves) consistently perform better than the base forms (red curves), both per epoch and in terms of the best case performance. Our complete model, COMP-FULL, shows overall the best (blue-solid) performance. It exceeds the best baseline by 17.1% (3D multi, Fig. 3d) and 3a."}, {"heading": "4.4. Experiments on real-world data (MS-COCO)", "text": "We proceed to evaluate our best-performing method COMP-FULL using the real context of MS-COCO (Section 4.1). We compare the same basics as before, plus two baselines with convergence behavior); for all baselines, we look at the best-performing model across all epochs. Fig. 4 gives details w.r.t. Individual object categories (4a), amount of training data (4b), volume of training data (4b), volume of object examinations (4c), and context, which we consider to be the best-performing model across all epochs. 4, there are single object categories (4b), volume of training data (4b), volume of object examinations (4d). MS-COCO-sub-sub-3g, COCO-sub-sub."}, {"heading": "5. Conclusion", "text": "We have introduced an improved CNN architecture and a novel loss function based on the inductive bias of compositionality, which follows the intuition that the representation of a part of an image should be similar to the corresponding part of the representation of that image, and is implemented as additional layers and connections of an existing CNN. Our experiments suggest that the bias of compositionality helps to learn representations that become more generalized when networks are trained from scratch, and improves performance in object recognition tasks on both synthetic and real data. Obvious next steps include applying it to tasks that explicitly require spatial localization, such as image evaluation, and combining it with pre-trained networks. We thank John Bauer and Robert Hafner for supporting the experiment infrastructure."}], "references": [{"title": "Software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In ICCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Context models and out-of-context objects", "author": ["M.J. Choi", "A. Torralba", "A.S. Willsky"], "venue": "Pattern Recognition Letters,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Contextual object detection using set-based classification", "author": ["R.G. Cinbis", "S. Sclarof"], "venue": "In ECCV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "An empirical study of context in object detection", "author": ["S.K. Divvala", "D. Hoiem", "J.H. Hays", "A.A. Efros", "M. Hebert"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["S.M.A. Eslami", "N. Heess", "T. Weber", "Y. Tassa", "D. Szepesvari", "K. Kavukcuoglu", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Recognition using visual phrases", "author": ["A. Farhadi", "M.A. Sadeghi"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Towards scalable representations of object categories: Learning a hierarchy of parts", "author": ["S. Fidler", "A. Leonardis"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Context based object categorization: A critical survey", "author": ["C. Galleguillos", "S. Belongie"], "venue": "CVIU,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Object-centric representation learning from unlabeled videos", "author": ["R. Gao", "D. Jayaraman", "K. Grauman"], "venue": "In ACCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Exploring person context and local scene context for object detection", "author": ["S. Gupta", "B. Hariharan", "J. Malik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbelaez", "R. Girshick", "J. Malik"], "venue": "In ECCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbelez", "R. Girshick", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Learning with side information through modality hallucination", "author": ["J. Hoffman", "S. Gupta", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1962}, {"title": "Cortical feedback improves discrimination between figure and background by v1", "author": ["J.M. Hup", "A. James", "B.R. Payne", "S.G. Lomber", "P. Girard", "J. Bullier"], "venue": "v2 and v3 neurons. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Slow and steady feature analysis: Higher order temporal coherence in video", "author": ["D. Jayaraman", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Lei Ba"], "venue": "ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J.H.P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Vision: A Computational Investigation into the Human Representation and Processing of Visual Information", "author": ["D. Marr"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1982}, {"title": "Using the forest to see the trees: A graphical model relating features, objects, and scenes", "author": ["K. Murphy", "A. Torralba", "W.T. Freeman"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "The role of context in object recognition", "author": ["A. Oliva", "A. Torralba"], "venue": "TRENDS in Cognitive Sciences,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Learning the compositional nature of visual object categories for recognition", "author": ["B. Ommer", "J.M. Buhmann"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Is object localization for free? weakly-supervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Shared parts for deformable partbased models", "author": ["P. Ott", "M. Everingham"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Occlusion patterns for object class detection", "author": ["B. Pepik", "M. Stark", "P. Gehler", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y.-L. Park", "A. Gupta"], "venue": "In ECCV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Texture segregation causes early figure enhancement and later ground suppression in areas v1 and v4 ofvisual cortex", "author": ["J. Poort", "M.W. Self", "B. v. Vugt", "H. Malkki", "P.R. Roelfsema"], "venue": "Cerebral Cortex,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Object recognition by scene alignment", "author": ["B.C. Russell", "A. Torralba", "C. Liu", "R. Fergus", "W.T. Freeman"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning with hierarchical-deep models", "author": ["R. Salakhutdinov", "J.B. Tenenbaum", "A. Torralba"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Learning and-or templates for object recognition and detection", "author": ["Z. Si", "S.-c. Zhu"], "venue": "PAMI, 35:2189\u20132205,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Very deep convolutional networks for large scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C.-Y. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "In ICML,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR-WS,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "In NIPS,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Towards deep compositional networks. arxiv, 2016", "author": ["D. Tabernik", "M. Kristan", "J.L. Wyatt", "A. Leonardis"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Semantic part segmentation using compositional model combining shape and appearance", "author": ["J. Wang", "A. Yuille"], "venue": "In CVPR,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Learning and-or models to represent context and occlusion for car detection and viewpoint estimation", "author": ["T. Wu", "B. Li", "S. Zhu"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Complexity of representation and inference in compositional models with part sharing", "author": ["A. Yuille", "R. Mottaghi"], "venue": "JMLR, 2016", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Part and appearance sharing: Recursive compositional models for multi-view multi-object detection", "author": ["L. Zhu", "Y. Chen", "A. Torralba", "W. Freeman", "A. Yuille"], "venue": "In CVPR,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Latent hierarchical structural learning for object detection", "author": ["L. Zhu", "Y. Chen", "A. Yuille", "W. Freeman"], "venue": "In CVPR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 19, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 36, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 34, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 19, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 153, "endOffset": 157}, {"referenceID": 40, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 182, "endOffset": 190}, {"referenceID": 11, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 182, "endOffset": 190}, {"referenceID": 12, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 214, "endOffset": 218}, {"referenceID": 17, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 237, "endOffset": 245}, {"referenceID": 42, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 237, "endOffset": 245}, {"referenceID": 5, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 271, "endOffset": 274}, {"referenceID": 14, "context": "Specifically, the interleaving of locally connected filter and pooling layers [15] bears similarity to the visual cortex\u2019s interleaving of simple cells, which have localized receptive fields, and complex cells, which have wider receptive fields and greater local invariance.", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth in-", "startOffset": 63, "endOffset": 74}, {"referenceID": 9, "context": "Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth in-", "startOffset": 63, "endOffset": 74}, {"referenceID": 16, "context": "Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth in-", "startOffset": 63, "endOffset": 74}, {"referenceID": 36, "context": "Figure 1: For a standard CNN (VGG, [37]), the presence of a nearby object (cup) greatly affects the activations in the region of an object of interest (airplane).", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "formation [14], and using physical interaction with the environment [31] to bias representations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "formation [14], and using physical interaction with the environment [31] to bias representations.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "It is also in line with findings from neuroscience that suggest separate processing of figure and ground regions in the visual cortex [16, 32].", "startOffset": 134, "endOffset": 142}, {"referenceID": 31, "context": "It is also in line with findings from neuroscience that suggest separate processing of figure and ground regions in the visual cortex [16, 32].", "startOffset": 134, "endOffset": 142}, {"referenceID": 36, "context": "1 visualizes the difference in activations between a CNN trained without (VGG [37]) and with our compositionality objective1).", "startOffset": 78, "endOffset": 82}, {"referenceID": 33, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 43, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 41, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 45, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 23, "context": "Compositional models have existed since the early days of computer vision [24] and have appeared mainly in two different varieties.", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 26, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 46, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 45, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 47, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 157, "endOffset": 165}, {"referenceID": 28, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 157, "endOffset": 165}, {"referenceID": 35, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 201, "endOffset": 209}, {"referenceID": 43, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 201, "endOffset": 209}, {"referenceID": 37, "context": "The second flavor designs neural network-based representations in the form of recursive neural networks [38], imposing hierarchical priors on Deep Boltzman Machines [34], or introducing parametric network units that are themselves compositional [43].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "The second flavor designs neural network-based representations in the form of recursive neural networks [38], imposing hierarchical priors on Deep Boltzman Machines [34], or introducing parametric network units that are themselves compositional [43].", "startOffset": 165, "endOffset": 169}, {"referenceID": 41, "context": "The second flavor designs neural network-based representations in the form of recursive neural networks [38], imposing hierarchical priors on Deep Boltzman Machines [34], or introducing parametric network units that are themselves compositional [43].", "startOffset": 245, "endOffset": 249}, {"referenceID": 27, "context": "Recent work [28] constrains CNN activations to lie within object masks in the context of weakly-supervised localization.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 100, "endOffset": 107}, {"referenceID": 16, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 100, "endOffset": 107}, {"referenceID": 9, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 195, "endOffset": 199}, {"referenceID": 30, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 238, "endOffset": 242}, {"referenceID": 8, "context": "It is well known that context plays a major role in visual recognition, both in human and artificial vision systems [9, 26, 5].", "startOffset": 116, "endOffset": 126}, {"referenceID": 25, "context": "It is well known that context plays a major role in visual recognition, both in human and artificial vision systems [9, 26, 5].", "startOffset": 116, "endOffset": 126}, {"referenceID": 4, "context": "It is well known that context plays a major role in visual recognition, both in human and artificial vision systems [9, 26, 5].", "startOffset": 116, "endOffset": 126}, {"referenceID": 24, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 196, "endOffset": 203}, {"referenceID": 3, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 196, "endOffset": 203}, {"referenceID": 2, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 238, "endOffset": 241}, {"referenceID": 6, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 29, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 10, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 44, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 32, "context": "At the extreme, object classes can be successfully recognized even in the absence of local information by relying exclusively on scene context [33].", "startOffset": 143, "endOffset": 147}, {"referenceID": 22, "context": "In the following, we use object masks (as provided by standard data sets such as MS-COCO [23]) as the basis for compositionality.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "Their relative contributions are controlled by the hyperparameter \u03b3 \u2208 [0, 1], to yield", "startOffset": 70, "endOffset": 76}, {"referenceID": 18, "context": "Since Lc is of a standard form, we can optimize it like any CNN via SGD (specifically, using the ADAM optimizer [19] and Tensorflow [1]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Since Lc is of a standard form, we can optimize it like any CNN via SGD (specifically, using the ADAM optimizer [19] and Tensorflow [1]).", "startOffset": 132, "endOffset": 135}, {"referenceID": 22, "context": "3) and real images (MS-COCO [23], Sect.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "We create two variants of the popular MNIST dataset [22], in analogy to the two aforementioned 3D object datasets.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "MS-COCO [23] constitutes a move away from \u201ciconic\u201d views of objects towards a dataset in which objects frequently occur in their respective natural", "startOffset": 8, "endOffset": 12}, {"referenceID": 39, "context": "Like BASELINE, but with dropout [40] and l2-regularization.", "startOffset": 32, "endOffset": 36}, {"referenceID": 36, "context": "256 VGG [37] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 39, "context": "We compare to the same baselines as before, plus two baselines with dropout [40] and l2-regularization (see Sect.", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "Second, it also leads to better localization when backtracing classification decisions to the input images, which we implement by applying guided backpropagation [39] (Fig.", "startOffset": 162, "endOffset": 166}, {"referenceID": 36, "context": "COMP-FULL outperforms both BASELINEAUG and VGG [37] by considerable margins.", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "To our knowledge, only [28] reports classification (not detection) performance on MS-COCO, achieving 62.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "8% mAP on the full set of 80 classes using fixed lower-layer weights from ImageNet pre-training [20] and an elaborate multi-scale, sliding-window network architecture.", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "We believe this to be an encouraging result that is complementary to the gains reported by [28] and leave the combination of both as a promising avenue for future work.", "startOffset": 91, "endOffset": 95}, {"referenceID": 36, "context": "VGG [37]", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "VGG [37]", "startOffset": 4, "endOffset": 8}, {"referenceID": 38, "context": "Figure 6: Backtracing classification activations (MS-COCO categories, denoted by column labels) to test images using guided backpropagation [39].", "startOffset": 140, "endOffset": 144}], "year": 2017, "abstractText": "Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via applicationspecific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.", "creator": "LaTeX with hyperref package"}}}