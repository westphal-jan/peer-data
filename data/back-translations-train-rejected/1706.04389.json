{"id": "1706.04389", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Fine-grained human evaluation of neural versus phrase-based machine translation", "abstract": "We compare three approaches to statistical machine translation (pure phrase-based, factored phrase-based and neural) by performing a fine-grained manual evaluation via error annotation of the systems' outputs. The error types in our annotation are compliant with the multidimensional quality metrics (MQM), and the annotation is performed by two annotators. Inter-annotator agreement is high for such a task, and results show that the best performing system (neural) reduces the errors produced by the worst system (phrase-based) by 54%.", "histories": [["v1", "Wed, 14 Jun 2017 09:59:47 GMT  (172kb,D)", "http://arxiv.org/abs/1706.04389v1", "12 pages, 2 figures, The Prague Bulletin of Mathematical Linguistics"]], "COMMENTS": "12 pages, 2 figures, The Prague Bulletin of Mathematical Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["filip klubi\\v{c}ka", "antonio toral", "v\\'ictor m s\\'anchez-cartagena"], "accepted": false, "id": "1706.04389"}, "pdf": {"name": "1706.04389.pdf", "metadata": {"source": "CRF", "title": "Fine-grained human evaluation of neural versus phrase-based machine translation", "authors": ["Filip Klubi\u010dka", "Antonio Toral", "V\u00edctor M. S\u00e1nchez-Cartagena"], "emails": ["fklubick@ffzg.hr"], "sections": [{"heading": null, "text": "The error types in our note are consistent with Multidimensional Quality Metrics (MQM), and the note is performed by two annotators. Annotator concordance is high for such a task, and the results show that the most powerful system (neural) reduces the errors of the worst system (phrase-based) by 54%."}, {"heading": "1. Introduction", "text": "The idea behind it is that it is a way in which people are able to put themselves into the world, in which they are able to understand the world and understand what they are doing. (...) The idea behind it is that people in the world put themselves into the world they live in. (...) The idea behind it is to understand the people in the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. (...) The idea behind it is to put people into the world. \"(...) The idea behind it is to put people into the world."}, {"heading": "2. MT Systems", "text": "This section describes the MT systems and the data sets used in our experiments. We built PBMT, factored PBMT and NMT systems.The 3 systems were trained on the same parallel data. We looked at a series of publicly available English-Croatian parallel corpora, consisting of the DGT Translation Memory 2, HrEnWaC3, JRC Acquis 4, OpenSubtitles 2013, SETimes and Ted Talks. We linked all these corpora and performed cross-entropy-based data selection (Moore and Lewis, 2010), using the Development Goal / HrEnWaC3, JRC Acquis 4, OpenSubtitles 2013, SETimes and Ted Talks. For this purpose, we used the concatenation of the hrWaC corpus (Ljube\u0161i\u0107 and Klubi\u010dka, 2014) and the target side of the LETTS Model 4lt.As a development kit, we used the first 1,000 sets of the MT in the English hierarchy."}, {"heading": "2.1. Evaluation", "text": "As the table shows, the use of factor models leads to a significant improvement in pure PBMT (6% relative to BLEU). NMT, for its part, enables us to achieve another remarkable improvement; 14% relative to BLEU compared to the factored PBMT system and 21% compared to the original PBMT system."}, {"heading": "3. Error analysis", "text": "In this section we will report on the motivation for performing manual error analysis, describe the framework and general annotation process and present the results. The fact that Croatian is rich in flexibility, has a rather free word order and other similar phenomena that English does not have leads to specific translation problems. We were equally motivated to see how an NMT system would deal with the same problems. In fact, as shown in Section 2, the automatic evaluation for both systems shows a significant improvement compared to the PBMT system alone. However, as is the nature of automatic metrics, the automatic evaluation methods do not indicate whether any of the linguistic problems previously mentioned have been solved by the systems."}, {"heading": "3.1. Multidimensional Quality Metrics and the Slavic tagset", "text": "After looking at the different ways of performing the task of manual error assessment, the process begins with selecting the categories that we do not use in accordance with the suggested questions. However, we chose to use the MQM framework developed in the QTLaunchpad Project.8 This is a framework for describing and defining custom quality metrics. It provides a flexible vocabulary for quality issues and a mechanism for applying them to generate quality valuations.The main reason we chose the MQM framework was the flexibility of output types and their granularity - it gave us a reliable quality assessment methodology that allowed us to select and select the error markers we wish to use. MQM guidelines suggest a wide variety of markers for multiple markers.9 However, the full marking set is too comprehensive to be the core task for each of QTs."}, {"heading": "3.2. Annotation setup", "text": "To carry out the comments, we used translate510, a web-based tool that implements annotations of MT editions using hierarchical taxonomies, as in the case of MQM. We had two annotation specialists at our disposal, both of whom had previous experience with MQM and the same background - an MA in English Linguistics and Information Science. They were faced with the official annotation guidelines and the decision-making process 11. The annotation specialists commented on 100 random sentences from the one in section 2. These sentences were translated by all three MT systems, and the annotation specialists received the source text, a reference translation and the uncommented system output at the same time.All three translations were then commented on by our annotators (i.e. each system translated the same 100 sentences, each annotation specialist commented on the 300 translated sentences, resulting in a total of 600 annotateds.http / 10qotated / interlatated- www.2014- www.)."}, {"heading": "3.3. Inter-Annotator Agreement", "text": "Although carefully thought out and developed, MQMmetrics and manual MT evaluation are generally notorious for leading to low matches between annotators, as confirmed by the working group that has dealt with this issue, in particular Lommel et al. (2014b), who have worked specifically on MQM, and (Callison-Burch et al., 2007), who have studied several tasks. Therefore, it is important that we verify how well our annotators match the task at hand and whether this is consistent with other work done so far with MQM. Once the data has been commented, a match was observed at the sentence level, and a match between annotators was calculated using Cohen's Kappa metric (Cohen, 1960). Match was calculated on the annotations of each system separately, as well as on a concatenation of annotation notes, to see whether there is a difference in the overall match between the systems as well as the 0.1 overlap between the systems."}, {"heading": "3.4. Results of annotation", "text": "In fact, we are able to assert ourselves, we are able to put ourselves in a position, we are able to put ourselves in another world, we are able to put ourselves in another world, we are able to put ourselves in a different world, we are able to put ourselves in another world, we are able to put ourselves in another world, we are able to put ourselves in a position, we are able to put ourselves in, we are able to put ourselves in, we are in, we are in, we are in. \""}, {"heading": "4. Conclusion", "text": "The fine-grained manual evaluation conducted for the purpose of this research has provided answers to several questions, one of which has been the main driving force behind our development of the factor system: Is there any way to achieve better consistency in translation into Croatian? We can now confidently claim that factor models as a whole result of significantly fewer match errors than pure PBMTs. We can also confidently claim that NMT handles all types of matches better than pure PBMTs and factor errors, which confirms the results of other researchers \"NMT evaluations. Our system produces sentences with far fewer errors and a language that is more fluent and grammatical, which should be helpful when it comes to the task of post-processing. Furthermore, the error taxonomy developed for this research, while used only for the English-Croatian language direction, is applicable."}, {"heading": "Acknowledgements", "text": "We would like to thank Maja Popovi\u0107, who gave valuable advice on how to approach the commentary and evaluation, and Denis Kranj\u010di\u0107, who participated in the commentary task. Research that led to these results was funded by the Seventh Framework Programme of the European Union FP7 / 2007-2013 within the framework of the funding agreement PIAP-GA-2012-324414 (Abu-MaTran) and the Swiss National Fund 74Z0 _ 160501 (ReLDI)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We compare three approaches to statistical machine translation (pure phrase-based, factored phrase-based and neural) by performing a fine-grained manual evaluation via error annotation of the systems\u2019 outputs. The error types in our annotation are compliant with the multidimensional quality metrics (MQM), and the annotation is performed by two annotators. Inter-annotator agreement is high for such a task, and results show that the best performing system (neural) reduces the errors produced by the worst system (phrase-based) by 54%.", "creator": "XeLaTeX"}}}