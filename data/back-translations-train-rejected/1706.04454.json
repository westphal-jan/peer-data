{"id": "1706.04454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "abstract": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: a gradient based method appears to be first climbing uphill and then falling downhill between two points; whereas, in fact, they lie in the same basin.", "histories": [["v1", "Wed, 14 Jun 2017 12:50:00 GMT  (391kb,D)", "http://arxiv.org/abs/1706.04454v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["levent sagun", "utku evci", "v ugur guney", "yann dauphin", "leon bottou"], "accepted": false, "id": "1706.04454"}, "pdf": {"name": "1706.04454.pdf", "metadata": {"source": "CRF", "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "authors": ["Levent Sagun", "Utku Evci"], "emails": ["sagun@cims.nyu.edu", "ue225@nyu.edu", "ugurguney@gmail.com", "yann@dauphin.io", "leonb@fb.com"], "sections": [{"heading": "1 Introduction", "text": "In this thesis, we examine the geometry of the loss of the surface of supervised learning (1).D more expensive than the lens of its second order properties. To introduce the framework, we assume that we can use data in the form of input-label pairs, D = (xi, yi)} Ni = 1, where x-Rd and y-R are of a possibly unknown distribution, a model parameterized by w-RM; so that the number of examples N and the number of parameters of the system M is. It is also assumed that there is a predictor f (w, x). The supervised learning process aims to find f (w, x) that f-RM has the \"precise, non-negative loss function, which measures how close the predictor is to the true mark (f, x)."}, {"heading": "1.1 A historical overview", "text": "In fact, most of them are able to survive on their own, and they see themselves able to survive on their own."}, {"heading": "1.2 Overview of results", "text": "In an attempt to gain an understanding of some of the classical and modern problems described above, and hopefully gain further insights into the non-convex optimizations of high-dimensional spaces, we examine the loss function through their second-order properties through the spectrum of their Hessian matrix.The first observation is that Hessian is not slightly singular, but extreme in such a way that it has almost all its eigenvalues at or near zero, and these observations seem to hold even at random starting points of the space. Let's look at an artificial neural network with two layers (a total of 5K parameters) and descend by gradients. Figure 1 shows the full spectrum of Hessians at the random starting point of education and the last training point. Intuitively, this type of singularity should arise when (1) the data is essentially low dimensioned but live in a larger environment, and (2) the model is overparameterized."}, {"heading": "2 Generalized Gauss-Newton decomposition of the Hessian", "text": "To explore the spectrum, we will describe how the Hessian (w) can be divided into two significant matrices. (note, the loss function is given as the composition of two functions. (note). (note). (note.). (note.). (note.). (note). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note. (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note. (note.).). (note. (note. (note. (note.).). (note. (note. (note.). (note.). (note.). (note. (note.). (note.). (note. (note.). (note.). (note.). (note. (note. (.).). (note. (note.). (note.).). (note. (note.). (note.).). (note.). (note. (.). (note..). (). (note.). (..).). (.). (. (...).). (..). (.. (..)... ("}, {"heading": "2.1 The spectrum of the generalized Gauss-Newton matrix", "text": "In this section, we will show that the spectrum of the generalized Gauss-Newton matrix is essentially related to the values that can be theoretically characterized under certain conditions. Suppose that we can express the scaled gradient as g = Tx with the matrix T-M \u00b7 d, which depends only on the parameters w - which is the case for the spectrum of linear models. Then, we can assume that the examples are normalized in such a way that the entries of X are independent of the variance of the mean and unit. One of the first steps in investigating G is to understand its principle components. In particular, we would like to understand how the eigenvalues and eigenvectors of G correlate to the values of the order of magnitude in which we stand: = E (G) = 1N TTT."}, {"heading": "2.2 Applications and experiments on artificial data", "text": "This year it is so far that it will be able to use the mentionlcihsrcnlrVo rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "3 Larger scale experiments", "text": "In this section we show two experiments to answer the following questions: (1) How does the spectrum (and thus the geometry) change if we only increase the size of the system? (2) What does over-parameterization mean in the discussion about GD vs. SGD or large charge vs. small charge?"}, {"heading": "3.1 MNIST and growing the network size", "text": "We test the effect of enlarging the network with fixed data, architecture, and algorithm. We take 1K examples from the MNIST dataset and train all networks with the same step size until a sufficient hold condition is met. Then, we calculate the exact Hessian and draw its eigenvalues in Figures 5 and 6. Note that we use different methods to draw the left and right ranges: The left ranges are scaled to display the ratio of eigenvalues while the right ranges show the counts. The fact that this special selection produces consistent diagrams should not come as a surprise, as it confirms our previous suspicion that adding neurons in the system proportionally increases the mass, but the outliers on the right depend on the data, so their number should not change with the increasing dimensionality of the eigenedge parameter: Figure.0000 0.002 0.004 0.010 0.012 percent of eigenvalues."}, {"heading": "3.2 CIFAR10 and large/small batch comparison", "text": "A common way to create training profiles on a larger scale is to stop each epoch in order to obtain additional computing power to calculate the model in its current position. This becomes problematic when comparing training with different batch sizes, primarily because the larger batch model has fewer steps in a particular epoch. Remember that the total loss is averaged at a fixed point in the weight range, the empirical average of the gradients is an unbiased estimate of the expected gradient. Therefore, it is reasonable that the standards of the large batch coincide with those of the small batch. And, for a fair comparison, the same learning rate should be used for both gradients."}, {"heading": "4 Conclusion", "text": "We have shown that the level of singularity of the Hessian cannot be ignored. We are using the generalized Gauss-Newton decomposition of the Hessian to argue that the cluster of zero eigenvalues is to be expected in practical application, which allows us to divide the process of formation into two parts with initial rapid decay and ultimately slow progress. We are exploring the option that the inherent geometry at the lower end of the landscape, due to its extremely flat structure, could be responsible for the slow progress in the final phase. One of the most striking effects of flatness could be the interconnected structure of the solution space. We are wondering whether two predetermined solutions can be linked by a continuous solution path. This question has been examined in a recent paper: in Freeman and Bruna [2016] it was shown that for a hidden layer, reflected neural networks of the solution space are associated with the flatness of the solution space."}], "references": [{"title": "Random matrices and complexity of spin glasses", "author": ["Antonio Auffinger", "G\u00e9rard Ben Arous", "Ji\u0159\u00ed \u010cern\u1ef3"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Auffinger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auffinger et al\\.", "year": 2013}, {"title": "Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices", "author": ["Jinho Baik", "G\u00e9rard Ben Arous", "Sandrine P\u00e9ch\u00e9"], "venue": "The Annals of Probability,", "citeRegEx": "Baik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Baik et al\\.", "year": 2005}, {"title": "Energy landscapes for machine learning", "author": ["Andrew J Ballard", "Ritankar Das", "Stefano Martiniani", "Dhagash Mehta", "Levent Sagun", "Jacob D Stevenson", "David J Wales"], "venue": "Physical Chemistry Chemical Physics,", "citeRegEx": "Ballard et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ballard et al\\.", "year": 2017}, {"title": "On the principal components of sample covariance matrices", "author": ["Alex Bloemendal", "Antti Knowles", "Horng-Tzer Yau", "Jun Yin"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Bloemendal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bloemendal et al\\.", "year": 2016}, {"title": "Stochastic gradient learning in neural networks", "author": ["L\u00e9on Bottou"], "venue": "Proceedings of Neuro-N\u0131mes,", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Parallelization of a neural network learning algorithm on a hypercube. Hypercube and distributed computers", "author": ["J Bourrely"], "venue": "Elsiever Science Publishing,", "citeRegEx": "Bourrely.,? \\Q1989\\E", "shortCiteRegEx": "Bourrely.", "year": 1989}, {"title": "Entropy-sgd: Biasing gradient descent into wide valleys", "author": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "venue": "arXiv preprint arXiv:1611.01838,", "citeRegEx": "Chaudhari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chaudhari et al\\.", "year": 2016}, {"title": "Sharp minima can generalize for deep nets", "author": ["Laurent Dinh", "Razvan Pascanu", "Samy Bengio", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1703.04933,", "citeRegEx": "Dinh et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2017}, {"title": "Topology and geometry of deep rectified network optimization landscapes", "author": ["C Daniel Freeman", "Joan Bruna"], "venue": "arXiv preprint arXiv:1611.01540,", "citeRegEx": "Freeman and Bruna.,? \\Q2016\\E", "shortCiteRegEx": "Freeman and Bruna.", "year": 2016}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "venue": "arXiv preprint arXiv:1609.04836,", "citeRegEx": "Keskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Efficient backprop", "author": ["Y LeCun", "L Bottou", "GB ORR", "K-R M\u00fcller"], "venue": "Lecture notes in computer science,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Distribution of eigenvalues for some sets of random matrices", "author": ["Vladimir A Mar\u010denko", "Leonid Andreevich Pastur"], "venue": "Mathematics of the USSR-Sbornik,", "citeRegEx": "Mar\u010denko and Pastur.,? \\Q1967\\E", "shortCiteRegEx": "Mar\u010denko and Pastur.", "year": 1967}, {"title": "The landscape of empirical risk for non-convex losses", "author": ["Song Mei", "Yu Bai", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1607.06534,", "citeRegEx": "Mei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Numerical optimization, second edition", "author": ["Jorge Nocedal", "Stephen J Wright"], "venue": "Numerical optimization,", "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Gradient descent only converges to minimizers: Nonisolated critical points and invariant regions", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "Panageas and Piliouras.,? \\Q2016\\E", "shortCiteRegEx": "Panageas and Piliouras.", "year": 2016}, {"title": "Fast exact multiplication by the hessian", "author": ["Barak A Pearlmutter"], "venue": "Neural computation,", "citeRegEx": "Pearlmutter.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter.", "year": 1994}, {"title": "Explorations on high dimensional landscapes", "author": ["Levent Sagun", "V U\u011fur G\u00fcney", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "ICLR 2015 Workshop Contribution,", "citeRegEx": "Sagun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2014}, {"title": "Singularity of the hessian in deep learning", "author": ["Levent Sagun", "L\u00e9on Bottou", "Yann LeCun"], "venue": "arXiv preprint arXiv:1611.07476,", "citeRegEx": "Sagun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2016}, {"title": "No more pesky learning rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "ICML (3),", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. [2016]: Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers.", "startOffset": 103, "endOffset": 123}, {"referenceID": 14, "context": "And the behaviour of the two quantities may be drastically different (for a recent analysis on provable estimates see [Mei et al., 2016]).", "startOffset": 118, "endOffset": 136}, {"referenceID": 15, "context": "More involved algorithms, such as Newton type methods, make use of second order information [Nocedal and Wright, 2006].", "startOffset": 92, "endOffset": 118}, {"referenceID": 5, "context": "When the gradients are computationally expensive, one can alternatively use it\u2019s stochastic version (SGD) that replaces the above gradient with the gradient of averages of losses over subsets (such a subset will be called the mini-batch) of D (see [Bottou, 2010] for a classical reference).", "startOffset": 248, "endOffset": 262}, {"referenceID": 20, "context": "More involved optimal step size choices involve some kind of second order information that can be obtained from the Hessian of the loss function [Schaul et al., 2013].", "startOffset": 145, "endOffset": 166}, {"referenceID": 16, "context": "A relaxation of the above convergence to the case of non-isolated critical points can be found in [Panageas and Piliouras, 2016].", "startOffset": 98, "endOffset": 128}, {"referenceID": 4, "context": "When the gradients are computationally expensive, one can alternatively use it\u2019s stochastic version (SGD) that replaces the above gradient with the gradient of averages of losses over subsets (such a subset will be called the mini-batch) of D (see [Bottou, 2010] for a classical reference). The benefit of SGD on real-life time limits is obvious, and GD may be impractical for practical purposes in many problems. In any case, the stochastic gradient can be seen as an approximation to the true gradient, and hence it is important to understand how the two directions are related in the parameter space. Therefore, the discussion around the geometry of the loss surface can be enlightening in the comparison of the two algorithms: Does SGD locate solutions of a different nature than GD? Do they follow different paths? If so, which one is better in terms of generalization performance? For the second problem of expensive line-search, there are two classical solutions: using a small, constant step size, or scheduling the step size according to a certain rule. In practice, in the context of deep learning, the values for both approaches are determined heuristically, by trial and error. More involved optimal step size choices involve some kind of second order information that can be obtained from the Hessian of the loss function [Schaul et al., 2013]. From a computational point of view, obtaining the Hessian is extremely expensive, however obtaining some of its largest and smallest eigenvalues and eigenvectors are not that expensive. Is it enough to know only those eigenvalues and eigenvectors that are large in magnitude? How do they change through SGD? For the third problem, let\u2019s look at the Hessian a little closer. A critical point is defined by w such that ||\u2207L(w)|| = 0 and the nature of it can be determined by looking at the signs of its Hessian matrix. If all eigenvalues are positive the point is called a local minimum, if r of them are negative and the rest are positive, then it is called a saddle point with index r. At the critical point, the eigenvectors indicate the directions in which the value of the function locally changes. Moreover the changes are proportional to the corresponding -signed- eigenvalue. Under sufficient regularity conditions, it is rather straightforward to show that gradient based methods converge to points where gradient is zero. Recently Lee et al. [2016] showed that they indeed converge to minimizers.", "startOffset": 249, "endOffset": 2415}, {"referenceID": 4, "context": "Bottou [1991] points out that large eigenvalues of the Hessian of the loss can create the illusion of a local minima and GD can get stuck there, it further claims that the help of the inherent noise in SGD may help getting out of this obstacle.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "Bottou [1991] points out that large eigenvalues of the Hessian of the loss can create the illusion of a local minima and GD can get stuck there, it further claims that the help of the inherent noise in SGD may help getting out of this obstacle. The origin of this observation is due to Bourrely [1989], as well as numerical justifications.", "startOffset": 0, "endOffset": 302}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 10, "context": "A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016].", "startOffset": 111, "endOffset": 132}, {"referenceID": 7, "context": "Based on exactly this observation combined with the K-replica method of the previous paragraph, a recent work produced promising practical results [Chaudhari et al., 2016].", "startOffset": 147, "endOffset": 171}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al.", "startOffset": 46, "endOffset": 97}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks.", "startOffset": 46, "endOffset": 123}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks. They notably find that high error local minima traps do not appear when the model is over-parametrized. These concentration results can help explain why we find that the solutions attained by different optimizers like GD and SGD often have comparable training accuracies. However, while these methods find comparable solutions in terms of training error there is no guarantee they generalize equally. A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016]. They demonstrate that the large batch methods always generalize a little bit worse even when they have similar training accuracies. The paper further makes the observation that the basins found by small batch methods are wider, thereby contributing to the claim that wide basins, as opposed to narrow ones, generalize better 1. The final part of the historical account is devoted to the observation of flatness of the landscape in neural networks and it\u2019s consequences through the lens of the Hessian. In early nineties, Hochreiter and Schmidhuber [1997] remarks that there are parts of the landscape in which the weights can be perturbed without significantly changing the loss value.", "startOffset": 46, "endOffset": 1298}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks. They notably find that high error local minima traps do not appear when the model is over-parametrized. These concentration results can help explain why we find that the solutions attained by different optimizers like GD and SGD often have comparable training accuracies. However, while these methods find comparable solutions in terms of training error there is no guarantee they generalize equally. A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016]. They demonstrate that the large batch methods always generalize a little bit worse even when they have similar training accuracies. The paper further makes the observation that the basins found by small batch methods are wider, thereby contributing to the claim that wide basins, as opposed to narrow ones, generalize better 1. The final part of the historical account is devoted to the observation of flatness of the landscape in neural networks and it\u2019s consequences through the lens of the Hessian. In early nineties, Hochreiter and Schmidhuber [1997] remarks that there are parts of the landscape in which the weights can be perturbed without significantly changing the loss value. Such regions at the bottom of the landscape are called the flat minima, which can be considered as another way of saying a very wide minima. It is further noted that such minima have better generalization properties and a new loss function that makes use of the Hessian of the loss function has been proposed that targets the flat minima. The computational complexity issues have been attempted to be resolved using the R-operator of Pearlmutter [1994]. However, the new loss requires all the entries of the Hessian, and even with the R-operator it is unimaginably slow for today\u2019s large networks.", "startOffset": 46, "endOffset": 1882}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks. They notably find that high error local minima traps do not appear when the model is over-parametrized. These concentration results can help explain why we find that the solutions attained by different optimizers like GD and SGD often have comparable training accuracies. However, while these methods find comparable solutions in terms of training error there is no guarantee they generalize equally. A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016]. They demonstrate that the large batch methods always generalize a little bit worse even when they have similar training accuracies. The paper further makes the observation that the basins found by small batch methods are wider, thereby contributing to the claim that wide basins, as opposed to narrow ones, generalize better 1. The final part of the historical account is devoted to the observation of flatness of the landscape in neural networks and it\u2019s consequences through the lens of the Hessian. In early nineties, Hochreiter and Schmidhuber [1997] remarks that there are parts of the landscape in which the weights can be perturbed without significantly changing the loss value. Such regions at the bottom of the landscape are called the flat minima, which can be considered as another way of saying a very wide minima. It is further noted that such minima have better generalization properties and a new loss function that makes use of the Hessian of the loss function has been proposed that targets the flat minima. The computational complexity issues have been attempted to be resolved using the R-operator of Pearlmutter [1994]. However, the new loss requires all the entries of the Hessian, and even with the R-operator it is unimaginably slow for today\u2019s large networks. More recently, an exact numerical calculation of the Hessian have been carried out by Sagun et al. [2016]. It turns out that the Hessian is degenerate at any given point including the randomly chosen initial point, that the spectrum of it is composed of two parts: (1) the bulk, and (2) the outliers.", "startOffset": 46, "endOffset": 2133}, {"referenceID": 11, "context": "We study the Hessian using a decomposition [LeCun et al., 1998] as a sum of two matrices,\u2207L(w) = G(w) +H(w) where G is the sample covariance matrix of the gradients of model outputs and H is the Hessian of the model outputs.", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": "An example is the recent, Dinh et al. [2017], work shows how sharp minima can still generalize with proper modifications to the loss function.", "startOffset": 26, "endOffset": 45}, {"referenceID": 13, "context": "The result dates back to sixties and can be found in [Mar\u010denko and Pastur, 1967].", "startOffset": 53, "endOffset": 80}, {"referenceID": 1, "context": "One of the earliest relaxations appear in [Baik et al., 2005].", "startOffset": 42, "endOffset": 61}, {"referenceID": 3, "context": "for this slightly more general case with non-trivial correlations has been provided only recently by Bloemendal et al. [2016]. We will briefly review these results here see how they are related to the first term of the above decomposition.", "startOffset": 101, "endOffset": 126}, {"referenceID": 10, "context": "And this looks in line with the observations in Keskar et al. [2016], in that, it appears that the LB solution and SB solutions are separated by a barrier, and that the latter of which generalizes better.", "startOffset": 48, "endOffset": 69}, {"referenceID": 9, "context": "This question have been explored in a recent work: in Freeman and Bruna [2016] it shown that for one hidden layer rectified neural networks the solution space is connected which is consistent with the flatness of the landscape.", "startOffset": 54, "endOffset": 79}], "year": 2017, "abstractText": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. [2016]: Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecturealgorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: a gradient based method appears to be first climbing uphill and then falling downhill between two points; whereas, in fact, they lie in the same basin.", "creator": "LaTeX with hyperref package"}}}