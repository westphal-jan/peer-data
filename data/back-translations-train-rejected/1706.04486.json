{"id": "1706.04486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Learning and Evaluating Musical Features with Deep Autoencoders", "abstract": "In this work we describe and evaluate methods to learn musical embeddings. Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation. We consider autoencoding-based methods including denoising autoencoders, and context reconstruction, and evaluate the resulting embeddings on a forward prediction and a classification task.", "histories": [["v1", "Wed, 14 Jun 2017 13:35:46 GMT  (101kb,D)", "http://arxiv.org/abs/1706.04486v1", null], ["v2", "Thu, 15 Jun 2017 20:09:52 GMT  (101kb,D)", "http://arxiv.org/abs/1706.04486v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["mason bretan", "sageev oore", "doug eck", "larry heck"], "accepted": false, "id": "1706.04486"}, "pdf": {"name": "1706.04486.pdf", "metadata": {"source": "CRF", "title": "A Unit Selection Methodology for Music Generation Using Deep Neural Networks", "authors": ["Mason Bretan", "Sageev Oore", "Douglas Eck", "Larry Heck"], "emails": [], "sections": [{"heading": null, "text": "vector, which represents four contiguous bars of music and is derived from a symbolic representation. We look at autocoding-based methods such as the denocialization of autoencoders and the reconstruction of the context and evaluate the resulting embedding using a prediction and a classification task."}, {"heading": "1 Introduction", "text": "Music is a highly dimensional and complex domain. Many of the tasks that involve music analysis and information gathering focus on reducing this dimensionality by categorizing music with discrete labels that describe elements such as genre, composer, instrumentation or era. However, new music is constantly being absorbed, new genres are being identified / created, and therefore it is unreasonable to expect a musical dataset to provide correct and comprehensive examples of any of the above classifications. Any approach that depends on this will therefore not scale naturally. In this article, we focus on the search for and exploitation of the inherent continuity of musical space. Instead of music based on a discrete labeling scheme, we use deep neural networks to project music into a continuous, low-dimensional space that captures meaningful characteristics. To assess the effectiveness of this learned space, is known as the \"embedding,\" which we first determine which characteristics should encompass such a incorporation."}, {"heading": "2 Related Work", "text": "For example, Huang et al. [2] describe the use of a deeply structured semantic model Xiv: 170 6.04 486v 1 [cs.S D] June 14, 20on new latent semantic models to project queries and documents into a common, low-dimensional space in which cosmic similarity provides a good estimate of the relevance between pairs of original objects (i.e., before their projection).One clear intuition for finding such embeddings is that two textual excerpts can be very related, even if they do not use exactly the same keywords.The hope is that assignment to a low-dimensional, continuous embedding actually requires two such excerpts to nearby points. Likewise, two musical excerpts can be very related without using the exact same harmonic, melodic, or rhythmic structure."}, {"heading": "3 Data and Representation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset & Augmentation", "text": "To test the different networks and training methods, we used a collection of piano compositions by 25 artists. The distribution of the compositions among the artists is shown in Table 1. At least two songs were offered for testing by each performer, using only songs with a time signature in which the note value of a bar corresponds to a quarter note (i.e. the denominator of the time signature must have a value of 4).We expanded the data by transposing each piece into all keys, which also prevented the networks from simply learning the bias that composers might have had for certain keys."}, {"heading": "3.2 Representation", "text": "A piano roll representation of the music can be considered an mxn matrix in which m is the number of possible pitches and n represents the total time. In this work, we allow 60 total pitches from mid-note 36 (two octaves below the middle \"C\") to 96 (three octaves above). Any note outside this range is transposed into either the lowest or highest octave used to sequence note events based on a beat. For example, if a value of four tick-per-beat is displayed (or \"pulses per quarter-note\"), the smallest unit of time used to sequence a beat is displayed."}, {"heading": "4 Models", "text": "In this section, various methods for learning embedding are presented: while the architecture and number of learnable parameters remain roughly the same for each model, the inputs and outputs are modified according to the task at hand. Each model includes a series of shaft layers followed by fully connected layers that result in a 100-dimensional vector representing the embedding. This size was chosen because it is small enough to quickly calculate the search for the nearest neighbors via a database that consists of over a million possible data points for real-time applications, but is still large enough to embed many important musical features."}, {"heading": "4.1 Denoising Autoencoders", "text": "A single layer system can be larger than the other.) We also write h (i.e.) with no subscript the output of the final (i.e. nth) layer of this encoder. (i.e.)"}, {"heading": "4.2 Context Prediction", "text": "Similarly, we use the surrounding music of a particular four-bar unit to learn the music embedding. If Ui denotes the four-bar unit in a sequence of four bar units from a single composition, the idea is to use the information provided by Ui + 1 and Ui \u2212 1. Using an architecture identical to the previous autoencoder (including bound weights between encoder and decoder), we train two models each with tasks 1. Prediction - Input to the network is Ui + 1 and output is Ui + 1. Contextual prediction - Input to the network is Ui \u2212 1 + Ui + 1. By summing up the matrices of the surrounding units, we obtain symmetry between encoder and decoder, while we get 8 bars of context in a 60x96 space."}, {"heading": "4.3 Composer Classification", "text": "Another task is to classify four-bar units by their composer. A basic idea behind this task is that, in order to successfully predict composers, the learned characteristics necessarily capture some important information about the musical passage. This is analogous to transferring learning methods used in computer visual tasks where a network is first trained to label images and then, assuming relevant characteristics are learned, its parameters are used for additional tasks. The architecture of the classification network is identical to the encoder portion of the previously described autoencoder. However, instead of a decoder, the 100-bar embedding vector is attached to a single output layer of size 27 (one for each composer). For the loss function, we use softmax with cross-entropy."}, {"heading": "4.4 Regularized Training", "text": "In the last model, we use composer classification as a means of regulating the embedding of the previously described contextual predictive network. The general premise is in Figure 2. Given the original encoder / decoder network, an additional network is connected to the embedding layer. Specifically, for a unit Ui, we use its 100-dimensional embedding representation h (U) as the (fully connected) input to a hidden layer of 50 hidden units, which in turn is fully connected to a softmax layer of 27 units. The main objective of the network remains the same, as Ui then reconstructs Ui \u2212 1 + Ui + 1. The auxiliary task of component classification is imposed on embedding, so that the parameters of the network are optimized to not only reconstruct the context, but also to predict the composers."}, {"heading": "5 Experiments and Results", "text": "We use several techniques and measures to assess the effectiveness of our embeds.5.01 Forward Prediction & RankingWe train 3 stacked LSTM cells of 400 units each to predict the next step of a sequence.Specifically, the input into the network at each time step is a 100-dimensional embedding vector of a 4-stroke unit Ui. We train this network to predict the 8th unit Ui + 7 compared to the previous 7 units Ui, Ui + 1. Ui + 6. This means that 28 beats of context (in four clock blocks) are provided before the prediction.For each given target Ui + 7 as described above, we create a set of 1000 embedding vectors: one is h (Ui + 7), the true embedding for the target (in four clock blocks).The other 999 vectors are embedding of randomly selected units in the data set."}, {"heading": "5.1 Results", "text": "For the prediction task, we compiled the ranking order provided by each of the seven LSTM prediction networks for all 325,219 runs. Considering that the ranking distributions were considerably distorted towards zero (highest possible rank), we used the median instead of the mean. To test the significance between the ranking distributions of each model, we used the Levene test, which compares any possible model combination (21 possible comparisons). After a Bonferroni correction, statistical significance is given by p < 0.0037. All distributions differ significantly from each other. To compare the spreads of each distribution, we used Levene's test, which tests the null hypothesis that all input samples come from populations with equal deviations. For all seven distributions, the test statistics [W = 7398.17] showed significant differences in the < 0.0001."}, {"heading": "6 Conclusions", "text": "Not surprisingly, the network trained to identify composers performed much better in this task than the features learned from the resulting embedding of denotizing autoencoders and context prediction networks. However, the performance of the context prediction network improved due to this additional classification task. This network learned parameters that found a power balance between the two experimental tasks."}], "references": [{"title": "Comparison between language and music", "author": ["Mireille Besson", "Daniele Sch\u00f6n"], "venue": "Annals of the New York Academy of Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Chordripple: Recommending chords to help novice composers go beyond the ordinary", "author": ["Cheng-Zhi Anna Huang", "David Duvenaud", "Krzysztof Z. Gajos"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Music type classification by spectral contrast feature", "author": ["Dan-Ning Jiang", "Lie Lu", "Hong-Jiang Zhang", "Jian-Hua Tao", "Lian-Hong Cai"], "venue": "In Multimedia and Expo,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "A comparative study on content-based music genre classification", "author": ["Tao Li", "Mitsunori Ogihara", "Qi Li"], "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Chord2vec: Learning Musical Chord Embeddings", "author": ["Sephora Madjiheurem", "Lizhen Qu", "Christian Walder"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G Hinton"], "venue": "In Proc. SIGIR Workshop Information Retrieval and Applications of Graphical Models,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Many of the tasks involving music analysis and information retrieval focus on reducing this dimensionality by categorizing music with discrete labels describing elements such as genre, composer, instrumentation, or era [5, 4].", "startOffset": 219, "endOffset": 225}, {"referenceID": 3, "context": "Many of the tasks involving music analysis and information retrieval focus on reducing this dimensionality by categorizing music with discrete labels describing elements such as genre, composer, instrumentation, or era [5, 4].", "startOffset": 219, "endOffset": 225}, {"referenceID": 1, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 6, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 7, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 8, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 1, "context": "For example, Huang et al [2] describe the use of a deep structured semantic model", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "Indeed, while music and language have significant structural parallels[1], it is generally only recently that researchers have begun to explore how this may give rise to cross-pollination between the respective fields.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "In ChordRipple[3], Huang et al create a tool that helps composing students select a next chord in a sequence or replace a subsequence of chords within a longer progression.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Madjiheurem et al [6] compare the application of skip-gram-based models[8] and sequence-to-sequence models[10] for learning embeddings of chords.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "Madjiheurem et al [6] compare the application of skip-gram-based models[8] and sequence-to-sequence models[10] for learning embeddings of chords.", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "Madjiheurem et al [6] compare the application of skip-gram-based models[8] and sequence-to-sequence models[10] for learning embeddings of chords.", "startOffset": 106, "endOffset": 110}], "year": 2017, "abstractText": "In this work we describe and evaluate methods to learn musical embeddings. Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation. We consider autoencoding-based methods including denoising autoencoders, and context reconstruction, and evaluate the resulting embeddings on a forward prediction and a classification task.", "creator": "LaTeX with hyperref package"}}}