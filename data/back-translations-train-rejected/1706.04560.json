{"id": "1706.04560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Neural Models for Key Phrase Detection and Question Generation", "abstract": "We propose several neural models arranged in a two-stage framework to tackle question generation from documents. First, we estimate the probability of \"interesting\" answers in a document using a neural model trained on a question-answering corpus. The predicted key phrases are then used as answers to condition a sequence-to-sequence question generation model. Empirically, our neural key phrase detection models significantly outperform an entity-tagging baseline system. We demonstrate that the question generator formulates good quality natural language questions from extracted key phrases. The resulting questions and answers can be used to assess reading comprehension in educational settings.", "histories": [["v1", "Wed, 14 Jun 2017 16:06:18 GMT  (26kb)", "http://arxiv.org/abs/1706.04560v1", null], ["v2", "Thu, 14 Sep 2017 17:43:48 GMT  (371kb,D)", "http://arxiv.org/abs/1706.04560v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["sandeep subramanian", "tong wang", "xingdi yuan", "saizheng zhang", "adam trischler", "yoshua bengio"], "accepted": false, "id": "1706.04560"}, "pdf": {"name": "1706.04560.pdf", "metadata": {"source": "CRF", "title": "Neural Models for Key Phrase Detection and Question Generation", "authors": ["Sandeep Subramanian", "Tong Wang", "Xingdi Yuan", "Saizheng Zhang", "Adam Trischler"], "emails": ["sandeep.subramanian.1@umontreal.ca", "tong.wang@microsoft.com", "eric.yuan@microsoft.com", "saizheng.zhang@umontreal.ca", "adam.trischler@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.04 560v 1 [cs.C L] 14. Jun 2017We propose several neural models arranged in a two-step framework to tackle the creation of questions from documents. First, we estimate the probability of \"interesting\" answers in a document using a neural model trained on a corpus of questioners. Predicted key sentences are then used as answers to condition a sequence-to-sequence question generation model. Empirically, our models for recognizing neural key sentences clearly outperform an entity tagging base system. We show that the question generator formulates good quality natural language questions from extracted key sentences."}, {"heading": "1 Introduction", "text": "Many educational applications benefit from automatic questioning, including vocabulary evaluation (Brown et al., 2005), written support (Liu et al., 2012), and reading comprehension evaluation (Mitkov and Ha, 2003; Kunichika et al., 2004). Formulating questions that are tested at certain levels for specific skills requires significant human efforts that are difficult to scale, such as massive open online courses (MOOCs). Despite their application, the majority of existing models rely on rules-based methods that also do not work well across different domains and / or spellings. To address this limitation, we propose several end-to-end traceable neural models for automatic question generation."}, {"heading": "2 Related Work", "text": "In the meantime, various NLP techniques have been used in these systems to improve the quality of the generation, including parsing (Heilman and Smith, 2010), semantic role designation (Lindberg et al., 2013), and the use of lexicographic resources such as WordNet (Miller, 1995; Mitkov and Ha, 2003). However, the majority of proposed methods fall back on simple rule-based techniques, such as slot filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation euristics (Agarwal and Mannem, 2011; Ali et al., 2010)."}, {"heading": "3 Model Description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Key Phrase Detection", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2 Question Generation", "text": "(Our question generation model assumes a sequenceto-sequence framework (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al., 2014) and a pointer-softmax decoder (Gulcehre et al., 2016). It is a slightly modified version of the model described by Yuan et al. (2017). The model takes input from document D = (wd1,.., w d nd) and an answer A = (wa1,., w a na) and outputs a question Q = (w-q1,.., w-qnq nq). An input word w {d, a} i is first made by concatenating both word and character plane embeddings ei = < e w i; e ch i >."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We are conducting our experiments based on the SQuAD corpus (Rajpurkar et al., 2016), an automated understanding dataset consisting of over 100k crowdsourced question-answer pairs on 536 Wikipedia articles. Simple pre-processing, including reduction and word tokenization using NLTK (Bird, 2006), is being performed. SQuAD's test split is hidden from the public, so we extract 5,158 question-answer pairs (which are included in 23 Wikipedia articles) from the training set as a validation set and use the official development data to report test results."}, {"heading": "4.2 Quantitative Evaluation", "text": "Since each key phrase is itself a multiword unit, a value at word level F1 is unsuitable for evaluation due to the different lengths of the key sentences. We therefore propose a new metric, the hierarchical formula 1, which is invariant to the target length, by integrating an idea of alignment between the gold and the predicted phrases. The metric is calculated as follows: Given the prediction sequence e-i and the gold mark sequence ej, we first construct a pair of symbolic formula 1 scores fi, j matrix between the two phrases e-i and ej. Max pooling along the gold mark axis essentially evaluates the precision of each prediction, with partial matches being taken into account by the paired formula F1 (identical to the evaluation of a single answer in SQuAD) in the cells: pi = maxj (fi, j). Similarly, the callback for labels ej can be defined by the maximum pool axis along the single response to the evaluation (SAD)."}, {"heading": "4.3 Results and Discussions", "text": "The evaluation results are listed in Table 1. As expected, the unit marking the baseline has achieved the best recall, probably by over-generating candidate responses. On the other hand, the NES model has a much greater precision advantage and therefore exceeds the unit marking the baseline with remarkable margins in F1. This trend continues in comparison between the NES and the pointer network models.Qualitatively, we observe that the entity-based models have a strong bias towards numerical types, which often do not capture interesting information in an article. In Table 2, for example, the entity baselines can only mark the first and the first, missing all the key terms that are successfully detected by the pointer model. Furthermore, we note that entity-based systems tend to select the central topical entity as an answer that may contradict the distribution of interesting human-selected responses."}, {"heading": "5 Conclusion", "text": "We proposed a two-step framework to address the problem of generating questions from documents. First, we use a question-and-answer corpus to train a neural model to estimate the distribution of key phrases of interest to questioning people. Specifically, we propose two neural models, one that indicates the boundaries of key phrases with a pointer network. Compared to an entity tagging baseline, the proposed models show significantly better results. Then, we use a sequence-to-sequence model to generate questions based on the key phrases selected in the first step. Our question generator is inspired by an attention-based translation model and uses the pointer-to-sequence model to show the synchronization mechanism of both the word dynamics of a document and the understanding of the word dynamics of a document as well."}, {"heading": "Acknowledgement", "text": "We would like to thank Professor Yoshua Bengio for the insightful discussions that eventually led to some fundamental premises of this study."}, {"heading": "A Pointer Network", "text": "Pointer networks (Vinyals et al., 2015) are an extension of sequence-to-sequence models (Sutskever et al., 2014) in which the target sequence consists of positions in the source sequence. We encode the source sequence document into a sequence of annotation vectors hd = (hd1,... h nd) using an embedding table followed by a bidirectional LSTM. The decoder also consists of an embedding search shared with the encoder, followed by a unidirectional LSTM with an attention mechanism. We refer to the annotation vectors of the decoder as (hp1, h p 2,.., h p 2na \u2212 1, h p 2na), where na is the number of response keys, hp 1 and h p 2 correspond to the initial and end annotation vectors for the first answer key phrase, and so on."}, {"heading": "B Decoder Details of the Question Generation Model", "text": "With the pointer softmax mechanims, the decoder in the Generation Model question consists of a pointing decoder and a generative decodecoder.In the pointing decoder, the recurrence with two cascading LSTM cells c1 and c2: s (t) 1 = c1 (t \u2212 1) 2) (1) s (t \u2212 1) s (t) 2 (t) s (t) 1), s (t) 1) 1 (t) 1 (t) 2 (t \u2212 1) is the recursive state, y (t \u2212 1) is the embedding of the decoder output from the previous time step, and v (t) is the context vector to be briefly defined in Equation (3). At any time, the pointing decoder calculates a distribution (t) across the document word positions (i.e."}], "references": [{"title": "Automatic gap-fill question generation from text books", "author": ["Manish Agarwal", "Prashanth Mannem."], "venue": "Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics, pages", "citeRegEx": "Agarwal and Mannem.,? 2011", "shortCiteRegEx": "Agarwal and Mannem.", "year": 2011}, {"title": "Automation of question generation from sentences", "author": ["Husam Ali", "Yllias Chali", "Sadid A Hasan."], "venue": "Proceedings of QG2010: The Third Workshop on Question Generation. pages 58\u201367.", "citeRegEx": "Ali et al\\.,? 2010", "shortCiteRegEx": "Ali et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR", "citeRegEx": "Cho and Bengio.,? 2014", "shortCiteRegEx": "Cho and Bengio.", "year": 2014}, {"title": "Mind the gap: learning to choose gaps for question generation", "author": ["Lee Becker", "Sumit Basu", "Lucy Vanderwende."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Becker et al\\.,? 2012", "shortCiteRegEx": "Becker et al\\.", "year": 2012}, {"title": "Nltk: the natural language toolkit", "author": ["Steven Bird."], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics, pages 69\u201372.", "citeRegEx": "Bird.,? 2006", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Automatic question generation for vocabulary assessment", "author": ["Jonathan C Brown", "Gwen A Frishkoff", "Maxine Eskenazi."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. As-", "citeRegEx": "Brown et al\\.,? 2005", "shortCiteRegEx": "Brown et al\\.", "year": 2005}, {"title": "Ranking automatically generated questions using common human queries", "author": ["Yllias Chali", "Sina Golestanirad."], "venue": "The 9th International Natural Language Generation conference. page 217.", "citeRegEx": "Chali and Golestanirad.,? 2016", "shortCiteRegEx": "Chali and Golestanirad.", "year": 2016}, {"title": "Learning to ask: Neural question generation for reading comprehension", "author": ["Xinya Du", "Junru Shao", "Claire Cardie."], "venue": "arXiv preprint arXiv:1705.00106 .", "citeRegEx": "Du et al\\.,? 2017", "shortCiteRegEx": "Du et al\\.", "year": 2017}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.08148 .", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Good question! statistical ranking for question generation", "author": ["Michael Heilman", "Noah A Smith."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. As-", "citeRegEx": "Heilman and Smith.,? 2010a", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Rating computer-generated questions with mechanical turk", "author": ["Michael Heilman", "Noah A Smith."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computa-", "citeRegEx": "Heilman and Smith.,? 2010b", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Automated question generation methods for intelligent english learning systems and its evaluation", "author": ["Hidenobu Kunichika", "Tomoki Katayama", "Tsukasa Hirashima", "Akira Takeuchi."], "venue": "Proc. of ICCE.", "citeRegEx": "Kunichika et al\\.,? 2004", "shortCiteRegEx": "Kunichika et al\\.", "year": 2004}, {"title": "Deep questions without deep understanding", "author": ["Igor Labutov", "Sumit Basu", "Lucy Vanderwende."], "venue": "ACL (1). pages 889\u2013898.", "citeRegEx": "Labutov et al\\.,? 2015", "shortCiteRegEx": "Labutov et al\\.", "year": 2015}, {"title": "Race: Large-scale reading comprehension dataset from examinations", "author": ["Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy."], "venue": "arXiv preprint arXiv:1704.04683 .", "citeRegEx": "Lai et al\\.,? 2017", "shortCiteRegEx": "Lai et al\\.", "year": 2017}, {"title": "Generating natural language questions to support learning on-line", "author": ["David Lindberg", "Fred Popowich", "John Nesbit", "Phil Winne."], "venue": "ENLG 2013 page 105.", "citeRegEx": "Lindberg et al\\.,? 2013", "shortCiteRegEx": "Lindberg et al\\.", "year": 2013}, {"title": "Newsqa: A machine compre", "author": ["heer Suleman"], "venue": null, "citeRegEx": "Suleman.,? \\Q2016\\E", "shortCiteRegEx": "Suleman.", "year": 2016}, {"title": "Machine comprehension by text-to-text neural question generation", "author": ["Xingdi Yuan", "Tong Wang", "Caglar Gulcehre", "Alessandro Sordoni", "Philip Bachman", "Sandeep Subramanian", "Saizheng Zhang", "Adam Trischler."], "venue": "2nd Workshop on Representation", "citeRegEx": "Yuan et al\\.,? 2017", "shortCiteRegEx": "Yuan et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 5, "context": "Many educational applications can benefit from automatic question generation, including vocabulary assessment (Brown et al., 2005), writing support (Liu et al.", "startOffset": 110, "endOffset": 130}, {"referenceID": 13, "context": ", 2012), and assessment of reading comprehension (Mitkov and Ha, 2003; Kunichika et al., 2004).", "startOffset": 49, "endOffset": 94}, {"referenceID": 3, "context": "In this domain, question generation typically involves two inter-related components: first, a system to identify interesting entities or events (key phrases) within a passage or document (Becker et al., 2012); second, a question generator that constructs questions in natural language that ask specifically about the given key phrases.", "startOffset": 187, "endOffset": 208}, {"referenceID": 15, "context": "If this premise is correct, then the growing collection of crowd-sourced question-answering datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Lai et al., 2017) can be harnessed to learn models for key phrases of interest to human readers.", "startOffset": 101, "endOffset": 188}, {"referenceID": 8, "context": "mechanism (Gulcehre et al., 2016).", "startOffset": 10, "endOffset": 33}, {"referenceID": 13, "context": "Automatic question generation systems are often used to alleviate (or even eliminate) the burden of human generation of questions to assess reading comprehension (Mitkov and Ha, 2003; Kunichika et al., 2004).", "startOffset": 162, "endOffset": 207}, {"referenceID": 9, "context": "Various NLP techniques have been adopted in these systems to improve generation quality, including parsing (Heilman and Smith, 2010a; Mitkov and Ha, 2003), semantic role labeling (Lindberg et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 16, "context": "Various NLP techniques have been adopted in these systems to improve generation quality, including parsing (Heilman and Smith, 2010a; Mitkov and Ha, 2003), semantic role labeling (Lindberg et al., 2013), and the use of lexicographic resources like WordNet (Miller, 1995; Mitkov and Ha, 2003).", "startOffset": 179, "endOffset": 202}, {"referenceID": 16, "context": "However, the majority of proposed methods resort to simple rule-based techniques such as slot-filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al.", "startOffset": 117, "endOffset": 192}, {"referenceID": 6, "context": "However, the majority of proposed methods resort to simple rule-based techniques such as slot-filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al.", "startOffset": 117, "endOffset": 192}, {"referenceID": 14, "context": "However, the majority of proposed methods resort to simple rule-based techniques such as slot-filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al.", "startOffset": 117, "endOffset": 192}, {"referenceID": 0, "context": ", 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al., 2010) (e.", "startOffset": 47, "endOffset": 91}, {"referenceID": 1, "context": ", 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al., 2010) (e.", "startOffset": 47, "endOffset": 91}, {"referenceID": 7, "context": "For the latter, Du et al. (2017) used a sequence-to-", "startOffset": 16, "endOffset": 33}, {"referenceID": 18, "context": "Yuan et al. (2017) proposed a similar architecture but in addition improved model performance through policy gradient techniques.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Heilman and Smith (2010b) asked crowdworkers to rate the acceptability of computer-generated natural language questions as quiz questions, while Becker et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "Heilman and Smith (2010b) asked crowdworkers to rate the acceptability of computer-generated natural language questions as quiz questions, while Becker et al. (2012) solicited quality ratings of text chunks as potential gaps for Cloze-style questions.", "startOffset": 145, "endOffset": 166}, {"referenceID": 11, "context": "network (Hochreiter and Schmidhuber, 1997) into annotation vectors (h1, .", "startOffset": 8, "endOffset": 42}, {"referenceID": 8, "context": ", 2014) and a pointer-softmax decoder (Gulcehre et al., 2016).", "startOffset": 38, "endOffset": 61}, {"referenceID": 8, "context": ", 2014) and a pointer-softmax decoder (Gulcehre et al., 2016). It is a slightly modified version of the model described by Yuan et al. (2017). The model takes a Table 1: Model evaluation on question- and answer-generation.", "startOffset": 39, "endOffset": 142}, {"referenceID": 8, "context": "The RNN-based decoder employs the pointersoftmax mechanism (Gulcehre et al., 2016).", "startOffset": 59, "endOffset": 82}, {"referenceID": 4, "context": "Simple preprocessing is performed, including lower-casing and word tokenization using NLTK (Bird, 2006).", "startOffset": 91, "endOffset": 103}], "year": 2017, "abstractText": "We propose several neural models arranged in a two-stage framework to tackle question generation from documents. First, we estimate the probability of \u201cinteresting\u201d answers in a document using a neural model trained on a questionanswering corpus. The predicted key phrases are then used as answers to condition a sequence-to-sequence question generation model. Empirically, our neural key phrase detection models significantly outperform an entity-tagging baseline system. We demonstrate that the question generator formulates good quality natural language questions from extracted key phrases. The resulting questions and answers can be used to assess reading comprehension in educational settings.", "creator": "LaTeX with hyperref package"}}}