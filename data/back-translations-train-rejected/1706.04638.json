{"id": "1706.04638", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Proximal Backpropagation", "abstract": "We offer a generalized point of view on the backpropagation algorithm, currently the most common technique to train neural networks via stochastic gradient descent and variants thereof. Specifically, we show that backpropagation of a prediction error is equivalent to sequential gradient descent steps on a quadratic penalty energy. This energy comprises the network activations as variables of the optimization and couples them to the network parameters. Based on this viewpoint, we illustrate the limitations on step sizes when optimizing a nested function with gradient descent. Rather than taking explicit gradient steps, where step size restrictions are an impediment for optimization, we propose proximal backpropagation (ProxProp) as a novel algorithm that takes implicit gradient steps to update the network parameters. We experimentally demonstrate that our algorithm is robust in the sense that it decreases the objective function for a wide range of parameter values. In a systematic quantitative analysis, we compare to related approaches on a supervised visual learning task (CIFAR-10) for fully connected as well as convolutional neural networks and for an unsupervised autoencoder (USPS dataset). We demonstrate that ProxProp leads to a significant speed up in training performance.", "histories": [["v1", "Wed, 14 Jun 2017 18:59:13 GMT  (774kb)", "http://arxiv.org/abs/1706.04638v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["thomas frerix", "thomas m\\\"ollenhoff", "michael moeller", "daniel cremers"], "accepted": false, "id": "1706.04638"}, "pdf": {"name": "1706.04638.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["thomas.frerix@tum.de", "moellenh@in.tum.de", "michael.moeller@uni-siegen.de", "cremers@tum.de"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.04 638v 1 [cs.L G] 14 Ju"}, {"heading": "1 Introduction", "text": "In recent years, we have received considerable attention for solving problems such as the classification of computer systems [8] and the formation of larger learning systems (SGD). The formation of neural networks is achieved through the optimization of non-conventional methods, such as the comparison of LBFS with SGD (SGD)."}, {"heading": "2 Notation and model", "text": "We refer to the Euclidean norm for vectors and the Frobenius norm for matrices (1)."}, {"heading": "3 Penalty formulation of backpropagation", "text": "The parentage teration of the gradient on a nested function J (W, b; X, y) such as (2), (W, b) k + 1 = (W, b) k \u2212 \u03c4 \u0432 J (W k, bk; X, y), (3) is usually implemented using the back propagation algorithm [16]. As a basis for the proposed proximal optimization method, we will now establish a link between the classical baking propagation algorithm and the square streamlining functions of FormE (W, b, a, z) = Ly (WL \u2212 1, bL \u2212 1, aL \u2212 2)) + L \u2212 2 \u2211 l = 1\u03b3 2 \u0445\u0445\u043c (zl) \u2212 al \u0432 2 \u0432\u0438\u043c (Wl, bl, al \u2212 1) \u2212 zl b 2. (4) Under mild conditions, the parentage of the gradient leads to the convergence of the sequence of the minimizers from E to the minimization of J."}, {"heading": "4 Generalizing backpropagation", "text": "The interpretation of sentence 1 leads to the natural idea of replacing the explicit gradient steps a \u00a9, b \u00a9 and c \u00a9 in algorithm 1 with other - possibly more powerful - minimization steps. For example, for l = 1 step c \u00a9 of algorithm 1 (W1, b1) k + 1 = (W1, b1) k \u2212 \u03c4\u03c1 f (((W1, b1) k) (5) for f (W, b) = 12% \u03c6 (W, b, X) \u2212 z1% 2. As we will discuss in the next subsection, the substitution of the equation (5) by an implicit gradient step (W1, b1) k + 1 = (W1, b1) k \u2212 \u03c4\u03c1 f (W1, b1) k + 1) (6) exhibits several advantageous properties and therefore motivates the general concept presented in algorithm 2, namely the switch between forward modes of the network and sequential minimizations of the penalty function (4)."}, {"heading": "4.1 Implicit gradient steps", "text": "The proximal mapping of a function f: Rn \u2192 R is defined as the following [12]: prox\u03c4f (y): = argmin x \u00b7 Rnf (x) + 12haul | x \u2212 y | 2. (7) By restructuring the optimum conditions to (7) and adopting y = xk, it can be interpreted as an implicit gradient step that is evaluated at the new point xk + 1: xk + 1: = argmin x (x) + 12ctus = xk \u2212 [2] as an implicit gradient step. (8) The advantage of such proximal steps is that unlike explicit gradient departure (8), which is also known as the proximal point algorithm, it is unconditionally stable. (11) Unconditional stability means that (8) the energy f is reduced monotonically for each other step f."}, {"heading": "4.2 Computing the proximal steps", "text": "In this subsection, we will discuss the proximal operators for implicit gradient steps in relation to our variables W, b, a, and Z. For simplicity, we will limit the discussion to the fully connected case. Please note that Convolutionary Layers can be treated in a similar way, which we will explain in more detail in the appendix. 4.2.1 Proximal Steps in (W, b) Last Layer In the last layer, you must solve (W (k + 1) L \u2212 1, b (k + 1) L \u2212 1) = argmin W, b (k) L \u2212 1) Ly (Wa (k) L \u2212 2 + b1) + 12 layer."}, {"heading": "4.2.2 Proximal steps in a", "text": "Last layer Similar to the above case of (W, b), one must solvea (k + 1) L \u2212 2 = argmin a Ly (W (k + 1) L \u2212 1 a + b (k + 1) L \u2212 1) + \u03b3 2 | | a \u2212 a (k) L \u2212 2 | | 2 + 1 2\u03c4 | | | a \u2212 a (k) L \u2212 2 | | 2, (12), where we replace \u03c3 (z (k) L \u2212 2) = a (k) L \u2212 2 within the G penalty timeframe. This is a valid forward replacement. Again, for a square loss, the above problem becomes an instance of (9), and we propose to take explicit steps for a Softmax loss. First and hidden layers All layers except the last layer in the problem (k + 1) l \u2212 1 2 (k + 1) l + b (k + 1) l 1 | 2 (k + 1) l (1) and (2) l \u2212 1."}, {"heading": "4.2.3 Proximal steps in z", "text": "A proximal step in relation to E in relation to zl, i.e. implicit steps in b \u00a9, boils down to the non-convex minimization problem z (k + 1) l = argminz\u03b3 2 | | \u03c3 (z) \u2212 a (k + 1) l | | 2 + \u03c1 2 | | z (k) l \u2212 z | | 2 + 1 2\u03c4 | | z \u2212 z (k) l | | 2, (14), where we replace the forward variable z (k) l. Problem (14) disintegrates into independent univariate problems which, despite their non-convexity, can be solved to global optimality \u03c3 (x) = max (0, x). In the appendix we give the explicit solution of (14)."}, {"heading": "4.3 Explicit vs. implicit steps", "text": "Proposition 1 shows that algorithm 2 generates the gradient downward steps with explicit gradient steps and \u03b3 = \u03c1 = 1 / \u03c4. Motivated by the more general scheme of algorithm 2, we consider the following variants: 1. Explicit steps to all variables, 2. implicit steps to zl, explicit steps to (Wl, bl, al \u2212 1), 3. explicit steps to (zl, al \u2212 1), 4. explicit steps to zl, implicit steps to (Wl, bl, al \u2212 1) and 5. implicit steps to all variables. To examine their stability, we train a fully connected 256 \u2212 300 \u2212 100 \u2212 300 \u2212 256 autoencoder with ReLU nonlinearity and square losses using the above-mentioned updating equations."}, {"heading": "5 Proximal backpropagation as a first-order oracle", "text": "Algorithm 2 can be used as a gradient oracle for first-order methods (such as Adam [7]) or quasi-Newton methods (such as SFO [18]) by calculating a direction dk + 1 = (W, b) k + 1 \u2212 (W, b) k instead of the usual gradient. In fact, as seen in Proposition 1, for \u03c1 = 1\u03c4 and explicit gradient steps we obtain the direction of the steepest centered k + 1 = \u2212 \u03c4 (W, b) J (W, bk; X, y). (15) Note that it is generally not clear whether d is a downward direction for energy J, i.e. whether < < J, d > < 0. This property is often assumed in convergence proofs for gradient-based methods. Nevertheless, in practice we observed convergence across a wide range of parameters. For the experiments in this paper we refer to Gesterk + \u00b5k (Sv = 1 dynamics) with Sv = 1."}, {"heading": "6 Numerical evaluation", "text": "We evaluate our method in two representative experiments, a supervised visual learning problem on the CIFAR-10 dataset and an auto encoder on the handwritten USPS dataset."}, {"heading": "6.1 CIFAR-10 supervised learning", "text": "Fully connected network We trained a fully connected network with architecture 3072 \u2212 4000 \u2212 1000 \u2212 4000 \u2212 10, ReLU nonlinearity and transverse entropy + softmax loss. Weights were initialized according to the uniform distribution scheme described in [5] and a weight drop of 10 \u2212 6 was chosen. In Fig. 4, we compared SGD with the proposed method (ours). Impulse was selected to \u00b5 = 0.95 and batch size to 250 for both methods. For the proposed method, we chose slavish = 10 \u2212 3 and penalty parameters were selected as a practical example."}, {"heading": "6.2 USPS autoencoder", "text": "To compare with MAC [2], we trained a fully networked autoencoder with the same architecture as for our experiments in Section 4.3, used sigmoid activation functions, and trained on the USPS dataset with 5000 grayscale images of size 16 \u00d7 16 with minibatches of size 250. We used the same initialization as [2], where the initial weights in layer l were uniformly sampled from [\u2212 1 / \u221a nl \u2212 1, 1 / \u221a nl \u2212 1]. The authors of [2] kindly provided us with their MATLAB code to reproduce the results of their parallel MAC Target-s method, as well as the SGD and CG methods with which they were compared. Therefore, we are able to produce Figure 2 in [2] without minibatch-MAC on our computer with an Intel Core i7-6700HQ CPU with 2.60GHz and MATLAB's parallel Target-s Target-s-Solls-Solls-based algorithms with Figure 5 of the Solls = 6."}, {"heading": "7 Conclusion", "text": "We have proposed ProxProp as an efficient novel method of forming neural networks. To this end, we first demonstrated the equivalence of the classical backprop algorithm with an algorithm that alternates between sequential gradient steps on a square tightening function and forward steps through the network. We then developed a generalization of the backprop that replaces explicit gradient steps with implicit (proximal) steps. Numerical experiments show that ProxProp provides a stable reduction in objective function in a much larger range of step sizes than the classical backprop. A performance comparison with related minimization methods shows that our algorithm exhibits faster convergence and often provides lower overall non-convex objective values. We believe that the proposed framework provides an important bridge between ideas of convex optimization and deep learning."}, {"heading": "A Theoretical results", "text": "Note on Proposition 1: We first take a progression stage onE (W, b, a, z) = k (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K (K) (K) (K) (K (K) (K) (K (K) (K) (K (K) (K) (K (K) (K (K) (K (K) (K) (K (K) (K (K) (K (K) (K (K) (K (K (K) (K (K) (K (K (K) (K (K) (K (K) (K (K (K) (K) (K (K) (K (K) (K (K (K) (K (K (K) (K (K (K)) (K (K (K) (K (K (K (K (K (K)))) (K (K (K (K (K (K))) (K (K (K)) (K (K (K (K (K (K (K)), K (K (K (K (K (K (K (K (K))))))), K (K (K ("}, {"heading": "B Solution of the proximal subproblems", "text": "The solution to the problem (W (k + 1) L \u2212 1, b (k + 1) L \u2212 1, b (k + 1) L \u2212 1, b (k) L \u2212 2, b (k) K \u2212 2, b (k) K \u2212 2, b (k) K \u2212 2, k (k) K \u2212 2, b (k) K \u2212 2, b (k) K \u2212 2, b (k) K \u2212 2, b (k) K \u2212 2, b (k) K \u2212 2, b (k) K \u2212 2, k (k) K \u2212 2, k (k) K (k) K \u2212 2, k (k) K (k) K \u2212 2, K (k) K (k) K \u2212 2, K (k) K (k), k (k) K \u2212 2, k (k), k (k) K \u2212 2, k (k), k (k) K \u2212 2, k (k) K \u2212 2 (k), k \u2212 K \u2212 2 (k) K \u2212 2 (k), k \u2212 2 (k) K \u2212 2 (k), k (k) K \u2212 2, k (k) K \u2212 2, k (k) K \u2212 2, k (k) K \u2212 2, k (k) K \u2212 2, k (k) K \u2212 2, k (k) K \u2212 2 (k) K \u2212 2, k (k) K \u2212 2, k (k) K \u2212 2, k (k) K \u2212 2, k (k (k) K \u2212 2, k (k) K \u2212 2, k (k (k) K \u2212 2, k (k) K \u2212 2 \u2212 2, k (k), k (k (k) K \u2212 2, k (k), k (k (k) K \u2212 2 \u2212 2 \u2212 2 \u2212 2, K (k), k (k (k) K (k, k) K \u2212 2 \u2212 2 \u2212 2, K (k (k) K (k) K \u2212 2 \u2212 2, K (k, K (k) K (k) K \u2212 2 \u2212 2 \u2212 2, K (k (k) K (k (k) K (k) K \u2212 2 \u2212 2, K \u2212 2 \u2212 2, K (k) K (k) K (k"}], "references": [{"title": "Stochastic gradient learning in neural networks", "author": ["L\u00e9on Bottou"], "venue": "Proceedings of Neuro-N\u0131mes,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1991}, {"title": "Distributed optimization of deeply nested systems", "author": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "Weiran Wang"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Entropy-SGD: Biasing gradient descent into wide valleys", "author": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N. Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, ICCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi"], "venue": "In Field Guide to Dynamical Recurrent Networks", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Proceedings of the 25th International Conference of Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "On optimization methods for deep learning", "author": ["Quoc V Le", "Adam Coates", "Bobby Prochnow", "Andrew Y Ng"], "venue": "Proceedings of The 28th International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Deep learning via Hessian-free optimization", "author": ["James Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "R\u00e9gularisation d\u2019in\u00e9quations variationnelles par approximations successives", "author": ["B. Martinet"], "venue": "Rev. Francaise Inf. Rech. Oper., pages 154\u2013159,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1970}, {"title": "Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien", "author": ["Jean-Jacques Moreau"], "venue": "Bulletin de la Socie\u0301te\u0301 mathe\u0301matique de France,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1965}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Yurii Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1983}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer Series in Operations Research and Financial Engineering.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1951}, {"title": "Learning representations by backpropagating", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "errors. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "search. Nature,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["Jascha Sohl-Dickstein", "Ben Poole", "Surya Ganguli"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Proceedings of the 27th International Conference of Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Training neural networks without gradients: A scalable ADMM approach", "author": ["Gavin Taylor", "Ryan Burmeister", "Zheng Xu", "Bharat Singh", "Ankit Patel", "Tom Goldstein"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "In recent years neural networks have gained considerable attention in solving difficult correlation tasks such as classification in computer vision [8] or sequence learning [20] and as building blocks of larger learning systems [17].", "startOffset": 148, "endOffset": 151}, {"referenceID": 19, "context": "In recent years neural networks have gained considerable attention in solving difficult correlation tasks such as classification in computer vision [8] or sequence learning [20] and as building blocks of larger learning systems [17].", "startOffset": 173, "endOffset": 177}, {"referenceID": 16, "context": "In recent years neural networks have gained considerable attention in solving difficult correlation tasks such as classification in computer vision [8] or sequence learning [20] and as building blocks of larger learning systems [17].", "startOffset": 228, "endOffset": 232}, {"referenceID": 14, "context": "Since the introduction of stochastic gradient descent (SGD) [15, 1], several more sophisticated optimization methods have been studied.", "startOffset": 60, "endOffset": 67}, {"referenceID": 0, "context": "Since the introduction of stochastic gradient descent (SGD) [15, 1], several more sophisticated optimization methods have been studied.", "startOffset": 60, "endOffset": 67}, {"referenceID": 8, "context": "One such class is that of quasi-Newton methods, as for example the comparison of LBFGS with SGD in [9], Hessian-free approaches [10], and the Sum of Functions Optimizer in [18].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "One such class is that of quasi-Newton methods, as for example the comparison of LBFGS with SGD in [9], Hessian-free approaches [10], and the Sum of Functions Optimizer in [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "One such class is that of quasi-Newton methods, as for example the comparison of LBFGS with SGD in [9], Hessian-free approaches [10], and the Sum of Functions Optimizer in [18].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "Several works consider specific properties of energy landscapes of certain deep learning models such as frequent saddle points [4] and well-generalizable local optima [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Several works consider specific properties of energy landscapes of certain deep learning models such as frequent saddle points [4] and well-generalizable local optima [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov\u2019s Accelerated Gradient [13, 19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.", "startOffset": 177, "endOffset": 185}, {"referenceID": 18, "context": "Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov\u2019s Accelerated Gradient [13, 19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.", "startOffset": 177, "endOffset": 185}, {"referenceID": 6, "context": "Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov\u2019s Accelerated Gradient [13, 19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.", "startOffset": 210, "endOffset": 213}, {"referenceID": 5, "context": "Moreover, SGD often has difficulties in propagating a learning signal deeply into a network, commonly referred to as the vanishing gradient problem [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 1, "context": "Recently, the authors of [2] have tackled the problem of optimizing the nested objective function by explicitly introducing the network activations as variables of the optimization, also known as the method of auxiliary coordinates (MAC).", "startOffset": 25, "endOffset": 28}, {"referenceID": 20, "context": "[21] introduce additional auxiliary variables to further split linear and nonlinear transfer between layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In this work we draw a connection between the penalty formulations [2, 21] and the classical backpropagation algorithm [16].", "startOffset": 67, "endOffset": 74}, {"referenceID": 20, "context": "In this work we draw a connection between the penalty formulations [2, 21] and the classical backpropagation algorithm [16].", "startOffset": 67, "endOffset": 74}, {"referenceID": 15, "context": "In this work we draw a connection between the penalty formulations [2, 21] and the classical backpropagation algorithm [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "The gradient descent iteration on a nested function J(W , b;X, y) like (2), (W , b) = (W , b) \u2212 \u03c4\u2207J(W , b;X, y), (3) is commonly implemented using the backpropagation algorithm [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "1 Implicit gradient steps The proximal mapping of a function f : R \u2192 R is defined as the following [12]: prox\u03c4f (y) := argmin x\u2208Rn f(x) + 1 2\u03c4 ||x\u2212 y||.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "The advantage of such proximal steps is that in contrast to explicit gradient descent, the update scheme (8), also known as the proximal point algorithm [11], is unconditionally stable.", "startOffset": 153, "endOffset": 157}, {"referenceID": 4, "context": "1] as well as the adaptive initialization scheme suggested in [5], denoted as adaptive uniform and adaptive Gaussian respectively.", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "Algorithm 2 can be used as a gradient oracle for first-order methods (such as Adam [7]) or quasiNewton methods (such as SFO [18]), by using it to compute a direction d = (W , b) \u2212 (W , b) instead of the usual gradient.", "startOffset": 83, "endOffset": 86}, {"referenceID": 17, "context": "Algorithm 2 can be used as a gradient oracle for first-order methods (such as Adam [7]) or quasiNewton methods (such as SFO [18]), by using it to compute a direction d = (W , b) \u2212 (W , b) instead of the usual gradient.", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "Our method clearly outperforms SGD, and performs comparably to the recent work SFO [18] while being more memory efficient.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "The weights are initialized according to the uniform distribution scheme described in [5] and a weight-decay of 10 was chosen.", "startOffset": 86, "endOffset": 89}, {"referenceID": 17, "context": "We further compare the proposed method to SFO [18] and used the publicly available MATLAB implementation2 with 20 subfunctions (minibatch size of 2250).", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "Our method outperformsMAC [2] by several orders of magnitude (w.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "The methods denoted with an asterisk are based on code kindly provided by [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "To compare with MAC [2], we trained a fully connected autoencoder with the same architecture as for our experiments in Section 4.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "We used the same initialization as [2] with the initial weights in layer l being uniformly sampled from [\u22121/\u221anl\u22121, 1/\u221anl\u22121].", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "The authors of [2] kindly provided us with their MATLAB code to reproduce the results of their parallel MAC method, as well as the SGD and CG methods they compared to.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "We are therefore able to reproduce Figure 2 in [2] without the minibatch MAC on our computer with an Intel Core i7-6700HQ CPU with 2.", "startOffset": 47, "endOffset": 50}], "year": 2017, "abstractText": "We offer a generalized point of view on the backpropagation algorithm, currently the most common technique to train neural networks via stochastic gradient descent and variants thereof. Specifically, we show that backpropagation of a prediction error is equivalent to sequential gradient descent steps on a quadratic penalty energy. This energy comprises the network activations as variables of the optimization and couples them to the network parameters. Based on this viewpoint, we illustrate the limitations on step sizes when optimizing a nested function with gradient descent. Rather than taking explicit gradient steps, where step size restrictions are an impediment for optimization, we propose proximal backpropagation (ProxProp) as a novel algorithm that takes implicit gradient steps to update the network parameters. We experimentally demonstrate that our algorithm is robust in the sense that it decreases the objective function for a wide range of parameter values. In a systematic quantitative analysis, we compare to related approaches on a supervised visual learning task (CIFAR-10) for fully connected as well as convolutional neural networks and for an unsupervised autoencoder (USPS dataset). We demonstrate that ProxProp leads to a significant speed up in training performance.", "creator": "LaTeX with hyperref package"}}}