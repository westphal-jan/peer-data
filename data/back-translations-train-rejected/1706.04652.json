{"id": "1706.04652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Learning a visuomotor controller for real world robotic grasping using simulated depth images", "abstract": "We want to build robots that are useful in unstructured real world applications, such as doing work in the household. Grasping in particular is an important skill in this domain, yet it remains a challenge. One of the key hurdles is handling unexpected changes or motion in the objects being grasped and kinematic noise or other errors in the robot. This paper proposes an approach to learning a closed-loop controller for robotic grasping that dynamically guides the gripper to the object. We use a wrist-mounted sensor to acquire depth images in front of the gripper and train a convolutional neural network that directly learns the value function for grasp pose candidates. The training sensor data is generated in simulation, a major advantage over previous work that uses real robot experience, which is costly to obtain. Despite being trained in simulation, our approach works well on real noisy sensor images. We compare our controller in simulated and real robot experiments to a strong baseline for grasp pose detection, and find that our approach significantly outperforms the baseline in the presence of kinematic noise, perceptual errors and disturbances of the object during grasping.", "histories": [["v1", "Wed, 14 Jun 2017 19:50:09 GMT  (4976kb,D)", "https://arxiv.org/abs/1706.04652v1", null], ["v2", "Fri, 30 Jun 2017 21:18:20 GMT  (2772kb,D)", "http://arxiv.org/abs/1706.04652v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["ulrich viereck", "reas ten pas", "kate saenko", "robert platt"], "accepted": false, "id": "1706.04652"}, "pdf": {"name": "1706.04652.pdf", "metadata": {"source": "CRF", "title": "Learning a visuomotor controller for real world robotic grasping using simulated depth images", "authors": ["Ulrich Viereck", "Andreas ten Pas", "Robert Platt"], "emails": ["uliv@ccs.neu.edu", "rplatt@ccs.neu.edu", "atp@ccs.neu.edu", "saenko@bu.edu"], "sections": [{"heading": null, "text": "Keywords: robots, learning, manipulation"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 Related Work", "text": "Recent work in the field of grass root perception has used deep learning to locate grass root configurations in a manner similar to object recognition in computer vision [3, 7, 8, 4]. Such methods take potentially noisy sensor data as input and generate viable grass root estimates as output. However, these grass root recognition methods typically suffer from perception errors and inaccurate robotic kinematics [2]. Visual grass root methods use visual feedback to move a camera to a target position that is directly dependent on the object position. While there are numerous methods in this area [9], only a small number of previous work applies visual feedback directly to grasping [10, 11, 12]. In contrast to our work, existing methods require manual feature design or specifications. An active vision approach by Arruet al. acquires sensory data from different viewing angles to optimize the surface design [13]."}, {"heading": "3 Approach", "text": "We propose a new approach to the problem of learning a visuomotor control for robot gripping, inspired by the method of Levine et al. [5]. We mount a depth sensor near the wrist of the robot, as shown in Figure 1. At each control step, the system takes a depth image of the scene directly in front of the gripper and uses this sensor information to guide the hand. The controller converts to good gripper configurations from which the grippers can close and pick up the object. The approach is based on a revolutionary neural network that learns a distance function. It takes the depth image as input in conjunction with a hand shift of the candidate and provides an estimate of the distance to the next gripper as output. Figure 2 shows an overview of the approach. The key elements are: 1) the revolutionary neural network that is used to model the distance function (Section 3.1); 2) the approach to the simulation of the training set (Section 3.3); and 3) the simulation of the simulation of the implantation (3)."}, {"heading": "3.1 CNN Model", "text": "In fact, it is the case that it is a real phenomenon that has developed in recent years, namely in the way that it has occurred in recent years: in the way that it has happened in the USA and other countries, in the way that it has happened in the USA, in the way that it has happened in the USA, and in the way that it has happened in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the way that it has happened in the USA, in the way that it has happened in the United States of America, in the way that it has happened in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the way that it has happened in the USA, in the USA, in the way that it has happened in the USA, in the USA, in the way that it has happened in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3.3 Controller", "text": "Our controller takes measures that reduce the distance function modeled by the CNN object described in Section 3.1. Its basic function is outlined in Algorithm 1. The controller starts with the hand at a fixed initial height above the table in z-direction. In Step 3, the controller captures an image of the depth sensor attached to the wrist. In Step 4, it scans a series of actions by the candidate and selects those with the minimum distance from the next grip. In Step 5, the controller moves by a constant fraction size toward the selected action. The fact that the controller makes only a fractional motion at each time step smoothes out the motion and makes the controller more robust to detect isolated bad predictions by CNN. In Step 6, the controller approaches the object in z-direction by one step. This process repeats until the controller reaches the final hand height. 5An important point is that we limit the scanning to a region around the origin."}, {"heading": "4 Simulation Experiments", "text": "We perform a series of simulation experiments to evaluate our new Grip Controller (CTR) in relation to Grip Position Detection (GPD), a recently proposed one-shot method that also learns in the simulation and achieves high success rates [2]. We perform this comparison for two scenarios: one in which the manipulator moves exactly as commanded, and one in which the desired movements of the manipulator are corrupted by Gaussian median noise. All the following simulation data are averages of over 400 attempts. In each attempt, we create a scene in OpenRAVE with a random selection and placement of objects from the test set, as described in Section 3.2. The starting position of the camera is set to 0.3 m above the table. At each iteration, the camera height is reduced by a constant step until the height of 0.15 m is reached. We perform the controller for a total of 75 griterations of the test, with the final step ratio of 0.2 being reached to the target position and 0.15 to the actual object."}, {"heading": "4.1 Comparison with GPD baseline in the presence of kinematic noise", "text": "We compare the following scenarios: 1) INIT: No camera movement, distances measured from the starting position; 2) CT Rno noise: Run CTR from the starting position, without kinematic noise; 3) CT Rwith noise: Run CTR from the starting position, with kinematic noise; 4) GPDno noise: Move totop GPD prediction, without kinematic noise; 5) GPDwith noise: Move to top GPD prediction, with kinematic noise. The scenario \"with noise\" simulates the situation in which zero means Gaussian noise is added to the robotic hand shift without taking kinematic noise into account; 5) GPDwith noise = (x, y, \u03b8) + 0.4w (x, y, \u03b8)."}, {"heading": "4.2 Correction for perceptual errors made in single-shot prediction using the controller", "text": "Next, we compare the following two scenarios to characterize the benefits of the closed-loop controller over one-shot detection: 1) CNN only: Move to the top one-shot global prediction using the CNN regression model; 2) CNN + CT R: Move to the top one-shot global prediction and then run the controller. Figure 5 (right) shows that the controller improves the performance of our approach, even in a scenario without kinematic noise. This suggests that the controller can compensate for perceptual errors made in a single depth image, confirming similar results obtained by Levine et al. in [5]."}, {"heading": "5 Robot Experiments", "text": "It is indeed the case that we are able to outdo ourselves in all the areas in which we find ourselves and in all the areas in which we operate."}, {"heading": "5.2 Grasping objects in dense clutter", "text": "An example of such a scenario is shown in Figure 7. A run is terminated when three consecutive attempts result in gripping errors or when the remaining objects are out of the sensor's field of vision. In total, we perform ten passes of this experiment.The robot attempted 74 gripping attempts with our controller over the ten passes of this experiment, of which 74 attempts are eight failures (88.89% gripping error success rate).Five of the eight failures are caused by the object slipping out of its fingers during gripping, two are caused by slight misalignments in the last pose, and one is caused by a collision between a finger and the object that moved the object out of the closing region of the hand. In comparison, our GPD method would cause 96 grips over ten passes of this experiment. Only five of these attempts were successful (two failures were not a success rate), while we achieve a similar trigger rate."}, {"heading": "5.3 Grasping objects with changing orientations", "text": "This experiment evaluates the performance of our controller compared to the GPD baseline for a dynamic scenario in which the human experimenter manually shifts the position of the objects once during each gripping experiment. To achieve this, we pour the pile of overcrowded objects onto a sheet of paper and move the paper in random quantities after the third iteration of the controller. During the ten runs of this experiment, the robot attempted 75 grips with our controller. 17 of these 75 attempts were failures (77.33% gripping success rate). In comparison, the GPD attempted only 49 grips, of which 38 were failures (22.45% grasp success rate).The better performance of our controller makes sense because it is able to respond to the shift while the GPD cannot: it simply carries on gripping as if the object stack had not been shifted. This is a general advantage of a closed-loop controller over typical gripping perception methods [4, 7, 7]."}, {"heading": "6 Discussion", "text": "We developed a visual-motor controller that uses visual feedback from the gripper-mounted depth sensor to dynamically correct misalignment with the object when gripping. We trained a deep CNN model with simulated sensor data that directly learns the distance function for a given depth image and represents a plot. Generating training data in the simulation was more efficient than generating it on a real robot. We found that the CNN model, which was trained with simulated depth images, translates well into the realm of real sensor images after processing the images to correct invalid depth measurements. In addition, our controller was able to respond to object shifts and inaccurate placement of the handle relative to the object to be buried using the CNN model with the CNN model. In simulation experiments, our approach compensated for significant disruptive kinematics, while a bulky PD baseline did not work well with the controller that CNN corrected perceptual noise in the robot."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the National Science Foundation through IIS-1427081, NASA through NNX16AC48A and NNX13AQ85G, and ONR through N000141410047."}], "references": [{"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "High precision grasp pose detection in dense clutter", "author": ["M. Gualtieri", "A. ten Pas", "K. Saenko", "R. Platt"], "venue": "In Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "IJRR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "In Proc. IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "In Proc. International Symposium on Experimental Robotics (ISER),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Openrave: A planning architecture for autonomous robotics", "author": ["R. Diankov", "J. Kuffner"], "venue": "Technical Report CMU-RI-TR-08-34, Robotics Institute,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Leveraging big data for grasp planning", "author": ["D. Kappler", "J. Bohg", "S. Schaal"], "venue": "In Proc. IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Real-time grasp detection using convolutional neural networks", "author": ["J. Redmon", "A. Angelova"], "venue": "In Proc. IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Springer Handbook of Robotics", "author": ["B. Siciliano", "O. Khatib"], "venue": "Springer-Verlag New York, Inc., Secaucus, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Visual servoing for humanoid grasping and manipulation tasks", "author": ["N. Vahrenkamp", "S. Wieland", "P. Azad", "D.I. Gonzalez-Aguirre", "T. Asfour", "R. Dillmann"], "venue": "In Proc. IEEE-RAS International Conference on Humanoid Robots,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Combined shape, appearance and silhouette for simultaneous manipulator and object tracking", "author": ["P. Hebert", "N. Hudson", "J. Ma", "T. Howard", "T. Fuchs", "M. Bajracharya", "J. Burdick"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Computer vision for fruit harvesting robots \u2013 state of the art and challenges ahead", "author": ["K. Kapach", "E. Barnea", "R. Mairon", "Y. Edan", "O. Ben-Shahar"], "venue": "International Journal of Computational Vision and Robotics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Active vision for dexterous grasping of novel objects", "author": ["E. Arruda", "J. Wyatt", "M. Kopicki"], "venue": "In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "3dnet: Large-scale object class recognition from cad models", "author": ["W. Wohlkinger", "A. Aldoma Buchaca", "R. Rusu", "M. Vincze"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Using geometry to detect grasp poses in 3d point clouds", "author": ["A. ten Pas", "R. Platt"], "venue": "In Proc. of the International Symposium on Robotics Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Constructing force-closure grasps", "author": ["V.D. Nguyen"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1986}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proc. 22nd ACM International Conference on Multimedia,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recently, deep neural networks have been used to learn a variety of visuomotor skills for robotic manipulation including grasping, screwing a top on a bottle, mating a mega-block, and hanging a loop of rope on a hook [1].", "startOffset": 217, "endOffset": 220}, {"referenceID": 1, "context": "A number of researchers have recently proposed using deep learning for robotic grasping systems that perform well for novel objects presented in dense clutter [2, 3, 4].", "startOffset": 159, "endOffset": 168}, {"referenceID": 2, "context": "A number of researchers have recently proposed using deep learning for robotic grasping systems that perform well for novel objects presented in dense clutter [2, 3, 4].", "startOffset": 159, "endOffset": 168}, {"referenceID": 3, "context": "A number of researchers have recently proposed using deep learning for robotic grasping systems that perform well for novel objects presented in dense clutter [2, 3, 4].", "startOffset": 159, "endOffset": 168}, {"referenceID": 1, "context": "However, these systems still do not perform as well as we would like, achieving maximum grasp success rates of approximately 85% to 93% in ideal conditions [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "[5] used supervised deep networks to learn a closed-loop control policy for grasping novel objects in clutter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "While depth data is potentially less informative than RGB data, it can be simulated relatively accurately using ray tracing (we use OpenRAVE [6]).", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "[5], but takes images at a lower resolution and has many fewer layers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Our major finding is that in the absence of motor or sensor noise, our closed-loop grasp controller has similar performance to a recently developed grasp detection method [2] with very high grasp success rates.", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "Recent work in grasp perception has utilized deep learning to localize grasp configurations in a way that is analogous to object detection in computer vision [3, 7, 8, 4].", "startOffset": 158, "endOffset": 170}, {"referenceID": 6, "context": "Recent work in grasp perception has utilized deep learning to localize grasp configurations in a way that is analogous to object detection in computer vision [3, 7, 8, 4].", "startOffset": 158, "endOffset": 170}, {"referenceID": 7, "context": "Recent work in grasp perception has utilized deep learning to localize grasp configurations in a way that is analogous to object detection in computer vision [3, 7, 8, 4].", "startOffset": 158, "endOffset": 170}, {"referenceID": 3, "context": "Recent work in grasp perception has utilized deep learning to localize grasp configurations in a way that is analogous to object detection in computer vision [3, 7, 8, 4].", "startOffset": 158, "endOffset": 170}, {"referenceID": 1, "context": "However, these grasp detection methods typically suffer from perceptual errors and inaccurate robot kinematics [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "While there are numerous methods in this area [9], only a small amount of previous work addresses using visual feedback directly for grasping [10, 11, 12].", "startOffset": 46, "endOffset": 49}, {"referenceID": 9, "context": "While there are numerous methods in this area [9], only a small amount of previous work addresses using visual feedback directly for grasping [10, 11, 12].", "startOffset": 142, "endOffset": 154}, {"referenceID": 10, "context": "While there are numerous methods in this area [9], only a small amount of previous work addresses using visual feedback directly for grasping [10, 11, 12].", "startOffset": 142, "endOffset": 154}, {"referenceID": 11, "context": "While there are numerous methods in this area [9], only a small amount of previous work addresses using visual feedback directly for grasping [10, 11, 12].", "startOffset": 142, "endOffset": 154}, {"referenceID": 12, "context": "acquires sensor data from different view points to optimize surface reconstruction for reliable grasping during grasp planning [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "were one of the first to incorporate deep learning for grasp perception using visual feedback [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Our CNN is based on the LeNet network designed for handwritten digit classification [14].", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "[5] we apply an IP layer to the input pose vector (action) and then tile the resulting output over the spatial dimensions to match the dimensions of the Pool1 layer and sum elementwise.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We create a dataset in simulation using OpenRAVE [6] comprised of image-offset pairs and the corresponding distance-to-nearest-grasp labels.", "startOffset": 49, "endOffset": 52}, {"referenceID": 14, "context": "The objects were derived from CAD models contained within the 3DNet database [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "First, using the mesh model of the scene, we sample a large number of grasp candidates by filtering for robotic hand poses that are collision free and that contain parts of the visible environment between the robotic fingers (see [16]).", "startOffset": 230, "endOffset": 234}, {"referenceID": 16, "context": "Then, we test each candidate for force closure using standard methods [17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "We perform a series of experiments in simulation to evaluate our new grasp controller (CTR) relative to grasp pose detection (GPD), a recently proposed one-shot method that also learns in simulation and achieves high success rates [2].", "startOffset": 231, "endOffset": 234}, {"referenceID": 17, "context": "We use the deep learning framework Caffe [18] for training the network.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "in [5].", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "grasp pose detection (GPD), a strong baseline [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "IKFast [6] is used to convert the selected action (i.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "Then, we select and execute one of the detected grasps based on the heuristics outlined in [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Table 1: Average grasp success rates for our controller (CTR) and a recent grasp pose detection (GPD) method [2] on the UR5 robot.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "This is a general advantage of a closed-loop controller relative to typical grasp perception methods [4, 8, 3, 7, 2].", "startOffset": 101, "endOffset": 116}, {"referenceID": 7, "context": "This is a general advantage of a closed-loop controller relative to typical grasp perception methods [4, 8, 3, 7, 2].", "startOffset": 101, "endOffset": 116}, {"referenceID": 2, "context": "This is a general advantage of a closed-loop controller relative to typical grasp perception methods [4, 8, 3, 7, 2].", "startOffset": 101, "endOffset": 116}, {"referenceID": 6, "context": "This is a general advantage of a closed-loop controller relative to typical grasp perception methods [4, 8, 3, 7, 2].", "startOffset": 101, "endOffset": 116}, {"referenceID": 1, "context": "This is a general advantage of a closed-loop controller relative to typical grasp perception methods [4, 8, 3, 7, 2].", "startOffset": 101, "endOffset": 116}], "year": 2017, "abstractText": "We want to build robots that are useful in unstructured real world applications, such as doing work in the household. Grasping in particular is an important skill in this domain, yet it remains a challenge. One of the key hurdles is handling unexpected changes or motion in the objects being grasped and kinematic noise or other errors in the robot. This paper proposes an approach to learning a closed-loop controller for robotic grasping that dynamically guides the gripper to the object. We use a wrist-mounted sensor to acquire depth images in front of the gripper and train a convolutional neural network to learn a distance function to true grasps for grasp configurations over an image. The training sensor data is generated in simulation, a major advantage over previous work that uses real robot experience, which is costly to obtain. Despite being trained in simulation, our approach works well on real noisy sensor images. We compare our controller in simulated and real robot experiments to a strong baseline for grasp pose detection, and find that our approach significantly outperforms the baseline in the presence of kinematic noise, perceptual errors and disturbances of the object during grasping.", "creator": "LaTeX with hyperref package"}}}