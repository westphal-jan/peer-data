{"id": "1706.04701", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong", "abstract": "Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.", "histories": [["v1", "Thu, 15 Jun 2017 00:13:28 GMT  (356kb,D)", "http://arxiv.org/abs/1706.04701v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["warren he", "james wei", "xinyun chen", "nicholas carlini", "dawn song"], "accepted": false, "id": "1706.04701"}, "pdf": {"name": "1706.04701.pdf", "metadata": {"source": "CRF", "title": "Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong", "authors": ["Warren He", "James Wei", "Xinyun Chen", "Nicholas Carlini", "Dawn Song"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for us to be able to look for a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution. \""}, {"heading": "2 Overview", "text": "We start with an overview of background information, then define the threat models we use, and define the problem definition and setup for our experiments."}, {"heading": "2.1 Background: Adversarial Examples", "text": "Recent work has pointed out that models of deep learning are prone to contradictory examples: These models make false predictions about inputs that differ slightly from those correctly predicted [28, 7, 23, 26]. Suppose we have a classifier F with model parameters \u03b8. Let x be input to the classifier with a corresponding prediction of truth y. a contradictory example x is an input instance that comes close to x metrically d (x, x) by a certain distance, but causes the classifier to produce an incorrect output. Here, we consider only the x-satisfactory methods (x) = y.Primary work takes into account two classes of contradictory examples. Firstly, an untargeted contradictory example is an instance x that causes the classifier to produce an incorrect output: F\u03b8 (x) 6 = y.Secondly, a targeted contradictory example is a Fx method that causes the classifier to produce an incorrect output [perfect output]."}, {"heading": "2.2 Threat Models", "text": "In this paper, we assume that the adversary has complete knowledge of the model, including the model architecture, parameters, and defense strategies used in the model. Previous work has shown that this assumption can often be loosened [7, 24, 25], but for simplicity's sake, we assume this stronger threat model.Within these white box attackers, we look at two adversaries \"capacities. Static adversary: A static adversary is an attacker who is unaware of any defense mechanisms that may be in place to protect the model from adversarial examples. A static adversary can use existing methods to generate adversarial examples, but does not tailor attacks to a specific defense.Adaptive adversary: An adaptive adversary is an attacker who is aware of the defense methods used in the model and can adapt attacks accordingly."}, {"heading": "2.3 Problem Statement", "text": "In order to improve the robustness of models against opposing examples, we examine two directions before working. The first direction tries to produce correct predictions about opposing examples without compromising the accuracy of legitimate inputs; the second (newer) direction instead tries to detect opposing examples without introducing too many false positives. In this case, the model can reject an instance and refuse to classify those it recognizes as counterproductive. [21, 8, 30, 1] In this paper, we ask whether we apply multiple defense mechanisms against opposing examples, then the combined defense will be significantly stronger than any single original defense."}, {"heading": "2.4 Experimental Setup", "text": "To evaluate the effectiveness of the various defense strategies, we use two standard datasets, MNIST [18] and CIFAR-10 [16] datasets. For both datasets, we sample 100 images from the test set, filter out examples that are not correctly classified, and generate opposing examples based on the correctly classified images. In evaluating each defense strategy, we use the same model architectures described in their papers [30, 1, 6, 21, 5]. Adversarial examples on MNIST tend to have higher distortions than natural images. Adversarial example generation method. In this paper, we use an optimization-based approach to generate adversarial examples [3] that prove effective in searching for adversarial examples with small distortions."}, {"heading": "3 Adaptive attacks on feature squeezing", "text": "In this and the next section, we examine ensemble defense strategies that are deliberately constructed to have component defenses that work together to detect enemy examples. The first defense we study is squeezing, proposed by Xu et al. [30, 31]. Background: Feature Squeezing Defense, one generates a lower fixation version of the input image through a process known as \"squeezing\" before it is introduced into the classifier."}, {"heading": "3.1 Evading individual feature squeezing defense components", "text": "In these experiments, we assess whether contradictory examples for each component of the squeeze component are robust, i.e. whether contradictory examples remain contradictory even after each individual squeeze process (color depth reduction and spatial smoothing).These experiments attack the components of the combined squeeze component.This attack is necessary to overcome the combined recognition scheme in which the predicted label probabilities of squeezed images are compared with each other."}, {"heading": "3.1.1 Evading color-depth-reduction defense", "text": "The first method of squeezing an image that Xu al. suggests is the reduction of the color depth. This method rounds up the input to 2b even values that cover the same range, which we refer to the reduction of Bitcoins. We check whether a reduced color depth of the image is an adequate solution. We perform the optimization multiple times by applying random perturbations to the image so that it is able to determine the depth of the image."}, {"heading": "3.1.2 Evading spatial smoothing", "text": "Xu et al. propose a second method for feature squeezing, which applies a median filter to the input, replacing each pixel with the median value of a neighborhood around the pixel. To generate contrary examples that are incorrectly classified after spatial smoothing, we use the method from Section 2.4 with the addition of a median filter as part of the classification model. A median filter for TensorFlow was not available, so we implement our own. Attack results on MNIST are successfully evaluated. We evaluate a range of median filter sizes ranging from 1 x 2 to 5 x 5. For a 3 x 3 filter, with which Xu et al. achieved the best accuracy, we have created contrarian examples for all original images, with an average distortion of 1.29. Figure 3 shows a sample of these contrarian filter sizes that are contradictory."}, {"heading": "3.2 Evading combination of multiple squeezing techniques", "text": "This year, the time has come to ask only one question: \"What kind of country is it?,\" he asked, \"What kind of country is it?,\" \"What kind of country is it?,\" \"What kind of country is it?,\" \"What kind of country is it?,\" \"What kind of country is it?,\" \"What kind of country is it?,\" \"What kind of country is it?,\" he asked."}, {"heading": "4 Evading ensemble of specialists", "text": "We will study a second defense that combines multiple component defenses, an interplay of specialists proposed by Abbasi and Gagne to classify the classes. We will study a second defense that combines multiple component defenses, an interaction of specialists proposed by Abbasi and Gagne. Specialists will classify subgroups of classes as follows: Where C is the set of all K classes in the task, for each class there is a series of classes with which I am most often confused in adversarial examples. To calculate Ui, Abbasi and Gagne, select the top 80% of the failures caused by non-targeted FGSM attacks. Further, K additional subgroups will be defined: UK + i = C\\ Ui the complement group of Ui. For each group j = 1, 2K, a specialized classifier Fj is trained on a series of dataset classes."}, {"heading": "5 Evading ensemble of detectors", "text": "In the previous sections, we have examined ensembles of defense systems that are deliberately constructed to be useful together. In Xu et al. \"s work, the color depth is reduced to remove small changes to many pixels, and the median smoothing is reduced to avoid larger changes to a few pixels. Similarly, we will merge a number of recently proposed detectors that are not used in conjunction with another detector. We will only look at detectors that are applied to a fixed classification network for simplicity, and therefore examine the effectiveness of ensembling defense systems that are used in conjunction with other detectors."}, {"heading": "6 Conclusion", "text": "Our goal in this paper is to investigate whether several (possibly weak) defense mechanisms can be combined to create a strong defense. To achieve this goal, we have investigated three such defense mechanisms that combine multiple defense components: two recently proposed defense mechanisms designed on the grounds that their components should work well together, and one that combines disjointed recently proposed detectors. We have shown that an adaptive adversary can produce hostile examples with low distortion that fool all of the defense components and components we evaluated. The feature squashing detection scheme, which combines two methods of squeezing an input image, is at best slightly stronger than the pure color depth reduction. The Specialists + 1 group, which combines several specialized classifiers, slightly increases the required distortion, but here, the distortion is still low. We have also shown that the combination of a collection of recently proposed defense mechanisms can be applied to our counter-ineffectiveness, in particular."}, {"heading": "7 Acknowledgements", "text": "We would like to thank Arjun Bhagoji, Chang Liu and Richard Shin for providing valuable feedback, which has been partially supported by BDD (Berkeley Deep Drive), the CLTC (Center for Long-Term Cybersecurity), FORCES (Foundations Of Resilient Cyber-Physical Systems), which is supported by the National Science Foundation (NSF prize numbers CNS-1238959, CNS1238962, CNS-1239054, CNS-1239166), and the National Science Foundation under grant number TWC1409915. Any opinions, results, conclusions or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of the National Science Foundation. Cloud computing resources have been made available through a Microsoft Azure for Research Award."}, {"heading": "A Gumbel-Softmax reparameterization", "text": "In this context, it should be noted that the case is an accident."}], "references": [{"title": "Robustness to adversarial examples through an ensemble of specialists", "author": ["M. ABBASI", "C. GAGN\u00c9"], "venue": "arXiv preprint arXiv:1702.06856", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Vulnerability of deep reinforcement learning to policy induction attacks", "author": ["V. BEHZADAN", "A. MUNIR"], "venue": "arXiv preprint arXiv:1701.04143", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Towards evaluating the robustness of neural networks", "author": ["N. CARLINI", "D. WAGNER"], "venue": "arXiv preprint arXiv:1608.04644", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Adversarial examples are not easily detected: Bypassing ten detection methods", "author": ["N. CARLINI", "D. WAGNER"], "venue": "arXiv preprint arXiv:1705.07263", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Detecting adversarial samples from artifacts", "author": ["R. FEINMAN", "R.R. CURTIN", "S. SHINTRE", "A.B. GARD- NER"], "venue": "arXiv preprint arXiv:1703.00410", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Adversarial and clean data are not twins", "author": ["Z. GONG", "W. WANG", "KU", "W.-S"], "venue": "arXiv preprint arXiv:1704.04960", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. GOODFELLOW", "J. SHLENS", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1412.6572", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "On the (statistical) detection of adversarial examples", "author": ["K. GROSSE", "P. MANOHARAN", "N. PAPERNOT", "M. BACKES", "P. MCDANIEL"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. GU", "L. RIGAZIO"], "venue": "arXiv preprint arXiv:1412.5068", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "In Proceedings of the IEEE international conference on computer vision", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Early methods for detecting adversarial images", "author": ["D. HENDRYCKS", "K. GIMPEL"], "venue": "International Conference on Learning Representations (Workshop Track)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Adversarial attacks on neural network policies", "author": ["S. HUANG", "N. PAPERNOT", "I. GOODFELLOW", "Y. DUAN", "P. ABBEEL"], "venue": "arXiv preprint arXiv:1702.02284", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["E. JANG", "S. GU", "B. POOLE"], "venue": "arXiv preprint arXiv:1611.01144", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Adversarial examples for generative models", "author": ["J. KOS", "I. FISCHER", "D. SONG"], "venue": "arXiv preprint arXiv:1702.06832", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Delving into adversarial attacks on deep policies", "author": ["J. KOS", "D. SONG"], "venue": "5th International Conference on Learning Representations (ICLR) Workshop", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. KRIZHEVSKY"], "venue": "Tech. rep.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Adversarial examples in the physical world", "author": ["A. KURAKIN", "I. GOODFELLOW", "S. BENGIO"], "venue": "arXiv preprint arXiv:1607.02533", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LECUN", "L. BOTTOU", "Y. BENGIO", "P. HAFFNER"], "venue": "Proceedings of the IEEE 86,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Y. LIU", "X. CHEN", "C. LIU", "D. SONG"], "venue": "5th International Conference on Learning Representations (ICLR)", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["C.J. MADDISON", "A. MNIH", "TEH", "Y. W"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "On detecting adversarial perturbations", "author": ["J.H. METZEN", "T. GENEWEIN", "V. FISCHER", "B. BISCHOFF"], "venue": "arXiv preprint arXiv:1702.04267", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["MOOSAVI-DEZFOOLI", "S.-M", "A. FAWZI", "P. FROSSARD"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. NGUYEN", "J. YOSINSKI", "J. CLUNE"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. PAPERNOT", "P. MCDANIEL", "I. GOODFELLOW"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. PAPERNOT", "P. MCDANIEL", "I. GOODFELLOW", "S. JHA", "Z.B. CELIK", "A. SWAMI"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. PAPERNOT", "P. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI"], "venue": "IEEE European Symposium on Security and Privacy (EuroS&P) (2016),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. PAPERNOT", "P. MCDANIEL", "X. WU", "S. JHA", "A. SWAMI"], "venue": "In Security and Privacy (SP),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ERHAN", "I. GOODFELLOW", "R. FERGUS"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W. XIONG", "J. DROPPO", "X. HUANG", "F. SEIDE", "M. SELTZER", "A. STOLCKE", "D. YU", "G. ZWEIG"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Feature squeezing: Detecting adversarial examples in deep neural networks", "author": ["XU W", "EVANS D", "QI"], "venue": "arXiv preprint arXiv:1704.01155", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}], "referenceMentions": [{"referenceID": 9, "context": "Neural networks have achieved great performance on a wide range of application domains; in particular, they have demonstrated accuracies comparable or better than humans on datasets in the fields of image recognition [10] and speech recognition [29].", "startOffset": 217, "endOffset": 221}, {"referenceID": 28, "context": "Neural networks have achieved great performance on a wide range of application domains; in particular, they have demonstrated accuracies comparable or better than humans on datasets in the fields of image recognition [10] and speech recognition [29].", "startOffset": 245, "endOffset": 249}, {"referenceID": 27, "context": "However, recent work shows that deep learning models are susceptible to adversarial examples: inputs that are similar to a correctly classified input, but which are misclassified [28].", "startOffset": 179, "endOffset": 183}, {"referenceID": 14, "context": "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].", "startOffset": 212, "endOffset": 227}, {"referenceID": 13, "context": "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].", "startOffset": 212, "endOffset": 227}, {"referenceID": 11, "context": "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].", "startOffset": 212, "endOffset": 227}, {"referenceID": 1, "context": "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].", "startOffset": 212, "endOffset": 227}, {"referenceID": 16, "context": "Recent work has shown that adversarial examples remain even when subject to the lossy channel of being photographed [17].", "startOffset": 116, "endOffset": 120}, {"referenceID": 26, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 6, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 8, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 20, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 7, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 5, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 4, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 10, "context": "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.", "startOffset": 22, "endOffset": 49}, {"referenceID": 29, "context": "First, we study two recently proposed defenses, feature squeezing [30] and the specialists+1 ensemble method [1], each of which ensembles multiple defenses that compensate for each other\u2019s weaknesses.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "First, we study two recently proposed defenses, feature squeezing [30] and the specialists+1 ensemble method [1], each of which ensembles multiple defenses that compensate for each other\u2019s weaknesses.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "To study the question of ensemble defense in a broader scope, we also evaluate an ensemble of three independent, mutually compatible detection mechanisms [6, 21, 5].", "startOffset": 154, "endOffset": 164}, {"referenceID": 20, "context": "To study the question of ensemble defense in a broader scope, we also evaluate an ensemble of three independent, mutually compatible detection mechanisms [6, 21, 5].", "startOffset": 154, "endOffset": 164}, {"referenceID": 4, "context": "To study the question of ensemble defense in a broader scope, we also evaluate an ensemble of three independent, mutually compatible detection mechanisms [6, 21, 5].", "startOffset": 154, "endOffset": 164}, {"referenceID": 29, "context": "\u2022 We create effective attacks on feature squeezing [30], including individual squeezing methods and the combined adversarial example detection scheme.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "\u2022 We create an effective attack on the ensemble-ofspecialists defense [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 27, "context": "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].", "startOffset": 210, "endOffset": 225}, {"referenceID": 6, "context": "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].", "startOffset": 210, "endOffset": 225}, {"referenceID": 22, "context": "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].", "startOffset": 210, "endOffset": 225}, {"referenceID": 25, "context": "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].", "startOffset": 210, "endOffset": 225}, {"referenceID": 6, "context": "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].", "startOffset": 103, "endOffset": 106}, {"referenceID": 18, "context": "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].", "startOffset": 129, "endOffset": 133}, {"referenceID": 25, "context": "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].", "startOffset": 194, "endOffset": 198}, {"referenceID": 27, "context": "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].", "startOffset": 231, "endOffset": 242}, {"referenceID": 2, "context": "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].", "startOffset": 231, "endOffset": 242}, {"referenceID": 18, "context": "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].", "startOffset": 231, "endOffset": 242}, {"referenceID": 6, "context": "Prior work has shown this assumption can often be relaxed [7, 24, 25], however for simplicity we assume this stronger threat model.", "startOffset": 58, "endOffset": 69}, {"referenceID": 23, "context": "Prior work has shown this assumption can often be relaxed [7, 24, 25], however for simplicity we assume this stronger threat model.", "startOffset": 58, "endOffset": 69}, {"referenceID": 24, "context": "Prior work has shown this assumption can often be relaxed [7, 24, 25], however for simplicity we assume this stronger threat model.", "startOffset": 58, "endOffset": 69}, {"referenceID": 26, "context": "The first direction attempts to produce correct predictions on adversarial examples, while not compromising the accuracy on legitimate inputs [27, 7, 9].", "startOffset": 142, "endOffset": 152}, {"referenceID": 6, "context": "The first direction attempts to produce correct predictions on adversarial examples, while not compromising the accuracy on legitimate inputs [27, 7, 9].", "startOffset": 142, "endOffset": 152}, {"referenceID": 8, "context": "The first direction attempts to produce correct predictions on adversarial examples, while not compromising the accuracy on legitimate inputs [27, 7, 9].", "startOffset": 142, "endOffset": 152}, {"referenceID": 20, "context": "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].", "startOffset": 107, "endOffset": 121}, {"referenceID": 7, "context": "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].", "startOffset": 107, "endOffset": 121}, {"referenceID": 29, "context": "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].", "startOffset": 107, "endOffset": 121}, {"referenceID": 0, "context": "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].", "startOffset": 107, "endOffset": 121}, {"referenceID": 29, "context": "First and second are feature squeezing [30] and the specialists+1 ensemble method [1], both of which take this approach by construction.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "First and second are feature squeezing [30] and the specialists+1 ensemble method [1], both of which take this approach by construction.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "In particular, as an example demonstration, we ensemble three independent detection mechanisms [6, 21, 5] to build one detection mechanism.", "startOffset": 95, "endOffset": 105}, {"referenceID": 20, "context": "In particular, as an example demonstration, we ensemble three independent detection mechanisms [6, 21, 5] to build one detection mechanism.", "startOffset": 95, "endOffset": 105}, {"referenceID": 4, "context": "In particular, as an example demonstration, we ensemble three independent detection mechanisms [6, 21, 5] to build one detection mechanism.", "startOffset": 95, "endOffset": 105}, {"referenceID": 17, "context": "To evaluate the effectiveness of the different defense strategies, we use two standard datasets, MNIST [18] and CIFAR-10 [16] datasets.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "To evaluate the effectiveness of the different defense strategies, we use two standard datasets, MNIST [18] and CIFAR-10 [16] datasets.", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].", "startOffset": 114, "endOffset": 131}, {"referenceID": 0, "context": "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].", "startOffset": 114, "endOffset": 131}, {"referenceID": 5, "context": "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].", "startOffset": 114, "endOffset": 131}, {"referenceID": 20, "context": "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].", "startOffset": 114, "endOffset": 131}, {"referenceID": 4, "context": "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].", "startOffset": 114, "endOffset": 131}, {"referenceID": 2, "context": "In this paper, we use an optimization-based approach to generate adversarial examples [3], which is shown to be effective on finding adversarial examples with small distortions.", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "We omit details of the design choice and refer the reader to the original paper [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Each dimension of input images is scaled to [0,1], i.", "startOffset": 44, "endOffset": 49}, {"referenceID": 29, "context": "[30, 31].", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "On adversarial examples generated by a static adversary using FGSM [7] and JSMA [26], they show that their detector achieves 99.", "startOffset": 67, "endOffset": 70}, {"referenceID": 25, "context": "On adversarial examples generated by a static adversary using FGSM [7] and JSMA [26], they show that their detector achieves 99.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "We study a second defense that combines multiple component defenses, an ensemble of specialists, proposed by Abbasi and Gagn\u00e9 [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "Abbasi and Gagn\u00e9 [1] find that using an ensemble constructed this way successfully reduces the system\u2019s confidence (fraction of voting classifiers that voted for the winning class) on adversarial examples generated by a static attacker using FGSM [7], DeepFool [22], and Szegedy et al.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "Abbasi and Gagn\u00e9 [1] find that using an ensemble constructed this way successfully reduces the system\u2019s confidence (fraction of voting classifiers that voted for the winning class) on adversarial examples generated by a static attacker using FGSM [7], DeepFool [22], and Szegedy et al.", "startOffset": 247, "endOffset": 250}, {"referenceID": 21, "context": "Abbasi and Gagn\u00e9 [1] find that using an ensemble constructed this way successfully reduces the system\u2019s confidence (fraction of voting classifiers that voted for the winning class) on adversarial examples generated by a static attacker using FGSM [7], DeepFool [22], and Szegedy et al.", "startOffset": 261, "endOffset": 265}, {"referenceID": 27, "context": "\u2019s approach [28].", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "propose using adversarial training to detect adversarial examples [6].", "startOffset": 66, "endOffset": 69}, {"referenceID": 20, "context": "construct a similar scheme, however instead of using the original images as the input to the detector, they train on the inner convolutional layers of the network [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 4, "context": "examine the final hidden layer of a neural network and find that adversarial examples are separable from the original images by training a density estimate using Gaussian kernels [5].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "without the detector [4].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "We use the same notation as [4].", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "fenses [4].", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "To do this, [4] specifically defines", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker\u2019s task.", "startOffset": 168, "endOffset": 183}, {"referenceID": 6, "context": "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker\u2019s task.", "startOffset": 168, "endOffset": 183}, {"referenceID": 23, "context": "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker\u2019s task.", "startOffset": 168, "endOffset": 183}, {"referenceID": 18, "context": "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker\u2019s task.", "startOffset": 168, "endOffset": 183}], "year": 2017, "abstractText": "Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.", "creator": "LaTeX with hyperref package"}}}