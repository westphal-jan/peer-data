{"id": "1706.04721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Target Curricula via Selection of Minimum Feature Sets: a Case Study in Boolean Networks", "abstract": "We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.", "histories": [["v1", "Thu, 15 Jun 2017 02:08:54 GMT  (458kb,D)", "http://arxiv.org/abs/1706.04721v1", "Submitted to JMLR Jan. 17"], ["v2", "Wed, 1 Nov 2017 02:57:13 GMT  (459kb,D)", "http://arxiv.org/abs/1706.04721v2", "Accepted for publication in JMLR issue 18"]], "COMMENTS": "Submitted to JMLR Jan. 17", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["shannon fenn", "pablo moscato"], "accepted": false, "id": "1706.04721"}, "pdf": {"name": "1706.04721.pdf", "metadata": {"source": "CRF", "title": "Target Curricula via Selection of Minimum Feature Sets Target Curricula via Selection of Minimum Feature Sets: a Case Study in Boolean Networks", "authors": ["Shannon Fenn", "Pablo Moscato"], "emails": ["shannon.fenn@newcastle.edu.au", "pablo.moscato@newcastle.edu.au"], "sections": [{"heading": null, "text": "We show that hierarchical interdependencies between targets can be exploited by enforcing an appropriate curriculum through hierarchical loss functions. In multiple problems with circuits with known target difficulties, feedback Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample errors, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curriculum. We also show the same improvements for three real models and two gene regulatory network (GRN) inference problems. We position a simple a-priori method for identifying a suitable target sequence and assessing the strength of target relationships in Boolean MLCs. These methods use the intrinsic dimension as a proxy for the target difficulty estimated by optimal solutions to a combinatorial optimization problem called Minimum Feature-Set (FS)."}, {"heading": "1. Introduction", "text": "When students are first taught the concept of addition, they are not simply handed an eclectic set of many pairs of numbers and their summaries; rather, we provide them with a learning curve of carefully selected examples that increase in difficulty (i.e. number of digits).From standard classroom practice, we can identify two key sources of inspiration for effective human learning and teaching processes: the curriculum of examples and the curriculum of goals. This essay examines the latter. ar Xiv: 170 6.04 721v 1 [cs.A] 1 5Khan et al. (2011) found that people naturally follow a simple to hard example-based curriculum when teaching an unknown learner a one-goal concept. Methods of terminating examples in this way have been researched in the machine learning community for some time, with positive results for non-convex scenarios (Bengio et al., 2009).The typical explanation for the success of the second class's concepts is not exemplified in the convex curricula problem."}, {"heading": "1.1 Contributions", "text": "We provide three new papers: \u2022 we show that the use of hierarchical loss functions improves the generalization performance of FBNs trained on multiple circuit inference problems with natural target progression; \u2022 we describe a simple method of estimating the complexity and overlap of Boolean targets by reducing their intrinsic dimensions using solutions to the minFS problem; and \u2022 we demonstrate the success of the combined method in the same problem cases where knowledge of the target difficulty is not available. Later in this section, we define FBNs as a learning model, provide background information on MLC, and outline existing work on sample and target curricula. In Section 2, we define hierarchical loss functions as well as the minFS problem and how they can be used to identify target overlaps and estimate a suitable curriculum. Finally, in Section 3, we present results on several problems."}, {"heading": "1.2 Boolean Networks", "text": "In this section we offer definitions of the problem domain, its relevance and the representation of interest in this work. An FBN with l-inputs and m-outputs calculates a function of the form f: Bl \u2192 Bm by linking invoices with internal nodes (often referred to as gates). Internally, an FBN with l-inputs and m-outputs can be recognized."}, {"heading": "1.3 Learning with Boolean Networks", "text": "Since the late publication of Turing's first description of a machine that learns by changing the connections of a NAND-gate circuit (Turing, 1948), the majority of work on learning has been done with a Boolean network model in the field of physics. Some time later, Patarnello and Carnevali (1987), and later others (Goudarzi et al., 2014), FBNs showed that they could generalize well to some problems, despite the training sizes that are substantially smaller than the space of all possible patterns. Learning FBNs is a combinatorial optimization problem that is typically solved with metaheuristics such as SA or genetic algorithms. Patarnello and Carnevali (1987) optimized networks through the space of feedforward structures. Each movement in the search process consisted of a random change in a single connection."}, {"heading": "1.4 Example Curricula", "text": "The approach we present in this paper is strongly related to sample curricula. In this subsection, we give a brief outline of some relevant papers to better frame our approach. Curriculum learning by applying an example ranking can be divided into two main camps: Curriculum Learning (CL) and Self-directed Learning (SPL) (Kumar et al., 2010).1. With purely binary predictors and objectives, L1 and L2 loss are identical. In the former, ranking is determined a priori, while in the latter it is learned together with the model. Jiang et al. (2015) demonstrated a combined approach - intuitively referred to as self-directed curriculum learning - where constraints on example ranking are imposed by a fixed curriculum and ranking is learned together with the model, but subject to curriculity limitations of the curriculum."}, {"heading": "1.5 Target Curricula", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Learning targets in order", "text": "Here we describe three loss functions that we have used to enforce a target curriculum, as well as a new method for obtaining this curriculum that reduces the problem to the minimum feature set (minFS) problem."}, {"heading": "2.1 Hierarchical loss functions", "text": "Let's say a two-goal Boolean MLC, for which we have three candidate learners, and assume that we know that the first goal is \"easier\" under a certain definition than the second. The first goal denotes goal 1 correctly and goal 2 incorrectly for all examples. The second student does the opposite, and the third student denotes 50% of the examples incorrectly for both goals. All three networks have an equal L1 loser. The basic principle behind the loss functions we are about to define is that the first of the above learners should be preferred. By focusing on simpler goals earlier in the training process, the network is more likely to find substructures useful for later goals. The expectation is faster convergence speed, better generalization, or both.The L1 loss is simply the mean value of the error matrix. In addition, we consider three loss functions that hierarchically aggregate the error matrix in a more linear way:"}, {"heading": "2.2 Measuring target overlap and difficulty order", "text": "The main requirement for ordering targets is an estimate of their relative difficulty. It is probably most appropriate to apply domain knowledge where it is available, but in cases where this is impossible or difficult to achieve, we propose a rule of thumb: order by increasing the intrinsic dimension. Below, we present an intuitive approach using a combinatorial problem that has not significantly lengthened the learning time for the cases considered. In Section 3.4, we show that the target order estimated by this method provides general improvements that are achieved by means of the known order (Section 3.3). Our main assumption is that one target may be easier than another if its intrinsic dimension (the number of input variables on which it really depends) is smaller. This assumption is strong, but there is also a strong basis for it as a rule of thumb in the Boolean domain, since the intrinsic dimension of a function represents an upper limit for the size of the smallest Base on which it is implemented."}, {"heading": "2.2.1 The Minimum-Feature-Set Problem", "text": "The smallest intrinsic dimension of a Boolean function, which is fully consistent with a set of reasonable examples, can be found with the minimum feature set (minFS) problem. It begs the question: What is the smallest subset of predefined input functions on which a single target can still form a non-contradictory mapping? This equivalent decision problem is by no means new to the machine learning community (Davies and Russell, 1994), but to our knowledge it has not been used to estimate the relative goal difficulty in advance. This decision problem is known as the k-feature set (kFS) problem. Formally: k-feature-SetGiven: A binary n \u00d7 p matrix, M, where the lines n describe examples, and the columns p-input features and n-element target vector, T, and a positive integer solution k.question: S, k.question: S, k.k."}, {"heading": "2.2.2 Target ordering method", "text": "In view of an approach to solving the minFS problem, our proposed method of arranging targets is reduced to solving m of such cases; one for each target. In all cases, the input note matrix M is identical to the entire input matrix for the original learning problem. Each instance is optimally solved, resulting in a series of characteristics S1,..., Sm for each target. An estimate of the intrinsic dimension for a target i is then simply | Si |. Targets are sorted according to this estimate, with connections randomly interrupted, and the resulting total sequence is used in the loss functions defined in Section 2.1. These losses could be redefined so that a subset of targets are treated equally to avoid a tie, but we have not examined this possibility."}, {"heading": "2.2.3 Target overlap score", "text": "The estimate of the target overlap is also derived from the minFS solutions. For two targets, yi and yj, with solutions, Si and Sj, the target overlap is estimated by the Szymkiewicz-Simpson overlap coefficient: \u03c3 (i, j) = | Si, Sj | min (| Si |, | Sj |). (6) The overlap coefficient indicates a value between 0 and 1, where 0 indicates that there is an overlap between two sentences and 1 that one group is a subset of the other; values intervene when sentences partially overlap. This measurement was chosen in comparison with the more popular Jaccard index, because we want to consider all completely overlapping cases (A B) to be equivalent, regardless of their relative magnitude ratios. We can then define the nestness score, because the mean overlap coefficient between the consecutive targets is sufficient (if the formula is arranged according to 2.olm \u2212 1, where the overlapping value is \u2212 1)."}, {"heading": "3. Experimental Results", "text": "In optimizing L1, we looked at the transition from essentially random behavior to near-perfect prediction as the size of the training set increased, and with that in mind, we generated 1,000 samples each for a range of training set sizes that were selected to cover that transition with an appropriate margin. We give the size of the training set as a fraction of all possible patterns, s = | I | / 2l. For each sample, we trained a network for each loss function until they reached zero training error, allowing us to report the difference in test error between the network optimized under L1 and any of the remaining losses for the same training set. Using the same training set for each loss eliminates the significant variation in instance difficulty as a disturbance factor."}, {"heading": "3.1 Datasets", "text": "Here we define the problems we are looking at. First, we outline problems for which we already have a difficult curriculum, then we look at several published models of real phenomena, and finally we test our general approach to the problem of discovering rules for updating regulatory networks from discredited time series data 2."}, {"heading": "3.1.1 Circuit inference test-beds", "text": "For the first test rig, we selected five circuit inference problems: binary addition (add) and subtraction (sub), and cascaded variants of three typical circuit learning and genetic programming problems: multiplexer (cmux), majority (cmaj), and parity (cpar), which were chosen because they all have a natural hierarchy of objectives. Binary addition and subtraction are known multi-output problems. For the three remaining problems, multi-output versions are constructed by cascading single-output subcircuits; each successive output is the result of applying the original function over a larger set of inputs. With this method, we construct problems that we know to have a difficulty order, and what that order is. More detailed definitions of these problems are given in the online appendix. Cascaded parity and multiplexers have the strongest dependencies between consecutive targets, each of which can be directly calculated."}, {"heading": "3.1.2 Boolean Models of Real Phenomena", "text": "In addition to the problems mentioned above, we conducted similar experiments with real-world models, including a single-element commercial integrated circuit: the 74181 4-bit arithmetic logic unit (ALU) and two published biological models. The 74181 ALU has 14 inputs and 8 outputs. It performs 32 different arithmetic and logical operations on an 8-bit carry-in input and generates a 4-bit carry-in output, as well as 2 other carry-related outputs useful for faster calculations in concatenating multiple ALUs. The 74181 is a reverse engineering problem of realistic size, less idealized than the above problems, as there is no expected absolute arrangement to the targets. The biological models we considered as Boolean models of Fanconi Anemie / Breast Cancer (FA / CA) are all a reverse engineering problem of less realistic size than the ones mentioned above, as there are no objectives less."}, {"heading": "3.1.3 Learning Regulatory Networks From Time-Series Data", "text": "In conducting the above experiments, we observed that the primary performance gain is seen in smaller sample sizes. Therefore, we opted for a real-world test rig where a key problem is the extremely limited amount of data available: conclusions about Boolean GRN update models from the time series gene expression data. A desirable result is a synchronous update model as described in Section 3.1.2, and one way to achieve this is to treat each sequential pair of states as an input-output pair that predicts the next state from the current state. A further detail is given in the online appendix. Some pre-processing data of the data was needed to remove repeat patterns - these occur due to time disparities that we are not trying to model."}, {"heading": "3.2 The learning procedure", "text": "Here we describe the local search procedure for training. We learned each function by optimizing FBNs that consist only of NAND gates with stochastic local search. Since the NAND operator is functionally complete, the optimization process does not have to take into account node transmission functions and can instead learn each mapping by changing only the network structure.We determine the network size, ng, for all problems to 21 m and represent the structure with a 2 \u00d7 ng integer matrix, with column i containing the indices of the two sources for the ith node.We ensure that the network is forwarded by a topological order of nodes; each node can only accept input from nodes before it is put into the order.Since each DAG has at least one topological order, no structure is excluded by this limitation. The training involves performing stochastic local search in the space of the valid feed mbHC, with LAepic use being late-forward."}, {"heading": "3.3 Results for hierarchical loss functions", "text": "In this section, we show that the losses defined in Section 2.1 improve the subsequent improvements on all the issues mentioned in Section 3.1. We have also shown 95% confidence intervals as a transparent band in Figure 4. For Figure 3, this interval was too small for bars or bands to be visible. Overall test accuracy varies with the 7-bit cascade problem, which is shown at the top of Figure 3 and is expected to be a phase of generalization (Patarnello and Carnevali)."}, {"heading": "3.4 Evaluating the discovered curricula", "text": "Here, we evaluate the effectiveness of the proposed methods for estimating target overlaps and difficulty sequence described in Section 2 on the same dyno problems. We also observe how the improvement that hierarchical loss functions bring varies when random arrangements are used."}, {"heading": "3.4.1 Effectiveness of order estimation", "text": "We are aware of the difficulty of the syllabus for the problems in Section 3.1.1. Thus, we can quantify the effectiveness of the proposed target ordering method on these by using the rank correlation between known and estimated orders. As these are permutations, there are no ties to be taken into account, and we can use the simplest version of Kendall's determination given by: \u03c4 = P \u2212 Q P + Q, (8) where P is the number of target pairs placed in the same relative order between the two orders, and Q is the number not taken into account. As a correlation, the values in [\u2212 1, 1], where the two orders are identical, are 0 indicating that they are uncorrelated and \u2212 1 that they are inversed. We have the same experimental configuration as before, but for each training instance we randomly mixed the targets to reduce the effect of all deterministic distortions."}, {"heading": "3.4.2 How performance correlates with target order", "text": "The next experiment we describe had two main objectives: to examine the relationship between the performance of hierarchical loss functions and the given curriculum and to determine whether the difficulty-based curriculum that we have adopted as the basic truth is actually optimal. To do this, we used a slightly smaller single problem - the 5-bit addition - and trained networks that used Lgh with randomly generated target permutations and L1. This gives us another important baseline - random target orders - and also allows us to see how the performance of Lgh differs with \u03c4 (Eq.8), and when the least to most significant curriculum we propose is surpassed by another curriculum."}, {"heading": "3.5 Real world problems", "text": "Here we present the results for the experiments described in Sections 3.1.2 and 3.1.3. We lack an expected target order, so the results are available for all the ordered losses with automatically determined syllabus (Section 2.2).For the ALU, Mammalian Cycle and FA / BRCA pathways, we have the same regime as for the previous test beds, so we have presented the results in the same way. Figure 7 shows the mean difference in test accuracy between the individual hierarchical loss functions and baseline L1, because the sample size varies. We also see that in the mammalian cycle model there is much less differentiation between the hierarchical losses, in fact Lw appears to have brought the greatest benefit. Overall, in all three cases where the target sequence accuracy has been improved, and the proposed method clearly found a beneficial curriculum. For the time series datasets, the leave-one-out test regime did not include any deviations, so we presented the results in the 2 and 3 instead."}, {"heading": "4. Discussion and future work", "text": "We have shown that the loss of functions that pursue a goal to improve the overall performance of FBNs is primarily a series of hierarchical multi-purpose halls in which it is about addressing problems that are not able to identify themselves, but engage in addressing problems that affect themselves in the way they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented, and how they are presented in the way they are presented, and how they are presented in the way they are presented, and how they are presented in the way they are presented, and how they are presented in the way they are presented, and how they are presented in the way they are presented, and how they are presented in the way they are presented, and how they are presented in the way they are presented, and how they are presented in the way they are presented, how they are presented in the way they are presented, how they are presented in the way they are presented in the way they and how they are presented in the way they are presented, how they are presented in the way they are presented in the way they and how they are presented in the way they are presented, how they are presented in the way they are presented in the way they are presented, how they are presented in the way they are presented in the way they and how they are presented in the way they are presented, how they are presented in the way they are presented in the way they are presented and how they are presented in the way they are presented in the way they are presented, how they are presented in the way they are presented in the way they are presented in the way, how they are presented in the way they are presented in the way they are presented in the way they and how they are presented in the way they are presented in the way they are presented in the way they are presented in the way, and how they are presented in the way they are presented in the way they are presented and how they are presented in the way they are presented in the way they are presented in the way, and how they are presented in the way they are"}, {"heading": "Acknowledgments", "text": "We thank Natalie de Vries for her help in proofreading and discussing the manuscript and Drs. Alexandre Mendes and Nasimul Noman for their fruitful discussions in the early stages of the work. Pablo Moscato would also like to thank Prof. Miguel A'ngel Virasoro for a copy of Patarnello and Carnevali (1987) many years ago."}, {"heading": "1. Test-bed problems", "text": "Here we give more detailed definitions of the test bench problems discussed in the main text."}, {"heading": "In the following definitions \u2227 denotes conjunction, \u2228 inclusive disjunction, \u2295 exclusive disjunction and x negation.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Problems with known target curricula", "text": "Binary addition takes two n-bit inputs, x \u2212 and y, and calculates an n-bit1 output. An n-bit addition causes a problem with Ni = 2n and No = n.Binary subtraction is similarly defined. ith output can be expressed by zi = (xi-yi) - 1, which is almost identical to addition, but with bi represents an average Borrow value given by bi = (xi-yi) - 2. The three remaining problems arise from cascading known individual output circuits. Each successive output is the result of applying the original function over a larger set of inputs. With this method, we build circuits of different tastes that we know to be difficult."}, {"heading": "1.2 Problems with unknown target curricula", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.2.1 74181 ALU", "text": "This is an 8-bit arithmetic / logic unit model. It has 14 input channels and 8 output channels. It performs 32 different arithmetic and logical operations on an 8-bit carry-in input and generates a 4-bit carry-in output and 2 other carry-related outputs, which are useful for faster calculations when concatenating multiple ALUs. Datasheets for this IC are publicly available."}, {"heading": "1.2.2 Mammalian cell-cycle model", "text": "The complete model can be found in the references quoted in the main text. It consists of 10 nodes and therefore 10 inputs and 10 targets. In the published model, however, the Cyclin D node is not updated and is treated as a constant input, as such we do not treat it as a learnable target."}, {"heading": "1.2.3 FA/BRCA pathway model", "text": "The complete model can be found in the references quoted in the main text. It is much larger and consists of 28 nodes and therefore 28 input and 28 target characteristics."}, {"heading": "1.3 Time-series Regulatory Network data", "text": "The yeast dataset consists of binary time step values for 10 nodes: Start, SK, Cdc2 / Cdc13, Ste9, Rum1, Slp1, Cdc2 / Cdc13 *, Week1Mik1, Cdc25, PP and Phase. The \"start\" target was constant and therefore removed. E. coli dataset also consisted of binary time step values for 10 nodes G1 to G10. We found that the targets G7 and G10 were constant and therefore removed them."}, {"heading": "2. Late-Acceptance Hill Climbing variant", "text": "The algorithm has a single meta-parameter L, which represents the length of a cost history list. The value of L, which is used for each problem case, is specified in the main text. Algorithm 1 LAHC implementation Requirement: A scalar cost function: C () An initialization method: initialize () A neighbouring generation method: Neighbor () A cost history length: L > 0 An iteration limit: I > 0 A restart limit: R 1: 0 A goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-"}], "references": [{"title": "A novel mutual information-based Boolean network inference method from time-series gene expression data", "author": ["Shohag Barman", "Yung-Keun Kwon"], "venue": "PLOS ONE,", "citeRegEx": "Barman and Kwon.,? \\Q2017\\E", "shortCiteRegEx": "Barman and Kwon.", "year": 2017}, {"title": "Curriculum Learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "A late acceptance strategy in hill-climbing for examination timetabling problems", "author": ["Edmund K. Burke", "Yuri Bykov"], "venue": "In International Conference on the Practice and Theory of Automated Timetabling (PATAT),", "citeRegEx": "Burke and Bykov.,? \\Q2008\\E", "shortCiteRegEx": "Burke and Bykov.", "year": 2008}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "The linear ordering problem revisited", "author": ["Josu Ceberio", "Alexander Mendiburu", "Jose A. Lozano"], "venue": "European Journal of Operational Research,", "citeRegEx": "Ceberio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ceberio et al\\.", "year": 2015}, {"title": "The k-Feature Set problem is W[2]-complete", "author": ["Carlos Cotta", "Pablo Moscato"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cotta and Moscato.,? \\Q2003\\E", "shortCiteRegEx": "Cotta and Moscato.", "year": 2003}, {"title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "[cs],", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "NP-Completeness of Searches for Smallest Possible Feature Sets", "author": ["Scott Davies", "Stuart Russell"], "venue": "In AAAI Symposium on Intelligent Relevance,", "citeRegEx": "Davies and Russell.,? \\Q1994\\E", "shortCiteRegEx": "Davies and Russell.", "year": 1994}, {"title": "Learning, generalisation, and functional entropy in random automata networks", "author": ["Alireza Goudarzi", "Christof Teuscher", "Natali Gulbahce", "Thimo Rohlf"], "venue": "International Journal of Autonomous and Adaptive Communications Systems,", "citeRegEx": "Goudarzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goudarzi et al\\.", "year": 2014}, {"title": "Accurate Estimation of the Intrinsic Dimension Using Graph Distances: Unraveling the Geometric Complexity of Datasets", "author": ["Daniele Granata", "Vincenzo Carnevale"], "venue": "Scientific Reports,", "citeRegEx": "Granata and Carnevale.,? \\Q2016\\E", "shortCiteRegEx": "Granata and Carnevale.", "year": 2016}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["\u00c7a\u011flar G\u00fcl\u00e7ehre", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "G\u00fcl\u00e7ehre and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre and Bengio.", "year": 2016}, {"title": "Selfpaced Curriculum Learning", "author": ["Lu Jiang", "Deyu Meng", "Qian Zhao", "Shiguang Shan", "Alexander G. Hauptmann"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "How Do Humans Teach: On Curriculum Learning and Teaching Dimension", "author": ["Faisal Khan", "Bilge Mutlu", "Xiaojin Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Khan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2011}, {"title": "Self-paced Learning for Latent Variable Models", "author": ["M. Pawan Kumar", "Benjamin Packer", "Daphne Koller"], "venue": "In Proceedings of the 23rd International Conference on Neural Information Processing Systems,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Toward Optimal Ordering of Prediction Tasks", "author": ["Abhimanyu Lad", "Rayid Ghani", "Yiming Yang", "Bryan Kisiel"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Lad et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lad et al\\.", "year": 2009}, {"title": "Self-Paced Multi-Task Learning", "author": ["Changsheng Li", "Fan Wei", "Junchi Yan", "Weishan Dong", "Qingshan Liu", "Hongyuan Zha"], "venue": "[cs],", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning networks of neurons with Boolean logic", "author": ["Stefano Patarnello", "Paolo Carnevali"], "venue": "Europhysics Letters,", "citeRegEx": "Patarnello and Carnevali.,? \\Q1987\\E", "shortCiteRegEx": "Patarnello and Carnevali.", "year": 1987}, {"title": "Curriculum learning of multiple tasks", "author": ["Anastasia Pentina", "Viktoriia Sharmanska", "Christoph H. Lampert"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Pentina et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pentina et al\\.", "year": 2015}, {"title": "An in silico target identification using Boolean network attractors: Avoiding pathological phenotypes", "author": ["Arnaud Poret", "Jean-Pierre Boissel"], "venue": "Comptes Rendus Biologies,", "citeRegEx": "Poret and Boissel.,? \\Q2014\\E", "shortCiteRegEx": "Poret and Boissel.", "year": 2014}, {"title": "Classifier chains for multi-label classification", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoff Holmes", "Eibe Frank"], "venue": "Machine Learning,", "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "A fast meta-heuristic approach for the alpha-beta-k-feature set problem", "author": ["Mateus Rocha de Paula", "Regina Berretta", "Pablo Moscato"], "venue": "Journal of Heuristics,", "citeRegEx": "Paula et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Paula et al\\.", "year": 2015}, {"title": "Baby Steps: How \u201cLess is More\u201d in unsupervised dependency parsing", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky"], "venue": "In In NIPS: Grammar Induction, Representation of Language and Language Learning,", "citeRegEx": "Spitkovsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2009}, {"title": "Learning and generalization in random Boolean networks", "author": ["Christof Teuscher", "Natali Gulbahce", "Thimo Rohlf"], "venue": "In Dynamic Days 2007: International Conference on Chaos and Nonlinear Dynamics,", "citeRegEx": "Teuscher et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teuscher et al\\.", "year": 2007}, {"title": "Intelligent Machinery. Report for National Physical Laboratory", "author": ["Alan M. Turing"], "venue": "Reprinted in Ince, Dc (Editor)", "citeRegEx": "Turing.,? \\Q1992\\E", "shortCiteRegEx": "Turing.", "year": 1992}, {"title": "Learning in feedforward Boolean networks", "author": ["Christian Van den Broeck", "Ryoichi Kawai"], "venue": "Physical Review A,", "citeRegEx": "Broeck and Kawai.,? \\Q1990\\E", "shortCiteRegEx": "Broeck and Kawai.", "year": 1990}, {"title": "Multiplicative Multitask Feature Learning", "author": ["Xin Wang", "Jinbo Bi", "Shipeng Yu", "Jiangwen Sun", "Minghu Song"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Methods of scheduling examples this way have been explored in the machine learning community for some time, with positive results for non-convex scenarios (Bengio et al., 2009).", "startOffset": 155, "endOffset": 176}, {"referenceID": 1, "context": "The typical explanation for the success of example curricula in non-convex scenarios is that it imposes a form of transfer learning where the simpler concept is discovered first and subsequently informs the more complex concept (Bengio et al., 2009).", "startOffset": 228, "endOffset": 249}, {"referenceID": 9, "context": "The intrinsic dimension has been presented as a proxy for the overall complexity of arbitrary datasets (Granata and Carnevale, 2016) suggesting its promise as a metric for ordering targets.", "startOffset": 103, "endOffset": 132}, {"referenceID": 6, "context": "Recent literature presents methods for binarising the internal layers of deep neural nets precisely for these reasons (Courbariaux et al., 2016).", "startOffset": 118, "endOffset": 144}, {"referenceID": 8, "context": "Some time later, Patarnello and Carnevali (1987), and later others (Goudarzi et al., 2014), demonstrated that FBNs could generalise well on some problems, despite training set sizes significantly smaller than the space of all possible patterns.", "startOffset": 67, "endOffset": 90}, {"referenceID": 8, "context": "Moves which also changed the node activation have been considered by others (Van den Broeck and Kawai, 1990; Goudarzi et al., 2014), however in light of the NAND function\u2019s functional completeness, this becomes unnecessary.", "startOffset": 76, "endOffset": 131}, {"referenceID": 15, "context": "Some time later, Patarnello and Carnevali (1987), and later others (Goudarzi et al.", "startOffset": 17, "endOffset": 49}, {"referenceID": 8, "context": "Some time later, Patarnello and Carnevali (1987), and later others (Goudarzi et al., 2014), demonstrated that FBNs could generalise well on some problems, despite training set sizes significantly smaller than the space of all possible patterns. Learning FBNs is a combinatorial optimisation problem, typically solved using metaheuristics such as SA or genetic algorithms. Patarnello and Carnevali (1987) optimised networks by SA in the space of feedforward structures.", "startOffset": 68, "endOffset": 404}, {"referenceID": 16, "context": "Even for problems with multiple outputs, the only loss functions used to date (Patarnello and Carnevali, 1987; Teuscher et al., 2007; Goudarzi et al., 2014) have been analogues of the L1-loss 1: L1 = 1 m |I| m \u2211", "startOffset": 78, "endOffset": 156}, {"referenceID": 22, "context": "Even for problems with multiple outputs, the only loss functions used to date (Patarnello and Carnevali, 1987; Teuscher et al., 2007; Goudarzi et al., 2014) have been analogues of the L1-loss 1: L1 = 1 m |I| m \u2211", "startOffset": 78, "endOffset": 156}, {"referenceID": 8, "context": "Even for problems with multiple outputs, the only loss functions used to date (Patarnello and Carnevali, 1987; Teuscher et al., 2007; Goudarzi et al., 2014) have been analogues of the L1-loss 1: L1 = 1 m |I| m \u2211", "startOffset": 78, "endOffset": 156}, {"referenceID": 13, "context": "Curriculum learning by applying a ranking of examples can be split into two main camps: Curriculum Learning (CL) and Self-Paced Learning (SPL) (Kumar et al., 2010).", "startOffset": 143, "endOffset": 163}, {"referenceID": 11, "context": "Jiang et al. (2015) demonstrated a combined approach\u2014intuitively called Self-Paced Curriculum Learning\u2014where constraints on the example-ranking is imposed by a fixed curriculum and the ranking is learned jointly with the model but subject to the curriculum constraints.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "Jiang et al. (2015) demonstrated a combined approach\u2014intuitively called Self-Paced Curriculum Learning\u2014where constraints on the example-ranking is imposed by a fixed curriculum and the ranking is learned jointly with the model but subject to the curriculum constraints. SPL and CL emerge as special cases of their framework. They present positive results in comparison to baseline methods on matrix factorization and multimedia event detection problems. Spitkovsky et al. (2009) demonstrated the success of a curriculum of samples of increasing complexity on the problem of determining grammar from free-form sentences.", "startOffset": 0, "endOffset": 479}, {"referenceID": 3, "context": "5 Target Curricula Learning multiple targets concurrently with a shared representation has been shown, theoretically and experimentally, to improve generalisation results when there are relationships between the targets (Caruana, 1997; Wang et al., 2016).", "startOffset": 220, "endOffset": 254}, {"referenceID": 25, "context": "5 Target Curricula Learning multiple targets concurrently with a shared representation has been shown, theoretically and experimentally, to improve generalisation results when there are relationships between the targets (Caruana, 1997; Wang et al., 2016).", "startOffset": 220, "endOffset": 254}, {"referenceID": 3, "context": "5 Target Curricula Learning multiple targets concurrently with a shared representation has been shown, theoretically and experimentally, to improve generalisation results when there are relationships between the targets (Caruana, 1997; Wang et al., 2016). The number of publications discussing multi-label and multi-task learning is growing rapidly and using a curriculum of targets, while less general, is also beginning to garner attention. The importance of sharing knowledge between targets increases when there is a disparity in target complexity. G\u00fcl\u00e7ehre and Bengio (2016) showed a visual task, on which many popular methods\u2014including deep neural nets, SVMs and decision trees\u2014failed.", "startOffset": 221, "endOffset": 580}, {"referenceID": 3, "context": "5 Target Curricula Learning multiple targets concurrently with a shared representation has been shown, theoretically and experimentally, to improve generalisation results when there are relationships between the targets (Caruana, 1997; Wang et al., 2016). The number of publications discussing multi-label and multi-task learning is growing rapidly and using a curriculum of targets, while less general, is also beginning to garner attention. The importance of sharing knowledge between targets increases when there is a disparity in target complexity. G\u00fcl\u00e7ehre and Bengio (2016) showed a visual task, on which many popular methods\u2014including deep neural nets, SVMs and decision trees\u2014failed. This same task became solvable when a simpler hinting target was provided (in much the same way as introduced by ?). However, unless relative target difficulties are known (or suspected), we also face the issue of determining an appropriate order during learning. Pentina et al. (2015) presented a self-paced method using adaptive linear SVMs.", "startOffset": 221, "endOffset": 978}, {"referenceID": 3, "context": "5 Target Curricula Learning multiple targets concurrently with a shared representation has been shown, theoretically and experimentally, to improve generalisation results when there are relationships between the targets (Caruana, 1997; Wang et al., 2016). The number of publications discussing multi-label and multi-task learning is growing rapidly and using a curriculum of targets, while less general, is also beginning to garner attention. The importance of sharing knowledge between targets increases when there is a disparity in target complexity. G\u00fcl\u00e7ehre and Bengio (2016) showed a visual task, on which many popular methods\u2014including deep neural nets, SVMs and decision trees\u2014failed. This same task became solvable when a simpler hinting target was provided (in much the same way as introduced by ?). However, unless relative target difficulties are known (or suspected), we also face the issue of determining an appropriate order during learning. Pentina et al. (2015) presented a self-paced method using adaptive linear SVMs. The implementation was model-specific, providing information transfer by using the optimal weight vector for the most recently learned task to regularise the next task\u2019s weight vector. They discover the curricula by solving all remaining tasks and selecting the best fitting, resulting in a quadratic increase in training time over solving tasks individually. While this may be acceptable for a linear model, it would be prohibitive for a FBN. Li et al. (2016) implemented task curricula with linear models by weighting the contribution of each task-example pair to the loss function.", "startOffset": 221, "endOffset": 1497}, {"referenceID": 3, "context": "5 Target Curricula Learning multiple targets concurrently with a shared representation has been shown, theoretically and experimentally, to improve generalisation results when there are relationships between the targets (Caruana, 1997; Wang et al., 2016). The number of publications discussing multi-label and multi-task learning is growing rapidly and using a curriculum of targets, while less general, is also beginning to garner attention. The importance of sharing knowledge between targets increases when there is a disparity in target complexity. G\u00fcl\u00e7ehre and Bengio (2016) showed a visual task, on which many popular methods\u2014including deep neural nets, SVMs and decision trees\u2014failed. This same task became solvable when a simpler hinting target was provided (in much the same way as introduced by ?). However, unless relative target difficulties are known (or suspected), we also face the issue of determining an appropriate order during learning. Pentina et al. (2015) presented a self-paced method using adaptive linear SVMs. The implementation was model-specific, providing information transfer by using the optimal weight vector for the most recently learned task to regularise the next task\u2019s weight vector. They discover the curricula by solving all remaining tasks and selecting the best fitting, resulting in a quadratic increase in training time over solving tasks individually. While this may be acceptable for a linear model, it would be prohibitive for a FBN. Li et al. (2016) implemented task curricula with linear models by weighting the contribution of each task-example pair to the loss function. They used a regulariser in which tasks-example pairs with low training error receive larger values for these weights. However, this approach does not appear applicable to inferring highly non-linear discrete models. Lad et al. (2009) present the only model-agnostic approach to determine an optimal order of tasks that we are aware of.", "startOffset": 221, "endOffset": 1855}, {"referenceID": 19, "context": "The impact of target order is highlighted by Read et al. (2011). Their approach uses classifier chains: independent learners for each binary label, trained in an arbitrary order, with the output of all prior classifiers included as input to each subsequent classifier.", "startOffset": 45, "endOffset": 64}, {"referenceID": 9, "context": "Furthermore intrinsic dimension is showing promise as a method for estimating the complexity of arbitrary data (Granata and Carnevale, 2016) and as such we expect the validity of this idea may extend beyond the Boolean case.", "startOffset": 111, "endOffset": 140}, {"referenceID": 7, "context": "It asks: what is the smallest subset of given input features, on which a single target can still form a non-contradictory mapping? This problem, and particularly this view of it, is not in any way new to the machine learning community (Davies and Russell, 1994).", "startOffset": 235, "endOffset": 261}, {"referenceID": 7, "context": ", n] where Ti 6= Tj \u2203l \u2208 S such that Mi,l 6=Mj,l? That is: is there a cardinality k subset, S, of the input features, such that no pair of examples which have identical values across all features in S have a different value for the target feature? The problem is NP-complete (Davies and Russell, 1994) and assumed to not be fixedparameter tractable, when parametrised by k, under current complexity assumptions (Cotta and Moscato, 2003).", "startOffset": 275, "endOffset": 301}, {"referenceID": 5, "context": ", n] where Ti 6= Tj \u2203l \u2208 S such that Mi,l 6=Mj,l? That is: is there a cardinality k subset, S, of the input features, such that no pair of examples which have identical values across all features in S have a different value for the target feature? The problem is NP-complete (Davies and Russell, 1994) and assumed to not be fixedparameter tractable, when parametrised by k, under current complexity assumptions (Cotta and Moscato, 2003).", "startOffset": 411, "endOffset": 436}, {"referenceID": 18, "context": "The biological models we considered were Boolean models of the Fanconi Anemia/Breast Cancer (FA/BRCA) pathway and the mammalian cell cycle (Poret and Boissel, 2014).", "startOffset": 139, "endOffset": 164}, {"referenceID": 0, "context": "Barman and Kwon (2017) provide binarised time-series data for an E.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Training involved performing stochastic local search in the space of valid feed-forward structure, using the Late-Acceptance Hill Climbing (LAHC) meta-heuristic (Burke and Bykov, 2008) with random restarts.", "startOffset": 161, "endOffset": 184}, {"referenceID": 16, "context": "A phase transition between poor and near perfect generalisation is expected (Patarnello and Carnevali, 1987).", "startOffset": 76, "endOffset": 108}, {"referenceID": 9, "context": "A recent discussion of the use of intrinsic dimension as a proxy for the overall complexity of arbitrary datasets (Granata and Carnevale, 2016) is promising.", "startOffset": 114, "endOffset": 143}], "year": 2017, "abstractText": "We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models. We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi output circuitinference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems. We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.", "creator": "LaTeX with hyperref package"}}}