{"id": "1706.04815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension", "abstract": "Most existing works on machine reading comprehension are built under the answer extraction approach which predicts sub-spans from passages to answer questions. In this paper, we develop an extraction-then-generation framework for machine reading comprehension, in which the answer is generated from the extraction results. Specifically, we build the answer extraction model to predict the most important sub-spans from the passage as evidence, and develop the answer generation model which takes the evidence as additional features along with the question and passage to further elaborate the final answers. We build the answer extraction model with state-of-the-art neural networks for reading comprehension, and the answer generation model with sequence-to-sequence neural networks. Experiments on the MS-MARCO dataset show that the generation based approach achieves better results than pure answer extraction.", "histories": [["v1", "Thu, 15 Jun 2017 11:10:33 GMT  (725kb,D)", "http://arxiv.org/abs/1706.04815v1", null], ["v2", "Tue, 5 Sep 2017 11:55:01 GMT  (666kb,D)", "http://arxiv.org/abs/1706.04815v2", null], ["v3", "Mon, 25 Sep 2017 01:41:07 GMT  (664kb,D)", "http://arxiv.org/abs/1706.04815v3", null], ["v4", "Mon, 9 Oct 2017 06:58:31 GMT  (664kb,D)", "http://arxiv.org/abs/1706.04815v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chuanqi tan", "furu wei", "nan yang", "weifeng lv", "ming zhou"], "accepted": false, "id": "1706.04815"}, "pdf": {"name": "1706.04815.pdf", "metadata": {"source": "CRF", "title": "S-NET: FROM ANSWER EXTRACTION TO ANSWER GENERATION FOR MACHINE READING COMPREHEN- SION", "authors": ["Chuanqi Tan", "Furu Wei", "Nan Yang", "Weifeng Lv", "Ming Zhou"], "emails": ["tanchuanqi@nlsde.buaa.edu.cn", "lwf@buaa.edu.cn", "fuwei@microsoft.com", "nanya@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to outdo themselves, and they are able to outdo themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think they will be able to outdo themselves, and that they will be able to outdo themselves. \""}, {"heading": "2 RELATED WORK", "text": "Richardson et al. (2013) publish MCTest, whose goal is to select the best answer from four options that give the question and the passage. CNN / Daily-Mail (Hermann et al., 2015) and CBT (Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Unlike the above datasets, the SQuAD dataset (Rajpurkar et al., 2016), whose answer can be a much longer formulation, is more difficult. The answer in SQuAD is a segment of the text, or chip, from the corresponding proofreader passage. Similar to the SQuAD, MS-MARCO et al., 2016, the reading comprehension dataset al."}, {"heading": "3 OUR APPROACH", "text": "According to the flow chart in Figure 1, our approach consists of two parts as evidence extraction1 and response generation. The part on evidence extraction aims to extract snippets of evidence related to the question and passage. We use the gated attention recurrent neural network to match the question and passage, and then use pointer networks to predict the boundary of snippets of evidence. The part on response generation consists of generating the final answer based on the snippets of evidence using a sequence sequence sequence model. We comment on the boundary of the extracted snippets of evidence as an additional feature in the coding part, and apply an attentive decoder to the final answer."}, {"heading": "3.1 GATED RECURRENT UNIT", "text": "Instead of the basic RNN we use the Gated Recurrent Unit (GRU) (Cho et al., 2014). Equation 1 describes the mathematical model of the GRU. rt and zt are the gates and ht is the hidden state.zt = \u03c3 (Whzht \u2212 1 + BxZxt + bz) rt = \u03c3 (Whrht \u2212 1 + WxRxt + br) h \u0430t = \u03a6 (Wh (rt ht \u2212 1) + Bxxt + b) ht = (1 \u2212 zt) ht \u2212 1 + z h \u0442t (1a) 1In our model we use \"evidence extraction\" to represent the pure \"response extraction\" in previous work."}, {"heading": "3.2 EVIDENCE EXTRACTION", "text": "(Consider a question Q = {wQt} mt = 1 and a passage P = {wPt} nt = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 Q = 1 S = 1 S = 1 S = 1 S = 1 S = 1 S = 1 S = 1 S, S = 1 S 1 S = 1 S, S = 1 S 1 S 1 S = S, S = 1 S = S 1 S, S = 1 S = 1 S 1 S, S = 1 S = 1 S, S = 1 S = 1 S, S = 1 S 1 S, S = 1 S 1 S, S = S 1 S 1 S, S = S 1 S = S, S = S 1 S 1 S, S = S 1 S = S, S = S 1 S = S, S = S = S = S, S = S = S = S = S = S = S, S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S"}, {"heading": "3.3 ANSWER GENERATION", "text": "We first produce the representation hPt and h Q t of all words in the passage and question = Hi-value = 11 weight. in the production of the answer representation we combine the basic word embedding with an additional feature to show the start and end positions of the evidence snippet. HPt = BiGRU (h P t \u2212 1, [e p t, f p t]) hQt = BiGRU (h Q t \u2212 1, e Q t) (10) On top of the encoder we use GRU with attention as decoder to produce the answer. At each decoding step the GRU reads the previous word embed \u2212 1 and the previous context vector ct \u2212 1 as input to calculate the new hidden state dt. To initialize the hidden state of the GRU, we use a linear layer with the last hidden layer."}, {"heading": "4 EXPERIMENT", "text": "We conduct our experiments on the basis of the MS-MARCO dataset (Nguyen et al., 2016) and compare our extraction model with a single extraction model and other competing methods. Experimental results show that our model performs better in official evaluation metrics, and then we use a few examples to demonstrate the superiority of our model."}, {"heading": "4.1 DATASET AND EVALUATION METRICS", "text": "The MS-MARCO dataset is a user query to the Bing search engine, the context passages are real web documents and the answers are human-generated; the data has been divided into a training set (82,326 pairs), a development set (10,047 pairs) and a test set (9,650 pairs); the answers are not necessarily subsections of the passages, so the ratios in the official tool of the MS-MARCO evaluation are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), which are widely used in many areas."}, {"heading": "4.2 IMPLEMENTATION DETAILS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 TRAINING DATA", "text": "Since the answers are not necessarily partial voltages of the passages, we select for evidence extraction the range with the highest ROUGE-L score with the reference answer as the gold margin in the training. In addition, we only use the data with the ROUGE-L score of the response margin higher than 0.7, so we only use 71,417 training pairs in our experiments. For response generation, the training data consists of two parts. First, for all passages in the training data, we treat the passage as input, where the commented limit depends on the range with the highest ROUGE-L score, and the reference response as output. We only use the data with the ROUGE-L score of the response margin higher than 0.5. Secondly, we apply our evidence extraction model to all training data to obtain the extracted span. Then, we treat the passage to which this span belongs as an input model to train the 3.670 pairs."}, {"heading": "4.2.2 PARAMETER", "text": "For answer extraction, we use 300-dimensional unpacked, pre-trained GloVe embeddings (Pennington et al., 2014) 2 for question and passage without updating during the training. We use zero vectors to represent all words from the vocabulary. We set the hidden vector length to 150 for all levels. We also apply dropouts (Srivastava et al., 2014) between levels, with a failure rate of 0.1. For answer generation, we use an identical vocabulary for the input and output positions of the training data. We set the vocabulary size to 100,000 according to frequency and the other words. All word embeddings are updated during the training. We set the word embedsize to 300, set the feature embedding of the start and end positions of the extracted snippet to 50, and set all GRU-hidden state sizes to 150."}, {"heading": "4.2.3 DECODING", "text": "When decoding, we first perform our extraction model to obtain the extracted range and execute our generation model with the passage containing the range for the response sequence. Once we have generated the final sequence through the sequence-to-sequence model, we process the sequence with the following rules: \u2022 We hold it only once if the sequence-to-sequence model can generate duplicate words or phrases. \u2022 For all \"\" and the word and phrase that do not exist in the extracted response, we try to refine it by finding a word or phrase with the same adjacent words in the extracted range and passage. \u2022 If the generated response contains only one word, \"we use the extracted range as the final answer."}, {"heading": "4.3 BASELINE METHODS", "text": "We perform experiments with the following settings: AE: the model that extracts only a part of the response; AE + AG (S-Net): the model that consists of the part of evidence extraction and the part of response generation; and we also compare with other methods that represent the results in the MS-MARCO dataset, including FastQAExt (Weissenborn et al., 2017), Prediction (Wang & Jiang, 2016b) and ReasoNet (Shen et al., 2016)."}, {"heading": "4.4 RESULT", "text": "Table 2 shows ROUGE-L and BLEU-1 values on the development list of our model and competing approaches 3. As we see, our method exceeds the baseline and several state-of-the-art systems for both ROUGE-L and BLEU-1.2http: / / nlp.stanford.edu / data / glove.6B.zip. 3Excerpt from the MS-MARCO ranking http: / / www.msmarco.org / leaders.aspx"}, {"heading": "4.5 DISCUSSION", "text": "We compare the result of the answer extraction and answer generation in different synthetic categories in Table 4 with several examples in Table 5. For the questions whose answers can be accurately mapped in the passage, our answer generation model performs slightly worse, because the sequence-to-sequence model makes some deviation when copying extracted evidence. However, in other categories, our generation model achieves more or less improvements. We can observe that the largest category in the remaining questions is to refine the extracted evidence. In the first example in Table 5, our generation model adds \"es\" to the extracted snippet to synthesize the answer, completing the syntax of the sentence. In the next two examples, we find that the generation model is good at controlling the boundary of the answer. In the fourth example, the generation model refines the extracted answer by taking words into account in the question. The last example focuses on the follow-up question. The extraction model is good at controlling the boundary of the answer. In the fourth example, the GE refines the extracted answer by taking words into account in the question."}, {"heading": "5 CONCLUSION", "text": "This year, it has come to the point where it is only a matter of time before a result is achieved."}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank the organizers of MS-MARCO for their help in submitting contributions."}], "references": [{"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1631\u20131640", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "O.K. Victor Li"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das"], "venue": "arXiv preprint arXiv:1611.01436,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "D. Christopher Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "MS MARCO: A human generated machine reading comprehension", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "venue": "dataset. CoRR,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom"], "venue": null, "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "venue": "arXiv preprint arXiv:1611.01603,", "citeRegEx": "Seo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In NAACL HLT", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Fastqa: A simple and efficient neural architecture for question answering", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe"], "venue": "arXiv preprint arXiv:1703.04816,", "citeRegEx": "Weissenborn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "venue": "arXiv preprint arXiv:1611.01604,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Words or characters? fine-grained gating for reading", "author": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W. Cohen", "Ruslan Salakhutdinov"], "venue": "comprehension. CoRR,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "End-to-end reading comprehension with dynamic answer chunk ranking", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1610.09996,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Selective encoding for abstractive sentence summarization", "author": ["Qingyu Zhou", "Nan Yang", "Furu Wei", "Ming Zhou"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Machine reading comprehension (Nguyen et al., 2016), which attempts to enable machines to answer questions after reading a passage or a set of passages, attracts great attentions from both research and industry communities in recent years.", "startOffset": 30, "endOffset": 51}, {"referenceID": 18, "context": "Specifically, the extraction model is based on the R-Net (Wang et al., 2017).", "startOffset": 57, "endOffset": 76}, {"referenceID": 2, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 3, "context": ", 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage.", "startOffset": 15, "endOffset": 34}, {"referenceID": 10, "context": "Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging.", "startOffset": 49, "endOffset": 73}, {"referenceID": 7, "context": "Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages.", "startOffset": 31, "endOffset": 52}, {"referenceID": 6, "context": "The sequence-to-sequence model is widely-used in many tasks such as machine translation (Luong et al., 2015), parsing (Vinyals et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 23, "context": ", 2015b), summarization generation (Zhou et al., 2017) and response generation (Gu et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 1, "context": ", 2017) and response generation (Gu et al., 2016).", "startOffset": 32, "endOffset": 49}, {"referenceID": 4, "context": "Richardson et al. (2013) release MCTest whose goal is to select the best answer from four options given the question and the passage.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer.", "startOffset": 15, "endOffset": 801}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al.", "startOffset": 15, "endOffset": 900}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually.", "startOffset": 15, "endOffset": 922}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer.", "startOffset": 15, "endOffset": 1024}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset.", "startOffset": 15, "endOffset": 1110}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset can also be applied on the MS-MARCO dataset. Yu et al. (2016) and Lee et al.", "startOffset": 15, "endOffset": 1437}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset can also be applied on the MS-MARCO dataset. Yu et al. (2016) and Lee et al. (2016) solve SQuAD by ranking continuous text spans within passage.", "startOffset": 15, "endOffset": 1459}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset can also be applied on the MS-MARCO dataset. Yu et al. (2016) and Lee et al. (2016) solve SQuAD by ranking continuous text spans within passage. Yang et al. (2016) present a fine-grained gating mechanism to dynamically combine word-level and character-level representation and model the interaction between questions and passages.", "startOffset": 15, "endOffset": 1539}, {"referenceID": 0, "context": "We use Gated Recurrent Unit (GRU) (Cho et al., 2014) instead of basic RNN.", "startOffset": 34, "endOffset": 52}, {"referenceID": 12, "context": "Given question and passage representation {uQt }t=1 and {ut }t=1, Rockt\u00e4schel et al. (2015) propose generating sentence-pair representation {v t }t=1 via soft-alignment of words in the question and passage as follows: v t = GRU(v P t\u22121, ct) (3) where ct = att(u, [ut , v P t\u22121]) is an attention-pooling vector of the whole question (u ): sj = v tanh(W u u Q j +W P u u P t +W P v v P t\u22121) ai = exp(s t i)/\u03a3 m j=1exp(s t j) ct = \u03a3 m i=1a t iu Q i (4) Each passage representation v t dynamically incorporates aggregated matching information from the whole question.", "startOffset": 66, "endOffset": 92}, {"referenceID": 12, "context": "Given question and passage representation {uQt }t=1 and {ut }t=1, Rockt\u00e4schel et al. (2015) propose generating sentence-pair representation {v t }t=1 via soft-alignment of words in the question and passage as follows: v t = GRU(v P t\u22121, ct) (3) where ct = att(u, [ut , v P t\u22121]) is an attention-pooling vector of the whole question (u ): sj = v tanh(W u u Q j +W P u u P t +W P v v P t\u22121) ai = exp(s t i)/\u03a3 m j=1exp(s t j) ct = \u03a3 m i=1a t iu Q i (4) Each passage representation v t dynamically incorporates aggregated matching information from the whole question. Wang & Jiang (2016a) introduce match-LSTM, which takes uj as an additional input into the recurrent network.", "startOffset": 66, "endOffset": 585}, {"referenceID": 12, "context": "Given question and passage representation {uQt }t=1 and {ut }t=1, Rockt\u00e4schel et al. (2015) propose generating sentence-pair representation {v t }t=1 via soft-alignment of words in the question and passage as follows: v t = GRU(v P t\u22121, ct) (3) where ct = att(u, [ut , v P t\u22121]) is an attention-pooling vector of the whole question (u ): sj = v tanh(W u u Q j +W P u u P t +W P v v P t\u22121) ai = exp(s t i)/\u03a3 m j=1exp(s t j) ct = \u03a3 m i=1a t iu Q i (4) Each passage representation v t dynamically incorporates aggregated matching information from the whole question. Wang & Jiang (2016a) introduce match-LSTM, which takes uj as an additional input into the recurrent network. Wang et al. (2017) add another gate to the input ([ut , ct]) of RNN to determine the importance of passage parts.", "startOffset": 66, "endOffset": 692}, {"referenceID": 6, "context": "The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state dt with each encoder hidden state", "startOffset": 102, "endOffset": 122}, {"referenceID": 7, "context": "We conduct our experiments on the MS-MARCO dataset (Nguyen et al., 2016).", "startOffset": 51, "endOffset": 72}, {"referenceID": 8, "context": "The answers are not necessarily sub-spans of the passages so that the metrics in the official tool of MS-MARCO evaluation are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), which are widely used in many domains.", "startOffset": 131, "endOffset": 154}, {"referenceID": 5, "context": ", 2002) and ROUGE-L (Lin, 2004), which are widely used in many domains.", "startOffset": 20, "endOffset": 31}, {"referenceID": 9, "context": "For the answer extraction, we use 300-dimensional uncased pre-trained GloVe embeddings (Pennington et al., 2014)2 for both question and passage without update during training.", "startOffset": 87, "endOffset": 112}, {"referenceID": 14, "context": "We also apply dropout (Srivastava et al., 2014) between layers, with dropout rate 0.", "startOffset": 22, "endOffset": 47}, {"referenceID": 19, "context": "We also compare with other methods which report results in MS-MARCO dataset, including FastQAExt (Weissenborn et al., 2017), Prediction (Wang & Jiang, 2016b), and ReasoNet (Shen et al.", "startOffset": 97, "endOffset": 123}], "year": 2017, "abstractText": "Most existing works on machine reading comprehension are built under the answer extraction approach which predicts sub-spans from passages to answer questions. In this paper, we develop an extraction-then-generation framework for machine reading comprehension, in which the answer is generated from the extraction results. Specifically, we build the answer extraction model to predict the most important sub-spans from the passage as evidence, and develop the answer generation model which takes the evidence as additional features along with the question and passage to further elaborate the final answers. We build the answer extraction model with state-of-the-art neural networks for reading comprehension, and the answer generation model with sequence-to-sequence neural networks. Experiments on the MS-MARCO dataset show that the generation based approach achieves better results than pure answer extraction.", "creator": "LaTeX with hyperref package"}}}