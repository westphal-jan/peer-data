{"id": "1706.04902", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "A survey of cross-lingual embedding models", "abstract": "Cross-lingual embedding models allow us to project words from different languages into a shared embedding space. This allows us to apply models trained on languages with a lot of data, e.g. English to low-resource languages. In the following, we will survey models that seek to learn cross-lingual embeddings. We will discuss them based on the type of approach and the nature of parallel data that they employ. Finally, we will present challenges and summarize how to evaluate cross-lingual embedding models.", "histories": [["v1", "Thu, 15 Jun 2017 14:46:56 GMT  (1827kb,D)", "http://arxiv.org/abs/1706.04902v1", "23 pages, 17 figures, 2 tables"], ["v2", "Wed, 18 Oct 2017 10:44:06 GMT  (2430kb,D)", "http://arxiv.org/abs/1706.04902v2", "Very heavily improved and revised version"]], "COMMENTS": "23 pages, 17 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder"], "accepted": false, "id": "1706.04902"}, "pdf": {"name": "1706.04902.pdf", "metadata": {"source": "CRF", "title": "A survey of cross-lingual embedding models\u2217", "authors": ["Sebastian Ruder"], "emails": ["ruder.sebastian@gmail.com"], "sections": [{"heading": null, "text": "The availability of resources, training data and benchmarks in English therefore leads to a disproportionate focus on the English language and to a neglect of the abundance of other languages spoken around the world. Therefore, in our globalized society, where national boundaries are increasingly blurred, where the Internet provides equal access to information for everyone, it is imperative that we not only seek to eliminate prejudices regarding gender or race [Bolukbasi et al., 2016] that are inherent in our representations, but also aim to address our linguistic bias. In order to level this and the linguistic playing field, we would like to use our existing knowledge in English to equip our models with the ability to process other languages.Perfect examples of the space, in fact, we are unable to transform 1.T."}, {"heading": "1 Types of cross-lingual embedding models", "text": "In recent years, the number of job-related redundancies in the US has multiplied, while the number of job-related redundancies in the US has multiplied."}, {"heading": "2 Monolingual mapping", "text": "Methods that use monolingual mapping map monolingual word representations independently of each other on large monolingual corpora. They then attempt to learn a transformation matrix that matches representations in one language to representations in the other. Normally, they use a series of word-target word pairs that translate each other and are used as anchor words for learning mapping. Note that all of the following methods require that monolingual embedding spaces have already been trained. Unless otherwise specified, these embedding spaces were learned using the word2vec variants, the negative sampling skip program (SGNS), or the continuous bag of words (CBOW) on large monolingual corpora."}, {"heading": "2.1 Linear projection", "text": "Mikolov et al. have popularized the idea that vector spaces can encode meaningful relationships between words. Furthermore, they note that the geometric relationships that exist between words are similar across languages [Mikolov et al., 2013b], e.g. numbers and animals in English show a geometric constellation similar to their Spanish counterparts in Figure 2. This suggests that it may be possible to transform the vector space of one language into the space of another simply by using a linear projection with a transformation matrix W. To achieve this, they translate the 5,000 most common words from the source language and use these 5,000 translation pairs as a bilingual dictionary. They then learn W using stochastic gradient departure by minimizing the distance between the previously learned monolingual representations of the source word wi, which is transformed with W, and its translation zi in the bilingual dictionary: min Wxi \u2212 zi = 1."}, {"heading": "2.2 Projection via CCA", "text": "[Faruqui and Dyer, 2014] propose to use a different technique to learn linear mapping, using canonical correlation analysis (CCA) to project words from two languages into a common embedding space. In contrast to linear projection, CCA learns a transformation matrix for each language, as can be seen in Figure 3, where the transformation matrix V is used to project word representations from the embedding space into a new space, while W converts words from the embedding space into a new space, while W selects the target word for each source word to which it can be considered the same common embedding space. Similar to linear projection, CCA also requires a number of translation pairs in a new space, and its correlation can be maximized. Faruqui and Dyer obtain these pairs by selecting the target word for each source word to which they might appear most bivalent in a correlation of two languages."}, {"heading": "2.3 Normalisation and orthogonal transformation", "text": "[Xing et al., 2015] notice inconsistencies in the linear projection method of [Mikolov et al., 2013b] that they want to resolve. Let's remember that Mikolov et al. first learn monolingual word embeddings using the Skip-gram target, which is as follows: 1N + 1 + C \u2264 j \u2264 C, j 6 = 0 logP (wi + j | wi), where C is calculated the context length and P (wi + j | wi) is calculated with the softmax: P (wi + j | wi) = exp (cTwi + jcwi), w exp (cTwcwi).You then learn a linear transformation between the two monolingual vector spaces with: min = i Wxi \u2212 zi, where W is the projection matrix to be learned, and xi and zi are word vectors in the source and target languages that are similar."}, {"heading": "2.4 Max-margin and intruders", "text": "[Lazaridou et al., 2015a] identify another problem with the linear transformation goal of [Mikolov et al., 2013b]: They discover that the use of least squares as a goal for learning a projection matrix leads to hubris, i.e. some words tend to appear as the closest neighbors of many other words. To solve this, they use a margin-based (Max Margin) ranking loss [Collobert and Weston, 2008] to train the model to rate the correct translation vector yi of a source word xi, which is y-i higher than any other target word yj: k: j 6 = i max {0, \u03b3 + cos (y-i, yi) \u2212 cos (y-i, yj) \u2212 cos (y-i, yj) \u2212 cos (y-i, yj)}, where k is the number of negative examples and g is the margin.They show that the selection of max margin compared to the least squares significantly improves the truffiness of the choice and that the truffiness of the target function is higher."}, {"heading": "2.5 Multilingual CCA", "text": "[Ammar et al., 2016] extend the bilingual CCA projection method of [Faruqui and Dyer, 2014] to the multilingual environment by using the English embedding space as the basis for their multilingual embedding space. They learn the two projection matrices for any other language with English. The transformation from any target language space into the English embedding space can then be achieved by projecting the vectors into the CCA space using the transformation matrix W, as shown in Figure 3. Since the vectors are in the same space, vectors can be projected into the English embedding space using the inversion of V."}, {"heading": "2.6 Alignment-based projection", "text": "[Guo et al., 2015] propose a different method of projection based exclusively on word alignments. They count how often each word in the source language is aligned with each word in the target language in a parallel corpus, and store these counts in an alignment matrix A. To project a word wi from its source representation v (wSi) to its representation in the target space v (wi) T in the target space, they simply take the average of the embedding of its translations v (wj) T weighted according to their probability of alignment with the source word: v (wi) T = i, j \u0432A ci, j \u0445A ci, j \u00b7 v (wj) Twhere ci, j is the number of times in which the ith source word has been aligned with the jth target word. The problem with this method is that it only assigns Ovid embedding for words aligned in the reference parallel metrix. Gou alignment method is the signified source word (although TOW is the source word aligned with the TOith)."}, {"heading": "2.7 Orthogonal transformation, normalisation, and mean centering", "text": "In fact, most of us are able to play by the rules that they have imposed on themselves in order to play by the rules, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "2.8 Hybrid mapping with symmetric seed lexicon", "text": "The previous mapping approaches used a bilingual dictionary as an inherent part of their model, but did not pay particular attention to the quality of the dictionary entries, using either automatic translations of frequent words or word alignments of all words. [Vulic \u0301 and Korhonen, 2016] emphasize the role of the seed dictionary used to learn the projection matrix. They propose a hybrid model that first learns a first common bilingual embedding space, based on an existing lingual embedding model. Subsequently, they use this initial vector space to obtain translations for a list of frequent source words by projecting it into the space and using the closest neighbor in the target language as the translation. By using these translation pairs as seed words, they learn a projection matrix analogous to [Mikolov et al., 2013b]. In addition, they project a symmetry restriction that causes the second neighbors to fall back into the first 0000001, when their neighbors are in addition to their 0000001."}, {"heading": "2.9 Adversarial auto-encoder", "text": "All previous approaches to learning a transformation matrix between monolingual representations in different languages require either a dictionary or word alignments as a source of parallel data. [Baron, 2016], on the other hand, seeks to get closer to the elusive goal of creating linguistic representations without parallel data. He suggests using a contrarian auto-encoder to transform source embedding into the target embedding space. The auto-encoder is then trained to reconstruct the source embedding while the discriminator is trained to distinguish the projected source embedding from the actual target embedding, as shown in Figure 7. Although complicated, learning a transformation between languages without parallel data at this point does not seem practicable at all. Future approaches that aim to learn an assignment with less and less parallel data could bring us closer to this goal. However, it remains unclear, in general terms, whether the embedding space in another language can facilitate the incorporation of a project space."}, {"heading": "3 Pseudo-cross-lingual", "text": "The second type of cross-language model aims to construct a pseudo-cross-language corpus that captures interactions between words in different languages. Most approaches aim to identify words that can be translated into monolingual corpus of different languages and replace them with placeholders to ensure that translations of the same word have the same vector representation."}, {"heading": "3.1 Mapping of translations to same representation", "text": "[Xiao and Guo, 2014] propose the first pseudo-lingual method that translation pairs effectively employ: They first translate all words that occur in the source and target language corpus into the target language using Wiktionary. Since these translation pairs are still very loud, they filter them by removing multilingual words in the source and target language, as well as translations that do not appear in the target language corpus. From this bilingual dictionary, they now create a common vocabulary in which each translation pair has the same vector representation. For the training, they use the margin-based ranking of [Collobert and Weston, 2008] to rank correct word windows higher than corrupted ones, in which the middle word is replaced by an arbitrary word. Unlike the following methods, they do not explicitly construct a pseudo-lingual corpus, but instead feed into windows of both the source and target language - thus replacing the essence and the most common ones in the translation model."}, {"heading": "3.2 Random translation replacement", "text": "[Gouws and S\u00f8gaard, 2015], on the other hand, explicitly create a pseudo-translingual corpus: they use pairs of translations of words in the source and in the target language obtained through Google Translate. They link the source and target corpus and replace each word that is part of a translation pair with its translation equivalent with a 50% probability. They then train CBOW on this corpus. Interestingly, they also experiment with the exchange of words that are not based on translation but on language equivalence, i.e. words with the same language part are replaced with each other in different languages. While the exchange on language part leads to small improvements in the translinguistic tagging, the exchange based on language equivalents results in an even better performance of the task."}, {"heading": "3.3 On-the-fly replacement and polysemy handling", "text": "[Duong et al., 2016] suggest a similar approach to [Gouws and S\u00f8gaard, 2015]. They also use CBOW, which predicts the middle word in a window containing the surrounding words. Instead of randomly replacing each word in the corpus with its translation during pre-processing, they replace each middle word during training with a spontaneous translation. In addition to earlier approaches, they also try to handle polysemy explicitly by proposing an EMinspired method that chooses w-i as a substitute for the translation, the representation of which is most similar to the combination of the original word vwi and the context vector hi: w-i = argmaxw-dict (wi + hi, vw) cos (vwi + hi), where dict (wi) chooses the translations of Wire.They then learn together to predict both the words and their corresponding translations."}, {"heading": "3.4 Multilingual cluster", "text": "[Ammar et al., 2016] suggest a different approach, similar to the previous method used by [Gouws and S\u00f8gaard, 2015]: they use bilingual dictionaries to find clusters of synonymous words in different languages, then concatenate the monolingual corporas of different languages and replace tokens in the same cluster with the cluster ID. They then train SGNS on the concatenated corpus."}, {"heading": "3.5 Document merge and shuffle", "text": "Previous methods all use a bilingual dictionary or translation tool as a source for translation pairs that can be used as replacements. [Vulic \u0301 and Moens, 2016] present a model that eliminates translation pairs and learns cross-language embedding only from document-oriented data. In contrast to previous methods, the authors suggest merging not two monolingual corpora, but two coordinated documents from different languages into a pseudo-bilingual documentation, concatenating the documents and then mixing them by random permutation of the words. Since most methods are based on word embedding based on their context, mixing the documents would result in bilingual contexts for each word, allowing for the creation of a robust embedding space. However, since mixing is necessarily random, it could lead to suboptimal configurations. Therefore, they propose another fusion strategy that assumes that the structures are similar."}, {"heading": "4 Cross-lingual training", "text": "Cross-language training approaches focus exclusively on the optimization of the cross-language goal. These approaches are typically based on sentence orientation and not on a bilingual lexicon, and require a parallel corpus for the training."}, {"heading": "4.1 Bilingual compositional sentence model", "text": "The first approach, which optimizes only one cross-lingual goal, is the bilingual compositional typesetting model by [Hermann and Blunsom, 2013]. They train two models to produce sentence representations of aligned sentences in two languages and use the distance between the two sentence representations as the target. They minimize the following loss: Edist (a, b) = embedding the words in the corresponding sentence. The complete model is shown in Figure 8.They then train the model to obtain a higher score for correct translations than for randomly sampled erroneous translations using the maximum hinge loss of [Collobert and Weston, 2008]."}, {"heading": "4.2 Bilingual bag-of-words autoencoder", "text": "Instead of minimizing the distance between two sentence representations in different languages, [Lauly et al., 2013] aim to reconstruct the target sentence from the original source sentence. They start with a monolingual autocoder that encodes an input sentence as the sum of its word embeddings and tries to reconstruct the original source sentence. For efficient reconstruction, they opt for a tree-based decoder that resembles a hierarchical Softmax. They then expand this autocoder with a second decoder that reconstructs the aligned target sentence from the representation of the source sentence, as in Figure 9. Encoders and decoders have language-specific parameters. For an aligned sentence pair, they then train the model with four reconstruction losses: For each of the two sentences, they reconstruct the sentence to themselves and its equivalent in the other language."}, {"heading": "4.3 Distributed word alignment", "text": "While previous approaches required word alignments as a prerequisite for learning cross-language embeddings, [Koc \u0441isk\u00fd et al., 2014] they simultaneously learn word embeddings and alignments. Their model, Distributed Word Alignment, combines a distributed version of FastAlign [Dyer et al., 2013] with a language model. Similar to other bilingual approaches, they use the word in the source language sentence of an aligned sentence pair to predict the word in the target language set. They replace the standard multinomial translation probability of FastAlign with an energy function that attempts to bring the representation of a target word f close to the sum of the context words around the word ei in the source sentence: E (f, ei) = \u2212 (k \u00b2 s = \u2212 k rTei + sTs) rf \u2212 bfwo rei + s and rf \u2212 are vector representations of source and target words."}, {"heading": "4.4 Bilingual compositional document model", "text": "[Hermann and Blunsom, 2014] extend their approach [Hermann and Blunsom, 2013] to documents by applying their composition and objective function recursively to the composition of sentences in documents. Initially, sentence representations are calculated as in Section 4.1. These sentence representations are then fed into a document-level compositional vector model that integrates sentence representations in the same way as in Figure 10.The advantage of this method is that weaker monitoring can be used in the form of document-level alignment instead of or in conjunction with sentence-level alignment. The authors conduct experiments on both Europarl and a newly created corpus of multilingual TED talk transcriptions and find that the document signal significantly helps. In addition, they propose another compositional function that - instead of summarizing the representations - applies non-linearity to bigrampaars: f (x) = n i = tanxi (\u2212 1 + xi) They note that this composition is small, but less than proportional."}, {"heading": "4.5 Bag-of-words autoencoder with correlation", "text": "[Chandar et al., 2014] extend the approach of [Lauly et al., 2013] in two ways: Instead of using a tree-based decoder to calculate the loss of reconstruction, they reconstruct a sparse binary vector of word occurrences as in Figure 11. Due to the high dimensionality of the binary dictionary, the reconstruction is slower. As they perform the training using a mini-stack-gradient waste, where each mini-stack consists of adjacent sentences, they propose to merge the word bags of the mini-stack into a single dictionary and perform updates based on the merged word usage, noting that this performs well and even surpasses the tree-based decoder. Second, they propose to enhance the objective function that correlates the representations a (x), a (of language) and the target language by summing up all correlations of the two dimensions."}, {"heading": "4.6 Bilingual paragraph vectors", "text": "Similar to the previous methods [Pham et al., 2015], they learn sentence representations as a means of learning cross-language word embedding; they extend paragraph vectors [Le and Mikolov, 2014] to the multilingual environment by forcing aligned sentences of different languages to share the same vector representation as in Figure 12 where transmission is taking place. Common sentence representation is linked to the sum of the previous N-words in the sentence, and the model is trained to predict the next word in the sentence. To speed up the training, the authors use a hierarchical Softmax method. Since the model only learns representations for the sentences it has seen during the training, at test points for an unknown sentence, the sentence representation is randomly initialized and the model is trained to predict only the words in the sentence. Only the sentence vector is updated, while the other model parameters are frozen."}, {"heading": "4.7 Translation-invariant LSA", "text": "In addition to word embedding models such as Skip-gram, matrix factoration approaches have been used successfully in the past to learn word representation. One of the most popular methods is LSA, which [Gardner et al., 2015] is being extended as translation-invariant LSA to learn cross-language word embedding. It factorises a multilingual co-occurence matrix with the caveat that it should be invariant to translation, i.e. it should remain the same when multiplied by the corresponding word or context dictionary."}, {"heading": "4.8 Inverted indexing on Wikipedia", "text": "In contrast, [S\u00f8gaard et al., 2015] propose an approach that dispenses with any of these methods, but instead relies on the structure of the multilingual knowledge base Wikipedia, which they exploit by inversely indexing. Their method is based on the intuition that similar words are used to describe the same concepts in different languages. In Wikipedia, articles in multiple languages deal with the same concept. Typically, we would present each concept with the terms used to describe it in different languages. To learn cross-border word representations, we can now simply reverse the index and present a word in the way Wikipedia describes it. In this way, we are provided directly with cross-linguistic representations of words without making any tweaks. As a follow-up step, we can perform dimensionality reductions based on the word representations generated. While previous methods are able to effectively embed the word in the common space and not learn the parallel representations well."}, {"heading": "5 Joint optimisation", "text": "Models that use a common optimization aim precisely at this: they do not only consider a linguistic constraint, but jointly optimize monolingual and linguistic objectives. In practice, for two languages l1 and l2, these models optimize a monolingual loss M for each language and one or more terms that regulate the transfer from language l1 to l2 (and vice versa): Ml1 + Ml2 + \u03bb (L1 \u2192 L2 + L2 \u2192 L1), where \u03bb is an interpolation parameter that regulates the effects of linguistic regulation."}, {"heading": "5.1 Multi-task language model", "text": "The first jointly optimized model for learning cross-lingual representations was developed by [Klementiev et al., 2012]. They train a neural language model for each language and jointly optimize the monolingual maximum probability target of each language model with a word-based MT regularization term as the linguistic target. Therefore, the monolingual goal is to maximize the probability of the current word wt in view of its n surrounding words: M = log P (wt | wt \u2212 n + 1: t \u2212 1). This is optimized using the classical language model of [Bengio et al., 2003]. The cross-linguistic regularization term in turn encourages the representations of words that are often aligned with each other: B = 12 cT (A I) cwhere A is the matrix for capturing alignment values, I is the identity matrix, the Kronecker product, and c is the representation of the word wt."}, {"heading": "5.2 Bilingual matrix factorisation", "text": "[Zou et al., 2013] use a matrix factoring approach in the spirit of GloVe [Pennington et al., 2014] to learn cross-language word representations for English and Chinese. They create two alignment matrices Aen \u2192 zh and Azh \u2192 en using alignment counts that are automatically learned from the Chinese gigaword corpus. In Aen \u2192 zh, each element aij contains the number of times that the i-th Chinese word has been aligned with the j-th English word, each line being normalized to the sum of 1. If a word in the source language is intuitively aligned with only one word in the target language, then these words should have the same representation. If the target word is aligned with more than one source word, then its representation should be a combination of the representations of its aligned words. Intuitively, if a word in the source language is aligned with only one word in the target language, then those words should have the same representation."}, {"heading": "5.3 Bilingual skip-gram", "text": "[Luong et al., 2015] for their part, extend Skip-gram to cross-language setting and use Skip-gram targets as monolingual and cross-language targets. Instead of predicting only the surrounding words in the source language, they use the words in the source language to additionally predict their aligned words in the target language, as shown in Figure 13. For this purpose, they need word alignment information. They propose two ways to predict aligned words: In their first method, they automatically learn alignment information; when a word is not aligned, the alignments of its neighbors are used to predict. In their second method, they assume that words in the source and target sentences are monotonously aligned, with each source word at position i \u00b7 T / S aligned to the target word at position i \u00b7 T / S, with S and T being the source and target lengths. They find that a simple monotonic alignment is comparable to the insurmountable alignment in performance."}, {"heading": "5.4 Bilingual bag-of-words without word alignments", "text": "[Gouws et al., 2015] propose a bilingual vocabulary without Word Alignments (BilBOWA) that uses additional monolingual data. They use the Skip-gram target as a monolingual target and a novel sampled l2 loss as a cross-language regularizer as shown in Figure 14. Specifically, instead of relying on expensive word alignments, they simply assume that every word in a source sentence matches every word in the target sentence under a uniform alignment model. Thus, instead of minimizing the distance between words that have been aligned, they minimize the distance between the means of word representation in the aligned sentences shown in Figure 15, where se and sf are the sentences in the source and target languages. Thus, the cross-linguistic goal in the BilBOWA model is as follows:"}, {"heading": "5.5 Bilingual skip-gram without word alignments", "text": "A further extension of the skip program to the learning of cross-lingual representations is proposed by [Coulmance et al., 2015]. They also use the regular Skip-gram target as a monolingual target. For the cross-lingual target, they assume a similar assumption as [Gouws et al., 2015], assuming that every word in the source sentence is uniformly aligned with every word in the target sentence. Under the Skip-gram formulation, they treat every word in the target sentence as the context of every word in the source sentence, thus training their model to predict all words in the target sentence with the following Skip-gram target: E, F = 1 (sl1, sl2), E: Cl1, L2: wl1, E: sl1, E: sl2: sl1, E: sl2 \u2212 Log: cl2 \u2212 Log: D (wl1, cl2), where s is the sentence in the target sentence in the respective language, C: the query in the respective language, the target sentence, the C: the corpse, and the target object \u2212 object \u2212."}, {"heading": "5.6 Joint matrix factorisation", "text": "[Shi et al., 2015] use a common matrix factoration model to learn cross-border representations. In contrast to [Zou et al., 2013], they also consider additional monolingual data. Similar to the former, they also use the GloVe target [Pennington et al., 2014] as a monolingual target: Mli = f (X lijk) (w li \u00b7 c li k + b li wj + b li ck + bli \u2212 M lijk) wherewlij and c li k are the embeddings andM li jk the PMI value of a word-context pair (j, k) in the language li, b li ck and bli are the word-specific and language-specific bias terms. They then place transboundary restrictions on the monolingual representations as seen in Figure 17."}, {"heading": "5.7 Bilingual sparse representations", "text": "[Vyas and Carpuat, 2016] propose another method based on matrix factorization, which - in contrast to previous approaches - enables the learning of economical monolingual representations. First, they independently learn two monolingual word representations Xe and Xf in two different languages using GloVe [Pennington et al., 2014] on two large monolingual corporations. From these dense representations, they then learn monolingual, sparse representations by splitting X into two matrices A and D, minimizing the reconstruction error L2, with an additional restriction to A for spareness: Mli = vli = 1 \"AliiDTli \u2212 Xlii + 1\" alii, \"where vli is the number of dense word representations in the lip. However, the above equation creates only a sparse monolingual embedding. To learn bilingual embedding, they add a further restriction based on the number of word directions in the language learning process."}, {"heading": "5.8 Bilingual paragraph vectors (without parallel data)", "text": "[Mogadala and Rettinger, 2016] use a similar approach to [Pham et al., 2015], but also extend it to work without parallel data. They objectively use the paragraph vectors as monolingual objectiveM. They optimize this goal along with a transnational regulatory function that encourages the representation of words in the languages l1 and l2 to be close to each other. Their main innovation is that the transnational regulatory mechanism is adapted based on the nature of the training corpus. In addition to regulating the mean of word vectors in a sentence similar to the mean of word vectors in the aligned sentence to [Gouws et al., 2015] (the second term in the lower equation), they also regulate the paragraph vectors in the words l1 and SP l2 of the aligned sentences in the languages l1 and l2 to be close to each other."}, {"heading": "6 Incorporating visual information", "text": "A current field of research suggests incorporating visual information to improve the performance of monolingual [Lazaridou et al., 2015b] or cross-language [Vulic \u0301 et al., 2016] representations, which perform well in comparison tasks, demonstrate the use of zero-shot learning, and may ultimately be helpful in learning cross-language representations without (linguistic) parallel data."}, {"heading": "7 Challenges", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Functional modeling", "text": "Models for learning cross-linguistic representations have weaknesses with other vector space models of language: although they are very good at modeling the conceptual aspect of meanings evaluated in 3https: / / en.wikipedia.org / wiki / Procrustes _ analysisword similarity tasks, they fail to properly model the functional aspect of meaning, such as whether to note \"Give me a pencil\" or \"Give me that pencil.\""}, {"heading": "7.2 Word order", "text": "Secondly, due to the reliance on bag representations, current models for learning cross-language word embedding completely ignore word order. Models that do not know the word order, for example, assign exactly the same representation to the following pair of sentences [Landauer and Dumais, 1997], because they contain the same set of words, although they are completely different in meaning: - \"On that day, the office manager who drank hit the problem vendor with a bottle, but it was not serious.\" - \"It was not the sales manager who hit the bottle that day, but the office worker with a serious drinking problem.\""}, {"heading": "7.3 Compositionality", "text": "Most approaches to learning cross-language representations focus on word representations. These approaches are not easily able to form word representations to form representations of sentences and documents. Even approaches that learn word and sentence representations together do so by simply adding words in a sentence. In the future, it will be interesting to see if LSTMs or CNNs that can form more composite sentence representations can be used efficiently to learn cross-language representations."}, {"heading": "7.4 Polysemy", "text": "While mixing multiple senses of a word is already problematic when learning monolingual word representations, this problem is amplified in a lingual embedding space: monosemous words in one language may overlap with polysemic words in another language and thus not capture the entirety of linguistic relationships. There is already promising work on learning monolingual multi-meaning embeddings. We expect that learning cross-language multi-meaning embeddings will become increasingly important as it enables us to grasp finer-grained multilingual meanings."}, {"heading": "7.5 Feasibility", "text": "The final challenge concerns the feasibility of learning cross-language embedding oneself: languages are incredibly complex human artifacts. Learning a monolingual embedding space is already difficult; dividing such a vector space between two languages, and expecting that interlingual and intra-linguistic relations will be reliably reflected, then seems utopian. Moreover, some languages have linguistic characteristics that other languages lack; the ease of constructing a common embedding space between languages, and consequently the success of translinguistic transfer, is intuitively proportional to the similarity of languages: an embedding space shared between Spanish and Portuguese tends to capture more linguistic nuances than an embedding space populated with English and Chinese representations. If two languages are too different, linguistic transfer may not be possible at all - similar to the negative transfer that occurs when domains are adapted between very dissimilar domains."}, {"heading": "7.6 Evaluation", "text": "After measuring the models to get to know the linguistic representations, we now want to know which is the best method for the task we are interested in. Linguistic representation models were evaluated on a wide range of tasks such as document classification (CLDC), machine translation (MT), cross-language variation of the following tasks: named entity recognition, part of language conference, dependency analysis and dictionary production. In the context of CLDC evaluation by [Klementiev et al.] 40-dimensional word embedding is learned to classify documents in one language and evaluate documents in another language. As CLDC is one of the most widely used, here is an example of the evaluation table by [Mogadala and Rettinger] for this task."}, {"heading": "8 Conclusion", "text": "Models that allow us to learn cross-language representations have already proven themselves in a variety of tasks, such as machine translation (decoding and evaluation), automatic generation of bilingual dictionaries, linguistic information gathering, parallel body extraction and generation, and cross-language plagiarism detection. It will be interesting to see what further advances the future will bring."}], "references": [{"title": "Massively Multilingual Word Embeddings", "author": ["Ammar et al", "W. 2016] Ammar", "G. Mulcaire", "Y. Tsvetkov", "G. Lample", "C. Dyer", "N.A. Smith"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance", "author": ["Artetxe et al", "M. 2016] Artetxe", "G. Labaka", "E. Agirre"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders", "author": ["Barone", "A.V.M. 2016] Barone"], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP,", "citeRegEx": "Barone and Barone,? \\Q2016\\E", "shortCiteRegEx": "Barone and Barone", "year": 2016}, {"title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "author": ["Bolukbasi et al", "T. 2016] Bolukbasi", "Chang", "K.-W", "J. Zou", "V. Saligrama", "A. Kalai"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "An Autoencoder Approach to Learning Bilingual Word Representations", "author": ["Chandar et al", "S. 2014] Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V. Raykar", "A. Saha"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing", "author": ["Collobert", "Weston", "R. 2008] Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Trans-gram, Fast Cross-lingual Word-embeddings", "author": ["Coulmance et al", "J. 2015] Coulmance", "Marty", "J.-M", "G. Wenzek", "A. Benhalloum"], "venue": "EMNLP 2015,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "author": ["Duong et al", "L. 2016] Duong", "H. Kanayama", "T. Ma", "S. Bird", "T. Cohn"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16)", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "A simple, fast, and effective parameterization of IBM model 2", "author": ["Dyer et al", "C. 2013] Dyer", "V. Chahuneau", "N. Smith"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Faruqui", "Dyer", "M. 2014] Faruqui", "C. Dyer"], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Translation Invariant Word Embeddings", "author": ["Gardner et al", "M. 2015] Gardner", "K. Huang", "E. Paplexakis", "X. Fu", "P. Talukdar", "C. Faloutsos", "N. Sidiropoulos", "T. Mitchell"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "author": ["Gouws et al", "S. 2015] Gouws", "Y. Bengio", "G. Corrado"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard", "S. 2015] Gouws", "A. S\u00f8gaard"], "venue": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Cross-lingual Dependency Parsing Based on Distributed Representations", "author": ["Guo et al", "J. 2015] Guo", "W. Che", "D. Yarowsky", "H. Wang", "T. Liu"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Hermann", "Blunsom", "K.M. 2013] Hermann", "P. Blunsom"], "venue": "arXiv preprint arXiv:1312.6173", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Multilingual Models for Compositional Distributed Semantics", "author": ["Hermann", "Blunsom", "K.M. 2014] Hermann", "P. Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Inducing Crosslingual Distributed Representations of Words", "author": ["Klementiev et al", "A. 2012] Klementiev", "I. Titov", "B. Bhattarai"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Learning Bilingual Word Representations by Marginalizing Alignments", "author": ["Ko\u010disk\u00fd et al", "T. 2014] Ko\u010disk\u00fd", "K.M. Hermann", "P. Blunsom"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.", "author": ["Landauer", "Dumais", "T.K. 1997] Landauer", "S.T. Dumais"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Learning Multilingual Word Representations using a Bag-of-Words Autoencoder", "author": ["Lauly et al", "S. 2013] Lauly", "A. Boulanger", "H. Larochelle"], "venue": "NIPS WS on Deep Learning,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning", "author": ["Lazaridou et al", "A. 2015a] Lazaridou", "G. Dinu", "M. Baroni"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Combining Language and Vision with a Multimodal Skip-gram Model", "author": ["Lazaridou et al", "A. 2015b] Lazaridou", "T.P. Nghia", "M. Baroni"], "venue": "Proceedings of Human Language Technologies:", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Mikolov", "Q.V. 2014] Le", "T. Mikolov"], "venue": "International Conference on Machine Learning - ICML", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments", "author": ["Levy et al", "O. 2017] Levy", "A. S\u00f8gaard", "Y. Goldberg"], "venue": "EACL", "citeRegEx": "al. et al\\.,? \\Q2017\\E", "shortCiteRegEx": "al. et al\\.", "year": 2017}, {"title": "Bilingual Word Representations with Monolingual Quality in Mind", "author": ["Luong et al", "2015] Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "Workshop on Vector Modeling for NLP,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov et al", "T. 2013a] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Mikolov et al", "T. 2013b] Mikolov", "Q.V. Le", "I. Sutskever"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification", "author": ["Mogadala", "Rettinger", "A. 2016] Mogadala", "A. Rettinger"], "venue": null, "citeRegEx": "Mogadala et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mogadala et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Pennington et al", "J. 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Learning Distributed Representations for Multilingual Text Sequences", "author": ["Pham et al", "H. 2015] Pham", "Luong", "M.-T", "C.D. Manning"], "venue": "Workshop on Vector Modeling for NLP,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Learning Cross-lingual Word Embeddings via Matrix Co-factorization", "author": ["Shi et al", "T. 2015] Shi", "Z. Liu", "Y. Liu", "M. Sun"], "venue": "Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Inverted indexing for cross-lingual NLP. The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference of the Asian Federation of Natural Language Processing (ACL-IJCNLP", "author": ["S\u00f8gaard et al", "A. 2015] S\u00f8gaard", "Z. Agic", "H.M. Alonso", "B. Plank", "B. Bohnet", "A. Johannsen"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "author": ["Upadhyay et al", "S. 2016] Upadhyay", "M. Faruqui", "C. Dyer", "D. Roth"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Multi-Modal Representations for Improved Bilingual Lexicon Learning", "author": ["Vuli\u0107 et al", "I. 2016] Vuli\u0107", "D. Kiela", "S. Clark", "Moens", "M.-F"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "On the Role of Seed Lexicons in Learning Bilingual Word Embeddings", "author": ["Vuli\u0107", "Korhonen", "I. 2016] Vuli\u0107", "A. Korhonen"], "venue": "Proceedings of ACL,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", "author": ["Vuli\u0107", "Moens", "I. 2016] Vuli\u0107", "M.-F"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment", "author": ["Vyas", "Carpuat", "Y. 2016] Vyas", "M. Carpuat"], "venue": null, "citeRegEx": "Vyas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vyas et al\\.", "year": 2016}, {"title": "Distributed Word Representation Learning for Cross-Lingual Dependency Parsing", "author": ["Xiao", "Guo", "M. 2014] Xiao", "Y. Guo"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation", "author": ["Xing et al", "C. 2015] Xing", "C. Liu", "D. Wang", "Y. Lin"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Zou et al", "W.Y. 2013] Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Pseudocrosslingual (\u00a73) Mapping to same representation [Xiao and Guo, 2014] Lexicon Random translation replacement [Gouws et al., 2015] On-the-fly replacement with polysemy [Duong et al.", "startOffset": 115, "endOffset": 135}, {"referenceID": 12, "context": ", 2015] Bilingual bag-of-words, not word-aligned [Gouws et al., 2015] Sentence-aligned Bilingual skip-gram, not word-aligned [Coulmance et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 12, "context": "[Gouws et al., 2015] propose a Bilingual Bag-of-Words without Word Alignments (BilBOWA) that leverages additional monolingual data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Figure 14: The BilBOWA model [Gouws et al., 2015]", "startOffset": 29, "endOffset": 49}, {"referenceID": 12, "context": "Figure 15: Approximating word alignments with uniform alignments [Gouws et al., 2015]", "startOffset": 65, "endOffset": 85}, {"referenceID": 12, "context": "For the cross-lingual objective, they make a similar assumption as [Gouws et al., 2015] by supposing that every word in the source sentence is uniformly aligned to every word in the target sentence.", "startOffset": 67, "endOffset": 87}, {"referenceID": 12, "context": "These co-occurrences can be calculated without alignment information using a uniform alignment model as in [Gouws et al., 2015].", "startOffset": 107, "endOffset": 127}, {"referenceID": 12, "context": "In addition to regularising the mean of word vectors in a sentence to be close to the mean of word vectors in the aligned sentence similar to [Gouws et al., 2015] (the second term in the below equation), they also regularise the paragraph vectors SP l1 and SP l2 of aligned sentences in languages l1 and l2 to be close to each other.", "startOffset": 142, "endOffset": 162}, {"referenceID": 34, "context": ", 2015b] or cross-lingual [Vuli\u0107 et al., 2016] representations.", "startOffset": 26, "endOffset": 46}, {"referenceID": 12, "context": "4 [Gouws et al., 2015] 86.", "startOffset": 2, "endOffset": 22}], "year": 2017, "abstractText": "Cross-lingual embedding models allow us to project words from different languages into a shared embedding space. This allows us to apply models trained on languages with a lot of data, e.g. English to low-resource languages. In the following, we will survey models that seek to learn cross-lingual embeddings. We will discuss them based on the type of approach and the nature of parallel data that they employ. Finally, we will present challenges and summarize how to evaluate cross-lingual embedding models. In recent years, driven by the success of word embeddings, many models that learn accurate representations of words haven been proposed [Mikolov et al., 2013a, Pennington et al., 2014]. However, these models are generally restricted to capture representations of words in the language they were trained on. The availability of resources, training data, and benchmarks in English leads to a disproportionate focus on the English language and a negligence of the plethora of other languages that are spoken around the world. In our globalised society, where national borders increasingly blur, where the Internet gives everyone equal access to information, it is thus imperative that we do not only seek to eliminate bias pertaining to gender or race [Bolukbasi et al., 2016] inherent in our representations, but also aim to address our bias towards language. To remedy this and level the linguistic playing field, we would like to leverage our existing knowledge in English to equip our models with the capability to process other languages. Perfect machine translation (MT) would allow this. However, we do not need to actually translate examples, as long as we are able to project examples into a common subspace such as the one in Figure 1. Figure 1: A shared embedding space between two languages [Luong et al., 2015] \u2217This article originally appeared as a blog post at http://sebastianruder.com/ cross-lingual-embeddings/index.html on 28 November 2016. ar X iv :1 70 6. 04 90 2v 1 [ cs .C L ] 1 5 Ju n 20 17 Ultimately, our goal is to learn a shared embedding space between words in all languages. Equipped with such a vector space, we are able to train our models on data in any language. By projecting examples available in one language into this space, our model simultaneously obtains the capability to perform predictions in all other languages (we are glossing over some considerations here; for these, refer to Section 7. This is the promise of cross-lingual embeddings. Over the course of this survey, we will give an overview of models and algorithms that have been used to come closer to the elusive goal of capturing the relations between words in multiple languages in a common embedding space. Note that while neural MT approaches implicitly learn a shared cross-lingual embedding space by optimizing for the MT objective, we will focus on models that explicitly learn cross-lingual word representations throughout this blog post. These methods generally do so at a much lower cost than MT and can be considered to be to MT what word embedding models [Mikolov et al., 2013a, Pennington et al., 2014] are to language modelling. 1 Types of cross-lingual embedding models In recent years, various models for learning cross-lingual representations have been proposed. In the following, we will order them by the type of approach that they employ. Note that while the nature of the parallel data used is equally discriminatory and has been shown to account for inter-model performance differences [Levy et al., 2017], we consider the type of approach more conducive to understanding the assumptions a model makes and \u2013 consequently \u2013 its advantages and deficiencies. Cross-lingual embedding models generally use four different approaches: 1. Monolingual mapping: These models initially train monolingual word embeddings on large monolingual corpora. They then learn a linear mapping between monolingual representations in different languages to enable them to map unknown words from the source language to the target language. 2. Pseudo-cross-lingual: These approaches create a pseudo-cross-lingual corpus by mixing contexts of different languages. They then train an off-the-shelf word embedding model on the created corpus. The intuition is that the cross-lingual contexts allow the learned representations to capture cross-lingual relations. 3. Cross-lingual training: These models train their embeddings on a parallel corpus and optimize a cross-lingual constraint between embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space. 4. Joint optimization: These approaches train their models on parallel (and optionally monolingual data). They jointly optimise a combination of monolingual and cross-lingual losses. In terms of parallel data, methods may use different supervision signals that depend on the type of data used. These are, from most to least expensive: 1. Word-aligned data: A parallel corpus with word alignments that is commonly used for machine translation; this is the most expensive type of parallel data to use. 2. Sentence-aligned data: A parallel corpus without word alignments. If not otherwise specified, the model uses the Europarl corpus2 consisting of sentence-aligned text from the proceedings of the European parliament that is generally used for training Statistical Machine Translation models. 3. Document-aligned data: A corpus containing documents in different languages. The documents can be topic-aligned (e.g. Wikipedia) or label/class-aligned (e.g. sentiment analysis and multi-class classification datasets). 4. Lexicon: A bilingual or cross-lingual dictionary with pairs of translations between words in different languages. 5. No parallel data: No parallel data whatsoever. Learning cross-lingual representations from only monolingual resources would enable zero-shot learning across languages. http://www.statmt.org/europarl/", "creator": "LaTeX with hyperref package"}}}