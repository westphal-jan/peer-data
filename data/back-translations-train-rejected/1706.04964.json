{"id": "1706.04964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory", "abstract": "Deep neural networks are known to be difficult to train due to the instability of back-propagation. A deep \\emph{residual network} (ResNet) with identity loops remedies this by stabilizing gradient computations. We prove a boosting theory for the ResNet architecture. We construct $T$ weak module classifiers, each contains two of the $T$ layers, such that the combined strong learner is a ResNet. Therefore, we introduce an alternative Deep ResNet training algorithm, \\emph{BoostResNet}, which is particularly suitable in non-differentiable architectures. Our proposed algorithm merely requires a sequential training of $T$ \"shallow ResNets\" which are inexpensive. We prove that the training error decays exponentially with the depth $T$ if the \\emph{weak module classifiers} that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. Our results apply to general multi-class ResNets. A generalization error bound based on margin theory is proved and suggests ResNet's resistant to overfitting under network with $l_1$ norm bounded weights.", "histories": [["v1", "Thu, 15 Jun 2017 16:59:07 GMT  (327kb)", "http://arxiv.org/abs/1706.04964v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["furong huang", "jordan ash", "john langford", "robert schapire"], "accepted": false, "id": "1706.04964"}, "pdf": {"name": "1706.04964.pdf", "metadata": {"source": "CRF", "title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory", "authors": ["Furong Huang"], "emails": ["furongh@cs.umd.edu", "jordantash@gmail.com", "jcl@microsoft.com", "schapire@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.04 964v 1 [cs.L G] 1"}, {"heading": "1 Introduction", "text": "Dei nlrerueaeVngn nvo nde nlreruiaeaeVnlrsrsrteeoivnlrrteeoiiiuiuuuugnnlrrteeeeeegnn rf\u00fc ide nlrrreeaeaeVnlrrrsgn rf\u00fc nde nlrreeVnlrteeeeeeeeeaeVnlrrrrrgteeeeeeoeeeoeoeoVnuiuuueaeaeaeaeeeaeeeaeeeaeeeeeeaeeeeeeeeeeaeeeeeeeeeenrrr\u00fc rf\u00fc ide rf\u00fc-eaeaeaeeeeeeeaeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeVngrgrgnngnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnrue"}, {"heading": "1.1 Summary of Results", "text": "We propose a novel framework that provides multi-channel telescope sum increases (defined in Section 4) to characterize a feed forward ResNet in Section 3. We show that the top (final) output of a ResNet is a telescopic sum of its pairs of consecutive module differences. Theoretical analyses such as training error guarantees and generalization error limits for telescope sum increases are provided. To address the problem of worsening training errors with increasing depth, we implement a guaranteed learning algorithm called BoostResNet to sequentially train modules of ResNet. BoostResNet appropriately selects training samples or changes the cost function (Section 4 Theorem 4.2). BoostResNet trains the ResNet with guarantees: The training error exponentially disintegrates with the depth of the network. As discussed later in Section 4.4, Result Result Result Resource Modulus Resource Error is more effective than Resource Error Generalization Net."}, {"heading": "1.2 Related Works", "text": "There are two main ways to address this optimization problem: one is to select a loss function and network architecture that have better geometric properties, and the other is to improve the learning processes of the network. (The other is) There is extensive work (Girshick, 2015) in which there are many predefined loss functions and criteria that are commonly used. (The other is that selecting or modifying loss functions is about avoiding difficulties such as exploding / disappearing gradients or slow learning processes (Balduzzi et al, 2017). However, there are no rigorous principles for selecting a loss function in general.) Other work considers variations of MLP or CNN by adding identities or slow learning processes (Balduzzi et al, 2017)."}, {"heading": "3 ResNet in Telescoping Sum Boosting Framework", "text": "Since we remember from Equation 2, ResNet actually has a form similar to the strong classification in the raising (1).The key difference is that the raising is an interplay of estimated hypotheses (1).To solve this problem, we introduce an auxiliary linear classification wt at the top of each ResNet module ft (gt (x))) to construct a hypotheses module. Formally, a hypotheses module is defined asot (x) def (x).R (3) in the binary classification setting. Therefore, ot ot (x) = w (x) + gt (x) as a weak (x) as gt (x) = ft (x)."}, {"heading": "4 Telescoping Sum Boosting for Binary Classification", "text": "Remember that the weak module classifier is defined as ht (x) = \u03b1t + 1ot + 1 (x) \u2212 \u03b1tot (x). We limit ourselves to limited classifiers | ot (x) | \u2264 1. During this work we assume that the covariance between exp (\u2212 yot + 1 (x)) and exp (yot (x)) is not positive. We propose a learning algorithm whose training error decreases exponentially with the number of weak module classifiers T under a weak learning condition below."}, {"heading": "4.1 Weak Learning Condition", "text": "A na\u00efve weak learning condition would be that we see a much weaker learning condition as a consequence. However, Definition 4.1 (\u03b3-weak learning condition) is too strong. Even if this na\u00efve learning condition is close to 1, we are still looking for a weak learner who consistently performs better than him. Instead, we are looking at a much weaker learning condition as a consequence. Definition 4.1 (\u03b3-weak learning condition). Let's try to find a weak learner who consistently performs better than him. Definition 4.1 (\u03b3-weak learning condition). Let's look at the weak learning condition in relation to a pair of distributions (Dt, Dt \u2212 1 [yot (x)] > 0."}, {"heading": "4.2 BoostResNet", "text": "We propose a new training algorithm for the next 1 + 1 modulated algorithms, defined by setting the binary class classification as in algorithm 1 (1). (1) Specifically, we are introducing a training method for deep ResNets 1 and 2 that requires only a sequential training. (1) Each of the shallow ResNets ft (gt (x) + gt (x) is combined with an additional linear classifier (except for the uppermost classification). The weights of the ResNets are trained on these shallow ResNets. (1) The weights of the ResNets are trained in this way. (2) Linear classifiers are sorted out. (except for the uppermost classification.) The training algorithm is therefore a module-by-module procedure that follows a lowest up mode."}, {"heading": "4.3 Oracle Implementation for ResNet", "text": "In algorithm 2, the conversion of the oracle is synonymous with (ft, \u03b1t + 1, wt + 1) = arg min (f, \u03b1, v) 1mm \u2211 i = 1exp (\u2212 yi\u03b1v [f (xi))) + gt (xi)]) (5) In practice, there are various ways to implement Equation (5). For example, Janzamin et al. (Janzamin et al., 2015) propose a tensor decomposition technique that decomposes a tensor that is formed by transforming the characteristics x in combination with labels y and restores the weights of a single-layer neural network with guarantees. One can also use backpropagation, as numerous studies have shown that training based on flat networks with identity loops is relatively stable (Hardt & Ma, 2016; He et al., 2016)."}, {"heading": "4.4 Generalization Error Analysis", "text": "In this section, we analyze the generalization error to understand the possibility of overfitting under algorithm 1. The strong classifier or resnet is F (x) = \u2211 t ht (x) \u03b1T + 1. Now, we define the margin for example (x, y) as yF (x). For simplicity, we look at MLP resnet with multiple channels n and assume that the weight vector that connects a neuron at the layer t with its precise layer neurons is l1 norm limited by an edge, t \u2212 1. Note that there is a linear classifier w on the top, and we limit ourselves to l1 norm limited classifiers, i.e., the expected training examples are l \u00b2 norm limited r \u00b2 def = ES-D [maxi] [m]."}, {"heading": "5 Experiments", "text": "We compare our proposed strategies to e2eBP, which is a real problem at the net level (LeCun et al., 1998) and \"Street View\" house numbers (Netzer et al., 2011). There are two different types of architectures, both endowed with the same random seed. Our experiments were programmed and executed as part of Lua's learning process. \"Tesla P100 GPUM\" is a mini-patch that uses both BoostResNet and e2eBP. \"The learning rate has been initialized,\" says the author. \"The optimization method in e2e-4.\" The optimization method in e2e-4 is the way it is used."}, {"heading": "6 Conclusions and Future Works", "text": "Our proposed BoostResNet algorithm achieves exponentially declining training errors (with depth T) under the weak learning condition. BoostResNet is much more efficient in computational terms compared to end-to-end repropagation in the deep ResNet. More importantly, the memory required by BoostResNet is trivial compared to end-to-end repropagation. This is particularly advantageous given the limited GPU memory and large network depth. Our learning framework is, of course, for non-differentiable data. For example, our learning framework can be adapted to weak learning oracles using tensor decomposition techniques. Tensor decomposition, a spectral learning framework with theoretical guarantees, is applied to learning a layer of MLP in (Janzamin et al., 2015)."}, {"heading": "A Proof for Lemma 3.2: the strong learner is a ResNet", "text": "Proof. In our algorithm, the input of the next module is the output of the current module + 1 (x) = ft (gt (x)) + gt (x), (7). Thus, we get that each weak learning module isht (x) = \u03b1t + 1w t + 1 (ft (gt (x))) + gt (x)) \u2212 \u03b1tw t gt (x) (8) = \u03b1t + 1w t + 1gt + 1 (x) \u2212 \u03b1tw t t gt (x) gt (x), (9) and similarly ht + 1 = \u03b1t + 2w t t + 2gt + 2 (x) \u2212 \u03b1t + 1w t + 1gt + 1 (x). (10) Therefore, the sum of ht (x) and ht + 1 (x) isht (x) + ht + 1 (x) T = 1t (x) + 1t (x) + 1t (x) + 1t (x)."}, {"heading": "B Proof for Theorem 4.2: binary class telescoping sum boosting theory", "text": "In our analysis the 0-1 loss is limited by exponential loss.The training error is therefore limited by Pr i + empirical D1 (p (xi) + 1w (xi) + 1gT + 1 (xi) + 1gT + 1 (i) 6 = yi) (13) = m empirical (i) (m empirical) (i) 1 {empirical (xi) + 1 (xi) 6 = empirical (i) 1 {empirical (T) 1 (xi) 6 = empirical (xi) 6 = empirical (xi) (empirical) (empirical) 6 = empirical (empirical) empirical (empirical). (empirical) empirically (empirical) not (empirical). (empirical) (empirical) (empirical) (empirical)."}, {"heading": "C Proof for Lemma 4.3: Generalization Bound", "text": "Rademacher complexity technique is powerful for measuring the complexity of H any functional family h: X \u00b2 R, based on the simplicity of matching all datasets in H (where X is any space). Let S = < x1,.., xm > be a sample of m points in X. The empirical Rademacher complexity of H in relation to S is defined in beRS (H) def = Ep h (H) m [sup h] i = 1\u03c3ih (xi)] (33), where the Rademacher complexity in relation to m data points from the distribution D is defined by Rm (H) = ES. (34) Proposition C.1. (Theorem 1 Cortes et al. (2014)) LetH be a hypothesis set a decomposition H = 1Hi."}, {"heading": "D Proof for Theorem D: Margin and Generalization Bound", "text": "Theorem D.1. [Error of generalization] Given algorithm 1 \u2212 49, the error of generalization PrD (yF (x) \u2264 0) satisfiesPr D (yF (x) \u2264 0) \u2264 (1 + 21\u03b3 T + 1) \u2264 T + 1 \u2212 2 exp (\u2212 12 \u03b32T) + 4C0r itely log (2n) 2mT \u2211 t = 1\u043d (2) t (2) t (2) t (2) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t) t (1) t (1) t (1) t) \u2212 t (1) t (1) t (t) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1 t) t (1) t (1 t) t (1 t (1) t (1) t (1) t (1 t) t (1 t (1) t (1) t (1 t (1) t (1 t) t (1 t) t (1 t (1 t), 1 t (1 t) t (1 t) t (1 t (1 t) t (1 t (1 t) t (1 t) t (1 t (1 t) t (1 t (1 t) t (1 t) t (1 t) t (1 t) t (1 t) t (1 t) t (1 t) t (1 t) t (1 t) t (1 t (1 t (1 t) t (1 t (1 t) t) t (1 t (1 t (1 t), 1 t (1 t) t) t (1 t (1 t) t (1 t (1 t) t (1 t) t (1 t (1 t) t"}, {"heading": "E Telescoping Sum Boosting for Multi-calss Classification", "text": "Remember that the weak module classification is defined as the best possible function for the best possible training."}, {"heading": "F Proof for Theorem E.3 multi-class boosting theory", "text": "To characterise the training error, we use the exponential loss function \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "G Minimal Weak Learning Condition", "text": "Mukherjee and SchapireMukherjee & Schapire (2013) have introduced a sufficient and necessary weak learning condition based on a concept that edge over random.It is intuitive to limit the cost matrices to a space of the cost matrices Ceor, which puts the least cost on the correct label.Definition G.1. The space of the cost matrices Ceor and Rm \u00b7 C is the space of the cost matrices C and Ceor, which includes the cost matrices C (i, yi) and C (i, l) \u2264 C (i,), as well as the space of the cost matrices Ceor and Rm}. (80) Let us define a random classifier as a baseline predictor B, Rm \u00d7 C, where each row lies on a simplexB (i,:)."}, {"heading": "H Experiments", "text": "We study the training performance of e2eBP at various ResNet depths. Surprisingly, we see a worsening of the training error at e2eBP, although the identity loop of the ResNet is supposed to alleviate this problem. Despite the presence of identity loops, the e2eBP is ultimately susceptible to false local optima. This phenomenon is examined in more detail in Figures 6a and 6b, which each show how training and testing accuracy changes during the adjustment process. Our proposed sequential training method BoostResNet alleviates problems with gradient instability and continues to perform well at increasing depth."}], "references": [{"title": "The Shattered Gradients Problem: If resnets are the answer, then what is the question? arXiv preprint arXiv:1702.08591", "author": ["Balduzzi", "David", "Frean", "Marcus", "Leary", "Lennox", "JP Lewis", "Ma", "Kurt Wan-Duo", "McWilliams", "Brian"], "venue": null, "citeRegEx": "Balduzzi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2017}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Deep boosting. Pages 1179\u20131187", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Syed", "Umar"], "venue": "of: Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "citeRegEx": "Cortes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2014}, {"title": "Adaptive subgradientmethods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "Pages 23\u201337", "citeRegEx": "Freund et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1995}, {"title": "Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition", "author": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": null, "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Fast r-cnn", "author": ["Girshick", "Ross."], "venue": "Pages 1440\u20131448 of: Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "Girshick and Ross.,? 2015", "shortCiteRegEx": "Girshick and Ross.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation. Pages 580\u2013587", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "of: Proceedings of the IEEE conference on computer vision and pattern recognition", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks. Pages 249\u2013256", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "of: Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Identity Matters in Deep Learning", "author": ["Hardt", "Moritz", "Ma", "Tengyu"], "venue": "arXiv preprint arXiv:1611.04231", "citeRegEx": "Hardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Convolutional neural networks at constrained time cost. Pages 5353\u20135360", "author": ["He", "Kaiming", "Sun", "Jian"], "venue": "of: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition. Pages 346\u2013361", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "of: European Conference on Computer Vision. Springer", "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "Pages 1026\u20131034 of: Proceedings of the IEEE international conference on computer vision", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition. Pages 770\u2013778", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "of: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima"], "venue": "arXiv preprint arXiv:1506.08473", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. Pages 1097\u20131105 of: Advances in neural information processing systems", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient backprop. Pages 9\u201348 of: Neural networks: Tricks of the trade", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation. Pages 3431\u20133440", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "of: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Deep vs. shallow networks: An approximation theory perspective", "author": ["Mhaskar", "Hrushikesh N", "Poggio", "Tomaso"], "venue": "Analysis and Applications,", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "A theory of multiclass boosting", "author": ["Mukherjee", "Indraneel", "Schapire", "Robert E"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mukherjee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2013}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)", "author": ["Nesterov", "Yurii."], "venue": "Pages 543\u2013547 of: Doklady an SSSR, vol. 269.", "citeRegEx": "Nesterov and Yurii.,? 1983", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "Reading digits in natural images with unsupervised feature learning. Page 5 of: NIPS workshop on deep learning and unsupervised feature", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["Qian", "Ning."], "venue": "Neural networks, 12(1), 145\u2013151.", "citeRegEx": "Qian and Ning.,? 1999", "shortCiteRegEx": "Qian and Ning.", "year": 1999}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. Pages 91\u201399 of: Advances in neural information processing systems", "author": ["Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross", "Sun", "Jian"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning", "author": ["Rubinstein", "Reuven Y", "Kroese", "Dirk P"], "venue": null, "citeRegEx": "Rubinstein et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2013}, {"title": "A survey of decision tree classifier methodology", "author": ["Safavian", "S Rasoul", "Landgrebe", "David"], "venue": "IEEE transactions on systems, man, and cybernetics,", "citeRegEx": "Safavian et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Safavian et al\\.", "year": 1991}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "SelfieBoost: A Boosting Algorithm for Deep Learning", "author": ["Shalev-Shwartz", "Shai."], "venue": "arXiv preprint arXiv:1411.3436.", "citeRegEx": "Shalev.Shwartz and Shai.,? 2014", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Convolutional networks and learning invariant to homogeneous multiplicative scalings", "author": ["Tygert", "Mark", "Szlam", "Arthur", "Chintala", "Soumith", "Ranzato", "Marc\u2019Aurelio", "Tian", "Yuandong", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1506.08230", "citeRegEx": "Tygert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tygert et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Zeiler", "Matthew D."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler and D.,? 2012", "shortCiteRegEx": "Zeiler and D.", "year": 2012}, {"title": "Visualizing and understanding convolutional networks. Pages 818\u2013833", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "of: European conference on computer vision. Springer", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "LetH be a hypothesis set admitting a decomposition H", "author": ["Cortes"], "venue": null, "citeRegEx": "Cortes,? \\Q2014\\E", "shortCiteRegEx": "Cortes", "year": 2014}, {"title": "Let us define a random classifier as a baseline predictor B \u2208 Rm\u00d7C , where each row lies on a simplexB(i", "author": [], "venue": "Beor", "citeRegEx": "C..,? \\Q2013\\E", "shortCiteRegEx": "C..", "year": 2013}, {"title": "A weak classifier space H is boostable if and only if H satisfies the weak-learning condition defined in Definition G.3", "author": ["Lemma G"], "venue": null, "citeRegEx": "G.4.,? \\Q2013\\E", "shortCiteRegEx": "G.4.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "5 Experiments We compare our proposed BoostResNet algorithm with e2eBP training a ResNet on the MNIST (LeCun et al. , 1998) and street view house numbers (SVHN) (Netzer et al. , 2011) benchmark datasets. Two different types of architectures are tested: multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual network (CNN-ResNet). In each experiment the architecture of both algorithms is identical, and they are both initialized with the same random seed. Our experiments were programmed in the Torch deep learning framework for Lua and executed on NVIDIA Tesla P100 GPUs. In our training, a mini-batch size 100 is used for both BoostResNet and e2eBP. The learning rate is initialized at 1e-2 with a decaying rate of 1e-4. The optimization method used in e2eBP is the stateof-the-art Adaptive Moment Estimation (Adam) Kingma & Ba (2014) variant of SGD introduced earlier.", "startOffset": 103, "endOffset": 876}, {"referenceID": 15, "context": "The oracle we used in BoostResNet to solve the weak module classifier is Adam as well, but could be extended to tensor methods Janzamin et al. (2015), decision trees Safavian & Landgrebe (1991) or other nonlinear classifiers for non-differentiable data.", "startOffset": 127, "endOffset": 150}, {"referenceID": 15, "context": "The oracle we used in BoostResNet to solve the weak module classifier is Adam as well, but could be extended to tensor methods Janzamin et al. (2015), decision trees Safavian & Landgrebe (1991) or other nonlinear classifiers for non-differentiable data.", "startOffset": 127, "endOffset": 194}, {"referenceID": 2, "context": "(Theorem 1 Cortes et al. (2014)) LetH be a hypothesis set admitting a decomposition H = \u222ai=1Hi for some l > 1.", "startOffset": 11, "endOffset": 32}, {"referenceID": 2, "context": "According to lemma 2 of Cortes et al. (2016), the empirical Rademacher complexity is bounded as a function of r\u221e, \u039bt and n: Rm(Ft) \u2264 r\u221e\u039bt \u221a log(2n) 2m (40) Overall, with probability at least 1\u2212 \u03b4,", "startOffset": 24, "endOffset": 45}, {"referenceID": 38, "context": "G Minimal Weak Learning Condition Mukherjee and SchapireMukherjee & Schapire (2013) introduced a sufficient and necessary weak learning condition based on a concept called edge over random.", "startOffset": 24, "endOffset": 84}, {"referenceID": 38, "context": "G Minimal Weak Learning Condition Mukherjee and SchapireMukherjee & Schapire (2013) introduced a sufficient and necessary weak learning condition based on a concept called edge over random. It is intuitive to restrict the cost matrices to a space of edge-over-random cost-matrices Ceor who put the least cost on the correct label. Definition G.1. The space of edge-over-random cost matrices Ceor \u2286 Rm\u00d7C is the space of cost matrices C \u2208 Ceor that satisfies C(i, yi) \u2264 C(i, l), \u2200l 6= yi, \u2200i \u2208 {1, . . . ,m}. (80) Let us define a random classifier as a baseline predictor B \u2208 Rm\u00d7C , where each row lies on a simplexB(i, :) \u2208 \u2206{1, . . . , C}. We consider the space of baselines, called edge-over-random,who have a faint clue about the correct answer. Definition G.2. The space of edge-over-random baselines Mukherjee & Schapire (2013) Beor \u03b3 \u2286 R m\u00d7C is the space of baselinesB \u2208 Beor \u03b3 that satisfy B(i, yi) \u2265 B(i, l) + \u03b3, \u2200l 6= yi, \u2200i \u2208 {1, .", "startOffset": 24, "endOffset": 832}, {"referenceID": 38, "context": "i=1 C(i, :)B(i, :)\u22a4. (82) Lemma G.4. A weak classifier space H is boostable if and only if H satisfies the weak-learning condition defined in Definition G.3. Refer to Mukherjee and Schapire Mukherjee & Schapire (2013) for the proof.", "startOffset": 4, "endOffset": 218}], "year": 2017, "abstractText": "Deep neural networks are known to be difficult to train due to the instability of back-propagation. A deep residual network (ResNet) with identity loops remedies this by stabilizing gradient computations. We prove a boosting theory for the ResNet architecture. We construct T weak module classifiers, each contains two of the T layers, such that the combined strong learner is a ResNet. Therefore, we introduce an alternative Deep ResNet training algorithm, BoostResNet, which is particularly suitable in non-differentiable architectures. Our proposed algorithm merely requires a sequential training of T \u201cshallow ResNets\u201d which are inexpensive. We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. Our results apply to general multi-class ResNets. A generalization error bound based on margin theory is proved and suggests ResNet\u2019s resistant to overfitting under network with l1 norm bounded weights.", "creator": "LaTeX with hyperref package"}}}