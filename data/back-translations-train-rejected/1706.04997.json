{"id": "1706.04997", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Extracting Formal Models from Normative Texts", "abstract": "We are concerned with the analysis of normative texts - documents based on the deontic notions of obligation, permission, and prohibition. Our goal is to make queries about these notions and verify that a text satisfies certain properties concerning causality of actions and timing constraints. This requires taking the original text and building a representation (model) of it in a formal language, in our case the C-O Diagram formalism. We present an experimental, semi-automatic aid that helps to bridge the gap between a normative text in natural language and its C-O Diagram representation. Our approach consists of using dependency structures obtained from the state-of-the-art Stanford Parser, and applying our own rules and heuristics in order to extract the relevant components. The result is a tabular data structure where each sentence is split into suitable fields, which can then be converted into a C-O Diagram. The process is not fully automatic however, and some post-editing is generally required of the user. We apply our tool and perform experiments on documents from different domains, and report an initial evaluation of the accuracy and feasibility of our approach.", "histories": [["v1", "Thu, 15 Jun 2017 07:24:23 GMT  (33kb,D)", "http://arxiv.org/abs/1706.04997v1", "Extended version of conference paper at the 21st International Conference on Applications of Natural Language to Information Systems (NLDB 2016). arXiv admin note: substantial text overlap witharXiv:1607.01485"]], "COMMENTS": "Extended version of conference paper at the 21st International Conference on Applications of Natural Language to Information Systems (NLDB 2016). arXiv admin note: substantial text overlap witharXiv:1607.01485", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john j camilleri", "normunds gr\\=uz\\={\\i}tis", "gerardo schneider"], "accepted": false, "id": "1706.04997"}, "pdf": {"name": "1706.04997.pdf", "metadata": {"source": "CRF", "title": "Extracting Formal Models from Normative Texts", "authors": ["John J. Camilleri", "Normunds Gruzitis", "Gerardo Schneider"], "emails": ["john.j.camilleri@cse.gu.se,", "gerardo@cse.gu.se,", "normunds.gruzitis@lu.lv"], "sections": [{"heading": null, "text": "Keywords: information extraction, normative texts, dependency analysis, C-O diagrams"}, {"heading": "1 Introduction", "text": "Normative texts are natural language documents that deal with what needs to be done, can be done, or should not be done - also known as deontic standards. This class of documents often includes legal contracts, service terms, regulations, and service agreements. Our work involves analyzing such texts using formal methods. We achieve this by modelling normative documents within a formalism that allows us to perform complex queries and verify properties about them. The formalism used for this task is C-O diagrams [8, 15], which provides a language for visualizing normative texts that include the modalities of obligation, permission, and prohibition, indicated by the letters O, P, and F."}, {"heading": "2 Extracting Draft Predicate Candidates", "text": "The proposed approach is application-specific but domain-independent, assuming that normative texts (or contracts) tend to follow a certain restricted style of natural language, even if there are discrepancies between and within the domains.3 However, we do not impose grammatical or lexical constraints on the input texts, so we first apply a general-purpose parser that acquires a syntactical dependency tree representation for each sentence. Provided that the syntactical analysis does not contain significant errors, we then apply a set of interpretation rules and heuristics to the top of the dependency structures. If the extraction is successful, one or more candidates will be acquired for each input sentence, as shown in Table 1. More than one candidate will be applied in the case of explicit or implicit coordination of subjects, verbs, objects or main clauses. Table 1: Example input and partial output."}, {"heading": "2.1 Expected Input and Intended Output", "text": "The basic prerequisite for pre-processing the input text is that it is divided by sentences and that only the relevant sentences are included. In this experiment, we manually selected the relevant sentences and ignored (partial) titles, introductory notes, etc. Automatic analysis of the document structure is a separate problem. We also expect that sentences do not contain grammatical errors that would significantly affect the syntactical analysis and thus the output of our tool; the output is a table (in tabularly separated format) in which each row corresponds to a C-O diagram box (clause) containing fields for: Subject the agent of the clause; Verb the verbal component of an action; Object the object component of an action (optional); Modality obligation (P), Prohibition (F) or Declaration (D) for clauses that refer only to facts; Refinement whether a clause refers to the preceding clause by conjunction (SEquAND), Explication (OQ) or Addition (Addition);"}, {"heading": "2.2 Rules", "text": "We distinguish between rules and heuristics applied on top of Stanford dependencies. Rules are anything that explicitly follows from dependency relationships and part-of-speech tags. For example, the header of the subject naming phrase (NP) is labeled by nsubj, and the head of the direct object NP - by dobj (see Figure 2); the subject and object of the output table fields can be labeled simply by the respective phrases (as in Table 1). We also count as lexicalized rule cases when the decision can obviously be made by taking into account both the dependency designation and the header. For example, modal verbs and other utilities of the main verb are labeled as aux, but words like \"can\" and \"must\" clearly indicate the respective modality (P and O). Auxiliaries can be combined with other modifiers, for example, the modifiers \"not\" (neg) that are the terms that override the obligation (in such cases)."}, {"heading": "2.3 Heuristics", "text": "In addition to the obvious extraction rules, we apply a set of heuristic rules based on the development examples and our intuition about the scope and language of normative texts. First, tools are compared and classified with extended keywords. For6 example: words such as \"responsible,\" \"responsible\" and \"require\" most likely have an obligation. For prepositional phrases (PP) that have direct dependencies on verb, we first check whether they have a verb, an adjective or a noun. For6 example: words such as \"responsible\" and \"require.\""}, {"heading": "2.4 Post-editing", "text": "Since we do not intend our tool to be a complete replacement for a human knowledge engineer, a certain amount of post-editing is often required, which can be categorized into the following different types, listed here in approximate order of effort required: 1. Filling blank fields. 2. Advertising information to be added or removed from subject and object. 3. Modifying the verb or modality. 4. Refining into subsentences. 5. Complete paraphrasing."}, {"heading": "3 Experiments", "text": "In order to test the potential and feasibility of the proposed approach, we selected four normative texts from three different areas: two terms of use, a lease and a doctoral regulation document. In the development phase, we looked at the first 10 sentences of each document, on the basis of which the rules and heuristics were defined. We used the next 10 sentences of each document for evaluation. The four documents used in our experiments are: 1. Doctoral regulations of Chalmers University 2. Equipment rental agreement of RSO, Inc.33. Terms of use of GitHub, Inc.4 4. Terms of use of Facebook5After preparing the test kits and applying our tool, the tabular output of each sentence was manually evaluated according to the following criteria:"}, {"heading": "3.1 Evaluation Criteria", "text": "The other fields (time, advertising, terms and comments) in our table structure are not included in the evaluation criteria because they are too unstructured in themselves and always require post-processing to be formalized. Precision concerns the evaluation of the accuracy of the output of the tool. For each value in the respective fields, we assign a point at which it coincides with our evaluation of the correct value. If a single sentence leads to a refinement with several clauses, we evaluate each of these clauses individually. Recall is a measure of how much the intended value could be extracted from the tool. For each sentence in the original text, we check whether the correct fields have been extracted by the tool, evaluating accordingly. If a sentence should lead to several clauses, we evaluate the results for each of these individuals.The local precision for the summary and repetition in the original text is often identical if the results are not identical when the text is added together."}, {"heading": "3.2 Observations", "text": "The first observation from the results is that the value of Formula 1 varies greatly between documents, from 0.49 to 0.86, mainly due to the differences in language style in the documents. Overall, the application of heuristics, together with the rules, improves the scores attained.3 http: / / www.rsoinc.com / pdfs / equip _ rental _ revb.pdf 4 https: / / help.github.com / articles / github-terms-of-service / 5 https: / / www.facebook.com / legal / terms8On the one hand, many of the sentence patterns we deal with in Heuristics appear only in the development group and not in the test group. On the other hand, there are few cases that occur relatively frequently among the test examples but are not covered by the development group. For example, the introductory part of a sentence, the syntactical main sentence, can sometimes be meaningless to our formalism, and it should be ignored as the sub-clause is considered particularly important, so that the main clause is understood as such."}, {"heading": "3.3 Paraphrasing", "text": "In a sense, the task of extracting the correct fields from each sentence can be considered a paraphrase of the given sentence into one of the known patterns that can be handled by our rules. We give here some examples of errors that occurred in the experiments and that can currently only be corrected by a non-trivial paraphrase. GitHub reserves the right to change or terminate your access to the API at any time, temporarily or permanently, with or without noticing it. For this sentence, our tool accepts reservations as a verb and right as an object, but this really should be realized as permission with modification as a verb and access to the API as an object. It could also be refined as permission to act (modify, temporarily stop, permanently stop). Additional wording as at any time and with or without notice is actually not informative here, as it reflects the default behavior of formalism (i.e. the lack of restrictions)."}, {"heading": "To learn more about Platform, including how you can control what information other people may share with applications, read our Data Policy and Platform Page.", "text": "Phrases like this are generally unimportant to our goals and should be completely ignored, but we currently have no way to identify and ignore unhelpful phrases."}, {"heading": "4 Formal Analysis", "text": "The ultimate goal of formalizing normative texts is to be able to perform automated analyses, by which we mean executing queries of various kinds against the C-O diagram model. Syntactic queries are those that can be verified at the syntactical level, such as checking whether a text contains permissions for a particular agent or identifies obligations without limitations or reparations. These queries are based on predicates defined by individual clauses that serve as a building block for defining queries across the model. For example, the isObl (C) predicate is correct when clause C is an obligation. Syntactic queries are expressed by combining such predicators. Queries that deal with time constraints, possibility, and inventory are called semantic queries."}, {"heading": "5 Related Work", "text": "Our work can be regarded as similar to that of Wyner and Peters [19], who present a system for identifying and extracting rules from legal texts using the Stanford parser and other NLP tools within the GATE system [?] Their approach is somewhat more general, producing an annotated version of the original text as output. Ours is a more specific application of such techniques by having a well-defined output format that guides the design of our extraction tool, in particular the ability to define clauses with refinement. Mercatali et al. [16] deal with the automatic translation of textual representations of laws into a formal model, in their case UML. This underlying formalism is of course different where they are mainly interested in the hierarchical structure of documents rather than in the standards themselves. Their method does not use dependence or structure-related trees, but flat syntactical blocks."}, {"heading": "6 Conclusions and Future Work", "text": "As already mentioned in the introduction and in Section 4, the main motivation of our work is to perform complex formal analysis of normative texts by syntactic and (temporary) semantic queries through model verification. We help in this paper to bridge the gap between natural language texts and formal analysis tools by relieving the user of the burden of dealing directly with formal languages. Therefore, the approach presented here is not positioned as a possible substitute for a human; rather, it is seen as an aid for a semi-automatic transition from natural language to C-O diagrams. Although the results of the experiments reported here are indicative at best, the application of our technique to the case studies reported here has significantly increased efficiency in the task of \"coding\" documents in C-O diagrams. Moreover, they are promising enough to warrant further work in this direction."}], "references": [{"title": "A Theory of Timed Automata", "author": ["R. Alur", "D.L. Dill"], "venue": "Theoretical Computer Science 126(2),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "FrameNet CNL: A Knowledge Representation and Information Extraction Language", "author": ["G. Barzdins"], "venue": "Controlled Natural Language, LNCS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A Tutorial on UPPAAL 4.0", "author": ["G. Behrmann", "A. David", "K.G. Larsen"], "venue": "Tech. rep.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Timed Automata: Semantics, Algorithms and Tools", "author": ["J. Bengtsson", "W. Yi"], "venue": "Lectures on Concurrency and Petri Nets, LNCS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A CNL for Contract-Oriented Diagrams", "author": ["J.J. Camilleri", "G. Paganelli", "G. Schneider"], "venue": "Controlled Natural Language, LNCS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Discriminative reordering with chinese grammatical relations features", "author": ["P.C. Chang", "H. Tseng", "D. Jurafsky", "C.D. Manning"], "venue": "Third Workshop on Syntax and Structure in Statistical Translation. pp", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Information extraction from legal documents", "author": ["T.T. Cheng", "J. Cua", "M. Tan", "K. Yao", "R. Roxas"], "venue": "Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Specification and Verification of Normative Texts using C-O Diagrams", "author": ["G. D\u0131\u0301az", "M.E. Cambronero", "E. Mart\u0301\u0131nez", "G. Schneider"], "venue": "IEEE Transactions on Software Engineering", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Attempto Controlled English for Knowledge Representation", "author": ["N.E. Fuchs", "K. Kaljurand", "T. Kuhn"], "venue": "Reasoning Web. LNCS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A multilingual FrameNet-based grammar and lexicon for Controlled Natural Language. Language Resources and Evaluation", "author": ["N. Gruzitis", "D. Dannells"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Building the essential resources for finnish: the turku dependency treebank", "author": ["K. Haverinen", "J. Nyblom", "T. Viljanen", "V. Laippala", "S. Kohonen", "A. Missil", "S. Ojala", "T. Salakoski", "F. Ginter"], "venue": "Language Resources and Evaluation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Accurate Unlexicalized Parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL). pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Universal Stanford Dependencies: A cross-linguistic typology", "author": ["M.C. de Marneffe", "T. Dozat", "N. Silveira", "K. Haverinen", "F. Ginter", "J. Nivre", "C.D. Manning"], "venue": "Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC). pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The Stanford Typed Dependencies Representation", "author": ["M.C. de Marneffe", "C.D. Manning"], "venue": "Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation. pp", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "A Model for Visual Specification of e-Contracts", "author": ["E. Martinez", "E. Cambronero", "G. Diaz", "G. Schneider"], "venue": "Proceedings of the 7th International Conference on Services Computing (IEEE SCC)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Automatic Translation from Textual Representations of Laws to Formal Models through UML", "author": ["P. Mercatali", "F. Romano", "L. Boschi", "E. Spinicci"], "venue": "Proceedings of the 18th Annual Conference on Legal Knowledge and Information Systems (JURIX)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "A persian treebank with stanford typed dependencies", "author": ["M. Seraji", "C. Jahani", "B. Megyesi", "J. Nivre"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Parsing With Compositional Vector Grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "On rule extraction from regulations", "author": ["A. Wyner", "W. Peters"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "The formalism used for this task is C-O Diagrams [8, 15], which provides a language for visualising normative texts involving the modalities of obligation, permission and prohibition (forbiddance).", "startOffset": 49, "endOffset": 56}, {"referenceID": 14, "context": "The formalism used for this task is C-O Diagrams [8, 15], which provides a language for visualising normative texts involving the modalities of obligation, permission and prohibition (forbiddance).", "startOffset": 49, "endOffset": 56}, {"referenceID": 0, "context": "Models in this formalism can be converted in an automatic, deterministic way into networks of timed automata [1, 4], which are amenable to verification using the Uppaal model checker [3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 3, "context": "Models in this formalism can be converted in an automatic, deterministic way into networks of timed automata [1, 4], which are amenable to verification using the Uppaal model checker [3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 2, "context": "Models in this formalism can be converted in an automatic, deterministic way into networks of timed automata [1, 4], which are amenable to verification using the Uppaal model checker [3].", "startOffset": 183, "endOffset": 186}, {"referenceID": 11, "context": "Our method uses dependency structures obtained from a general-purpose statistical parser, namely the Stanford Parser [12, 18], which are then processed using custom rules and heuristics that we have specified based on a small development corpus in order to produce a table of clauses (draft predicate candidates).", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "Our method uses dependency structures obtained from a general-purpose statistical parser, namely the Stanford Parser [12, 18], which are then processed using custom rules and heuristics that we have specified based on a small development corpus in order to produce a table of clauses (draft predicate candidates).", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "In our experiment, we use the Stanford Parser whose accuracy on Penn Treebank (the WSJ section) is around 90% [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 136, "endOffset": 139}, {"referenceID": 10, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 18, "context": "Our work can be seen as similar to that of Wyner and Peters [19], who present a system for identifying and extracting rules from legal texts using the Stanford parser and other NLP tools within the GATE system [? ].", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "[16] tackle the automatic translation of textual representations of laws to a formal model, in their case UML.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] also describe a system for extracting structured information for texts in a specific legal domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This may be done using either a general-purpose CNL such as Attempto Controlled English (ACE) [9] which comes with a parser to discourse representation structures, or a custom CNL.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "[5] have designed a custom CNL which can be parsed directly into a C-O Diagram.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The FrameNet-CNL framework [2, 10] proposes an approach to information extraction problem by combining CNL with FrameNet\u2014a lexicographic database describing word meanings based on the principles of frame semantics.", "startOffset": 27, "endOffset": 34}, {"referenceID": 9, "context": "The FrameNet-CNL framework [2, 10] proposes an approach to information extraction problem by combining CNL with FrameNet\u2014a lexicographic database describing word meanings based on the principles of frame semantics.", "startOffset": 27, "endOffset": 34}], "year": 2017, "abstractText": "We are concerned with the analysis of normative texts\u2014 documents based on the deontic notions of obligation, permission, and prohibition. Our goal is to make queries about these notions and verify that a text satisfies certain properties concerning causality of actions and timing constraints. This requires taking the original text and building a representation (model) of it in a formal language, in our case the C-O Diagram formalism. We present an experimental, semi-automatic aid that helps to bridge the gap between a normative text in natural language and its C-O Diagram representation. Our approach consists of using dependency structures obtained from the state-of-the-art Stanford Parser, and applying our own rules and heuristics in order to extract the relevant components. The result is a tabular data structure where each sentence is split into suitable fields, which can then be converted into a C-O Diagram. The process is not fully automatic however, and some post-editing is generally required of the user. We apply our tool and perform experiments on documents from different domains, and report an initial evaluation of the accuracy and feasibility of our approach.", "creator": "LaTeX with hyperref package"}}}