{"id": "1706.05039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Distributed Transfer Linear Support Vector Machines", "abstract": "Transfer learning has been developed to improve the performances of different but related tasks in machine learning. However, such processes become less efficient with the increase of the size of training data and the number of tasks. Moreover, privacy can be violated as some tasks may contain sensitive and private data, which are communicated between nodes and tasks. We propose a consensus-based distributed transfer learning framework, where several tasks aim to find the best linear support vector machine (SVM) classifiers in a distributed network. With alternating direction method of multipliers, tasks can achieve better classification accuracies more efficiently and privately, as each node and each task train with their own data, and only decision variables are transferred between different tasks and nodes. Numerical experiments on MNIST datasets show that the knowledge transferred from the source tasks can be used to decrease the risks of the target tasks that lack training data or have unbalanced training labels. We show that the risks of the target tasks in the nodes without the data of the source tasks can also be reduced using the information transferred from the nodes who contain the data of the source tasks. We also show that the target tasks can enter and leave in real-time without rerunning the whole algorithm.", "histories": [["v1", "Thu, 15 Jun 2017 18:53:11 GMT  (362kb,D)", "http://arxiv.org/abs/1706.05039v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["rui zhang", "quanyan zhu"], "accepted": false, "id": "1706.05039"}, "pdf": {"name": "1706.05039.pdf", "metadata": {"source": "CRF", "title": "Distributed Transfer Linear Support Vector Machines", "authors": ["Rui Zhang", "Quanyan Zhu"], "emails": ["rz885@nyu.edu", "qz494@nyu.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before there is an agreement."}, {"heading": "II. CENTRALIZED TRANSFER LEARNING", "text": "In this section, we present a centralized transfer learning approach to SVMs. Consider T learning tasks with T = 1,..., T denotes the set of tasks. We assume that each task has a labeled learning set Dt = {(xtn, ytn), but has the same dimension p. For each task, a linear SVM aims to have a maximum differentiation function gt (xt) = 1, where Xt Rp represents the input space of the task. Note that Xt is different for each task, but has the same dimension p. For each task, a linear SVM aims to find a maximum differentiation function gt (xt) = characters (xTt w Rp), which gives input test data xt \u2212 1 or + 1. Definition variables {w, b) can be found by solving the following minimization problems."}, {"heading": "III. DISTRIBUTED TRANSFER LEARNING", "text": "[...] [...] [...] (...] (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) () (...) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () ()) () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () (() () () (() (() () () () (() () () () () () () ((() () () (() () () () () () () ((() () () () () () () ("}, {"heading": "IV. NUMERICAL EXPERIMENTS", "text": "In this section, we present numerical experiments of the DTSVM. We use the MNIST database of handwritten digits to evaluate the distributed transfer learning algorithm [15]. The MNIST database contains images of the digits \"0\" to \"9.\" In addition, we set the classification of \"3\" and \"6\" as task 1, using source task \"5\" and \"4\" as task 2 and \"9\" as task 3. Note that task 1 and 2 are the target tasks we aim to reduce their classification risks, while task 3 is the source task that will help us achieve this. All images have been pre-processed using Main Component Analysis (PCA) in vectors with a dimension of 10 [16]. We further define the degree of a node v V as the actual number of adjacent nodes Bv divided by the most achievable number of neighbors and the degree of the network V as the average degree of the distributed purposes."}, {"heading": "V. CONCLUSION", "text": "With the help of ADMoM, we have developed a fully distributed algorithm (DTSVM) in which each task in each node operates its own data without transferring training data to other tasks and adjacent nodes. Numerical experiments have shown that our DTSVM algorithm can improve the performance of target tasks that lack training data or have unbalanced training labels. We have also shown that our algorithm can improve the performance of nodes that lack data from the source tasks by sending information from the nodes that contain data from the source tasks. We have shown that our algorithm is suitable for online learning, where the target tasks can freely enter or leave the training of the source tasks in real time. One direction for future work is to extend the current framework to non-linear algorithms and other machine learning algorithms."}, {"heading": "APPENDIX A", "text": "Problem (5) can be solved in a distributed way with ADMoM (8), which solves the following problem: min {r, \u03c9) F1 (r) + F2 (n) s.t. Mr =. (13) with the following iterations: r (k + 1).argmin r F1 (r) + \u03b1 (k) T Mr \u2212 n (k).n (k).n (k + 1).n (k + 1).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).k (n).k (k).k (k).k (n).k (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).n (k).k (k (k).k (k).n (k).n (k (k).n).n (k (k),.n (k).n (k (k).k (k (k).n).n (k (k (k).n).n (k (k (k),.n).n (k (k (k).n).n).n (k (k (k).n),.n (k (.n).n).n).n (k (.n).n (.n).n (.k (.k (k).k (k).k (k).n).k (k (k).k (k).n).n (k (k).n).n).n (k (k (k).n).n ("}], "references": [{"title": "Training support vector machines: an application to face detection", "author": ["E. Osuna", "R. Freund", "F. Girosi"], "venue": "Computer vision and pattern recognition, 1997. Proceedings., 1997 IEEE computer society conference on, pp. 130\u2013136, IEEE, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A machine learning approach to building domain-specific search engines", "author": ["A. McCallum", "K. Nigam", "J. Rennie", "K. Seymore"], "venue": "IJCAI, vol. 99, pp. 662\u2013667, Citeseer, 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Transfer learning for visual categorization: A survey", "author": ["L. Shao", "F. Zhu", "X. Li"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 5, pp. 1019\u20131034, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 10, pp. 1345\u2013 1359, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Boosting for transfer learning", "author": ["W. Dai", "Q. Yang", "G.-R. Xue", "Y. Yu"], "venue": "Proceedings of the 24th international conference on Machine learning, pp. 193\u2013200, ACM, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 109\u2013117, ACM, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Consensus-based distributed support vector machines", "author": ["P.A. Forero", "A. Cano", "G.B. Giannakis"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 1663\u20131707, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Semisupervised multitask learning with gaussian processes", "author": ["G. Skolidis", "G. Sanguinetti"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 12, pp. 2101\u20132112, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "Proceedings of the 24th international conference on Machine learning, pp. 759\u2013766, ACM, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Distributed support vector machines", "author": ["A. Navia-V\u00e1zquez", "E. Parrado-Hernandez"], "venue": "Neural Networks, IEEE Transactions on, vol. 17, no. 4, pp. 1091\u20131097, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Scaling up machine learning: Parallel and distributed approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["S. Ben-David", "R. Schuller"], "venue": "Learning Theory and Kernel Machines, pp. 567\u2013580, Springer, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": ", face detection [1] and search engines [2].", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": ", face detection [1] and search engines [2].", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "Traditionally, machine learning makes predictions or classifications based on the assumption that the training and the testing data come from the same source or distribution [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "However, this assumption may not hold in many real applications [4]; for example, the training data can be outdated, or insufficient to build a good classifier.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "It has been shown that machine learning tasks can benefit from other similar tasks by knowledge transfer [3], [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "It has been shown that machine learning tasks can benefit from other similar tasks by knowledge transfer [3], [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "Since parts of the outdated data still contain useful information, knowledge can be transferred from them to train a classifier together with the new data [5].", "startOffset": 155, "endOffset": 158}, {"referenceID": 5, "context": "For traditional transfer learning, training data are communicated between tasks [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "For example, training data may come from different nodes of a wireless sensor network (WSN), and their communication with a fusion center can be either costly or restricted due to scalability, privacy or power limitations [7].", "startOffset": 222, "endOffset": 225}, {"referenceID": 7, "context": "With alternating direction method of multipliers (ADMoM) [8], the centralized problem can be solved in a fully distributed way.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "Note that the problem of transfer learning between tasks in one node can be viewed as a transfer learning problem studied in [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "Besides, the problem of distributed machine learning with a single task is a distributed support vector machines (DSVM) problem recently studied in [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "Our research is closely related to multitask learning [6], [9], transfer learning [10], and distributed machine learning [7], [11], [12].", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "Our research is closely related to multitask learning [6], [9], transfer learning [10], and distributed machine learning [7], [11], [12].", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "Our research is closely related to multitask learning [6], [9], transfer learning [10], and distributed machine learning [7], [11], [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "Our research is closely related to multitask learning [6], [9], transfer learning [10], and distributed machine learning [7], [11], [12].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "Our research is closely related to multitask learning [6], [9], transfer learning [10], and distributed machine learning [7], [11], [12].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "Our research is closely related to multitask learning [6], [9], transfer learning [10], and distributed machine learning [7], [11], [12].", "startOffset": 132, "endOffset": 136}, {"referenceID": 3, "context": "In this paper, we consider multi-task learning as a scenario of transfer learning, since it can be easily extended to transfer learning by assigning a larger weight to the loss function of the target tasks [4].", "startOffset": 206, "endOffset": 209}, {"referenceID": 5, "context": "Both [6] and [9] have focused on multitask learning, and they have shown that multi-task learning outperforms single-task learning.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Both [6] and [9] have focused on multitask learning, and they have shown that multi-task learning outperforms single-task learning.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "have presented in [10] a self-taught learning framework where the knowledge from unlabeled data is used to improve the performance of a given classification task.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Both [11] and [7] have presented approaches on DSVMs where information, such as support vectors and decision variables, is communicated between nodes to achieve a better global performance more efficiently.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "Both [11] and [7] have presented approaches on DSVMs where information, such as support vectors and decision variables, is communicated between nodes to achieve a better global performance more efficiently.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "\u2022 We develop a distributed transfer support vector machine (DTSVM) algorithm with Alternating Direction Method of Multipliers (ADMoM) [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 12, "context": "Decision variables {\u0175\u2217 t , b\u0302\u2217 t } can be found by solving the following minimization problem [13]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "With the assumption that different tasks are related to each other on the basis of similarity between distributions of samples Xt [14], the decision variables \u0175t , b\u0302t can be divided into: \u0175t = w0 +wt ; b\u0302t = b0 +bt , where w0 and", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "b0 are common terms over all tasks, while wt and bt are task specific terms [4], [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "b0 are common terms over all tasks, while wt and bt are task specific terms [4], [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "Problem (5) can be solved iteratively in a distributed way with ADMoM [8], which is shown as the following proposition.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "Evolution of the global risks of DTSVM and DSVM [7] training Task 1 and Task 3.", "startOffset": 48, "endOffset": 51}, {"referenceID": 14, "context": "All the images have been pre-processed with principal component analysis (PCA) into vectors with a dimension of 10 [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "The algorithm of CSVM can be acquired from [13].", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "The algorithm of DSVM can be found in [7], which only shares the values of decision variables during the training process.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Problem (5) can be solved in a distributed way with ADMoM [8], which solves the following problem:", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "We follow a similar step in [7], by setting", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "2 and Appendix A in [8].", "startOffset": 20, "endOffset": 23}], "year": 2017, "abstractText": "Transfer learning has been developed to improve the performances of different but related tasks in machine learning. However, such processes become less efficient with the increase of the size of training data and the number of tasks. Moreover, privacy can be violated as some tasks may contain sensitive and private data, which are communicated between nodes and tasks. We propose a consensus-based distributed transfer learning framework, where several tasks aim to find the best linear support vector machine (SVM) classifiers in a distributed network. With alternating direction method of multipliers, tasks can achieve better classification accuracies more efficiently and privately, as each node and each task train with their own data, and only decision variables are transferred between different tasks and nodes. Numerical experiments on MNIST datasets show that the knowledge transferred from the source tasks can be used to decrease the risks of the target tasks that lack training data or have unbalanced training labels. We show that the risks of the target tasks in the nodes without the data of the source tasks can also be reduced using the information transferred from the nodes who contain the data of the source tasks. We also show that the target tasks can enter and leave in real-time without rerunning the whole algorithm.", "creator": "LaTeX with hyperref package"}}}