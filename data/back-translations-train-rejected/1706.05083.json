{"id": "1706.05083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation", "abstract": "This work presents a novel approach to jointly tackling Automatic Post-Editing (APE) and Word-Level Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems. Word-level features which have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models which use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.", "histories": [["v1", "Thu, 15 Jun 2017 20:47:03 GMT  (80kb,D)", "https://arxiv.org/abs/1706.05083v1", "APE/QE System Description Paper for WMT/CMT 2017"], ["v2", "Sat, 15 Jul 2017 12:38:48 GMT  (81kb,D)", "http://arxiv.org/abs/1706.05083v2", "APE/QE System Description Paper for WMT/CMT 2017"]], "COMMENTS": "APE/QE System Description Paper for WMT/CMT 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris hokamp"], "accepted": false, "id": "1706.05083"}, "pdf": {"name": "1706.05083.pdf", "metadata": {"source": "CRF", "title": "Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation", "authors": ["Chris Hokamp"], "emails": ["chokamp@computing.dcu.ie"], "sections": [{"heading": "1 Introduction", "text": "In a common scenario, human translators are able to perform a correct machine translation (MT) by correcting errors and omissions until a perfect translation is produced. Several studies have shown that this process, called \"post-editing,\" is faster than the translation from the ground up (Specia, 2011) or the interactive machine translation (Green et al., 2016). A relatively new line of research has attempted to build models that automatically correct errors in MT (Simard et al., 2007; Bojar et al., 2015; Junczys-Dowmunt and Grundkiewicz, 2016). Automatic post-editing (APE) usually provides a system that produces the original translation as a black box that cannot be modified or verified. An APE system has access to the same data that a human translator would see: a source sentence and a translation hypothesis identical."}, {"heading": "2 Related Work", "text": "Two important areas of research have recently enabled breakthroughs in QE and APE.1code, which can be found at: https: / / github.com / chrishokamp / constrained _ decoding"}, {"heading": "2.1 Automatic Post-Editing", "text": "APE and QE training datasets consist of (SRC, MT, PE) three-way models, with the post-edited reference in the workflow described above being created by a human translator. However, publicly available APE datasets are relatively small compared to parallel datasets used to train machine translation systems. Junczys-Dowmunt and Grundkiewicz (2016) present a method for generating a large synthetic training dataset from a parallel corpus (SRC, REF) by first translating the reference into the source language and then translating this \"pseudo-source\" back into the target language, leading to a \"pseudo-hypothesis\" that is likely more similar to the reference corpus than a direct translation from the source to the target. The publication of these synthetic training data was an important contribution to improving APE results, with Junczys-Dowmunt and Grundwicz representing RPE (SRPE) and RPE (SRPE) similarities."}, {"heading": "2.2 Quality Estimation", "text": "Martins et al. (2016) introduced a stacked architecture, using a very wide range of features within a structured prediction framework to make a big leap in the state of the art for QE at the Word level. Indeed, some features are the results of standalone feedback models and recurring neural network models that are then stacked into the final system. Although their approach creates a very good end model, the training and feature extraction steps are quite complicated. An additional disadvantage of this approach is that it requires the \"jackknifing\" of training data for the standalone models that provide features of the stacked model to avoid reworking in the stacked ensemble, which requires training k versions of each model type, with k being the number of jackknife splits. Our approach is most similar to Martins et al. (2017), the biggest differences are: we do not use internal features of the original MT system, and we do not need to create an ensemble. \""}, {"heading": "2.3 Factored Inputs", "text": "Alexandrescu and Kirchoff (2006) introduced linguistic factors for neural language models. The core idea is to learn embedding for linguistic features such as POS tags and dependency names and to add additional features to the word embedding of input. Recent work has shown that the performance of NMT can also be improved by linking embedding for additional \"factors\" at word level with embedding at word input level (Sennrich and Haddow, 2016). Thus, the input representation ej for each source input xj with factors F becomes Equation 1: ej = | F | nk = 1Ekxjk (1), where the vector concatenation is dictated, Ek is the embedding matrix of factor k and citik is a single hot vector for the k-th input factor."}, {"heading": "3 Models", "text": "In this section, we describe the five model types used for APE-QE, as well as the ensembles of these models that overall prove to be the most powerful. We design several features that are used as input for APE. The operating hypothesis is that features that have proven useful for quality estimation should also have a positive impact on APE performance. Our base models are the same models used in Junczys-Dowmunt (2016) 2. The authors provide trained SRC \u2192 PE and MT \u2192 PE models that correspond to the last four checkpoints of fine-tuning the models to the 500K training data associated with the 20 times sampled internal APE data. These models are referred to as SRC and MT."}, {"heading": "3.1 Word Alignments", "text": "Previous work has shown that source-target alignment information is a critical component of current state-of-the-art word-level QE systems (Kreutzer et al., 2015; Martins et al., 2016). Sequential inputs for structured predictions, as well as feedback and recurring models in existing work maintain the source-side characteristics for each target word using the word alignments provided by WMT task organizers. However, this information is unlikely to be available for quality estimation in many real-world use cases, and the use of this information also means that the MT system used to create the hypotheses is not a \"black box\" that is part of the definition of the QE task. Clearly, accessing the word-alignment information of an SMT system provides a lot of insight into the underlying model. Because our models rely on synthetic training data, and because we want to view the MT system as a true black box, we use the word-alignment information of an SMT system instead."}, {"heading": "3.2 Inputting Both Source and Target", "text": "According to Crego et al. (2016), we train a model that uses the concatenated source and MT as input, the two sequences being separated by a special BREAK token. We call this system SRC + MT."}, {"heading": "3.3 Part-of-Speech and Dependency Labels", "text": "Sennrich and Haddow (2016) showed that information such as POS tags, NER labels and syntactic roles can be included in the input of NMT models, thereby improving performance in general. Inspired by this idea, we select some of the most powerful features of Martins et al. (Martins et al., 2016) and incorporate them as input factors into the SRC + MT factor model. The basic representation is the concatenated SRC + MT (also with a special BREAK token). For each word in the English source and the German hypothesis, we obtain the part-of-speech tag, the dependency relationship and the part-of-speech of the headword and incorporate these as input factors. In both English and German, we use spaCy3 to extract these features for all training, development and test data. The resulting model is illustrated in Figure 1."}, {"heading": "3.4 Extending Factors to Subword Encoding", "text": "Our NMT models use subword encoding (Sennrich et al., 2016), but the additional factors are calculated at word level. Therefore, the factors must also be segmented to match BPE segmentation. We use the {BILOU} prefixes common in sequence markup tasks such as NER to extend3https: / / spacy.io / factor vocabularies, and assign each word level factor to the subordinate segmentation of the source or target text. Table 3 shows the input representations for each of the model types using an example from the 2016 WMT test data."}, {"heading": "3.5 Ensembling NMT Models", "text": "We intersected the parameters of the four best control points of each model type and created an ensemble of the resulting five models, called the Avg-All Baseline. Then, using MERT (Och, 2003), we tuned this ensemble to TER (APE) and F1-Mult (QE). The tuned models are called Avg-All APE-Tuned or Avg-All QE-Tuned. After observing that source-driven models have the best QE performance of a single model (see Section 5), we created a final model consisting of the four individual SRC models, and the averaged models of each other type (a total of eight models), called the 4-SRC + Avg-All QE Tune."}, {"heading": "3.6 Tuning", "text": "Table 2 shows the final weights for each type of ensemble after tuning. In line with the two-model ensemble presented in Martins et al. (2017), tuning models for F1 consoles result in much more weight being given to the SRC model, while TER tuning prefers models with access to the MT hypothesis."}, {"heading": "4 Experiments", "text": "According to Junczys-Dowmunt and Grundkiewicz (2016), we first train each model type using the large (4M) synthetic training data and then match each model 20x using the 500K dataset associated with the task-internal training data. Finally, for SRC + MT and SRC + MT factor, we train each model for a small number of iterations using the minimal risk training implementation available in Nematus (Shen et al., 2016). Table 4 shows the best dev result after each stage of the training. For both APE and QE, we only use the Qask-specific training data provided for the WT task 2017 (Shen et al., 2016)."}, {"heading": "5 Results", "text": "Table 1 shows the results of our experiments using the WMT 16 development and test kits. For each system, we measure performance using BLEU and TER, the metrics used in the APE task, and also using F1 Mult, the primary metric used in the Word Level QE task. General labeling accuracy is a secondary metric for QE. All systems with input factors significantly improve APE performance over baselines. For QE, the trends are less clear, but point to a crucial difference between optimization for TER and F1 _ product: F1 _ product optimization probably lowers the threshold for \"changing\" a word, as opposed to copying from the MT hypothesis. This hypothesis is supported by the observation that the source-based APE system outperforms all other individual models on the QE metrics. As the source-based systems are not forced to copy a word from the MT hypothesis, this hypothesis is likely to be incorrect if they are forced to use the best of words from the input, http E is likely to make the best of the input."}, {"heading": "6 Conclusion", "text": "This paper presented APE-QE, unifying models for APE and QE at the word level, using the flexibility of NMT to leverage the informative features of QE. Models with different input representations are combined and matched to APE or QE, thereby achieving state-of-the-art in both tasks. The complementarity of these tasks points to future exploration possibilities, such as joint training using QE labels and reference translations, as well as the inclusion of other features as input factors."}, {"heading": "Acknowledgments", "text": "This project was funded by the Science Foundation Ireland at the ADAPT Centre for Dig-ital Content Technology (www.adaptcentre.ie) at Dublin City University under the SFI Research Centres Programme (Grant 13 / RC / 2106), which is co-funded by the European Regional Development Fund and the EU research and innovation programme Horizon 2020 under grant agreement 645452 (QT21). Marcin JunczysDowmunt provided essential guidance on the tuning implementation of APE and QE ensembles."}], "references": [{"title": "Factored neural language models", "author": ["Andrei Alexandrescu", "Katrin Kirchhoff."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers. Association for Computational Linguistics, Strouds-", "citeRegEx": "Alexandrescu and Kirchhoff.,? 2006", "shortCiteRegEx": "Alexandrescu and Kirchhoff.", "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Systran\u2019s pure neural machine translation systems", "author": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aurelien Coquard", "Yongchao Deng"], "venue": null, "citeRegEx": "Crego et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Crego et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Kevin Knight, Ani Nenkova, and Owen Rambow, editors, HLT-NAACL. The Association for Compu-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "The efficacy of human postediting for language translation", "author": ["Spence Green", "Jeffrey Heer", "Christopher D. Manning."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, New", "citeRegEx": "Green et al\\.,? 2013", "shortCiteRegEx": "Green et al\\.", "year": 2013}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the First Conference on Machine Translation. Association for", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Quality estimation from scratch (quetch): Deep learning for word-level translation quality estimation", "author": ["Julia Kreutzer", "Shigehiko Schamoni", "Stefan Riezler."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine", "citeRegEx": "Kreutzer et al\\.,? 2015", "shortCiteRegEx": "Kreutzer et al\\.", "year": 2015}, {"title": "MARMOT: A toolkit for translation quality estimation at the word level", "author": ["Varvara Logacheva", "Chris Hokamp", "Lucia Specia."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoro\u017e,", "citeRegEx": "Logacheva et al\\.,? 2016", "shortCiteRegEx": "Logacheva et al\\.", "year": 2016}, {"title": "Unbabel\u2019s participation in the wmt16 word-level translation quality estimation shared task", "author": ["Andr\u00e9 F.T. Martins", "Ram\u00f3n Astudillo", "Chris Hokamp", "Fabio Kepler."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computa-", "citeRegEx": "Martins et al\\.,? 2016", "shortCiteRegEx": "Martins et al\\.", "year": 2016}, {"title": "Pushing the limits of translation quality estimation", "author": ["Andr\u00e9 FT Martins", "Marcin Junczys-Dowmunt", "Fabio N Kepler", "Ram\u00f3n Astudillo", "Chris Hokamp"], "venue": null, "citeRegEx": "Martins et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2017}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proc. of the Annual Meeting on Association for Computational Linguistics. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Nematus: a toolkit", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, Au-", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Rule-based translation with statistical phrase-based post-editing", "author": ["Michel Simard", "Nicola Ueffing", "Pierre Isabelle", "Roland Kuhn."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. pages 203\u2013206.", "citeRegEx": "Simard et al\\.,? 2007", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Exploiting objective annotations for measuring translation post-editing effort", "author": ["Lucia Specia."], "venue": "Proceedings of the 15th Conference of the European Association for Machine Translation. pages 73\u201380.", "citeRegEx": "Specia.,? 2011", "shortCiteRegEx": "Specia.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems. MIT Press, Cam-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Kevin Knight, Ani Nenkova, and Owen Rambow, editors, NAACL HLT 2016,", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Several studies has shown that this process, referred to as \"post-editing\", is faster than translation from scratch (Specia, 2011), or interactive machine translation (Green et al.", "startOffset": 116, "endOffset": 130}, {"referenceID": 4, "context": "Several studies has shown that this process, referred to as \"post-editing\", is faster than translation from scratch (Specia, 2011), or interactive machine translation (Green et al., 2013).", "startOffset": 167, "endOffset": 187}, {"referenceID": 15, "context": "A relatively recent line of research has tried to build models which correct errors in MT automatically (Simard et al., 2007; Bojar et al., 2015; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 104, "endOffset": 185}, {"referenceID": 5, "context": "A relatively recent line of research has tried to build models which correct errors in MT automatically (Simard et al., 2007; Bojar et al., 2015; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 104, "endOffset": 185}, {"referenceID": 18, "context": "This can be viewed as a sequence-tosequence task (Sutskever et al., 2014), and is also similar to multi-source machine translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 19, "context": ", 2014), and is also similar to multi-source machine translation (Zoph and Knight, 2016; Firat et al., 2016).", "startOffset": 65, "endOffset": 108}, {"referenceID": 3, "context": ", 2014), and is also similar to multi-source machine translation (Zoph and Knight, 2016; Firat et al., 2016).", "startOffset": 65, "endOffset": 108}, {"referenceID": 7, "context": "structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence (Logacheva et al., 2016; Martins et al., 2016).", "startOffset": 193, "endOffset": 239}, {"referenceID": 8, "context": "structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence (Logacheva et al., 2016; Martins et al., 2016).", "startOffset": 193, "endOffset": 239}, {"referenceID": 7, "context": "structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence (Logacheva et al., 2016; Martins et al., 2016). However, Martins et al. (2017) recently proposed a new method of word-level QE using APE, which simply uses an APE system to produce a \"pseudo-post-edit\" given a source sentence and an MT hypothesis.", "startOffset": 194, "endOffset": 272}, {"referenceID": 8, "context": "Martins et al. (2017), showed that APE systems with no knowledge about the QE task already provide a very strong baseline for QE.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "in Martins et al. (2017), by incorporating features which have proven effective for Word Level QE as \"factors\" in the input to Neural Machine Translation (NMT) systems.", "startOffset": 3, "endOffset": 25}, {"referenceID": 5, "context": "Junczys-Dowmunt and Grundkiewicz (2016) introduce a method for generating a large synthetic training dataset from a parallel corpus of (SRC,REF ) by first translating the reference to the source language, and then translating this \"pseudo-source\" back into the target language, resulting in a \u201cpseudohypothesis\" which is likely to be more similar to", "startOffset": 0, "endOffset": 40}, {"referenceID": 8, "context": "Our approach is most similar to Martins et al. (2017), the major differences are: we do not use any internal features from the original MT system, and we do not need to \"jackknife\" in order to create a stacked ensemble.", "startOffset": 32, "endOffset": 54}, {"referenceID": 12, "context": "Recent work has shown that NMT performance can also be improved by concatenating embeddings for additional word-level \"factors\" to source-word input embeddings (Sennrich and Haddow, 2016).", "startOffset": 160, "endOffset": 187}, {"referenceID": 6, "context": "nent of current state-of-the-art word level QE systems (Kreutzer et al., 2015; Martins et al., 2016).", "startOffset": 55, "endOffset": 100}, {"referenceID": 8, "context": "nent of current state-of-the-art word level QE systems (Kreutzer et al., 2015; Martins et al., 2016).", "startOffset": 55, "endOffset": 100}, {"referenceID": 1, "context": "The attention model for NMT produces a normalized vector of weights at each timestep, where the weights can be viewed as the \"alignment probabilities\" for each source word (Bahdanau et al., 2014).", "startOffset": 172, "endOffset": 195}, {"referenceID": 2, "context": "Following Crego et al. (2016), we train a model which takes the concatenated source and MT as input.", "startOffset": 10, "endOffset": 30}, {"referenceID": 8, "context": "(Martins et al., 2016), and include them as input factors to the SRC+MT-factor model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Our NMT models use subword encoding (Sennrich et al., 2016), but the additional factors are computed at the word level.", "startOffset": 36, "endOffset": 59}, {"referenceID": 10, "context": "We then tune this ensemble for TER (APE) and F1-Mult (QE), using MERT (Och, 2003).", "startOffset": 70, "endOffset": 81}, {"referenceID": 8, "context": "In line with the two-model ensemble presented in Martins et al. (2017), tuning models for F1-Mult results in much more weight", "startOffset": 49, "endOffset": 71}, {"referenceID": 11, "context": "All of our models are trained using Nematus (Sennrich et al., 2017).", "startOffset": 44, "endOffset": 67}, {"referenceID": 5, "context": "Following Junczys-Dowmunt and Grundkiewicz (2016), we", "startOffset": 10, "endOffset": 50}, {"referenceID": 14, "context": "Finally, for SRC+MT and SRC+MT-factor we continued fine-tuning each model for a small number of iterations using the min-risk training implementation available in Nematus (Shen et al., 2016).", "startOffset": 171, "endOffset": 190}, {"referenceID": 16, "context": "system into OK,BAD labels for QE, we use the APE hypothesis as a \"pseudo-reference\", which is then aligned with the original MT hypothesis using TER (Snover et al., 2006).", "startOffset": 149, "endOffset": 170}], "year": 2017, "abstractText": "This work presents a novel approach to Automatic Post-Editing (APE) and WordLevel Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems. Word-level features that have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models that use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.", "creator": "LaTeX with hyperref package"}}}