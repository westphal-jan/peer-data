{"id": "1706.05086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Evaluating Noisy Optimisation Algorithms: First Hitting Time is Problematic", "abstract": "A key part of any evolutionary algorithm is fitness evaluation. When fitness evaluations are corrupted by noise, as happens in many real-world problems as a consequence of various types of uncertainty, a strategy is needed in order to cope with this. Resampling is one of the most common strategies, whereby each solution is evaluated many times in order to reduce the variance of the fitness estimates. When evaluating the performance of a noisy optimisation algorithm, a key consideration is the stopping condition for the algorithm. A frequently used stopping condition in runtime analysis, known as \"First Hitting Time\", is to stop the algorithm as soon as it encounters the optimal solution. However, this is unrealistic for real-world problems, as if the optimal solution were already known, there would be no need to search for it. This paper argues that the use of First Hitting Time, despite being a commonly used approach, is significantly flawed and overestimates the quality of many algorithms in real-world cases, where the optimum is not known in advance and has to be genuinely searched for. A better alternative is to measure the quality of the solution an algorithm returns after a fixed evaluation budget, i.e., to focus on final solution quality. This paper argues that focussing on final solution quality is more realistic and demonstrates cases where the results produced by each algorithm evaluation method lead to very different conclusions regarding the quality of each noisy optimisation algorithm.", "histories": [["v1", "Tue, 13 Jun 2017 09:44:34 GMT  (233kb,D)", "https://arxiv.org/abs/1706.05086v1", "4 pages, 4 figurs, 1 table"], ["v2", "Wed, 12 Jul 2017 11:20:05 GMT  (233kb,D)", "http://arxiv.org/abs/1706.05086v2", "4 pages, 4 figurs, 1 table"]], "COMMENTS": "4 pages, 4 figurs, 1 table", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["simon m lucas", "jialin liu", "diego p\\'erez-li\\'ebana"], "accepted": false, "id": "1706.05086"}, "pdf": {"name": "1706.05086.pdf", "metadata": {"source": "CRF", "title": "Evaluating Noisy Optimisation Algorithms: First Hitting Time is Problematic", "authors": ["Simon M. Lucas", "Jialin Liu"], "emails": ["sml@essex.ac.uk", "jialin.liu@essex.ac.uk", "dperez@essex.ac.uk"], "sections": [{"heading": null, "text": "This year, it will be able to leave the country to return to the EU, where it will be able to integrate into the EU."}, {"heading": "II. TEST PROBLEMS", "text": "We looked at two problems: the OneMax problem, which was corrupted by additive Gaussian noise, and an artificial optimization problem of the game result."}, {"heading": "A. OneMax problem with additive Gaussian noise", "text": "The n-bit OneMax problem corrupted by additive Gaussian noise is formalized as: f (x) = n \u2211 i = 1 xi + N (0, 1), (1) where x is an n-bit binary string, N (\u00b5, \u03c32) stands for Gaussian noise with mean \u00b5 and variance \u03c32. This problem is referred to in the rest of the paper as the Noisy OneMax problem."}, {"heading": "B. Optimisation of game outcome", "text": "We also consider an artificial game outcome optimization problem, referred to in the rest of the paper as a noisy PMax problem, or a game parameter setting for a given AI agent. The excuse for this problem is that we are trying to optimize the win rate for a given agent, but due to the stochastic nature of the game and / or the agent playing it, the result of any simulation of the game is a win or a loss with a given probability. This results in a very loud optimization problem. The noisy PMax problem was directly inspired by recent work on developing game parameters to maximize the win rate of a skilled agent over less skilled agents [8]. In this artificial model, x is treated as an n-bit binary number, and the true win rate of x is called Pwin () = V alue (great value) between a game (2x1) and a probability (1q) with 1 xx."}, {"heading": "III. EXPERIMENTAL SETTINGS", "text": "Both the RMHC and the (1 + 1) EA with mutation probability 1n (n is the solution dimension) were used to optimize each of the two problems with each of the two evaluation methods. in each of the following two cases, we measure the percentage of times the optimizer returns the optimal solution. the algorithm 1 Random Mutation Hill Climber (RMHC) with the resampling rate r-N + to maximize a noisy binary problem. n is the problem dimension. the fitness (i) (x) refers to the noisy fitness function for the given x.Uniformly Random Mutation Hill Climber (RMHC) with the resampling rate r-N + to maximize a noisy binary problem."}, {"heading": "IV. RESULTS", "text": "Figures 1 and 2 show the percentage of return of an optimal solution by the (1 + 1) -EA (left) and the RMHC (right). We focus on a case where the search space is small (solution dimension n = 10), but also the assessment budget is low. Details of the algorithm are identical, except for the holding conditions shown in (E1) and (E2) respectively (Section III). To highlight the difference between results using holding conditions (E1) and (E2), the numbers of the best and worst success rates in each experiment are summarized in Table I."}, {"heading": "A. Noisy OneMax problem", "text": "If each algorithm works for a fixed maximum number of fitness ratings (T = 500), the RMHC is significantly more likely to find the optimal solution than the (1 + 1) -EA (Figure 1).Table I shows that the (1 + 1) -EA with (E1) as algorithm 2 (1 + 1) -EA with mutation probability 1n and sampling rate r \u00b2 N + to maximize a noisy n-dimensional binary problem. Fitness (i) (x) refers to the ith call of the noisy fitness function for the given x. Stop state indicates the best success rate with (E1), but the really best success rate is only 38.98% without knowing the optimal solution in advance; the RMHC with (E1) as the stop state gives the best success rate as 97.44%, with the decline falling to 65.44% if (E2) the optimal resampling rate is used as the optimal resource rate."}, {"heading": "B. Noisy PMax problem", "text": "Figure 2 shows the percentage of times an optimal solution is returned over 10,000 optimization attempts using the (1 + 1) -EA (left) and the RMHC (right) of the noisy-PMax problem described in Section II-B, using the two stop conditions discussed above. Table I shows that the (1 + 1) -EA with (E1) as a stop condition yields the best success rate with 19.56%, but the really best success rate is only 1% (without assuming that the optimal solution is known); the RMHC with (E1) as a stop condition yields the best success rate with 28.67%, but the really best success rate is only 1.14%. Therefore, the percentage of times an optimal solution is returned using the stop condition (E1) is much higher than the percentage achieved when using the stop condition (E2). However, the pre-knowledge of the optimal solution for real-time use (and therefore, the optimal access to the free-time application is rarely optimistic)."}, {"heading": "V. CONCLUSION", "text": "To the best of the authors \"knowledge, the current work around runtime analysis or predicting the optimal resampling rate is based on the assumption that the optimal solution is known. Once the optimal noise-free fitness level is reached or the optimal solution is found, optimization stops immediately, which results in the suction state and the solution does not escape from the suction state. In the real world, we are unlikely to know the optimal solution in advance as there will be no reason to look for it. The main conclusion is that First Hitting Time should not be used to evaluate the performance of noisy optimization algorithms as it can dramatically overestimate the performance of an algorithm and greatly underestimate the amount of resampling required to achieve optimal performance. The conclusion is supported by the results of executing two simple but widely used optimization algorithms on two different test problems."}], "references": [{"title": "Analysis of the (1+ 1) EA for a Noisy OneMax", "author": ["S. Droste"], "venue": "Genetic and Evolutionary Computation\u2013GECCO 2004. Springer, 2004, pp. 1088\u20131099.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Effectiveness of Sampling for Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "Y. Jin", "Z.-H. Zhou"], "venue": "Parallel Problem Solving from Nature\u2013PPSN XIII. Springer, 2014, pp. 302\u2013311.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of Runtime of Optimization Algorithms for Noisy Functions over Discrete Codomains", "author": ["Y. Akimoto", "S. Astete-Morales", "O. Teytaud"], "venue": "Theoretical Computer Science, vol. 605, pp. 42\u201350, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal Resampling for the Noisy OneMax Problem", "author": ["J. Liu", "M. Fairbank", "D. P\u00e9rez-Li\u00e9bana", "S.M. Lucas"], "venue": "arXiv preprint arXiv:1607.06641, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Effectiveness of Sampling for Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "K. Tang", "X. Yao", "Y. Jin", "Z.-H. Zhou"], "venue": "Evolutionary Computation, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-Parameter Black-Box Optimization Benchmarking 2010: presentation of the Noisy Functions", "author": ["S. Finck", "N. Hansen", "R. Ros", "A. Auger"], "venue": "Technical Report 2009/21, Research Center PPE, Tech. Rep., 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Noisy Evolutionary Optimization Algorithms-A Comprehensive Survey", "author": ["P. Rakshit", "A. Konar", "S. Das"], "venue": "Swarm and Evolutionary Computation, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 5, "context": "In the noisy case, some resampling techniques2 are often used aiming at cancelling the effect of noise [6], [7], [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "In the noisy case, some resampling techniques2 are often used aiming at cancelling the effect of noise [6], [7], [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "In the noisy case, some resampling techniques2 are often used aiming at cancelling the effect of noise [6], [7], [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "In most of the previous work [1], [2], [4], the runtime analysis is based on the assumption that once the optimal solution is hit, an absorbing state is reached and the optimisation terminates.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "In most of the previous work [1], [2], [4], the runtime analysis is based on the assumption that once the optimal solution is hit, an absorbing state is reached and the optimisation terminates.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "In most of the previous work [1], [2], [4], the runtime analysis is based on the assumption that once the optimal solution is hit, an absorbing state is reached and the optimisation terminates.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "[2] stated that resampling was useless in the 10-bit Noisy OneMax problem described in this paper and Liu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] concluded that in the same noise model, the optimal resampling rate for the RMHC increased with the problem dimension, and for the 10-bit Noisy OneMax problem, the optimal resampling rate was 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In both works [2], [4], the analysis was based on the condition that the optimisation terminated as soon", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "In both works [2], [4], the analysis was based on the condition that the optimisation terminated as soon", "startOffset": 19, "endOffset": 22}], "year": 2017, "abstractText": "A key part of any evolutionary algorithm is fitness evaluation. When fitness evaluations are corrupted by noise, as happens in many real-world problems as a consequence of various types of uncertainty, a strategy is needed in order to cope with this. Resampling is one of the most common strategies, whereby each solution is evaluated many times in order to reduce the variance of the fitness estimates. When evaluating the performance of a noisy optimisation algorithm, a key consideration is the stopping condition for the algorithm. A frequently used stopping condition in runtime analysis, known as \u201cFirst Hitting Time\u201d, is to stop the algorithm as soon as it encounters the optimal solution. However, this is unrealistic for real-world problems, as if the optimal solution were already known, there would be no need to search for it. This paper argues that the use of First Hitting Time, despite being a commonly used approach, is significantly flawed and overestimates the quality of many algorithms in real-world cases, where the optimum is not known in advance and has to be genuinely searched for. A better alternative is to measure the quality of the solution an algorithm returns after a fixed evaluation budget, i.e., to focus on final solution quality. This paper argues that focussing on final solution quality is more realistic and demonstrates cases where the results produced by each algorithm evaluation method lead to very different conclusions regarding the quality of each noisy optimisation algorithm.", "creator": "LaTeX with hyperref package"}}}