{"id": "1706.05087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Plan, Attend, Generate: Character-level Neural Machine Translation with Planning in the Decoder", "abstract": "We investigate the integration of a planning mechanism into an encoder-decoder architecture with attention for character-level machine translation. We develop a model that plans ahead when it computes alignments between the source and target sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the strategic attentive reader and writer (STRAW) model. Our proposed model is end-to-end trainable with fully differentiable operations. We show that it outperforms a strong baseline on three character-level decoder neural machine translation on WMT'15 corpus. Our analysis demonstrates that our model can compute qualitatively intuitive alignments and achieves superior performance with fewer parameters.", "histories": [["v1", "Tue, 13 Jun 2017 23:11:04 GMT  (401kb,D)", "https://arxiv.org/abs/1706.05087v1", "Accepted to Rep4NLP 2017 Workshop at ACL 2017 Conference"], ["v2", "Fri, 23 Jun 2017 06:31:05 GMT  (401kb,D)", "http://arxiv.org/abs/1706.05087v2", "Accepted to Rep4NLP 2017 Workshop at ACL 2017 Conference"]], "COMMENTS": "Accepted to Rep4NLP 2017 Workshop at ACL 2017 Conference", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["caglar gulcehre", "francis dutil", "adam trischler", "yoshua bengio"], "accepted": false, "id": "1706.05087"}, "pdf": {"name": "1706.05087.pdf", "metadata": {"source": "CRF", "title": "Plan, Attend, Generate: Character-Level Neural Machine Translation with Planning", "authors": ["Caglar Gulcehre", "Francis Dutil", "Adam Trischler", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it will be able to retaliate, \"he said.\" We have never waited so long to be able to find a solution, \"he said.\" We have never waited so long to be able to find a solution, \"he said.\" We have never waited so long to be able to find a solution, \"he said."}, {"heading": "2 Planning for Character-level Neural Machine Translation", "text": "We now describe how to carefully integrate a planning mechanism into a sequence-to-sequence architecture (Bahdanau et al., 2015). Our model first creates a plan, then calculates a soft alignment based on the plan and generates in the decoder at each time step. We call our model PAG (Plan Attend Generate)."}, {"heading": "2.1 Notation and Encoder", "text": "For input, our model receives a sequence of symbols, X = (x0, \u00b7 \u00b7, x | X |), where | X | specifies the length of X. It processes these with the encoder, a bidirectional RNN. At each input position i, we obtain the annotation vector hi by linking the forward and backward arranged encoder states, hi = [h \u2192 i; h \u2190 i], where h \u2192 i specifies the hidden state of the forward directed RNN of the encoder and h \u2190 i specifies the hidden state of the backward arranged RNN of the encoder. Through the decoder, the model predicts a sequence of output marks, Y = (y1, \u00b7 \u00b7 \u00b7, y | Y |). We use st to indicate the hidden state of the decoder RNN, which generates the target output tag in time step t."}, {"heading": "2.2 Alignment and Decoder", "text": "This is the first time that a country in which it is a country in which it is not a country, but a country in which it is a country in which it is not a country, but a country in which it is a country, a country in which it is a country, a country in which it is a country, a country in which it is a country, a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "2.2.1 Alignment Repeat", "text": "In order to reduce the calculation costs of the model, we also propose an alternative approach to calculating the candidate alignment plan matrix in each step. Specifically, we propose a model variant that reuses the alignment from the previous time step until the connection switch is activated for which the model calculates a new alignment. We call this variant Repeat, Plan, Maintain and Generate (rPAG). rPAG can be seen as learning an explicit alignment plan matrix that also reduces the memory consumption of the model. We offer pseudocode for rPAG in Algorithm 1. Algorithm: Pseudocode for updating the repeat alignment and commitment vector.for j {1, \u00b7 \u00b7 \u00b7 ft} \u00b7 soSoft \u00b7 \u00b7 t = 1 \u00b7 f = 1 \u00b7 f = 1 \u00b7 f = 1 \u00b7 f = (= 1 ft = 1 ft = 1 ft = 1)"}, {"heading": "2.3 Training", "text": "We use a deep output layer (Pascanu et al., 2013) to calculate the conditional distribution over output markers, (6) where there is a matrix of learned parameters and we have omitted the bias for brevity. Function fo is an MLP with tanh activation. The complete model, which includes both the encoder and the decoder, is jointly trained to minimize the (conditional) negative log likelihoodL = \u2212 1 N \u00b2 n = 1 log (y (n) | x (n)))."}, {"heading": "3 Experiments", "text": "In our NMT experiments, we use byte-pair encoding (BPE) (Sennrich et al., 2015) for the source sequence and character representation for the target, the same setup described in Chung et al. (2016). We also use the same pre-processing as in this work.2We test our planning models against the background of WMT '15 tasks for English to German (En \u2192 De), English to Czech (En \u2192 Cs) and English to Finnish (En \u2192 Fi) language pairs. We present the experimental results in Table 1.2 Our implementation is based on the code available at https. / / github.com / nyu-cdecAs is a baseline we use."}, {"heading": "4 Conclusions and Future Work", "text": "In this paper, we addressed a fundamental problem in the neural generation of long sequences by integrating planning into the alignment mechanism of sequence architecture to the problem of machine translation. We proposed two different planning mechanisms: PAG, which constructs explicit plans in the form of stored matrices, and rPAG, which implicitly plans and is computationally cheaper. PAG's approach empirically improves alignment over long input sequences. In machine translation experiments, models with a planning mechanism outperform state-of-the-art in almost all language pairs with fewer parameters. As future work, we plan to test our planning mechanism on the results of the model and other sequence tasks."}, {"heading": "A Qualitative Translations from both Models", "text": "In Table 2, we present sample translations of our model and baseline along with the basic truth. 31 A Republican strategy to counter Obama's re-election A Republican strategy to counter Obama's re-election A Republican strategy to counter Obama's election 2 Republican leaders justify their policies by the need to combat electoral fraud. Republican leaders have justified their policies by the need to combat electoral fraud. Republican leaders have justified their policies by the need to combat electoral fraud. Republican leaders have justified their policies by the need to combat electoral fraud. 3 The U.S. Attorney General has stepped in to enforce the most controversial laws. United States attorneys general intervene to enact the most controversial laws."}], "references": [{"title": "Layer normalization", "author": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton."], "venue": "arXiv preprint arXiv:1607.06450 .", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR) .", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville."], "venue": "arXiv preprint arXiv:1308.3432 .", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.06147 .", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.08148 .", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Memory augmented neural networks with wormhole connections", "author": ["Caglar Gulcehre", "Sarath Chandar", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1701.08718 .", "citeRegEx": "Gulcehre et al\\.,? 2017", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2017}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole."], "venue": "arXiv preprint arXiv:1611.01144 .", "citeRegEx": "Jang et al\\.,? 2016", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "arXiv preprint arXiv:1610.03017 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1604.00788 .", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh."], "venue": "arXiv preprint arXiv:1611.00712 .", "citeRegEx": "Maddison et al\\.,? 2016", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1312.6026 .", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu."], "venue": "Advances in Neural Information Processing Systems. pages 3486\u20133494.", "citeRegEx": "Vezhnevets et al\\.,? 2016", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 8, "context": "Character-level neural machine translation (NMT) is an attractive research problem (Lee et al., 2016; Chung et al., 2016; Luong and Manning, 2016) because it addresses important issues encountered in word-level NMT.", "startOffset": 83, "endOffset": 146}, {"referenceID": 4, "context": "Character-level neural machine translation (NMT) is an attractive research problem (Lee et al., 2016; Chung et al., 2016; Luong and Manning, 2016) because it addresses important issues encountered in word-level NMT.", "startOffset": 83, "endOffset": 146}, {"referenceID": 9, "context": "Character-level neural machine translation (NMT) is an attractive research problem (Lee et al., 2016; Chung et al., 2016; Luong and Manning, 2016) because it addresses important issues encountered in word-level NMT.", "startOffset": 83, "endOffset": 146}, {"referenceID": 5, "context": "Word-level NMT systems can suffer from problems with rare words(Gulcehre et al., 2016) or data sparsity, and the existence of compound words without explicit segmentation in certain language pairs can make learning alignments and translations more difficult.", "startOffset": 63, "endOffset": 86}, {"referenceID": 14, "context": "This planning mechanism is inspired by the strategic attentive reader and writer (STRAW) of Vezhnevets et al. (2016).", "startOffset": 92, "endOffset": 117}, {"referenceID": 3, "context": "NMT performance of encoder-decoder models with attention deteriorates as sequence length increases (Cho et al., 2014; Sutskever et al., 2014), and this effect can be more pronounced at the character-level NMT.", "startOffset": 99, "endOffset": 141}, {"referenceID": 13, "context": "NMT performance of encoder-decoder models with attention deteriorates as sequence length increases (Cho et al., 2014; Sutskever et al., 2014), and this effect can be more pronounced at the character-level NMT.", "startOffset": 99, "endOffset": 141}, {"referenceID": 1, "context": "We now describe how to integrate a planning mechanism into a sequence-to-sequence architecture with attention (Bahdanau et al., 2015).", "startOffset": 110, "endOffset": 133}, {"referenceID": 7, "context": "For the model to operate discretely, we use the recently proposed Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2016) in conjunction with the straight-through estimator (Bengio et al.", "startOffset": 87, "endOffset": 129}, {"referenceID": 10, "context": "For the model to operate discretely, we use the recently proposed Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2016) in conjunction with the straight-through estimator (Bengio et al.", "startOffset": 87, "endOffset": 129}, {"referenceID": 2, "context": ", 2016) in conjunction with the straight-through estimator (Bengio et al., 2013) to backpropagate through ct.", "startOffset": 59, "endOffset": 80}, {"referenceID": 2, "context": ", 2016) in conjunction with the straight-through estimator (Bengio et al., 2013) to backpropagate through ct. The model further learns the temperature for the Gumbel-Softmax as proposed in Gulcehre et al. (2017). Both the commitment vector and the action plan matrix are initialized with ones; this initialization is not modified through training.", "startOffset": 60, "endOffset": 212}, {"referenceID": 15, "context": "We also experimented with training ct using REINFORCE (Williams, 1992) but found that Gumbel-Softmax led to better performance.", "startOffset": 54, "endOffset": 70}, {"referenceID": 11, "context": "We use a deep output layer (Pascanu et al., 2013) to compute the conditional distribution over output tokens,", "startOffset": 27, "endOffset": 49}, {"referenceID": 14, "context": "As noted in (Vezhnevets et al., 2016), the proposed model can learn to recompute very often which decreases the utility of planning.", "startOffset": 12, "endOffset": 37}, {"referenceID": 12, "context": "In our NMT experiments we use byte pair encoding (BPE) (Sennrich et al., 2015) for the source sequence and character representation for the target, the same setup described in Chung et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 4, "context": ", 2015) for the source sequence and character representation for the target, the same setup described in Chung et al. (2016). We also use the same preprocessing as in that work.", "startOffset": 105, "endOffset": 125}, {"referenceID": 4, "context": "(2016), with the attention mechanisms in both the baseline and (r)PAG conditioned on both layers of the encoder\u2019s biscale GRU (h1 and h2 \u2013 see (Chung et al., 2016) for more detail).", "startOffset": 143, "endOffset": 163}, {"referenceID": 4, "context": "com/nyu-dl/dl4mt-cdec As a baseline we use the biscale GRU model of Chung et al. (2016), with the attention mechanisms in both the baseline and (r)PAG conditioned on both layers of the encoder\u2019s biscale GRU (h1 and h2 \u2013 see (Chung et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "As can be seen from Table 1, layer normalization (Ba et al., 2016) improves the performance of the PAG model significantly.", "startOffset": 49, "endOffset": 66}, {"referenceID": 4, "context": "(\u2020) denotes the results of the baseline that we trained using the hyperparameters reported in (Chung et al., 2016) and the code provided with that paper.", "startOffset": 94, "endOffset": 114}], "year": 2017, "abstractText": "We investigate the integration of a planning mechanism into an encoder-decoder architecture with attention for character-level machine translation. We develop a model that plans ahead when it computes alignments between the source and target sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the strategic attentive reader and writer (STRAW) model. Our proposed model is end-to-end trainable with fully differentiable operations. We show that it outperforms a strong baseline on three characterlevel translation tasks from WMT\u201915. Analysis demonstrates that our model computes qualitatively intuitive alignments and achieves superior performance with fewer parameters.", "creator": "LaTeX with hyperref package"}}}