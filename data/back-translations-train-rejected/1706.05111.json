{"id": "1706.05111", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "A Mixture Model for Learning Multi-Sense Word Embeddings", "abstract": "Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.", "histories": [["v1", "Thu, 15 Jun 2017 23:07:06 GMT  (26kb)", "http://arxiv.org/abs/1706.05111v1", "*SEM 2017"]], "COMMENTS": "*SEM 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dai quoc nguyen", "dat quoc nguyen", "ashutosh modi", "stefan thater", "manfred pinkal"], "accepted": false, "id": "1706.05111"}, "pdf": {"name": "1706.05111.pdf", "metadata": {"source": "CRF", "title": "A Mixture Model for Learning Multi-Sense Word Embeddings", "authors": ["Dai Quoc Nguyen", "Dat Quoc Nguyen", "Ashutosh Modi", "Stefan Thater", "Manfred Pinkal"], "emails": ["pinkal}@coli.uni-saarland.de", "dat.nguyen@students.mq.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.05 111v 1 [cs.C L] 15 Jun 2017Technique for generating meaning representations for words. In order to obtain good representations, it is important to consider different senses of a word. In this article, we propose a mixing model for learning multi-meaning word embeddings. Our model generalizes previous work by inducing different weights of different senses of a word. Experimental results show that our model exceeds previous models for standard evaluation tasks."}, {"heading": "1 Introduction", "text": "In this context, the issue of embedding the syntactic and semantic properties of a word is also to be understood by representing the word in the form of a real vector (Mikros and Hovy, 2016; Nguyen et al., 2013a, 2017; Levy and Goldberg, 2014), but usually word embedding is not taken into account by using the word in the form of a real vector (Mikros et al., 2013a, 2013a). The word embedding is not taken into account when dealing with lexical ambiguities. Thus, for example, the word bank is usually represented by a single vector representation."}, {"heading": "2 The mixture model", "text": "In this section we introduce the MSWE 2 model in which we present all SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW-SW"}, {"heading": "3 Experiments", "text": "We evaluate MSWE based on two distinct tasks: word similarity and word analogy. We also provide experimental results obtained through the base model Word2Vec Skip-gram and other previous work. Note that not all previous results in this work are mentioned for comparison, since the training corpus used in most previous research work is much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; 1We use a 3 / 4 power (Mikolov et al., 2013b) as the sound distribution. Levy et al., 2015) In addition, there are differences in the pre-processing steps that could affect the results. We could also improve the results obtained by using a larger training corpus, but this is not the central point of our work. The goal of our work is that the embedding of theme and word can be combined into a single mixing model, which leads to good improvements, as noted irically."}, {"heading": "3.1 Experimental Setup", "text": "After Huang et al. (2012) and Neelakantan et al. (2014), we use the Wesbury Lab Wikipedia Corpus (Shaoul and Westbury, 2010), which has 2M articles with approximately 990M words for training. In the pre-processing stage, texts are mapped to 0 and punctuation marks are removed. We extract a vocabulary of 200,000 common word marks from the pre-processed corpus. Words that do not appear in the vocabulary are mapped to a special token UNK, in which we use the embedding of UNK for unknown words in the benchmark data sets."}, {"heading": "3.2 Word Similarity", "text": "The task of word similarity evaluates the quality of the word embedding models (Reisinger and Mooney, 2010). For a given set of word pairs, the evaluation is done by calculating the correlation between the similarity values of corresponding word embedding pairs with human judgment. Following Reisinger and Mooney (2010), the ranking correlation reflects better word embedding models. (2012), Neelakantan et al. (2014), we calculate the similarity values for a word pair (w, w) with or without their respective contexts (c, c) as: GlobalSim (w, w), Huang et al al (2012), Neelakantan et al al al al al al al al al al al al al al. (2014), we calculate the similarity values for a word pair (w, w) with or without their respective contexts (c, c) as: GlobalSim (w, w) = vector in vector c (vw), w), Avgecs in context."}, {"heading": "3.2.1 Results for word similarity", "text": "Table 2 compares the evaluation results of MSWE with results reported in previous work on the standard word similarity task when using GlobalSim. We use subscriptions 50 and 200 to denote the T = 50 and T = 200 theme model, respectively. Table 2 shows that our model outperforms the base model Word2Vec Skip-gram (in the fifth row from the bottom). Specifically, MSWE achieves a significant improvement of 2.92 in the Spearman rank correlation on the RW dataset (which corresponds to a relative improvement of about 8.5%).Compared to the published results, MSWE achieves the highest accuracy on the RW, SCS, WS353 and MEN datasets and achieves the second highest result on the SIMLEX dataset, indicating that MSWE learns better representations for words taking different meanings into account."}, {"heading": "3.2.2 Results for contextual word similarity", "text": "As shown in Table 3, MSWE performs better than the closely related model of Cheng et al. (2015) and generally achieves good results for this context-sensitive dataset. Although we achieve better scores when using GlobalSim than Neelakantan et al. (2014) and Chen et al. (2014), we are outperformed by them when using AvgSim and AvgSimC. Neelakantan et al. (2014) grouped the embedding of context words around each target word to predict its meaning, and Chen et al. (2014) used pre-coached word embedding to initialize vector representations of senses from WordNet, while we use a fixed number of topics as senses for words in MSWE."}, {"heading": "3.3 Word Analogy", "text": "We evaluate the embedding models on the word analogy task introduced by Mikolov et al. (2013a), which aims to answer questions in the form of \"a is to b how c is to?,\" which are referred to as \"a: b \u2192 c:?\" (e.g. \"Hanoi: Vietnam \u2192 Bern:?\"). There are 8,869 semantic and 10,675 syntactic questions in 14 categories. Each question is answered by finding the most suitable word closest to \"vb \u2212 va + vc\" in terms of cosmic similarity, and the answer is correct only if the word found corresponds exactly to the gold standard (correct) for the question. We report on accuracies in Table 4 and show that MSWE achieves better results compared to the baseline Word2Vec Skip-gram. In particular, MSWE achieves an accuracy of about 69.7%, which is higher than the accuracy achieved by Word2Vec-Skip-gram."}, {"heading": "4 Conclusions", "text": "The results show that our model performs better than Word2Vec and delivers highly competitive results on the standard evaluation tasks. In future work, we will explore better methods for taking into account the context information. We also plan to explore different approaches to calculating the mixing weights in our model. For example, if a large sensory corpus is available for training, the mixing weights could be defined by the frequency (sensory number) distributions rather than the probability distributions generated by a topic model. In addition, it is possible to view the weights of the senses as additional model parameters that are then learned during training."}, {"heading": "Acknowledgments", "text": "This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) as part of CRC 1102 \"Information Density and Linguistic Coding.\" We thank anonymous reviewers for their helpful comments."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Latent Dirichlet Allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Improving distributed representation of word sense via wordnet gloss composition and context clustering", "author": ["Tao Chen", "Ruifeng Xu", "Yulan He", "Xuan Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1025\u20131035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Syntaxaware multi-sense word embeddings for deep compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1531\u20131542.", "citeRegEx": "Cheng and Kartsaklis.,? 2015", "shortCiteRegEx": "Cheng and Kartsaklis.", "year": 2015}, {"title": "Contextual text understanding in distributional semantic space", "author": ["Jianpeng Cheng", "Zhongyuan Wang", "Ji-Rong Wen", "Jun Yan", "Zheng Chen."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "ACM Transactions on Information Systems 20:116\u2013131.", "citeRegEx": "Finkelstein et al\\.,? 2002", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Supersense embeddings: A unified model for supersense interpretation, prediction, and utilization", "author": ["Lucie Flekova", "Iryna Gurevych."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Flekova and Gurevych.,? 2016", "shortCiteRegEx": "Flekova and Gurevych.", "year": 2016}, {"title": "Word embedding evaluation and combination", "author": ["Sahar Ghannay", "Benoit Favre", "Yannick Estve", "Nathalie Camelin."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016).", "citeRegEx": "Ghannay et al\\.,? 2016", "shortCiteRegEx": "Ghannay et al\\.", "year": 2016}, {"title": "Sensembed: Learning sense", "author": ["Roberto Navigli"], "venue": null, "citeRegEx": "Navigli.,? \\Q2015\\E", "shortCiteRegEx": "Navigli.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Sun."], "venue": "AAAI", "citeRegEx": "Sun.,? 2015b", "shortCiteRegEx": "Sun.", "year": 2015}, {"title": "Event embeddings for seman", "author": ["Ashutosh Modi"], "venue": null, "citeRegEx": "Modi.,? \\Q2016\\E", "shortCiteRegEx": "Modi.", "year": 2016}, {"title": "Improving Topic Models", "author": ["Mark Johnson"], "venue": null, "citeRegEx": "Johnson.,? \\Q2015\\E", "shortCiteRegEx": "Johnson.", "year": 2015}, {"title": "Evaluation methods for", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q2015\\E", "shortCiteRegEx": "Joachims.", "year": 2015}, {"title": "The westbury", "author": ["Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul and Westbury.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul and Westbury.", "year": 2010}, {"title": "Recursive deep models", "author": ["Christopher Potts"], "venue": null, "citeRegEx": "Potts.,? \\Q2013\\E", "shortCiteRegEx": "Potts.", "year": 2013}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguis-", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations via gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "International Conference on Learning Representations (ICLR) .", "citeRegEx": "Vilnis and McCallum.,? 2015", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2015}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia", "author": ["Zhaohui Wu", "C. Lee Giles."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. pages 2188\u20132194.", "citeRegEx": "Wu and Giles.,? 2015", "shortCiteRegEx": "Wu and Giles.", "year": 2015}, {"title": "Improving short text classification by learning vector representations of both words and hidden topics", "author": ["Heng Zhang", "Guoqiang Zhong."], "venue": "Knowledge-Based Systems 102:76\u201386.", "citeRegEx": "Zhang and Zhong.,? 2016", "shortCiteRegEx": "Zhang and Zhong.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017).", "startOffset": 171, "endOffset": 333}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al.", "startOffset": 8, "endOffset": 744}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses.", "startOffset": 8, "endOffset": 765}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses.", "startOffset": 8, "endOffset": 789}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al.", "startOffset": 8, "endOffset": 945}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al.", "startOffset": 8, "endOffset": 965}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al.", "startOffset": 8, "endOffset": 989}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al.", "startOffset": 35, "endOffset": 183}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.", "startOffset": 35, "endOffset": 208}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.", "startOffset": 35, "endOffset": 240}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets.", "startOffset": 35, "endOffset": 408}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets.", "startOffset": 35, "endOffset": 440}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al.", "startOffset": 35, "endOffset": 554}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al.", "startOffset": 35, "endOffset": 574}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skip-gram model (Mikolov et al.", "startOffset": 35, "endOffset": 594}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skip-gram model (Mikolov et al.", "startOffset": 35, "endOffset": 621}, {"referenceID": 1, "context": "Specifically, we train a topic model (Blei et al., 2003) to obtain the topic-to-word and document-to-topic probability distributions which are then used to infer the weights of topics.", "startOffset": 37, "endOffset": 56}, {"referenceID": 6, "context": "MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics.", "startOffset": 51, "endOffset": 115}, {"referenceID": 20, "context": "MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics.", "startOffset": 51, "endOffset": 115}, {"referenceID": 1, "context": ", wd,Md}, we apply a topic model (Blei et al., 2003) to obtain the topic-to-word Pr(w|t) and document-to-topic Pr(t|d) probability distributions.", "startOffset": 33, "endOffset": 52}, {"referenceID": 15, "context": "(2014), we use the Wesbury Lab Wikipedia corpus (Shaoul and Westbury, 2010) containing over 2M articles with about 990M words for training.", "startOffset": 48, "endOffset": 75}, {"referenceID": 7, "context": "We firstly use a small subset extracted from the WS353 dataset (Finkelstein et al., 2002) to tune the hyper-parameters of the baseline Word2Vec Skip-gram model for the word similarity task (see Section 3.", "startOffset": 63, "endOffset": 89}, {"referenceID": 1, "context": "The vanilla Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003) is not scalable to a very large corpus, so we explore faster online topic models developed for large corpora.", "startOffset": 58, "endOffset": 77}, {"referenceID": 7, "context": "WS353 353 Finkelstein et al. (2002) SIMLEX 999 Hill et al.", "startOffset": 10, "endOffset": 36}, {"referenceID": 7, "context": "WS353 353 Finkelstein et al. (2002) SIMLEX 999 Hill et al. (2015) SCWS 2003 Huang et al.", "startOffset": 10, "endOffset": 66}, {"referenceID": 7, "context": "WS353 353 Finkelstein et al. (2002) SIMLEX 999 Hill et al. (2015) SCWS 2003 Huang et al. (2012)", "startOffset": 10, "endOffset": 96}, {"referenceID": 2, "context": "MEN 3000 Bruni et al. (2014)", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.", "startOffset": 4, "endOffset": 23}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.2 \u2013 \u2013 Hill et al. (2015) \u2013 41.", "startOffset": 4, "endOffset": 55}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.2 \u2013 \u2013 Hill et al. (2015) \u2013 41.4 \u2013 65.5 69.9 Vilnis and McCallum (2015) \u2013 32.", "startOffset": 4, "endOffset": 101}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.2 \u2013 \u2013 Hill et al. (2015) \u2013 41.4 \u2013 65.5 69.9 Vilnis and McCallum (2015) \u2013 32.23 \u2013 65.49 71.31 Schnabel et al. (2015) \u2013 \u2013 \u2013 64.", "startOffset": 4, "endOffset": 146}, {"referenceID": 8, "context": "9 Flekova and Gurevych (2016) \u2013 \u2013 \u2013 \u2013 74.", "startOffset": 2, "endOffset": 30}, {"referenceID": 3, "context": "3 Chen et al. (2014) 66.", "startOffset": 2, "endOffset": 21}, {"referenceID": 4, "context": "As shown in Table 3, MSWE scores better than the closely related model proposed by Cheng et al. (2015) and generally obtains good results for this context sensitive dataset.", "startOffset": 83, "endOffset": 103}, {"referenceID": 4, "context": "As shown in Table 3, MSWE scores better than the closely related model proposed by Cheng et al. (2015) and generally obtains good results for this context sensitive dataset. Although we produce better scores than Neelakantan et al. (2014) and Chen et al.", "startOffset": 83, "endOffset": 239}, {"referenceID": 3, "context": "(2014) and Chen et al. (2014) when using GlobalSim, we are outperformed by them when using AvgSim and AvgSimC .", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "(2014) and Chen et al. (2014) when using GlobalSim, we are outperformed by them when using AvgSim and AvgSimC . Neelakantan et al. (2014) clustered the embeddings of the context words around each target word to predict its sense and Chen et al.", "startOffset": 11, "endOffset": 138}, {"referenceID": 3, "context": "(2014) and Chen et al. (2014) when using GlobalSim, we are outperformed by them when using AvgSim and AvgSimC . Neelakantan et al. (2014) clustered the embeddings of the context words around each target word to predict its sense and Chen et al. (2014) used pre-trained word embeddings to initialize vector representations of senses taken from WordNet, while we use a fixed number of topics as senses for words in MSWE.", "startOffset": 11, "endOffset": 252}, {"referenceID": 0, "context": "3 Baroni et al. (2014) 68.", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": "3 Baroni et al. (2014) 68.0 Neelakantan et al. (2014) 64.", "startOffset": 2, "endOffset": 54}, {"referenceID": 0, "context": "3 Baroni et al. (2014) 68.0 Neelakantan et al. (2014) 64.0 Ghannay et al. (2016) 62.", "startOffset": 2, "endOffset": 81}], "year": 2017, "abstractText": "Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.", "creator": "LaTeX with hyperref package"}}}