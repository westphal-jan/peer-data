{"id": "1706.05140", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "An Automatic Approach for Document-level Topic Model Evaluation", "abstract": "Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.", "histories": [["v1", "Fri, 16 Jun 2017 03:53:38 GMT  (271kb,D)", "http://arxiv.org/abs/1706.05140v1", "10 pages; accepted for the Twenty First Conference on Computational Natural Language Learning (CoNLL 2017)"]], "COMMENTS": "10 pages; accepted for the Twenty First Conference on Computational Natural Language Learning (CoNLL 2017)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shraey bhatia", "jey han lau", "timothy baldwin"], "accepted": false, "id": "1706.05140"}, "pdf": {"name": "1706.05140.pdf", "metadata": {"source": "CRF", "title": "An Automatic Approach for Document-level Topic Model Evaluation", "authors": ["Shraey Bhatia", "Jey Han Lau", "Timothy Baldwin"], "emails": ["shraeybhatia@gmail.com,", "jeyhan.lau@gmail.com,", "tb@ldwin.net"], "sections": [{"heading": "1 Introduction", "text": "Topic models like latent Dirichlet allocation (Blei et al., 2003) together learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualization (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017). One feature of LDA theme models that have contributed to their popularity is that they are highly configurable and can be structured to capture a variety of statistical dependencies, such as between topics (Lead and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al, 2004), or between documents associated with individuals in different network relationships (Wang and Blei, 2011)."}, {"heading": "2 Related Work", "text": "Perplexity or pre-existing probability has long been used as an intrinsic metric to evaluate theme models (Wallach et al., 2009). Chang et al. (2009) proposed two human judgement tasks, at theme and document level, and showed that there is a low correlation between perplexity and direct human evaluations of theme model quality. Both tasks took the form of \"intruders\" tasks, whereby the subjects were entrusted with identifying an intruder theme for a particular topic or an intruder theme for a particular document. Specifically, the word intrusionar Xiv: 170 6.05 140v 1 [cs.C L] 16 Jun 2017 added an intruder word to the top 5 theme words, and the refuseniks were asked to identify the intruder word."}, {"heading": "3 Datasets and Topic Models", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "4 Topic-level Evaluation: Topic Coherence", "text": "Although the method is successful in assessing topic quality, it says little about the link between documents and topics. As we will see, a topic model can produce topics that are coherent - in the sense of the npmi association - but represent a poor descriptor of the overall concepts in the document collection. We first calculate the topic coherence for all 5 topic models via APNEWS and BNC using npmi (Lau et al., 2014) and present the results in Table 2.4. We see that lda and cluster consistently perform well in both datasets."}, {"heading": "5 Human Evaluation of Document-level Topic Allocations", "text": "In this section, we describe a series of manual evaluations of theme allocation at the document level in order to obtain a more holistic assessment of the true quality of the various theme models (in line with the original work by Chang et al. (2009))."}, {"heading": "5.1 Topic Intrusion", "text": "This year, it is as far as ever in the history of the city we are in. This year, it is as far as ever in the history of the city we are in."}, {"heading": "5.2 Direct Annotation of Topic Assignments", "text": "Newman et al. (2010b) proposed a more direct approach to topic coherence by asking people to rate topics directly on the basis of the top N words. Inspired by their methodology, we propose to comment directly on each topic assigned to a target document. We present human commentators with the target document and the top ranking topic (most likely) from each of the five topic models and ask them to rank each topic on an order scale of 0-3. At model level, we take the mean rating of all document-theme pairings for this topic model (once more than 100 documents per collection).5 We summarize the results in Table 6.We note that lda performs significantly better in the case of APNEWS than ctm and hca, whereas for BNC, lda and ctm are fairly close together, with hca and ntm performing poorly across both datasets. The general trend for APNEWlc > ctc > hsintm are not consistent in general > cttsc > hsintm >."}, {"heading": "6 Automatic Evaluation", "text": "One limitation of the intruder task is that it requires manual annotation, making it unsuitable for large-scale or automatic evaluation. We present the first attempt to automate the prediction of the intruder topic, with the aim of developing a summary approach to topic model evaluation. 5The 100 documents used for this task differed from the topic coherence documents used in Section 5.1 (as motivated in Sections 4 and 5)."}, {"heading": "6.1 Methodology", "text": "We build a support vector regression (SVR) model (Joachims, 2006) to evaluate topics that contain a document to select the intruder theme. We first explain an intuition of the characteristics that the SVR has. To evaluate topics for a document, we first have to calculate the probability of a topic given document d, i.e. P (t | d). We can reverse the condition using the Bayes rule: P (t | d) P (t) P (t).P (t) P (t) P (t) as the probability of the document d, since the probability of the document d is constant for the topics we rank. Next, we represent the topic t with its top N most likely words, which gives: P (t | d).P (d) artip (d) P (w1, wN), wN)."}, {"heading": "6.2 System results", "text": "We see that the trend line for the precision of the system model is very much in line with the precision of the human model. Generally, the best systems - lda and ctm - and the worst systems - ntm and cluster.The correlation between the two is very high at r = 0.88 and 0.87 for APNEWS and BNC, respectively. This suggests that the automated method is a reliable means of evaluating the quality of the theme model at the document level.9We use the default hyperparameter values for the SVR (C = 0.01) and therefore do not require a development kit for tuning.10Note that the precision of the system model is a binary value for each combination of documentation topics, as there is - unlike several annotators - only one system that selects an intrusion word."}, {"heading": "7 Discussion", "text": "In order to better understand the differences between people and systems, we need to focus on a number of documents and associated issues."}, {"heading": "8 Conclusion", "text": "We show empirically that there can be large discrepancies between theme coherence and document-theme associations. In developing an artificial theme model, we have shown that a theme model can simultaneously produce themes that are coherent but make the document collection appear largely indescribable. We propose a method for automatically predicting theme quality at the document level and found encouraging correlation with manual evaluation, suggesting that it can be used as an alternative approach for extrinsic theme model evaluation."}, {"heading": "Acknowledgements", "text": "This research was partially supported by the Australian Research Council."}], "references": [{"title": "Evaluating topic coherence using distributional semantics", "author": ["Nikos Aletras", "Mark Stevenson."], "venue": "Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10). Potsdam, Germany, pages 13\u201322.", "citeRegEx": "Aletras and Stevenson.,? 2013", "shortCiteRegEx": "Aletras and Stevenson.", "year": 2013}, {"title": "Latent Dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Experiments with non-parametric topic models", "author": ["Wray L Buntine", "Swapnil Mishra."], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 881\u2013890.", "citeRegEx": "Buntine and Mishra.,? 2014", "shortCiteRegEx": "Buntine and Mishra.", "year": 2014}, {"title": "User guide for the British National Corpus", "author": ["Lou Burnard"], "venue": null, "citeRegEx": "Burnard.,? \\Q1995\\E", "shortCiteRegEx": "Burnard.", "year": 1995}, {"title": "A novel neural topic model and its supervised extension", "author": ["Ji."], "venue": "Proceedings of AAAI 2015. pages 2210\u20132216.", "citeRegEx": "Ji.,? 2015", "shortCiteRegEx": "Ji.", "year": 2015}, {"title": "Visualizing topic models", "author": ["Allison June-Barlow Chaney", "David M. Blei."], "venue": "Proceedings of the 6th International Conference on Weblogs and Social Media (ICWSM 2012). Dublin, Ireland.", "citeRegEx": "Chaney and Blei.,? 2012", "shortCiteRegEx": "Chaney and Blei.", "year": 2012}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-Graber", "David M. Blei."], "venue": "Advances in Neural Information Processing Systems 21 (NIPS-09). Vancouver, Canada, pages 288\u2013296.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Sampling table configurations for the hierarchical poisson-dirichlet process", "author": ["Changyou Chen", "Lan Du", "Wray Buntine."], "venue": "Machine Learning and Knowledge Discovery in Databases pages 296\u2013311.", "citeRegEx": "Chen et al\\.,? 2011", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Accounting for burstiness in topic models", "author": ["Gabriel Doyle", "Charles Elkan."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. pages 281\u2013288.", "citeRegEx": "Doyle and Elkan.,? 2009", "shortCiteRegEx": "Doyle and Elkan.", "year": 2009}, {"title": "Using word embedding to evaluate the coherence of topics from Twitter data", "author": ["Anjie Fang", "Craig Macdonald", "Iadh Ounis", "Philip Habel."], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information", "citeRegEx": "Fang et al\\.,? 2016", "shortCiteRegEx": "Fang et al\\.", "year": 2016}, {"title": "Training linear SVMs in linear time", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 217\u2013226.", "citeRegEx": "Joachims.,? 2006", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "The sensitivity of topic coherence evaluation to topic cardinality", "author": ["Jey Han Lau", "Timothy Baldwin."], "venue": "Proceedings of NAACL-HLT . pages 483\u2013487.", "citeRegEx": "Lau and Baldwin.,? 2016", "shortCiteRegEx": "Lau and Baldwin.", "year": 2016}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["Jey Han Lau", "David Newman", "Timothy Baldwin."], "venue": "Proceedings of EACL 2014. pages 530\u2013539.", "citeRegEx": "Lau et al\\.,? 2014", "shortCiteRegEx": "Lau et al\\.", "year": 2014}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Association for Computational Linguistics (ACL) System Demonstrations.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Workshop at the International Conference on Learning Representations, 2013. Scottsdale, USA.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Mimno et al\\.,? 2011", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "Visualizing document collections and search results using topic mapping", "author": ["David Newman", "Timothy Baldwin", "Lawrence Cavedon", "Sarvnaz Karimi", "David Martinez", "Justin Zobel."], "venue": "Journal of Web Semantics 8(2\u20133):169\u2013175.", "citeRegEx": "Newman et al\\.,? 2010a", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Automatic evaluation of topic coherence", "author": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Newman et al\\.,? 2010b", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "The author-topic model for authors and documents", "author": ["Michal Rosen-Zvi", "Thomas Griffiths", "Mark Steyvers", "Padhraic Smyth."], "venue": "Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence. pages 487\u2013494.", "citeRegEx": "Rosen.Zvi et al\\.,? 2004", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Evaluating visual representations for topic understanding and their effects on manually generated labels", "author": ["Alison Smith", "Tak Yeon Lee", "Forough PoursabziSangdeh", "Jordan Boyd-Graber", "Kevin Seppi", "Niklas Elmqvist", "Leah Findlater."], "venue": "Transac-", "citeRegEx": "Smith et al\\.,? 2017", "shortCiteRegEx": "Smith et al\\.", "year": 2017}, {"title": "Evaluation methods for topic models", "author": ["Hanna M Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno."], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML 2009). Montreal, Canada, pages 1105\u20131112.", "citeRegEx": "Wallach et al\\.,? 2009", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["Chong Wang", "David M. Blei."], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 448\u2013456.", "citeRegEx": "Wang and Blei.,? 2011", "shortCiteRegEx": "Wang and Blei.", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Topic models such as latent Dirichlet allocation (Blei et al., 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 17, "context": ", 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017).", "startOffset": 273, "endOffset": 338}, {"referenceID": 5, "context": ", 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017).", "startOffset": 273, "endOffset": 338}, {"referenceID": 20, "context": ", 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017).", "startOffset": 273, "endOffset": 338}, {"referenceID": 19, "context": "One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (Blei and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al., 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011).", "startOffset": 293, "endOffset": 317}, {"referenceID": 22, "context": ", 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011).", "startOffset": 89, "endOffset": 110}, {"referenceID": 6, "context": "Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct human assessment of topic model quality (Chang et al., 2009), motivating the need for automatic topic model evaluation methods which emulate human assessment.", "startOffset": 140, "endOffset": 160}, {"referenceID": 18, "context": "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.", "startOffset": 91, "endOffset": 199}, {"referenceID": 16, "context": "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.", "startOffset": 91, "endOffset": 199}, {"referenceID": 0, "context": "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.", "startOffset": 91, "endOffset": 199}, {"referenceID": 12, "context": "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.", "startOffset": 91, "endOffset": 199}, {"referenceID": 9, "context": "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.", "startOffset": 91, "endOffset": 199}, {"referenceID": 12, "context": ", 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al., 2014).", "startOffset": 223, "endOffset": 241}, {"referenceID": 21, "context": "Perplexity or held-out likelihood has long been used as an intrinsic metric to evaluate topic models (Wallach et al., 2009).", "startOffset": 101, "endOffset": 123}, {"referenceID": 6, "context": "Chang et al. (2009) proposed two human judgement tasks, at the topic and document levels, and showed that there is low correlation between perplexity and direct human evaluations of topic model quality.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "(2014) proposed an improved method for estimating observed coherence based on normalised pmi (npmi), and further automated the word intruder detection task based on a combination of word association features (pmi, npmi, CP1, and CP2) in a learn-to-rank model (Joachims, 2006).", "startOffset": 259, "endOffset": 275}, {"referenceID": 6, "context": "Building on the work of Chang et al. (2009), Lau et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 6, "context": "Building on the work of Chang et al. (2009), Lau et al. (2014) proposed an improved method for estimating observed coherence based on normalised pmi (npmi), and further automated the word intruder detection task based on a combination of word association features (pmi, npmi, CP1, and CP2) in a learn-to-rank model (Joachims, 2006).", "startOffset": 24, "endOffset": 63}, {"referenceID": 3, "context": "We use two document collections for our experiments: APNEWS and the British National Corpus (\u201cBNC\u201d: Burnard (1995)).", "startOffset": 100, "endOffset": 115}, {"referenceID": 13, "context": "In terms of preprocessing, we use Stanford CoreNLP (Manning et al., 2014) to tokenise words and sentences.", "startOffset": 51, "endOffset": 73}, {"referenceID": 6, "context": "Similarly to Chang et al. (2009), we base our", "startOffset": 13, "endOffset": 33}, {"referenceID": 1, "context": "\u2022 lda (Blei et al., 2003) uses a symmetric Dirichlet prior to model both document-level topic mixtures and topic-level word mixtures.", "startOffset": 6, "endOffset": 25}, {"referenceID": 2, "context": "\u2022 hca (Buntine and Mishra, 2014) is an extension to LDA to capture word burstiness (Doyle and Elkan, 2009), based on the observation that there tends to be higher likelihood of generating a word which has already been seen recently.", "startOffset": 6, "endOffset": 32}, {"referenceID": 8, "context": "\u2022 hca (Buntine and Mishra, 2014) is an extension to LDA to capture word burstiness (Doyle and Elkan, 2009), based on the observation that there tends to be higher likelihood of generating a word which has already been seen recently.", "startOffset": 83, "endOffset": 106}, {"referenceID": 7, "context": "Word generation is modelled by a Pitman\u2013Yor process (Chen et al., 2011).", "startOffset": 52, "endOffset": 71}, {"referenceID": 18, "context": "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).", "startOffset": 123, "endOffset": 231}, {"referenceID": 16, "context": "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).", "startOffset": 123, "endOffset": 231}, {"referenceID": 0, "context": "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).", "startOffset": 123, "endOffset": 231}, {"referenceID": 12, "context": "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).", "startOffset": 123, "endOffset": 231}, {"referenceID": 9, "context": "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).", "startOffset": 123, "endOffset": 231}, {"referenceID": 12, "context": "We first compute topic coherence for all 5 topic models over APNEWS and BNC using npmi (Lau et al., 2014) and present the results in Table 2.", "startOffset": 87, "endOffset": 105}, {"referenceID": 6, "context": "ations of document-level topic allocations, in order to get a more holistic evaluation of the true quality of the different topic models (in line with the original work of Chang et al. (2009)).", "startOffset": 172, "endOffset": 192}, {"referenceID": 6, "context": "We formulate the task similarly to Chang et al. (2009), in presenting the human judges with a snippet from each document, along with four topics.", "startOffset": 35, "endOffset": 55}, {"referenceID": 10, "context": "We build a support vector regression (SVR) model (Joachims, 2006) to rank topics given a document to select the intruder topic.", "startOffset": 49, "endOffset": 65}, {"referenceID": 11, "context": "We use 10 words as it is the standard approach to visualising topics, but this is an important hyper-parameter which needs to be investigated further (Lau and Baldwin, 2016), which we leave to future work.", "startOffset": 150, "endOffset": 173}], "year": 2017, "abstractText": "Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topicand documentlevel model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of documentlevel topic allocations, and provide empirical evidence for its robustness.", "creator": "LaTeX with hyperref package"}}}