{"id": "1706.05148", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "Veiled Attributes of the Variational Autoencoder", "abstract": "Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds obscured by gross corruptions. However, this previously unexplored feature comes with the cost of potential model collapse to a degenerate distribution that may be less suitable as the basis for generating new samples.", "histories": [["v1", "Fri, 16 Jun 2017 05:31:10 GMT  (422kb,D)", "http://arxiv.org/abs/1706.05148v1", null], ["v2", "Fri, 23 Jun 2017 10:32:13 GMT  (425kb,D)", "http://arxiv.org/abs/1706.05148v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bin dai", "yu wang", "john aston", "gang hua", "david wipf"], "accepted": false, "id": "1706.05148"}, "pdf": {"name": "1706.05148.pdf", "metadata": {"source": "META", "title": "Veiled Attributes of the Variational Autoencoder", "authors": ["Bin Dai", "Yu Wang"], "emails": ["daib13@mails.tsinghua.edu.cn", "yw323@cam.ac.uk", "j.aston@statslab.cam.ac.uk", "ganghua@microsoft.com", "davidwip@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "We start with a DatasetX = {x) ni = 1 composed of n iid samples of some random variables x (Rd of interest), with the aim of calculating a tractable approximation to pledge (x), the knowledge of which would allow us to generate new samples of x. Furthermore, we assume that each sample is dominated by unobserved variables z (x), which are intractable in all but the simplest cases, so that variable self-coders (VAE) are a powerful means of optimizing a tractable upper limit."}, {"heading": "2 Affine Decoder and Probabilistic PCA", "text": "If we assume that \"x\" is fixed at some point, and the force \"z\" = 0 (while we remove the now undefined logbook (cf. cf. vAE model), then it is easy to see that the resulting VAE model is reduced to a traditional AE with a quadratic error function (cf. [1], a common practical assumption. To see this, cf. cf. cf. cf. cf. cf. cf. if \"x\" (cf. vAE model) in a different way than \"x\" (cf. cf. vAE model with a quadratic mean) and \"x\" (cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf."}, {"heading": "3 Partially Affine Decoder and Robust PCA", "text": "So far, we have considered narrow limitations on the complexity of the functional forms of both micro- and quantum physics. (...) Although this requires considerable capacity for the potential risk of overadjustment, we will soon see that the UAE is nevertheless able to regulate itself in a very precise sense: Global minimizers of the UAE lens will ultimately correspond with optimal solutions that can be analyzed in L, Sn and S, s.t. X = L + S, optimal solutions for both B and D. (...) By disconnecting we mean that, to verse from one minimum to another, we ascend the objective function at some point along the way. (...)"}, {"heading": "4 Degeneracies Arising from a Flexible Decoder Mean", "text": "In this section, we consider the case where \u00b5x is finally released from its affine captivity to connect with posterior colleagues in the wild. (This also means that overfitting cannot be avoided if \u00b5x is overparameterized.) This is because, at least at a high level, the regulatory effect of overfitting can be expressed in these situations, resulting in the following result: Theorem 3 Suppose = 1 (i.e., a latent dimension of only one), and the latent dimension of only one (a scalar), and a > x for a fixed vector a, a piexI, and \u00b5x is an arbitrary piecewise linear function with n segments. Then the VAE target is unlimited by a trivial solution."}, {"heading": "5 Experiments", "text": "In this section, we evaluate empirically three concrete hypotheses directly derived from our previous analyses, each of which has far-reaching consequences in terms of how UAE models should be applied and interpreted in practice: (i) If the mean function of the decoder is allowed to be nonlinear, the UAE should behave like a nonlinear extension of RPCA, but with natural regulatory mechanisms that help to avoid local minima and / or overadjustments to outliers. Thus, it is likely to outperform either RPCA algorithms or an AU on a variety of varied recovery / outlier discovery problems that have nothing to do with the generative modeling tasks for which the UAE was originally designed. (ii) If the latent representation of the UAE z is greater than necessary (meaning that its dimension is higher than the underlying data diversity), we mean that we expect an unnecessary layer of AE."}, {"heading": "5.1 Hypothesis (i) Evaluation", "text": "If our theory is universally applicable, then a UAE with appropriate parameters is capable of significantly outperforming an analog deterministic AU (i.e., the UAE with its multiple dimensions and corruption processes). (We can show this UAE capability here for the first time in a series of manifold dimensions and corruption processes that restore a nonlinear version of what is commonly referred to as phase transition in the RPCA literature, but is corrupted with gross outliers. (4, 8, 15, 24) These plots evaluate the reconstruction quality of competing algorithms for each sub-space dimension and escape rate that creates a heat map distinguishing success from failure. Of course, explicit knowledge of the basic truth of low dimensional diversity is required to achieve this."}, {"heading": "5.2 Hypothesis (ii) Evaluation", "text": "We train analog AE and UAE models with nonlinear decoders that represent networks on the MNIST dataset of handwritten numerical images [17] as they are varied. We use all n = 70000 samples, each in size 28 x 28. The encoder is a 4-layer ReLU network (d-1000-500-250-2000), with d = 28 x 28 = 784. We draw values from n = 70000 samples, each in size 28 x 28, 15, 20, 25, 30, 35, 40}. In addition, \u00b5z and \u0445z share the first three hidden layers, but have separate output layers, similar [16]. The decoder is a 4-layer network that simply reverses the encoder structure. The first layer of the decoder means that the network can be expressed as h1 = W 1z + b1 (before non-linearity), which is equivalent in isolation to the decoder or middle model."}, {"heading": "5.3 Hypothesis (iii) Evaluation", "text": "We create histograms of all diagonal elements of {\u03a3 (i) z} n i = 1as noise ratios and multiple dimensions vary under the same conditions described in Section 5.1. The results are presented in Figure 3, where we observe that for simpler problems, i.e. smaller outlier ratios and / or smaller manifold dimensions, the diagonal elements of \u03a3 (i) z are forced toward zero. In order for this to happen, \u00b5x (\u00b5z [x]) must fit exactly to the inlier positions, while \u0394x, where necessary, must reflect the outlier support. This situation allows the dataset to overpower the KL regulator, which would otherwise prevent it (i) z from ever being insufficient in the region surrounding a global optima of the VAE energy function. In contrast, for harder problems (i.e. higher outlier ratios and / or manifold dimensions), the data can be adjusted relative to the direction of the L (and vice versa), so that the influences of the data are more complex (i)."}, {"heading": "6 Discussion", "text": "Although originally developed as a viable deep generative model or tied to data probability, in this work we have uncovered certain behaviors of the UAE that are not obvious at the first inspection. For example, the latent covariance \u044bz serves not only its supposed role in bringing diversity into the learned generational process, but also as an important smoothing mechanism that helps in the robust recovery of corrupt samples, even if this sometimes requires behavior (i.e. zero convergence) that may conflict with its original design purpose."}, {"heading": "Appendix A \u2212 Additional MNIST Dataset Experiment", "text": "Here we examine the practical denunciation of MNIST data corrupted by outliers using a VAE model. Outliers are added to the handwritten MNIST data [17] by randomly replacing 5% to 50% of the pixels with a value that is sampled evenly from [0, 255] to createX. The encoder is the same as in section??, i.e. a 4-layer ReLU network (d-1000-500-250-\u043c), where we should provide significantly better performance for the dimension of e.g. We train a VAE using both aspects: Same for 1 and Same for 5 latent samples (z (i, t)} \u03c4t = 1 for each x (i), observing that the latter, which is closer to the posterior, should perform significantly better. We compare the VAE with convex RPCA for the task of restoring the original, uncorrupted worthiness anchor type, superimposed by this RISB structure, which is used for cleaning RISB."}, {"heading": "Appendix B \u2212 Proof of Lemma 1", "text": "Under the given assumptions, the UAE costs can be simplified: asL (\u03b8, \u03c6) = \u2211 i {Eq\u03c6 (z | x (i)) [1 \u03bb \u00b2 \u00b2 \u00b2 (i) \u2212 conservation (i) \u2212 array (i) \u2212 array (i) \u2212 W\u00b5 (i) + d log (i) + tr (i) z \u00b2 (i) z \u00b2 (i) z \u00b2 \u00b2 (i) x (i) x (i) x \u00b2 (i) x (i) z \u00b2 (i) z \u00b2 (i) z \u00b2) z \u00b2 (i) z \u00b2 (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z (i) z) z (i) z (i) z (i) z (i) z (i) z (i) z) z (i) z (i) z) z (i) z (i) z (i) z (i) z (i) z) z (i) z (i) z (i) z (i) z) z (i) z)) z (i) z (i) z (i) (i) z) z (i)"}, {"heading": "Appendix C \u2212 Proof of Theorem 1", "text": "First, for part 1 of the theorem, since WRR > = WPP > W > = WW > = WW > for each rotation R and permutation P, then it is obvious that ifW * must also be a minimum of (5), W * R and W * P. Likewise, since we also have this minimum of (6), we must have this minimum of (7) (W * log of (7), the minimum of (7), W * log of (7), W * P must be a minimum of (7), W * Lsep (W *) = minimum of (W *)."}, {"heading": "Appendix D \u2212 Proof of Theorem 2", "text": "The basic high-level strategy is as follows: we first present a solution that (12) is sufficient and carefully defines the achievable objective functional value for (0,) x (33) and (2) small. We then analyze a lower limit on UAE costs and show that no solution can be significantly better, namely any solution that has to match the performance of our original proposal (12). As this is a lower limit, this implies that no other solution can both minimize and not satisfy the UAE target (12). We will now go to the details. Define \u00b5 (i) z, (x) z (x) and p)."}, {"heading": "Appendix E \u2212 Proof of Corollary 1", "text": "Under the given conditions, the semi-affinity UAE costs are simplified to the function L (W, x, \u00b5z) = \u2211 i {(x (i) \u2212 W\u00b5 (i) > (\u03a3 (i) x) \u2212 1 (x (i) \u2212 W\u00b5 (i) z + log."}, {"heading": "Appendix F \u2212 Proof of Theorem 3", "text": "(i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) ("}], "references": [{"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "International Conference on Computational Statistics, page 177\u2013187,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological Cybernetics,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Robust principal component analysis? J", "author": ["E. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "ACM, 58(2), May", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM J. Optimization, 21(3), June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun"], "venue": "International Conference on Artificial Intelligence and Statistics, page 192\u2013204,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "author": ["A. Choromanska", "Y. LeCun", "G. Ben Arous"], "venue": "Conference on Learning Theory, page 1756\u20131760,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian robust principal component analysis", "author": ["X. Ding", "L. He", "L. Carin"], "venue": "IEEE Trans. Image Processing, 20(12), Dec.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 35(11),", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A Journey into Linear Analysis", "author": ["D.J.H. Garling"], "venue": "Cambridge University Press,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "arXiv:1406.2661,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["K. Kawaguchi"], "venue": "Advances in Neural Information Processing Systems 29, pages 586\u2013594,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Partial sum minimization of singular values in RPCA for low-level vision", "author": ["T.H. Ohand H. Kim", "Y.W. Tai", "J.C. Bazin", "I.S. Kweon"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "International Conference on Learning Representations,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, Nov", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Correlation adaptive subspace segmentation by trace lasso", "author": ["C. Lu", "J. Feng", "Z. Lin", "S. Yan"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted Boltzman machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "International conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories", "author": ["S. Rao", "R. Tron", "R. Vidal", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 32(10),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "International Conference on Learning Representations,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic principal component analysis", "author": ["M. Tipping", "C. Bishop"], "venue": "J. Royal Statistical Society, Series B, 61(3):611\u2013622,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Non-convex rank minimization via an empirical Bayesian approach", "author": ["D. Wipf"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "Given that this integral is intractable in all but the simplest cases, variational autoencoders (VAE) represent a powerful means of optimizing with respect to \u03b8 a tractable upper bound on \u2212 log p\u03b8(x) [16, 21].", "startOffset": 200, "endOffset": 208}, {"referenceID": 20, "context": "Given that this integral is intractable in all but the simplest cases, variational autoencoders (VAE) represent a powerful means of optimizing with respect to \u03b8 a tractable upper bound on \u2212 log p\u03b8(x) [16, 21].", "startOffset": 200, "endOffset": 208}, {"referenceID": 15, "context": "At least for practical purposes, one way around this is to replace the troublesome expectation with a Monte Carlo stochastic approximation [16, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 20, "context": "At least for practical purposes, one way around this is to replace the troublesome expectation with a Monte Carlo stochastic approximation [16, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 1, "context": "Therefore, assuming all the required moments \u03bcz , \u03a3z , \u03bcx, and \u03a3x are differentiable with respect to \u03c6 and \u03b8, the entire model can be updated using SGD [2].", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].", "startOffset": 145, "endOffset": 163}, {"referenceID": 6, "context": "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].", "startOffset": 145, "endOffset": 163}, {"referenceID": 10, "context": "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].", "startOffset": 145, "endOffset": 163}, {"referenceID": 13, "context": "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].", "startOffset": 145, "endOffset": 163}, {"referenceID": 21, "context": "This mirrors the rich tradition of analyzing deep networks under various simplifications such as linear layers or iid random activation patterns [6, 7, 11, 14, 22].", "startOffset": 145, "endOffset": 163}, {"referenceID": 3, "context": "In fact, when the decoder mean \u03bcx is restricted to an affine function of z, we prove that the VAE model collapses to a form of robust PCA [4, 5], a recently celebrated technique for separating data into low-rank (low-dimensional) and sparse outlier components.", "startOffset": 138, "endOffset": 144}, {"referenceID": 4, "context": "In fact, when the decoder mean \u03bcx is restricted to an affine function of z, we prove that the VAE model collapses to a form of robust PCA [4, 5], a recently celebrated technique for separating data into low-rank (low-dimensional) and sparse outlier components.", "startOffset": 138, "endOffset": 144}, {"referenceID": 0, "context": "We expose that a central, albeit underappreciated role of the VAE encoder covariance \u03a3z is to smooth out undesirable minima in the energy landscape of what would otherwise amount to a traditional deterministic autoencoder (AE) [1], even if at times it may not alter the globally optimal solution, or contribute to sample diversity when operating as a generative model.", "startOffset": 227, "endOffset": 230}, {"referenceID": 0, "context": "If we assume that \u03a3x is fixed at some \u03bbI , and force \u03a3z = 0 (while removing the now undefined log |\u03a3z| term), then it is readily apparent that the resultant VAE model reduces to a traditional AE with squared-error loss function [1], a common practical assumption.", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "With affine encoder and decoder models, the resulting deterministic network will simply learn principle components like vanilla PCA, a well-known special case of the AE [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 22, "context": "The objective (5) is the same as that used by certain probabilistic PCA models [23], even though the latter is originally derived in a completely different manner.", "startOffset": 79, "endOffset": 83}, {"referenceID": 3, "context": "This problem represents the canonical form of robust principal component analysis (RPCA) [4, 5], decomposing a data matrixX into low-rank principal factors L = UV and a sparse outlier component S.", "startOffset": 89, "endOffset": 95}, {"referenceID": 4, "context": "This problem represents the canonical form of robust principal component analysis (RPCA) [4, 5], decomposing a data matrixX into low-rank principal factors L = UV and a sparse outlier component S.", "startOffset": 89, "endOffset": 95}, {"referenceID": 3, "context": "\u2022 A number of celebrated results have stipulated conditions [4, 5] whereby global solutions of the convex relaxation into nuclear and `1 norm components given by min L,S \u221a n \u00b7 rank \u2016L\u2016\u2217 + \u2016S\u20161, s.", "startOffset": 60, "endOffset": 66}, {"referenceID": 4, "context": "\u2022 A number of celebrated results have stipulated conditions [4, 5] whereby global solutions of the convex relaxation into nuclear and `1 norm components given by min L,S \u221a n \u00b7 rank \u2016L\u2016\u2217 + \u2016S\u20161, s.", "startOffset": 60, "endOffset": 66}, {"referenceID": 8, "context": "Moreover, although we must defer to a longer journal version to present a formal treatment, with some mild additional conditions, Theorem 2 can naturally be extended to the case where the deocoder mean function is generalized to subsume non-linear, union-of-subspace models as commonly used in subspace clustering problems [9, 20].", "startOffset": 323, "endOffset": 330}, {"referenceID": 19, "context": "Moreover, although we must defer to a longer journal version to present a formal treatment, with some mild additional conditions, Theorem 2 can naturally be extended to the case where the deocoder mean function is generalized to subsume non-linear, union-of-subspace models as commonly used in subspace clustering problems [9, 20].", "startOffset": 323, "endOffset": 330}, {"referenceID": 23, "context": "7 Based on details of the proof of Theorem 2, it can be shown that, excluding small-order terms A more rudimentary form of this smoothing has been observed in much simpler empirical Bayesian models derived using Fenchel duality [24].", "startOffset": 228, "endOffset": 232}, {"referenceID": 11, "context": "Of course an analogous issue exists with generative adversarial networks (GAN) as well, a popular competing deep generative model composed of a generator network analogous to the VAE decoder, and a discriminator network that replaces the VAE encoder in a loose sense [12].", "startOffset": 267, "endOffset": 271}, {"referenceID": 3, "context": "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].", "startOffset": 231, "endOffset": 245}, {"referenceID": 7, "context": "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].", "startOffset": 231, "endOffset": 245}, {"referenceID": 14, "context": "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].", "startOffset": 231, "endOffset": 245}, {"referenceID": 23, "context": "We demonstrate this VAE capability here for the first time across an array of manifold dimensions and corruption percentages, recreating a nonlinear version of what are commonly termed phase transition plots in the RPCA literature [4, 8, 15, 24].", "startOffset": 231, "endOffset": 245}, {"referenceID": 18, "context": "Data Generation: First we draw n low-dimensional samples z \u2208 R from N (z; 0, I) and pass them through a 3-layer network with ReLU activations [19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "The layer sizes are 1000, 2000, and d respectively, each created using the initialization procedure from [13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "The encoder covariance shares the first two layers, and also has an exponential layer appended at the output to produce only positive values, consistent with the design in [16].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.", "startOffset": 114, "endOffset": 128}, {"referenceID": 7, "context": "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.", "startOffset": 114, "endOffset": 128}, {"referenceID": 14, "context": "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.", "startOffset": 114, "endOffset": 128}, {"referenceID": 23, "context": "This confirms Hypothesis (i) and suggests that VAEs are a viable candidate for replacing existing RPCA algorithms [4, 8, 15, 24] in regimes where a single linear subspace is an inadequate signal representation.", "startOffset": 114, "endOffset": 128}, {"referenceID": 16, "context": "We train analogous AE and VAE models with nonlinear decoder mean networks on the MNIST dataset of handwritten digit images [17] as \u03ba is varied.", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Also, \u03bcz and \u03a3z share the first three hidden layers, but have separate output layers, similar to [16].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "Outliers are added to MNIST handwritten digit data [17] by randomly replacing from 5% to 50% of the pixels with a value uniformly sampled from [0, 255] to createX .", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Note that RPCA is commonly used for unsupervised cleaning of this type of data [9], and MNIST is known to have significant low-rank structure [18] as shown in Figure 4(a).", "startOffset": 79, "endOffset": 82}, {"referenceID": 17, "context": "Note that RPCA is commonly used for unsupervised cleaning of this type of data [9], and MNIST is known to have significant low-rank structure [18] as shown in Figure 4(a).", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "The first inequality stems from Hadamard\u2019s inequality [10] applied to", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds obscured by gross corruptions. However, this previously unexplored feature comes with the cost of potential model collapse to a degenerate distribution that may be less suitable as the basis for generating new samples.", "creator": "LaTeX with hyperref package"}}}