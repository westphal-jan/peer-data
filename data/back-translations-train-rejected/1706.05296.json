{"id": "1706.05296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning", "abstract": "We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the \"lazy agent\" problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.", "histories": [["v1", "Fri, 16 Jun 2017 14:47:21 GMT  (8473kb,D)", "http://arxiv.org/abs/1706.05296v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peter sunehag", "guy lever", "audrunas gruslys", "wojciech marian czarnecki", "vinicius zambaldi", "max jaderberg", "marc lanctot", "nicolas sonnerat", "joel z leibo", "karl tuyls", "thore graepel"], "accepted": false, "id": "1706.05296"}, "pdf": {"name": "1706.05296.pdf", "metadata": {"source": "CRF", "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning", "authors": ["Peter Sunehag"], "emails": ["sunehag@google.com", "guylever@google.com", "audrunas@google.com", "lejlot@google.com", "vzambaldi@google.com", "jaderberg@google.com", "lanctot@google.com", "sonnerat@google.com", "jzl@google.com", "karltuyls@google.com", "thore@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "1.1 Other Related Work", "text": "Schneider et al. (1999) consider the optimization of the sum of individual reward functions by optimizing the local composition of individual value functions learned from them. Russell and Zimdars (2003) summarize the Q functions of independent learning agents with individual rewards before greedily making the global action selection to optimize the overall reward. Our approach works only with a team reward and learns value substitution autonomously from experience, and it differs in a similar way from the approach with coordinating graphs (Guestrin et al., 2002) and the Max-plus algorithm (Kuyer et al., 2008; van der Pol and Oliehoek, 2016). Other work dealing with team rewards in cooperative constellations is based on differential rewards (Tumer and Wolpert, 2004), which measure the impact of an agent's action on the overall reward of the system. For example, imagine a football team using the number of goals as a training signal."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reinforcement Learning", "text": "We recall some key concepts of the RL setting (Sutton and Barto, 1998), an agent-environment framework (Russell and Norvig, 2010) in which an agent interacts sequentially with the environment through a sequence of schedules, t = 1, 2, 3,., by executing actions and observing and receiving rewards, aiming to maximize cumulative reward, typically modelled as a Markov decision-making process (e.g. Puterman, 1994) defined by a tuple < S, T1, T, R > covering the state space S, action space A, a (possibly stochastic) reward function R: S \u00d7 S \u2192 R start distribution T: P (S) and transition function T: S A \u00b7 P (S), where P (X) denotes probability distributions over the set X."}, {"heading": "2.3 Multi-Agent Reinforcement Learning", "text": "We consider problems where observations and actions are distributed among d-agents and are represented as d-dimensional tuples of primitive observations in O and actions in A. As in the standard MARL, the underlying environment is modeled as a Markov game, in which actions are selected and performed simultaneously, and new observations are simultaneously perceived as the result of a transition to a new state (Littman, 1994, 2001; Hu and Wellman, 2003; Busoniu et al., 2008). Although agents have individual observations and are responsible for individual actions, each actor receives only the common reward, and we seek to optimize Rt as defined above. This is consistent with the Dec-POMDP framework (Oliehoek et al., 2008; Oliehoek and Amato, 2016). If we designate h-points: = (h1, h2,..., hd) a swab of agent history, is a common policy in general terms a map of action: we write Hd-2 (in particular, HP-h)."}, {"heading": "3 A Deep-RL Architecture for Coop-MARL", "text": "Building on purely independent DQN-style agents (see Figure 1), we add enhancements to overcome the identified problems with the MARL problem. Our main contribution to value substitution is made by the network in Figure 2.The main assumption we make and exploit is that the common agency value function for the system can be additively broken down into value functions. (h1, h2,..., a2, a2,..., ad). (h i, ai), where the Q function depends only on the local observations of the respective agent. We learn Q-i by backpropagating gradients from the Q-Learning rule, i.e. Q-i is implicitly learned rather than by reward-specific agents i, and we do not impose any limitations that the Q-value functions are for a specific reward."}, {"heading": "4 Experiments", "text": "We introduce a series of two-player domains and experimentally evaluate the introduced value substitution agents with varying degrees of improvement and evaluate each addition in a logical sequence. We use two central agents as baselines, one of which is reintroduced here on the basis of learned value substitution, and a single agent who learns directly from the common reward signal. We perform this series of experiments in the same form of two-dimensional labyrinth environments used by Leibo et al. (2017), but with different tasks that have more demanding coordination needs. Agents have a small 3 x 5 x 5 cm observation window, the first dimension being an RGB channel, the second and third dimensions being the labyrinth dimensions, and each actor sees a field 2 squares and 4 squares on both sides, see Figure 1 and 2. The simple graphs of our domains help with running speed, while, especially due to their multi-observer character, still represents a very small element of divisibility and observation window in this field."}, {"heading": "4.1 Agents", "text": "Our agent learning algorithm is based on DQN (Mnih et al., 2015) and includes its signature techniques of experience reproduction and target networks, which are augmented with an LSTM value network as in Hausknecht and Stone (2015) (to alleviate severe partial observation), learning with shortened back propagation by time, multi-stage updates with forward-facing permission tracks (Harb and Precup, 2016) (which helps propagate learning back through longer sequences), and the dueling architecture (Wang et al., 2016) (which accelerates learning by generalizing in the action space). Since observations are viewed from a local perspective, we do not benefit from revolutionary networks, but use a fully connected linear layer to process the observations. Our network architectures process input first with a fully connected linear layer with 32 hidden units followed by a linear layer followed by a U layer followed by a STU layer followed by a STY followed by a STY followed by a STY followed by a STY layer."}, {"heading": "4.2 Environments", "text": "We use 2D networking worlds with the same basic functions as Leibo et al. (2017), but with other tasks such as Switch, Fetch and Checkers. We have observations of byte values of size 3 \u00d7 5 \u00d7 5 (RGB), which represent a window depending on the position and orientation of the player, with no effect on our games, except for illuminating a row or column of squares. The actions are: Step forward, Step to the right, Rotate to the right, Use of bars and Stand to the right."}, {"heading": "4.3 Results", "text": "In fact, it is the case that you will be able to put yourself at the top without being able to do what you want to do in order to do it."}, {"heading": "5 Conclusions", "text": "We have found that the two na\u00efve approaches - individual agents who learn directly from team reward and fully centralized agents - offer unsatisfactory solutions, as previous literature has found in simpler environments, while our value decomposition networks do not suffer from the same problems and perform much better on a range of more complex tasks. In addition, the approach can be well combined with weight distribution and information channels, resulting in agents that consistently optimally solve our new benchmark challenges. Value decomposition networks are a step toward automatic decomposition of complex learning problems into local, easier-to-learn sub-problems. In future work, we will investigate the scaling of value decomposition as teams grow, making individual learners even more confusing with team reward (they see the rewards largely from actions by other agents), and centralized learners even more impractical."}, {"heading": "Appendix A: Plots", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix B: Diagrams", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Analyzing and visualizing multiagent rewards in dynamic and stochastic environments", "author": ["A.K. Agogino", "K. Tumer"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Agogino and Tumer.,? \\Q2008\\E", "shortCiteRegEx": "Agogino and Tumer.", "year": 2008}, {"title": "Social reward shaping in the prisoner\u2019s dilemma", "author": ["M. Babes", "E.M. de Cote", "M.L. Littman"], "venue": "In 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "Babes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Babes et al\\.", "year": 2008}, {"title": "The complexity of decentralized control of Markov Decision Processes", "author": ["D.S. Bernstein", "S. Zilberstein", "N. Immerman"], "venue": "Proceedings of the 16th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "Bernstein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2000}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["L. Busoniu", "R. Babuska", "B.D. Schutter"], "venue": "IEEE Transactions of Systems, Man, and Cybernetics Part C: Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. Claus", "C. Boutilier"], "venue": "In Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference,", "citeRegEx": "Claus and Boutilier.,? \\Q1998\\E", "shortCiteRegEx": "Claus and Boutilier.", "year": 1998}, {"title": "Local approximation of difference evaluation functions", "author": ["M. Colby", "T. Duchow-Pressley", "J.J. Chung", "K. Tumer"], "venue": "In Proceedings of the Fifteenth International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Colby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Colby et al\\.", "year": 2016}, {"title": "Potential-based difference rewards for multiagent reinforcement learning", "author": ["S. Devlin", "L. Yliniemi", "D. Kudenko", "K. Tumer"], "venue": "In Proceedings of the Thirteenth International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Potential-based reward shaping for finite horizon online POMDP planning", "author": ["A. Eck", "L. Soh", "S. Devlin", "D. Kudenko"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Eck et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eck et al\\.", "year": 2016}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["J.N. Foerster", "Y.M. Assael", "N. de Freitas", "S. Whiteson"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Coordinated reinforcement learning", "author": ["C. Guestrin", "M.G. Lagoudakis", "R. Parr"], "venue": "In Proceedings of the Nineteenth International Conference on Machine Learning,", "citeRegEx": "Guestrin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2002}, {"title": "Investigating recurrence and eligibility traces in deep Q-networks", "author": ["J. Harb", "D. Precup"], "venue": "In Deep Reinforcement Learning Workshop,", "citeRegEx": "Harb and Precup.,? \\Q2016\\E", "shortCiteRegEx": "Harb and Precup.", "year": 2016}, {"title": "Cooperation and Communication in Multiagent Deep Reinforcement Learning", "author": ["M.J. Hausknecht"], "venue": "PhD thesis, The University of Texas", "citeRegEx": "Hausknecht.,? \\Q2016\\E", "shortCiteRegEx": "Hausknecht.", "year": 2016}, {"title": "Deep recurrent Q-learning for partially observable MDPs", "author": ["M.J. Hausknecht", "P. Stone"], "venue": "CoRR, abs/1507.06527,", "citeRegEx": "Hausknecht and Stone.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Combining reward shaping and hierarchies for scaling to large multiagent systems", "author": ["C. HolmesParker", "A. Agogino", "K. Tumer"], "venue": "Knowledge Engineering Review,", "citeRegEx": "HolmesParker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "HolmesParker et al\\.", "year": 2016}, {"title": "Nash q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hu and Wellman.,? \\Q2003\\E", "shortCiteRegEx": "Hu and Wellman.", "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Multiagent reinforcement learning for urban traffic control using coordination graphs. In Machine Learning and Knowledge Discovery in Databases", "author": ["L. Kuyer", "S. Whiteson", "B. Bakker", "N.A. Vlassis"], "venue": "European Conference,", "citeRegEx": "Kuyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kuyer et al\\.", "year": 2008}, {"title": "The world of independent learners is not Markovian", "author": ["G.J. Laurent", "L. Matignon", "N.L. Fort-Piat"], "venue": "Int. J. Know.-Based Intell. Eng. Syst.,", "citeRegEx": "Laurent et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2011}, {"title": "Multi-agent Reinforcement Learning in Sequential Social Dilemmas", "author": ["J.Z. Leibo", "V. Zambaldi", "M. Lanctot", "J. Marecki", "T. Graepel"], "venue": "In Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "Leibo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Leibo et al\\.", "year": 2017}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "In Machine Learning, Proceedings of the Eleventh International Conference,", "citeRegEx": "Littman.,? \\Q1994\\E", "shortCiteRegEx": "Littman.", "year": 1994}, {"title": "Friend-or-foe q-learning in general-sum games", "author": ["M.L. Littman"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning (ICML", "citeRegEx": "Littman.,? \\Q2001\\E", "shortCiteRegEx": "Littman.", "year": 2001}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S.J. Russell"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning (ICML", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "A Concise Introduction to Decentralized POMDPs", "author": ["F.A. Oliehoek", "C. Amato"], "venue": "SpringerBriefs in Intelligent Systems", "citeRegEx": "Oliehoek and Amato.,? \\Q2016\\E", "shortCiteRegEx": "Oliehoek and Amato.", "year": 2016}, {"title": "Optimal and approximate q-value functions for decentralized pomdps", "author": ["F.A. Oliehoek", "M.T.J. Spaan", "N.A. Vlassis"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Panait and Luke.,? \\Q2005\\E", "shortCiteRegEx": "Panait and Luke.", "year": 2005}, {"title": "Modeling difference rewards for multiagent learning (extended abstract)", "author": ["S. Proper", "K. Tumer"], "venue": "In Proceedings of the Eleventh International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Proper and Tumer.,? \\Q2012\\E", "shortCiteRegEx": "Proper and Tumer.", "year": 2012}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig.,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2010}, {"title": "Q-decomposition for reinforcement learning agents", "author": ["S.J. Russell", "A. Zimdars"], "venue": "In Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "Russell and Zimdars.,? \\Q2003\\E", "shortCiteRegEx": "Russell and Zimdars.", "year": 2003}, {"title": "Distributed value functions", "author": ["J.G. Schneider", "W. Wong", "A.W. Moore", "M.A. Riedmiller"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning (ICML", "citeRegEx": "Schneider et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 1999}, {"title": "Learning multiagent communication with backpropagation", "author": ["S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "CoRR, abs/1605.07736,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Reinforcement Learning", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri.", "year": 2010}, {"title": "A survey of collectives", "author": ["K. Tumer", "D. Wolpert"], "venue": "Collectives and the Design of Complex Systems,", "citeRegEx": "Tumer and Wolpert.,? \\Q2004\\E", "shortCiteRegEx": "Tumer and Wolpert.", "year": 2004}, {"title": "Multiagent learning: Basics, challenges, and prospects", "author": ["K. Tuyls", "G. Weiss"], "venue": "AI Magazine,", "citeRegEx": "Tuyls and Weiss.,? \\Q2012\\E", "shortCiteRegEx": "Tuyls and Weiss.", "year": 2012}, {"title": "Coordinated deep reinforcement learners for traffic light control", "author": ["E. van der Pol", "F.A. Oliehoek"], "venue": "NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems,", "citeRegEx": "Pol and Oliehoek.,? \\Q2016\\E", "shortCiteRegEx": "Pol and Oliehoek.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "T. Schaul", "M. Hessel", "H. van Hasselt", "M. Lanctot", "N. de Freitas"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "We consider the cooperative multi-agent reinforcement learning (MARL) problem (Panait and Luke, 2005; Busoniu et al., 2008; Tuyls and Weiss, 2012), in which a system of several learning agents must jointly optimize a single reward signal \u2013 the team reward \u2013 accumulated over time.", "startOffset": 78, "endOffset": 146}, {"referenceID": 3, "context": "We consider the cooperative multi-agent reinforcement learning (MARL) problem (Panait and Luke, 2005; Busoniu et al., 2008; Tuyls and Weiss, 2012), in which a system of several learning agents must jointly optimize a single reward signal \u2013 the team reward \u2013 accumulated over time.", "startOffset": 78, "endOffset": 146}, {"referenceID": 36, "context": "We consider the cooperative multi-agent reinforcement learning (MARL) problem (Panait and Luke, 2005; Busoniu et al., 2008; Tuyls and Weiss, 2012), in which a system of several learning agents must jointly optimize a single reward signal \u2013 the team reward \u2013 accumulated over time.", "startOffset": 78, "endOffset": 146}, {"referenceID": 17, "context": "In general each agent is then faced with a non-stationary learning problem because the dynamics of its environment effectively changes as teammates change their behaviours through learning (Laurent et al., 2011).", "startOffset": 189, "endOffset": 211}, {"referenceID": 2, "context": "Because of this inability to explain its own observed rewards naive independent RL is often unsuccessful: for example Claus and Boutilier (1998) show that independent Q-learners cannot distinguish teammates\u2019 exploration from stochasticity in the environment, and fail to solve even an apparently trivial, 2-agent, stateless, 3 \u00d7 3-action problem and the general Dec-POMDP problem is known to be intractable (Bernstein et al., 2000; Oliehoek and Amato, 2016).", "startOffset": 407, "endOffset": 457}, {"referenceID": 24, "context": "Because of this inability to explain its own observed rewards naive independent RL is often unsuccessful: for example Claus and Boutilier (1998) show that independent Q-learners cannot distinguish teammates\u2019 exploration from stochasticity in the environment, and fail to solve even an apparently trivial, 2-agent, stateless, 3 \u00d7 3-action problem and the general Dec-POMDP problem is known to be intractable (Bernstein et al., 2000; Oliehoek and Amato, 2016).", "startOffset": 407, "endOffset": 457}, {"referenceID": 23, "context": "the true objective (Ng et al., 1999; Devlin et al., 2014; Eck et al., 2016).", "startOffset": 19, "endOffset": 75}, {"referenceID": 6, "context": "the true objective (Ng et al., 1999; Devlin et al., 2014; Eck et al., 2016).", "startOffset": 19, "endOffset": 75}, {"referenceID": 7, "context": "the true objective (Ng et al., 1999; Devlin et al., 2014; Eck et al., 2016).", "startOffset": 19, "endOffset": 75}, {"referenceID": 11, "context": "Further, in the context of the introduced agent, we evaluate weight sharing, role information and information channels as additional enhancements that have recently been reported to improve sample complexity and memory requirements (Hausknecht, 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 232, "endOffset": 298}, {"referenceID": 8, "context": "Further, in the context of the introduced agent, we evaluate weight sharing, role information and information channels as additional enhancements that have recently been reported to improve sample complexity and memory requirements (Hausknecht, 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 232, "endOffset": 298}, {"referenceID": 32, "context": "Further, in the context of the introduced agent, we evaluate weight sharing, role information and information channels as additional enhancements that have recently been reported to improve sample complexity and memory requirements (Hausknecht, 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 232, "endOffset": 298}, {"referenceID": 3, "context": "Because of this inability to explain its own observed rewards naive independent RL is often unsuccessful: for example Claus and Boutilier (1998) show that independent Q-learners cannot distinguish teammates\u2019 exploration from stochasticity in the environment, and fail to solve even an apparently trivial, 2-agent, stateless, 3 \u00d7 3-action problem and the general Dec-POMDP problem is known to be intractable (Bernstein et al.", "startOffset": 118, "endOffset": 145}, {"referenceID": 2, "context": "Because of this inability to explain its own observed rewards naive independent RL is often unsuccessful: for example Claus and Boutilier (1998) show that independent Q-learners cannot distinguish teammates\u2019 exploration from stochasticity in the environment, and fail to solve even an apparently trivial, 2-agent, stateless, 3 \u00d7 3-action problem and the general Dec-POMDP problem is known to be intractable (Bernstein et al., 2000; Oliehoek and Amato, 2016). Though we here focus on 2 player coordination, we note that the problems with individual learners and centralized approaches just gets worse with more agents since then, most rewards do not relate to the individual agent and the action space grows exponentially for the fully centralized approach. One approach to improving the performance of independent learners is to design individual reward functions, more directly related to individual agent observations. However, even in the single-agent case, reward shaping is difficult and only a small class of shaped reward functions are guaranteed to preserve optimality w.r.t. the true objective (Ng et al., 1999; Devlin et al., 2014; Eck et al., 2016). In this paper we aim for more general autonomous solutions, in which the decomposition of the team value function is learned. We introduce a novel learned additive value-decomposition approach over individual agents. Implicitly, the value decomposition network aims to learn an optimal linear value decomposition from the team reward signal, by back-propagating the total Q gradient through deep neural networks representing the individual component value functions. This additive value decomposition is specifically motivated by avoiding the spurious reward signals that emerge in purely independent learners.The implicit value function learned by each agent depends only on local observations, and so is more easily learned. Our solution also ameliorates the coordination problem of independent learning highlighted in Claus and Boutilier (1998) because it effectively learns in a centralised fashion at training time, while agents can be deployed individually.", "startOffset": 408, "endOffset": 2009}, {"referenceID": 9, "context": "Our approach works with only a team reward, and learns the value-decomposition autonomously from experience, and it similarly differs from the approach with coordination graphs (Guestrin et al., 2002) and the max-plus algorithm (Kuyer et al.", "startOffset": 177, "endOffset": 200}, {"referenceID": 16, "context": ", 2002) and the max-plus algorithm (Kuyer et al., 2008; van der Pol and Oliehoek, 2016).", "startOffset": 35, "endOffset": 87}, {"referenceID": 35, "context": "Other work addressing team rewards in cooperative settings is based on difference rewards (Tumer and Wolpert, 2004), measuring the impact of an agent\u2019s action on the full system reward.", "startOffset": 90, "endOffset": 115}, {"referenceID": 28, "context": "Russell and Zimdars (2003) sums the Q-functions of independent learning agents with individual rewards, before making the global action selection greedily to optimize for total reward.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "When the worse player takes a shot the outcome is on average much worse, and the weaker player learns to avoid taking shots (Hausknecht, 2016).", "startOffset": 124, "endOffset": 142}, {"referenceID": 5, "context": "high learnability), but can be impractical as it requires knowledge about the system state (Colby et al., 2016; Agogino and Tumer, 2008; Proper and Tumer, 2012).", "startOffset": 91, "endOffset": 160}, {"referenceID": 0, "context": "high learnability), but can be impractical as it requires knowledge about the system state (Colby et al., 2016; Agogino and Tumer, 2008; Proper and Tumer, 2012).", "startOffset": 91, "endOffset": 160}, {"referenceID": 27, "context": "high learnability), but can be impractical as it requires knowledge about the system state (Colby et al., 2016; Agogino and Tumer, 2008; Proper and Tumer, 2012).", "startOffset": 91, "endOffset": 160}, {"referenceID": 0, "context": ", 2016; Agogino and Tumer, 2008; Proper and Tumer, 2012). Other approaches can be found in Devlin et al. (2014); HolmesParker et al.", "startOffset": 8, "endOffset": 112}, {"referenceID": 0, "context": ", 2016; Agogino and Tumer, 2008; Proper and Tumer, 2012). Other approaches can be found in Devlin et al. (2014); HolmesParker et al. (2016); Babes et al.", "startOffset": 8, "endOffset": 140}, {"referenceID": 0, "context": ", 2016; Agogino and Tumer, 2008; Proper and Tumer, 2012). Other approaches can be found in Devlin et al. (2014); HolmesParker et al. (2016); Babes et al. (2008).", "startOffset": 8, "endOffset": 161}, {"referenceID": 33, "context": "We recall some key concepts of the RL setting (Sutton and Barto, 1998), an agent-environment framework (Russell and Norvig, 2010) in which an agent sequentially interacts with the environment over a sequence of timesteps, t = 1, 2, 3, .", "startOffset": 46, "endOffset": 70}, {"referenceID": 29, "context": "We recall some key concepts of the RL setting (Sutton and Barto, 1998), an agent-environment framework (Russell and Norvig, 2010) in which an agent sequentially interacts with the environment over a sequence of timesteps, t = 1, 2, 3, .", "startOffset": 103, "endOffset": 129}, {"referenceID": 22, "context": "Although we concentrate on DQN-based agent architectures, our techniques are also applicable to policy gradient methods such as A3C (Mnih et al., 2016).", "startOffset": 132, "endOffset": 151}, {"referenceID": 18, "context": "Our study focuses on deep architectures for the value function similar to those used by Mnih et al. (2015), and our approach incorporates the key techniques of target networks and experience replay employed there, making the update into a stochastic gradient step.", "startOffset": 88, "endOffset": 107}, {"referenceID": 10, "context": "Since we consider partially observed environments our Q-functions are defined over agent observation histories, Q(ht, at), and we incorporate a recurrent network similarly to Hausknecht and Stone (2015). To speed up learning we add the dueling architecture of Wang et al.", "startOffset": 175, "endOffset": 203}, {"referenceID": 10, "context": "Since we consider partially observed environments our Q-functions are defined over agent observation histories, Q(ht, at), and we incorporate a recurrent network similarly to Hausknecht and Stone (2015). To speed up learning we add the dueling architecture of Wang et al. (2016) that represent Q using a value and an advantage function, including multi-step updates with a forward view eligibility trace (e.", "startOffset": 175, "endOffset": 279}, {"referenceID": 14, "context": "in MARL, the underlying environment is modeled as a Markov game where actions are chosen and executed simultaneously, and new observations are perceived simultaneously as a result of a transition to a new state (Littman, 1994, 2001; Hu and Wellman, 2003; Busoniu et al., 2008).", "startOffset": 211, "endOffset": 276}, {"referenceID": 3, "context": "in MARL, the underlying environment is modeled as a Markov game where actions are chosen and executed simultaneously, and new observations are perceived simultaneously as a result of a transition to a new state (Littman, 1994, 2001; Hu and Wellman, 2003; Busoniu et al., 2008).", "startOffset": 211, "endOffset": 276}, {"referenceID": 25, "context": "This is consistent with the Dec-POMDP framework (Oliehoek et al., 2008; Oliehoek and Amato, 2016).", "startOffset": 48, "endOffset": 97}, {"referenceID": 24, "context": "This is consistent with the Dec-POMDP framework (Oliehoek et al., 2008; Oliehoek and Amato, 2016).", "startOffset": 48, "endOffset": 97}, {"referenceID": 18, "context": "The simple graphics of our domains helps with running speed while, especially due to their multi-agent nature and severe partial observability and aliasing (very small observation window combined with map symmetries), they still pose a serious challenge and is comparable to the state-of-the-art in multi-agent reinforcement learning (Leibo et al., 2017), which exceeds what is common in this area (Tuyls and Weiss, 2012).", "startOffset": 334, "endOffset": 354}, {"referenceID": 36, "context": ", 2017), which exceeds what is common in this area (Tuyls and Weiss, 2012).", "startOffset": 51, "endOffset": 74}, {"referenceID": 18, "context": "We perform this set of experiments on the same form of two dimensional maze environments used by Leibo et al. (2017), but with different tasks featuring more challenging coordination needs.", "startOffset": 97, "endOffset": 117}, {"referenceID": 21, "context": "Our agent\u2019s learning algorithm is based on DQN (Mnih et al., 2015) and includes its signature techniques of experience replay and target networks, enhanced with an LSTM value-network as in Hausknecht and Stone (2015) (to alleviate severe partial observability), learning with truncated back-propagation through time, multi-step updates with forward view eligibility traces (Harb and Precup, 2016) (which helps propagating learning back through longer sequences) and the dueling architecture (Wang et al.", "startOffset": 47, "endOffset": 66}, {"referenceID": 10, "context": ", 2015) and includes its signature techniques of experience replay and target networks, enhanced with an LSTM value-network as in Hausknecht and Stone (2015) (to alleviate severe partial observability), learning with truncated back-propagation through time, multi-step updates with forward view eligibility traces (Harb and Precup, 2016) (which helps propagating learning back through longer sequences) and the dueling architecture (Wang et al.", "startOffset": 314, "endOffset": 337}, {"referenceID": 38, "context": ", 2015) and includes its signature techniques of experience replay and target networks, enhanced with an LSTM value-network as in Hausknecht and Stone (2015) (to alleviate severe partial observability), learning with truncated back-propagation through time, multi-step updates with forward view eligibility traces (Harb and Precup, 2016) (which helps propagating learning back through longer sequences) and the dueling architecture (Wang et al., 2016) (which speeds up learning by generalizing across the action space).", "startOffset": 432, "endOffset": 451}, {"referenceID": 15, "context": "The Adam (Kingma and Ba, 2014) learning rate scheme initialized with 0.", "startOffset": 9, "endOffset": 30}, {"referenceID": 10, "context": ", 2015) and includes its signature techniques of experience replay and target networks, enhanced with an LSTM value-network as in Hausknecht and Stone (2015) (to alleviate severe partial observability), learning with truncated back-propagation through time, multi-step updates with forward view eligibility traces (Harb and Precup, 2016) (which helps propagating learning back through longer sequences) and the dueling architecture (Wang et al.", "startOffset": 130, "endOffset": 158}, {"referenceID": 10, "context": ", 2015) and includes its signature techniques of experience replay and target networks, enhanced with an LSTM value-network as in Hausknecht and Stone (2015) (to alleviate severe partial observability), learning with truncated back-propagation through time, multi-step updates with forward view eligibility traces (Harb and Precup, 2016) (which helps propagating learning back through longer sequences) and the dueling architecture (Wang et al., 2016) (which speeds up learning by generalizing across the action space). Since observations are from a local perspective, we do not benefit from convolutional networks, but use a fully connected linear layer to process the observations. Our network architectures first process the input using a fully connected linear layer with 32 hidden units followed by a ReLU layer, and then an LSTM, with 32 hidden units followed by a ReLU layer, and finally a linear dueling layer, with 32 units. This produces a value function V (s) and advantage function A(s, a), which are combined to compute a Q-function Q(s, a) = V (s) +A(s, a) as described in Wang et al. (2016). Layers of 32 units are sufficiently expressive for these tasks with limited observation windows.", "startOffset": 315, "endOffset": 1106}, {"referenceID": 18, "context": "We use 2D grid worlds with the same basic functioning as Leibo et al. (2017), but with different tasks we call Switch, Fetch and Checkers.", "startOffset": 57, "endOffset": 77}], "year": 2017, "abstractText": "We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the \u201clazy agent\u201d problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.", "creator": "LaTeX with hyperref package"}}}