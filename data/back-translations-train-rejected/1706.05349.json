{"id": "1706.05349", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "Active learning in annotating micro-blogs dealing with e-reputation", "abstract": "Elections unleash strong political views on Twitter, but what do people really think about politics? Opinion and trend mining on micro blogs dealing with politics has recently attracted researchers in several fields including Information Retrieval and Machine Learning (ML). Since the performance of ML and Natural Language Processing (NLP) approaches are limited by the amount and quality of data available, one promising alternative for some tasks is the automatic propagation of expert annotations. This paper intends to develop a so-called active learning process for automatically annotating French language tweets that deal with the image (i.e., representation, web reputation) of politicians. Our main focus is on the methodology followed to build an original annotated dataset expressing opinion from two French politicians over time. We therefore review state of the art NLP-based ML algorithms to automatically annotate tweets using a manual initiation step as bootstrap. This paper focuses on key issues about active learning while building a large annotated data set from noise. This will be introduced by human annotators, abundance of data and the label distribution across data and entities. In turn, we show that Twitter characteristics such as the author's name or hashtags can be considered as the bearing point to not only improve automatic systems for Opinion Mining (OM) and Topic Classification but also to reduce noise in human annotations. However, a later thorough analysis shows that reducing noise might induce the loss of crucial information.", "histories": [["v1", "Fri, 16 Jun 2017 17:07:24 GMT  (1048kb,D)", "https://arxiv.org/abs/1706.05349v1", null], ["v2", "Mon, 26 Jun 2017 18:09:57 GMT  (1049kb,D)", "http://arxiv.org/abs/1706.05349v2", null], ["v3", "Mon, 3 Jul 2017 13:55:08 GMT  (1254kb,D)", "http://arxiv.org/abs/1706.05349v3", "Journal of Interdisciplinary Methodologies and Issues in Science - Vol 3 - Contextualisation digitale - 2017"], ["v4", "Mon, 25 Sep 2017 21:58:04 GMT  (1254kb,D)", "http://arxiv.org/abs/1706.05349v4", "Journal of Interdisciplinary Methodologies and Issues in Science - Vol 3 - Contextualisation digitale - 2017"]], "reviews": [], "SUBJECTS": "cs.SI cs.CL", "authors": ["jean-val\\`ere cossu", "alejandro molina-villegas", "mariana tello-signoret"], "accepted": false, "id": "1706.05349"}, "pdf": {"name": "1706.05349.pdf", "metadata": {"source": "CRF", "title": "Active learning in annotating micro-blogs dealing with e-reputation", "authors": ["Jean-Val\u00e8re Cossu", "Alejandro Molina-Villegas", "Mariana Tello-Signoret"], "emails": ["jvcossu@gmail.com"], "sections": [{"heading": null, "text": "This means that most people who have lived and worked in the United States for the past ten years have also come up with the idea of questioning their identity and identity in other countries of the world; this means that people in other countries of the world have come up with the idea of questioning their identity and identity; this means that people in other countries of the world have come up with the idea of questioning their identity and identity; this means that people in other countries of the world have come up with the idea of questioning their identity and identity; this means that people in other countries of the world have come up with the idea of questioning their identity and identity; this means that people in other countries of the world have come up with the idea; this means that people in other countries of the world have come up with the idea."}, {"heading": "II RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Tweets mining", "text": "Previous work on reputation monitoring in tweet collections and streams has been done to extract messages that require particular attention from a reputation manager, Amig\u00f3 et al. (2013). For example, recent posts on this topic on Twitter were made in the context of the Replab 1 and TASS 2 challenges, in which laboratory heads provide a framework for evaluating online reputation management systems on Twitter. Reputation polarity is very different from standard sentiment analysis, as both the author and facts and opinions need to be taken into account. The aim is to find out what impact information has on the reputation of a particular entity, regardless of whether the message contains an opinion or not (i.e. messages merely report factually incorrect governance decisions). In order to illustrate when ten people disagree with the mood of a given text, it is about whether what is acceptable or relevant to one person is the same for others, multilingual aspects, when dealing with the most important linguistic challenges, and the contextual challenges of classifying them."}, {"heading": "2.2 Data building", "text": "Crowd sourcing is an increasingly popular and collaborative approach to acquiring research results commented corpora with the idea of collecting comments from volunteers, which is an advantage over expert-based comments. Although conceiving such a dataset of training examples. http: / / www.limosine-project.eu / events / replab2013 2. http: / / www.sepln.org / workshops / tass / 2014 / tass2014.phpJ. of Interd. Method. and Issues in Science Open-access journal: http: / / jimis.episciences.org3 ISSN: 2430-3038, JIMIS, Creative Commons Volume: 3 - Year: 2017, DOI: 10.18713 / JIMIS-3-2has proved quite interesting challenge, Amig\u00f3 et al. (2013), Villena Rom\u00e1n et al. (2013), it is still expensive and relatively inaccurate."}, {"heading": "III CROWD-SOURCED ANNOTATION STAGE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Annotation platform for E-Reputation Analysis of tweets in French", "text": "To analyze the public image of French politicians on Twitter, we have designed an annotation platform where users receive tweets and are asked to first identify the opinion passage, then assign it to a polarity, and finally identify its specific aspect target. Our web architecture, presented in Figure 1, is based on three-level models that allow quick adjustment to any required annotation, since usually the source code of the top level needs to be changed. System demo can be tested at http: / / dev.termwatch.es / ~ molina / sentaatool / info / systeme _ description.html. Figure 2 shows the interface used during the annotation of tweets and their main components: 1. Tweet section allows selection, but no change. 2. Polarity buttons indicate the polarity of a selected passage and make the target text bar appear when pressed. 3. Targets section contains an editable target text bar, 1."}, {"heading": "3.2 Annotation design", "text": "The following 9J. of Interd. Method. and Issues in Science Open-access journal: http: / / jimis.episciences.org5 ISSN: 2430-3038, JIMIS, Creative Commons Volume: 3 - Year: 2017, DOI: 10.18713 / JIMIS-010917-3-2aspects were finally chosen to describe French politicians: attribute 5, evaluation, skills, ethics, instruction 6, communication, person, policy line, project, addition of the entity itself and the case that does not belong to this list. Furthermore, the aspects are broken down into sub-aspects such as surveys and support in the case of attributes, indicating the characteristics of the entity expressed in pools and support. All 23 sub-aspects have been prepared for this fine-grained description and reporting, with polarity levels ranging from very positive (considered very positive) to negative (used as very clear) for an opinion."}, {"heading": "3.3 First annotated dataset, descriptive Statistics", "text": "This dataset 7 consists of 11527 manual annotations expressing the views of two French politicians over time, 5286 annotations for Fran\u00e7ois Hollande (FH) and 6241 annotations for Nicolas Sarkozy (NS). The data has been commented by 20 academics from different fields, Table 1 provides some additional details. Interestingly, NLP researchers and industry people tend to focus on terms or N-grams with shorter annotations (in relation to selected passages), which are likely to follow algorithm schemes and keyword extraction for dashboards respectively. At the same time, engineers and political scientists tend to select larger portions of text. To deal with the subjectivity of annotations, we have allowed a tweet to be commented on no more than three times by different commentators."}, {"heading": "3.3.1 Opinions", "text": "For a reasonable analysis as observed in the literature for comparable annotation tasks, Carrillode Albornoz et al. (2014); Villena Rom\u00e1n et al. (2013) take into account only three polarity levels, grouping positive / very positive and negative / very negative opinions and ignoring ambiguous opinions. Throughout the dataset, opinions are negative with a slight difference between the two units; for example, 47% of opinions on NS are positive for 20% and 5. Survey results and comments 6. Call for Voting 7. The raw dataset is available there: http: / / mediamining.univ-lyon2.fr / velcin / imagiweb / dataset.htmlJ. of the Interd. Method and editions in Science Open-Access Journal: http: / / jimis.episciences.org6 ISSN: 2430-3038, Creative Commons Volume: 3 - Year: 2017, DOI Open-Access Journal: http: / jimisccien.Year-Commons-2012, IS930024.0.org.SN: DO.03%"}, {"heading": "3.3.2 Aspects", "text": "As a global class, the unity aspect dominates with 23%, followed by political line and ethics with 13% and 11% respectively. Interesting is the development of the frequency of each aspect according to time. Some aspects are much more dependent on time, such as arrangement and communication, which reach very high frequencies just before the election and then disappear again. Both candidates received positive ratings for the arrangement because this aspect is (seldom) dedicated to clear encouragement or warning before voting for a unit. On the contrary, for communication, FH achieved a better result than its competitor."}, {"heading": "3.3.3 Annotator bias and disagreements", "text": "In fact, it is the case that most of us are in a position to put ourselves in another world, in which they get lost in another world, in which they find themselves in another world, in which they find themselves in another world, in which they do not find themselves again, in which they do not find themselves again, in which they do not find themselves again, in which they do not find themselves again, in which they live themselves, in which they live themselves, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4.1 Data Diversity", "text": "Our goal is then to train machine learning methods with human behaviors in order to disseminate their knowledge and automatically tag upcoming data. An important thing before handling a large unannounced data set is to be sure of the reliability of the training. As reported in the literature, Artstein and Poesio (2008) and as we have just seen, human annotations of language traits and concepts are prone to human error. These errors must be taken into account in the model of the learning process, as it is known that the quality of manual annotations is critical when it comes to training automatic methods. We assume that the goal is not to create the most reliable data sets in the sense of a particular spectrospect, but to build a consistent database. The method and the problems in Science Open Access Journal: http: / / jimis.episciences.org8 ISSN: 2430-3038, JIMIS, Creative Commons Volume Volume Volume Volume J10.97- 2017: DOI-18217"}, {"heading": "4.2 Harmonization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Annotators-based decisions", "text": "It is still possible to estimate the difficulty of the task with intercommentary agreements such as Kappa, Kohen (1960); Cohn et al. (1994); Koehn and Knight (2003); Sabou et al. (2014), but as soon as inconsistencies were identified, what could be done? In our case, each tweet was commented on once or three times, and as we previously noted serious inconsistencies at the text level, we chose a majority-based rule system. For any commented content with divergent annotations, we chose, whenever possible, the human commentary that has a relative majority according to: Label Frequency > 1 / Number of Labels (1) J. of Interd. Method. and Issues in Science Open-access journal: http: / / jimis.episciences.org9 ISSN: 2430-3038, JIMIS, Creative Commons Volume: 3 - Year: 2017, DOI: 10.18713 / JIM90102-102-3-17"}, {"heading": "4.2.2 Profiles-based decisions", "text": "An important aspect of social networks is the ability for users to respond to each other and thus build their own network. As an additional feature, we can assume that a user belongs to a group or has the same opinion (or aspect) as the person to whom he replies or re-tweets. Furthermore, given the political dimensions of the data set, we assume that gossip sheets expressing dislike of a candidate cannot find anything positive in a single message. We then have to pay attention to these comments. For example, we can assume that users have more than 100 negative messages relating to a particular entity. We can hardly imagine that the next tweet is positive and even if it has been commented on as such, it can be withdrawn or re-validated. This process can be seen as smoothing out the user's point of view, even if we know that this is an assumption that is not always verified."}, {"heading": "4.2.3 Contents-based decisions", "text": "In particular, we first examined feelings that are carried by hashtags, e.g. # LesSocialos is always associated with negative opinions about FH, tweets that comment on this hashtag as positive should attract attention. Hashtags are used to mark groups and topics on Twitter; they can be divided into three types: - Topic hashtags that are used to comment on crude topics, e.g. # LeDebat (# TheDebate) # Karachi (case); - Sentiment hashtags, e.g. # Idiot (# Idiot), # Deception (# Disappointment), # LesSocialos (# Socialists - with bashing) and various stylish forms of umpitoyable, umpitres, umpopcorn (Bashing IMMP Party); - Sentiment-topic hashtags that capture both sentiment and target topic, e.g. # ViveHollande (# LongLiveHollande), # Sarkozy Taime (Sarkoid.You # Wejiot #.or24ot)."}, {"heading": "V SETTING-UP MACHINE LEARNING FRAMEWORK : ISSUES AND CHALLENGES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Machine Learning Committee-based correction", "text": "In contrast to, Dagan and Engelson (1995), we consider another committee-based validation, composed of several classifiers described above under very light supervision. Domain non-specialists review various random samples of system results to validate the process. Some studies have worked well in the first direction, such as Liere and Tadepalli (1997), where the authors have obtained 2 to 30-fold reductions in the amount of human annotations for text categorization. Following the rule-based corrections, we now resort in all remaining cases to multiple classifiers, which are used to \"comment\" on the training corpus itself. A large number of methods have already been explored to correct the bias of annotators. Having multiple annotations is a case, however, that we allow an important fact here that we do not consider annotations as a gold standard reference, and we can specifically challenge them if none of them are considered to be a genuine designation for the additional annotation systems that we assume has for several of them."}, {"heading": "5.1.1 Classifiers", "text": "For the purposes of this experiment, and following the background literature, Cossu et al. (2015) examined statistical NLP, Sparck Jones (1972); Salton and Buckley (1988). N-grams also constitute the Tweet discriminant wordbag (BOW) representation using normalized (tf) inverse term frequencies (tf-idf) and Gini criterion, Cossu et al. (2015); Torres-Moreno et al. (2012). The statistical BOW approach is also used to calculate the similarity of a given tweet to each BOW class and to evaluate tweets according to Jaccard index, cosinine distance, and the number of multiple classifiers (Poisson-based classifiers, Hidden Markov model) Cossu et al. (2013). We also proposed a kNN-based classification method that uses the same discrimination factor as in the test line from the 2nd BOW representative document onwards."}, {"heading": "5.1.2 Metrics", "text": "The absolute values from the confusion matrix are used to calculate common text mining metrics such as accuracy, which is easy to interpret, but can be easily deceived under unbalanced test sets. For example, a non-informative method that returns all tweets in the same class (in our case all \"NEGATIVE\") may be highly accurate. In addition, for each class we calculate an average F score based on Precision and Recall, typical for categorization tasks, which is calculated as follows: F _ Score = \u2211 c2 \u00d7 (Precisionc \u00d7 Recallc) Precisionc + Recallc Number of classes (2)"}, {"heading": "5.1.3 Datasets", "text": "We divided the corpus into two parts, sorted chronologically: Training (Tr) and Development (D). D was built over the last 3 months (approximately 800 unique content associated with each company), and this initial subset was expanded to include unlabeled tweets extracted from January 2012 to December 2014: - A first sentence concerning FH with 240k tweets (approximately 6700 tweets per month) J. of Interd. Method. and Issues in Science Open-Access Journal: http: / / jimis.episciences.org12 ISSN: 2430-3038, JIMIS, Creative Commons Volume: 3 - Year: 2017, DOI: 10.18713 / JIMIS-010917-3-2 - A second sentence concerning NS with 81k tweets (approximately 2500 tweets per month) This new data is used for the validation process and the experts need it to draw conclusions on a large scale."}, {"heading": "5.1.4 Integrating users information", "text": "For users affected by profile-based annotation corrections, we have considered smoothing out machine learning approaches (as summarized in Figure 5). We first inserted a class tag into the vocabulary of the tweets tested in the future (representing the main polarity to which they were associated by the classifiers in the BOW of their tweets), but that tag implies that the user does not change his mind. In order to prevent this bias and also to accept that people can change their minds without breaking the robustness of the BOW, we then added the user ID with the associated probabilities of the classes, Li et al. (2011). In this way, by looking at that user's past, we punish the contribution of the class without closing the door to further changing the user's opinion. As we are in an active process, it will automatically return over time to the premise that a user has only one opinion."}, {"heading": "5.2 Wrap-up", "text": "Even though we only have 17% additional raw comments regarding FH, it focuses on many more corrections in terms of opinions, while conversely the trend in terms of aspects is reversed. We can mainly explain this with the label distribution, since the positive aspects do not really exist with FH, but it reduces the complexity of the task. ML and content-based approaches have not done much to improve the annotation process for opinion recognition, while profile statistics seemed to play a key role. Furthermore, it is interesting to note that even after a committee statement, it was still impossible for NS to agree on a label for some messages that were eventually rejected. Finally, in many cases regarding aspects that neither the rules nor the committee were able to provide an additional arbitrator to provide a supplementary comment. J. J. J. of Interd Methods and Issues in Science Open Access Journal: http: / / jimis.cepises.org"}, {"heading": "5.3 Expansion, temporal propagation", "text": "Now that the training set has been corrected, we can use our classifiers to comment on a large number of untagged messages (as summarized in Figure 6), and the untagged examples can be used with uncontrolled or monitored learning methods to improve classification performance and correction of the tagged examples by applying the above rules to content and user level according to a principle of homogeneity. In addition, we have considered \"outliers\" that differ from the rest of the data, in our case in terms of match or content. Initially, we considered outliers as tweets where neither systems nor commentators agreed on the same label. These tweets are ignored due to lack of understanding. We have also excluded unique content without common words with other content and with the tagged sentence. A second interpretation of reliable outliers is to consider tweets for which each system has agreed to the same label by adding them to the Iterina, 2015 (al)."}, {"heading": "VI EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Evaluation data", "text": "We consider as a test data set a selection of 5200 tweets in 2013 (430 per month) for the NS and 3600 tweets (March and April 2013) for the FH. These selected tweets were automatically commented on using the workflow shown below and also manually checked by a political science expert according to the annotation guidelines (as summarised in Figure 8). Note that we divided the group for the NS unit into two parts: a first, in which the automatic annotator name was completely hidden (similar to raw tweets), and a second, in which the automatic annotator name was shown (validation / correction level if it was incorrect).The FH unit test set was validated according to this second scheme. Below, we compare the expert comment with the automatically generated hypotheses. The objective of this facility is twofold: First, we intend to evaluate the performance of the machine learning approach in an operational scenario. Second, we will not want to estimate the IS.IS.Second, as an IS.can be influenced by IS.Access."}, {"heading": "6.2 Results", "text": "As preliminary experiments, we first report in Table 5 the system performance for the classification tasks (polarity or aspect) of the two units studied on our test sets. To keep things simple, we report only on the performance of a cosine-based approach and the combination of all the machine learning techniques used during the annotation process. Although there is a significant improvement in the evaluation of the classification, the most important thing is that the combination of classifiers also appears robust enough to handle the wide variety of hypotheses. Then, regarding the fact that the annotator was able to see the automatic designation (or not for half of the NS tweets) when commenting on the tweets, the differences are not significant for the polarity classification (accuracy between.62 and.63 for the combination of classifiers). Although the task of annotating the tweet according to only one aspect is difficult, we can consider that the annotator was not validating the proposed aspect because it was not validated by validity."}, {"heading": "VII CONCLUSION AND PERSPECTIVES", "text": "This task is even more difficult when it comes to combining it with the specific aspects. In this paper, we have presented an approach to the annotation of a French political opinion dataset, from annotation design to machine learning experiments. First, we have shown that we can improve our dataset and achieve good classification performance, even though statistical methods do not require linguistic and domain-specific processing. This makes our approach easily applicable to other languages and datasets. Instead of dealing with more complex modeling, the experiments reported in this paper have shown that by incorporating additional Twitter features combined with light knowledge, this can provide robust support for improving both the annotation quality and the classification performance. We used methods that are known to remain simple, but also reported, in order to achieve results that are as good as those previously proposed with the most modern approaches to comparable topics, Cossu et Comparing Data Efficiency to those proposed by our experts in 2015."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Dr. Eric Sanjuan, Pr Marc El-Beze and the entire Imagiweb team in particular Dr Caroline Brun.J. of Interd. Method. and Issues in Science Open-access journal: http: / / jimis.episciences.org18 ISSN: 2430-3038, JIMIS, Creative Commons Volume: 3 - Year: 2017, DOI: 10.18713 / JIMIS-010917-3-2R\u00e9f\u00e9rences Amig\u00f3 E., De Albornoz J. C., Chugur I., Corujo A., Gonzalo J., Mart\u00edn T., Meij E., De Rijke M., Spina D. (2013).Overview of replab 2013: Evaluating reputation monitoring systems Accvaluation."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "R\u00e9sum\u00e9 Elections unleash strong political views on Twitter, but what do people really think about politics ? Opinion and trend mining on micro blogs dealing with politics has recently attracted researchers in several fields including Information Retrieval and Machine Learning (ML). Since the performance of ML and Natural Language Processing (NLP) approaches are limited by the amount and quality of data available, one promising alternative for some tasks is the automatic propagation of expert annotations. This paper intends to develop a so-called active learning process for automatically annotating French language tweets that deal with the image (i.e., representation, web reputation) of politicians. Our main focus is on the methodology followed to build an original annotated dataset expressing opinion from two French politicians over time. We therefore review state of the art NLP-based ML algorithms to automatically annotate tweets using a manual initiation step as bootstrap. This paper focuses on key issues about active learning while building a large annotated data set from noise. This will be introduced by human annotators, abundance of data and the label distribution across data and entities. In turn, we show that Twitter characteristics such as the author\u2019s name or hashtags can be considered as the bearing point to not only improve automatic systems for Opinion Mining (OM) and Topic Classification but also to reduce noise in human annotations. However, a later thorough analysis shows that reducing noise might induce the loss of crucial information.", "creator": "LaTeX with hyperref package"}}}