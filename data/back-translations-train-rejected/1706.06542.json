{"id": "1706.06542", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Extract with Order for Coherent Multi-Document Summarization", "abstract": "In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.", "histories": [["v1", "Mon, 12 Jun 2017 04:54:41 GMT  (64kb,D)", "http://arxiv.org/abs/1706.06542v1", "6 pages, Accepted in TextGraphs-11: Graph-based Methods for Natural Language Processing, Workshop at ACL 2017, Vancouver, Canada, August 2017"]], "COMMENTS": "6 pages, Accepted in TextGraphs-11: Graph-based Methods for Natural Language Processing, Workshop at ACL 2017, Vancouver, Canada, August 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mir tafseer nayeem", "yllias chali"], "accepted": false, "id": "1706.06542"}, "pdf": {"name": "1706.06542.pdf", "metadata": {"source": "CRF", "title": "Extract with Order for Coherent Multi-Document Summarization", "authors": ["Mir Tafseer Nayeem", "Yllias Chali"], "emails": ["mir.nayeem@uleth.ca", "chali@cs.uleth.ca"], "sections": [{"heading": "1 Introduction", "text": "The purpose of the automatic summary of individual documents is to find the most relevant information in a text and present it in compressed form. There are two types of summaries: abstract summary and extractive summary. Abstractive methods, which are still a growing field, are highly complex because they require extensive natural language generation to rewrite the sentences. Therefore, the research community focuses more on extractive summaries, which select outstanding (important) sentences from the source document without making any changes in order to create a summary. Summary is classified as a single document or multi-document based on the number of source documents. The overlap of information between the documents on the same topic makes the summary of multiple documents more difficult than the task of summarizing individual documents. A crucial step in creating a summary summary is to enlarge the sentence in a logical way to make the arrangement of individual sentences not fully readable."}, {"heading": "2 Related Work", "text": "During a decade, several extractive approaches to automatic summary have been developed, implementing a number of machine learning, graph-based and optimization techniques. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods for calculating the sentence meaning for text summary. The RegSum system (Hong and Nenkova, 2014) uses a monitored model for predicting the meaning of words. Treating the summary of multiple documents as a submodular maximization problem has proved successful (Lin and Bilmes, 2011). Unfortunately, none of the above systems has taken into account the coherence of the final extracted summary. Recent work using neural networks (Cheng and Lapata, 2016) suggested an attentive encoder decoder decoder (Nallapati et al., 2017)."}, {"heading": "3 Sentence Extraction", "text": "We describe here successively each of the steps involved in the process of sentence extraction, such as sentence ranking, sentence clustering and sentence selection."}, {"heading": "3.1 Preprocessing", "text": "Our system first takes a set of related texts as input and prepares them, including tokenization, part-of-speech (POS) marking, stop word removal and lemmatization. We use NLTK toolkit1 to pre-process each sentence to get a more accurate representation of the information."}, {"heading": "3.2 Sentence Similarity", "text": "We take the pre-trained word embeddings2 (Mikolov et al., 2013) of all non-stop words in a sentence and take the weighted vector sum according to the term frequency (TF) of a word (w) in a sentence (S). Where E is the word embedding model and idx (w) is the index of the word w. Formally, for a given sentence S in document D the weighted sum is S = \u2211 w, S TF (w, S) \u00b7 E [idx (w)] Then we calculate cosine similarity between the sentence vectors we get from the above equation to find the relative distance between Si and Sj. We also calculate NESim (Si, Sj) by finding the named entities present in Si and Sj, using NLTK toolkit (SLTK toolkit), and then calculating their overlapps.Cosim (Sj, Sj / Sj | Si), http | Si | Sj in the Sj | Si."}, {"heading": "3.3 Sentence Ranking", "text": "In this section, we classify the sentences by applying the TextRank algorithm (Mihalcea and Tarau, 2004), which contains the construction of an undirected diagram in which sentences are vertices and form weighted edges by a similarity metric connecting sentences. TextRank determines the similarity based on the lexical overlap between two sentences. However, this algorithm has a serious disadvantage: If two sentences speak about the same subject without using overlapping words, there is no edge between them. Instead, we use the continuous skip grammar model introduced by (Mikolov et al., 2013) to measure the semantic similarity together with the unit. We use the similarity function described in Equation (1) by using \u03bb = 0.3.After we have our graph, we can execute the main algorithm on it. Here, for each vertice a point, a score of 1 is initialized and the text rank of 1 is repeated in the Sick-Sick-Value i (Sick-Sick-Sick-Value) i (Sick-Sick-Sick-Sick-Value) i (Sick-Sick-Sick-Value) i is repeated in the Sick-Sick-Sick-Sick-Value i (Sick-Sick-Sick-Value) i (Sick-Sick-Sick-Sick-Sick-Sick-Si)."}, {"heading": "3.4 Sentence Clustering", "text": "We use a hierarchical agglomerative clustering (Murtagh and Legendre, 2014) with full linkage criteria. This method proceeds incrementally, starting with each set that is considered a cluster, and merging the pair of similar clusters after each step using a bottom-up approach. The full linkage criterion determines the metrics used for the merger strategy. In calculating the clusters, we use the similarity function described in Equation (1) with \u03bb = 0.4. We use a similarity threshold (\u03c4 = 0.5) to stop the clustering process. If we do not find a cluster pair with a similarity above the threshold, the process stops and the clusters are released. The clusters may be small, but are highly coherent, as any set they contain must be similar to any other set in the same cluster.This cluster step is very important for two main reasons: (1) The selection of at most one set of each cluster (the two sets of related information) reduces the number of clusters."}, {"heading": "3.5 Sentence Selection", "text": "In this thesis, we use the concept-based ILP framework introduced in (Gillick and Favre, 2009) with some appropriate modifications to select the best subset of sentences. This approach aims to extract sentences that cover as many important concepts as possible, while the summary length is within a pre-budgeted constraint. Unlike (Gillick and Favre, 2009), which use bigrams as concepts, we use keyphrases as concepts. Keyphrases are the words or phrases that represent the main themes of a document. Sentences that contain the most relevant keyphrases are important for summary generation. We extracted the keyphrases from the document cluster using RAKE3 (Rose et al., 2010). We assign weight to each keyphrase that RAKE.Let wi consider the weight of the keyphrase i and ki to be a binary variable indicating the presence of the keyi and the presence of the phrase."}, {"heading": "4 Sentence Ordering", "text": "The classical reordering approaches include conclusions from the weighted sentence diagram (Barzilay et al., 2002) or perform a chronological order algorithm (Cohen et al., 1999) that sorts sentences based on timestamp and position. We propose a simple, greedy approach to the sentence order in multi-document settings. Our assumption is that a good sentence sequence implies the similarity between all adjacent sentences since word repetition (more precisely, named holistic repetition) is one of the formal signs of text coherence (Barzilay et al., 2002). We define the coherence of document D, which consists of sentences from S1 to Sn, in the following equation. To calculate Sim (Si, Si + 1) we use the similarity function described in Equation (1) with \u03bb = 0.5, giving the designated units a slight preference. Coherence (D) = n \u2212 closely developed."}, {"heading": "5 Evaluation", "text": "We evaluate our system ILPRankSumm (ILP-based sentence selection with TextRank for Extractive Summarization) against ROUGE5 (Lin, 2004) to DUC 2004 (Task-2, Length limit (L) = 100 words), but ROUGE values tend to overlap lexically at the surface level and are insensitive to summary coherence. Furthermore, sophisticated coherence measures are rarely used to summarize, which is why many of the previous systems used human assessments to measure readability. Therefore, we evaluate our summary coherence against (Lapata and Barzilay, 2005) (Barzilay and Lapata, 2008), which define coherence probabilities for a number of judgments."}, {"heading": "5.1 Baseline Systems", "text": "We compare our system with basic data (LexRank, GreedyKL) and state-of-the-art systems (Submodular, ICSISumm). LexRank (Erkan and Radev, 2004) presents input texts as graphs in which nodes5ROUGE-1.5.5 is represented with the following options: -n 2 -m -u -c 95 -x -r 1000 -f A -p 0.5 -t 0 are the sentences and the edges are formed between two sentences if the cosinal similarity is above a certain threshold. The sentence meaning is calculated by executing the PageRank algorithm on the graph. GreedyKL (Haghighi and Vanderwende, 2009) iteratively selects the next sentence for the summary, which minimizes the KL divergence between the estimated word distributions. (Lin and Bilmes, 2011) treat the document summarization problem as maximizing a submodular function, minimizing the number of characters and not achieving optimum combinations."}, {"heading": "5.2 Results", "text": "Our results include R-1, R-2 and R-SU4, which count matches in unigrams, bigrams and skipbigrams respectively, with the skip bigrams allowing four words in between. According to Table 1, R-1, R2 values obtained through our system exceed all baseline and state-of-the-art systems on DUC 2004 records. One of the main reasons for the improved R-1 and R-2 value is the use of keyphrases. Furthermore, there is no significant difference between our proposed system and the submodular one in the case of R-SU4. We also get a higher probability of coherence due to our statutory engineering. The output of the system for a randomly selected document set (e.g. d30015t) from DUC 2004 is shown in Table 2."}, {"heading": "5.3 Limitations", "text": "One of the essential features of the text summary systems is the ability to generate a fixed-length summary (DUC 2004, Task-2: Length limit = 100 words). According to (Hong et al., 2014), all tables of contents of the previous research have either shortened the summary to 100th word or removed the last sentence from the summary. In this essay, we follow the second to create a grammatical summary. However, the first produces a certain ungrammatic sentence, later, in the worst case, a lot of information can be lost if the sentences are long. We focus more on the grammaticality of the final summary."}, {"heading": "6 Conclusion and Future Work", "text": "In this work, we have implemented an ILP-based sentence selection, along with TextRank scores and key phrases for extractive summaries of multiple documents. We continue to model coherence to increase the readability of the generated summary. Evaluation results strongly suggest the benefits of using continuous word vector representations in all steps of the overall system. In the future, we will focus on extracting sentences together to maximize informativeness and readability while minimizing redundancy using the same ILP model. In addition, we will try to propose a solution to the length limitation problem."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their useful comments. Research reported in this paper was conducted at the University of Lethbridge and supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada Discovery Grant and the University of Lethbridge."}], "references": [{"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["Regina Barzilay", "Noemie Elhadad", "Kathleen R. McKeown."], "venue": "J. Artif. Int. Res. 17(1):35\u201355.", "citeRegEx": "Barzilay et al\\.,? 2002", "shortCiteRegEx": "Barzilay et al\\.", "year": 2002}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Comput. Linguist. 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin-", "citeRegEx": "Cheng and Lapata.,? 2016", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Learning to order things", "author": ["William W. Cohen", "Robert E. Schapire", "Yoram Singer."], "venue": "J. Artif. Int. Res. 10(1):243\u2013270.", "citeRegEx": "Cohen et al\\.,? 1999", "shortCiteRegEx": "Cohen et al\\.", "year": 1999}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R. Radev."], "venue": "J. Artif. Int. Res. 22(1):457\u2013479.", "citeRegEx": "Erkan and Radev.,? 2004", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "A scalable global model for summarization", "author": ["Dan Gillick", "Benoit Favre."], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. Association for Computational Linguistics, Stroudsburg, PA, USA, ILP \u201909,", "citeRegEx": "Gillick and Favre.,? 2009", "shortCiteRegEx": "Gillick and Favre.", "year": 2009}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Compu-", "citeRegEx": "Haghighi and Vanderwende.,? 2009", "shortCiteRegEx": "Haghighi and Vanderwende.", "year": 2009}, {"title": "A repository of state of the art and competitive baseline summaries for generic news summarization", "author": ["Kai Hong", "John Conroy", "Benoit Favre", "Alex Kulesza", "Hui Lin", "Ani Nenkova."], "venue": "Proceedings of the Ninth International Conference on Language Re-", "citeRegEx": "Hong et al\\.,? 2014", "shortCiteRegEx": "Hong et al\\.", "year": 2014}, {"title": "Improving the estimation of word importance for news multidocument summarization", "author": ["Kai Hong", "Ani Nenkova."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Associ-", "citeRegEx": "Hong and Nenkova.,? 2014", "shortCiteRegEx": "Hong and Nenkova.", "year": 2014}, {"title": "Automatic evaluation of text coherence: Models and representations", "author": ["Mirella Lapata", "Regina Barzilay."], "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers Inc., San Francisco,", "citeRegEx": "Lapata and Barzilay.,? 2005", "shortCiteRegEx": "Lapata and Barzilay.", "year": 2005}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Association for Computational Linguistics,", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "A class of submodular functions for document summarization", "author": ["Hui Lin", "Jeff Bilmes."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1. Association for Computa-", "citeRegEx": "Lin and Bilmes.,? 2011", "shortCiteRegEx": "Lin and Bilmes.", "year": 2011}, {"title": "Textrank: Bringing order into texts", "author": ["Rada Mihalcea", "Paul Tarau."], "venue": "Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004. Association for Computational Linguistics, Barcelona, Spain, pages 404\u2013411.", "citeRegEx": "Mihalcea and Tarau.,? 2004", "shortCiteRegEx": "Mihalcea and Tarau.", "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Ward\u2019s hierarchical agglomerative clustering method: Which algorithms implement ward\u2019s criterion? J", "author": ["Fionn Murtagh", "Pierre Legendre."], "venue": "Classif. 31(3):274\u2013295.", "citeRegEx": "Murtagh and Legendre.,? 2014", "shortCiteRegEx": "Murtagh and Legendre.", "year": 2014}, {"title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents", "author": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."], "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9,", "citeRegEx": "Nallapati et al\\.,? 2017", "shortCiteRegEx": "Nallapati et al\\.", "year": 2017}, {"title": "Topical coherence for graph-based extractive summarization", "author": ["Daraksha Parveen", "Hans-Martin Ramsl", "Michael Strube."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Parveen et al\\.,? 2015", "shortCiteRegEx": "Parveen et al\\.", "year": 2015}, {"title": "Integrating importance, non-redundancy and coherence in graph-based extractive summarization", "author": ["Daraksha Parveen", "Michael Strube."], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence. AAAI Press, IJCAI\u201915, pages", "citeRegEx": "Parveen and Strube.,? 2015", "shortCiteRegEx": "Parveen and Strube.", "year": 2015}, {"title": "Automatic keyword extraction from individual documents", "author": ["Stuart Rose", "Dave Engel", "Nick Cramer", "Wendy Cowley."], "venue": "Text Mining pages 1\u201320.", "citeRegEx": "Rose et al\\.,? 2010", "shortCiteRegEx": "Rose et al\\.", "year": 2010}, {"title": "Exploring text links for coherent multi-document summarization", "author": ["Xun Wang", "Masaaki Nishino", "Tsutomu Hirao", "Katsuhito Sudoh", "Masaaki Nagata."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguis-", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization.", "startOffset": 8, "endOffset": 31}, {"referenceID": 12, "context": "LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization.", "startOffset": 45, "endOffset": 71}, {"referenceID": 8, "context": "The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance.", "startOffset": 18, "endOffset": 42}, {"referenceID": 11, "context": "Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin and Bilmes, 2011).", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": "In very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al.", "startOffset": 43, "endOffset": 67}, {"referenceID": 15, "context": "In very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2017) used", "startOffset": 112, "endOffset": 136}, {"referenceID": 17, "context": "(Parveen and Strube, 2015; Parveen et al., 2015) ar X iv :1 70 6.", "startOffset": 0, "endOffset": 48}, {"referenceID": 16, "context": "(Parveen and Strube, 2015; Parveen et al., 2015) ar X iv :1 70 6.", "startOffset": 0, "endOffset": 48}, {"referenceID": 19, "context": "Moreover, a recent work (Wang et al., 2016) actually proposed a multi-document summarization system that combines both coherence and informativeness but this system is limited to syntactic linkages between entities.", "startOffset": 24, "endOffset": 43}, {"referenceID": 13, "context": "We take the pre-trained word embeddings2 (Mikolov et al., 2013) of all the non stopwords in", "startOffset": 41, "endOffset": 63}, {"referenceID": 12, "context": "In this section, we rank the sentences by applying TextRank algorithm (Mihalcea and Tarau, 2004) which involves constructing an undirected graph", "startOffset": 70, "endOffset": 96}, {"referenceID": 13, "context": "Instead, we use the continuous skip-gram model introduced by (Mikolov et al., 2013) to measure the semantic similarity", "startOffset": 61, "endOffset": 83}, {"referenceID": 14, "context": "erative clustering (Murtagh and Legendre, 2014) with a complete linkage criteria.", "startOffset": 19, "endOffset": 47}, {"referenceID": 5, "context": "work introduced in (Gillick and Favre, 2009) with some suitable changes to select the best subset of sentences.", "startOffset": 19, "endOffset": 44}, {"referenceID": 5, "context": "Unlike (Gillick and Favre, 2009) which uses bigrams as concepts, we use keyphrases as concepts.", "startOffset": 7, "endOffset": 32}, {"referenceID": 18, "context": "We extracted the keyphrases from the document cluster using RAKE3 (Rose et al., 2010).", "startOffset": 66, "endOffset": 85}, {"referenceID": 5, "context": "In addition to (Gillick and Favre, 2009), we put", "startOffset": 15, "endOffset": 40}, {"referenceID": 0, "context": "Classic reordering approaches include inferring order from weighted sentence graph (Barzilay et al., 2002), or perform a chronological ordering algorithm (Cohen et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 3, "context": ", 2002), or perform a chronological ordering algorithm (Cohen et al., 1999) that sorts sentences based on timestamp and position.", "startOffset": 55, "endOffset": 75}, {"referenceID": 0, "context": "Our assumption is that a good sentence order implies the similarity between all adjacent sentences since word repetition (more specifically, named entity repetition) is one of the formal sign of text coherence (Barzilay et al., 2002).", "startOffset": 210, "endOffset": 233}, {"referenceID": 10, "context": "We evaluate our system ILPRankSumm (ILP based sentence selection with TextRank for Extractive Summarization) using ROUGE5 (Lin, 2004) on DUC 2004 (Task-2, Length limit(L) = 100 words).", "startOffset": 122, "endOffset": 133}, {"referenceID": 9, "context": "For this reason, we evaluate our summary coherence using (Lapata and Barzilay, 2005) (Barzilay and Lapata, 2008) which defines coherence probabilities for an ordered set of sentences.", "startOffset": 57, "endOffset": 84}, {"referenceID": 1, "context": "For this reason, we evaluate our summary coherence using (Lapata and Barzilay, 2005) (Barzilay and Lapata, 2008) which defines coherence probabilities for an ordered set of sentences.", "startOffset": 85, "endOffset": 112}, {"referenceID": 4, "context": "LexRank(Erkan and Radev, 2004) represents input texts as graph where nodes", "startOffset": 7, "endOffset": 30}, {"referenceID": 6, "context": "GreedyKL (Haghighi and Vanderwende, 2009) iteratively selects the next sentence for the summary that will minimize the KL divergence between the estimated word distributions.", "startOffset": 9, "endOffset": 41}, {"referenceID": 11, "context": "(Lin and Bilmes, 2011) treat the document summarization problem as maximizing a Submodular function under a budget constraint.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "On the other hand, ICSISumm (Gillick and Favre, 2009) employs a global linear optimization framework, finding the globally optimal summary rather than choosing sentences according to their", "startOffset": 28, "endOffset": 53}, {"referenceID": 7, "context": "The summaries generated by the baselines and the state-of-the-art extractive summarizers on the DUC 2004 dataset were collected from (Hong et al., 2014).", "startOffset": 133, "endOffset": 152}, {"referenceID": 7, "context": "According to (Hong et al., 2014) all the summarizer from the previous research either truncated the summary to 100th word, or removed the last sentence from the summary set.", "startOffset": 13, "endOffset": 32}], "year": 2017, "abstractText": "In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.", "creator": "LaTeX with hyperref package"}}}