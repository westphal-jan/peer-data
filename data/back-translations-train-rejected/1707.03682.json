{"id": "1707.03682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "A Deep Learning Approach for Blind Drift Calibration of Sensor Networks", "abstract": "Temporal drift of sensory data is a severe problem impacting the data quality of wireless sensor networks (WSNs). With the proliferation of large-scale and long-term WSNs, it is becoming more important to calibrate sensors when the ground truth is unavailable. This problem is called \"blind calibration\". In this paper, we propose a novel deep learning method named projection-recovery network (PRNet) to blindly calibrate sensor measurements online. The PRNet first projects the drifted data to a feature space, and uses a powerful deep convolutional neural network to recover the estimated drift-free measurements. We deploy a 24-sensor testbed and provide comprehensive empirical evidence showing that the proposed method significantly improves the sensing accuracy and drifted sensor detection. Compared with previous methods, PRNet can calibrate 2x of drifted sensors at the recovery rate of 80% under the same level of accuracy requirement. We also provide helpful insights for designing deep neural networks for sensor calibration. We hope our proposed simple and effective approach will serve as a solid baseline in blind drift calibration of sensor networks.", "histories": [["v1", "Fri, 16 Jun 2017 17:10:13 GMT  (2981kb)", "http://arxiv.org/abs/1707.03682v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["yuzhi wang", "anqi yang", "xiaoming chen", "pengjun wang", "yu wang", "huazhong yang"], "accepted": false, "id": "1707.03682"}, "pdf": {"name": "1707.03682.pdf", "metadata": {"source": "CRF", "title": "A Deep Learning Approach for Blind Drift Calibration of Sensor Networks", "authors": ["Yuzhi Wang", "Anqi Yang", "Xiaoming Chen", "Pengjun Wang", "Yu Wang"], "emails": ["yz-wang12@mails.tsinghua.edu.cn;", "yang-aq14@mails.", "wangpj@tsinghua.edu.cn;", "yu-wang@tsinghua.edu.cn;", "yanghz@tsinghua.edu.cn).", "xchen7@nd.edu)."], "sections": [{"heading": null, "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "II. PRELIMINARIES", "text": "In this section we present the necessary preparatory work of this work, including the problem of blind calibration for sensor networks and the existing work on this problem."}, {"heading": "A. Problem Formulation", "text": "In view of a sensor network located in an area of interest, we assume that the measurement signal within the sensor space is continuous, and the measurements collected by the sensors are spatially and temporally discrete."}, {"heading": "B. Related Work", "text": "This year it is so far that it is only a matter of time before it will be so far, until it is so far, until it is so far."}, {"heading": "III. CALIBRATION WITH DEEP FULL CONVOLUTIONAL NEURAL NETWORK", "text": "In order to design a CNN-based method for blind calibration of sensor networks, there are two major challenges: \u2022 Many existing work on CNNs are proposed for image processing; how do we design the network architecture for sensor rift calibration? \u2022 Deep neural networks need to be trained with a large amount of training data, while the sensory data collected by a particular sensor network is limited; how do you train the neural network with limited sensor data? \u2022 To solve the first problem, we expand the idea of the previous SPSR framework [9] by designing a projection and recovery architecture called PRNet. The first layer projects the drifting measurements onto a functional space, and this layer is trained to hold the drift characteristics; the following recovery layers are trained to merge the characteristics into drift-free results. When the training converges, the network can automatically melt features from these measurement-free and extract them."}, {"heading": "A. Convolution on Sensory Data", "text": "Before describing the architecture of PRNet, we present our basic idea of applying coils to sensory data. Let Y N \u00d7 T denote the matrix with sensory data from a sensor network, where the rows represent measurements from N different sensors, and the columns are measured at T different times. Input of a folding layer is a 3-D tensor called X (N, T, c), where N, T, and c represent its numbers rows, columns, and channels. Therefore, we convert the measuring matrix Y N \u00d7 T into a tensor Y (N, T, 1), so that it can be fed into a folding layer."}, {"heading": "B. Architecture of PRNet", "text": "The architecture of PRNet is derived from the basic CNN for sensory data. We first review the key idea of the previous weight and calibration environment. [8] If the drift-free measurements are located in a signal subreoom, a projection matrix P can be obtained that is satisfactory PY = P (X + D) = P D (5), where X, D and Y represent the ground truth signal. This equation is the key point of the subspace method, as the projection eliminates the unknown ground-based truth signal and a lowerdimensional observation of the drift. In our work, we expand this drift projection by implementing it with an N-shaped global convolution layer in which N is the number of sensors and a temporary window size."}, {"heading": "C. Sensor Re-Arrangement", "text": "This year, the number of unemployed has tripled compared to the previous year, and the number of unemployed has increased by 2.2 percent compared to the previous year."}, {"heading": "D. Training Data Generation", "text": "Because sensor networks are used in different sensor fields, they vary in data functions, so the calibration model for a specified sensor field needs to be equipped with sensor data from the same field. To collect sensor data from a single sensor network, a neural network needs to be formed with a large amount of data. Therefore, we propose a data synthesis and augmentation method to capture the training data.We assume that the sensors are calibrated prior to deployment so that the sensor data is collected within a short period of time and maintains the properties of the sensor field. On the contrary, sensor flow and noise are usually caused by faults rather than ideal factors in the sensor hardware."}, {"heading": "E. Training", "text": "The training process of a neural network is to minimize a loss function in relation to the input data by adjusting the network parameters. PRNet's loss function includes the projection loss and the recovery loss, referred to as L P R = L P + L R. (21) Recalling equivalent (5) that the key function of the projection layer is to obtain a drift observation of drift and noise parameters Algorithm 1: Create a patch for training input: X IN \u00b7 TI: a patch drift-free measurements, TP: initial sensor measurements, TP: patch length, \u03c30, \u03c3b, \u03c3d, \u03c3n: drift and noise parameters Output: Y p: a patch drift-free measurements, Xp: a patch drift-free measurements, 1 v \u2190 (vi j) N \u00d7 TP with vi j sampled parameters from N (0, \u03c3 2n); 2???"}, {"heading": "11 else", "text": "Therefore, the projection loss is designed to be called L P = 1 2 | D | NTP [1]. [F] stands for the function of the projection layer f (Y Pi) \u2212 f p (Y Pi \u2212 X Pi)). [F] 2F (22), where X P and Y P are patches of drift-free and drifting measurements. [F] stands for the function of the projection layer f (Y Pi) \u2212 f p (Y Pi)). [F), where X P and Y P are patches of drift-free and drifting measurements. [F] stands for the function of the projection layer f [P] and D stands for the training data. [F] stands for the recovery loss, we simply use the mean square error (MSE) between the calibrated measurements and the basic truth signal."}, {"heading": "IV. PERFORMANCE EVALUATION", "text": "In this section, we will use a real sensor data set to evaluate the proposed method and compare our method with two existing blind calibration methods."}, {"heading": "A. Datasets", "text": "Although there are many WSN projects, some of which have open datasets, we do not know whether these sensors have drifted in each position and exactly how the measurements are. \"We,\" it says in the study, \"have the metrics in each position multiple redundant sensors to ensure accurate measurements.\" \"We,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,"}, {"heading": "B. Training Settings", "text": "In both the test bed and the simulated datasets, we use the first 8,000 samples of each sensor to create the training dataset; the patch length of the extension Tp is set to 20 and the minibatch size to 64; for the test dataset, we set the dimension of the projection space to 64 and for the simulated dataset to 128; the weight parameters are initialized using the initialization method proposed by He et al. [34]; and the bias parameters are initialized to zeros. As the sensor measurement matrix is already arranged in sensor measurement units, we use an identity matrix to bypass the rearrangement; for the simulated dataset, we obtain the rearrangement matrix using the closest algorithm."}, {"heading": "C. Comparison With Existing Methods", "text": "We compare the calibration performance of PRNet with two representative existing calibration methods, including the PSR-TSBL method that we apply to the calibration effects. (SR-TSBL algorithm) The calibration effects of the calibration effects of the calibration effects of the calibration effects between the calibration effects and the calibration effects of the calibration effects indicating the calibration efficiency that we call: RMSE (X, X) def = D-X-2FNTwo the calibration effectiveness and X."}, {"heading": "D. Generalization Ability: Different Types of Drift and Noise Tolerance", "text": "In this experiment, we test the generalizability of sensor-i-time, as we generate it, is a random model, in which we have to deal with other types of drift. \"It,\" according to the title, \"is the way in which we simulate this type of drift in the USA.\" It, \"according to the title,\" is the way in which \"we simulate\" this type of drift. \"It,\" according to the author, \"is the way in which\" we simulate \"this type of drift,\" for which \"we first have a random number of end values as the greatest drift values it can achieve.\" Next, \"we create\" linear drift values. \"It,\" according to the author, \"is the way in which\" we create the linear drift-te., \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the,\" the \"the\" the \"the,\" the \"the\" the \"the\" the, \"the\" the \"the\" the, the \"the\" the \"the\" the, the \"the\" the \"the\" the \"the\" the \"the\""}, {"heading": "V. EXPLORATION ON ARCHITECTURE AND PARAMETERS", "text": "In this section, we will use the simulated dataset to examine the impact of various settings on PRNet, including architecture design, temporary kernel size, training patch size, and data relocation."}, {"heading": "A. Architecture Design", "text": "In this experiment we demonstrate the effectiveness of the PRNet architecture by comparing its performance with three other architectures, including: \u2022 PRNet-RLO: the same architecture with PRNet, trained with recovery loss only (RLO); \u2022 FCN: a two-stage perfect recovery (FCN), shown in Fig. 10 (a); ColFCN: a large convolution kernel in the last layer, shown in Fig. 10 (b), ColFCN is similar to a \"reverse\" PRNet.The numbers of layers, parameter sizes, computing time complexity, computing amounts, and the size of the receptive fields of these networks are listed in Table III. The proposed PRNet has the smallest compilation set and a small parameter size, the ColFCN has the largest receptive field size."}, {"heading": "B. Patch Size Selection", "text": "In Section III-D, we claim that the patch size of the training data should be slightly larger than the receptive field of the neural network over the time dimension. In this experiment, we test the performance on different patch sizes. As shown in Table III, the receptive field size of PRNet over the time dimension is 15. We set the patch size Tp to 10, 12, 15, 20, and 40, and train 5 PRNets. Since the patch loss is related to the patch size, we use the validation RMSE to compare their performance.Figure 12 shows the validation RMSEs of PRNets trained with different patch sizes. If the patch size is 10, the validation RMSE remains at a high level and even increases slightly with training siterations. This is because the training patch cannot even fill the receptive field of the network, and the characteristics of the training data are quite different from the test validation data for PRIS and are limited in comparison to the PRIS."}, {"heading": "C. Benefit From Temporal Correlation", "text": "In this experiment, we show how PRNet uses the temporal correlation and how the temporal correlation contributes to the calibration performance. We derive a purely spatial PRNet, which is based on the original PRNet, in which all ks-kt folding cores are replaced by ks-1 cores. This means that the purely spatial PRNet is calculated on measurements taken at a single time. Next, we increase the channels of the purely spatial PRNet, so that it has the parameter size of 348.3KB and the normalized computational set of 1.164MOps, which are slightly larger than the original PRNet. We map the training loss and validation of RMSEs of the two PRNet models during the pre-training in Fig. 13, where both the training loss and the validation RMSE of the spatial-temporal PRNet data are lower than that of the purely spatial PRNet. Therefore, the spatial-temporal folding PRNet helps to use the correlation of the measurement data."}, {"heading": "D. Necessity of Data Re-Arrangement", "text": "In this experiment, we demonstrate why a rearrangement of the sensors is indispensable. We train three PRNets with the same architecture, using the same training data, but different rearrangement matrices, including \u2022 without rearrangement: set M to an identity matrix; \u2022 next neighbor: calculate M with the closest neighboring algorithm; \u2022 Euler circuit: rearrangement of the sensors by crossing the Euler circuit on the edge-duplicated MST. Training losses and validation of the RMSEs of these PRNets during the pre-training process are in Fig. 14. One, which was rearranged without rearrangement, has the greatest training loss and validation RMSE. The other two PRNets achieve similar validation RMSEs, but the one trained with Euler circuit-based rearrangement exhibits lower training losses. Since the Euler circuit solution ensures that the maximum correlation should be minimized to the organized distance of the sensors."}, {"heading": "VI. DISCUSSION", "text": "Although we have had many empirical experimental results to evaluate the effectiveness of PRNet, although it is difficult to theoretically explain why PRNet surpasses existing methods. In this section we will try to provide an intuitive explanation.PRNet has two stages: projection and restoration. In section III-B we have explained that the projection stage is a natural extension of the linear low-dimensional projection used in the subspace method and can be used in other applications such as non-linear sensor fields. For the restoration layers, we claim that the key component is the non-linear ReLU activation function, since ReLU (x) = max (0, x). (30) We first consider the case that the activation functions are linear, then the general restoration function is linear as well. The optimal solution for this case is a linear, least square regression, which does not have the ability to detect and calibrate drifting sensors."}, {"heading": "VII. CONCLUSION", "text": "With the increasing use of large-scale, long-term wireless sensor networks, sensor drift is becoming a serious problem, while sensor calibration is impractical one by one when a sensor network can be as large as hundreds of sensors. Blind calibration is a handy scheme that restores drift-free sensory data from drifting measurements without the truth of the ground, but it is difficult to calibrate general surveillance sensor networks blindly because there is no prior data model and low-density use. In this paper, we propose a deep learning approach to blind calibration of sensor measurements called PRNet before use. We expect sensors to be calibrated before use so that the measurements collected during the initial phase can be treated as drift-free. Using the proposed data augmentation method to generate training data from initial measurements, we train PRNet to automatically push the spatial and temporal characteristics of sensor measurements."}], "references": [{"title": "Wireless sensor network survey", "author": ["J. Yick", "B. Mukherjee", "D. Ghosal"], "venue": "Comput. Netw., vol. 52, no. 12, pp. 2292\u20132330, Aug. 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Application specific sensor node architecture optimization\u2014Experiences from field deployments", "author": ["W. Liu"], "venue": "Proc. IEEE ASP-DAC, Jan./Feb. 2012, pp. 389\u2013394.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Internet of Things for smart cities", "author": ["A. Zanella", "N. Bui", "A. Castellani", "L. Vangelista", "M. Zorzi"], "venue": "IEEE Internet Things J., vol. 1, no. 1, pp. 22\u201332, Feb. 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "An environment monitoring system for precise agriculture based on wireless sensor networks", "author": ["J. Xia", "Z. Tang", "X. Shi", "L. Fan", "H. Li"], "venue": "Proc. MSN, Dec. 2011, pp. 28\u201335.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A reliable transfer protocol for multi-parameter data collecting in wireless sensor networks", "author": ["X. Fei"], "venue": "Proc. ICACT, Jan. 2013, pp. 569\u2013573.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Sensor network data fault types", "author": ["K. Ni"], "venue": "ACM Trans. Sensor Netw., vol. 5, no. 3, May 2009, Art. no. 25.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Blind calibration of sensor networks", "author": ["L. Balzano", "R. Nowak"], "venue": "Proc. IPSN, Apr. 2007, pp. 79\u201388.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Blind drift calibration of sensor networks using signal space projection and Kalman filter", "author": ["Y. Wang", "A. Yang", "Z. Li", "P. Wang", "H. Yang"], "venue": "Proc. 10th ISSNIP, Apr. 2015, pp. 1\u20136.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Blind drift calibration of sensor networks using sparse Bayesian learning", "author": ["Y. Wang", "A. Yang", "Z. Li", "X. Chen", "P. Wang", "H. Yang"], "venue": "IEEE Sensors J., vol. 16, no. 16, pp. 6249\u20136260, Aug. 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Drift aware wireless sensor networks", "author": ["M. Takruri", "S. Challa"], "venue": "Proc. ICIF, Jul. 2007, pp. 1\u20137.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Distributed recursive algorithm for auto calibration in drift aware wireless sensor networks", "author": ["M. Takruri", "K. Aboura", "S. Challa"], "venue": "Proc. Innov. Adv. Techn. Syst., Comput. Sci. Softw. Eng., 2008, pp. 21\u201325.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Online drift correction in wireless sensor networks using spatiotemporal modeling", "author": ["M. Takruri", "S. Rajasegarar", "S. Challa", "C. Leckie", "M. Palaniswami"], "venue": "Proc. ICIF, Jun./Jul. 2008, pp. 1\u20138.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Geospatial estimationbased auto drift correction in wireless sensor networks", "author": ["D. Kumar", "S. Rajasegarar", "M. Palaniswami"], "venue": "ACM Trans. Sensor Netw., vol. 11, no. 3, May 2015, Art. no. 50.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Study of long-term drift of a porous silicon humidity sensor and its compensation using ann technique", "author": ["T. Islam", "H. Saha"], "venue": "Sens. Actuators A, Phys., vol. 133, no. 2, pp. 472\u2013479, 2007.  WANG et al.: DEEP LEARNING APPROACH FOR BLIND DRIFT CALIBRATION OF SENSOR NETWORKS  4171", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Pressure sensor drifts in argo and their impacts", "author": ["P.M. Barker", "J.R. Dunn", "C.M. Domingues", "S.E. Wijffels"], "venue": "J. Atmos. Ocean. Technol., vol. 28, no. 8, pp. 1036\u20131049, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "System-level calibration for data fusion in wireless sensor networks", "author": ["R. Tan"], "venue": "ACM Trans. Sensor Netw., vol. 9, no. 3, 2013, Art. no. 28.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Calibrate without calibrating: An iterative approach in participatory sensing network", "author": ["C. Xiang", "P. Yang", "C. Tian", "H. Cai", "Y. Liu"], "venue": "IEEE Trans. Parallel Distrib. Syst., vol. 26, no. 2, pp. 351\u2013361, Feb. 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A collaborative approach to in-place sensor calibration", "author": ["V. Bychkovskiy", "S. Megerian", "D. Estrin"], "venue": "Proc. IPSN, 2003, pp. 301\u2013316.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Blind mobile sensor calibration using an informed nonnegative matrix factorization with a relaxed rendezvous model", "author": ["C. Dorffer", "M. Puigt", "G. Delmaire", "G. Roussel"], "venue": "Proc. ICASSP, vol. 2, Mar. 2016, pp. 2941\u20132945.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "A blind calibration scheme exploiting mutual calibration relationships for a dense mobile sensor network", "author": ["B.-T. Lee", "S.-C. Son", "K. Kang"], "venue": "IEEE Sensors J., vol. 14, no. 5, pp. 1518\u20131526, May 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Proc. NIPS, 2012, pp. 341\u2013349.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to diagnose with LSTM recurrent neural networks", "author": ["Z.C. Lipton"], "venue": "Proc. ICLR, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun"], "venue": "Neural Comput., vol. 1, no. 4, pp. 541\u2013551, 1989.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. ECCV, Mar. 2016, pp. 630\u2013645.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift.", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR, Jun. 2016, pp. 770\u2013778.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity matters in deep learning", "author": ["M. Hardt", "T. Ma"], "venue": "Proc. ICLR, 2017, Art. no. 45833.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "The loss surface of residual networks: Ensembles and the role of batch normalization.", "author": ["E. Littwin", "L. Wolf. (Nov"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "On blocking rules for the bootstrap with dependent data", "author": ["P. Hall", "J.L. Horowitz", "B.-Y. Jing"], "venue": "Biometrika, vol. 82, no. 3, pp. 561\u2013574, 1995.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Recent developments in bootstrapping time series", "author": ["J. Berkowitz", "L. Kilian"], "venue": "Econ. Rev., vol. 19, no. 1, pp. 1\u201348, 2000.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "Why is the sum of independent normal random variables normal?", "author": ["B. Eisenberg", "R. Sullivan"], "venue": "Math. Mag., vol. 81,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Adam: A method for stochastic optimization.", "author": ["D. Kingma", "J. Ba"], "venue": "(Jul", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proc. ICML, Jun. 2009, pp. 41\u201348.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 1026\u20131034.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "WSNs can gather information from the environment and transmit the collected data to users [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "They have important usage in many emerging applications such as environmental monitoring [2], smart cities [3], precise agriculture [4], etc.", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "They have important usage in many emerging applications such as environmental monitoring [2], smart cities [3], precise agriculture [4], etc.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "They have important usage in many emerging applications such as environmental monitoring [2], smart cities [3], precise agriculture [4], etc.", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "In practice, many WSNs have hundreds of sensors deployed [2], [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "In practice, many WSNs have hundreds of sensors deployed [2], [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "[6] give an example of a drifted", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This problem is called blind calibration [7].", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "rules for feature extraction and sensor calibration [7]\u2013[13].", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "rules for feature extraction and sensor calibration [7]\u2013[13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Usually, sensor drift is a long-term process and smoothly increases over time [14], [15], so its value may be at the same order of magnitude with the signal.", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "Usually, sensor drift is a long-term process and smoothly increases over time [14], [15], so its value may be at the same order of magnitude with the signal.", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "Many existing blind calibration methods rely on specific application features, such as a prior data model [16], [17], dense-deployment [18], or sensor mobility [19], [20].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "Many existing blind calibration methods rely on specific application features, such as a prior data model [16], [17], dense-deployment [18], or sensor mobility [19], [20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 17, "context": "Many existing blind calibration methods rely on specific application features, such as a prior data model [16], [17], dense-deployment [18], or sensor mobility [19], [20].", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": "Many existing blind calibration methods rely on specific application features, such as a prior data model [16], [17], dense-deployment [18], or sensor mobility [19], [20].", "startOffset": 160, "endOffset": 164}, {"referenceID": 19, "context": "Many existing blind calibration methods rely on specific application features, such as a prior data model [16], [17], dense-deployment [18], or sensor mobility [19], [20].", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "[11], where the ground truth of a sensor is first predicted using the neighbour sensors\u2019 measurements, and then a Kalman filter (KF) is employed to track the sensor\u2019s drift.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Following works apply different prediction functions in this framework, including support vector regression (SVR) [12] and Kriging interpolation [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "Following works apply different prediction functions in this framework, including support vector regression (SVR) [12] and Kriging interpolation [13].", "startOffset": 145, "endOffset": 149}, {"referenceID": 6, "context": "Balzano and Nowak [7] first proposed the idea of signal subspace where the sensory data lie in, so a part of calibration parameters can be obtained by solving a homogeneous linear system.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "In our previous works [8], [9], we extend this idea by modeling the drift calibration problem as sparse signal recovery, and use Kalman filter or sparse Bayesian learning to", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "In our previous works [8], [9], we extend this idea by modeling the drift calibration problem as sparse signal recovery, and use Kalman filter or sparse Bayesian learning to", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "Experiments [8], [9] show that the subspace methods are more stable and more accurate than the prediction methods.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "Experiments [8], [9] show that the subspace methods are more stable and more accurate than the prediction methods.", "startOffset": 17, "endOffset": 20}, {"referenceID": 20, "context": "Some applications, such as image denoising and inpaiting [21] aiming at restoring TABLE I", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "[22] use a recurrent neural network (RNN) to detect events from segments of clinical measurements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "CNNs [23] are widely used in image processing.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "To solve the first issue, we extend the idea of the previous SPSR framework [9] by designing a projection-recovery CNN", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "Similar to previous works [9], [12], we assume that the", "startOffset": 26, "endOffset": 29}, {"referenceID": 11, "context": "Similar to previous works [9], [12], we assume that the", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "According to [8], if the drift-free measurements lie in a signal subspace, a projection matrix P can be obtained, which satisfies", "startOffset": 13, "endOffset": 16}, {"referenceID": 23, "context": "(b) Structure of a ResUnit [24].", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "The term BN refers to Batch Normalization [25], which helps accelerate the training.", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "The architecture of these recovery layers is derived from ResNet [24], [26], which is a state-ofthe-art CNN architecture widely used in CV applications.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "The architecture of these recovery layers is derived from ResNet [24], [26], which is a state-ofthe-art CNN architecture widely used in CV applications.", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "Some recent works [27], [28] found that this special architecture has better representational ability and is easier to train than conventional CNN architectures.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "Some recent works [27], [28] found that this special architecture has better representational ability and is easier to train than conventional CNN architectures.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "We first set s[1] to 1, and choose the nearest non-visited neighbour sensor for the next step until all sensors are visited.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "On the contrary, sensor drift and noise, are usually caused by errors and non-ideal factors of sensor hardware [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 28, "context": "This is also widely used in other research areas such as time-series bootstrapping [29], [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "This is also widely used in other research areas such as time-series bootstrapping [29], [30].", "startOffset": 89, "endOffset": 93}, {"referenceID": 30, "context": "is still Gaussian [31], given by", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "We use the Adam optimizer [32] to minimize the loss function.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "We apply a simple curriculum learning [33] strategy on selecting the drift emulation parameters \u03c30, \u03c3b, \u03c3d and \u03c3n .", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "1) Testbed Dataset: We use the same testbed described in our previous work [9] deployed in our lab to build the dataset.", "startOffset": 75, "endOffset": 78}, {"referenceID": 33, "context": "[34], and the bias parameters are initialized to zeros.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We compare the calibration performance of PRNet with two representative existing calibration methods, including \u2022 SPSR-TSBL: a subspace-based method proposed in [9], which projects drifted measurements to a lower-dimensional drift-observation subspace, and then estimates drift values using the T-SBL algorithm; \u2022 SVR-KF: a prediction-based drift calibration algorithm proposed in [12], which uses SVR to predict sensor measurements and a Kalman filter to smooth the estimated drift.", "startOffset": 161, "endOffset": 164}, {"referenceID": 11, "context": "We compare the calibration performance of PRNet with two representative existing calibration methods, including \u2022 SPSR-TSBL: a subspace-based method proposed in [9], which projects drifted measurements to a lower-dimensional drift-observation subspace, and then estimates drift values using the T-SBL algorithm; \u2022 SVR-KF: a prediction-based drift calibration algorithm proposed in [12], which uses SVR to predict sensor measurements and a Kalman filter to smooth the estimated drift.", "startOffset": 381, "endOffset": 385}, {"referenceID": 8, "context": "For the T-SBL algorithm, according to [9], we set the block size L to 5.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "The dimension of the signal subspaces estimated by SPSR-TSBL are 5 in the testbed dataset and 17 in the simulated dataset, which means the theoretical limits of drift sensors for the SPSR framework are 18 and 32 [8].", "startOffset": 212, "endOffset": 215}, {"referenceID": 5, "context": "In real-world applications, we can also add possible corruptions such as instant pulse noise [6] to cover application specific data features.", "startOffset": 93, "endOffset": 96}, {"referenceID": 26, "context": "In [27], Hardt and Ma propose a reduced ResNet architecture, and prove that each ResUnit can map some of the input to arbitrary values while keeping the remaining values unchanged.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In addition, as ResNet is optimization-friendly [27], [28], it is possible that by training PRNet, it can converge to a near optimal case that the calibrated measurements of non-drifted sensors keep the same with the input, while those of the drifted", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "In addition, as ResNet is optimization-friendly [27], [28], it is possible that by training PRNet, it can converge to a near optimal case that the calibrated measurements of non-drifted sensors keep the same with the input, while those of the drifted", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Temporal drift of sensory data is a severe problem impacting the data quality of wireless sensor networks (WSNs). With the proliferation of large-scale and long-term WSNs, it is becoming more important to calibrate sensors when the ground truth is unavailable. This problem is called \"blind calibration\". In this paper, we propose a novel deep learning method named projection-recovery network (PRNet) to blindly calibrate sensor measurements online. The PRNet first projects the drifted data to a feature space, and uses a powerful deep convolutional neural network to recover the estimated driftfree measurements. We deploy a 24-sensor testbed and provide comprehensive empirical evidence showing that the proposed method significantly improves the sensing accuracy and drifted sensor detection. Compared with previous methods, PRNet can calibrate 2\u00d7 of drifted sensors at the recovery rate of 80% under the same level of accuracy requirement. We also provide helpful insights for designing deep neural networks for sensor calibration. We hope our proposed simple and effective approach will serve as a solid baseline in blind drift calibration of sensor networks.", "creator": null}}}