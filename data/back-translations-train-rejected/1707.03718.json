{"id": "1707.03718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation", "abstract": "Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3x640x360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.", "histories": [["v1", "Wed, 14 Jun 2017 20:37:17 GMT  (2586kb,D)", "http://arxiv.org/abs/1707.03718v1", "5 pages, 5 figures, GitHub:this https URL"]], "COMMENTS": "5 pages, 5 figures, GitHub:this https URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["abhishek chaurasia", "eugenio culurciello"], "accepted": false, "id": "1707.03718"}, "pdf": {"name": "1707.03718.pdf", "metadata": {"source": "CRF", "title": "LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation", "authors": ["Abhishek Chaurasia", "Eugenio Culurciello"], "emails": ["aabhish@purdue.edu", "euge@purdue.edu"], "sections": [{"heading": null, "text": "In fact, it is the case that most people who have lived in the US in recent years are not able to understand themselves, but that they are able to understand the world and what it is about. (...) It is not that they are able to understand the world. (...) It is not that they are able to understand the world. (...) It is that they are not able to understand the world. (...) It is that they do not understand the world. (...) It is that they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. \"(...) It is as if they do not understand the world. (...) It is as if they do not understand the world, as if they do it, as if they do it, if they do it, if they do it, if they do it."}, {"heading": "II. RELATED WORK", "text": "Semantic segmentation involves tagging every single pixel of an image, and thus preserving spatial information later becomes paramount. A neural network architecture used for scene analysis can be divided into encoder and decoder networks, which are essentially discriminatory or generative networks. State-of-the-art segmentation networks generally use categorization models that predominantly win the ImageNet Large Scale Visual Recognition Challenge (ILSCRC) as their discriminator. The generator either uses the stored pooling indices of discriminators, or learns the parameters using convolution to perform upsampling. In addition, the encoder and decoder decoder decoder can be either symmetrical (the same number of layers in encoders and decoders with the same number of pooling and depooling layers), or they can be asymmetrical."}, {"heading": "III. NETWORK ARCHITECTURE", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "IV. RESULTS", "text": "We compare LinkNet with existing architectures using two different metrics: 1) Performance in terms of speed: \u2022 Number of operations required to perform a forward pass of the network \u2022 Time required to perform a forward pass 2) Accuracy interactions on Cityscapes [20] and CamVid [21] datasets."}, {"heading": "A. Performance Analysis", "text": "We report on LinkNet's inference speed on the embedded NVIDIA TX1 system module, as well as on the widely used NVIDIA TitanX. Table II compares the inference time for a single input frame at different resolutions. As shown in the figures provided, LinkNet can process very high-resolution images at 8.5 fps on the GPU. More importantly, it can deliver real-time performance even on NVIDIA TX1. \"-\" indicates that the network was unable to process images at this resolution on the embedded device. As the default image resolution, we select 640 \u00d7 360 and specify the number of operations required to process the image of that resolution, in Table III. The number of operations determines the lead time of a network, so reducing the number of parameters is more important than decreasing the efficiency of our approach."}, {"heading": "B. Benchmarks", "text": "We use Torch7 [33] machine learning tool for training with RMSProp as an optimization algorithm. The network was trained with four NVIDIA TitanXs. Since the classes present in all datasets are highly unbalanced, we use a custom class weight scheme defined as wclass = 1ln (1.02 + pclass). This class weight scheme was of [19] and it gave us better results than the average frequency. As proposed in Cityscapes, we use intersections over Union (IoU) and instance intersection over Union (iIoU) as our performance metric, rather than using pixel-by-pixel accuracy. To prove that the bypass connections help, each table contains IoU and iIoU values with as little as instance intersection over Union (iIoU)."}, {"heading": "V. CONCLUSION", "text": "We have proposed a novel neural network architecture designed from the ground up specifically for semantic segmentation, our main goal being to efficiently use scarce resources on embedded platforms compared to full-fledged deep learning workstations. Our work offers great benefits in this task, while exceeding existing base models that require an order of magnitude larger computing and storage requirements. Applying the proposed network to the NVIDIA TX1 hardware illustrates portable embedded solutions in real-time. Although the main goal was to run the network on mobile devices, we have found that it is also highly efficient on high-end GPUs such as NVIDIA Titan X. This could prove useful in data center applications where a large number of high-resolution images need to be processed. Our network allows large-scale calculations to be performed in a much faster and more efficient manner, which could result in significant savings."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was partially supported by grants from the Office of Naval Research (ONR) N00014-12-1-0167, N00014-15-1-2791 and MURI N00014-10-1-0278. We thank NVIDIA Corporation for supporting the TX1, Titan X, K40 GPUs used for this research."}], "references": [{"title": "Convolutional networks for images, speech, and time series,", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition,", "author": ["M.A. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks,", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition,", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep residual learning for image recognition,", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Going deeper with convolutions,", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning,", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks,", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Efficient object localization using convolutional networks,", "author": ["J. Tompson", "R. Goroshin", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Combining appearance and structure from motion features for road scene understanding,", "author": ["P. Sturgess", "K. Alahari", "L. Ladicky", "P.H. Torr"], "venue": "BMVC 2012-23rd British Machine Vision Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture,", "author": ["D. Eigen", "R. Fergus"], "venue": "Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Rgb-(d) scene labeling: Features and algorithms,", "author": ["X. Ren", "L. Bo", "D. Fox"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Learning hierarchical features for scene labeling,", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Multimodal deep learning,", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "You only look once: Unified, real-time object detection,", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks,\u201d in Advances in neural information processing", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Ssd: Single shot multibox detector,", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.-Y. Fu", "A.C. Berg"], "venue": "European Conference on Computer Vision. Springer,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Enet: A deep neural network architecture for real-time semantic segmentation,", "author": ["A. Paszke", "A. Chaurasia", "S. Kim", "E. Culurciello"], "venue": "arXiv preprint arXiv:1606.02147,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "The cityscapes dataset for semantic urban scene understanding,", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Segmentation and recognition using structure from motion point clouds,", "author": ["G.J. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling,", "author": ["V. Badrinarayanan", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1505.07293,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Learning deconvolution network for semantic segmentation,", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation,", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation,", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs,", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Multi-scale context aggregation by dilated convolutions,", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Reseg: A recurrent neural networkbased model for semantic segmentation,", "author": ["F. Visin", "M. Ciccone", "A. Romero", "K. Kastner", "K. Cho", "Y. Bengio", "M. Matteucci", "A. Courville"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Conditional random fields as recurrent neural networks,", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding,", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines,", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift,", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions,", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1606.00915,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 118, "endOffset": 121}, {"referenceID": 4, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 156, "endOffset": 160}, {"referenceID": 9, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "Convolutional neural networks\u2019 (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "shifted their focus towards scene understanding because of the surge in tasks like augmented reality and self-driving vehicle, and one of the main step involved in it is pixel-level classification/semantic segmentation [13], [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 12, "context": "shifted their focus towards scene understanding because of the surge in tasks like augmented reality and self-driving vehicle, and one of the main step involved in it is pixel-level classification/semantic segmentation [13], [14].", "startOffset": 225, "endOffset": 229}, {"referenceID": 1, "context": "Inspired by auto-encoders [3], [15], most of the existing techniques for semantic segmentation use encoder-decoder pair as core of their network architecture.", "startOffset": 26, "endOffset": 29}, {"referenceID": 13, "context": "Inspired by auto-encoders [3], [15], most of the existing techniques for semantic segmentation use encoder-decoder pair as core of their network architecture.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].", "startOffset": 181, "endOffset": 185}, {"referenceID": 18, "context": "The proposed network was tested on popular datasets: Cityscapes [20] and CamVid [21] and its processing times was recorded on NVIDIA Jetson TX1 Embedded Systems module as well as on Titan X GPU.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "The proposed network was tested on popular datasets: Cityscapes [20] and CamVid [21] and its processing times was recorded on NVIDIA Jetson TX1 Embedded Systems module as well as on Titan X GPU.", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "In [22] a pre-trained VGG was used as discriminator.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Later on researchers came up with the idea of deep deconvolution network [23], [24], fully convolutional network (FCN) combined with skip architecture [25], which eliminated the need of saving pooling indices.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "Later on researchers came up with the idea of deep deconvolution network [23], [24], fully convolutional network (FCN) combined with skip architecture [25], which eliminated the need of saving pooling indices.", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "Later on researchers came up with the idea of deep deconvolution network [23], [24], fully convolutional network (FCN) combined with skip architecture [25], which eliminated the need of saving pooling indices.", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "Standard pre-trained encoders such as: AlexNet [4], VGG [5], and GoogLeNet [7] have been used for segmentation.", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "Standard pre-trained encoders such as: AlexNet [4], VGG [5], and GoogLeNet [7] have been used for segmentation.", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "Standard pre-trained encoders such as: AlexNet [4], VGG [5], and GoogLeNet [7] have been used for segmentation.", "startOffset": 75, "endOffset": 78}, {"referenceID": 24, "context": "processing steps, like the use of Conditional Random Field (CRF) [26], [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "processing steps, like the use of Conditional Random Field (CRF) [26], [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "Instead of using networks which were designed to perform image classification, [27] proposed to use networks specifically designed for dense predictions.", "startOffset": 79, "endOffset": 83}, {"referenceID": 26, "context": "Apart from this, recently recurrent neural networks (RNNs) were used to get contextual information [28] and to optimize CRF [29]; but the use of RNN in itself makes it computationally very expensive.", "startOffset": 99, "endOffset": 103}, {"referenceID": 27, "context": "Apart from this, recently recurrent neural networks (RNNs) were used to get contextual information [28] and to optimize CRF [29]; but the use of RNN in itself makes it computationally very expensive.", "startOffset": 124, "endOffset": 128}, {"referenceID": 28, "context": "Some work was also done in designing efficient network [30], [19], where DCNN was optimized to get a faster forward processing time but with a decrease in prediction accuracy.", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "Some work was also done in designing efficient network [30], [19], where DCNN was optimized to get a faster forward processing time but with a decrease in prediction accuracy.", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "Here, conv means convolution and full-conv means full convolution [25].", "startOffset": 66, "endOffset": 70}, {"referenceID": 29, "context": "We use batch normalization between each convolutional layer and which is followed by ReLU non-linearity [31], [32].", "startOffset": 104, "endOffset": 108}, {"referenceID": 30, "context": "We use batch normalization between each convolutional layer and which is followed by ReLU non-linearity [31], [32].", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "The later portion of encoder consists of residual blocks [6] and are represented as encoder-block(i).", "startOffset": 57, "endOffset": 60}, {"referenceID": 23, "context": "We use the technique of full-convolution in our decoder as proposed earlier by [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "[22] linked encoder with decoder through pooling indices, which are not trainable parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "2) Performace interms of accuracy on Cityscapes [20] and CamVid [21] datasets.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "2) Performace interms of accuracy on Cityscapes [20] and CamVid [21] datasets.", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "This class weighing scheme has been taken from [19] and it gave us better results than mean average frequency.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "As suggested in Cityscapes [20], we use intersections over union (IoU) and instance-level intersection over union (iIoU) as our performance metric instead of using pixel-wise accuracy.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "We also compare LinkNet\u2019s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "We also compare LinkNet\u2019s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].", "startOffset": 91, "endOffset": 95}, {"referenceID": 31, "context": "We also compare LinkNet\u2019s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].", "startOffset": 110, "endOffset": 114}, {"referenceID": 32, "context": "We also compare LinkNet\u2019s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "as test set [20].", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "We trained on our network on 19 classes that was provided in the official evaluation scripts [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "4: LinkNet prediction on Cityscapes [20] test set.", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "5: LinkNet prediction on CamVid [21] test set.", "startOffset": 32, "endOffset": 36}, {"referenceID": 19, "context": "b) CamVid: It is another automotive dataset which contains 367 training, 101 validation, and 233 testing images [21].", "startOffset": 112, "endOffset": 116}], "year": 2017, "abstractText": "Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3 \u00d7 640 \u00d7 360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.", "creator": "LaTeX with hyperref package"}}}