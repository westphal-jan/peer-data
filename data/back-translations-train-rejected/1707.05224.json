{"id": "1707.05224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Detection, Recognition and Tracking of Moving Objects from Real-time Video via Visual Vocabulary Model and Species Inspired PSO", "abstract": "In this paper, we address the basic problem of recognizing moving objects in video images using Visual Vocabulary model and Bag of Words and track our object of interest in the subsequent video frames using species inspired PSO. Initially, the shadow free images are obtained by background modelling followed by foreground modeling to extract the blobs of our object of interest. Subsequently, we train a cubic SVM with human body datasets in accordance with our domain of interest for recognition and tracking. During training, using the principle of Bag of Words we extract necessary features of certain domains and objects for classification. Subsequently, matching these feature sets with those of the extracted object blobs that are obtained by subtracting the shadow free background from the foreground, we detect successfully our object of interest from the test domain. The performance of the classification by cubic SVM is satisfactorily represented by confusion matrix and ROC curve reflecting the accuracy of each module. After classification, our object of interest is tracked in the test domain using species inspired PSO. By combining the adaptive learning tools with the efficient classification of description, we achieve optimum accuracy in recognition of the moving objects. We evaluate our algorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative analysis of our algorithm against the existing state-of-the-art trackers shows very satisfactory and competitive results.", "histories": [["v1", "Fri, 2 Jun 2017 11:09:10 GMT  (1572kb)", "http://arxiv.org/abs/1707.05224v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["kumar s ray", "anit chakraborty", "sayandip dutta"], "accepted": false, "id": "1707.05224"}, "pdf": {"name": "1707.05224.pdf", "metadata": {"source": "CRF", "title": "FFECTIVE recognition of objects for tracking in real-time video stream and processing of data involve integration of background modelling, shadow removal, foreground modelling", "authors": [], "emails": ["ksray@isical.ac.in)", "ianitchakraborty@gmail.com).", "sayandip199309@gmail.com)"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. BRIEF REVIEW OF RELATED WORKS", "text": "In recent years, the number of those capable of skyrocketing has multiplied, both in the US and in Europe."}, {"heading": "III. PROPOSED METHOD", "text": "To do this, we first model the background and remove the hard shadows from the background to determine the exact area that the object occupies in a frame. Next, we model the foreground and peel the background model without shadows to obtain the blob of an object. Before we detect the object within the blob, we train a machine learning-inspired visual vocabulary model with a series of objects that can represent our sphere of interest for detection and tracking. In both cases, i.e. in the case of a training phase, we extract the features of the objects in the training data according to the principle of Bagof Words, and in the testing phase, we use the same technique to extract the features of the objects in the test data. Reclassification of extracted feature sets, we track the features of other detected objects in successive video frames via Species inspired PSO."}, {"heading": "A. Background Modeling", "text": "In fact, it is a way in which one has to adapt to the background in order to have an idea for the background modeling. (...) In our work, we have made some changes to the same work and procedure. (...) In any case, we have an image that is characterized by the subtraction of two consecutive video frames and the subtraction of the current video frames with the background modeling. (...) In order to cope with a sudden illumination, we have to deal with the same work. \"(...)"}, {"heading": "B. Shadow removal", "text": "As mentioned in [41] by Xu et al., the shadow removal approach is formulated in Figure 2.The normalization of r, g, b is formulated as follows: \u00b2 2 + 2 + 2, (14) 2 + 2 + 2, (15) 2 + 2 + 2, (16) where r, g, b + color channels are entered into the image, r ', b', g 'construct the shadow-free hue of invariant images. The application of Gaussian's smooth filter suppresses the high-frequency textures in invariant and original images. These smoothed color images are converted to grayscales to recognize the edges, formally: () () () (), () () () (), where the image is strongly filtered."}, {"heading": "C. Foreground Modelling and Reconstruction", "text": "In fact, it is the case that most of them are able to decide whether they are able or not, whether they are able to achieve their goals."}, {"heading": "E. Region of Interest Tracking", "text": "In fact, it is a matter of the kind and manner in which it has taken place in recent years, namely of a time in which it is a matter of time, in which it is a matter of time, in which it is a matter of time, in which it is a matter of eternity, in which it is a matter of time, in which it is a matter of time, in which it is a matter of time, up to a time, in which it is a matter of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of eternity, of e"}, {"heading": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "text": "The proposed method is evaluated using benchmark data sets and compared with existing state-of-the-art tracking algorithms. Brief descriptions of the experimental data sets are shown followed by experimental parameter settings and analysis. All tests were performed on an Intel 5th Gen Core i7, 2.10 GHz processor with 6 Gigabytes of RAM and 2 Gigabytes of NVDIA GeForce GPU and the algorithm was implemented using the MATLAB '16 development tool."}, {"heading": "A. Experimental Datasets", "text": "We evaluate the proposed algorithm on benchmark TB100 sequences, namely iLIDS [4], VIVID [46], Walking2 [50] and Woman [40]. In the following section we discuss the various attributes of the above datasets. The primary challenges in the iLIDS dataset [4] (Intelligent Detection Image Library) are scale variation (SV), in plane rotation (IPR), occlusion (OCC), low resolution (LR) and illumination variation (IV). This video consists of a total of four people walking to and from the camera view, with the camera mounted at an isometric angle where only one man carries a trolley. The video is 10 minutes long, at 25 frames per second, each frame having a dimension of 720x570 pixels."}, {"heading": "B. Experimental Setting", "text": "All of the above data sets consist of RGB color channels, which are more extensive for image processing, so we temporarily convert them to grayscale with the equation: gray = 0.299R + 0.587G + 0.114B. In addition, we update the background model with the process mentioned in (14) (15) and (16) of our proposed method. Then, we process each frame with its original RGB color channels and reconstruct a new image by feeding only the normalized R and G color channel values. This normalized image removes the shadow component from the image, which essentially helps with precise tracking. After this step, we convert this image into a binary image by subtracting the background. (b) We dilate the image using some morphological operations. However, this image does not preserve the edges sufficiently."}, {"heading": "C. Analysis and Evaluation", "text": "In fact, we are able to go in search of a solution that will enable us, that will enable us to go in search of a solution that will enable us, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position where we are."}, {"heading": "V. CONCLUSION", "text": "In this essay, object detection, detection of detected objects based on the visual vocabulary model and tracking of detected objects based on species-inspired PSO are presented. We train different objects separately in multiple images with multiple aspects and camera perspectives to find the best keyword points for detection. Then, we verify the extracted characteristics of the train images. These keyword points are applied to the regions based on visual feature point analysis. Comparative analysis is performed using visual key points. Finally, the characteristics of the detected objects are presented using the PMK approach [Section IV, D (d)] for feature match. The object is satisfactorily detected. After detection of the object, the detection of the specific object of our interest is performed in Section IV (D). Finally, the characteristics of the detected objects are tracked by the PSO inspired by the species, which can also efficiently deal with tracking subset-up algorithms, as shown in Figure 8 of the proposed algorithm."}], "references": [{"title": "Track and Segment: An Iterative Unsupervised Approach for Video Object Proposals", "author": ["F. Xiao", "Y.J. Lee"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Finding local leaf vein patterns for legume characterization and classification", "author": ["M.G. Larese", "P.M. Granitto"], "venue": "Machine Vision and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A Method for Detecting Long Term Left Baggage Based on Heat Map", "author": ["P. Foggia", "A. Greco", "A Saggese", "M. Vento"], "venue": "VISAPP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Object Tracking Benchmark", "author": ["Y. Wu", "J. Lim", "M. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Visual tracking via probability continuous outlier model", "author": ["D. Wang", "H. Lu"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 3478\u20133485.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual tracking via locality sensitive histograms", "author": ["S. He", "Q. Yang", "R.W.H. Lau", "J. Wang", "M.-H. Yang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2013, pp. 2427\u2013 2434.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Least soft-thresold squares tracking", "author": ["D. Wang", "H. Lu", "M.-H. Yang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2013, pp. 2371\u20132378.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time compressive tracking", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "Proc. 12th Eur. Conf. Comput. Vis., 2012, pp. 864\u2013877.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust tracking via weakly supervised ranking SVM", "author": ["Y. Bai", "M. Tang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1854\u20131861.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1822\u2013 1829.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Distribution fields for tracking", "author": ["L. Sevilla-Lara", "E. Learned-Miller"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1910\u20131917.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Real time robust l1 tracker using accelerated proximal gradient approach", "author": ["C. Bao", "Y. Wu", "H. Ling", "H. Ji"], "venue": "Proc.  Fig. 14. Accuracy vs Visual Vocabulary of the proposed algorithm, plotted on 4 datasets.  12 IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1830\u2013 1837.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally orderless tracking", "author": ["S. Oron", "A. Bar-Hillel", "D. Levi", "S. Avidan"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1940\u20131947.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 2042\u20132049.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Online robust image alignment via iterative convex optimization", "author": ["Y. Wu", "B. Shen", "H. Ling"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1808\u20131814.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 7, pp. 1619\u20131632, Aug. 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time visual tracking using compressive sensing", "author": ["H. Li", "C. Shen", "Q. Shi"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H.S. Torr"], "venue": "Proc. IEEE Int. Conf. Comput. Vis., 2011, pp. 263\u2013270.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust visual tracking and vehicle classification via sparse representation", "author": ["X. Mei", "H. Ling"], "venue": "PAMI 33,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Context tracker: Exploring supporters and distracters in unconstrained environments", "author": ["T.B. Dinh", "N. Vo", "G. Medioni"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2011, pp. 1177\u2013 1184.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust tracking using local sparse appearance model and K-selection", "author": ["B. Liu", "J. Huang", "L. Yang", "C. Kulikowsk"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2011, pp. 1313\u20131320.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust visual tracking using autoregressive hidden markov model", "author": ["Z. Kalal", "J. Matas", "K. Mikolajczyk", "\u201cP-N Learning: Bootstrapping binary classifiers by structural constraints", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2010", "pp. 49\u201356. [26] X. Mei", "H. Ling", "\u201cRobust visual tracking using l1 minimization", "\u201d in Proc. 12th IEEE Int. Conf. Comput. Vis.", "2009", "pp. 1436\u20131443. [27] S. Stalder", "H. Grabner", "L. van Gool", "\u201cBeyond semisupervised tracking: Tracking should be as simple as detection", "but not simpler than recognition", "\u201d in Proc. 12th IEEE Int. Conf. Comput. Vis. Workshop", "2009", "pp. 1409\u20131416. [28] P. Dolla \u0301r", "B. Babenko", "S. Belongie", "P. Perona", "Z. Tu", "\u201cMultiple Component Learning for Object Detection", "\u201d Proc. European Conf. Computer Vision", "2008. [29] P. Felzenszwalb", "D. McAllester", "D. Ramanan", "\u201cA Discriminatively Trained", "Multiscale", "Deformable Part Model", "\u201d Proc. IEEE Conf. Computer Vision", "Pattern Recognition", "H. 2008. [30] Grabner", "C. Leistner", "Bischof", "D.H.: Semi-supervised On-Line Boosting for Robust Tracking. In: Forsyth", "P. Torr", "Zisserman", "A. (eds.) ECCV 2008", "Part I. LNCS", "vol. 5302", "pp. 234\u2013247. Springer", "Heidelberg (2008) [31] G. Li", "Y. Wang", "W. Shu", "Real-time moving object detection for video monitoring systems", "International Symposium on Intelligent Information Technology Application", "2008. [32] D. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang", "\u201cIncremental learning for robust visual tracking", "\u201d Int. J. Comput. Vis.", "vol. 77", "no. 1", "pp. 125\u2013141", "2008. [33] S. Avidan", "\u201cEnsemble tracking", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 29", "no. 2", "pp. 261\u2013271", "Feb. 2007. [34] F. Tang", "S. Brennan", "Q. Zhao", "H. Tao", "\u201cCo-Tracking using semi- supervised support vector machines", "\u201d in Proc. 11th IEEE Conf. Comput. Vis. Pattern Recognit.", "2007", "pp. 1\u20138. [35] A. Adam", "E. Rivlin", "I. Shimshoni", "\u201cRobust Fragments- Based Tracking Using the Integral Histogram", "\u201d Proc. IEEE Conf. Computer Vision", "Pattern Recognition", "vol. 1", "pp. 798-805", "2006. [36] V. Lepetit", "P. Fua", "\u201cKeypoint Recognition Using Randomized Trees", "\u201d IEEE Trans. Pattern Analysis", "Machine Intelligence", "vol. 28", "no. 9", "pp. 1465-1479", "Sept. 2006. [37] D. Nister", "H. Stewenius", "\u201cScalable Recognition with a Vocabulary Tree\u201d", "Department of Computer Science", "University of Kentucky", "2006. [38] H. Grabner", "M. Grabner", "H. Bischof", "\u201cReal-time tracking via on-line boosting", "\u201d in Proc. British Mach. Vis. Conf.", "2006", "pp. 6.1\u2013 6.10. [39] O. Tuzel", "F. Porikli", "P. Meer", "\u201cRegion covariance: A fast descriptor for detection", "classification", "\u201d in Proc. 9th Eur. Conf. Comput. Vis.", "2006", "pp. 589\u2013600. [40] A. Adam", "E. Rivlin", "I. Shimshoni", "\u201cRobust fragmentsbased tracking using the integral histogram", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2006", "pp. 798\u2013805. [41] L. Xu", "F. Qi", "R. Jiang", "\"Shadow Removal from a Single Image\"", "Proc. IEEE Int'l Conf. Intelligent Systems Design", "Applications", "pp. 1049-1054", "2006. [42] N. Dalal", "B. Triggs", "\u201cHistograms of oriented gradients for human detection", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2005", "pp. 886\u2013893. [43] R.T. Collins", "Y. Liu", "M. Leordeanu", "\u201cOnline selection of discriminative tracking features", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 27", "no. 10", "pp. 1631\u20131643", "Oct. 2005. [44] F. Porikli", "\u201cIntegral histogram: A fast way to extract histograms in cartesian spaces", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2005", "pp. 829\u2013836. [45] S.T. Birchfield", "S. Rangarajan", "\u201cSpatiograms versus histograms for region-based tracking", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2005", "pp. 1158\u20131163. [46] R. Collins", "X. Zhou", "S.K. Teh", "\u201cAn open source tracking testbed", "evaluation web site", "\u201d in Proc. IEEE Int. Workshop Perform. Eval. Tracking Surveillance", "2005", "pp. 17\u2013 24. [47] P. Viola", "M.J. Jones", "\u201cRobust real-time face detection", "\u201d Int. J. Comput. Vis.", "vol. 57", "no. 2", "pp. 137\u2013154", "2004. [49] S. Avidan", "\u201cSupport vector tracking", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 26", "no. 8", "pp. 1064\u20131072", "Aug. 2004. [50] R.B. Fisher", "\u201cThe PETS04 surveillance ground-truth data sets", "\u201d in Proc. IEEE Int. Workshop Perform. Eval. Tracking Surveillance", "2004", "A. pp. 1\u20135. [51] Jepson", "D. Fleet", "Maraghi", "T.: Robust online appearance models for visual tracking. PAMI 25", "1296\u20131311 (2003) [52] D. Comaniciu", "V. Ramesh", "P. Meer", "\u201cKernel-based object tracking", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 25", "no. 5", "pp. 564\u2013577", "May 2003. [53] R.T. Collins", "\u201cMean-shift blob tracking through scale space", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2003", "pp. 234\u2013240.  13 [54] T. Ojala", "M. Pietik\u00e4inen", "T. M\u00e4enp\u00e4\u00e4", "\u201cMultiresolution grayscale", "rotation invariant texture classification with local binary patterns", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 24", "no. 7", "pp. 971\u2013987", "Jul. 2002. [55] P. P_erez", "C. Hue", "J. Vermaak", "M. Gangnet", "\u201cColorbased probabilistic tracking", "\u201d in Proc. 7th Eur. Conf. Comput. Vis.", "2002", "pp. 661\u2013675 [56] M. Isard", "J. Maccormick", "\u201cBramble: A Bayesian Multiple-Blob Tracker", "\u201d Proc. IEEE Int\u2019l Conf. Computer Vision", "vol. 2", "pp. 34-41", "2001. [57] M. Isard", "A. Blake", "\u201cCondensation-Conditional density propagation for visual tracking", "\u201d Int. J. Comput. Vis.", "vol. 29", "no. 1", "pp. 5\u201328", "1998. [58] D. Riahi", "G.-A. Bilodeau", "\u201cMultiple object tracking based on sparse generative appearance modeling", "\u201d in Image Processing (ICIP)", "2015 IEEE International Conference on. IEEE", "2015", "pp. 4017\u20134021. [59] Chenglong Bao", "Yi Wu", "Haibin Ling", "Hui Ji", "\u201cReal time robust l1 tracker using accelerated proximal gradient approach", "\u201d in CVPR. IEEE", "2012", "pp. 1830\u20131837. [60] C. Stauffer", "W.E.L. Grimson", "\"Adaptive Background Mixture Models for Real-Time Tracking\"", "Proc. Computer Vision", "Pattern Recognition 1999 (CVPR '99)", "1999-June [62] S. McKenna", "Y. Raja", "S. Gong", "\"Tracking Colour Objects Using Adaptive Mixture Models\"", "Image", "Vision Computing J.", "vol. 17", "pp. 223-229", "1999. [63] C. Bishop", "Neural Networks for Pattern Recognition", "Oxford University Press", "New York", "1995. [64] B.S. Everitt", "D.J. Hand", "Finite Mixture Distributions", "Chapman", "Hall", "New York", "1981. [65] G.J. McLachlan", "K.E. Basford", "Mixture Models: Inference", "Applications to Clustering", "Marcel Dekker Inc.", "New York", "1988. [66] C.E. Priebe", "Adaptive mixtures", "Journal of the American Statistics Association 89 (427) (1994) 796\u2013806. [67] C.E.Priebe", "D.J. Maxchette", "Adaptive mixtures: recursive nonparametric pattern recognition", "Pattern Recognition 24 (12) (1991) 1197\u20131209. [68] C.E. Priebe", "D.J. Marchette", "Adaptive mixture density estimation", "Pattern Recognition 26 (5) (1993) 771\u2013785. [69] D. Park", "J. Kwon", "K. Lee"], "venue": "CVPR, pp. 1964\u20131971, IEEE, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1964}], "referenceMentions": [{"referenceID": 15, "context": "In recent years, several attentions [51, 49, 43, 38, 32, 30, 17, 18, 20] have been given in this direction to achieve and share the goal of this paper.", "startOffset": 36, "endOffset": 72}, {"referenceID": 16, "context": "In recent years, several attentions [51, 49, 43, 38, 32, 30, 17, 18, 20] have been given in this direction to achieve and share the goal of this paper.", "startOffset": 36, "endOffset": 72}, {"referenceID": 18, "context": "In recent years, several attentions [51, 49, 43, 38, 32, 30, 17, 18, 20] have been given in this direction to achieve and share the goal of this paper.", "startOffset": 36, "endOffset": 72}, {"referenceID": 18, "context": "Generally, appearance based tracking algorithms are of two types mainly; generative [51, 32, 20, 18] and discriminative [49, 43, 38, 30, 17].", "startOffset": 84, "endOffset": 100}, {"referenceID": 16, "context": "Generally, appearance based tracking algorithms are of two types mainly; generative [51, 32, 20, 18] and discriminative [49, 43, 38, 30, 17].", "startOffset": 84, "endOffset": 100}, {"referenceID": 15, "context": "Generally, appearance based tracking algorithms are of two types mainly; generative [51, 32, 20, 18] and discriminative [49, 43, 38, 30, 17].", "startOffset": 120, "endOffset": 140}, {"referenceID": 5, "context": "Several tracking algorithms based on static appearance models exists, which are either trained using only the first few consecutive set of iterations or defined manually [56], [36], [7], [35].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "[7] developed a locality sensitive histogram at each pixel for finer distribution of the visual feature points for object tracking in video scenes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Local binary patterns (LBP) [54] as well as Haar-like features [47] have been proposed for appearance based tracking of objects [17], [38], [19], [9].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Local binary patterns (LBP) [54] as well as Haar-like features [47] have been proposed for appearance based tracking of objects [17], [38], [19], [9].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Local binary patterns (LBP) [54] as well as Haar-like features [47] have been proposed for appearance based tracking of objects [17], [38], [19], [9].", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "Recently pixel based segmentations have been applied [2] to handle tracking.", "startOffset": 53, "endOffset": 56}, {"referenceID": 16, "context": "[18], semi-boosting [30], support vector machine (SVM) [49], boosting [38], structured output SVM [19], and online multi-", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18], semi-boosting [30], support vector machine (SVM) [49], boosting [38], structured output SVM [19], and online multi-", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "instance boosting [17].", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": ", MIL [17], OAB [38], IVT", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "[32], L1 [26], TLD [23] and likes.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "The principle idea behind this approach [19], is to divide the groundtruth particles of the object into various species according to the species object numbers and successfully model the relations and the partial visibility among varied species.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "Annealed Gaussian Based PSO (AGPSO) An annealed Gaussian based PSO algorithm [21] is considered in this paper, as in conventional PSO requires careful and fine tuning of various parameters.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "We evaluate the proposed algorithm on benchmark TB100 sequences, namely, iLIDS [4], VIVID [46], Walking2 [50] and Woman [40].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "The primary challenges in the iLIDS dataset [4] (imagery Library for Intelligent Detection Systems) is the Scale Variation (SV), In Plane Rotation (IPR), Occlusion (OCC), Low Resolution (LR) and Illumination Variation (IV).", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Next, a comparative analysis of the Frame Per Second (FPS), provided in [5], is demonstrated in Table 5.", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "Detection Accuracy on iLIDS Dataset [4]", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "ASLA [11] 2012 79.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "24% DFT [12] 2012 81.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "36% MIL [17] 2009 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "29% PCOM [6] 2014 79.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "87% LSS [8] 2013 78.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "ASLA [11] 2012 86.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "3% DFT [12] 2012 85.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "9% MIL [17] 2009 90.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "2% PCOM [6] 2014 86.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "7% LSS [8] 2013 87.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "ASLA [11] 2012 88.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "2% DFT [12] 2012 87.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "8% MIL [17] 2009 90.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "2% PCOM [6] 2014 88.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "3% LSS [8] 2013 90.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "ASLA [11] 2012 87.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "8% DFT [12] 2012 88.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "9% MIL [17] 2009 91.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "2% PCOM [6] 2014 88.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "4% LSS [8] 2013 89.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 68, "endOffset": 71}], "year": 2017, "abstractText": "In this paper, we address the basic problem of recognizing moving objects in video images using Visual Vocabulary model and Bag of Words and track our object of interest in the subsequent video frames using species inspired PSO. Initially, the shadow free images are obtained by background modelling followed by foreground modeling to extract the blobs of our object of interest. Subsequently, we train a cubic SVM with human body datasets in accordance with our domain of interest for recognition and tracking. During training, using the principle of Bag of Words we extract necessary features of certain domains and objects for classification. Subsequently, matching these feature sets with those of the extracted object blobs that are obtained by subtracting the shadow free background from the foreground, we detect successfully our object of interest from the test domain. The performance of the classification by cubic SVM is satisfactorily represented by confusion matrix and ROC curve reflecting the accuracy of each module. After classification, our object of interest is tracked in the test domain using species inspired PSO. By combining the adaptive learning tools with the efficient classification of description, we achieve optimum accuracy in recognition of the moving objects. We evaluate our algorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative analysis of our algorithm against the existing state-of-the-art trackers shows very satisfactory and competitive results.", "creator": "Microsoft\u00ae Word 2016"}}}