{"id": "1708.03994", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Data Sets: Word Embeddings Learned from Tweets and General Data", "abstract": "A word embedding is a low-dimensional, dense and real- valued vector representation of a word. Word embeddings have been used in many NLP tasks. They are usually gener- ated from a large text corpus. The embedding of a word cap- tures both its syntactic and semantic aspects. Tweets are short, noisy and have unique lexical and semantic features that are different from other types of text. Therefore, it is necessary to have word embeddings learned specifically from tweets. In this paper, we present ten word embedding data sets. In addition to the data sets learned from just tweet data, we also built embedding sets from the general data and the combination of tweets with the general data. The general data consist of news articles, Wikipedia data and other web data. These ten embedding models were learned from about 400 million tweets and 7 billion words from the general text. In this paper, we also present two experiments demonstrating how to use the data sets in some NLP tasks, such as tweet sentiment analysis and tweet topic classification tasks.", "histories": [["v1", "Mon, 14 Aug 2017 02:34:17 GMT  (506kb)", "http://arxiv.org/abs/1708.03994v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["quanzhi li", "sameena shah", "xiaomo liu", "armineh nourbakhsh"], "accepted": false, "id": "1708.03994"}, "pdf": {"name": "1708.03994.pdf", "metadata": {"source": "CRF", "title": "Data Sets: Word Embeddings Learned from Tweets and General Data", "authors": ["Quanzhi Li", "Sameena Shah", "Xiaomo Liu", "Armineh Nourbakhsh"], "emails": ["armineh.nourbakhsh}@thomsonreuters.com"], "sections": [{"heading": "1. Introduction", "text": "In this study, word embedding or word vectors are also referred to as computer-generated large networks. They help us to use learning algorithms to achieve better performance in natural language processing (NLP) by summarizing similar words and also using them in many NLP applications, such as tweets with sentiment analysis (Socher et al. 2013; Mass et al. 2014; Tang et al. 2014, Li et al.), text classification (Matt 2015; Li et al. 2016a), and recommendation (Li et al. 2016b). Traditional bag-of-words and bag-of-n-grams capture the semantics of words, or the distances between words. This means that words \"walk\" and \"eat\" are equally distant in the fact that \"walk\" should be closer to \"than\" eat. \"Based on word embedding\" walk \"and\" run \"run\" run. \""}, {"heading": "2. Technologies Used for Building Word Embedding Data Sets", "text": "In this section, we describe three technologies used to build our word vector models: distributed word representation, phrase recognition, and tweet spam filtering."}, {"heading": "2.1 Distributed Word Representation", "text": "A distributed language representation X consists of an embedding of each vocabulary word in space S with dimension D, where D is the dimension of the latent representation space. Embedding is learned to optimize an objective function defined on the original text, such as the probability of word occurrences. Word embedding models have been researched in previous studies (Collobert et al. 2011; Mikolov et al. 2013b; Socher et al. 2014). Collobert et al. (2011) introduce the C & W model to learn word embedding based on syntactic word contexts. Another implementation is the word2vec model from (Mikolov et al. 2013a et al. 2013b). This model features two training options, the Continuous Bag of Words and the Skip-gram model to learn word embedding based on syntactical contexts. The Skip-Learning model of high-quality word preference and a large number of distributors is a representative method."}, {"heading": "2.2 Phrase Identification", "text": "To identify phrases from TweetData and GeneralData, we use the approach described in Mikolov et al. 2013b. The good thing about this approach is that we can build many reasonable phrases without greatly increasing the vocabulary. On the other hand, if we train the Skip-gram model using all n-grams, it would be very memory-intensive. To identify phrases, a simple data-driven approach is used, where phrases are built based on the unigram and bigram without greatly increasing the vocabulary."}, {"heading": "2.3 Tweet Spam Filter", "text": "We use the tweet \"spam\" filtering algorithm described in (Liu et al. 2016) to identify spam tweets (or noise tweets). It is more than just the elimination of standard spam. There are a lot of tweets that carry little semantic substance, such as profanity, chat, advertising, etc. The aim of this spam filter is to label all these categories as spam and to receive only those informative tweets that contain information of interest. The spam filtering algorithm is a hybrid approach that combines both rules-based and learning-based methods. Inspired by studies by Yardi et al. (2009) and Song et al. (2011), this approach uses features of a follow-to-friend relationship, tweet publishing frequency, and other indicators to detect default spam. Profanity and advertising can be largely removed by using keyword lists."}, {"heading": "3. Word Embedding Data Sets", "text": "In this section, we first introduce the two training datasets TweetData and GeneralData, which are used to generate the word embedding, and the basic steps for their pre-processing. Then, we introduce our ten training datasets (models) listed in Table 1. Their names are self-explanatory. Here, with / without spam, means whether the spam tweets are included in the training data or not. Word + Phrases means that this embedding dataset contains both words and phrases. TweetDataWithoutSpam + GeneralData means that we use both TweetData (without spam) and GeneralData for training this embedding model. These ten datasets cover all eight embedding datasets with TweetData, which are combinations of the use of spam tweets or not, including phrases or not, and integration with GeneralData or not. In addition to these eight datasets (datasets 5 and Data 6), the training datasets are only generates with general datasets."}, {"heading": "3.1 TweetData and the Preprocessing Steps", "text": "The tweets used to build our embedding models date from October 2014 to October 2016, and were acquired by Twitter's public 1% streaming API and Twitter's decahose data (10% of Twitter's streaming data). We selected randomly selected tweets from that period to make them more representative. Only English-language tweets are used to create word embeddings. In total, there are 390 million English tweets. Based on our spam filter, there are approximately 198 million non-spam tweets and 192 million spam tweets. The size of the training data will affect the quality of the word vectors. Our experiments based on tweet topic classification and word similarity tasks show that after the number of training tweets is over 100 million, performance barely changes. However, from 5,000 to 1 million tweets, the thrust is very significant. Detailed information for each embedded data set is processed in Table 2, 3 & 4. Each tweet resulting from the training process will be processed as URLs and URLs."}, {"heading": "3.2 GeneralData and the Preprocessing Steps", "text": "Another reason for constructing our own GeneralDatacollection is that we wanted to combine TweetData and GeneralData as training data in order to learn general word conditions that relate to both the available data and the available data. (2013b) This model contains 300-dimensional vectors for about 3 million unique words and phrases. Phrases were obtained using the same data-driven approach described in Section 2.2. Although this model was trained on a large amount of data, it has some limitations: (1) it was trained only on news articles, the words and phrases in this mode are cascading; and (2) the texts were not cleaned before being fed into the training algorithm. Therefore, there are many junk tokens in this model and they can affect the performance of applications using it. We will discuss this problem in our second experiment. Another reason for constructing our own GeneralDatacollection is that we use TweetData and General Data together as training data."}, {"heading": "3.3 Word Embedding Data Sets and Metadata", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Dataset1: TweetDataWithoutSpam_Word", "text": "For this vector model, the training tweets do not include spam tweets, and the GeneralData capture will not be significantly improved if the results are not used. We only look at single terms and no multi-word phrases. In total, 2.8 billion words per-type tweets are pro-processed, with a term frequency threshold of 5 (tokens less than 5 occurrences in the training data set are discarded), the total number of unique tweets (hashtags and words) in this model is 1.9 million words per-processed, the word embedding model dimension size is set to 300, the dimension size will affect the quality of a vector, larger dimension size will give a better quality, and we conducted experiments to see how the performance of the tweets with different word sizes, and word changes for word classification will change."}, {"heading": "3.3.8 Dataset8: TweetDataWithoutSpam+GeneralData_Word+Phrase", "text": "The only difference between this dataset and Dataset7 is that this dataset contains both words and phrases. Consequently, the total number of unique terms in this dataset is much larger, namely 3.7 million, than that in Dataset7, which is 1.7 million, although this model has a higher term frequency threshold, 15. Table 4 contains the associated metadata for this model. 3.3.9 Dataset9: TweetDataWithSpam + GeneralData _ Word In this dataset, both spam and non-spam tweets from TweetdData are used in conjunction with GeneralData for training purposes. It contains word-only embeddings. Related metadata are also shown in Table 4. 3.10 Dataset10: TweetDataWithSpam + GeneralData _ Word + Phrase Compared to Dataset9, this dataset contains both words and phrases. Table 4 shows that this dataset has the clearest terms, namely 4.4 million."}, {"heading": "3.4 How to Retrieve the Embeddings", "text": "Each published embed dataset contains a binary file that contains the words (and phrases, for records including phrases) and their embedding. It also contains a text file that contains a list of all the words (and phrases) in that dataset and their frequency. The text file is for reference purposes only, users can simply use the binary embed file without the text file. There are several options to retrieve the embeddings from the binary model files. Here, we list just a few: - Use Python's gensim package that has the implementation of word2vec, available at: http: / / radimrehurek.com / gensim /. - Use a Java implementation of word2vec, available at: http: / / deeplearning4j.org / word2vec.html.2013 (Naket 2013)."}, {"heading": "4. Experiments", "text": "We conducted two experiments: the first uses a tweet mood analysis to show how the word embed data set is used, and the second tests four embedded data sets in tweets to show their performance differences."}, {"heading": "4.1 Experiment on Tweet Sentiment Analysis", "text": "In this experiment, we will show how the word embedding can be used in a real NLP task, namely in Tweet Sentiment Analysis. We will not compare our approach with other methods, as this does not fall within the scope of this article. In this experiment, we will only use a vector model, Dataset1: TweetDataWithoutSpam _ Word, to demonstrate how it can be used. The other embedding sets can be used in the same way. In Tweet Sentiment Analysis, we will evaluate precision, callback, F measurement and accuracy. It is a two-sided classification task, i.e. we have two polarities: positive and negative. 4.1.1 Sentiment Analysis Data Set The experiment will be performed on the basis of a benchmark data set originating from Paper 9 of SemEval 2014 (Sara et al. 2014)."}, {"heading": "4.1.2 Word Embeddings for Tweet Representation", "text": "Given a tweet, we process it through the following steps: - First, use the same steps in Section 3.1 to pre-process the tweet and get its purified text. - Second, for each word, we look for its embedding from the vector model. The result is a 300-dimensional vector of real values. If a token is not included in the model, we can either ignore this token, use a vector whose values are all 0 to represent this token, or use the average of embeddings of words with the lowest frequency in the model. In this experiment, we simply ignore the token if it is not present in the model file. Normally, they are misspelled words. - Well, for each word of this tweet we have a real value vector. Since tweets have different lengths, we must use a fixed-length vector to represent a tweet so that we can use it in any learning algorithm or application."}, {"heading": "4.2 Experiment on Tweet Topic Classification", "text": "This experiment aims to show how some of the embedded datasets perform differently on the Tweet topic classification task. We will not test all ten models in this expert experiment. These datasets may perform differently in different applications. Users can try them in their use cases to see which tweet performs best. In this experiment we used embedding models learned from the following four datasets: Dataset1, Dataset2, Dataset6 and Dataset8. The reason for selecting these four datasets is that we can perform the following comparisons: Word vs. Word + Phrase (Dataset 1 vs. TweetData + Phrase), TweetData vs. TweetData + GeneralData (Dataset8), that we can perform the following comparisons: Word vs. Word + Phrase (Dataset 1 vs. TweetData + Phrase), Datetas2 vs. Datas2."}, {"heading": "5. Conclusion", "text": "Distributed word representations can benefit many NLP-related tasks. In this paper, ten word embedding datasets are presented, which were derived from approximately 400 million tweets and billions of words of general text data. These word embedding datasets can be used in Twitter-related NLP tasks. Our experiments also showed how these embedding are used. Experimental results show that context is important when a classification task is pending. Vectors trained on tweet data, for example, are more useful than vectors trained on long-form content. Furthermore, our experiments show that phrase recognition (even at the expense of processing time) can generate more useful vectors. Finally, noise filtering and spam detection can be helpful pre-processing steps, especially when the task is to recognize the semantic topic tweets."}], "references": [{"title": "w00t! feeling great today!\" chatter in twitter: Identification and prevalence", "author": ["R. Balasubramanyan", "A. Kolcz"], "venue": "The international conference on Advances in Social Network Analysis and Mining (ASONAM", "citeRegEx": "Balasubramanyan and Kolcz.,? \\Q2013\\E", "shortCiteRegEx": "Balasubramanyan and Kolcz.", "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["Fan", "R.-E", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Improvements to Platt's SMO Algorithm for SVM Classifier Design", "author": ["S.S. Keerthi", "S.K. Shevade", "C. Bhattacharyya", "K.R.K. Murthy"], "venue": "Neural Computation", "citeRegEx": "Keerthi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2001}, {"title": "Learning word vectors for sentiment analysis, In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["A. Maas", "R. Daly", "P. Pham", "D. Huang", "A. Ng", "C. Potts"], "venue": null, "citeRegEx": "Maas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2012}, {"title": "Document Classification by Inversion of Distributed Language Representations", "author": ["T. Matt"], "venue": "The 53th ACL conference,", "citeRegEx": "Matt,? \\Q2015\\E", "shortCiteRegEx": "Matt", "year": 2015}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "Dean J"], "venue": "In Proceedings of Workshop at ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "Dean J"], "venue": "In Proceedings of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "2016a, TweetSift: Tweet Topic Classification Based on Entity Knowledge Base and Topic Enhanced Word Embedding, the 25th ACM International Conference on Information and Knowledge Management (CIKM 2016)", "author": ["Quanzhi Li", "Sameena Shah", "Xiaomo Liu", "Armineh Nourbakhsh", "Rui Fang"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Hashtag Recommendation Based on Topic Enhanced Embedding, Tweet Entity Data and Learning to Rank, the 25th ACM International Conference on Information and Knowledge Management (CIKM 2016)", "author": ["Quanzhi Li", "Sameena Shah", "Armineh Nourbakhsh", "Xiaomo Liu", "Rui Fang"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "2016c, Tweet Sentiment Analysis by Incorporating Sentiment-Specific Word Embedding and Weighted Text Features, 2016", "author": ["Quanzhi Li", "Sameena. Shah", "Rui Fang", "Armineh Nourbakhsh", "Xiaomo Liu"], "venue": "IEEE/ACM International Conference on Web Intelligence", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Fast Training of Support Vector Machines using Sequential Minimal Optimization", "author": ["J. Platt"], "venue": "Advances in Kernel Methods - Support Vector Learning,", "citeRegEx": "Platt,? \\Q1998\\E", "shortCiteRegEx": "Platt", "year": 1998}, {"title": "Semeval-2013 task 2: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Sara Rosenthal", "Zornitsa Kozareva", "Veselin Stoyanov", "Alan Ritter", "Theresa Wilson."], "venue": "Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), volume 13.", "citeRegEx": "Nakov et al\\.,? 2013", "shortCiteRegEx": "Nakov et al\\.", "year": 2013}, {"title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "author": ["Rosenthal", "Sara", "Ritter", "Alan", "Nakov", "Preslav", "Stoyanov", "Veselin"], "venue": "Proceedings of the 8 International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Rosenthal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2014}, {"title": "Vector-based Models of Semantic Composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Proceedings of ACL", "citeRegEx": "Mitchell and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Spam filtering in twitter using sender-receiver relationship", "author": ["Song, J., Lee, S.", "J. Kim"], "venue": "Recent Advances in Intrusion Detection, 301\u2013317. Springer.", "citeRegEx": "S. and Kim,? 2011", "shortCiteRegEx": "S. and Kim", "year": 2011}, {"title": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification, the 52th ACL conference", "author": ["D. Tang", "F. Wei", "Y. Yang", "M. Zhou", "T. Liu", "B. Qin"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Suspended accounts in retrospect: An analysis of twitter spam", "author": ["K. Thomas", "C. Grier", "D. Song", "V. Paxson"], "venue": "The ACM Internet Measurement Conference (IMC", "citeRegEx": "Thomas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2011}, {"title": "Detecting spam in a twitter network", "author": ["Yardi, S., Romero, D.", "G Schoenebeck"], "venue": "First Monday 15(1).", "citeRegEx": "D. and Schoenebeck,? 2009", "shortCiteRegEx": "D. and Schoenebeck", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "2016c), text classification (Matt 2015; Li et al. 2016a), and recommendation (Li et al.", "startOffset": 28, "endOffset": 56}, {"referenceID": 1, "context": "Word embedding models have been researched in previous studies (Collobert et al. 2011; Mikolov et al. 2013b; Socher et al. 2014).", "startOffset": 63, "endOffset": 128}, {"referenceID": 15, "context": "Word embedding models have been researched in previous studies (Collobert et al. 2011; Mikolov et al. 2013b; Socher et al. 2014).", "startOffset": 63, "endOffset": 128}, {"referenceID": 1, "context": "Word embedding models have been researched in previous studies (Collobert et al. 2011; Mikolov et al. 2013b; Socher et al. 2014). Collobert et al. (2011) introduce the C&W model to learn word embeddings based on the syntactic contexts of words.", "startOffset": 64, "endOffset": 154}, {"referenceID": 18, "context": "Given that Twitter has already prevented spam with harmful URLs (Thomas et al. 2011), we only concentrate on signals from user profiles and tweet content.", "startOffset": 64, "endOffset": 84}, {"referenceID": 6, "context": "A pre-built word embedding model is provided by Mikolov et al. (2013b), and it was trained on part of Google News dataset (https://code.", "startOffset": 48, "endOffset": 71}, {"referenceID": 12, "context": "2013 (Nakov et al. 2013).", "startOffset": 5, "endOffset": 24}, {"referenceID": 15, "context": "The most common methods use the maximum (max), minimum (min), or average (ave) of the embeddings of all words (or just the important words) in a tweet (Socher et al. 2014; Tang et al. 2014).", "startOffset": 151, "endOffset": 189}, {"referenceID": 17, "context": "The most common methods use the maximum (max), minimum (min), or average (ave) of the embeddings of all words (or just the important words) in a tweet (Socher et al. 2014; Tang et al. 2014).", "startOffset": 151, "endOffset": 189}, {"referenceID": 14, "context": "A study from (Mitchell and Lapata 2008) shows that using multiplication of embeddings can also give good performance.", "startOffset": 13, "endOffset": 39}, {"referenceID": 11, "context": "In this experiment, we applied several classification algorithms to find out which one performs the best, such as LibLinear, SMO (Keerthi et al. 2011; Platt 1998), Random Forest and Logistic Regression.", "startOffset": 129, "endOffset": 162}, {"referenceID": 2, "context": "Their performance was comparable, with the LibLinear model (Fan et al. 2008) performing slightly better.", "startOffset": 59, "endOffset": 76}, {"referenceID": 8, "context": "The labeled data set are from (Li et al. 2016).", "startOffset": 30, "endOffset": 46}, {"referenceID": 11, "context": "SMO is a sequential minimal optimization algorithm for training a support vector classifier (Keerthi et al. 2011; Platt 1998).", "startOffset": 92, "endOffset": 125}], "year": 2017, "abstractText": "A word embedding is a low-dimensional, dense and realvalued vector representation of a word. Word embeddings have been used in many NLP tasks. They are usually generated from a large text corpus. The embedding of a word captures both its syntactic and semantic aspects. Tweets are short, noisy and have unique lexical and semantic features that are different from other types of text. Therefore, it is necessary to have word embeddings learned specifically from tweets. In this paper, we present ten word embedding data sets. In addition to the data sets learned from just tweet data, we also built embedding sets from the general data and the combination of tweets with the general data. The general data consist of news articles, Wikipedia data and other web data. These ten embedding models were learned from about 400 million tweets and 7 billion words from the general text. In this paper, we also present two experiments demonstrating how to use the data sets in some NLP tasks, such as tweet sentiment analysis and tweet topic classification tasks.", "creator": "Word"}}}