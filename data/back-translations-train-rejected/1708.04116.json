{"id": "1708.04116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Early Improving Recurrent Elastic Highway Network", "abstract": "To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.", "histories": [["v1", "Mon, 14 Aug 2017 13:39:28 GMT  (103kb,D)", "http://arxiv.org/abs/1708.04116v1", "9 pages, 3 figures"]], "COMMENTS": "9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hyunsin park", "chang d yoo"], "accepted": false, "id": "1708.04116"}, "pdf": {"name": "1708.04116.pdf", "metadata": {"source": "CRF", "title": "Early Improving Recurrent Elastic Highway Network", "authors": ["Hyunsin Park", "Chang D. Yoo"], "emails": ["hs.park@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2 Background", "text": "Consider a hidden state transition model S of a recursive network, ht = S (ht \u2212 1, xt), (1) in which xt-RDx and ht \u2212 1-RDh are the input vector in time step t and hidden state vector in time step t \u2212 1, respectively. The hidden state transition model of the standard RNN can be represented as follows: SRNN (ht \u2212 1, xt) = tanh (WR [ht \u2212 1 xt 1])), (2) in which WR-RDh (Dh + Dx + 1) and tanh are a weight matrix, including bias and elementary hyperbolic tangent function, respectively rively.On the other hand, the hidden state transition model of an LSTM is as follows: SLSTM (ht \u2212 1, ct \u2212 1, xt) and tanh is a weight matrix, (ct \u2212 tanh), (tanh function, ct, cgate (Rxt), (Rxt-1, Dh \u2212 1)."}, {"heading": "3 Early Improving Recurrent Elastic Highway Network", "text": "In this section, Early Improving Recurrent Elastic Highway Network (EI-REHN) is introduced, a recursive network that is able to vary and adjust the repetition depth between input intervals; the intermediate transition from the (r \u2212 1) -th intermediate repetition depth to the r-th layer is ashrt = g t srt + (1 \u2212 grt) hr \u2212 1t, (9) where Git and s r t are a gating function for adaptive repetition depth or residual component; the gating function is designed to decrease exponentially when the intermediate repetition level increases; as a result, the intermediate repetition effect stops when the gating function reaches zero; the exponentially decreasing gating function allows the most significant intermediate hidden state updates to take place so early that significant performance gains between individual parameters are achieved in order to reduce the number of parameters."}, {"heading": "3.1 Adaptive recurrence depth", "text": "A gating function for the adaptive determination of the intermediate repetition depth as a function of xt and ht \u2212 1 is given as follows: grt = d r \u00b2 g \u00b2 rt, (10) where drt and g \u00b2 r \u00b2 t are elastic gating or resting gating. the elastic gating function results as follows: drt = max (\u03b2 + e \u00b2 -e (\u03b1 + \u03b1t) r, 0), (11) \u03b2 = sigm (\u03b2 \u00b2), (12) \u03b1 = soft plus (\u03b1), (13) \u03b1t = sigm (Wa [ht \u2212 1 xt 1]), (14) where \u03b2, \u03b1, \u03b1t and Wa \u00b2 RDh \u00b7 (Dh + Dx + 1) initial gating preset, global degradation depth, local degradation rate and weight matrix for residual gating function are recursive respectively."}, {"heading": "3.2 Dynamic weight matrix", "text": "In any case, the reurrence layer r, where the reurrence layer r, the reurrence layer r, the reurrence depth is very deep and can be impractical to all weight matrices for any weight sizes."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Synthetic data", "text": "This section aims to show the effectiveness of adaptive repetition depth on a synthetic dataset by comparing it with other recursive networks. To this end, we first constructed a synthetic dataset for sequential regression described in Algorithm 2. The inputs, N, T, Rmax, are the number of samples, the maximum recurrence depth, and affine transformation parameters, respectively. We set the inputs as (N, T, RX, RX, RX)."}, {"heading": "4.2 Human activity recognition", "text": "In this subsection, we describe sequence classification experiments using Human Activity Recognition Using Smartphones Data Set (HAR) [17]. In the HAR dataset, 30 people performed six activities (WALKING, WALKING _ UPSTAIRS, WALKING _ DOWNSTAIRS, SITTING, STANDING, LAYING) by carrying a smartphone on their waist.3-axis linear acceleration and 3-axial angular velocity are captured using its embedded accelerometer and gyroscope.The dataset is divided into 7352 training sequences and 2947 test sequences.For human activity detection on the HAR dataset, the input sequence is modeled by RNN, LSTM, RHN and EI-REHN."}, {"heading": "4.3 Language modeling", "text": "In this subsection, word-level language modeling based on the Penn TreeBank dataset [18] is described; the model architecture is presented in Figure 3. For C-size vocabulary, the One Hot vector is used to represent the word input at the time t, wt, wt, RC and word embedding vector, with Uwt passing the word embedding vector into EI-REHN. Hidden activation of EI-REHN, ht, is multiplied by the transposed word embedding matrix UT based on [19, 20] and finally the Softmax layer is applied to calculate the next word prediction w = t + 1. In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], pointer sentinel networks [22], the Ensemble of LSTMs [21] and RHN [10], and show that the performance proposed in the RN model is comparable to the performance of the x22, and that the proposed in the RRence model is obtained."}, {"heading": "5 Conclusion", "text": "In order to model time-changing nonlinear temporal dynamics in sequential data, a recursive network was studied that is able to vary and adjust the recurrence depth between input intervals. By integrating into the recursive network, which combines a shortcut path with a residual path, with a corrected regating function that is best described as a corrected exponentially decreasing function, the network is able to have a varying recurrence depth. In addition, we propose a dynamic weight matrix construction for recurrent layers, which expands the capacity of the existing recursive network. To support the effectiveness of the proposed network, we conducted three experiments that represent a regression of synthetic data, recognition of human activity, and language modeling on the Penn Treebank dataset."}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["A. Graves", "N. Jaitly"], "venue": "in ICML, vol", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Presentation at Google, Mountain View, 2nd April, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ICML, 2012, pp. 1159\u20131166.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention.", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "in ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 4507\u20134515.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2015, pp. 2377\u20132385.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent highway networks", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Koutn\u00edk", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatially adaptive computation time for residual networks", "author": ["M. Figurnov", "M.D. Collins", "Y. Zhu", "L. Zhang", "J. Huang", "D. Vetrov", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1612.02297, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Hypernetworks", "author": ["D. Ha", "A. Dai", "Q.V. Le"], "venue": "arXiv preprint arXiv:1609.09106, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation, vol. 4, no. 1, pp. 131\u2013139, 1992. 8", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A public domain dataset for human activity recognition using smartphones.", "author": ["D. Anguita", "A. Ghio", "L. Oneto", "X. Parra", "J.L. Reyes-Ortiz"], "venue": "ESANN,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, vol. 19, no. 2, pp. 313\u2013330, 1993.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["H. Inan", "K. Khosravi", "R. Socher"], "venue": "arXiv preprint arXiv:1611.01462, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Using the output embedding to improve language models", "author": ["O. Press", "L. Wolf"], "venue": "arXiv preprint arXiv:1608.05859, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Pointer sentinel mixture models", "author": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "venue": "arXiv preprint arXiv:1609.07843, 2016. 9", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 190, "endOffset": 193}, {"referenceID": 4, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 213, "endOffset": 216}, {"referenceID": 5, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 236, "endOffset": 239}, {"referenceID": 6, "context": "As a measure to safeguard against such problem when constructing a very deep network, ResNet [7] and Highway Networks [8] were proposed.", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "As a measure to safeguard against such problem when constructing a very deep network, ResNet [7] and Highway Networks [8] were proposed.", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "In creating temporally deep RNNs, Long Short-Term Memory (LSTM, [9]), Gated Recurrent Unit (GRU, [3]), and Recurrent Highway Network (RHN, [10]) were introduced to both find learning stability and model long-range temporal dependency.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "In creating temporally deep RNNs, Long Short-Term Memory (LSTM, [9]), Gated Recurrent Unit (GRU, [3]), and Recurrent Highway Network (RHN, [10]) were introduced to both find learning stability and model long-range temporal dependency.", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "In creating temporally deep RNNs, Long Short-Term Memory (LSTM, [9]), Gated Recurrent Unit (GRU, [3]), and Recurrent Highway Network (RHN, [10]) were introduced to both find learning stability and model long-range temporal dependency.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "A general concept of time-dependent adaptive computation time (ACT) is introduced into RNN in [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "In Spatially Adaptive Computation Time [12], the number of executed layers for the regions of the image is dynamically adjusted.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Expanding on the idea recently proposed in ACT [11], with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input.", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "The weight update procedure is just an expansion of the idea underlying hypernetworks [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "And the weight parameters for all the intermediate recurrence layers are calculated based on a hypernetwork [13] that is a sub-network to generate the weights for another network.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "To solve this problem, we propose a dynamic weight matrix utilizing the concept of hypernetworks [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "weight scaling of hypernetworks in [13], weight matrix decomposition proposed by Jurgen Schmidhuber in [14], etc.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "weight scaling of hypernetworks in [13], weight matrix decomposition proposed by Jurgen Schmidhuber in [14], etc.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "For all the experiments in this paper, Tensorflow toolkit [15] was used.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "For training the network, Adam optimizer [16] was adopted with 20 mini-batch size, 100 epochs, and 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "In this subsection, we describe sequence classification experiments by using Human Activity Recognition Using Smartphones Data Set (HAR) [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "In this subsection, word level language modeling on the Penn TreeBank dataset [18] is described.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "The hidden activation of EI-REHN, ht, is multiplied by the transposed word embedding matrix, U based on [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 19, "context": "The hidden activation of EI-REHN, ht, is multiplied by the transposed word embedding matrix, U based on [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 9, "context": "For the parameter optimization, we follow the experimental settings described in [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "We just replace the \u2018RHNCell\u2019 class in the code of [10] with an \u2019EIREHNCell\u2019 class that is an implementation of the proposed model.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "Test LSTM [21] 66M 82.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "4 Pointer Sentinel networks [22] 21M 72.", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "9 Ensemble of LSTMs [21] - 71.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "7 RHN [10] 24M 68.", "startOffset": 6, "endOffset": 10}], "year": 2017, "abstractText": "To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-theart recurrent networks in all three experiments.", "creator": "LaTeX with hyperref package"}}}