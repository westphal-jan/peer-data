{"id": "1708.04198", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "A scalable multi-core architecture with heterogeneous memory structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)", "abstract": "Neuromorphic computing systems comprise net- works of neurons that use asynchronous events for both computation and communication. This type of representation offers several advantages in terms of bandwidth and power consumption in neuromorphic electronic systems. However, managing the traffic of asynchronous events in large scale systems is a daunting task, both in terms of circuit complexity and memory requirements. Here we present a novel routing methodology that employs both hierarchical and mesh routing strategies and combines heterogeneous memory structures for minimizing both memory requirements and latency, while maximizing programming flexibility to support a wide range of event-based neural network architectures, through parameter configuration. We validated the proposed scheme in a prototype multi-core neuromorphic processor chip that employs hybrid analog/digital circuits for emulating synapse and neuron dynamics together with asynchronous digital circuits for managing the address-event traffic. We present a theoretical analysis of the proposed connectivity scheme, describe the methods and circuits used to implement such scheme, and characterize the prototype chip. Finally, we demonstrate the use of the neuromorphic processor with a convolutional neural network for the real-time classification of visual symbols being flashed to a dynamic vision sensor (DVS) at high speed.", "histories": [["v1", "Mon, 14 Aug 2017 16:28:02 GMT  (9526kb,D)", "http://arxiv.org/abs/1708.04198v1", "17 pages, 14 figures"], ["v2", "Wed, 16 Aug 2017 17:50:21 GMT  (7787kb,D)", "http://arxiv.org/abs/1708.04198v2", "17 pages, 14 figures"]], "COMMENTS": "17 pages, 14 figures", "reviews": [], "SUBJECTS": "cs.AR cs.AI", "authors": ["saber moradi", "ning qiao", "fabio stefanini", "giacomo indiveri"], "accepted": false, "id": "1708.04198"}, "pdf": {"name": "1708.04198.pdf", "metadata": {"source": "CRF", "title": "A scalable multi-core architecture with heterogeneous memory structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)", "authors": ["Saber Moradi", "Ning Qiao", "Fabio Stefanini", "Giacomo Indiveri"], "emails": [], "sections": [{"heading": null, "text": "This year, it has come to the point where there is only one person who is able to move around in order to explore the world."}, {"heading": "II. Memory optimized routing", "text": "This is the case with many neural network models, of which there are only a few. For example, in the US, most of them are unable to identify themselves."}, {"heading": "III. Mixed-mode hierarchical-mesh routing architecture", "text": "In this section, we propose a multi-core routing architecture that uses the memory optimization scheme of Section II: each cluster in Figure 1 is mapped to a \"core,\" the interfaces are implemented by asynchronous routers, and the tags are stored in asynchronous Content Addressable Memory (CAM) blocks. CAM blocks (each neuron) contain multiple tags that represent the address of the sources that this neuron has subscribed to. To optimize event routing within the network, we took a mixed-mode approach that combines the benefits of mesh routing schemes (low bandwidth requirements but high latency) with those of hierarchical routing inputs (low latency but high bandwidth requirements). Specifically, this architecture considers a hierarchical routing scheme that combines the benefits of mesh routing schemes with those of hierarchical routing requests (high bandwidth requirements)."}, {"heading": "A. Quasi Delay-Insensitive asynchronous circuit design methodology", "text": "The asynchronous circuits implementing the routers R1, R2 and R3 > > > and the general routing architecture were synthesized using the QDI approach [23], following the Communicating Hardware Processes (CHP) [24] and the Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most4 R1Co re0R1Core3Co re2R1Core1R1Core1R1R1R1 Co re 16 R1Core19Co re18R1Core27Co re28R1Core31Co re30R1Core31Co Core31Co Core1Core29R1Co re20R1 Core23 Co re 22 R1Corre21R1R1R1R1111111R1 Co Co co 16 R1Core27R1111111111111111111R1CoreCo Corre111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111"}, {"heading": "B. Asynchronous routing fabric", "text": "Subsequently, we will describe the block diagrams of the QDI routing architecture that will be implemented following the CHP design methodology. As an example, the design selection, which includes 256 neurons / core (i.e., C), assumes that each neuron can copy its output to four different targets; and subsequently, we assume that one tile corresponds to a single VLSI chip that requires only one level of the R2 arbiters in the hierarchy."}, {"heading": "IV. A multi-core neuromorphic processor prototype", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "A. The core memory/computing module", "text": "The block diagram of the circuits that comprise the synapse memory and the dynamic circuits of the synapse is shown together with the circuits of the neurons in Figure 9. Each of these nodes implements the memory and computing operations of the architecture simultaneously: There are 64 10-bit CAM words, 64 2-bit SRAM cells, \"four\" synapse circuits, and \"one\" leaky neuron circuits per node. The asynchronous CAM memory is used to store the tag of the source address to which the neuron is connected, while the 2-bit SRAM memories are used to program the type of synapse circuits to be used. Depending on the content of the SRAM, a synapse can be programmed to exhibit one of four possible behaviors: fast excitatory, slow excitatory, subtractive, neuraptic inhibitor subtractive circuit events that transform the synapse wave event globally into a secondary event, or each of the synaptic motion events."}, {"heading": "B. Asynchronous Content-Addressable Memory circuits", "text": "In this context, it should be noted that the case is an accident."}, {"heading": "V. Experimental results", "text": "The idea is that it is a way of connecting people who are able to survive on their own without any loss of functionality; the chip specifications, including speed and latency, are capable of dealing with a major problem, such as how communication between neurons would usually happen within the chip; the other limiting factor is the transmission time that the CAM needs to process the incoming events. Again, in this case, and similar to most memory designs that make the use of asynchronous interfaces, we have the worst-case scenario that guarantees the correct functionality of the CAM block."}, {"heading": "VI. Discussion", "text": "One of the most attractive features of neuromorphic processors is their ability to implement massively parallel computer architectures that are co-located within their computer nodes. In addition to enabling ultra-low data-driven processing, this feature enables the construction of very large neural networks without interfering with the Neumann bottleneck problem. [15] And in other areas of data communications, bottlenecks typical of centralized systems can be detected. [15] Distributed storage systems come at the cost of non-trivial routines. Neuromorphic solutions that optimize this overhead, sacrificing communication bandwidth, have sacrificed flexibility and programmability."}, {"heading": "VII. Conclusions", "text": "In this paper, we presented a two-step routing scheme that minimizes the storage requirements required to implement scalable and reconfigurable spiking neural networks with limited connectivity. We used this scheme to analytically resolve the trade-off between \"point-to-point\" connectivity, which increases the memory budget in favor of connection specificity, and \"broadcasting,\" which reduces the memory budget by distributing the same signal across populations of neurons. We presented QDI circuits and building blocks for implementing this routing scheme in asynchronous VLSI technology, and presented a prototype neuromorphic processor chip that integrates such building blocks along with mixed analog / digital neurons and synapses circuits of system-based architecture based on asynchronous VLSI technology. We demonstrated that even with the conservative 0.18-um VLSI technology used in the power consumption of the prototype neuromorphic processor-based architecture, we are able to demonstrate the neuromorpaler-based architecture, which is capable of producing multiple neurological chip-based solutions based on three-dimensional architecture."}, {"heading": "Acknowledgments", "text": "We are grateful to Rajit Manohar from Yale University for providing us with the tutorials and tips on designing asynchronous circuits, as well as CAD tools for synthesis and verification of asynchronous circuits. We thank our colleagues at the Institute of Neuroscience in Zurich for stimulating discussions and this work was supported by the EU ERC grant \"neuroP\" (257219)."}, {"heading": "A. Routing memory minimization constraints", "text": "Here we offer an intuitive explanation for why the two-step routing scheme presented in Section II limits the number of neurons as it minimizes memory usage. First, we consider a simple case in which two subsets of K neurons project only within their groups. In this case, the two groups actually correspond to two separate networks and there is no need to store more than K words (which we will call here) to identify the neurons and thus implement connectivity for that network. Obviously, a larger network with more groups sharing the same property of local connectivity would have no effect on K, so K is constant with the number of neurons in the network. It is clear that adding a connection from one group to another can potentially cause an address collision, i.e., two neurons with the same tag project on the same population, and therefore more than K15tags are required in this case to implement different neurality patterns in the same network."}, {"heading": "B. Communicating Hardware Process (CHP)", "text": "Here is a list of the most commonly used CHP commands that cover the design of all the blocks represented in this paper: \u2022 b; ou ou ou ou: \u2022 b; e means the value of e via channel X. \u2022 Receive: Y? v means a value via channel Y and stores it in variable form. \u2022 Probe: The Boolean expression X is true if a communication via channel X can be completed without suspending it. \u2022 Sequential composition: S; T \u2022 Parallel composition: S'T or S \u2022 Assignment: a: = b. This statement means that \"the value of b applies to a.\" We also write an explanation for a: = true, and an explanation for a: = false. \u2022 Selection: [G1 \u2192 S1] Gn \u2192 Sn], where Gi's Boolean expressions (guards) and Si's program parts are. The execution of this command corresponds to waiting until one of the guards is true and then executing a statement with the true guard."}, {"heading": "D. Asynchronous Content-Addressable Memory (CAM) timing assumptions", "text": "In fact, it is the case that you will be able to put yourself at the top, in the way that you put yourself at the top."}], "references": [{"title": "An event-based neural network architecture with an asynchronous programmable synaptic memory", "author": ["S. Moradi", "G. Indiveri"], "venue": "Biomedical Circuits and Systems, IEEE Transactions on, vol. 8, no. 1, pp. 98\u2013107, February 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A re-configurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses", "author": ["N. Qiao", "H. Mostafa", "F. Corradi", "M. Osswald", "F. Stefanini", "D. Sumislawska", "G. Indiveri"], "venue": "Frontiers in Neuroscience, vol. 9, no. 141, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations", "author": ["B.V. Benjamin", "P. Gao", "E. McQuinn", "S. Choudhary", "A.R. Chandrasekaran", "J. Bussat", "R. Alvarez-Icaza", "J. Arthur", "P. Merolla", "K. Boahen"], "venue": "Proceedings of the IEEE, vol. 102, no. 5, pp. 699\u2013716, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "I. Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickner", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": "Science, vol. 345, no. 6197, pp. 668\u2013673, Aug 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The SpiNNaker project", "author": ["S. Furber", "F. Galluppi", "S. Temple", "L. Plana"], "venue": "Proceedings of the IEEE, vol. 102, no. 5, pp. 652\u2013665, May 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A 32GBit/s communication SoC for a waferscale neuromorphic system", "author": ["S. Scholze", "H. Eisenreich", "S. H\u00f6ppner", "G. Ellguth", "S. Henker", "M. Ander", "S. H\u00e4nzsche", "J. Partzsch", "C. Mayr", "R. Sch\u00fcffny"], "venue": "INTEGRATION, the VLSI journal, vol. 45, no. 1, pp. 61\u201375, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Computing with networks of spiking neurons on a biophysically motivated floating-gate based neuromorphic integrated circuit.", "author": ["S. Brink", "S. Nease", "P. Hasler"], "venue": "Neural networks: the official journal of the International Neural Network Society,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A VLSI network of spiking neurons with an asynchronous static random access memory", "author": ["S. Moradi", "G. Indiveri"], "venue": "Biomedical Circuits and Systems Conference (BioCAS), 2011. IEEE, November 2011, pp. 277\u2013280.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Address-event asynchronous local broadcast protocol", "author": ["S. Deiss", "T. Delbruck", "R. Douglas", "M. Fischer", "M. Mahowald", "T. Matthews", "A. Whatley"], "venue": "World Wide Web page, 1994, http://www.ini.uzh.ch/ \u0303amw/scx/aeprotocol.html.  14", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Point-to-point connectivity between neuromorphic chips using address-events", "author": ["K. Boahen"], "venue": "IEEE Transactions on Circuits and Systems II, vol. 47, no. 5, pp. 416\u201334, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems", "author": ["P. Dayan", "L. Abbott"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Multicasting mesh aer: a scalable assembly approach for reconfigurable neuromorphic structured aer systems. application to convnets", "author": ["C. Zamarre\u00f1o-Ramos", "A. Linares-Barranco", "T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Biomedical Circuits and Systems, IEEE Transactions on, vol. 7, no. 1, pp. 82\u2013102, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical address event routing for reconfigurable large-scale neuromorphic systems", "author": ["J. Park", "T. Yu", "S. Joshi", "C. Maier", "G. Cauwenberghs"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201315, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Can programming be liberated from the von neumann style?: a functional style and its algebra of programs", "author": ["J. Backus"], "venue": "Communications of the ACM, vol. 21, no. 8, pp. 613\u2013641, 1978.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "Memory and information processing in neuromorphic systems", "author": ["G. Indiveri", "S.-C. Liu"], "venue": "Proceedings of the IEEE, vol. 103, no. 8, pp. 1379\u20131397, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Resistive random access memory (ReRAM) based on metal oxides", "author": ["H. Akinaga", "H. Shima"], "venue": "Proceedings of the IEEE, vol. 98, no. 12, pp. 2237\u20132251, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Neuromorphic electronic circuits for building autonomous cognitive systems", "author": ["E. Chicca", "F. Stefanini", "C. Bartolozzi", "G. Indiveri"], "venue": "Proceedings of the IEEE, vol. 102, no. 9, pp. 1367\u20131388, 9 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Formation and maintenance of neuronal assemblies through synaptic plasticity", "author": ["A. Litwin-Kumar", "B. Doiron"], "venue": "Nature communications, vol. 5, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural circuits of the neocortex", "author": ["R. Douglas", "K. Martin"], "venue": "Annual Review of Neuroscience, vol. 27, pp. 419\u201351, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Embedding of cortical representations by the superficial patch system", "author": ["D. Muir", "N. Da Costa", "C. Girardin", "S. Naaman", "D. Omer", "E. Ruesch", "A. Grinvald", "R. Douglas"], "venue": "Cerebral Cortex, vol. 21, no. 10, pp. 2244\u20132260, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A memory-efficient routing method for large-scale spiking neural networks", "author": ["S. Moradi", "N. Imam", "R. Manohar", "G. Indiveri"], "venue": "Circuit Theory and Design, (ECCTD), 2013 European Conference on. IEEE, 2013, pp. 1\u20134.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "The limitations to delay-insensitivity in asynchronous circuits", "author": ["A.J. Martin"], "venue": "Proceedings of the Sixth MIT Conference on Advanced Research in VLSI, ser. AUSCRYPT \u201990. Cambridge, MA, USA: MIT Press, 1990, pp. 263\u2013278.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "Communicating Sequential Processes", "author": ["C.A.R. Hoare"], "venue": "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1985}, {"title": "Asynchronous techniques for system-on-chip design", "author": ["A. Martin", "M. Nystrom"], "venue": "Proceedings of the IEEE, vol. 94, pp. 1089\u20131120, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Reconfigurable asynchronous logic", "author": ["R. Manohar"], "venue": "Custom Integrated Circuits Conference. IEEE, 2006, pp. 13\u201320.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamical systems in neuroscience: The geometry of excitability and bursting", "author": ["E. Izhikevich"], "venue": "The MIT press,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Firing patterns in the adaptive exponential integrate-and-fire model", "author": ["R. Naud", "N. Marcille", "C. Clopath", "W. Gerstner"], "venue": "Biological Cybernetics, vol. 99, no. 4\u20135, pp. 335\u2013347, November 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Synaptic dynamics in analog VLSI", "author": ["C. Bartolozzi", "G. Indiveri"], "venue": "Neural Computation, vol. 19, no. 10, pp. 2581\u20132603, Oct 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "32-bit configurable bias current generator with sub-off-current capability", "author": ["T. Delbruck", "R. Berner", "P. Lichtsteiner", "C. Dualibe"], "venue": "International Symposium on Circuits and Systems, (ISCAS), 2010, IEEE. Paris, France: IEEE, 2010, pp. 1647\u20131650.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Content-addressable memory (CAM) circuits and architectures: A tutorial and survey", "author": ["K. Pagiamtzis", "A. Sheikholeslami"], "venue": "IEEE Journal of Solid-State Circuits, vol. 41, no. 3, pp. 712\u2013727, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Scaling mixed-signal neuromorphic processors to 28nm fd-soi technologies", "author": ["N. Qiao", "G. Indiveri"], "venue": "Biomedical Circuits and Systems Conference, (BioCAS), 2016. IEEE, 2016, pp. 552\u2013555.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing", "author": ["P.U. Diehl", "D. Neil", "J. Binas", "M. Cook", "S.-C. Liu", "M. Pfeiffer"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Spiking deep convolutional neural networks for energy-efficient object recognition", "author": ["Y. Cao", "Y. Chen", "D. Khosla"], "venue": "International Journal of Computer Vision, vol. 113, no. 1, pp. 54\u201366, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Energy-efficient neuromorphic classifiers", "author": ["D. Marti", "M. Rigotti", "M. Seok", "S. Fusi"], "venue": "Neural Computation, vol. 28, pp. 2011\u20132044, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["S.K. Esser", "P.A. Merolla", "J.V. Arthur", "A.S. Cassidy", "R. Appuswamy", "A. Andreopoulos", "D.J. Berg", "J.L. McKinstry", "T. Melano", "D.R. Barch", "C. di Nolfo", "P. Datta", "A. Amir", "B. Taba", "M.D. Flickner", "D.S. Modha"], "venue": "Proceedings of the National Academy of Science, vol. 113, no. 41, pp. 11 441\u20131446, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Mapping from framedriven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing\u2013application to feedforward convnets", "author": ["J.A. P\u00e9rez-Carrasco", "B. Zhao", "C. Serrano", "B. Acha", "T. Serrano- Gotarredona", "S. Chen", "B. Linares-Barranco"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2706\u20132719, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Poker-dvs and mnist-dvs. their history, how they were made, and other details.", "author": ["T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Spike event based learning in neural networks", "author": ["J.A. Henderson", "T.A. Gibson", "J. Wiles"], "venue": "arXiv preprint arXiv:1502.05777, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Feedforward categorization on aer motion events using cortex-like features in a spiking neural network", "author": ["B. Zhao", "R. Ding", "S. Chen", "B. Linares-Barranco", "H. Tang"], "venue": "IEEE transactions on neural networks and learning systems, vol. 26, no. 9, pp. 1963\u20131978, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1963}, {"title": "Hfirst: a temporal approach to object recognition", "author": ["G. Orchard", "C. Meyer", "R. Etienne-Cummings", "C. Posch", "N. Thakor", "R. Benosman"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 10, pp. 2028\u20132040, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Hots: A hierarchy of event-based time-surfaces for pattern recognition", "author": ["X. Lagorce", "G. Orchard", "F. Gallupi", "B.E. Shi", "R. Benosman"], "venue": "2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "An eventdriven multi-kernel convolution processor module for event-driven vision sensors", "author": ["L. Camunas-Mesa", "C. Zamarreno-Ramos", "A. Linares-Barranco", "A. Acosta- Jimenez", "T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Solid-State Circuits, IEEE Journal of, vol. 47, no. 2, pp. 504\u2013 517, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A forecast-based STDP rule suitable for neuromorphic implementation", "author": ["S. Davies", "F. Galluppi", "A.D. Rast", "S.B. Furber"], "venue": "Neural Networks, vol. 32, pp. 3\u201314, 2012.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "A quantitative map of the circuit of cat primary visual cortex", "author": ["T. Binzegger", "R. Douglas", "K. Martin"], "venue": "Journal of Neuroscience, vol. 24, no. 39, pp. 8441\u201353, 2004.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Recurrent neuronal circuits in the neocortex", "author": ["R. Douglas", "K. Martin"], "venue": "Current Biology, vol. 17, no. 13, pp. R496\u2013R500, 2007.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "The importance of being hierarchical", "author": ["N. Markov", "H. Kennedy"], "venue": "Current opinion in neurobiology, vol. 23, no. 2, pp. 187\u2013194, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Specifications of nanoscale devices and circuits for neuromorphic computational systems", "author": ["B. Rajendran", "Y. Liu", "J. sun Seo", "K. Gopalakrishnan", "L. Chang", "D. Friedman", "M. Ritter"], "venue": "Electron Devices, IEEE Transactions on, vol. 60, no. 1, pp. 246\u2013253, Jan 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Resistive Switching: From Fundamentals of Nanoionic Redox Processes to Memristive Device Applications", "author": ["G. Indiveri", "E. Linn", "S. Ambrogio"], "venue": "Weinheim, Germany: Wiley-VCH Verlag GmbH & Co. KGaA,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In an effort to develop a new generation of brain-inspired non von Neumann computing systems, several neuromorphic computing platforms have been proposed in recent years, that implement spike-based re-configurable neural networks [1]\u2013 [8].", "startOffset": 230, "endOffset": 233}, {"referenceID": 7, "context": "In an effort to develop a new generation of brain-inspired non von Neumann computing systems, several neuromorphic computing platforms have been proposed in recent years, that implement spike-based re-configurable neural networks [1]\u2013 [8].", "startOffset": 235, "endOffset": 238}, {"referenceID": 8, "context": "Despite being developed with different goals in mind and following different design strategies, most of these architectures share the same data representation and signal communication protocol: the Address-Event Representation (AER) [9], [10].", "startOffset": 233, "endOffset": 236}, {"referenceID": 9, "context": "Despite being developed with different goals in mind and following different design strategies, most of these architectures share the same data representation and signal communication protocol: the Address-Event Representation (AER) [9], [10].", "startOffset": 238, "endOffset": 242}, {"referenceID": 10, "context": "The type of processing and functionality of these spiking neural networks is determined by their specific structure and parameters, such as the properties of the neurons or the weights of the synapses [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 2, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 207, "endOffset": 210}, {"referenceID": 11, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 212, "endOffset": 216}, {"referenceID": 3, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 356, "endOffset": 359}, {"referenceID": 4, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 361, "endOffset": 364}, {"referenceID": 12, "context": "In particular, most approaches proposed either make use of 2D mesh routing schemes, with maximum flexibility, but at the cost of large resource usage, or tree routing schemes which minimize latency and power, but are more restrictive in the types of networks that can be supported (see [13] for a comprehensive overview comparing most of the approaches that have been proposed in the literature).", "startOffset": 286, "endOffset": 290}, {"referenceID": 12, "context": "In [13] the authors proposed a hierarchical address event routing scheme (HiAER) that overcomes some of the limitations of previous flat treebased approaches.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "However, as with many of the previous approaches [5], the HiAER architecture stores the routing tables in external memory banks implemented using Dynamic Random Access Memory (DRAM).", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "von Neumann bottleneck problem [14], [15].", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "von Neumann bottleneck problem [14], [15].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "The approach we propose combines the advantages of all previous approaches proposed up to now by combining 2D-mesh with hierarchical routing, implementing a 2-stage routing scheme for minimizing memory usage, which employs a combination of point-to-point sourceaddress routing and multi-cast destination-address routing, and by using heterogeneous memory structures distributed within and across the neuron and synapse arrays which are optimally suited to exploit emerging memory technologies such as Resistive Random Access Memory (RRAM) [16].", "startOffset": 539, "endOffset": 543}, {"referenceID": 16, "context": "The routing fabric implemented in this chip uses new designs of quasi-delay insensitive asynchronous circuits, synthesized following the Communicating Hardware Process (CHP) formalism, while the neural and synaptic dynamics are designed using ultralow power subthreshold neuromorphic circuits [17].", "startOffset": 293, "endOffset": 297}, {"referenceID": 17, "context": ", for most of the real and artificial neural networks [18], [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": ", for most of the real and artificial neural networks [18], [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Inspired by the connectivity patterns in biological neural networks [20], [21] we developed a novel routing scheme that exploits their clustered connectivity structure to reduce memory requirements in large-scale neural networks.", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "Inspired by the connectivity patterns in biological neural networks [20], [21] we developed a novel routing scheme that exploits their clustered connectivity structure to reduce memory requirements in large-scale neural networks.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": ", the fan-out operation) is divided into two distinct stages [22].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "In addition to distributing the memory resources among multiple nodes (neurons) within the computing fabric, therefore eliminating the von Neumann bottleneck [14], [15], this two-stage routing scheme allows us to minimize the total digital memory for configuring the", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "In addition to distributing the memory resources among multiple nodes (neurons) within the computing fabric, therefore eliminating the von Neumann bottleneck [14], [15], this two-stage routing scheme allows us to minimize the total digital memory for configuring the", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "To optimize the event routing within the network, we adopted a mixed-mode approach that combines the advantages of mesh routing schemes (low bandwidth requirements, but high latency), with those of hierarchical routing ones (low latency, but high bandwidth requirements) [3].", "startOffset": 271, "endOffset": 274}, {"referenceID": 22, "context": "The asynchronous circuits that implement the R1, R2, and R3 routers, and the overall routing architecture, were synthesized using the QDI approach [23], by following the Communicating Hardware Processes (CHP) [24] and Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "The asynchronous circuits that implement the R1, R2, and R3 routers, and the overall routing architecture, were synthesized using the QDI approach [23], by following the Communicating Hardware Processes (CHP) [24] and Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "The asynchronous circuits that implement the R1, R2, and R3 routers, and the overall routing architecture, were synthesized using the QDI approach [23], by following the Communicating Hardware Processes (CHP) [24] and Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most", "startOffset": 256, "endOffset": 260}, {"referenceID": 25, "context": "The CHP language provides a set of formal constructs for synthesizing a large class of programs [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "Finally, HSE programs are transformed into a set of \u201cproduction rules\u201d which are abstract descriptions of digital Complementary Metal-Oxide-Semiconductor (CMOS) Very Large Scale Integration (VLSI) circuits [25].", "startOffset": 206, "endOffset": 210}, {"referenceID": 3, "context": "The memory optimization theory and the hierarchical routing fabric can be used with either pure digital logic approaches [4], or mixed mode analog/digital ones [17].", "startOffset": 121, "endOffset": 124}, {"referenceID": 16, "context": "The memory optimization theory and the hierarchical routing fabric can be used with either pure digital logic approaches [4], or mixed mode analog/digital ones [17].", "startOffset": 160, "endOffset": 164}, {"referenceID": 16, "context": "The analog circuits are operated in the subthreshold domain to minimize the dynamic power consumption and to implement biophysically realistic neural and synaptic behaviors, with biologically plausible temporal dynamics [17].", "startOffset": 220, "endOffset": 224}, {"referenceID": 1, "context": "and Fire (AdExp-I&F) neuron circuits of the type described and fully characterized in [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 26, "context": "The negative feedback mechanism of the adaptation block and the tunable reset potential of the Potassium block introduce two extra variables in the dynamic equation of the neuron that endow it with a wide variety of dynamical behaviors [27], [28].", "startOffset": 236, "endOffset": 240}, {"referenceID": 27, "context": "The negative feedback mechanism of the adaptation block and the tunable reset potential of the Potassium block introduce two extra variables in the dynamic equation of the neuron that endow it with a wide variety of dynamical behaviors [27], [28].", "startOffset": 242, "endOffset": 246}, {"referenceID": 28, "context": "Synapses and biophysically realistic synapse dynamic are implemented using sub-threshold Differential Pair Integrator (DPI) log-domain filters, of the type proposed in [29], and described in [17].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Synapses and biophysically realistic synapse dynamic are implemented using sub-threshold Differential Pair Integrator (DPI) log-domain filters, of the type proposed in [29], and described in [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 29, "context": "The analog circuit parameters governing the behavior and dynamics of the neurons and synapses are set by programmable onchip temperature compensated bias-generators [30].", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "Alternative neuromorphic processors that use pure digital design styles typically timemultiplex the computing resources and are still faced with the problem of having to transfer state memory back and forth from areas devoted to computing to areas devoted to memory storage [4], [5].", "startOffset": 274, "endOffset": 277}, {"referenceID": 4, "context": "Alternative neuromorphic processors that use pure digital design styles typically timemultiplex the computing resources and are still faced with the problem of having to transfer state memory back and forth from areas devoted to computing to areas devoted to memory storage [4], [5].", "startOffset": 279, "endOffset": 282}, {"referenceID": 30, "context": "The CAM cells make use of NOR-type 9T circuits and of a pre-charge-high Match-Line scheme [31].", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "According to circuit simulation studies [32], the throughput of the R3 router can reach up to 1Gevents/sec in a 28 nm process.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 28, "endOffset": 31}, {"referenceID": 12, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 53, "endOffset": 56}, {"referenceID": 18, "context": "Therefore neural networks that combine several hidden layers of this form, known as deep neural networks (DNNs), can be efficiently trained to achieve very high performance on a wide range of visual and non-visual tasks [19].", "startOffset": 220, "endOffset": 224}, {"referenceID": 32, "context": "Recently, pre-trained CNNs and DNNs have been successfully mapped into spike-based hardware, achieving remarkable accuracy, while minimizing power consumption [33]\u2013[36].", "startOffset": 159, "endOffset": 163}, {"referenceID": 35, "context": "Recently, pre-trained CNNs and DNNs have been successfully mapped into spike-based hardware, achieving remarkable accuracy, while minimizing power consumption [33]\u2013[36].", "startOffset": 164, "endOffset": 168}, {"referenceID": 36, "context": "Furthermore, there are growing efforts towards the direct use of asynchronous event-based data produced by spiking sensors such as the DVS [37]\u2013[42].", "startOffset": 139, "endOffset": 143}, {"referenceID": 41, "context": "Furthermore, there are growing efforts towards the direct use of asynchronous event-based data produced by spiking sensors such as the DVS [37]\u2013[42].", "startOffset": 144, "endOffset": 148}, {"referenceID": 37, "context": "Here we tailored the design of the CNN for efficient real-time classification of event streams using an AER data-set recently made available [38].", "startOffset": 141, "endOffset": 145}, {"referenceID": 42, "context": "5 million events with a peak rate of slightly above 8 Meps [43].", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "Indeed, it has been recently shown [33], [35] that if these networks are designed appropriately, they can tolerate the limited precision that affects the analog circuits in the chip, while preserving high accuracy figures that are comparable to those obtained by state-of-the-art Deep Neural Networks (DNNs) running on hardware consuming orders of magnitude more power.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "Indeed, it has been recently shown [33], [35] that if these networks are designed appropriately, they can tolerate the limited precision that affects the analog circuits in the chip, while preserving high accuracy figures that are comparable to those obtained by state-of-the-art Deep Neural Networks (DNNs) running on hardware consuming orders of magnitude more power.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "This is evident if one analyzes the amount of resources required by comparable architectures, such as the one proposed in [4], for different types of CNNs architectures, as recently reported in [36].", "startOffset": 122, "endOffset": 125}, {"referenceID": 35, "context": "This is evident if one analyzes the amount of resources required by comparable architectures, such as the one proposed in [4], for different types of CNNs architectures, as recently reported in [36].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "13: Memory scaling comparison between the TrueNorth architecture [4] and this work.", "startOffset": 65, "endOffset": 68}, {"referenceID": 35, "context": "For TrueNorth, 4 data points (black dots) are extrapolated from CNN benchmark models described in [36] and fitted with a quadratic function (red curve).", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "(2), but adding 2 extra bits per neuron for 4 synaptic weight types as in [36].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "scale neural networks, without running into the von Neumann bottleneck problem [14], [15], and in other data communication bottleneck problems typical of centralized systems.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "scale neural networks, without running into the von Neumann bottleneck problem [14], [15], and in other data communication bottleneck problems typical of centralized systems.", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "Neuromorphic solutions that have optimized this overhead and, with it, the communication bandwidth, have sacrificed flexibility and programmability [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "On the other hand, solutions that have maximized network configurability, have sacrificed silicon real-estate for on-chip memory [4], or resorted to using external memory banks [5] (and therefore eliminated the advantage of having memory and computation co-localized).", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "On the other hand, solutions that have maximized network configurability, have sacrificed silicon real-estate for on-chip memory [4], or resorted to using external memory banks [5] (and therefore eliminated the advantage of having memory and computation co-localized).", "startOffset": 177, "endOffset": 180}, {"referenceID": 43, "context": "While there have been ad-hoc solutions proposed in the literature for utilizing off-chip memory more efficiently in neuromorphic systems [44], there is no consensus around a systematic approach that would explicitly allow to trade-off flexibility and memory to meet the requirements of specific applications.", "startOffset": 137, "endOffset": 141}, {"referenceID": 44, "context": "The trade-offs and design points we chose for the prototype device built to validate our approach were determined by the study of cortical networks in mammalian brains [45]\u2013[47].", "startOffset": 168, "endOffset": 172}, {"referenceID": 46, "context": "The trade-offs and design points we chose for the prototype device built to validate our approach were determined by the study of cortical networks in mammalian brains [45]\u2013[47].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "The use of on-chip heterogeneous memory structures makes this architecture ideal for exploiting the emerging memory structures based on nano-scale resistive memories [16], [48], [49].", "startOffset": 166, "endOffset": 170}, {"referenceID": 47, "context": "The use of on-chip heterogeneous memory structures makes this architecture ideal for exploiting the emerging memory structures based on nano-scale resistive memories [16], [48], [49].", "startOffset": 172, "endOffset": 176}, {"referenceID": 48, "context": "The use of on-chip heterogeneous memory structures makes this architecture ideal for exploiting the emerging memory structures based on nano-scale resistive memories [16], [48], [49].", "startOffset": 178, "endOffset": 182}, {"referenceID": 31, "context": "As scaling studies have demonstrated that such architecture can outperform the current state-of-theart systems when implemented using a 28 nm Fully-Depleted Silicon on Insulator (FDSOI) process [32], we are confident that such approach can lead to the design of a new generation of neuromorphic processors for solving a wide range of practical applications that require real-time processing of eventbased sensory signals, using ultra low-power, low latency, and compact systems.", "startOffset": 194, "endOffset": 198}], "year": 2017, "abstractText": "Neuromorphic computing systems comprise networks of neurons that use asynchronous events for both computation and communication. This type of representation offers several advantages in terms of bandwidth and power consumption in neuromorphic electronic systems. However, managing the traffic of asynchronous events in large scale systems is a daunting task, both in terms of circuit complexity and memory requirements. Here we present a novel routing methodology that employs both hierarchical and mesh routing strategies and combines heterogeneous memory structures for minimizing both memory requirements and latency, while maximizing programming flexibility to support a wide range of event-based neural network architectures, through parameter configuration. We validated the proposed scheme in a prototype multi-core neuromorphic processor chip that employs hybrid analog/digital circuits for emulating synapse and neuron dynamics together with asynchronous digital circuits for managing the address-event traffic. We present a theoretical analysis of the proposed connectivity scheme, describe the methods and circuits used to implement such scheme, and characterize the prototype chip. Finally, we demonstrate the use of the neuromorphic processor with a convolutional neural network for the real-time classification of visual symbols being flashed to a dynamic vision sensor (DVS) at high speed.", "creator": "LaTeX with hyperref package"}}}