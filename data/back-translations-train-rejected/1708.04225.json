{"id": "1708.04225", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Deep Object-Centric Representations for Generalizable Robot Learning", "abstract": "Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose a method where general purpose pretrained visual models serve as an object-centric prior for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine relevant objects from a few demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent meta-attention locates possible objects in the scene, and a task-specific attention identifies which objects are predictive of the demonstrations. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse relevant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.", "histories": [["v1", "Mon, 14 Aug 2017 17:42:59 GMT  (9157kb,D)", "http://arxiv.org/abs/1708.04225v1", null], ["v2", "Fri, 25 Aug 2017 00:14:15 GMT  (9157kb,D)", "http://arxiv.org/abs/1708.04225v2", null], ["v3", "Tue, 26 Sep 2017 17:06:36 GMT  (5898kb,D)", "http://arxiv.org/abs/1708.04225v3", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV", "authors": ["coline devin", "pieter abbeel", "trevor darrell", "sergey levine"], "accepted": false, "id": "1708.04225"}, "pdf": {"name": "1708.04225.pdf", "metadata": {"source": "CRF", "title": "Deep Object-Centric Representations for Generalizable Robot Learning", "authors": ["Coline Devin", "Pieter Abbeel", "Trevor Darrell", "Sergey Levine"], "emails": ["coline@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "trevor@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "In this paper, we explore the question of how to design a perception system for learning-based robotic manipulation while using the associated visual tasks without requiring extensive and time-consuming training of visual models for each new skill. The problem is that object-manipulation learning strategies require a vision that requires a vision. We assume that a small number of demonstrations are available."}, {"heading": "2 Related Work", "text": "Our method combines previous knowledge of \"objectivity\" from upstream visual models with an attention mechanism for recognizing specific task-related objects. A number of previous works have attempted to combine general object priority with more specific object recognition in the context of robotics and other visual tasks. Ekvall et al. used regional suggestions and SIFT features to quickly detect objects in the context of SLAM [3]. Previous work used an active search approach in which the camera could zoom into certain parts of the receptive field to search at higher resolutions [7]. In manipulation, SIFT features were also used to estimate objects and localize objects, collecting object-specific training data individually for the task [2, 20]. Similar to these previous works, our method limits the observations to object-centered foreseeing."}, {"heading": "3 Deep Visuomotor Object Centric Representations", "text": "The goal of our method is to provide a simple and efficient process for quickly acquiring task-specific visual representations in the context of political learning that we have previously learned. Specifically, we aim to compress an image observation into a vector so that a robotic policy can be easily learned with \u03bd as part of the observation space. To achieve this improvement, the compression process itself must include some prior knowledge of the visual world, and also drastically improve the generalization capabilities of the resulting policy if it is learned in scenes of limited diversity and visual variability. To achieve this improvement, we need to include some prior knowledge of the visual world. To this end, we impose an objective-centered structure of our representation that is learned even from prior visual data in the form of standard computer image datasets. We define a two-tiered hierarchy of attention across scenes for political learning tasks. The high-level tasks we call meta-attention for all tasks."}, {"heading": "3.1 Meta-Attention with Region Proposal Networks and Visual Features", "text": "Although a number of meta-attention mechanisms are possible, we use a Regional Proposal Network (RPN) [16] to provide meta-attention in our methodology. The RPN is a fully revolutionary and end-to-end trained method of object recognition, where the proposals are regressed and evaluated in parallel. The object suggestions o0,..., oN are the crops proposed by the RPN, and the position component g of oi is the boundary box coordinates of the proposal. We define the semantic component f as a mean pool above the regional suggestion profile of the revolutionary characteristics that were pre-trained in the ImageNet classification [17]. In the experiments, we use Conv5 by AlexNet [9], which results in a 256-dimensional feature vector for f (oi), which is then normalized to have strength 1. \"Since the evolutionary layers for the classification could have been multiplied with a data set."}, {"heading": "3.2 Learning Task-Specific Attention from Demonstrations", "text": "The learnable weights in local attention are the values of W, which relate to the visual characteristics of each crop. W should learn what types of objects are to be observed for a particular task. The aim of this section is to prepare W for demonstrations to take care of task-relevant objects. Once W is learned, any reinforcement learning algorithm could be used to obtain a policy as a function of the objects being handled. In a demonstration of a task, the objects relevant to the task will be predictable for future robot configurations. We optimize W as part of a larger neural network, which is displayed at the top of Figure 2, to predict the future movement of the final action position of the robot. The network for this consists of two hidden layers of 80 units each. To propagate backwards through W, soft attention is used. First, we use a Boltzman distribution to obtain a probability p (oi | w j) for each proposed object."}, {"heading": "3.3 Handpicking Task-Specific Attention", "text": "If no demonstration of a task is available, local attention can be specified by selecting the object hypothesis o for the task and storing its f (o). In policy learning, local attention will then take care of the object with the most similar f (o). Unlike the previous approach, this method can only balance each dimension of the attribute vectors equally. Therefore, we recommend using the handpicked features as initialization for the method in Section 3.2."}, {"heading": "4 Experiments", "text": "We evaluate our proposed object-centric model on several real robot manipulation tasks. Experiments are selected to evaluate two metrics: the reliability of this representation for robot learning and how it impacts on visual changes in the environment. By participating in features trained on the data diversity in ImageNet, we expect that strategies learned from our visual representation naturally have an impact on visual changes. Harsh attention to regional proposals should ensure robustness against distracting monsters that would repel properties from the object in a soft Argmax environment. The aim of this evaluation is to show that the model enables both political learning and generalization to new object instances and environments. Furthermore, we show that the scope of generalization can be controlled by the objects seen during the demonstrations. For each experiment, 6 kinesthetic demonstrations are provided by a human."}, {"heading": "4.1 Generalizing across visual changes", "text": "In fact, it is a very strange situation, in which most of us see ourselves able to change the world, and in which we see ourselves able to change the world, \"he said in an interview with the\" New York Times, \"in which he focused on the roles of the\" New York Times \"and the\" New York Times. \""}, {"heading": "4.2 Learning to ignore distractors", "text": "In the previous experiment, generalizing about cups was a desired result. However, it is easy to imagine that a task requires specific attention that must be poured specifically into the brown cup and all other cups must be ignored. Our method provides a simple method to optimize to which the task-specific attention responds sensitively. In order to achieve task-specific attention that has a smaller scope, a pink cup is added as a distraction maneuver during the demonstrations. At the test point, the pouring policy consistently selects the brown training mug over the pink mug and the black-and-white mug. This indicates that the inclusion of distractors in the demonstrations helps limit the scope of attention to ignore these distraction maneuvers. Figure 6a shows how an attention initiated only on the brown mug is distracted by the distraction mug. After fine-tuning the demonstrations, the attention is firmly focused on the 100% pink mug with either the brown one or the black one."}, {"heading": "4.3 Increasing the scope of generalization", "text": "Attention can also be drawn to more objects. For example, attention devoted to oranges cannot always be generalized to other citrus fruits. However, by adding a few demonstrations with limes and lemons, we show that the same attention can be finely tuned to focus on these additional fruits. As shown in Figure 6b, attention is only focused on oranges when they are first initialized, but fine-tuning widens the scope of attention to include the limes and lemons present in the demonstration data. The resulting far-reaching policy is robust to disruptors such as an apple, an apricot, and a violet cup, but is confused by the orange and green cups. The round base of the citrus cups may seem to be idealized fruits."}, {"heading": "5 Discussion", "text": "In this paper, we have proposed a visual representation for learning robotic skills that allows the use of object-centric priors from pre-formed visual models to achieve robust perception for strategies trained in a single scene with a single object. Our approach uses regional suggestion networks as meta-attention, selecting potential objects in the scene regardless of the task, and then quickly selecting a task-specific representation through an attention mechanism based on visual features that can be trained from a few demonstrations. Since the visual features used for object suggestions are themselves invariant to differences in lighting, appearance, viewing angle and object instance, the resulting vision system can be effectively generalized to new object instances without additional training."}, {"heading": "Acknowledgments", "text": "Coline Devin is supported by Huawei Technologies and is an NSF Graduate Research Fellow. We would like to thank Ronghang Hu for providing a tensor flow port from Region Proposal Networks."}], "references": [{"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["P. Agrawal", "A.V. Nair", "P. Abbeel", "J. Malik", "S. Levine"], "venue": "Advances in Neural Information Processing Systems, pages 5074\u20135082,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Object recognition and full pose registration from a single image for robotic manipulation", "author": ["A. Collet", "D. Berenson", "S.S. Srinivasa", "D. Ferguson"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on, pages 48\u201355. IEEE,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Object detection and mapping for service robot tasks", "author": ["S. Ekvall", "D. Kragic", "P. Jensfelt"], "venue": "Robotica, 25(2):175\u2013187,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep visual foresight for planning robot motion", "author": ["C. Finn", "S. Levine"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["C. Finn", "X.Y. Tan", "Y. Duan", "T. Darrell", "S. Levine", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512\u2013519. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep predictive policy training using reinforcement learning", "author": ["A. Ghadirzadeh", "A. Maki", "D. Kragic", "M. Bj\u00f6rkman"], "venue": "arXiv preprint arXiv:1703.00727,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Quick 3d object detection and localization by dynamic active search with multiple active cameras", "author": ["T. Kawanishi", "H. Murase", "S. Takagi"], "venue": "Pattern Recognition, 2002. Proceedings. 16th International Conference on, volume 2, pages 605\u2013608. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, pages 1071\u20131079,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "International Symposium on Experimental Robotics (ISER 2016),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Altun"], "venue": "AAAI,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 3406\u20133413. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y.-L. Park", "A. Gupta"], "venue": "European Conference on Computer Vision, pages 3\u201318. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Advances in neural information processing systems, pages 91\u201399,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in neural information processing systems, pages 568\u2013576,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A textured object recognition pipeline for color and depth image data", "author": ["J. Tang", "S. Miller", "A. Singh", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 3467\u20133474. IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "JMLR, 11,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "We describe an object-centric approach to robot learning, where the object prior is provided by region proposal networks [16] and object information is provided by high level features from AlexNet [9].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "We describe an object-centric approach to robot learning, where the object prior is provided by region proposal networks [16] and object information is provided by high level features from AlexNet [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "used region proposals and SIFT features for quickly detecting objects in the context of SLAM [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Prior work used an active search approach where the camera could zoom in certain parts of the receptive field to search at higher resolutions [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In manipulation, SIFT features have also been used for 3D pose estimation and object localization, using object-specific training data gathered individually for the task [2, 20].", "startOffset": 170, "endOffset": 177}, {"referenceID": 19, "context": "In manipulation, SIFT features have also been used for 3D pose estimation and object localization, using object-specific training data gathered individually for the task [2, 20].", "startOffset": 170, "endOffset": 177}, {"referenceID": 10, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 132, "endOffset": 144}, {"referenceID": 14, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 132, "endOffset": 144}, {"referenceID": 13, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 132, "endOffset": 144}, {"referenceID": 5, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 199, "endOffset": 205}, {"referenceID": 4, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 199, "endOffset": 205}, {"referenceID": 0, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 237, "endOffset": 243}, {"referenceID": 3, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 237, "endOffset": 243}, {"referenceID": 13, "context": "Some methods have sought to address this by collecting large amounts of data with many objects [14, 12].", "startOffset": 95, "endOffset": 103}, {"referenceID": 11, "context": "Some methods have sought to address this by collecting large amounts of data with many objects [14, 12].", "startOffset": 95, "endOffset": 103}, {"referenceID": 18, "context": "Such features have previously been shown to transfer effectively across visual perception tasks and provide a good general-purpose visual representation [19, 18].", "startOffset": 153, "endOffset": 161}, {"referenceID": 17, "context": "Such features have previously been shown to transfer effectively across visual perception tasks and provide a good general-purpose visual representation [19, 18].", "startOffset": 153, "endOffset": 161}, {"referenceID": 15, "context": "Although a number of meta-attention mechanisms are possible, we use a region proposal network (RPN) [16] to provide meta-attention in our method.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "We define the semantic component f to be a mean-pool over the region proposal crop of the convolutional features pretrained on ImageNet classification [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "In the experiments, we use conv5 of AlexNet [9], resulting in a 256-dimensional feature vector for f (oi), which is then normalized to have magnitude 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "The network is optimized with the Adam optimizer [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "To learn to perform the task, we use the guided policy search algorithm [10], which involves training local time-varying linear-Gaussian controllers for a number of different instances of the task, which in our case correspond to different positions of the objects, and then using supervised learning to learn a single global neural network policy that can perform the task for all of the different object positions.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "Note that our method can be used with any reinforcement learning algorithm, including both deep reinforcement learning methods (such as the one used in our experiments) and trajectory-centric reinforcement learning algorithms such as PI2 [21] or REPS [13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 12, "context": "Note that our method can be used with any reinforcement learning algorithm, including both deep reinforcement learning methods (such as the one used in our experiments) and trajectory-centric reinforcement learning algorithms such as PI2 [21] or REPS [13].", "startOffset": 251, "endOffset": 255}, {"referenceID": 4, "context": "while previous methods [5] are able to learn this task for the particular mug seen during training, our method can generalize to new mug instances and to cluttered environments.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "We compare to the method described in [11], where policies are learned directly from raw pixels and pretrained on a labeled data for detecting the target object.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "An approach that learns robot skills directly from pixels such as [12] could not be expected to know that the brown mug and the pink mug are similar objects.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "We investigate this by training a policy from raw pixels with the architecture described in [12].", "startOffset": 92, "endOffset": 96}], "year": 2017, "abstractText": "Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose a method where general purpose pretrained visual models serve as an object-centric prior for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine relevant objects from a few demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent meta-attention locates possible objects in the scene, and a task-specific attention identifies which objects are predictive of the demonstrations. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse relevant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.", "creator": "LaTeX with hyperref package"}}}