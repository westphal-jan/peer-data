{"id": "1708.04236", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Motion Planning under Partial Observability using Game-Based Abstraction", "abstract": "We study motion planning problems where agents move inside environments that are not fully observable and subject to uncertainties. The goal is to compute a strategy for an agent that is guaranteed to satisfy certain safety and performance specifications. Such problems are naturally modelled by partially observable Markov decision processes (POMDPs). Because of the potentially huge or even infinite belief space of POMDPs, verification and strategy synthesis is in general computationally intractable. We tackle this difficulty by exploiting typical structural properties of such scenarios; for instance, we assume that agents have the ability to observe their own positions inside an environment. Ambiguity in the state of the environment is abstracted into non-deterministic choices over the possible states of the environment. Technically, this abstraction transforms POMDPs into probabilistic two-player games (PGs). For these PGs, efficient verification tools are able to determine strategies that approximate certain measures on the POMDP. If an approximation is too coarse to provide guarantees, an abstraction refinement scheme further resolves the belief space of the POMDP. We demonstrate that our method improves the state of the art by orders of magnitude compared to a direct solution of the POMDP.", "histories": [["v1", "Mon, 14 Aug 2017 15:49:21 GMT  (102kb)", "http://arxiv.org/abs/1708.04236v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["leonore winterer", "sebastian junges", "ralf wimmer", "nils jansen", "ufuk topcu", "joost-pieter katoen", "bernd becker"], "accepted": false, "id": "1708.04236"}, "pdf": {"name": "1708.04236.pdf", "metadata": {"source": "CRF", "title": "Motion Planning under Partial Observability using Game-Based Abstraction", "authors": ["Leonore Winterer", "Sebastian Junges", "Ralf Wimmer", "Nils Jansen", "Ufuk Topcu", "Bernd Becker"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves."}, {"heading": "II. FORMAL FOUNDATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Probabilistic games", "text": "For a finite or countable infinite quantity X, let's develop strategies: X \u2192 [0, 1] any number of available actions (PSD), so that \"x-X\" (x) = 1 is a probability distribution over X and \"dist\" (X) is the set of all probability distributions over X. The Dirac distribution is a tuple G = (S1, S2, sinit, Act, P), where S = S1 (y) = 0 for y 6 = x.Definition 1 (Probabilistic game) A probability game (PG) is a tuple G = (S1, S2, sinit, Act, P), where S = S1 (S2) is an infinite set of states, S1 is the states of player 1, S2 the states of player 2, sinit (S) is the initial state, Act is a finite set of actions, and P: S \u00b7 Act 7 \u2192 This is a (partial) probable transitional function."}, {"heading": "B. Partial observability", "text": "In this case, the underlying model is a partially observable Markov decision process. Definition 3 (POMDP) A partially observable Markov decision process (POMDP) is a tuple D = (M, O, \u03bb) in such a way that M = (S, sinit, Act, P) is the underlying MDP decision process, O is a finite series of observations, and \u03bb: S \u2192 O is the observation function. We demand that states with the same observations have the same set of enabled measures, i. e., \u03bb (s) = (s) implies an act (s) = a finite set of observations. More general observation functions MDP have been considered in the literature, with the last action and provision of adistrication via O. There is a polynomial transformation of the general case to the POPs."}, {"heading": "C. Specifications", "text": "Considering a group G'S of target states and a group B'S of bad states, we consider quantitative reach avoidance properties of the form \u0430 = P > p (\u00ac B U G). The specification is met by a PG if player 1 has a strategy that is at least p probable for all player 2 strategies to reach a target state without reaching a bad state in between. In POMDPs, unthinking deterministic strategies are sufficient to prove or refute the satisfaction of such specifications [31]. In POMDPs, observation-based strategies are necessary in their full universality [32]."}, {"heading": "III. METHODOLOGY", "text": "We first intuitively describe the problem and list the assumptions we make. After formalizing the setting, we present a formal problem. We present the intuition behind the concept of game-based abstraction for MDPs, as applied to POMDPs, and determine the correctness of our method."}, {"heading": "A. Problem Statement", "text": "One agent is controllable (Agent 0), the other agents (also called adversaries) move stochastically according to a fixed, randomized strategy based on their own location and the location of Agent 0. We assume that all agents move alternately. An agent's position defines the physical location within the world as well as additional characteristics such as the orientation of the agent. A diagram models all possible movements of an agent between positions that are referred to as an agent's world chart. Therefore, the nodes in the diagram clearly refer to positions, while several nodes can refer to the same physical location in the world. We require that the diagram contain no blockages: for each position, there is at least one edge in the diagram corresponding to a possible action that an agent can execute. A collision occurs when Agent 0 shares his position with another agent."}, {"heading": "B. Formal setting", "text": "We define initially an individual world diagram for each agent egg \u2264 egg \u2264 n over each position of agent i. We define initially an individual world diagram for each agent i. We define an individual world diagram for each agent i. The world diagrams Gi for agent via Loc are a tuple Gi = (Vi) DP = (Vi), egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg, egg"}, {"heading": "C. Abstraction", "text": "We propose an abstraction method for the world POMDPs based on a game-based abstraction; < GBAR), originally defined for MDPs [10], [11]. GBAR for MDPs: For an MDP M = (S, sinit, Act, P), we assume a partition. GBAR takes the partition and turns each block into an abstract state Bi; these blocks form the Player 1 states. Then Act (Bi) = s-Bi-Act (s) with ki = 1 Bi = S. GBAR takes each block into an abstract state Bi; these blocks form the Player 1 states."}, {"heading": "D. Refinement of the PG", "text": "In the GBAR approach described above, we will extract relevant information for an optimal strategy. Specifically, the behavior of Agent 1 (the opponent) is amplified (approached): \u2022 We abstract probabilistic movements from Agent 1 outside the visible range into non-determinism. \u2022 We allow leaps into Agent 1's movements, i.e., Agent 1 can change its position in the PG. This is impossible in the POMDP; these movements become traces. In our construction, due to the lack of this information, we cannot find a sure strategy, the abstraction needs to be refined. In GBAR for MDPs [11], abstract states are divided heuristically, yielding an approximation across borders. In our construction, we cannot arbitrarily split abstract states: this would destroy the one-on-one correspondence between abstract states and observations."}, {"heading": "E. Refinement of the Graph", "text": "The proposed approach cannot solve every scenario - the problem is undecidable [12]. Therefore, if the method does not find a p-safe scheduler, we do not know whether such a scheduler exists. However, with increased visibility, the maximum level of safety does not decrease either in the POMDP or in the PG. To identify good spots for increased visibility, we can use the analysis results: places where a collision occurs are most likely good candidates."}, {"heading": "IV. CASE STUDY AND IMPLEMENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Description", "text": "For our experiments we choose the following scenario: A (controllable) robot R and a vacuum cleaner VC move in a two-dimensional grid world with statically opaque obstacles. Neither R nor VC are allowed to leave the grid or visit grid cells occupied by a static obstacle. The position of R contains the cell CR (the location) and a wind direction. R can move a grid cell forward or rotate 90 \u00b0 in both directions without changing its location. The position of VC is determined exclusively by its cell CVC. At each step VC can shift a cell in any wind direction. We assume that VC moves with equal probability to all available successor cells. The sensors on R perceive VC only within a viewing range r around the CR. Specifically: VC is visible when VC is within the range of vision of CR \u2212 CVC and there is no grid cell with a static obstacle on the straight line from the center of the CR to the center of the CVC, which means that VC is located behind the camera and that VC can observe the position of the camera."}, {"heading": "B. Tool-Chain", "text": "In order to synthesize strategies for the scenario described above, we have implemented a toolchain in Python: The input consists of the grid with the positions of all obstacles, the location of the cameras and the viewing area. As output, two PRISM files are created: A PG formulation of the abstraction, including a one-step refinement of the history to be analyzed by PRISM games [9], and the original POMDP for PRISM-Pomp [13]. For multi-level refinement of the history, additional regions can be defined. PG encoding includes a pre-calculated lookup table for the visibility relationship. PG is described by two parallel processes: one for player 1 and one for player 2. Since only R can make decisions, they are listed in player 1 actions, while the moves of VC are stored in player 2 actions. Specifically, the process for R contains its location, and the process for VC contains either its location or a remote value."}, {"heading": "V. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experimental Setup", "text": "All the experiments were carried out on a computer with a 3.6 GHz Intel R \u00a9 CoreTM i7-4790 CPU and 16 GB of RAM running Ubuntu Linux 16.04. We designated experiments that use more than 5400 s of CPU time as a timeout and 10 GB of memory as a meme-out (MO). We considered several variants of the scenario described in IV-A. The robot always started in the upper left corner and had the lower right corner as the target. The VC started in the lower right corner. In all variants, the visual range was 3. We evaluated the following five scenarios: SC1 rooms of different sizes without obstacles. SC2 rooms of different sizes with a cross-shaped obstacle in the middle, which is scaled with increasing grid size. SC3 A 25 x 25 space with up to 70 randomly placed obstacles. SC4 Two rooms (together 10 x 20), as shown in Fig. 2. The door connecting the two rooms to each other, is a potential point of view of the cameras by not improving the upper side of the R x, the ability to the upper side of the camera."}, {"heading": "B. Results", "text": "Table I shows the direct comparison between the POMDP description and the abstraction for SC1. The first column indicates the grid size. Subsequently, the table lists the number of states, non-deterministic decisions, and transitions of the model, first for the POMDP and then for the PG. Results include the safety probability induced by the optimal scheduler (\"result\"), the runtime (all in seconds), the PRISM for the construction of the state space from the symbolic description (\"model time\"), and finally the time to resolve the POMDP / PG (\"sol time\"). The last column shows the safety probability calculated using the fully observable MDP; it is an upper limit for the probability that is achievable for each grid. Note: Optimal schedulers from this MDP are generally not observation-based and therefore for the POMDP (\"so. time of recovery of the PRISM files is identical in all cases to the second phase GSD constructed data)."}, {"heading": "C. Evaluation", "text": "Consider SC1: while PRISM pomp delivers results on very small examples within a reasonable amount of time, the 6 \u00d7 6 grid already delivers a meme-out."}, {"heading": "10 297686 581135 1093201 0.9976 2.10 89.7 285.0 0.9999", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "40 234012 454652 823410 0.9706 2.74 87.3 179.1 0.9999", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "60 198927 385803 679321 0.6476 3.12 59.4 201.5 0.9999", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "70 187515 363401 633884 0.6210 3.30 59.4 116.1 0.9896", "text": "The probability of safety is lower for small grids, as there is less room for R to avoid VC, and there are relatively more situations where R is trapped in a corner or against a wall. Note that for the MDP, the state space for an n \u00d7 n grid in O (n4) could not be calculated as an upper limit for the 50 \u00d7 50 grid, as the construction of the state space is a mem-out, compared to a state space in O (r2n2) for the PG in which r is the field of vision. In Table II, for the SC5 benchmarks, we see that the safety probability for grids with a longer corridor goes down. This is because, in abstraction, the robot can hit the VC several times as it drives down the corridor."}, {"heading": "VI. DISCUSSION", "text": "By adding an opponent who assumes the worst-case state, a PG is obtained. In general, this transforms the POMDP at hand into a partially observable PG that remains insoluble. However, the division according to observed equivalences can lead to a fully observable PG. PGs can be analyzed by black box algorithms as implemented, e.g. in PRISM games, which also provide an optimal timetable. However, the strategy from the PG can be applied to the POMDP, which yields the actual (higher) level of security. In general, the abstraction can be too coarse, as has been successfully demonstrated in the above examples that game-based abstraction is not too coarse when making some assumptions about the POMDP. These assumptions are often of course fulfilled by motion planning scenarios. The assumptions from Sect. III-A can be loosened in several respects."}, {"heading": "VII. CONCLUSION", "text": "Experiments show that this approach is promising. In future work, we will expand our approach to a broader class of POMDPs and improve the refinement steps, including an automatic refinement loop."}], "references": [{"title": "Dynamic Programming and Markov Processes, 1st ed", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1960}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "vol. 101, no. 1, pp. 99\u2013134, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Control of probabilistic systems under dynamic, partially known environments with temporal logic specifications.", "author": ["T. Wongpiromsarn", "E. Frazzoli"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A survey of point-based POMDP solvers", "author": ["G. Shani", "J. Pineau", "R. Kaplow"], "venue": "Autonomous Agents and Multi-Agent Systems, vol. 27, no. 1, pp. 1\u201351, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "The probabilistic model checking landscape.", "author": ["J.-P. Katoen"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "PRISM 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "vol. 6806, 2011, pp. 585\u2013591.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "A storm is coming: A modern probabilistic model checker", "author": ["C. Dehnert", "S. Junges", "J.-P. Katoen", "M. Volk"], "venue": "CoRR, vol. abs/1702.04311, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "PRISM-games: A model checker for stochastic multi-player games", "author": ["T. Chen", "V. Forejt", "M.Z. Kwiatkowska", "D. Parker", "A. Simaitis"], "venue": "vol. 7795, 2013, pp. 185\u2013191.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Verification and refutation of probabilistic specifications via games", "author": ["M. Kattenbelt", "M. Huth"], "venue": "ser. LIPIcs, vol. 4. Schloss Dagstuhl, 2009, pp. 251\u2013262.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A game-based abstraction-refinement framework for Markov decision processes", "author": ["M. Kattenbelt", "M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "vol. 36, no. 3, pp. 246\u2013280, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "What is decidable about partially observable Markov decision processes with \u03c9-regular objectives", "author": ["K. Chatterjee", "M. Chmel\u0131\u0301k", "M. Tracol"], "venue": "vol. 82, no. 5, pp. 878\u2013911, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Verification and control of partially observable probabilistic systems", "author": ["G. Norman", "D. Parker", "X. Zou"], "venue": "Real-Time Systems, vol. 53, no. 3, pp. 354\u2013402, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Scaling up Gaussian belief space planning through covariance-free trajectory optimization and automatic differentiation", "author": ["S. Patil", "G. Kahn", "M. Laskey", "J. Schulman", "K. Goldberg", "P. Abbeel"], "venue": "Algorithmic Foundations of Robotics XI, ser. Springer Tracts in Advanced Robotics, vol. 107, 2014, pp. 515\u2013533.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Sampling-based motion planning with sensing uncertainty.", "author": ["B. Burns", "O. Brock"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Rapidly-exploring random belief trees for motion planning under uncertainty.", "author": ["A. Bry", "N. Roy"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Control in belief space with temporal logic specifications", "author": ["C.-I. Vasile", "K. Leahy", "E. Cristofalo", "A. Jones", "M. Schwager", "C. Belta"], "venue": "2016, pp. 7419\u20137424.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Randomized belief-space replanning in partiallyobservable continuous spaces", "author": ["K. Hauser"], "venue": "Algorithmic Foundations of Robotics IX, 2010, pp. 193\u2013209.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Closed-loop belief space planning for linear, Gaussian systems.", "author": ["M.P. Vitus", "C.J. Tomlin"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Extending the applicability of POMDP solutions to robotic tasks", "author": ["D.K. Grady", "M. Moll", "L.E. Kavraki"], "venue": "IEEE Trans. Robotics, vol. 31, no. 4, pp. 948\u2013961, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "The verification of probabilistic systems under memoryless partial-information policies is hard", "author": ["L. de Alfaro"], "venue": "DTIC Document, Tech. Rep., 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Qualitative analysis of POMDPs with temporal logic specifications for robotics applications", "author": ["K. Chatterjee", "M. Chmel\u0131\u0301k", "R. Gupta", "A. Kanodia"], "venue": "2015, pp. 325\u2013330.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal cost almost-sure reachability in POMDPs", "author": ["\u2014\u2014"], "venue": "vol. 234, pp. 26\u201348, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Discretized approximations for POMDP with average cost", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "UAI. AUAI Press, 2004, p. 519.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Verification of partial-information probabilistic systems using counterexample-guided refinements", "author": ["S. Giro", "M.N. Rabe"], "venue": "vol. 7561, 2012, pp. 333\u2013348.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Assume-guarantee reasoning framework for MDP-POMDP.", "author": ["X. Zhang", "B. Wu", "H. Lin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Counterexample-guided abstraction refinement for POMDPs", "author": ["\u2014\u2014"], "venue": "CoRR, vol. abs/1701.06209, 2017.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "Partial-observation stochastic games: How to win when belief fails", "author": ["K. Chatterjee", "L. Doyen"], "venue": "vol. 15, no. 2, pp. 16:1\u201316:44, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantitative verification and strategy synthesis for stochastic games", "author": ["M. Svorenov\u00e1", "M. Kwiatkowska"], "venue": "Eur. J. Control, vol. 30, pp. 15\u201330, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Principles of Model Checking", "author": ["C. Baier", "J.-P. Katoen"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The complexity of stochastic games", "author": ["A. Condon"], "venue": "Inf. Comput., vol. 96, no. 2, pp. 203\u2013224, 1992.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1992}, {"title": "Introduction to Stochastic Dynamic Programming", "author": ["S.M. Ross"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1983}, {"title": "Magnifying-lens abstraction for Markov decision processes", "author": ["L. de Alfaro", "P. Roy"], "venue": "vol. 4590. Springer, 2007, pp. 325\u2013338.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Offline motion planning for dynamical systems with uncertainties aims at finding a strategy for an agent that ensures certain desired behavior [1].", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "For many robotic applications, however, information about the current state of the environment is not observable [2], [3], [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "For many robotic applications, however, information about the current state of the environment is not observable [2], [3], [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "The belief state together with an update function form a (possibly infinite) MDP, commonly referred to as the underlying belief MDP [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Quantitative verification techniques like probabilistic model checking [6] provide comprehensive guarantees on such a strategy.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "For finite MDPs, tools like PRISM [7] or storm [8] employ efficient model checking algorithms to asses the probability to reach a certain set of states.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "For finite MDPs, tools like PRISM [7] or storm [8] employ efficient model checking algorithms to asses the probability to reach a certain set of states.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "addition to the choices of the agent: The POMDP abstraction results in a probabilistic two-player game (PG) [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "The automated abstraction procedure is inspired by gamebased abstraction [10], [11] of potentially infinite MDPs, where states are lumped in a similar fashion.", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "The automated abstraction procedure is inspired by gamebased abstraction [10], [11] of potentially infinite MDPs, where states are lumped in a similar fashion.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "As we target an undecidable problem [12], our approach is not complete in the sense that it does not always obtain a strategy which yields the required optimal probability.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "model checking using [9] on PG on POMDP", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "The tool-chain uses PRISM-games [9] as a model checker for PGs.", "startOffset": 32, "endOffset": 35}, {"referenceID": 11, "context": "For the motion planning scenario considered, our preliminary results indicate an improvement of orders in magnitude over the state of the art in POMDP verification [13].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "An overview on point-based value iteration for POMDPs is given in [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Other methods employ control techniques to synthesize strategies with safety considerations under observation and dynamics noise [2], [18], [19].", "startOffset": 129, "endOffset": 132}, {"referenceID": 16, "context": "Other methods employ control techniques to synthesize strategies with safety considerations under observation and dynamics noise [2], [18], [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "Other methods employ control techniques to synthesize strategies with safety considerations under observation and dynamics noise [2], [18], [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "Preprocessing of POMDPs in motion planning problems for robotics is suggested in [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "General verification problems for POMDPs and their decidability have been studied in [21], [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "General verification problems for POMDPs and their decidability have been studied in [21], [22].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "A recent survey about decidability results and algorithms for \u03c9-regular properties is given in [12], [23].", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "A recent survey about decidability results and algorithms for \u03c9-regular properties is given in [12], [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "The probabilistic model checker PRISM has recently been extended to support POMDPs [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "Partly based on the methods from [24], it produces lower and upper bounds for a variety of queries.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "In [25], an iterative refinement is proposed to solve POMDPs: Starting with total information, strategies that depend on unobservable information are excluded.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [26], a compositional framework for reasoning about POMDPs is introduced.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "Refinement based on counterexamples is considered in [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "Partially observable probabilistic games have been considered in [28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "Finally, an overview of applications for PGs is given in [29].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "For a finite or countably infinite set X , let \u03bc : X \u2192 [0, 1] such that \u2211", "startOffset": 55, "endOffset": 61}, {"referenceID": 28, "context": "A strategy \u03c3 for a PG resolves all non-deterministic choices, yielding an induced MC, for which a probability measure over the set of infinite paths is defined by the standard cylinder set construction [30].", "startOffset": 202, "endOffset": 206}, {"referenceID": 1, "context": "For many applications, not all system states are observable [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 21, "context": "There is a polynomial transformation of the general case to the POMDP definition used here [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "A formal treatment of belief MDPs is beyond the scope of this paper, for details see [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 29, "context": "For MDPs and PGs, memoryless deterministic strategies suffice to prove or disprove satisfaction of such specifications [31].", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "For POMDPs, observation-based strategies in their full generality are necessary [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": ", Gn, a set of collision states Collision, and a set of goal states Goals, an observationbased strategy \u03c3 \u2208 \u03a3 D for D is p-safe for p \u2208 [0, 1], if", "startOffset": 136, "endOffset": 142}, {"referenceID": 0, "context": "We want to compute a psafe strategy for a given p \u2208 [0, 1].", "startOffset": 52, "endOffset": 58}, {"referenceID": 8, "context": "We propose an abstraction method for world POMDPs that builds on game-based abstraction (GBAR), originally defined for MDPs [10], [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "We propose an abstraction method for world POMDPs that builds on game-based abstraction (GBAR), originally defined for MDPs [10], [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "For the classes of properties we consider, a memoryless deterministic strategy suffices for PGs to achieve the maximal probability of reaching a goal state without collision [31].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "In GBAR for MDPs [11], abstract states are split heuristically, yielding a finer over-approximation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 31, "context": "c) Region-based multi-step history refinement: As the refinement above blows up the state space drastically, we utilize a technique called magnifying lens abstraction [33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "The proposed approach cannot solve every scenario \u2014 the problem is undecidable [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "As output, two PRISM files are created: A PG formulation of the abstraction including one-step history refinement, to be analyzed using PRISM-games [9], and the original POMDP for PRISM-pomdp [13].", "startOffset": 148, "endOffset": 151}, {"referenceID": 11, "context": "As output, two PRISM files are created: A PG formulation of the abstraction including one-step history refinement, to be analyzed using PRISM-games [9], and the original POMDP for PRISM-pomdp [13].", "startOffset": 192, "endOffset": 196}], "year": 2017, "abstractText": "We study motion planning problems where agents move inside environments that are not fully observable and subject to uncertainties. The goal is to compute a strategy for an agent that is guaranteed to satisfy certain safety and performance specifications. Such problems are naturally modeled by partially observable Markov decision processes (POMDPs). Because of the potentially huge or even infinite belief space of POMDPs, verification and strategy synthesis is in general computationally intractable. We tackle this difficulty by exploiting typical structural properties of such scenarios; for instance, we assume that agents have the ability to observe their own positions inside an environment. Ambiguity in the state of the environment is abstracted into non-deterministic choices over the possible states of the environment. Technically, this abstraction transforms POMDPs into probabilistic two-player games (PGs). For these PGs, efficient verification tools are able to determine strategies that approximate certain measures on the POMDP. If an approximation is too coarse to provide guarantees, an abstraction refinement scheme further resolves the belief space of the POMDP. We demonstrate that our method improves the state of the art by orders of magnitude compared to a direct solution of the POMDP.", "creator": "LaTeX with hyperref package"}}}