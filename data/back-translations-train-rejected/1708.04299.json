{"id": "1708.04299", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks", "abstract": "While there have been significant advances in detecting emotions from speech and image recognition, emotion detection on text is still under-explored and remained as an active research field. This paper introduces a corpus for text-based emotion detection on multiparty dialogue as well as deep neural models that outperform the existing approaches for document classification. We first present a new corpus that provides annotation of seven emotions on consecutive utterances in dialogues extracted from the show, Friends. We then suggest four types of sequence-based convolutional neural network models with attention that leverage the sequence information encapsulated in dialogue. Our best model shows the accuracies of 37.9% and 54% for fine- and coarse-grained emotions, respectively. Given the difficulty of this task, this is promising.", "histories": [["v1", "Mon, 14 Aug 2017 20:01:44 GMT  (998kb,D)", "http://arxiv.org/abs/1708.04299v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sayyed m zahiri", "jinho d choi"], "accepted": false, "id": "1708.04299"}, "pdf": {"name": "1708.04299.pdf", "metadata": {"source": "CRF", "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks", "authors": ["Sayyed M. Zahiri", "Jinho D. Choi"], "emails": ["sayyed.zahiri@emory.edu", "jinho.choi@emory.edu"], "sections": [{"heading": "1 Introduction", "text": "A variety of research has been conducted to identify emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010). The recent advent of natural language processing and machine learning has made the task of recognizing emotions in text possible, but since emotions are not necessarily transferred to text, quantifying different types of emotions that consist solely of text is generally a challenge. Another difficult aspect of this task is the lack of annotated datasets. There are few publicly available datasets (Strapparava and Mihalcea, 2008; Mohammad and Bravo-Marquez, 2017)."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Text-based Emotion Detection", "text": "Text-based emotion recognition is at an early stage of natural language processing, although it has attracted a lot of attention lately. There are three common methods that researchers have used to detect emotions from texts: keyword-based, learning-based, and hybrid methods from these two. In the first method, the classification is done using emotional keywords. Strapparava et al. (2004) categorized emotions by mapping keywords in sentences to lexical representations of affective concepts. Chaumartin (2007) did the emotion detection on news headlines. The performance of these keyword-based approaches was rather unsatisfactory due to the fact that the semantics of keywords strongly depend on the contexts, and it is significantly influenced by the absence of these keywords (Shaheen et al., 2014) Two types of machine-learning approaches were used for the second method, supervised approaches, in which training examples are used to classify statistical and non-classifiable categories in documents."}, {"heading": "2.2 Sequence Modeling on Dialogue", "text": "Shi et al. (2016) proposed the multi-channel CNN for cross-language dialogue tracking. Dufour et al. (2016) introduced a theme model that takes into account all the information contained in sub-dialogues in order to track the states of dialogue introduced by the DSTC5 challenge (Kim et al., 2016). Stolcke et al. (2000) proposed a statistical approach to modelling the dialogue act through human phone conversations."}, {"heading": "2.3 Attention in Neural Networks", "text": "The attention mechanism is widely used in the field of computer vision (Mnih et al., 2014; Xu et al., 2015) and has recently become very popular in the processing of natural language. In particular, the inclusion of the attention mechanism has reached a peak in machine translation and in answering questions (Bahdanau et al., 2014; Hermann et al., 2015; dos Santos et al., 2016). Our attention mechanism differs from previous work, which focused attention on two statistical embeddings, while our approach focuses attention on static and dynamically generated embeddings."}, {"heading": "3 Corpus", "text": "The Character Mining Project provides transcripts of the TV series Friends; transcripts of all the Sea Sons of the show are available to the public on JSON.1 Each season consists of episodes, each episode contains scenes, each scene contains utterances in which each utterance gives information to the narrator. To this end, we take transcripts from the first four seasons and create a corpus by adding another layer of annotations with emotions. As a result, our corpus includes 97 episodes, 897 scenes and 12,606 expressions in which each utterance is commented on with one of the seven emotions borrowed from the six primary emotions of the Emotional Wheel Willcox (1982): sad, crazy, scared, powerful, peaceful, joyful and neutral by default. Table 1 describes a scene with seven utterances and their corresponding annotations from crowdsourcing."}, {"heading": "3.1 Crowdsourcing", "text": "Our commentary tasks are performed on the Amazon Mechanical Turk. Each MTurk HIT shows a scene in which each utterance in the scene is commented on by four crowdworkers, who are asked to select the most relevant emotions associated with that utterance. To assign a suitable budget to each HIT, the corpus is divided into four groups, with all scenes in each group limited to the number of utterances [5, 10), [11, 15, 15), [15, 20], [20, 25] and budgeted at 10, 13, 17, 20 cents per HIT or. Each HIT takes an average of about 2.5 minutes, and the total annotation costs about $680. The annotation quality for 20% of each HIT is manually checked, and those with poor quality are commented again."}, {"heading": "3.2 Inter-Annotator Agreement", "text": "Two types of measurements are used to evaluate the concordance between the commentators (Table 3). First, Cohen's kappa is used to measure the concordance between two commentators, while Fleiss'kappa is used for three and four commentators. Second, the partial concordance (a concordance between two commentators) is measured to illustrate the improvement1nlp.mathcs.emory.edu / character-miningfrom less to a larger number of commentators. However, among all types of commentary groups, kappa values of about 14% are achieved. Such low values are more likely because emotion detection is highly subjective, so commentators often judge different emotions, which are all acceptable for the same expression. This can also be attributed to the limitation of our dataset, whereas a higher kappa value of about 14% could be achieved if the commentators find a neutral annotator (e.g. text, speech, image)."}, {"heading": "3.3 Voting and Ranking", "text": "In view of the quadruple remark, we first divide the data set into five folds (Table 4): for the first three folds, the remark based on the majority decision is considered a gold label (a1); the least absolute error (LAE) is then measured for each annotator by comparing its remark with the gold labels; for the last two folds, the remark generated by the annotator with the minimum LAE is selected as gold, which is reasonable, as these annotators generally produce higher quality annotations; with this scheme, 75.5% of the data set can be intentionally labeled with gold from the vote, and the rest can be ranked by order."}, {"heading": "3.4 Analysis", "text": "Table 5 shows the distribution of all emotions in our corpus. Together, the two most dominant emotions, neutral and joyful, make up over 50% of the data set, which does not seem to be balanced, although it is understandable, given the character of this series, that it is a comedy. However, if one considers the coarse-grained emotions as positive, negative and neutral, they result in about 40%, 30% and 30%, respectively, giving a more balanced distribution. The last column shows the ratio of comments that all four commentators agree with. Only about 1% of the comments show complete agreement for peaceful and powerful, which is reasonable as these emotions are often confused with neutral."}, {"heading": "3 Emotions 7 Emotions Ratio 4-Agree", "text": "Figure 1 illustrates how the emotions of the six main characters progress and are influenced by further expressions over time within a scene. Figure 2 shows the confusion matrix in terms of the (di-) agreement of the comments. Lines correspond to the labels obtained through the voting scheme (Section 3.3), and columns represent the emotions selected by each of the four commentators. The two dominant emotions, neutral and joyful, cause the greatest confusion in commentators, while the smaller emotions such as sad, powerful or peaceful show a good agreement in terms of the diagrams."}, {"heading": "3.5 Comparison", "text": "The ISEAR database consists of 7,666 statements and six emotions, 2 drawn from a psychologist-led study of 3,000 participants; the SemEval '07 Task 14 dataset was compiled from news headlines; it contained 250 sentences annotated with six emotions (Strapparava and Mihalcea, 2007); and the WASSA' 17 Task 1 dataset was compiled from 7,097 tweets and labeled with four emotions (Mohammad and Bravo-Marquez, 2017).Participants in this joint task were expected to develop a model for recognizing the intensity of emotions in tweets; our corpus is larger than most other text-based corpora and the only way to provide emotional annotations on dialogue sequences conveyed through successive voices.2emotion-research.net / toolbox / toolboxdatabase.2006-10-13.2581092615"}, {"heading": "4 Sequence-Based Convolutional Neural Networks (SCNN)", "text": "A unique aspect of our corpus is that it preserves the original sequence of utterances given to each dialogue so that it can solve this problem as a sequence classification task. In this section, sequence-based CNN models are presented that use the emotion sequence of previous utterances to detect the emotion of the current utterance. In addition, attention mechanisms are proposed to better optimize these SCNN models."}, {"heading": "4.1 Sequence Unification: Concatenation", "text": "Figure 3 describes our first SCNN model. The input to SCNN is an embedding of matrixX-Rt \u00b7 m, where t is the maximum number of tokens in each expression and m is the embedding size. Each line in x represents a token in the expression. At any time, region sizes in the order of 1, 2, 3,..., r \u00b2, r \u00b2 n are taken into account. Each region has the f-number of filters. As a result of the application of convolution and max-pooling, a univariant characteristic vector ~ h-Rr \u00b7 f is generated for the current expression. In the next step, the dense characteristic vectors from the current expression and k-1 previous expressions within the same dialogue are linked in columns. As a result of this concatenation, the vector ~ h-Rr \u00b7 f \u00b7 f \u00b7 k is generated for the current expression. In the next step, the dense characteristic vectors from the current expression and k-1 are linked to F-F-F-F F-F-F-F-F-F are combined with the previous expressions within the same dialogue column F-F-F-F, with the previous expression F-F-F-F-F, whereas the previous expression F-F-F-F is applied in the preceding the preceding the preceding vector."}, {"heading": "4.2 Sequence Unification: Convolution", "text": "s refer the model in section 4.1 to SCNNc. In the second proposed model, referring to SCNNv (fig. 4), two separate 2-D turns, Conv1 and Conv2, are used for the unification of sequences. Input to Conv1 is the same embedding matrix X. Input to Conv2 is another matrix Y-Rk \u00b7 r \u00b7 f, which is a line-by-line concatenation of the dense vectors generated from Conv1 of the current utterance and k-1 of previous utterances. Conv2 has regional sizes of \u03b2 {1,.., b}, b-N with the number of filters for each regional size. Output of Conv2 is the vector ~ v-Rd \u00b7 b. Conv1 and Conv2 are conceptually identical, although they have different regional sizes, filters and other hyperparameters."}, {"heading": "4.3 Attention Mechanism", "text": "In essence, this attention model is a weighted arithmetic sum of the feature vector of the current utterance, with weights selected based on the relevance of each element of that feature vector based on the unified feature vectors from the previous utterances. Figure 5 draws attention to the current feature vector SCNNc, SCNNac. In this model, the current feature vector ~ h and the k-1 previous feature vectors are linked in series to produce Z-Rk \u00b7 r \u00b7 f. An attention matrix A-Rr \u00b7 f \u00b7 k is applied to the current feature vector. The weights of this attention matrix are learned in light of the past feature vectors. The result of this operation is ~ u = h \u00d7 Z, ~ u-R1 \u00d7 rf."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Corpus", "text": "Our corpus is divided into training, development, and evaluation groups, each comprising 77, 11, and 9 episodes. Although episodes are randomly assigned, all expressions from the same episode remain in the same group to preserve sequence information. Further attempts are made to maintain similar proportions for each emotion across different groups. Table 6 shows the distribution of data sets."}, {"heading": "5.2 Preprocessing", "text": "In this thesis, we use the Word embedding model Word2vec introduced by Mikolov et al. (2013). Word embedding is trained separately using FriendsTV broadcast protocols, Amazon reviews, New York Times, and Wall Street Journal articles."}, {"heading": "5.3 Models", "text": "We report on the results of the following four models: SCNNc, SCNNac, SCNNv and SCNNav. For each of the models mentioned, we collect the results based on n {1,..., 5} previous statements. For a better comparison, we also report on the results obtained by the base CNN (Kim, 2014) and RNN-CNN. RNNCNN is our replication of the model proposed by Donahue et al. (2015) to merge time series information for visual recognition and description. RNNCNN that we use consists of a Long Short Term Memory (LSTM); the input to LSTM are the feature vectors generated by CNN for all utterances in a scene and we train RNN-CNN to tune all hyperparameters."}, {"heading": "5.4 Results", "text": "Table 7 summarizes the overall performance of our proposed sequence-based CNN models on the development table. The first column of the table indicates the number of previous statements included in our model. We report both accuracy and F1score to evaluate the performance of our models. F1 score, by taking into account false positives and false negatives, is generally a better method of measuring performance on a corpus of unevenly distributed classes. In SCNNc and SCNNac, by including previous three statements, and in SCNNv and SCNNav, by taking into account previous five statements, the best results are obtained. We also conducted our experiments with taking into account more than five expressions, but no significant improvement.Table 8 summarizes the overall performance of our models on the evaluation table. To compare our proposed models with some other baseline models, we also include the performance of CNN and RNN-CNN."}, {"heading": "5.5 Analysis", "text": "Figure 7 shows the confusion matrix of gold labels and predictions for our best model, SCNNac. Most often, all emotions are confused with neutrality. Peaceful has the highest confusion rate with neutrality; 30% of the total number of examples in this class are confused with neutrality. While joyful and powerful people have the lowest confusion rates with neutrality (13.8% and 20.4%), we have split the impact of sequence numbers based on the number of expressions in the scenes described in Section 3.1. After reviewing the performance of SCNNc and SCNNac on four batches, we noticed these two models performed better by considering the previous three sequences (about 4% thrust in the first two batches)."}, {"heading": "6 Conclusion", "text": "The experimental results showed that our proposed models exceeded CNN's baseline model. As the annotations of emotions from texts are usually subjective, we plan to deploy more annotation specialists in the future to improve the quality of the current corpus. In order to fully evaluate the performance of our proposed models, we also intend to implement various combinations of attention mechanisms and expand the size of our corpus by commenting on further seasons of Friends TV."}], "references": [{"title": "Affect in Text and Speech", "author": ["Cecilia Ovesdotter Alm."], "venue": "Ph.D. thesis, University of Illinois at Urbana-Champaign.", "citeRegEx": "Alm.,? 2008", "shortCiteRegEx": "Alm.", "year": 2008}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Deep heterogeneous feature fusion for template-based face recognition", "author": ["Navaneeth Bodla", "Jingxiao Zheng", "Hongyu Xu", "JunCheng Chen", "Carlos Castillo", "Rama Chellappa."], "venue": "Applications of Computer Vision (WACV), 2017 IEEE Winter Con-", "citeRegEx": "Bodla et al\\.,? 2017", "shortCiteRegEx": "Bodla et al\\.", "year": 2017}, {"title": "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis", "author": ["Sven Buechel", "Udo Hahn."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational", "citeRegEx": "Buechel and Hahn.,? 2017", "shortCiteRegEx": "Buechel and Hahn.", "year": 2017}, {"title": "Using a heterogeneous dataset for emotion analysis in text", "author": ["Soumaya Chaffar", "Diana Inkpen."], "venue": "Canadian Conference on Artificial Intelligence. Springer, pages 62\u201367.", "citeRegEx": "Chaffar and Inkpen.,? 2011", "shortCiteRegEx": "Chaffar and Inkpen.", "year": 2011}, {"title": "Upar7: A knowledge-based system for headline sentiment tagging", "author": ["Fran\u00e7ois-R\u00e9gis Chaumartin."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, pages 422\u2013425.", "citeRegEx": "Chaumartin.,? 2007", "shortCiteRegEx": "Chaumartin.", "year": 2007}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the IEEE", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Attentive pooling networks", "author": ["C\u0131cero Nogueira dos Santos", "Ming Tan", "Bing Xiang", "Bowen Zhou."], "venue": "CoRR, abs/1602.03609 .", "citeRegEx": "Santos et al\\.,? 2016", "shortCiteRegEx": "Santos et al\\.", "year": 2016}, {"title": "Using youtube comments for text-based emotion recognition", "author": ["Yasmina Douiji", "Hajar Mousannif", "Hassan Al Moatassime."], "venue": "Procedia Computer Science 83:292\u2013299.", "citeRegEx": "Douiji et al\\.,? 2016", "shortCiteRegEx": "Douiji et al\\.", "year": 2016}, {"title": "Tracking dialog states using an authortopic based representation", "author": ["Richard Dufour", "Mohamed Morchid", "Titouan Parcollet."], "venue": "Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, pages 544\u2013551.", "citeRegEx": "Dufour et al\\.,? 2016", "shortCiteRegEx": "Dufour et al\\.", "year": 2016}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin."], "venue": "arXiv preprint arXiv:1705.03122 .", "citeRegEx": "Gehring et al\\.,? 2017", "shortCiteRegEx": "Gehring et al\\.", "year": 2017}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u20131701.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The fifth dialog state tracking challenge", "author": ["Seokhwan Kim", "Luis Fernando D\u2019Haro", "Rafael E Banchs", "Jason D Williams", "Matthew Henderson", "Koichiro Yoshino"], "venue": "In Proceedings of the 2016 IEEE Workshop on Spoken Language Technology (SLT)", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882 .", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks 3361(10):1995", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": null, "citeRegEx": "LeCun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Recurrent neural network for text classification with multi-task learning", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "arXiv preprint arXiv:1605.05101 .", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["Patrick Lucey", "Jeffrey F Cohn", "Takeo Kanade", "Jason Saragih", "Zara Ambadar", "Iain Matthews."], "venue": "Computer Vision and Pattern Recog-", "citeRegEx": "Lucey et al\\.,? 2010", "shortCiteRegEx": "Lucey et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in neural information processing systems", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "WASSA-2017 shared task on emotion intensity", "author": ["Saif M. Mohammad", "Felipe Bravo-Marquez."], "venue": "Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA). Copenhagen, Denmark.", "citeRegEx": "Mohammad and Bravo.Marquez.,? 2017", "shortCiteRegEx": "Mohammad and Bravo.Marquez.", "year": 2017}, {"title": "Emotion recognition from text using knowledge-based ann", "author": ["Young-Soo Seol", "Dong-Joo Kim", "Han-Woo Kim."], "venue": "Proceedings of ITCCSCC. pages 1569\u20131572.", "citeRegEx": "Seol et al\\.,? 2008", "shortCiteRegEx": "Seol et al\\.", "year": 2008}, {"title": "Emotion recognition from text based on automatically generated rules", "author": ["Shadi Shaheen", "Wassim El-Hajj", "Hazem Hajj", "Shady Elbassuoni."], "venue": "Data Mining Workshop (ICDMW), 2014 IEEE International Conference on. IEEE, pages 383\u2013392.", "citeRegEx": "Shaheen et al\\.,? 2014", "shortCiteRegEx": "Shaheen et al\\.", "year": 2014}, {"title": "A multichannel convolutional neural network for cross-language", "author": ["Hongjie Shi", "Takashi Ushio", "Mitsuru Endo", "Katsuyoshi Yamagami", "Noriaki Horii"], "venue": null, "citeRegEx": "Shi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language", "author": ["Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Y Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Dialogue act modeling for automatic tagging and recognition", "author": ["Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martin", "Carol Van Ess-Dykema", "Marie Meteer"], "venue": null, "citeRegEx": "Stolcke et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stolcke et al\\.", "year": 2000}, {"title": "Semeval2007 task 14: Affective text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, pages 70\u201374.", "citeRegEx": "Strapparava and Mihalcea.,? 2007", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2007}, {"title": "Learning to identify emotions in text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "Proceedings of the 2008 ACM symposium on Applied computing. ACM, pages 1556\u20131560.", "citeRegEx": "Strapparava and Mihalcea.,? 2008", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2008}, {"title": "Wordnet affect: an affective extension of wordnet", "author": ["Carlo Strapparava", "Alessandro Valitutti"], "venue": "In LREC. Citeseer,", "citeRegEx": "Strapparava and Valitutti,? \\Q2004\\E", "shortCiteRegEx": "Strapparava and Valitutti", "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["Alex Waibel", "Toshiyuki Hanazawa", "Geoffrey Hinton", "Kiyohiro Shikano", "Kevin J Lang."], "venue": "IEEE transactions on acoustics, speech, and signal processing 37(3):328\u2013339.", "citeRegEx": "Waibel et al\\.,? 1989", "shortCiteRegEx": "Waibel et al\\.", "year": 1989}, {"title": "The feeling wheel: A tool for expanding awareness of emotions and increasing spontaneity and intimacy", "author": ["Gloria Willcox."], "venue": "Transactional Analysis Journal 12(4):274\u2013276.", "citeRegEx": "Willcox.,? 1982", "shortCiteRegEx": "Willcox.", "year": 1982}, {"title": "Modeling spatial-temporal clues in a hybrid deep learning framework for video classification", "author": ["Zuxuan Wu", "Xi Wang", "Yu-Gang Jiang", "Hao Ye", "Xiangyang Xue."], "venue": "Proceedings of the 23rd ACM international conference on Multimedia. ACM, pages", "citeRegEx": "Wu et al\\.,? 2015", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention (2015)", "author": ["K Xu", "J Ba", "R Kiros", "A Courville", "R Salakhutdinov", "R Zemel", "Y Bengio."], "venue": "arxiv preprint. arXiv preprint arXiv:1502.03044 .", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Emotion detection from speech", "author": ["Feng Yu", "Eric Chang", "Ying-Qing Xu", "HeungYeung Shum"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2001}, {"title": "Spontaneous emotional facial expression detection", "author": ["Zhihong Zeng", "Yun Fu", "Glenn I Roisman", "Zhen Wen", "Yuxiao Hu", "Thomas S Huang."], "venue": "Journal of multimedia 1(5):1\u20138.", "citeRegEx": "Zeng et al\\.,? 2006", "shortCiteRegEx": "Zeng et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 26, "context": "Human emotions have been widely studied in the realm of psychological and behavioral sciences as well as computer science (Strapparava and Mihalcea, 2008).", "startOffset": 122, "endOffset": 154}, {"referenceID": 33, "context": "A wide variety of researches have been conducted in detecting emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010).", "startOffset": 111, "endOffset": 167}, {"referenceID": 34, "context": "A wide variety of researches have been conducted in detecting emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010).", "startOffset": 111, "endOffset": 167}, {"referenceID": 16, "context": "A wide variety of researches have been conducted in detecting emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010).", "startOffset": 111, "endOffset": 167}, {"referenceID": 25, "context": "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).", "startOffset": 42, "endOffset": 143}, {"referenceID": 0, "context": "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).", "startOffset": 42, "endOffset": 143}, {"referenceID": 19, "context": "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).", "startOffset": 42, "endOffset": 143}, {"referenceID": 3, "context": "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).", "startOffset": 42, "endOffset": 143}, {"referenceID": 29, "context": "Nonetheless, CNN are often not used for sequence modeling (Waibel et al., 1989; LeCun et al., 1995; Gehring et al., 2017) because their basic architecture does not take previous sequence information into account.", "startOffset": 58, "endOffset": 121}, {"referenceID": 10, "context": "Nonetheless, CNN are often not used for sequence modeling (Waibel et al., 1989; LeCun et al., 1995; Gehring et al., 2017) because their basic architecture does not take previous sequence information into account.", "startOffset": 58, "endOffset": 121}, {"referenceID": 28, "context": "One common approach to alleviate this issue is using recurrent neural networks (Sutskever et al., 2014; Liu et al., 2016).", "startOffset": 79, "endOffset": 121}, {"referenceID": 15, "context": "One common approach to alleviate this issue is using recurrent neural networks (Sutskever et al., 2014; Liu et al., 2016).", "startOffset": 79, "endOffset": 121}, {"referenceID": 21, "context": "The performance of these keyword-based approaches had been rather unsatisfactory due to the fact that the semantics of the keywords heavily depend on the contexts, and it is significantly affected by the absence of those keywords (Shaheen et al., 2014)", "startOffset": 230, "endOffset": 252}, {"referenceID": 5, "context": "Chaumartin (2007) did emotion detection on news headlines.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Chaffar and Inkpen (2011) detected emotions from several corpora collected from blog, news headline, and fairy tale using a supervised approach.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Chaffar and Inkpen (2011) detected emotions from several corpora collected from blog, news headline, and fairy tale using a supervised approach. Douiji et al. (2016) used YouTube comments and developed an unsupervised learning algorithm to detect emotions from the comments.", "startOffset": 0, "endOffset": 166}, {"referenceID": 4, "context": "Chaffar and Inkpen (2011) detected emotions from several corpora collected from blog, news headline, and fairy tale using a supervised approach. Douiji et al. (2016) used YouTube comments and developed an unsupervised learning algorithm to detect emotions from the comments. Their approach gave comparative performance to supervised approaches such as Chaffar and Inkpen (2011) that support vector machines were employed for statistical learning.", "startOffset": 0, "endOffset": 378}, {"referenceID": 20, "context": "Seol et al. (2008) used an ensemble of a keywordbased approach and knowledge-based artificial neural networks to classify emotions in drama, novel, and public web diary.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "(2016) introduced a topic model that considered all information included in sub-dialogues to track the dialogue states introduced by the DSTC5 challenge (Kim et al., 2016).", "startOffset": 153, "endOffset": 171}, {"referenceID": 19, "context": "Shi et al. (2016) proposed multi-channel CNN for cross-language dialogue tracking.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Dufour et al. (2016) introduced a topic model that considered all information included in sub-dialogues to track the dialogue states introduced by the DSTC5 challenge (Kim et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Dufour et al. (2016) introduced a topic model that considered all information included in sub-dialogues to track the dialogue states introduced by the DSTC5 challenge (Kim et al., 2016). Stolcke et al. (2000) proposed a statistical approach to model dialogue act on human-to-human telephone conversations.", "startOffset": 0, "endOffset": 209}, {"referenceID": 18, "context": "Attention mechanism has been widely employed in the field of computer vision (Mnih et al., 2014; Xu et al., 2015) and recently become popular in natural language processing as well.", "startOffset": 77, "endOffset": 113}, {"referenceID": 32, "context": "Attention mechanism has been widely employed in the field of computer vision (Mnih et al., 2014; Xu et al., 2015) and recently become popular in natural language processing as well.", "startOffset": 77, "endOffset": 113}, {"referenceID": 1, "context": "In particular, incorporating attention mechanism has achieved the stateof-the-art performance in machine translation and question answering tasks (Bahdanau et al., 2014; Hermann et al., 2015; dos Santos et al., 2016).", "startOffset": 146, "endOffset": 216}, {"referenceID": 11, "context": "In particular, incorporating attention mechanism has achieved the stateof-the-art performance in machine translation and question answering tasks (Bahdanau et al., 2014; Hermann et al., 2015; dos Santos et al., 2016).", "startOffset": 146, "endOffset": 216}, {"referenceID": 30, "context": "As a result, our corpus comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)\u2019s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.", "startOffset": 193, "endOffset": 208}, {"referenceID": 23, "context": "Pioneered by Snow et al. (2008), crowdsourcing has been widely used for the creation of many corpora in natural language processing.", "startOffset": 13, "endOffset": 32}, {"referenceID": 30, "context": "It is worth mentioning that crowd workers were asked to choose from 37 emotions for the first 25% of the annotation, which comprised 36 secondary emotions from Willcox (1982) and the neutral emotion.", "startOffset": 160, "endOffset": 175}, {"referenceID": 25, "context": "The SemEval\u201907 Task 14 dataset was created from news headlines; it contained 250 sentences annotated with six emotions (Strapparava and Mihalcea, 2007).", "startOffset": 119, "endOffset": 151}, {"referenceID": 19, "context": "The WASSA\u201917 Task 1 dataset was collected from 7,097 tweets and labeled with four emotions (Mohammad and Bravo-Marquez, 2017).", "startOffset": 91, "endOffset": 125}, {"referenceID": 17, "context": "In this work we utilize Word2vec word embedding model introduced by Mikolov et al. (2013). The word embedding is trained separately using Friends", "startOffset": 68, "endOffset": 90}, {"referenceID": 13, "context": "For a better comparison we also report the results achieved from base CNN (Kim, 2014) and RNN-CNN.", "startOffset": 74, "endOffset": 85}, {"referenceID": 6, "context": "RNNCNN is our replication of the model proposed by Donahue et al. (2015) to fuse time-series information for visual recognition and description.", "startOffset": 51, "endOffset": 73}, {"referenceID": 30, "context": "To fuse the generated dense feature vectors we applied different combinations of regularized feature fusion networks similar to the networks employed in Wu et al. (2015) and Bodla et al.", "startOffset": 153, "endOffset": 170}, {"referenceID": 2, "context": "(2015) and Bodla et al. (2017). However, for our task, none of these fusion networks performed better than the 1-D convolutional layer we utilized.", "startOffset": 11, "endOffset": 31}], "year": 2017, "abstractText": "While there have been significant advances in detecting emotions from speech and image recognition, emotion detection on text is still under-explored and remained as an active research field. This paper introduces a corpus for text-based emotion detection on multiparty dialogue as well as deep neural models that outperform the existing approaches for document classification. We first present a new corpus that provides annotation of seven emotions on consecutive utterances in dialogues extracted from the show, Friends. We then suggest four types of sequence-based convolutional neural network models with attention that leverage the sequence information encapsulated in dialogue. Our best model shows the accuracies of 37.9% and 54% for fineand coarsegrained emotions, respectively. Given the difficulty of this task, this is promising.", "creator": "LaTeX with hyperref package"}}}