{"id": "1708.04321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Distance and Similarity Measures Effect on the Performance of K-Nearest Neighbor Classifier - A Review", "abstract": "The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested example and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures? This review attempts to answer the previous question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. We found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances. In addition, the performance of the KNN degraded only about $20\\%$ while the noise level reaches $90\\%$, this is true for all the distances used. This means that the KNN classifier using any of the top $10$ distances tolerate noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing to other distances.", "histories": [["v1", "Mon, 14 Aug 2017 20:52:35 GMT  (847kb,D)", "http://arxiv.org/abs/1708.04321v1", "50 pages, 6 figures, 14 tables"]], "COMMENTS": "50 pages, 6 figures, 14 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["v b surya prasath", "haneen arafat abu alfeilat", "omar lasassmeh", "ahmad b a hassanat"], "accepted": false, "id": "1708.04321"}, "pdf": {"name": "1708.04321.pdf", "metadata": {"source": "CRF", "title": "Distance and Similarity Measures Effect on the Performance of K-Nearest Neighbor Classifier \u2013 A Review", "authors": ["V. B. Surya Prasatha", "Haneen Arafat Abu Alfeilat", "Omar Lasassmeh", "Ahmad B. A. Hassanat"], "emails": ["prasaths@missouri.edu"], "sections": [{"heading": null, "text": "The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, but its performance competes with the most complex classifiers in the literature. The core of this classifier mainly depends on measuring the distance or similarity between the example tested and the training examples. This raises an important question, which distance measurements should be used for the KNN classifier among a large number of real-world distance and similarity measurements? This verification attempts to answer the previous question by evaluating the performance of the KNN (measured by accuracy, precision and retrieval) against a large number of distance measurements tested on a number of real-world datasets, with and without the addition of different noise levels. The experimental results show that the performance of the KNN classifier (measured by accuracy, precision and retrieval) depends substantially on the distance used, the results showed large differences between the performance of different distances. We found that a recently proposed performance is applied to the best distance when not applied to the other data set."}, {"heading": "1. Introduction", "text": "Classification is an important issue in data science in general and pattern recognition in particular. K's nearest neighbour (short: KNN) is one of the corresponding authors. Tel.: + 1 573 882 8391 Email address: prasaths @ missouri.edu (V. B. Surya Prasath) Preprint submitted to Elsevier 16 August 2017ar Xiv: 170 8.04 321v 1 [cs.L G] 14 Aof the oldest, simplest and most precise algorithms for pattern classification and regression models. KNN was proposed by Fix & Hodges (1951) and subsequently modified by Cover & Hart (1967). KNN was identified as one of the ten best methods in data mining (Wu et al., 2008)."}, {"heading": "1.1. Related works", "text": "Several studies have been conducted to analyze the performance of the KNN classifier using different distance metrics. Each study has been applied to different types of datasets with different distributions, data types, and using different numbers of distance and similarity of metrics. Each study has been applied to different types of datasets with different distributions, types of data, and methods, including the different types of metrics performed in different distributions. Chomboon and his colleagues (Chomboon et al., 2015) analyzed the performance of the KNN classifier using 11 distance metrics. Their experiment has been applied to eight types of synthetic datasets with different types of distributions generated using MATLAB. They divided each dataset into 70% for training sets and 30% for the test set. Results showed that the Manhattan, Minkowski, Chebychev, Euclidean, Euclidean, and Euclidean."}, {"heading": "1.2. Contributions", "text": "In the KNN classifier, the distances between the test sample and the training samples are determined by different measuring instruments. Therefore, distance measurements play a crucial role in determining the final classification results (Hu et al., 2016). Euclidean distance is the most widely used distance measurement variable in the KNN classification, but few studies examined the impact of different distance measurements on the performance of KNN, who used a small number of distances, a small number of datasets, or both. Such scarcity in experiments does not prove which distance can best be used with the KNN classifier. Therefore, this verification attempts to close this gap by testing a large number of distance measurements on a large number of different datasets, in addition to the distance measurements that are least affected by added noise. The KNN classifier is best at dealing with loud data, so we need to examine the impact of the choice of different power measurements on the KN when classifying a fictitious."}, {"heading": "1.3. Organization", "text": "We have organized our review as follows: First, in Section 2, we give an introductory overview of the KNN classification method and present its history, characteristics, pros and cons. We review the definitions of various distances used in conjunction with KNN. Section 3 explains the data sets used in classification experiments, the structure of the experimental model and the metrics for performance evaluation. We present and discuss the results achieved by the experimental framework. Finally, in Section 4, we present the conclusions and possible future directions."}, {"heading": "2. KNN and distance measures", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Brief overview of KNN claasifier", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrVo"}, {"heading": "2.2. Noisy data", "text": "The presence of noise in the data is mainly related to the way in which data from their environment is collected and pre-processed (Nettleton, OrriolsPuig, & Fornells, 2010).Noise-related data is in some ways a corrupt form of data that leads to a partial change in data values. Two main sources of noise can be identified: First, the implicit errors caused by measurement tools, such as the use of different sensor types. Second, the random errors caused by stacking processes or experts, during the collection of data, for example, errors during the digitization process. Based on these two sources of error, two types of noise can be classified in a given data set (Zhu & Wu, 2004): 1. Class noise: occurs when the sample is mislabeled due to multiple causes, such as data entry errors during the labeling process, or the inadequacy of any information being used."}, {"heading": "2.3. Distance measures review", "text": "The first occurrence of the word distance is found in the writings of Aristotle (384 AC - 322 AC) = function of two. He argued that the word distance means: \"It is between extremities that distance is greatest\" or \"things that have something between them, i.e. a certain distance.\" Besides, \"distance has the meaning of dimension [as in space three dimensions, length, breadth and depth].\" Euclid, one of the most important mathematicians of ancient history, used the word distance only in his third postulate of the Principia (Euclid, 1956): \"Any circle can be described by a center and a distance.\" Distance is a numerical description of how far apart units are. In data mining, distance means what it means for elements of a space that are to be near or far away. Synonyms for distance include color, diversity and synonyms for similarity."}, {"heading": "3. Experimental framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Datasets used for experiments", "text": "The experiments were conducted on twenty-eight sets of data that represent real-life classification problems collected by the UCI Machine Learning Repository (Lichman, 2013). The UCI Machine Learning Repository is a collection of databases, domain theories, and data generators used by the machine learning community for empirical analysis of machine learning algorithms, created in 1987 by David Aha and fellow students at UC Irvine. Since then, it has been used as the primary source of machine learning data by students, teachers, and researchers around the world. Each set of data consists of a series of examples. Each example is defined by a number of attributes, and all examples within the data are represented by the same number of attributes. One of these attributes is referred to as the class attribute that contains the class value (label) of the data: [Label] of the data whose values are predicted to test the examples."}, {"heading": "3.2. Experimental setup", "text": "Each dataset is divided into two sets of data, one for training and the other for testing. To this end, 34% of the dataset is used for testing and 66% of the data is used for training; the value of K is set to 1 for simplicity; the 34% of the data used as a test sample was randomly selected, and each experiment on each dataset was repeated 10 times to obtain random examples of testing and training; the entire experimental framework is shown in Figure 3. Our experiments are divided into two main parts: 1. The first part of the experiments aims to find the best distance measurements that can be used by KNN classifiers without noise in the datasets. We have used all 54 distances verified in Section 2.3.2. The second part of the experiments aims to find the best distance measurement used by KNN classifiers in the case of noisy data. In this work, we define \"the best method is performed with the highest accuracy.\""}, {"heading": "3.3. Performance evaluation measures", "text": "Accuracy is calculated to evaluate the overall performance of the classifier. It is defined as the ratio of test samples correctly classified to the number of samples tested. Accuracy (or positive predictive value) is the proportion of instances called that are relevant, while callback (or sensitivity) is the proportion of relevant instances (or sensitivity) called that are from relevant instances that are retrieved. These measurements can be constructed by calculating: 1. True positive (TP): The number of correctly classified examples of a particular class (how we calculate these measures for each class).True negative (TN) classes that do not belong to the specific classes."}, {"heading": "3.4. Experimental results and discussion", "text": "To this end, two sets of tests have been carried out: the aim of the first is to compare the performance of the KNN classifiers when used with each of the 54 noise-free distance and similarity measures reviewed in Section 2.3; and the second series is designed to find the most robust distance that has the least impact at different noise levels."}, {"heading": "3.5. Without noise", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3.6. With noise", "text": "This is why most people who are able to move are able to move without being afraid, that they are able to move, and that they are able to move."}, {"heading": "3.7. Precise evaluation of the effects of noise", "text": "To justify why some distances are either less or more affected by noise, the following examples 3.1 and 3.2 are designed to show the impact of noise on the final decision of the KNN classifier with Hassanate (HasD) and the standard Euclidean (ED) distances. However, we assume that we have two training vectors (v1 and v2) which have three attributes for each, in addition to a test vector (v3). We calculate the distances between v3 and v2 using Euclidean and Hassanate distances. This example shows that the KNN classification is done with two different distances on clean data (v3)."}, {"heading": "4. Conclusions", "text": "This review evaluates the performance (accuracy, precision and retrieval) of the KNN classification with a large number of distance measurements that can be used in the case of noisy data. A large number of experiments carried out for the purposes of this review, and the results and analyses of these experiments show the following: 1. The performance of the KNN classification depends substantially on the distance used, which has large gaps between the performance of the different distances. 2. The results show that the gaps in the individual areas are closed. 3. The gaps in the areas where the gaps are closed. 3. The gaps in the areas are closed. 3. The gaps in the areas are closed."}], "references": [{"title": "Combining Jaccard and Mahalanobis Cosine distance to enhance the face recognition rate", "author": ["A. Abbad", "H. Tairi"], "venue": "WSEAS Transactions on Signal Processing,", "citeRegEx": "Abbad and Tairi,? \\Q2016\\E", "shortCiteRegEx": "Abbad and Tairi", "year": 2016}, {"title": "Slope finder \u2013 A distance measure for DTW based isolated word speech recognition", "author": ["A. Akila", "E. Chandra"], "venue": "International Journal of Engineering And Computer Science,", "citeRegEx": "Akila and Chandra,? \\Q2013\\E", "shortCiteRegEx": "Akila and Chandra", "year": 2013}, {"title": "On enhancing the performance of nearest neighbour classifiers using Hassanat distance metric", "author": ["M. Alkasassbeh", "G.A. Altarawneh", "A.B. Hassanat"], "venue": "Canadian Journal of Pure and Applied Sciences,", "citeRegEx": "Alkasassbeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alkasassbeh et al\\.", "year": 2015}, {"title": "Voting over multiple condensed nearest neighbors", "author": ["E. Alpaydin"], "venue": "Artificial Intelligence Review, 11, 115\u2013132.", "citeRegEx": "Alpaydin,? 1997", "shortCiteRegEx": "Alpaydin", "year": 1997}, {"title": "Approximate nearest neighbor queries in fixed dimensions", "author": ["S. Arya", "D.M. Mount"], "venue": "4th annual ACM/SIGACT-SIAM Symposium on Discrete Algorithms", "citeRegEx": "Arya and Mount,? \\Q1993\\E", "shortCiteRegEx": "Arya and Mount", "year": 1993}, {"title": "A comparison of nearest neighbor search algorithms for generic object recognition", "author": ["F. Bajramovic", "F. Mattern", "N. Butko", "J. Denzler"], "venue": "In Advanced Concepts for Intelligent Vision Systems (pp", "citeRegEx": "Bajramovic et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bajramovic et al\\.", "year": 2006}, {"title": "Performance evaluation of distance metrics: application to fingerprint recognition", "author": ["S.D. Bharkad", "M. Kokare"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bharkad and Kokare,? \\Q2011\\E", "shortCiteRegEx": "Bharkad and Kokare", "year": 2011}, {"title": "Survey of Nearest Neighbor Techniques", "author": ["N. Bhatia", "Vandana"], "venue": "International Journal of Computer Science and Information Security,", "citeRegEx": "Bhatia and Vandana.,? \\Q2010\\E", "shortCiteRegEx": "Bhatia and Vandana.", "year": 2010}, {"title": "On a measure of divergence between two statistical population defined by their population distributions", "author": ["A. Bhattachayya"], "venue": "Bulletin Calcutta Mathematical Society, 35, 99\u2013109.", "citeRegEx": "Bhattachayya,? 1943", "shortCiteRegEx": "Bhattachayya", "year": 1943}, {"title": "Software Similarity and Classification", "author": ["S. Cesare", "Y. Xiang"], "venue": "Springer.", "citeRegEx": "Cesare and Xiang,? 2012", "shortCiteRegEx": "Cesare and Xiang", "year": 2012}, {"title": "Comprehensive survey on distance/similarity measures between probability density functions", "author": ["Cha", "S.-H."], "venue": "International Journal of Mathematical Models and Methods in Applied Sciences, 1(4), 300\u2013307.", "citeRegEx": "Cha and S..H.,? 2007", "shortCiteRegEx": "Cha and S..H.", "year": 2007}, {"title": "Pattern Recognition in Bioinformatics", "author": ["M. Chetty", "A. Ngom", "S. Ahmad"], "venue": null, "citeRegEx": "Chetty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chetty et al\\.", "year": 2008}, {"title": "An empirical study of distance metrics for k-nearest neighbor algorithm", "author": ["K. Chomboon", "C. Pasapichi", "T. Pongsakorn", "K. Kerdprasop", "N. Kerdprasop"], "venue": "In The 3rd International Conference on Industrial Application Engineering", "citeRegEx": "Chomboon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chomboon et al\\.", "year": 2015}, {"title": "An extension of the coefficient of divergence for use with multiple characters", "author": ["P.J. Clark"], "venue": "Copeia, 1952 (2), 61\u201364.", "citeRegEx": "Clark,? 1952", "shortCiteRegEx": "Clark", "year": 1952}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and Hart,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart", "year": 1967}, {"title": "A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms", "author": ["J. Derrac", "S. Garcia", "D. Molina", "F. Herrera"], "venue": "Swarm and Evolutionary Computation,", "citeRegEx": "Derrac et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Derrac et al\\.", "year": 2011}, {"title": "Encyclopedia of distances", "author": ["E. Deza", "M.M. Deza"], "venue": "Springer.", "citeRegEx": "Deza and Deza,? 2009", "shortCiteRegEx": "Deza and Deza", "year": 2009}, {"title": "Measures of the amount of ecologic association between species", "author": ["L.R. Dice"], "venue": "Ecology, 26 (3), 297\u2013302.", "citeRegEx": "Dice,? 1945", "shortCiteRegEx": "Dice", "year": 1945}, {"title": "The Thirteen Books of Euclid\u2019s Elements", "author": ["Euclid."], "venue": "Courier Corporation.", "citeRegEx": "Euclid.,? 1956", "shortCiteRegEx": "Euclid.", "year": 1956}, {"title": "Discriminatory analysis", "author": ["E. Fix", "J.L. Hodges"], "venue": "Nonparametric discrimination; consistency properties. Technical Report 4, USAF School of Aviation Medicine, Randolph Field, TX, USA, 1951.", "citeRegEx": "Fix and Hodges,? 1951", "shortCiteRegEx": "Fix and Hodges", "year": 1951}, {"title": "Bioinformatics: Applications in life and environmental sciences", "author": ["M.H. Fulekar"], "venue": "Springer.", "citeRegEx": "Fulekar,? 2009", "shortCiteRegEx": "Fulekar", "year": 2009}, {"title": "Data clustering: Theory, algorithms, and applications", "author": ["G. Gan", "C. Ma", "J. Wu"], "venue": null, "citeRegEx": "Gan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2007}, {"title": "The reduced nearest neighbour rule", "author": ["G. Gates"], "venue": "IEEE Transactions on Information Theory, 18, 431\u2013433.", "citeRegEx": "Gates,? 1972", "shortCiteRegEx": "Gates", "year": 1972}, {"title": "An empirical comparison of dissimilarity measures for time series classification", "author": ["R. Giusti", "G. Batista"], "venue": "Brazilian Conference on Intelligent Systems (BRACIS) (pp. 82\u201388)", "citeRegEx": "Giusti and Batista,? \\Q2013\\E", "shortCiteRegEx": "Giusti and Batista", "year": 2013}, {"title": "The choice of metrics for clustering algorithms", "author": ["P. Grabusts"], "venue": "Environment. Technology. Resources , 70\u201376.", "citeRegEx": "Grabusts,? 2011", "shortCiteRegEx": "Grabusts", "year": 2011}, {"title": "Error detecting and error correcting codes", "author": ["R.W. Hamming"], "venue": "Bell System technical journal, 131 (1), 147\u2013160.", "citeRegEx": "Hamming,? 1958", "shortCiteRegEx": "Hamming", "year": 1958}, {"title": "The condensed nearest neighbour rule", "author": ["P. Hart"], "venue": "IEEE Transactions on Information Theory, 14, 515\u2013516.", "citeRegEx": "Hart,? 1968", "shortCiteRegEx": "Hart", "year": 1968}, {"title": "Dimensionality invariant similarity measure", "author": ["A.B. Hassanat"], "venue": "Journal of American Science, 10 (8), 221\u201326.", "citeRegEx": "Hassanat,? 2014", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Solving the problem of the k parameter in the KNN classifier using an ensemble learning approach", "author": ["A.B. Hassanat"], "venue": "International Journal of Computer Science and Information Security, 12 (8), 33\u201339.", "citeRegEx": "Hassanat,? 2014", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Solving the problem of the k parameter in the KNN classifier using an ensemble learning approach", "author": ["A.B. Hassanat", "M.A. Abbadi", "G.A. Altarawneh", "A.A. Alhasanat"], "venue": "International Journal of Computer Science and Information Security,", "citeRegEx": "Hassanat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hassanat et al\\.", "year": 2014}, {"title": "Compressed domain image retrieval: a comparative study of similarity metrics", "author": ["M. Hatzigiorgaki", "A. Skodras"], "venue": "Proceedings of SPIE", "citeRegEx": "Hatzigiorgaki and Skodras,? \\Q2003\\E", "shortCiteRegEx": "Hatzigiorgaki and Skodras", "year": 2003}, {"title": "An empirical modification to linear wave theory", "author": ["T. Hedges"], "venue": "Proc. Inst. Civ. Eng., 61, 575\u2013579.", "citeRegEx": "Hedges,? 1976", "shortCiteRegEx": "Hedges", "year": 1976}, {"title": "Neue Begr\u00fcndung der Theorie quadratischer Formen von unendlichvielen Ver\u00e4nderlichen", "author": ["E.E. Hellinger"], "venue": "f\u00fcr die reine und angewandte Mathematik, 136, 210\u2013271.", "citeRegEx": "Hellinger,? 1909", "shortCiteRegEx": "Hellinger", "year": 1909}, {"title": "The distance function effect on k-nearest neighbor classification for medical", "author": ["Hu", "L.-Y", "Huang", "M.-W", "Ke", "S.-W", "Tsai", "C.-F"], "venue": "datasets. SpringerPlus,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Etude comparative de la distribution florale dans une portion des Alpes et du Jura", "author": ["P. Jaccard"], "venue": null, "citeRegEx": "Jaccard,? \\Q1901\\E", "shortCiteRegEx": "Jaccard", "year": 1901}, {"title": "An invariant form for the prior probability in estimation problems", "author": ["H. Jeffreys"], "venue": "Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences (Vol. 186, pp. 453\u2013461). 46", "citeRegEx": "Jeffreys,? 1946", "shortCiteRegEx": "Jeffreys", "year": 1946}, {"title": "Classifier based on inverted indexes of neighbors", "author": ["M. Jirina", "M.J. Jirina"], "venue": "Institute of Computer Science. Academy of Sciences of the Czech Republic", "citeRegEx": "Jirina and Jirina,? \\Q2008\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2008}, {"title": "Using singularity exponent in distance based classifier", "author": ["M. Jirina"], "venue": "Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA2010). Cairo", "citeRegEx": "Jirina and Jirina,? \\Q2010\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2010}, {"title": "Experiments of distance measurements in a foliage plant retrieval system", "author": ["A. Kadir", "L.E. Nugroho", "A. Susanto", "P.S. Insap"], "venue": "International Journal of Signal Processing, Image Processing and Pattern Recognition,", "citeRegEx": "Kadir et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kadir et al\\.", "year": 2012}, {"title": "A Review of data classification Using Knearest neighbour Algorithm", "author": ["A. Kataria", "M.D. Singh"], "venue": "International Journal of Emerging Technology and Advanced Engineering,", "citeRegEx": "Kataria and Singh,? \\Q2013\\E", "shortCiteRegEx": "Kataria and Singh", "year": 2013}, {"title": "Voting nearest-neighbour subclassifiers", "author": ["M. Kubat", "Cooperson", "M. Jr."], "venue": "Proceedings of the 17th International Conference on Machine Learning (ICML), (pp. 503\u2013510)", "citeRegEx": "Kubat et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kubat et al\\.", "year": 2000}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Kullback and Leibler,? \\Q1951\\E", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Mixed-data classificatory programs I - Agglomerative systems", "author": ["G.N. Lance", "W.T. Williams"], "venue": "Australian Computer Journal,", "citeRegEx": "Lance and Williams,? \\Q1967\\E", "shortCiteRegEx": "Lance and Williams", "year": 1967}, {"title": "Retrieved from UC Irvine Machine Learning Repository: http://archive.ics.uci.edu/ml", "author": ["M. Lichman"], "venue": null, "citeRegEx": "Lichman,? \\Q2013\\E", "shortCiteRegEx": "Lichman", "year": 2013}, {"title": "Development of face recognition system for use on the NAO robot", "author": ["G.A. Lindi"], "venue": "Stavanger University, Norway.", "citeRegEx": "Lindi,? 2016", "shortCiteRegEx": "Lindi", "year": 2016}, {"title": "On the Impact of Distance Metrics in InstanceBased Learning Algorithms", "author": ["N. Lopes", "B. Ribeiro"], "venue": "Iberian Conference on Pattern Recognition and Image Analysis (pp", "citeRegEx": "Lopes and Ribeiro,? \\Q2015\\E", "shortCiteRegEx": "Lopes and Ribeiro", "year": 2015}, {"title": "Multidimensional Modelling of Image Fidelity Measures", "author": ["M. Macklem"], "venue": "Burnaby, BC, Canada: Simon Fraser University.", "citeRegEx": "Macklem,? 2002", "shortCiteRegEx": "Macklem", "year": 2002}, {"title": "Text categorization with Knearest neighbor approach", "author": ["S. Manne", "S. Kotha", "S.S. Fatima"], "venue": "In Proceedings of the International Conference on Information Systems Design and Intelligent Applications 2012 (INDIA 2012) held in Visakhapatnam,", "citeRegEx": "Manne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Manne et al\\.", "year": 2012}, {"title": "A study of the effect of different types of noise on the precision of supervised learning techniques", "author": ["D.F. Nettleton", "A. Orriols-Puig", "A. Fornells"], "venue": "Artificial intelligence review ,", "citeRegEx": "Nettleton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nettleton et al\\.", "year": 2010}, {"title": "Contributions to the theory of the \u03c72 test", "author": ["J. Neyman"], "venue": "proceedings of the first Berkeley symposium on mathematical statistics and probability.", "citeRegEx": "Neyman,? 1949", "shortCiteRegEx": "Neyman", "year": 1949}, {"title": "An agglomerative method for classification of plant communities", "author": ["L. Orloci"], "venue": "Journal of Ecology , 55(1), 193\u2013206.", "citeRegEx": "Orloci,? 1967", "shortCiteRegEx": "Orloci", "year": 1967}, {"title": "Content based video retrieval systems", "author": ["B. Patel", "B. Meshram"], "venue": "International Journal of UbiComp,", "citeRegEx": "Patel and Meshram,? \\Q2012\\E", "shortCiteRegEx": "Patel and Meshram", "year": 2012}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "author": ["K. Pearson"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 50 (302), 157\u2013175.", "citeRegEx": "Pearson,? 1900", "shortCiteRegEx": "Pearson", "year": 1900}, {"title": "Clustering narrow-domain short texts by using the Kullback-Leibler distance", "author": ["D. Pinto", "Benedi", "J.-M", "P. Rosso"], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics,", "citeRegEx": "Pinto et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2007}, {"title": "Human computer interaction using hand gestures", "author": ["P. Premaratne"], "venue": "Springer.", "citeRegEx": "Premaratne,? 2014", "shortCiteRegEx": "Premaratne", "year": 2014}, {"title": "Analysis of distance measures using k-nearest", "author": ["M. Punam", "T. Nitin"], "venue": "International Journal of Science and Research,", "citeRegEx": "Punam and Nitin,? \\Q2015\\E", "shortCiteRegEx": "Punam and Nitin", "year": 2015}, {"title": "Perceptual metrics for image database", "author": ["Y. Rubner", "C. Tomasi"], "venue": null, "citeRegEx": "Rubner and Tomasi,? \\Q2013\\E", "shortCiteRegEx": "Rubner and Tomasi", "year": 2013}, {"title": "Tackling the problem of classification with noisy data using multiple classifier Systems: Analysis of the performance and robustness", "author": ["J.A. Saez", "M. Galar", "J. Luengo", "F. Herrera"], "venue": "Information Sciences,", "citeRegEx": "Saez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saez et al\\.", "year": 2013}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications Review, 3\u201355.", "citeRegEx": "Shannon,? 2001", "shortCiteRegEx": "Shannon", "year": 2001}, {"title": "A Comparison Study on Similarity and Dissimilarity Measures in Clustering Continuous Data", "author": ["A.S. Shirkhorshidi", "S. Aghabozorgi", "T.Y. Wah"], "venue": "PloS one,", "citeRegEx": "Shirkhorshidi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shirkhorshidi et al\\.", "year": 2015}, {"title": "Information radius", "author": ["R. Sibson"], "venue": "Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete, 14 (2), 149\u2013160.", "citeRegEx": "Sibson,? 1969", "shortCiteRegEx": "Sibson", "year": 1969}, {"title": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons", "author": ["T. Sorensen"], "venue": "Biol. Skr., 5, 1\u201334.", "citeRegEx": "Sorensen,? 1948", "shortCiteRegEx": "Sorensen", "year": 1948}, {"title": "Distances and similarities in intuitionistic fuzzy sets", "author": ["E. Szmidt"], "venue": "Springer.", "citeRegEx": "Szmidt,? 2013", "shortCiteRegEx": "Szmidt", "year": 2013}, {"title": "New developments in generalized information measures", "author": ["I.J. Taneja"], "venue": "Advances in Imaging and Electron Physics, 91, 37\u2013135.", "citeRegEx": "Taneja,? 1995", "shortCiteRegEx": "Taneja", "year": 1995}, {"title": "A detailed analysis of the KDD CUP 99 data set", "author": ["M. Tavallaee", "E. Bagheri", "W. Lu", "A.A. Ghorbani"], "venue": "In IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA", "citeRegEx": "Tavallaee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tavallaee et al\\.", "year": 2009}, {"title": "Distances and other dissimilarity measures in chemometrics", "author": ["R. Todeschini", "D. Ballabio", "V. Consonni"], "venue": "Encyclopedia of Analytical Chemistry", "citeRegEx": "Todeschini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Todeschini et al\\.", "year": 2015}, {"title": "A new concept of higher-order similarity and the role of distance/similarity measures in local classification methods", "author": ["R. Todeschini", "V. Consonni", "F.G. Grisoni", "D. Ballabio"], "venue": "Chemometrics and Intelligent Laboratory", "citeRegEx": "Todeschini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Todeschini et al\\.", "year": 2016}, {"title": "Some inequalities for information divergence and related measures of discrimination", "author": ["F. Topsoe"], "venue": "IEEE Transactions on information theory, 46 (4), 1602\u20131609.", "citeRegEx": "Topsoe,? 2000", "shortCiteRegEx": "Topsoe", "year": 2000}, {"title": "Data Analysis in Management with SPSS Software", "author": ["J.P. Verma"], "venue": "Springer.", "citeRegEx": "Verma,? 2012", "shortCiteRegEx": "Verma", "year": 2012}, {"title": "A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms", "author": ["D. Wettschereck", "D.W. Aha", "T. Mohri"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Wettschereck et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wettschereck et al\\.", "year": 1997}, {"title": "A study of summer foliage insect communities in the Great Smoky Mountains", "author": ["R.H. Whittaker"], "venue": "Ecological monographs, 22 (1), 1\u201344.", "citeRegEx": "Whittaker,? 1952", "shortCiteRegEx": "Whittaker", "year": 1952}, {"title": "Individual comparisons by ranking methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin, 1 (6), 80\u201383.", "citeRegEx": "Wilcoxon,? 1945", "shortCiteRegEx": "Wilcoxon", "year": 1945}, {"title": "Chemical similarity searching", "author": ["P. Willett", "J.M. Barnard", "G.M. Downs"], "venue": "Journal of chemical information and computer sciences ,", "citeRegEx": "Willett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Willett et al\\.", "year": 1998}, {"title": "Computer programs for hierarchical polythetic classification (\u201csimilarity analyses\u201d)", "author": ["W.T. Williams", "G.N. Lance"], "venue": "The Computer,", "citeRegEx": "Williams and Lance,? \\Q1966\\E", "shortCiteRegEx": "Williams and Lance", "year": 1966}, {"title": "Reduction techniques for exemplarbased learning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Machine learning,", "citeRegEx": "Wilson and Martinez,? \\Q2000\\E", "shortCiteRegEx": "Wilson and Martinez", "year": 2000}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H Motoda"], "venue": "Knowledge and information systems,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Query dependent ranking using k-nearest neighbor", "author": ["G. Xiubo", "L. Tie-Yan", "T. Qin", "A. Andrew", "H. Li", "Shum", "H.-Y"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (pp. 115\u2013122)", "citeRegEx": "Xiubo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xiubo et al\\.", "year": 2008}, {"title": "An algorithm for remote sensing image classification based on artificial immune B-cell network. The International Archives of the Photogrammetry", "author": ["S. Xu", "Y. Wu"], "venue": "Remote Sensing and Spatial Information Sciences,", "citeRegEx": "Xu and Wu,? \\Q2008\\E", "shortCiteRegEx": "Xu and Wu", "year": 2008}, {"title": "Improving text categorization methods for event tracking", "author": ["Y. Yang", "T. Ault", "T. Pierce", "C.W. Lattimer"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval (pp. 65\u201372)", "citeRegEx": "Yang et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2000}, {"title": "LazyLSH: Approximate nearest neighbor search for multiple distance functions with a single index", "author": ["Y. Zheng", "Q. Guo", "A.K. Tung", "S. Wu"], "venue": "International Conference on Management of Data (pp. 2023\u20132037)", "citeRegEx": "Zheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2016}, {"title": "TopEVM: using co-occurrence and topology patterns of enzymes in metabolic networks to construct phylogenetic trees", "author": ["T. Zhou", "K.C. Chan", "Z. Wang"], "venue": "In IAPR International Conference on Pattern Recognition in Bioinformatics (pp. 225\u2013236)", "citeRegEx": "Zhou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2008}, {"title": "Class noise vs. attribute noise: A quantitative study", "author": ["X. Zhu", "X. Wu"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Zhu and Wu,? \\Q2004\\E", "shortCiteRegEx": "Zhu and Wu", "year": 2004}], "referenceMentions": [{"referenceID": 75, "context": "KNN has been identified as one of the top ten methods in data mining (Wu et al., 2008).", "startOffset": 69, "endOffset": 86}, {"referenceID": 76, "context": "Thus, KNN comprises the baseline classifier in many pattern classification problems such as pattern recognition (Xu & Wu, 2008), text categorization (Manne, Kotha, & Fatima, 2012), ranking models (Xiubo et al., 2008), object recognition (Bajramovic et al.", "startOffset": 196, "endOffset": 216}, {"referenceID": 5, "context": ", 2008), object recognition (Bajramovic et al., 2006), and event recognition (Yang et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 78, "context": ", 2006), and event recognition (Yang et al., 2000) applications.", "startOffset": 31, "endOffset": 50}, {"referenceID": 25, "context": "KNN was proposed in 1951 by Fix & Hodges (1951), and then modified by Cover & Hart (1967). KNN has been identified as one of the top ten methods in data mining (Wu et al.", "startOffset": 78, "endOffset": 90}, {"referenceID": 5, "context": ", 2008), object recognition (Bajramovic et al., 2006), and event recognition (Yang et al., 2000) applications. KNN is a non-parametric algorithm Kataria & Singh (2013). Non-Parametric means either there are no parameters or fixed number of parameters irrespective of size of data.", "startOffset": 29, "endOffset": 168}, {"referenceID": 5, "context": ", 2008), object recognition (Bajramovic et al., 2006), and event recognition (Yang et al., 2000) applications. KNN is a non-parametric algorithm Kataria & Singh (2013). Non-Parametric means either there are no parameters or fixed number of parameters irrespective of size of data. Instead, parameters would be determined by the size of the training dataset. While there are no assumptions that need to be made to the underlying data distribution. Thus, KNN could be the best choice for any classification study that involves a little or no prior knowledge about the distribution of the data. In addition, KNN is one of the lazy learning methods. This implies storing all training data and waits until having the test data produced, without having to create a learning model Wettschereck, Aha, & John (1997).", "startOffset": 29, "endOffset": 807}, {"referenceID": 12, "context": "Chomboon and co-workers (Chomboon et al., 2015) analyzed the performance of KNN classifier using 11 distance measures.", "startOffset": 24, "endOffset": 47}, {"referenceID": 64, "context": "Punam & Nitin (2015) evaluated the performance of KNN classifier using Chebychev, Euclidean, Manhattan, distance measures on KDD dataset (Tavallaee et al., 2009).", "startOffset": 137, "endOffset": 161}, {"referenceID": 12, "context": "Chomboon and co-workers (Chomboon et al., 2015) analyzed the performance of KNN classifier using 11 distance measures. These include Euclidean, Mahalanobis, Manhattan, Minkowski, Chebychev, Cosine, Correlation, Hamming, Jaccard, Standardized Euclidean and Spearman distances. Their experiment had been applied on eight binary synthetic datasets with various kinds of distributions that were generated using MATLAB. They divided each dataset into 70% for training set and 30% for the testing set. The results showed that the Manhattan, Minkowski, Chebychev, Euclidean, Mahalanobis, and Standardized Euclidean distance measures achieved similar accuracy results and outperformed other tested distances. Punam & Nitin (2015) evaluated the performance of KNN classifier using Chebychev, Euclidean, Manhattan, distance measures on KDD dataset (Tavallaee et al.", "startOffset": 25, "endOffset": 722}, {"referenceID": 12, "context": "Chomboon and co-workers (Chomboon et al., 2015) analyzed the performance of KNN classifier using 11 distance measures. These include Euclidean, Mahalanobis, Manhattan, Minkowski, Chebychev, Cosine, Correlation, Hamming, Jaccard, Standardized Euclidean and Spearman distances. Their experiment had been applied on eight binary synthetic datasets with various kinds of distributions that were generated using MATLAB. They divided each dataset into 70% for training set and 30% for the testing set. The results showed that the Manhattan, Minkowski, Chebychev, Euclidean, Mahalanobis, and Standardized Euclidean distance measures achieved similar accuracy results and outperformed other tested distances. Punam & Nitin (2015) evaluated the performance of KNN classifier using Chebychev, Euclidean, Manhattan, distance measures on KDD dataset (Tavallaee et al., 2009). The KDD dataset contains 41 features and two classes which type of data is numeric. The dataset was normalized before conducting the experiment. To evaluate the performance of KNN, accuracy, sensitivity and specificity measures were calculated for each distance. The reported results indicate that the use of Manhattan distance outperform the other tested distances, with 97.8% accuracy rate, 96.76% sensitivity rate and 98.35% Specificity rate. Hu et al. (2016) analyzed the effect of distance measures on KNN classifier for medical domain datasets.", "startOffset": 25, "endOffset": 1327}, {"referenceID": 27, "context": "Alkasassbeh, Altarawneh, & Hassanat (2015) investigated the effect of Euclidean, Manhattan and Hassanat (Hassanat, 2014) distance metrics on the performance of the KNN classifier, with K ranging from 1 to the square root of the size of the training set, considering only the odd K\u2019s.", "startOffset": 104, "endOffset": 120}, {"referenceID": 27, "context": "In addition to experimenting on other classifiers such as the Ensemble Nearest Neighbor classifier (ENN) (Hassanat, 2014), and the Inverted Indexes of Neighbors Classifier (IINC) (Jirina & Jirina, 2010).", "startOffset": 105, "endOffset": 121}, {"referenceID": 17, "context": "data, which were chosen from the UCI machine learning repository, and four distance metrics including Euclidean, Cosine, Chi square, and Minkowsky distances. They divided each dataset into 90% of data as training and 10% as testing set, with k values from ranging from 1 to 15. The experimental results showed that Chi square distance function was the best choice for the three different types of datasets. However, using the Cosine, Euclidean and Minkowsky distance metrics performed the \u2018worst\u2019 over the mixed type of datasets. The \u2018worst\u2019 performance means the method with the lowest accuracy. Todeschini, Ballabio, & Consonni (2015); Todeschini et al.", "startOffset": 102, "endOffset": 637}, {"referenceID": 17, "context": "data, which were chosen from the UCI machine learning repository, and four distance metrics including Euclidean, Cosine, Chi square, and Minkowsky distances. They divided each dataset into 90% of data as training and 10% as testing set, with k values from ranging from 1 to 15. The experimental results showed that Chi square distance function was the best choice for the three different types of datasets. However, using the Cosine, Euclidean and Minkowsky distance metrics performed the \u2018worst\u2019 over the mixed type of datasets. The \u2018worst\u2019 performance means the method with the lowest accuracy. Todeschini, Ballabio, & Consonni (2015); Todeschini et al. (2016) analyzed the effect of eighteen different distance measures on the performance of KNN classifier using eight benchmark datasets.", "startOffset": 102, "endOffset": 663}, {"referenceID": 13, "context": "The investigated distance measures included Manhattan, Euclidean, Soergel, Lance\u2013Williams, contracted Jaccard\u2013 Tanimoto, Jaccard\u2013Tanimoto, Bhattacharyya, Lagrange, Mahalanobis, Canberra, Wave-Edge, Clark, Cosine, Correlation and four Locally centered Mahalanobis distances. For evaluating the performance of these distances, the non-error rate and average rank were calculated for each distance. The result indicated that the \u2018best\u2019 performance were the Manhattan, Euclidean, Soergel, Contracted Jaccard\u2013Tanimoto and Lance\u2013Williams distance measures. The \u2018best\u2019 performance means the method with the highest accuracy. Lopes & Ribeiro (2015) analyzed the impact of five distance metrics, namely Euclidean, Manhattan, Canberra, Chebychev and Minkowsky in instance-based learning algorithms.", "startOffset": 198, "endOffset": 641}, {"referenceID": 13, "context": "The investigated distance measures included Manhattan, Euclidean, Soergel, Lance\u2013Williams, contracted Jaccard\u2013 Tanimoto, Jaccard\u2013Tanimoto, Bhattacharyya, Lagrange, Mahalanobis, Canberra, Wave-Edge, Clark, Cosine, Correlation and four Locally centered Mahalanobis distances. For evaluating the performance of these distances, the non-error rate and average rank were calculated for each distance. The result indicated that the \u2018best\u2019 performance were the Manhattan, Euclidean, Soergel, Contracted Jaccard\u2013Tanimoto and Lance\u2013Williams distance measures. The \u2018best\u2019 performance means the method with the highest accuracy. Lopes & Ribeiro (2015) analyzed the impact of five distance metrics, namely Euclidean, Manhattan, Canberra, Chebychev and Minkowsky in instance-based learning algorithms. Particularly, 1-NN Classifier and the Incremental Hypersphere Classifier (IHC) Classifier, they reported the results of their empirical evaluation on fifteen datasets with different sizes showing that the Euclidean and Manhattan metrics significantly yield good results comparing to the other tested distances. Alkasassbeh, Altarawneh, & Hassanat (2015) investigated the effect of Euclidean, Manhattan and Hassanat (Hassanat, 2014) distance metrics on the performance of the KNN classifier, with K ranging from 1 to the square root of the size of the training set, considering only the odd K\u2019s.", "startOffset": 198, "endOffset": 1143}, {"referenceID": 13, "context": "The investigated distance measures included Manhattan, Euclidean, Soergel, Lance\u2013Williams, contracted Jaccard\u2013 Tanimoto, Jaccard\u2013Tanimoto, Bhattacharyya, Lagrange, Mahalanobis, Canberra, Wave-Edge, Clark, Cosine, Correlation and four Locally centered Mahalanobis distances. For evaluating the performance of these distances, the non-error rate and average rank were calculated for each distance. The result indicated that the \u2018best\u2019 performance were the Manhattan, Euclidean, Soergel, Contracted Jaccard\u2013Tanimoto and Lance\u2013Williams distance measures. The \u2018best\u2019 performance means the method with the highest accuracy. Lopes & Ribeiro (2015) analyzed the impact of five distance metrics, namely Euclidean, Manhattan, Canberra, Chebychev and Minkowsky in instance-based learning algorithms. Particularly, 1-NN Classifier and the Incremental Hypersphere Classifier (IHC) Classifier, they reported the results of their empirical evaluation on fifteen datasets with different sizes showing that the Euclidean and Manhattan metrics significantly yield good results comparing to the other tested distances. Alkasassbeh, Altarawneh, & Hassanat (2015) investigated the effect of Euclidean, Manhattan and Hassanat (Hassanat, 2014) distance metrics on the performance of the KNN classifier, with K ranging from 1 to the square root of the size of the training set, considering only the odd K\u2019s. In addition to experimenting on other classifiers such as the Ensemble Nearest Neighbor classifier (ENN) (Hassanat, 2014), and the Inverted Indexes of Neighbors Classifier (IINC) (Jirina & Jirina, 2010). Their experiments were conducted on 28 datasets taken from the UCI machine learning repository, the reported results show that Hassanat distance outperformed both of Manhattan and Euclidean distances in most of the tested datasets using the three investigated classifiers. Lindi (2016) investigated three distance metrics to use the best performer among them with the KNN classifier, which was employed as a matcher for their face recognition system that was proposed for the NAO robot.", "startOffset": 198, "endOffset": 1874}, {"referenceID": 33, "context": "Therefore, distance measures play a vital role in determining the final classification output (Hu et al., 2016).", "startOffset": 94, "endOffset": 111}, {"referenceID": 29, "context": "The first problem was solved either by using all the examples and taking the inverted indexes (Jirina & Jirina, 2008), or using ensemble learning (Hassanat et al., 2014).", "startOffset": 146, "endOffset": 169}, {"referenceID": 79, "context": "& , 2000) and Wilson & Martinez (2000), or using approximate KNN classification such as (Arya & Mount, 1993) and (Zheng et al., 2016).", "startOffset": 113, "endOffset": 133}, {"referenceID": 57, "context": "The generation of noise can be classified by three main characteristics (Saez et al., 2013):", "startOffset": 72, "endOffset": 91}, {"referenceID": 18, "context": "Euclid, one of the most important mathematicians of the ancient history, used the word distance only in his third postulate of the Principia (Euclid, 1956): \u201cEvery circle can be described by a centre and a distance\u201d.", "startOffset": 141, "endOffset": 155}, {"referenceID": 24, "context": "2 Chebyshev (CD): Chebyshev distance is also known as maximum value distance (Grabusts, 2011), Lagrange (Todeschini, Ballabio, & Consonni, 2015) and chessboard distance (Premaratne, 2014).", "startOffset": 77, "endOffset": 93}, {"referenceID": 54, "context": "2 Chebyshev (CD): Chebyshev distance is also known as maximum value distance (Grabusts, 2011), Lagrange (Todeschini, Ballabio, & Consonni, 2015) and chessboard distance (Premaratne, 2014).", "startOffset": 169, "endOffset": 187}, {"referenceID": 68, "context": "This distance is appropriate in cases when two objects are to be defined as different if they are different in any one dimension (Verma, 2012).", "startOffset": 129, "endOffset": 142}, {"referenceID": 61, "context": "3 Sorensen distance (SD): The Sorensen distance (Sorensen, 1948), also known as Bray\u2013Curtis is one of the most commonly applied measurements to express relationships in ecology, environmental sciences and related fields.", "startOffset": 48, "endOffset": 64}, {"referenceID": 34, "context": "1 Jaccard distance (JacD): The Jaccard distance measures dissimilarity between sample sets, it is a complementary to the Jaccard similarity coefficient (Jaccard, 1901) and is obtained by subtracting the Jaccard coefficient from one.", "startOffset": 152, "endOffset": 167}, {"referenceID": 17, "context": "3 Dice distance (DicD): The dice distance is derived from the dice similarity (Dice, 1945), which is a complementary to the dice similarity and is obtained by subtracting the dice similarity from one.", "startOffset": 78, "endOffset": 90}, {"referenceID": 50, "context": "4 Chord distance (ChoD): A modification of Euclidean distance (Gan, Ma, & Wu, 2007), which was introduced by Orloci (Orloci, 1967) to be used in analyzing community composition data (Legendre & Legendre, 2012).", "startOffset": 116, "endOffset": 130}, {"referenceID": 8, "context": "1 Bhattacharyya distance (BD): The Bhattacharyya distance measures the similarity of two probability distributions (Bhattachayya, 1943).", "startOffset": 115, "endOffset": 135}, {"referenceID": 32, "context": "4 Hellinger distance (HeD): Hellinger distance also called Jeffries - Matusita distance (Abbad & Tairi, 2016) was introduced in 1909 by Hellinger (Hellinger, 1909), it is a metric used to measure the similarity between two probability distributions.", "startOffset": 146, "endOffset": 163}, {"referenceID": 52, "context": "4 Pearson \u03c7 distance (PCSD): Pearson \u03c7 distance (Pearson, 1900), also called \u03c7 distance.", "startOffset": 48, "endOffset": 63}, {"referenceID": 58, "context": "Shannon entropy distance measures: The distance measures belonging to this family are related to the Shannon entropy (Shannon, 2001).", "startOffset": 117, "endOffset": 132}, {"referenceID": 35, "context": "These distances include Kullback-Leibler, Jeffreys, K divergence, Topsoe, Jensen-Shannon, Jensen difference distances. 6.1 Kullback-Leibler distance (KLD): Kullback-Leibler distance was introduced by Kullback & Leibler (1951), it is also known as KL divergence, relative entropy, or information deviation, which measures the difference between two probability distributions.", "startOffset": 42, "endOffset": 226}, {"referenceID": 35, "context": "2 Jeffreys Distance (JefD): Jeffreys distance (Jeffreys, 1946), also called J-divergence or KL2- distance, is a symmetric version of the KullbackLeibler distance.", "startOffset": 46, "endOffset": 62}, {"referenceID": 67, "context": "4 Topsoe Distance (TopD): The Topsoe distance (Topsoe, 2000), also called information statistics, is a symmetric version of the KullbackLeibler distance.", "startOffset": 46, "endOffset": 60}, {"referenceID": 60, "context": "6 Jensen difference distance (JDD): Jensen difference distance was introduced by Sibson (1969).", "startOffset": 81, "endOffset": 95}, {"referenceID": 46, "context": "1 Vicis-Wave Hedges distance (VWHD): The so-called \u201dWave-Hedges distance\u201d has been applied to compressed image retrieval (Hatzigiorgaki & Skodras, 2003), content based video retrieval (Patel & Meshram, 2012), time series classification (Giusti & Batista, 2013),image fidelity (Macklem, 2002), finger print recognition (Bharkad & Kokare, 2011), etc.", "startOffset": 276, "endOffset": 291}, {"referenceID": 31, "context": "Vicissitude distance measures: Vicissitude distance family consists of four distances, Vicis-Wave Hedges, Vicis Symmetric, Max Symmetric \u03c7 and Min Symmetric \u03c7 distances. These distances were generated from syntactic relationship for the aforementioned distance measures. 7.1 Vicis-Wave Hedges distance (VWHD): The so-called \u201dWave-Hedges distance\u201d has been applied to compressed image retrieval (Hatzigiorgaki & Skodras, 2003), content based video retrieval (Patel & Meshram, 2012), time series classification (Giusti & Batista, 2013),image fidelity (Macklem, 2002), finger print recognition (Bharkad & Kokare, 2011), etc.. Interestingly, the source of the \u201dWave-Hedges\u201d metric has not been correctly cited, and some of the previously mentioned resources allude to it incorrectly as Hedges (1976). The source", "startOffset": 98, "endOffset": 796}, {"referenceID": 27, "context": "Even the name of the distance \u201dWave-Hedges\u201d is questioned (Hassanat, 2014).", "startOffset": 58, "endOffset": 74}, {"referenceID": 63, "context": "3 Taneja Distance (TanD): (Taneja, 1995)", "startOffset": 26, "endOffset": 40}, {"referenceID": 20, "context": "4 Pearson Distance (PeaD): The Pearson distance is derived from the Pearsons correlation coefficient, which measures the linear relationship between two vectors (Fulekar, 2009).", "startOffset": 161, "endOffset": 176}, {"referenceID": 25, "context": "7 Hamming Distance (HamD): Hamming distance (Hamming, 1958) is a distance metric that measures the number of mismatches between two vectors.", "startOffset": 44, "endOffset": 59}, {"referenceID": 38, "context": "9 \u03c7 statistic Distance (CSSD): The \u03c7 statistic distance was used for image retrieval (Kadir et al., 2012), histogram (Rubner & Tomasi, 2013), etc.", "startOffset": 85, "endOffset": 105}, {"referenceID": 70, "context": "10 Whittaker\u2019s index of association Distance (WIAD): Whittaker\u2019s index of association distance was designed for species abundance data (Whittaker, 1952).", "startOffset": 135, "endOffset": 152}, {"referenceID": 27, "context": "13 Hassanat Distance (HasD): Hassanat Distance introduced by Hassanat (2014).", "startOffset": 3, "endOffset": 77}, {"referenceID": 27, "context": "By satisfying all the metric properties this distance was proved to be a metric by Hassanat (2014). In this metric no matter what the difference between two values is, the distance will be in the range of 0 to 1.", "startOffset": 83, "endOffset": 99}, {"referenceID": 43, "context": "Datasets used for experiments The experiments were done on twenty eight datasets which represent real life classification problems, obtained from the UCI Machine Learning Repository (Lichman, 2013).", "startOffset": 182, "endOffset": 197}, {"referenceID": 71, "context": "To further analyze the performance of Hassanat distance comparing with other top distances we used the Wilcoxon\u2019s rank-sum test (Wilcoxon, 1945).", "startOffset": 128, "endOffset": 144}, {"referenceID": 15, "context": "05) then we reject the null hypothesis, and conclude that there is a significant difference between the tested samples; otherwise we cannot conclude anything about the significant difference (Derrac et al., 2011).", "startOffset": 191, "endOffset": 212}], "year": 2017, "abstractText": "The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested example and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures? This review attempts to answer the previous question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. We found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances. In addition, the performance of the KNN degraded only about 20% while the noise level reaches 90%, this is true for all the distances used. This means that the KNN classifier using any of the top 10 distances tolerate noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing to other distances.", "creator": "LaTeX with hyperref package"}}}