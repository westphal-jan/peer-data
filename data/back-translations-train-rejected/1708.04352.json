{"id": "1708.04352", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Benchmark Environments for Multitask Learning in Continuous Domains", "abstract": "As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly. In this work, we describe a benchmark set of tasks that we have developed in an extendable framework based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains.", "histories": [["v1", "Mon, 14 Aug 2017 22:55:03 GMT  (560kb,D)", "http://arxiv.org/abs/1708.04352v1", "Accepted at Lifelong Learning: A Reinforcement Learning Approach Workshop @ ICML, Sydney, Australia, 2017"]], "COMMENTS": "Accepted at Lifelong Learning: A Reinforcement Learning Approach Workshop @ ICML, Sydney, Australia, 2017", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peter henderson", "wei-di chang", "florian shkurti", "johanna hansen", "david meger", "gregory dudek"], "accepted": false, "id": "1708.04352"}, "pdf": {"name": "1708.04352.pdf", "metadata": {"source": "META", "title": "Benchmark Environments for Multitask Learning in Continuous Domains", "authors": ["Peter Henderson", "Wei-Di Chang", "Florian Shkurti", "Johanna Hansen", "David Meger", "Gregory Dudek"], "emails": ["<peter.henderson@mail.mcgill.ca>.", "@ICML,"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if most people are able to survive themselves by going in search of another way that leads them to another world. (...) It is not as if they go into another world. (...) It is not as if they go into another world. (...) It is as if they go into another world. (...) It is as if they go into another world. (...) It is as if they go into another world. (...) It is as if they go into another world. (...). (...) It is as if they go into another world. (...) It is as if they enter into another world. (...) It is as if they enter into another world. (...) It is as if they enter into another world. (...) It is as if they enter into another world."}, {"heading": "2. Related Work", "text": "Several papers examine multitasking or transfer learning with MuJoCo tasks. These tasks include: navigating around a wall (where a wall separates an agent from its target); the OpenAI Gym Reacher environment with an additional image status room of the environment; jumping over a wall with a model similar to the OpenAI HalfCheetah environment (Finn et al., 2016); varying the gravity of different standard OpenAI Gym benchmark environments (Reacher, Hopper, Humanoid, HalfCheetah) and transferring between the modified environments; adding motor noise to the same group of environments (Christiano et al., 2016); simulated gripping and stacking with a Jaco arm (Rusu et al., 2016); and several custom grips and manipulation tasks to demonstrate invariant feature spaces (Gupta et al., 2017). Other papers examine the use of classical control systems with a series of amplifiers and simple robotics for each one."}, {"heading": "3. Environments", "text": "In our first release of the gym-extensions framework2, we include a number of modifications to the standard gym environments as well as novel continuous domains, and provide a framework that allows easy modification of the environment traits. 2Found at: https: / / github.com / Breakend / gym-extensions /. Pull requests and issues are welcome. Further details for each environment are provided in the opensource repository, as well as a place to upload new benchmark algorithms."}, {"heading": "3.1. Mujoco", "text": "We base our modified environments on the existing \"running\" (Humanoid, Hopper, Half-Cheetah and Walker2d) and \"arm-based\" (Pusher and Striker) environments in OpenAI Gym. First, we provide a high-level overview of our modifications and group suggestions, then we show the specific environment names in our benchmarking results."}, {"heading": "3.1.1. GRAVITY MODIFICATIONS", "text": "For the running agents, we provide ready-made environments with different scales of simulated Earth-like gravity ranging from half to half the normal gravity level (\u2212 4.91 to \u2212 12.26 m \u00b7 s \u2212 2 in steps of 0.25 gearth). We suggest that a successful multitasking learning algorithm extracts the underlying structure of the hakiness and reuses the applicable knowledge without forgetting how to run under different gravity conditions."}, {"heading": "3.1.2. WALL AND SENSOR ENVIRONMENTS", "text": "Inspired by the wall jumping experiment in (Finn et al., 2016), we are building a series of similar environments by adding a multi-beam noise range sensor to the OpenAI running tasks. We emit rays from the runner's upper body (with an arc of 90 degrees, 10 beams, a maximum sensor distance of 10 meters and normalized readings to a range of [0, 1]) for the measurements. We offer the usual running tasks with sensor perception activated (no readings as there is no wall) and additional environments with a wall located in the agent's path at a point pulled by an even distribution from 1.8 to 3.8 meters from the agent's starting point."}, {"heading": "3.1.3. MORPHOLOGY MODIFICATIONS", "text": "We define \"large\" body parts as scaling the mass and width of the limbs by 1.25 and \"small\" body parts as scaling by 0.75. We also group limbs for environments with multiple appendages (i.e. humanoid trunk includes the abdomen; humanoid thigh includes the hips; all appendages include both the left / right as well as the front / rear part simultaneously, so that a modified thigh includes both thighs)."}, {"heading": "3.1.4. ROBOT ARM MODIFICATIONS", "text": "In the OpenAI Striker and Pusher tasks, a 7 DoF arm tries to punch a ball into a hole or push a pin into a goal position. We expand these tasks to randomly shift the goal position for the pusher task, and move the ball start position for the Striker task randomly. As with the original tasks, we tie the varied goal or start state within a limited uniform distribution as a domain accordingly."}, {"heading": "3.1.5. HUMANOID MULTITASK", "text": "The reward scale for this task is quite large, but it is based on the HumanoidStandup v1 environment in OpenAI Gym. In addition, we offer a version of each environment with a sensor display as in Section 3.1.2. If no wall is used, all sensors read zero. If a wall is used, everyone returns a distance to the wall as described above."}, {"heading": "3.2. 2D Navigation", "text": "We also provide several novel 2D environments that focus on navigation tasks with continuous action spaces to enable benchmarking of learning tasks that require implicit memory. Tasks take place in a pre-defined occupancy grid map, similar to (Tamar et al., 2016). We choose to set the layout and shape of obstacles as the only unique feature for localization within the map. Apart from this information, the environment does not have a texture map or other distinctive features. We offer three different types of navigation tasks that vary according to difficulty level: \u2022 Image-based navigation, where the agent has access to the entire map, including his own position within the map and the destination in the map as part of the image data. \u2022 State-based navigation, where the agent has access to his own position on the map and the distance and carrying to the nearest obstacle. A simpler version also includes destination coordinates based on navigation around the beam area only."}, {"heading": "4. Multitask Sets", "text": "We develop several sets of intuitive workgroups that can serve as simple benchmarks limited in complexity both within the group and in our list order. Specific environment names can be found in Table 1, 2, 3 and 4. For navigation tasks, we list the environments inline here.We present the following environment groups: \u2022 Modified environments with different gravity parameters 6 \u2022 Modified environments with sensor evaluations (simply zero if no wall) and permutated with a random wall in the running path \u2022 The OpenAI Gym Striker environment with both random start position of the object and random target state \u2022 The OpenAI Gym Pusher environment with random start position of the object as well as random target state \u2022 Learning for a human model \u2022 Learning to stand up, walk and jump over walls for a Hu-Manoid model \u2022 Learning to walk with different articulations."}, {"heading": "5. Baseline Experiments", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6. Conclusion", "text": "Our first paper explores how to give more flexibility to standard MuJoCo environments in OpenAI gyms: by modifying gravity, adding sensor evaluations and a random wall obstruction, by disrupting body part sizes, and by adding random target / start positions for arm environments. We also add a number of original environments to learn guidelines for continuous navigation tasks. In future publications, we also plan standard environments for: adding motor noise, arm environments where the final target position has a speed (so the arm must track the target), and making sensor-based environments more realistic (and thus more transferable to real systems)."}], "references": [{"title": "Online multi-task learning for policy gradient methods", "author": ["Ammar", "Haitham B", "Eaton", "Eric", "Ruvolo", "Paul", "Taylor", "Matthew"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Multitask learning. In Learning to learn, pp. 95\u2013133", "author": ["Caruana", "Rich"], "venue": null, "citeRegEx": "Caruana and Rich.,? \\Q1998\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1998}, {"title": "Transfer from simulation to real world through learning deep inverse dynamics model", "author": ["Christiano", "Paul", "Shah", "Zain", "Mordatch", "Igor", "Schneider", "Jonas", "Blackwell", "Trevor", "Tobin", "Joshua", "Abbeel", "Pieter", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1610.03518,", "citeRegEx": "Christiano et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Christiano et al\\.", "year": 2016}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Generalizing skills with semisupervised reinforcement learning", "author": ["Finn", "Chelsea", "Yu", "Tianhe", "Fu", "Justin", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1612.00429,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Ghahramani", "Zoubin", "Turner", "Richard E", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1611.02247,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["Gupta", "Abhishek", "Devin", "Coline", "Liu", "YuXuan", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1703.02949,", "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Adapting learned robotics behaviours through policy adjustment", "author": ["Higuera", "Juan Camilo Gamboa", "Meger", "David", "Dudek", "Gregory"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Higuera et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Higuera et al\\.", "year": 2017}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["McCloskey", "Michael", "Cohen", "Neal J"], "venue": "Psychology of learning and motivation,", "citeRegEx": "McCloskey et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCloskey et al\\.", "year": 1989}, {"title": "Adaptive smoothed online multi-task learning", "author": ["Murugesan", "Keerthiram", "Liu", "Hanxiao", "Carbonell", "Jaime", "Yang", "Yiming"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Murugesan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Murugesan et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1511.06342,", "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Policy distillation", "author": ["Rusu", "Andrei A", "Colmenarejo", "Sergio Gomez", "Gulcehre", "Caglar", "Desjardins", "Guillaume", "Kirkpatrick", "James", "Pascanu", "Razvan", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Sim-toreal robot learning from pixels with progressive nets", "author": ["Rusu", "Andrei A", "Vecerik", "Matej", "Roth\u00f6rl", "Thomas", "Heess", "Nicolas", "Pascanu", "Razvan", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael", "Moritz", "Philipp"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["Thrun", "Sebastian"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Thrun and Sebastian.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and Sebastian.", "year": 1996}], "referenceMentions": [{"referenceID": 10, "context": "Additionally, by training on multiple tasks, the agent can exploit common traits to gain efficiency and generalize to unseen tasks (Caruana, 1998; Murugesan et al., 2016; Finn et al., 2016).", "startOffset": 131, "endOffset": 189}, {"referenceID": 4, "context": "Additionally, by training on multiple tasks, the agent can exploit common traits to gain efficiency and generalize to unseen tasks (Caruana, 1998; Murugesan et al., 2016; Finn et al., 2016).", "startOffset": 131, "endOffset": 189}, {"referenceID": 11, "context": "However, despite several authors\u2019 attempts to demonstrate multitask learning in both Atari tasks and the DeepMind Lab Labyrinth (Parisotto et al., 2015; Rusu et al., 2015; Jaderberg et al., 2016), this task that has yet to be convincingly solved.", "startOffset": 128, "endOffset": 195}, {"referenceID": 12, "context": "However, despite several authors\u2019 attempts to demonstrate multitask learning in both Atari tasks and the DeepMind Lab Labyrinth (Parisotto et al., 2015; Rusu et al., 2015; Jaderberg et al., 2016), this task that has yet to be convincingly solved.", "startOffset": 128, "endOffset": 195}, {"referenceID": 8, "context": "However, despite several authors\u2019 attempts to demonstrate multitask learning in both Atari tasks and the DeepMind Lab Labyrinth (Parisotto et al., 2015; Rusu et al., 2015; Jaderberg et al., 2016), this task that has yet to be convincingly solved.", "startOffset": 128, "endOffset": 195}, {"referenceID": 4, "context": "These tasks include: navigating around a wall (where a wall separates an agent from its goal); the OpenAI Gym Reacher environment with an added image state space of the environment; jumping over a wall using a model similar to the OpenAI HalfCheetah environment (Finn et al., 2016); varying the gravity of various standard OpenAI Gym benchmark environments (Reacher, Hopper, Humanoid, HalfCheetah) and transferring between the modified environments; adding motor noise to the same set of environments (Christiano et al.", "startOffset": 262, "endOffset": 281}, {"referenceID": 2, "context": ", 2016); varying the gravity of various standard OpenAI Gym benchmark environments (Reacher, Hopper, Humanoid, HalfCheetah) and transferring between the modified environments; adding motor noise to the same set of environments (Christiano et al., 2016); simulated grasping and stacking using a Jaco arm (Rusu et al.", "startOffset": 227, "endOffset": 252}, {"referenceID": 13, "context": ", 2016); simulated grasping and stacking using a Jaco arm (Rusu et al., 2016); and several custom grasping and manipulation tasks to demonstrate learning invariant feature spaces (Gupta et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 6, "context": ", 2016); and several custom grasping and manipulation tasks to demonstrate learning invariant feature spaces (Gupta et al., 2017).", "startOffset": 109, "endOffset": 129}, {"referenceID": 0, "context": "These include: a simple mass spring damper task, cart-pole with continuous control; a threelink inverted pendulum with continuous control; a quadrotor control task (Ammar et al., 2014); a double-linked pendulum task; a modified cartpole balancing task which can transfer to physical system (Higuera et al.", "startOffset": 164, "endOffset": 184}, {"referenceID": 7, "context": ", 2014); a double-linked pendulum task; a modified cartpole balancing task which can transfer to physical system (Higuera et al., 2017).", "startOffset": 113, "endOffset": 135}, {"referenceID": 4, "context": "Inspired by the wall jumping experiment in (Finn et al., 2016), we build a set of similar environments by extending the OpenAI running tasks to use a multi-beam noiseless range sensor.", "startOffset": 43, "endOffset": 62}, {"referenceID": 3, "context": "For an initial baseline, we simply run the RLLab (Duan et al., 2016) implementation of Trust Region Policy Optimization (Schulman et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 14, "context": ", 2016) implementation of Trust Region Policy Optimization (Schulman et al., 2015) (TRPO) using an identical policy network to (Gu et al.", "startOffset": 59, "endOffset": 82}, {"referenceID": 5, "context": ", 2015) (TRPO) using an identical policy network to (Gu et al., 2016)7.", "startOffset": 52, "endOffset": 69}, {"referenceID": 3, "context": "The results we see are on a comparable scale to (Duan et al., 2016).", "startOffset": 48, "endOffset": 67}], "year": 2017, "abstractText": "As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly. In this work, we describe a benchmark set of tasks that we have developed in an extendable framework based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains.", "creator": "LaTeX with hyperref package"}}}