{"id": "1708.04390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "Fluency-Guided Cross-Lingual Image Captioning", "abstract": "Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual setting. Different from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of fluency in the translated sentences, we propose in this paper a fluency-guided learning framework. The framework comprises a module to automatically estimate the fluency of the sentences and another module to utilize the estimated fluency scores to effectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both fluency and relevance of the generated captions in Chinese, but without using any manually written sentences from the target language.", "histories": [["v1", "Tue, 15 Aug 2017 03:46:31 GMT  (1014kb,D)", "http://arxiv.org/abs/1708.04390v1", "9 pages, 2 figures, accepted as ORAL by ACM Multimedia 2017"]], "COMMENTS": "9 pages, 2 figures, accepted as ORAL by ACM Multimedia 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["weiyu lan", "xirong li", "jianfeng dong"], "accepted": false, "id": "1708.04390"}, "pdf": {"name": "1708.04390.pdf", "metadata": {"source": "META", "title": "Fluency-Guided Cross-Lingual Image Captioning", "authors": ["Weiyu Lan", "Xirong Li", "Jianfeng Dong"], "emails": ["(xirong@ruc.edu.cn).", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computing methodologies \u2192 Scene understanding; Natural language generation; KEYWORDS Cross-lingual image captioning, English-Chinese, Sentence uency ACM Reference format: Weiyu Lan, Xirong Li, and Jianfeng Dong. 2017. Fluency-Guided CrossLingual Image Captioning. In Proceedings of MM '17, 23-27 October 2017, Mountain View, CA, USA., 9 pages. DOI: h ps: / / doi.org / 10.1145 / 3123266.3123366"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able, in which they are able, in which they are able to integrate, in which they are able, in which they are able, in which they are able, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 PROGRESS ON IMAGE CAPTIONING", "text": "In fact, it is the case that the images that have been created in recent years are not only images, but also images that have been created in recent years. [10] In fact, the images that have been created in recent years are images. [11] In fact, the images that have been created in the past are images. [12] In fact, the images are images that have been created in the past. [13] In the past, it has been shown that the images are images that have been created in the past. [13] In the present, it is the case that the images are images that have been created in the past. [13] In the present, it is the case that they are images that have been created in the past."}, {"heading": "3 OUR APPROACH", "text": "Our goal is to develop a caption model for a target language, but without the need for manual captions in that language for training. This is achieved through a novel linguistic use of captions from a source language. Since public records for captions are available in English [16, 26, 43], while Chinese is the most widely spoken language in the world, we consider English-Chinese to be the linguistic language. Let's use English sentences that describe a given set of captions. Machine translation of these sentences allows us to automatically obtain their Chinese equivalents {Sc}. To this end, we suggested a frequency-focused learning framework, as in Figure 2. We then transfer the measurement Senture to automate it as a single Estimation."}, {"heading": "3.1 Sentence Fluency Estimation", "text": "It is therefore worthwhile to come to the idea that it is a matter of a way in which it is about a way in which it is about the terms that people in the individual countries do not understand. (...) It is the way in which people in the individual countries relate to each other in the most different terms. (...) It is the way in which people in the individual countries relate to each other in the most different expressions. (...) It is the way in which people in the most different expressions relate to each other in the most different expressions. (...) It is the way in which people in the most different expressions relate to each other in the most different expressions. (...) It is the way in which people in the most different expressions relate to each other in the most different expressions. (...) It is the way in which people in the different expressions relate to each other in the most different expressions relate to the most different expressions. (...) It is the way in which people in the most different expressions relate to each other in the most different expressions relate to the most different expressions. (...) It is the way in which people in the most different expressions relate to the most different expressions relate to each other in the most different expressions relate to the most different expressions. (...) It is the way in which people in which people in the most different expressions relate to the most different expressions relate to each other in the most different expressions relate to the most different expressions. (...) It is the way in which people in which people in the most different expressions relate to the most different expressions relate to the most different expressions relate to each other in the most different expressions."}, {"heading": "3.2 Model for Image Captioning", "text": "For the Chinese image processing generation, we are following a popular CNN + LSTM approach, that of Vinyals et al. [36]. Formally, we are aiming for an automatic prediction of a Chinese sentence S = (w1, w2,..., wn), which shortly describes the visual content of the image. [36] A probabilistic model is used to estimate the posterior probability of a certain sequence of words given by the image of the image. [40] The probability is expressed as p (S | I; I; I). Applying the chain rule along with the log probability for the ease of calculation, we havelogp (S; I; I)."}, {"heading": "3.3 Fluency-Guided Training", "text": "Now that the sentence uency classi er and the image capturing model have been introduced, we are ready to discuss how to guide the training process in the light of the estimated uency and consequently generate be-formed Chinese captions. While the question is new when we consider uency as a measure of the importance of each training sample, we see some conceptual similarity to a machine learning scenario in which some samples are more important than others. A typical case is learning from a data set with highly unbalanced classes in which one could consider downsampling classes in the majority, over-sampling classes in minorities or re-weighting of samples [7, 38]. In our context, common sentences are relatively scarce. Inspired by such a combination, we propose three strategies for uency-guided training, Strategy I: Fluency only."}, {"heading": "4 EXPERIMENTS", "text": "The main purpose of our experiments is to test whether a cross-language subtitling model trained by uency-guided learning can produce more Chinese captions that are more relevant than learning from the entire set of machine-translated sentences. We call this baseline \"Without Uency.\" Since estimating sentence frequency is a prerequisite for uency-guided learning, we first evaluate this component."}, {"heading": "4.1 Sentence Fluency Estimation", "text": "Setup. In order to learn the four-way sentence uency classsi er, a number of paired bilingual sentences called uent / not uent is a prerequisite. We aim to select a representative and diverse sentence for manual verification, while manual annotation remains affordable. To this end, we randomly sample 2k and 6k English sentences from Flickr8k [16] and MSCOCO [26] in our laboratory. Specifically, each Chinese sentence was presented separately to two commentators, asking them to rate the sentence as usual, insufficient, or di cult to say. A sentence is considered uent if it contains no obvious grammatical errors and is consistent with the language habits of Chinese. Sentences that receive inconsistent notes or as di cult."}, {"heading": "4.2 Image Caption Generation", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.3 Discussion", "text": "While we are studying English-Chinese as an instance of a cross-language caption, given the availability of some language comments in that language, it is easy to expand the proposed method to another target language. Note that, compared to manually writing sentences for training images, given the English captions and their machine translation results, manual annotations for language modelling are much less. Marking language comments requires only one click. By contrast, if the translation is unsatisfactory, you need to make a number of edits to the translated caption. According to our experiments, 89% of the translations provided are re-edited by commentators. Consequently, it takes an average of about 64 seconds to get a decent Chinese caption, while just 5 seconds are needed to get a comment label for language comments."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we present an approach to cross-language captioning through the use of machine translation. To address the lack of translation competence in machine translated sentences, a learning framework based on translation competence is proposed. Experiments with two English-Chinese data sets, i.e. Flickr8k-cn and Flickr30-cn, support our conclusions as follows: less than 30% of the translated sentences are considered sufficient, leaving much room for further improvement in the current machine translation. Meanwhile, the proposed scan translation management is the challenge. Judging by BLEU-4, ROUGE and CIDER, which place emphasis on predicting relevant terms, the proposed approach is on a par with the baseline that learns from all translated sentences."}, {"heading": "ACKNOWLEDGMENTS", "text": "is a work supported by the National Science Foundation of China (No. 61672523, 71531012). We thank the anonymous reviewers for their insightful comments. A Titan X Pascal GPU used for this research was donated by NVIDIA Corporation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "author": ["R. Bernardi", "R. Cakici", "D. Ellio", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "J. Artif. Intell. Res", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Re-evaluation the Role of Bleu in Machine Translation Research", "author": ["C. Callison-Burch", "M. Osborne", "P. Koehn"], "venue": "In EACL", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Improving Translation Fluency with Search- Based Decoding and a Monolingual Statistical Machine Translation Model for Automatic Post-Editing", "author": ["J.-S. Chang", "S.-S. Lin"], "venue": "ROCLING", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["D. Chen", "C. Manning"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Meteor Universal: Language Speci\u0080c Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "In EACL Workshop", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "\u008ce Foundations of Cost-Sensitive Learning", "author": ["C. Elkan"], "venue": "In IJCAI", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Multilingual Image Description with Neural Sequence Models", "author": ["D. Ellio", "S. Frank", "E. Hasler"], "venue": "arXiv preprint arXiv:1510.04709", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "DeViSE: A Deep Visual-Semantic Embedding", "author": ["A. Frome", "G. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image \u008bestion Answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Arti\u0080cial Intelligence Research", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Principles of context-based machine translation evaluation", "author": ["E. Hovy", "M. King", "A. Popescu-Belis"], "venue": "Machine Translation 17,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Generating A\u0082ective Captions using Concept And Syntax Transition Networks", "author": ["T. Karayil", "P. Blandfort", "D. Borth", "A. Dengel"], "venue": "In MM", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "TACL", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Adding Chinese Captions to Images", "author": ["X. Li", "W. Lan", "J. Dong", "H. Liu"], "venue": "In ICMR", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Image Captioning with both Object and Scene Information", "author": ["X. Li", "X. Song", "L. Herranz", "Y. Zhu", "S. Jiang"], "venue": "In MM", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["C. Lin"], "venue": "In ACL workshop", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Microso\u0089 coco: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "In ICLR", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "BosonNLP: An Ensemble Approach for Word Segmentation and POS Tagging", "author": ["K. Min", "C. Ma", "T. Zhao", "H. Li"], "venue": "NLPCC", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Cross-lingual image caption generation", "author": ["T. Miyazaki", "N. Shimizu"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Self-critical Sequence Training for Image Captioning", "author": ["S. J Rennie", "E. Marcheret", "Y. Mroueh", "J. Ross", "V. Goel"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Grounded compositional semantics for \u0080nding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "\u008b. Le", "C. Manning", "A. Ng"], "venue": "TACL", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Statistical machine translation with readability constraints. In Nodalida", "author": ["S. Stymne", "J. Tiedemann", "C. Hardmeier", "J. Nivre"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Yfcc100m: \u008ce new data in multimedia research", "author": ["B. \u008comee", "D. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L. Li"], "venue": "Commun. ACM 59,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C Lawrence Zitnick", "D. Parikh"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Image captioning with deep bidirectional LSTMs", "author": ["C. Wang", "H. Yang", "C. Bartz", "C. Meinel"], "venue": "In MM", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Cost-sensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs", "author": ["G. Weiss", "K. McCarthy", "B. Zabar"], "venue": "DMIN", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Q. Wu", "C. Shen", "L. Liu", "A. Dick", "A. van den Hengel"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Show, A\u008aend and Tell: Neural Image Caption Generation with Visual A\u008aention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Boosting image captioning with a\u008aributes", "author": ["T. Yao", "Y. Pan", "Y. Li", "Z. Qiu", "T. Mei"], "venue": "arXiv preprint arXiv:1611.01646", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["J. Zhou", "Y. Cao", "X. Wang", "P. Li", "W. Xu"], "venue": "TACL", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 21, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 24, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 33, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 34, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 7, "context": "Only few studies have been conducted for image captioning in a cross-lingual se\u008aing [8, 23, 29].", "startOffset": 84, "endOffset": 95}, {"referenceID": 20, "context": "Only few studies have been conducted for image captioning in a cross-lingual se\u008aing [8, 23, 29].", "startOffset": 84, "endOffset": 95}, {"referenceID": 26, "context": "Only few studies have been conducted for image captioning in a cross-lingual se\u008aing [8, 23, 29].", "startOffset": 84, "endOffset": 95}, {"referenceID": 0, "context": "While the use of web-scale data has substantially improved machine translation quality [1, 40, 44], we observe that the \u0083uency of machine-translated Chinese sentences is o\u0089en unsatisfactory.", "startOffset": 87, "endOffset": 98}, {"referenceID": 40, "context": "While the use of web-scale data has substantially improved machine translation quality [1, 40, 44], we observe that the \u0083uency of machine-translated Chinese sentences is o\u0089en unsatisfactory.", "startOffset": 87, "endOffset": 98}, {"referenceID": 14, "context": "Fluency here means \u201cthe extent to which each sentence reads naturally\u201d [17].", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "\u008cree leading approaches have been explored [2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 13, "context": "[16] propose to exploit similarity in the visual space to transfer candidate training descriptions to a query image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": ", [11, 18, 32] similarly rank existing descriptions but in a common multimodal space for the visual and textual data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": ", [11, 18, 32] similarly rank existing descriptions but in a common multimodal space for the visual and textual data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 29, "context": ", [11, 18, 32] similarly rank existing descriptions but in a common multimodal space for the visual and textual data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": "In [13, 20, 36] , a CNN pretrained on the ImageNet classi\u0080cation task is used to encode an image, and a Recurrent Neural Network (RNN) is then used to decode the visual representation, outpu\u008aing a sequence of words as the caption.", "startOffset": 3, "endOffset": 15}, {"referenceID": 17, "context": "In [13, 20, 36] , a CNN pretrained on the ImageNet classi\u0080cation task is used to encode an image, and a Recurrent Neural Network (RNN) is then used to decode the visual representation, outpu\u008aing a sequence of words as the caption.", "startOffset": 3, "endOffset": 15}, {"referenceID": 33, "context": "In [13, 20, 36] , a CNN pretrained on the ImageNet classi\u0080cation task is used to encode an image, and a Recurrent Neural Network (RNN) is then used to decode the visual representation, outpu\u008aing a sequence of words as the caption.", "startOffset": 3, "endOffset": 15}, {"referenceID": 37, "context": "[41] introduced an a\u008aention mechanism that incorporates visual context during sentence generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "More recently, using scene information [24] and high-level concepts / a\u008aributes as visual representation [39] or as an external input for RNN [42] is shown to obtain encouraging improvements over a standard CNN-RNN image captioning model.", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "More recently, using scene information [24] and high-level concepts / a\u008aributes as visual representation [39] or as an external input for RNN [42] is shown to obtain encouraging improvements over a standard CNN-RNN image captioning model.", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "More recently, using scene information [24] and high-level concepts / a\u008aributes as visual representation [39] or as an external input for RNN [42] is shown to obtain encouraging improvements over a standard CNN-RNN image captioning model.", "startOffset": 142, "endOffset": 146}, {"referenceID": 34, "context": "[37] propose a deeper bidirectional variant of Long Short Term Memory (LSTM) to take both history and future context into account in image captioning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "A concept and syntax transition network [19] is presented to deal with large real-world captioning datasets such as YFCC100M [34].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "A concept and syntax transition network [19] is presented to deal with large real-world captioning datasets such as YFCC100M [34].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "Furthermore, in [31], reinforcement learning is also utilized to train the CNN-RNN based model directly on test metrics of the captioning task, showing significant gains in performance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "[8] address this topic as a translation problem, generating a description in the target language for a given image with a strong assumption that source-language descriptions are already provided for the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "To train a Japanese captioning model, Miyazaki and Shimizu [29] use crowd sourcing to collect Japanese descriptions of the MSCOCO training set [26].", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "To train a Japanese captioning model, Miyazaki and Shimizu [29] use crowd sourcing to collect Japanese descriptions of the MSCOCO training set [26].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "[23] have made a \u0080rst a\u008aempt in this direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "However, they use the translated text as it is, directly training a Chinese captioning model using machine-translated sentences from the Flickr8k dataset [16].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "Because public datasets for image captioning are in English [16, 26, 43] while Chinese is the most spoken language in the world, we consider English-toChinese as the cross-lingual se\u008aing.", "startOffset": 60, "endOffset": 72}, {"referenceID": 23, "context": "Because public datasets for image captioning are in English [16, 26, 43] while Chinese is the most spoken language in the world, we consider English-toChinese as the cross-lingual se\u008aing.", "startOffset": 60, "endOffset": 72}, {"referenceID": 39, "context": "Because public datasets for image captioning are in English [16, 26, 43] while Chinese is the most spoken language in the world, we consider English-toChinese as the cross-lingual se\u008aing.", "startOffset": 60, "endOffset": 72}, {"referenceID": 3, "context": "It is worth noting that we do not intend to revise {Sc } to make them more \u0083uent, as this remains an open problem in machine translation [4, 33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 30, "context": "It is worth noting that we do not intend to revise {Sc } to make them more \u0083uent, as this remains an open problem in machine translation [4, 33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 12, "context": "LSTM [15], for its capability of modeling long-term word dependency in natural language text, has been used to learn a meaningful and compact representation for a given sentence [12, 22].", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "LSTM [15], for its capability of modeling long-term word dependency in natural language text, has been used to learn a meaningful and compact representation for a given sentence [12, 22].", "startOffset": 178, "endOffset": 186}, {"referenceID": 19, "context": "LSTM [15], for its capability of modeling long-term word dependency in natural language text, has been used to learn a meaningful and compact representation for a given sentence [12, 22].", "startOffset": 178, "endOffset": 186}, {"referenceID": 25, "context": "We employ BOSON [28], a cloud based platform providing rich Chinese natural language processing service.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "(2) using stochastic gradient descent with Adam [21] on batches of size 64.", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "We employ BOSON and a Stanford parser [5] to acquire Chinese and English POS tags, respectively.", "startOffset": 38, "endOffset": 41}, {"referenceID": 33, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Following [20, 36], per iteration we apply beam search to maintain the k best candidate sentences, with a beam of size 5.", "startOffset": 10, "endOffset": 18}, {"referenceID": 33, "context": "Following [20, 36], per iteration we apply beam search to maintain the k best candidate sentences, with a beam of size 5.", "startOffset": 10, "endOffset": 18}, {"referenceID": 11, "context": "To extract image representations, we use a pre-trained ResNet152 [14] which achieved state-of-the-art results for image classi\u0080cation and detection in both ImageNet and COCO competitions.", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "A typical case is learning from a data set with highly unbalanced classes, where one might consider downsampling classes in majority, over-sampling classes in minority or re-weighting samples [7, 38].", "startOffset": 192, "endOffset": 199}, {"referenceID": 35, "context": "A typical case is learning from a data set with highly unbalanced classes, where one might consider downsampling classes in majority, over-sampling classes in minority or re-weighting samples [7, 38].", "startOffset": 192, "endOffset": 199}, {"referenceID": 6, "context": "\u008cis strategy makes full use of the translated sentences by cost-sensitive learning [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 13, "context": "To this end we sample at random 2k and 6k English sentences from Flickr8k [16] and MSCOCO [26] respectively.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "To this end we sample at random 2k and 6k English sentences from Flickr8k [16] and MSCOCO [26] respectively.", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "To the best of our knowledge, Flickr8k-cn [23] is the only public dataset suited for this purpose.", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Each test image in Flickr8k-cn is associated with \u0080ve Chinese sentences, obtained by manually translating the corresponding \u0080ve English sentences from Flickr8k [16].", "startOffset": 160, "endOffset": 164}, {"referenceID": 39, "context": "In addition to Flickr8k-cn, we construct another test set by extending Flickr30k [43] to a bilingual version.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "Besides Flickr8k-cn [23], we construct Flickr30k-cn, a bilingual version of Flickr30k [43] obtained by English-toChinese machine translation of its train / val sets and human translation of its test set.", "startOffset": 20, "endOffset": 24}, {"referenceID": 39, "context": "Besides Flickr8k-cn [23], we construct Flickr30k-cn, a bilingual version of Flickr30k [43] obtained by English-toChinese machine translation of its train / val sets and human translation of its test set.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "Flickr8k-cn [23] Flickr30k-cn (this work)", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "Similar to [23], we hire \u0080ve Chinese students who are \u0083uent in English (passing the national College English Test 6).", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "To verify the e\u0082ectiveness of our \u0083uency-guided approach, we compare with the following three alternatives: (1) \u2018Late translation\u2019 [23], which generates Chinese captions by automatically translating the output of an English captioning model.", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": "Furthermore, to understand the performance gap between the proposed approach and the method directly using manually wri\u008aen Chinese captions, we train a Chinese model using Flickr8k-cn [23], the only dataset that provides manually wri\u008aen Chinese captions for training.", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "\u008ce only exception is METEOR [6], which is inapplicable for evaluating Chinese sentences due to the lack of a structured thesaurus such as WordNet in Chinese.", "startOffset": 28, "endOffset": 31}, {"referenceID": 27, "context": "BLEU is originally designed for automatic machine translation where they compute the geometric mean of n-gram based precision for the candidate sentence with respect to the references and adds a brevity-penalty to discourage overly short sentences [30].", "startOffset": 248, "endOffset": 252}, {"referenceID": 22, "context": "ROUGE is an evaluation metric based on F-measure of longest common sub-sequence [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 32, "context": "CIDEr is a metric developed speci\u0080cally for evaluating image captioning [35].", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "Although BLEU [30] is designed to account for \u0083uency, it has been criticized in the context of machine translation for being loosely approximate human judgments [3].", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "Although BLEU [30] is designed to account for \u0083uency, it has been criticized in the context of machine translation for being loosely approximate human judgments [3].", "startOffset": 161, "endOffset": 164}], "year": 2017, "abstractText": "Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual se\u008aing. Di\u0082erent from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of \u0083uency in the translated sentences, we propose in this paper a \u0083uency-guided learning framework. \u008ce framework comprises a module to automatically estimate the \u0083uency of the sentences and another module to utilize the estimated \u0083uency scores to e\u0082ectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both \u0083uency and relevance of the generated captions in Chinese, but without using any manually wri\u008aen sentences from the target language.", "creator": "LaTeX with hyperref package"}}}