{"id": "1708.04439", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "Extractive Summarization using Deep Learning", "abstract": "This paper proposes a text summarization approach for factual reports using a deep learning model. This approach consists of three phases: feature extraction, feature enhancement, and summary generation, which work together to assimilate core information and generate a coherent, understandable summary. We are exploring various features to improve the set of sentences selected for the summary, and are using a Restricted Boltzmann Machine to enhance and abstract those features to improve resultant accuracy without losing any important information. The sentences are scored based on those enhanced features and an extractive summary is constructed. Experimentation carried out on several articles demonstrates the effectiveness of the proposed approach.", "histories": [["v1", "Tue, 15 Aug 2017 09:08:50 GMT  (158kb,D)", "http://arxiv.org/abs/1708.04439v1", "Accepted to 18th International Conference on Computational Linguistics and Intelligent Text Processing"]], "COMMENTS": "Accepted to 18th International Conference on Computational Linguistics and Intelligent Text Processing", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["sukriti verma", "vagisha nidhi"], "accepted": false, "id": "1708.04439"}, "pdf": {"name": "1708.04439.pdf", "metadata": {"source": "CRF", "title": "Extractive Summarization using Deep Learning", "authors": ["Sukriti Verma"], "emails": ["dce.sukriti@gmail.com", "vagisha.nda@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Unsupervised, Single Document, Deep Learning, Extractive"}, {"heading": "1 Introduction", "text": "A summary can be defined as a text consisting of one or more texts containing a substantial portion of the information from the original text (s), which is no more than half of the original text (s). According to [2], the text summary is the process of distilling the most important information from a source (or sources) to produce an abbreviated version for a particular user and a task (s). If this is done using a computer, we call it automatic text summary. This process can be considered a form of compression, and it necessarily suffers from loss of information, but it is essential to address the information overload due to the abundance of text material available on the Internet."}, {"heading": "2 Related Works", "text": "In fact, we are in a position to go in search of a solution that enables us, enables us to go in search of a solution."}, {"heading": "3 Proposed Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preprocessing", "text": "Pre-processing is crucial when it comes to processing text. Ambiguities can be caused by different verbs of a single word, different accepted spellings of a particular word, plural and singular terms of the same thing. In addition, words such as a, an, the, of, etc. are known as stopwords. These are certain high frequency words that do not contain any information and do not serve our purpose of summarizing in any way. At this stage, we do the following: 1. Segmentation of the document: The text is divided into paragraphs to track which paragraph each sentence belongs to and what the position of a sentence is in its respective paragraph. 2. Segmentation of the paragraph: The paragraphs are further divided into sentences. 3. Word normalization: Each sentence is divided into words and the words are standardized. Normalization involves lamatization and results in all words being tagged in a common verb form."}, {"heading": "3.2 Feature Extraction", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We've never waited so long to be able to find a solution, \"he said.\" But we've never waited so long to be able to find a solution, \"he said.\" We've never waited so long to be able to find a solution, \"he said.\" We've never waited so long to be able to find a solution, \"he said."}, {"heading": "3.3 Feature Enhancement", "text": "The Sentence Feature Matrix was generated with each set of 9 feature vector values. Afterwards, a recalculation is performed on this matrix to improve and abstract the feature vectors, so that complex features are formed from simple characteristics. This step improves the quality of the summary. For improvement and abstraction, the Sentence Feature Matrix is given as input to a Boltzmann Limited Machine (RBM), which has a hidden layer and a visible layer. For the learning process, a single hidden layer is sufficient, which is based on the size of our training data. The RBM we use has 9 perceptrons in each layer with a learning rate of 0.1. We use the method of Persistent Contrastive Divergence to try during the learning process [17]. We have given the RBM for 5 epochs with a stack size of 4 and 4 parallel Gibbs chains, which can be trained for each of the persistent characteristics to be multiplied by the CD."}, {"heading": "3.4 Summary Generation", "text": "The most relevant sentence is the first sentence in that sorted list and is selected as part of the subset of sentences that will form the summary. Then, we select the sentence with the highest Jaccard similarity to the first sentence, which is strictly selected from the top half of the sorted list. This process is repeated recursively and step by step to select more sentences until a custom sum limit is reached. Subsequently, the sentences are reordered in the order of occurrence in the original text, resulting in a coherent summary rather than a set of hay wire sentences."}, {"heading": "4 Results and Performance Evaluation", "text": "Several factual reports from different areas of health, technology, news, sports, etc. with different number of sentences were used for experiments and evaluations.The proposed algorithm was executed on each of these algorithms and system-generated summaries were compared with the summaries created by man.Feature Extraction and Enhancement is proposed as in Sections 3.2 and 3.3 for all documents.The values of the feature-vector sum and the extended feature-vector sum for each sentence of such document.The Restricted Boltzmann machine has extracted a hierarchical representation from data that originally had little discrepancy, hence detecting the latent factors.The sentences were then ranked on the basis of the final feature-vector sum and summaries are generated as proposed in Fig. 3.4.The evaluation of the system-generated summaries is based on three basic measures: Precision, Precision and Recall-measure is assumed to be successful in the number of original sentences [18]."}, {"heading": "F \u2212Measure = 2 \u2217Recall \u2217 Precision", "text": "recall + precision (8)"}, {"heading": "5 Comparative Analysis", "text": "The existing approach has been applied to the same set of articles with only one layer of RBM, rather than two as specified, and the averages of Precision, Recall and F-Measure have been plotted to draw a comparison between the existing approach and the proposed approach, while keeping the calculation quantity constant. The proposed approach has an average accuracy value of 0.7 and an average recall value of 0.63, both higher than that of the existing approach. Therefore, the proposed approach is better suited to summarising factual reports."}, {"heading": "6 Conclusion", "text": "The algorithm runs separately for each input document, rather than learning rules from a corpus, as each document is unique in itself. This is an advantage that our approach offers. We extract 9 features from the given document and improve them to evaluate each sentence. Newer approaches use 2 stacked RBMs to enhance the features. Our approach uses only one RBM and works effectively and efficiently for factual reports. This was demonstrated by manually selecting factual descriptions from multiple areas and comparing system-generated summaries with those written by humans. This approach can be further developed by adapting the extracted features to the needs of the user and further adapting the hyperparameters of the RBM to minimize processing and errors in coded values.We would like to extend our gratitude to Dr. Daya Gupta, professor at the Department of Computer Science and Technology at the University of Delhi, for providing these scientific and technological insights."}], "references": [{"title": "Automated Text Summarization", "author": ["E.H. Hovy"], "venue": "In Ruslan Mitkov (ed): The Oxford Handbook of Computational Linguistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "The TIPSTER SUMMAC Text Summarization Evaluation", "author": ["I. Mani", "D. House", "G. Klein", "L. Hirschman", "T. Firmin", "B. Sundheim"], "venue": "Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, pp. 7785. Association for Computational Linguistics Stroudsburg, PA, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Extracting Sentence Segments for Text Summarization: A Machine Learning approach", "author": ["W.T. Chuang", "J. Yang"], "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 152-159. ACM New York, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Query relevant summarization using FAQs", "author": ["A. Berger", "V. Mittal"], "venue": "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pp. 294301. Association for Computational Linguistics Stroudsburg, PA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "The Automatic Creation of Literature Abstracts", "author": ["H.P. Luhn"], "venue": "IBM Journal of Research and Development, vol. 2, issue 2, 159165", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1958}, {"title": "Machine-Made Index for Technical Literature - An Experiment", "author": ["P. Baxendale"], "venue": "IBM Journal of Research and Development, vol. 2, issue 4, 354\u2013361", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1958}, {"title": "New methods in Automatic Extracting", "author": ["H.P. Edmundson"], "venue": "Journal of the ACM, vol. 16, issue 2, 264285", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1969}, {"title": "iDVS - An Interactive Multi-Document Visual Summarization System: Machine Learning and Knowledge Discovery in Databases, LNCS, vol", "author": ["Y. Zhang", "D. Wang", "T. Li"], "venue": "6913, pp. 569-584. Springer, Heidelberg", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic Document Modeling for Syntax Removal in Text Summarization", "author": ["W.M. Darling", "F. Song"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 642-647. ACM Press, Stroudsburg, PA, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Single Document Keyphrase Extraction using Neighborhood Knowledge", "author": ["X. Wan", "J. Xiao"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Document Summarization using Conditional Random Fields", "author": ["D. Shen", "J.T. Sun", "H. Li", "Q. Yang", "Z. Chen"], "venue": "Proceedings of the 20th international joint conference on Artifical Intelligence, pp. 2862-2867. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Extractive Summarization Using Supervised and Semi-supervised Learning", "author": ["K.F. Wong", "M.J. Wu", "W.J. Li"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics Volume 1, pp. 985-992. Association for Computational Linguistics Stroudsburg, PA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning Object Classes from Image Thumbnails through Deep Neural Networks", "author": ["E.K. Chen", "X.K. Yang", "H.Y. Zha", "R. Zhang", "W.J. Zhang"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "A Comparative Study on Ranking and Selection Strategies for Multi Document Summarization", "author": ["F. Jin", "M.L. Huang", "X.Y. Zhu"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pp. 525-533. Association for Computational Linguistics Stroudsburg, PA, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Bilingual Automatic Text Summarization Using Unsupervised Deep Learning", "author": ["S.P. Singh", "A. Kumar", "A. Mangal", "S. Singhal"], "venue": "2016 International Conference on Electrical, Electronics, and Optimization Techniques. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "An Approach for Text Summarization using Deep Learning Algorithm", "author": ["G. PadmaPriya", "K. Duraiswamy"], "venue": "Journal of Computer Science, vol. 10, issue 1, 19", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "A summary can be defined as a text produced from one or more texts, containing a significant portion of the information from the original text(s), and that is no longer than half of the original text(s) [1].", "startOffset": 203, "endOffset": 206}, {"referenceID": 1, "context": "According to [2], text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version for a particular user and task(s).", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "It is broken down into three phases: feature extraction [3], feature enhancement, and summary generation based on values of those features.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "Most early work on text summarization was focused on technical documents and early studies on summarization aimed at summarizing from pre-given documents without any other requirements, which is usually known as generic summarization [4].", "startOffset": 234, "endOffset": 237}, {"referenceID": 4, "context": "Luhn [5] proposed that the frequency of a particular word in an article provides a useful measure of its significance.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Baxendale [6] examined 200 paragraphs and found that in 85% of the paragraphs, the topic sentence came as the first one and in 7% of the time, it was the last sentence.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Edmundson [7] focused his work around the importance of word frequency and positional importance as features.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "Upcoming researchers in text summarization have approached it problem from many aspects such as natural language processing [8], statistical modelling [9] and machine learning.", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "Upcoming researchers in text summarization have approached it problem from many aspects such as natural language processing [8], statistical modelling [9] and machine learning.", "startOffset": 151, "endOffset": 154}, {"referenceID": 9, "context": "Text Summarization can be done for one document, known as single-document summarization [10], or for multiple documents, known as multi-document summarization [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "Text Summarization can be done for one document, known as single-document summarization [10], or for multiple documents, known as multi-document summarization [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 11, "context": "On basis of the writing style of the final summary generated, text summarization techniques can be divided into extractive methodology and abstractive methodology [12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "mar, extractive approaches, which select a subset of sentences from the input documents to form a summary instead of paraphrasing like a human [13], are the mainstream in the area.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "how to select a subset of those ranked units [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "On the basis of these features, a score is calculated for each sentence and sentences are arranged in decreasing order of their scores [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "F-Measure is defined as follows [19]:", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "This paper proposes a text summarization approach for factual reports using a deep learning model. This approach consists of three phases: feature extraction, feature enhancement, and summary generation, which work together to assimilate core information and generate a coherent, understandable summary. We are exploring various features to improve the set of sentences selected for the summary, and are using a Restricted Boltzmann Machine to enhance and abstract those features to improve resultant accuracy without losing any important information. The sentences are scored based on those enhanced features and an extractive summary is constructed. Experimentation carried out on several articles demonstrates the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}