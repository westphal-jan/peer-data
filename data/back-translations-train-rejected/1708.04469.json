{"id": "1708.04469", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "Comparison of Decoding Strategies for CTC Acoustic Models", "abstract": "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR. For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string. We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area.", "histories": [["v1", "Tue, 15 Aug 2017 12:05:02 GMT  (509kb)", "http://arxiv.org/abs/1708.04469v1", "5 pages. To appear in Interspeech 2017"]], "COMMENTS": "5 pages. To appear in Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thomas zenkel", "ramon sanabria", "florian metze", "jan niehues", "matthias sperber", "sebastian st\\\"uker", "alex waibel"], "accepted": false, "id": "1708.04469"}, "pdf": {"name": "1708.04469.pdf", "metadata": {"source": "CRF", "title": "Comparison of Decoding Strategies for CTC Acoustic Models", "authors": ["Thomas Zenkel", "Ramon Sanabria", "Florian Metze", "Jan Niehues", "Matthias Sperber", "Sebastian St\u00fcker", "Alex Waibel"], "emails": ["sebastian.stueker}@kit.edu", "ahw}@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.04 469v 1 [cs.C L] 15 Aug 201 7lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent under CTC loss, so that a speech model (LM) can be conveniently integrated during decoding, using the traditional separation of acoustic and linguistic components in ASR. For fixed vocabularies, Weighted Finite State Transducers provide a strong foundation for the efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a simple solution for open vocabulary recognition and purely neural models and can be decoded using beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a specific word."}, {"heading": "1. Introduction", "text": "Traditionally, the acoustic models (AMs) of an automatic speech recognition system are based on HMMs [1], in which the emission probabilities of each state were modeled using a Gaussian mixing model. \u2022 Since AM works with phonemes as its target, it had to be combined with an pronunciation lexicon that maps the sequences of phonemes to words, and a word-based LM [2] when decoding AM information. Recent work has focused on solutions that come close to end-to-end systems. Connectionist Temporal Classification (CTC) can directly model acoustic models [3] without relying on alignment between the audio sequence and the symbol sequence."}, {"heading": "2. Related work", "text": "The simplest decoding algorithm is to select the most likely character in each frame. This is usually used to provide character error rates (CERs) during the formation of the acoustic model, and can also be used to calculate Word Error Rates (WHO), since the acoustic model has an idea of word boundaries. Word boundaries can be modeled using a spatial symbol or by capitalizing the first letter of each word [11]. While the decoding of CTC acoustic models works well without adding external linguistic information, an enormous amount of training data should be used to achieve competitive results. A traditional approach to performing decoding via CTC is to add linguistic information at the word level. Early work has done this with an ordinary beam search, that is to perform a broad initial search in the time dimension and a fixed number of partial transcriptions at each step."}, {"heading": "3. Acoustic Model", "text": "The AM of our system is composed of several RNN layers, followed by a Soft-Max layer. RNN layers, which are composed by bidirectional LSTM units [19], provide the ability to learn complex, long-term dependencies. A sequence of multiple linguistic features forms the input of our model. For each input, the AM gives a probability distribution over its target alphabet. The entire model is jointly trained under the CTC loss function [3]. Formally, let us define a sequence of n-dimensional acoustic characteristics X = (x1,., xT) of the length distribution T as input of our model and L as a sequence of labels of our alphabet. These labels can be either signs or phonemes. We supplement L with a special blank symbol that we call the cross symbol L-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross-cross"}, {"heading": "4. Decoding Strategies", "text": "In this section, we describe different approaches to creating a transcription given the static sequence of probabilities generated by the acoustic model."}, {"heading": "4.1. Greedy Search", "text": "To create a transcription without adding any linguistic information, we use the decoding method of [3] and greedily search for the best way to get a transcription. The mapping of the path to a transcription is straightforward and works with the squash function: z = B (p). For character-based CTC acoustic models, this method can already provide useful transcriptions."}, {"heading": "4.2. Weighted Finite State Transducer", "text": "To improve the simple greedy search, the Weighted Finite State Transducer (WFST) approach adds linguistic information at the word level. First, we process the probability sequence with the previous probability of each unit of the extended label set L \u2032. p (X | k) VP (k | X) / P (k) (4) This has no real theoretical motivation, since the result is only proportional to a probability distribution. However, by dividing the previous probability units, which appear more likely at a particular position than their average, by a single unit in L, we obtain a high value. The WFST search diagram consists of three individual components: \u2022 A token WFST maps a sequence of units in L \u00b2 to a single unit in L, by using the squash function B \u2022 A Lexicon WFST maps sequences from units in L to words \u2022 A grammar WFST encodes the allowable word sequences in L \u00b2 and a word in L can be generated by using the lexical model most based on the WST, which is based on the WST."}, {"heading": "4.3. Beam Search with Char RNN", "text": "Unlike the WFST-based approach, we can apply the probabilities directly at the character level using this method. At first, we assume that the alphabet of the character based on LM is equal to L. We want to find a transcription that has a high probability on both the acoustic and the speech model. Since we need to sum up z across all possible paths for a transcription and want to add the LM information as early as possible, our goal is to solve the following equation: argmax z-1 (z) = pT-t = 1ytpt \u00b7 P-LM (pt | B (p1: t \u2212 1)) (5) Note that we cannot estimate a usable probability for the blank label limit with the language model, so we can set P-LM symbols."}, {"heading": "4.4. Attention Based Sequence to Sequence Model", "text": "The attention-based approach is an example of a sequence to the sequence model. As in common neural machine translation models, input is transformed into a sequence of words. Therefore, the system first encodes the string with an RNN-based encoder and generates a sequence of hidden representations h = (h1,...) of length T. During decoding, we calculate an attention vector a = (a1,..., aT) with p = 1 for each output word based on the current hidden state of the decoder. With the hidden representation and attention vector, we can now calculate the context vector c: c = T \u0445 t = 1at \u00b7 ht (7) The decoder uses the context vector to generate a probability distribution over the word."}, {"heading": "5. Training", "text": "This section describes the training process of the acoustic model and the linguistically motivated models used in the various decoding approaches."}, {"heading": "5.1. Acoustic Model", "text": "This dataset consists of 2,400 two-sided phone calls lasting approximately 300 hours. It consists of over 500 speakers with different US accents speaking over 501 code and included within EESEN: https: / / github.com / srvk / eesenrandomly picked topics. We select 4,000 utterances as the validation set for hyperparameter tuning. Our target markers are either phonemes or characters. We also expand the training set to obtain a more general model with two techniques. First, by reducing the frame rate, applying a sub-sampling and finally adding an offset, we increase the number of training samples. Second, by adding a factor of 10 to our training set, making minor changes to the speed, pitch and tempo of the audio files. The model consists of five bi-directional LSTM levels with 320 units in each direction. It is trained using EEN [20]."}, {"heading": "5.2. Weighted Finite State Transducer", "text": "As explained in Section 4.2, our WFST implementation consists of three individual components, which are implemented using Kaldis [21] FST tools. We determine the weights of the WFST lexicon using a lexicon that assigns each word to a sequence of CTC names. WFST grammar is modelled using the probabilities of a trigram and 4 gram language model, which is smoothed out with Kneser-Ney smears [22]. We create the language model using Fisher transcripts and the transcripts of the acoustic training data using SRILM [23]."}, {"heading": "5.3. Character Language Model", "text": "We train the character LM with Fisher transcripts (LDC2004T19, LDC2005T19) and the transcripts of the acoustic training data (LDC97S62). Validation is done on the basis of the transcription of the acoustic validation data. These transcriptions are cleaned up by removing punctuation marks and duplicate expressions. The result is a training text of about 23 million words and 112 million characters. The alphabet of the character LM consists of 28 characters, a start and sentence symbol, a space character and a symbol representing unknown characters. We cut all sentences to a maximum length of 128 characters. We use an embedding size of 64 characters, a single-layer LSTM with 2048 units and a Softmax layer implemented with DyNet [24] as a neural model. The training is performed with all the data by randomly selecting a batch until the validation data is achieved."}, {"heading": "5.4. Attention Based Sequence to Sequence Model", "text": "We decipher the acoustic model without any linguistic information using the greedy method of Section 4.1. The sequence of the generated characters is used as input into our model, and we use the word sequence in the reference transcription as our desired output during the training. To implement the attention-based encoder decoder, we use the Nematus toolkit [27]. In our experiments, we use GRU units in the encoder and the target sequence is generated with conditional GRU units [27]. We use the standard network architecture with an embedding size of 500 and a hidden layer size of 1024, and our output vocabulary consists of almost 30,000 words. For regularization, we use dropouts [28]. Due to time constraints, we only use segments with a maximum length of 100 characters."}, {"heading": "6. Experiments", "text": "We use the 2000 HUB5 \"Eval2000\" (LDC2002S09) data sets for evaluation. They consist of a \"switchboard\" subset, which is similar to training data, and a \"callhome\" subset. This subset allows us to analyze the robustness of the individual approaches. \"Greedy Search\" uses an alphabet consisting of uppercase and lowercase letters."}, {"heading": "7. Conclusions", "text": "In this paper, we compare different decoding approaches for CTC acoustic models trained on the same open source platform. A \"traditional\" context-independent WFST approach performs best, but the open vocabulary character RNN approach performs relatively poorly only by about 10% and produces a surprisingly low number of \"OOV\" errors. We believe that the Seq2Seq approach also performs reasonably and is very easy to learn, since the CTC model already generates symbolic tokenization of input, and this trick allows us to outperform real end-to-end approaches such as [30]. We believe that these results show that there are currently a variety of different algorithms that can be used for speech recognition in a neural environment, and that there may not be a \"one size fits all\" approach in the foreseeable future. While WFST can be well understood and executed quickly, the Seq2Seq approach could evolve well into the machine-based NN during translation, which may be better suited to integrate into its English character."}, {"heading": "8. Acknowledgments", "text": "This work was supported by the Carl Zeiss Foundation and the CLICS exchange program, using the Extreme Science and Engineering Discovery Environment (XSEDE), which is funded by the National Science Foundation with ACI-1548562."}, {"heading": "9. References", "text": "[1] L. Rabiner and B. Juang, \"An introduction to hidden markov models,\" ieee assp networks, vol. 3, no. 1, pp. 4-16, 1986. [2] H. Soltau, F. Metze, C. Fugen, and A. Waibel, \"A one-pass decoder based on polymorphic linguistic context assignment,\" in Automatic Speech Recognition and Understanding, 2001. [3] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, \"Connectionist temporal classification.\" Xiuk, \"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks\" in Proceedings of the 23rd International Conference on Machine learning. ACM, 2006, pp. 369-376. [4 D. Bahdanau, K. Cho, and Y."}], "references": [{"title": "An introduction to hidden markov models", "author": ["L. Rabiner", "B. Juang"], "venue": "ieee assp magazine, vol. 3, no. 1, pp. 4\u201316, 1986.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "A one-pass decoder based on polymorphic linguistic context assignment", "author": ["H. Soltau", "F. Metze", "C. Fugen", "A. Waibel"], "venue": "Automatic Speech Recognition and Understanding, 2001. ASRU\u201901. IEEE Workshop on. IEEE, 2001, pp. 214\u2013217.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4945\u2013 4949.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Listen, attend and spell", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "arXiv preprint arXiv:1508.01211, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Lexicon-free conversational speech recognition with networks.", "author": ["A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng"], "venue": "in HLT-NAACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Character-level incremental speech recognition with recurrent neural networks", "author": ["K. Hwang", "W. Sung"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5335\u20135339.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Advances in all-neural speech recognition", "author": ["G. Zweig", "C. Yu", "J. Droppo", "A. Stolcke"], "venue": "arXiv preprint arXiv:1609.05935, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition", "author": ["H. Soltau", "H. Liao", "H. Sak"], "venue": "arXiv preprint arXiv:1610.09975, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["A. Graves", "N. Jaitly"], "venue": "in ICML, vol", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Firstpass large vocabulary continuous speech recognition using bidirectional recurrent dnns", "author": ["A.Y. Hannun", "A.L. Maas", "D. Jurafsky", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1408.2873, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallelizing wfst speech decoders", "author": ["C. Mendis", "J. Droppo", "S. Maleki", "M. Musuvathi", "T. Mytkowicz", "G. Zweig"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5325\u20135329.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Intelligible language modeling with input switched affine networks", "author": ["J.N. Foerster", "J. Gilmer", "J. Chorowski", "J. Sohl-Dickstein", "D. Sussillo"], "venue": "arXiv preprint arXiv:1611.09434, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiplicative lstm for sequence modelling", "author": ["B. Krause", "L. Lu", "I. Murray", "S. Renals"], "venue": "arXiv preprint arXiv:1609.07959, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint ctc-attention based end-to-end speech recognition using multi-task learning", "author": ["S. Kim", "T. Hori", "S. Watanabe"], "venue": "arXiv preprint arXiv:1609.06773, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "An empirical exploration of ctc acoustic models", "author": ["Y. Miao", "M. Gowayyed", "X. Na", "T. Ko", "F. Metze", "A. Waibel"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 2623\u20132627.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310\u2013318.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Srilm-an extensible language modeling toolkit.", "author": ["A. Stolcke"], "venue": "in Interspeech,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["G. Neubig", "C. Dyer", "Y. Goldberg", "A. Matthews", "W. Ammar", "A. Anastasopoulos", "M. Ballesteros", "D. Chiang", "D. Clothiaux", "T. Cohn", "K. Duh", "M. Faruqui", "C. Gan", "D. Garrette", "Y. Ji", "L. Kong", "A. Kuncoro", "G. Kumar", "C. Malaviya", "P. Michel", "Y. Oda", "M. Richardson", "N. Saphra", "S. Swayamdipta", "P. Yin"], "venue": "arXiv preprint arXiv:1701.03980, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "arXiv preprint arXiv:1609.08144, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Nematus: a Toolkit for Neural Machine Translation", "author": ["R. Sennrich", "O. Firat", "K. Cho", "A. Birch", "B. Haddow", "J. Hitschler", "M. Junczys-Dowmunt", "S. L\u201daubli", "A.V. Miceli Barone", "J. Mokry", "M. Nadejde"], "venue": "Proceedings of the Demonstrations at the 15th Conference of the European Chapter of the Association for Computational Linguistics, Valencia, Spain, 2017.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["L. Lu", "X. Zhang", "S. Renais"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2016, pp. 5060\u20135064.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Traditionally, Acoustic Models (AMs) of an Automatic Speech Recognition system followed a generative approach based on HMMs [1] where the emission probabilities of each state were modeled with a Gaussian Mixture Model.", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Since the AM works with phonemes as a target, during decoding the information of the AM had to be combined with a pronunciation lexicon, which maps sequences of phonemes to words, and a word based LM [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "Connectionist Temporal Classification (CTC) acoustic models [3] can directly model the mapping between speech features and symbols without having to rely on an alignment between the audio sequence and the symbol sequence.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Other end-to-end approaches that are inspired by recent developments in machine learning system such as [4] are [5, 6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Other end-to-end approaches that are inspired by recent developments in machine learning system such as [4] are [5, 6].", "startOffset": 112, "endOffset": 118}, {"referenceID": 5, "context": "Other end-to-end approaches that are inspired by recent developments in machine learning system such as [4] are [5, 6].", "startOffset": 112, "endOffset": 118}, {"referenceID": 6, "context": "\u2022 WFST search with word language model [7, 8]", "startOffset": 39, "endOffset": 45}, {"referenceID": 7, "context": "\u2022 WFST search with word language model [7, 8]", "startOffset": 39, "endOffset": 45}, {"referenceID": 8, "context": "\u2022 Beam search using character RNN language model [9, 10]", "startOffset": 49, "endOffset": 56}, {"referenceID": 9, "context": "\u2022 Beam search using character RNN language model [9, 10]", "startOffset": 49, "endOffset": 56}, {"referenceID": 10, "context": "Word boundaries can be modeled with a space symbol or by capitalizing the first letter of each word [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "While decoding CTC acoustic models without adding external linguistic information works well, a vast amount of training data should be used to get competitive results [12].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "For including linguistic information when adding a new character to a transcription, word based LMs were preprocessed [13, 14].", "startOffset": 118, "endOffset": 126}, {"referenceID": 13, "context": "For including linguistic information when adding a new character to a transcription, word based LMs were preprocessed [13, 14].", "startOffset": 118, "endOffset": 126}, {"referenceID": 6, "context": "Weighted Finite State Transducers (WFST) present a generalized word based approach [7, 15].", "startOffset": 83, "endOffset": 90}, {"referenceID": 14, "context": "Weighted Finite State Transducers (WFST) present a generalized word based approach [7, 15].", "startOffset": 83, "endOffset": 90}, {"referenceID": 15, "context": "Due to recent developments of character based LMs [16, 17], it is also a competitive option to directly add character level linguistic information during the beam search.", "startOffset": 50, "endOffset": 58}, {"referenceID": 16, "context": "Due to recent developments of character based LMs [16, 17], it is also a competitive option to directly add character level linguistic information during the beam search.", "startOffset": 50, "endOffset": 58}, {"referenceID": 8, "context": "Currently, one of the most promising approaches is to use a character based RNN and query it when a new character is added to the transcription [9, 10].", "startOffset": 144, "endOffset": 151}, {"referenceID": 9, "context": "Currently, one of the most promising approaches is to use a character based RNN and query it when a new character is added to the transcription [9, 10].", "startOffset": 144, "endOffset": 151}, {"referenceID": 10, "context": "This information can be processed by another CTC model [11] or by an attention based system [4] to produce a more linguistically reasonable transcription.", "startOffset": 55, "endOffset": 59}, {"referenceID": 3, "context": "This information can be processed by another CTC model [11] or by an attention based system [4] to produce a more linguistically reasonable transcription.", "startOffset": 92, "endOffset": 95}, {"referenceID": 17, "context": "Recent approaches combine a CTC model with an attention based mechanism and are able to train this model jointly [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "RNN layers, which are composed by bidirectional LSTM units [19], provide the ability to learn complex, long term dependencies.", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "The whole model is jointly trained under the CTC loss function [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "To perform the sum over all path we will use a technique inspired by the traditional dynamic programming method used in HMMs, the forward-backward algorithm [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "To create a transcription without adding any linguistic information we use the decoding procedure of [3] and greedily search the best path p \u2208 L :", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "As it is infeasible to calculate an exact solution to equation 6, we apply a beam search similar to [10].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "It is trained using EESEN [20].", "startOffset": 26, "endOffset": 30}, {"referenceID": 20, "context": "These components are implemented using Kaldi\u2019s [21] FST tools.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "The grammar WFST is modeled by using the probabilities of a trigram and 4-gram language model smoothed with Kneser-Ney [22] discounting.", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "We create the language model based on Fisher transcripts and the transcripts of the acoustic training data using SRILM [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "We use a embedding size of 64 for the characters, a single layer LSTM with 2048 Units and a softmax layer implemented with DyNet [24] as our neural model.", "startOffset": 129, "endOffset": 133}, {"referenceID": 24, "context": "Training is performed with the whole data using Adam [25] by randomly picking a batch until convergence on the validation data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "01, which is inspired by [26].", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "For implementing the attention based encoder decoder, we use the Nematus toolkit [27].", "startOffset": 81, "endOffset": 85}, {"referenceID": 26, "context": "In our experiments we use GRU units in the encoder and the target sequence is generated using conditional GRU units [27].", "startOffset": 116, "endOffset": 120}, {"referenceID": 27, "context": "For regularization, we use dropout [28].", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "trained using Adadelta [29] and a mini-batch size of 80.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "Char Beam [9] Character 30.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "4% Char Beam [11] Character 32.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "8% WFST [11] Character 26.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "1% Seq2Seq [11] Character 37.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "As in [11], an upper case character denotes the start of a new word.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "Our character based system is competitive to the recently published results in [11], which represent state of the art results.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "For open vocabulary, character based speech recognition we report an improvement of over 1% WER compared to previous results [11, 9].", "startOffset": 125, "endOffset": 132}, {"referenceID": 8, "context": "For open vocabulary, character based speech recognition we report an improvement of over 1% WER compared to previous results [11, 9].", "startOffset": 125, "endOffset": 132}, {"referenceID": 29, "context": "This trick allows us to outperform true end-to-end approaches such as [30].", "startOffset": 70, "endOffset": 74}], "year": 2017, "abstractText": "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR. For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string. We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area.", "creator": "LaTeX with hyperref package"}}}