{"id": "1708.04498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "Self-adaptive node-based PCA encodings", "abstract": "In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it is able to calculate the principal component analysis (PCA) in a distributed fashion across nodes. It simplifies existing network structures by removing intralayer weights, essentially cutting the number of weights that need to be trained in half.", "histories": [["v1", "Fri, 16 Jun 2017 12:29:41 GMT  (19kb)", "http://arxiv.org/abs/1708.04498v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.SE", "authors": ["leonard johard", "victor rivera", "manuel mazzara", "jooyoung lee"], "accepted": false, "id": "1708.04498"}, "pdf": {"name": "1708.04498.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Leonard Johard", "Victor Rivera", "Manuel Mazzara"], "emails": ["l.johard@innopolis.ru", "v.rivera@innopolis.ru", "m.mazzara@innopolis.ru", "j.lee@innopolis.ru"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.04 498v 1 [cs.N E] 16 Jun 2017"}, {"heading": "1 Introduction", "text": "Innovative engineering is always looking for intelligent solutions that can be used on the territory for both civil and military applications, while at the same time aiming to create adequate tools to support developers throughout the development process so that correct software can be used. Modern technological solutions involve the enormous use of sensors to monitor an equipped area and collect data, which are then mined and analyzed for specific purposes. Classics are smart buildings and smart cities [1,2]. Sensor integration across multiple platforms can generate huge amounts of data that need to be analyzed in real time by both algorithmic means and human operators. The nature of this information is a priori unpredictable, as sensors are likely to encounter both naturally variable conditions in the field and disinformation attempts aimed at network protocols. This information must be transmitted through a distributed battle cloud with variable but limited bandwidth, which must be resistant to multiple protocols in each node."}, {"heading": "2 Linear sensor encodings", "text": "Linear encoding of sensor information has the disadvantage of not being able to perform certain optimizations, such as high-efficiency bit-level Hoffman-like encoding. On the other hand, it is very robust if it encodes continuous data because it is isometric, which means that we will not see much disruption in sample removal and make linear encoding highly suitable for later machine learning analysis and human observation. This isometry also makes the encoding resistant to noise data transmission, which is essential for efficient scaling of real-time data on the network.The benefit of possible non-linear encoding is further reduced if we take into account the uncertainty in our data distribution estimation.A small error in our knowledge can cause major inefficiency in coding and large losses in lossy compression. In linear encoding, all these aspects are limited, especially when considering the simple use of regulation methods."}, {"heading": "3 PCA in networks", "text": "The problem of encoding in nodal networks is usually viewed from the perspective of neural networks. We will maintain this terminology to preserve the vocabulary that is prevalent in the literature. (D) We will continue here with a new, simple notation that we will later use to illustrate the new algorithms. (D) In this case, each output can be described as yi = XTw, where x is the input vector and w is the weights of the neurons and i is the index of the input data. (D) The outputs form the basis of the input space and when we evaluate the input data. (D)"}, {"heading": "3.1 Generalized Hebbian Algorithm", "text": "The idea behind the generalized Hebbic Algorithm (GHA) [8] is as follows: 1. Use Oja's rule to get wi 2. Use deflation to eliminate the variance along ewi 3. i: = i + 1. Go to Step 1Subtraction of the w dimension projects space into the subspace spanned by the remaining major components. The target function y (vi) 22 for all eigenvectors vi that are not eliminated by this projection. Repeating the algorithm after this step guarantees that we get the largest remaining component in each step. GHA requires several steps to calculate the smaller components and uses a specialized architecture.The signal must pass through 2 (n \u2212 1) neurons to calculate the n-th major component and uses two different types of neurons to achieve this. We define the information as the quadratic variance of the transmitted signal as a unit and seek encodings to maximize the space."}, {"heading": "3.2 Distributed PCA", "text": "Principal component analysis is optimal linear encoding, which minimizes the reconstruction error, but still leaves room for improvement. (Can you do better?) In PCA, as much information as possible is put into each successive component, leaving the encoding vulnerable to the loss of a node or neuron, potentially losing much of the information. (Theorem 1) In this sense, the PCA subspace remains the optimal subspace, regardless of the vectors chosen to encircle it. This encoding minimizes the maximum possible error of any combination n \u2212 1. Theorem 1. There is a coding of the PCA space, so that the information along each component is equivalent. (In = Im, n, m) This encoding minimizes the maximum possible error of any combination n \u2212 1 component. Proof. Based on eigenvectors vi, we can rotate any pair of vectors in the vectors spanned by these vectors."}, {"heading": "3.3 Simple Hebbian PCA", "text": "We propose a new method for calculating the PCA encoding: X \u2192 Y in a single time step and using a single weight matrix W = 29. For use in distributed transmission systems, an ideal algorithm should only process local and explicitly transmitted information relating to X and Y from its neighbors. In other words, each node has its own knowledge of its neighbors, but not its own weight or other information. The simple Hebbian PCA is described when the pseudo code in algorithm 1.Algorithm 1 ASHPRequire: Initializes eigenweight Vector wi Require: Input Matrix X Require: Number of iterations T Require: Number of nodes N Require: Step size 1 to T do 1 to i-N doyi."}, {"heading": "4 Conclusions", "text": "We have proposed an algorithm, Simple Hebbian PCA, and demonstrated that it is able to calculate the PCA in a distributed way across all nodes. It simplifies existing network structures by removing the intralayer weights, essentially halving the number of weights to be trained. This means that the proposed algorithm has an architecture that can be used to organize the flow of information across distributed networks with a minimum of communication effort. It automatically adapts in real time so that the transferred data covers the optimal subspace for reconstructing the original sensory data and is relatively resistant to data corruption. In future work, we will provide empirical results of convergence properties. We will also seek to derive symmetrical versions of our algorithm, which uses the same learning algorithm for each node, or an alternative formulation that uses symmetrical intralayer connections."}], "references": [{"title": "Microservice-based iot for smart buildings,", "author": ["K. Khanda", "D. Salikhov", "K. Gusmanov", "M. Mazzara", "N. Mavridis"], "venue": "in 31st International Conference on Advanced Information Networking and Applications Workshops,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Jolie good buildings: Internet of things for smart building infrastructure supporting concurrent apps utilizing distributed microservices,", "author": ["D. Salikhov", "K. Khanda", "K. Gusmanov", "M. Mazzara", "N. Mavridis"], "venue": "Selected Papers of the First International Scientific Conference Convergent Cognitive Information Technologies (Convergent 2016),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Implementing the internet of things vision in industrial wireless sensor networks,", "author": ["C. Kruger", "G.P. Hancke"], "venue": "Industrial Informatics (INDIN),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A connectionist actor-critic algorithm for faster learning and biological plausibility,", "author": ["L. Johard", "E. Ruffaldi"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neural network implementations for pca and its extensions,", "author": ["J. Qiu", "H. Wang", "J. Lu", "B. Zhang", "K.-L. Du"], "venue": "ISRN Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Simplified neuron model as a principal component analyzer,", "author": ["E. Oja"], "venue": "Journal of mathematical biology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1982}, {"title": "Optimal unsupervised learning in a single-layer linear feedforward neural network,", "author": ["T.D. Sanger"], "venue": "Neural networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Representation learning: A review and new perspectives,", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A self-organizing network for principalcomponent analysis,", "author": ["J. Rubner", "P. Tavan"], "venue": "EPL (Europhysics Letters),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "A neural network learning algorithm for adaptive principal component extraction (apex),", "author": ["S. Kung", "K. Diamantaras"], "venue": "in International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "A hebbian/anti-hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data,", "author": ["C. Pehlevan", "T. Hu", "D.B. Chklovskii"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Classic examples are smart buildings and smart cities [1,2].", "startOffset": 54, "endOffset": 59}, {"referenceID": 1, "context": "Classic examples are smart buildings and smart cities [1,2].", "startOffset": 54, "endOffset": 59}, {"referenceID": 2, "context": "The scaling of the information distribution also benefits from a pure feedforward nature, since the need for bidirectional communication scales poorly with the likely network latency and information loss, both of which are considerable in practical scenarios [3,4].", "startOffset": 259, "endOffset": 264}, {"referenceID": 3, "context": "This requirement puts our desired adaptive system into the wider framework of recent highly scalable feedforward algorithms that have been inspired by biology [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 4, "context": "A recent review of current algorithms for performing principal component analysis (PCA) in a node network or neural network is [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "Equation 16 gives that ww = 1 This is learning algorithm is equivalent to Oja\u2019s rule [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "1 Generalized Hebbian Algorithm The idea behind the generalized Hebbian algorithm (GHA) [8] is as follows:", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "This is complementary and not antagonistic to the concept of sparse encodings disentangling the factors of variation [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 8, "context": "This algorithm has some degree of similarity to several existing algorithms, namely the Rubner-Tavan PCA algorithm [10], the APEX-algorithm [11] and their symmetric relatives [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "This algorithm has some degree of similarity to several existing algorithms, namely the Rubner-Tavan PCA algorithm [10], the APEX-algorithm [11] and their symmetric relatives [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "This algorithm has some degree of similarity to several existing algorithms, namely the Rubner-Tavan PCA algorithm [10], the APEX-algorithm [11] and their symmetric relatives [12].", "startOffset": 175, "endOffset": 179}], "year": 2017, "abstractText": "In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it is able to calculate the principal component analysis (PCA) in a distributed fashion across nodes. It simplifies existing network structures by removing intralayer weights, essentially cutting the number of weights that need to be trained in half.", "creator": "LaTeX with hyperref package"}}}