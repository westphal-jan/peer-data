{"id": "1708.04681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "Identifying Harm Events in Clinical Care through Medical Narratives", "abstract": "Preventable medical errors are estimated to be among the leading causes of injury and death in the United States. To prevent such errors, healthcare systems have implemented patient safety and incident reporting systems. These systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care. These reports are narratives in natural language and while they provide detailed information about the situation, it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients. In this work, we present a method based on attentive convolutional and recurrent networks for identifying harm events in patient care and categorize the harm based on its severity level. We demonstrate that our methods can significantly improve the performance over existing methods in identifying harm in clinical care.", "histories": [["v1", "Tue, 15 Aug 2017 20:38:37 GMT  (1018kb,D)", "http://arxiv.org/abs/1708.04681v1", "ACM-BCB 2017"]], "COMMENTS": "ACM-BCB 2017", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["arman cohan", "allan fong", "raj ratwani", "nazli goharian"], "accepted": false, "id": "1708.04681"}, "pdf": {"name": "1708.04681.pdf", "metadata": {"source": "META", "title": "Identifying Harm Events in Clinical Care  through Medical Narratives", "authors": ["Arman Cohan", "Allan Fong", "Raj Ratwani", "Nazli Goharian"], "emails": ["arman@ir.cs.georgetown.edu", "allan.fong@medstar.net", "raj.m.ratwani@medstar.net", "nazli@ir.cs.georgetown.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "Key concepts Natural language processing; Medical text; Deep learning"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to"}, {"heading": "2 RELATEDWORK", "text": "There is an increasing number of papers dealing with the categorization of patients and safety narratives in clinical care. Fong et al. [12] We have examined both the unstructured free texts and the structured data elements in safety reports to identify and evaluate similar events, evaluating the search methods that relate to the use of words, as well as the modeling of similar events. In another paper, Ong et al. [26], we have investigated the similar problem of identifying extreme events in patient safety and occurrences using Naive Bayes and SVM classes. In contrast, we focus on the problem of identifying damage and categorizing damage in medical narratives. We present neural networking methods that are able to capture information about safety events without using external features. We compare our results with features and show that our proposed methods are significant."}, {"heading": "3 METHODS", "text": "As explained in \u00a7 1, patient safety narratives can be complex, and identifying damage to the patient is a challenge in these reports. To effectively accomplish this task, we need a model that can capture both local characteristics and linguistic usage throughout the report. To achieve this goal, we propose a neural network consisting of multiple layers in which each layer is designed to address the above challenges. Our approach does not require feature engineering, and it automatically learns to identify important features from the raw text. We first describe the general outline of our model, and then describe each component in more detail. The proposed architecture is shown in Figure 1. Initially, the input report is pre-processed and presented as a matrix that matches the word embeddings. However, word deductions or distributed representations of words aim to associate words with densities that require general attention that this layer is associated with similar editing layers."}, {"heading": "3.1 Embedding layer", "text": "This layer processes the raw text corresponding to the medical report and presents it as a matrix of real numbers. This matrix consists of embedding the words in the report. We symbolize the text using a simple white space tokenizer and reduce all words. We then transform the input sequence of the tokens into a sequence of dense distribution vectors. Specifically, the embedding layer of each token wi represents a d-dimensional vector xi, and the sequenceW is represented as a matrix of real numbers X with dimensions X and R, where nmax is the maximum sequence length. Text input with a length greater than nmax is truncated and text input shorter than nmax is supplemented with zeros."}, {"heading": "3.2 Convolutional layer", "text": "A CNN is a neural network consisting of two main operations: folding and pooling. A folding is an operation between two functions where f is the primary vector and f is the primary vector. The folding operation between f and \u0430 is the length of the lter. Here, f is the input to the folding (word vectors extracted from the embedding layer) [n] [p] = Ki = \u2212 K f [n \u2212 i] x [i] [i], which denotes the folding operation, and L = 2K \u2212 1 is the length of the lter. Here, f is the input to the folding (word vectors extracted from the embedding layer). Characteristics are extracted by tangling the input text with a number of linear lender lters, adding a preload term and applying a nonlinearity. The result is a recognition of the features used in different steps."}, {"heading": "3.3 Recurrent layer", "text": "The result of the confrontation is a sequence of vectors, each of which is the concatenation of the individual terms (see above). - However, the interactions between the words are not particularly pronounced when the words are removed from each other. - The reactionary neural networks (RNNs) are a family of neural networks designed to process a sequence of values. We use an RNN at the top of the convolution to capture interactions along the entire sequence of words. RNNs are an extension of the multilayered perceptrons in which the output of each step is used as an additional input to the values. Activations reach the hidden layer of the network of both current external inputs and the hidden layer of hidden layer activations. The general formulation of an RNN is as follows: h (t)"}, {"heading": "3.4 Attention model", "text": "We use an attention model on top of our recurring layer to be able to capture the local characteristics that are more important in the present task. Limitation of the use of the regular recurring network for the classification task is that the last step of the recurring network loses some information about the sequence, especially when the sequence length becomes large [5]. This will not be a significant problem in short sentence classification tasks, but in our problem the reports may have several sentences and the sequence length may be long. While in theory the last step of the RNN is able to encode all the important information in the entire sequence, it tends to focus more on the newer time steps [31] and therefore loses some information in particular about the earlier time steps. With a bidirectional RNN we can partially mitigate this problem if the last state of the backward RNN together with the last state of the forward RNN is able to capture the information at the beginning and end of the sequence."}, {"heading": "3.5 Training", "text": "Let us specify all the parameters in the network, including the weights associated with each of the layers described in previous sections. Then, the entire network is trained to minimize the following loss function: J (\u0443) = \u2212 C \u2211 c = 1 1 [y * = c] log Pr (Y = c | x) (8), where C is the number of damage severity classes and y * is the basic truth label for the input report x, 1 [\u00b7] the indicator function and the probability of each damage severity class is estimated across the network."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "We use two large sets of data, consisting of patient safety and incident reports from different healthcare systems, which are sometimes referred to in health informatics as \"patient safety reports,\" but are generally used to identify and characterize errors in patient care. This study was approved by the Institutional Review Board of the MedStar Health Research Institute (Protocol 2014-101). The characteristics of the data sets are in Table 2. We note that one of the data sets (DS2) is larger than the other (DS1) and the length of the reports is rather different. Each data set consists of reports on specific categories of patient care. Statistics on each of the damage levels are in Table 3. Damage events (right side of the table) are generally much less common than events without actual harm (left side of the table). We divide each data set into three sets of training, validation and testing with appropriate distribution of 60%, 20% and 20% of the total data selected from empirically gathered data."}, {"heading": "4.2 Evaluation", "text": "We evaluate the effectiveness of our models when it comes to identifying damage by using standard metrics to evaluate classifications, namely precision, recall, F-1 and Area Under the Curve (AUC)."}, {"heading": "4.3 Baselines", "text": "To compare the performance of the proposed methods, we consider the following basic lines: \u2022 SVM bow - SVM with linear cornel with n-gram bag of words (bow) features [33]. We are experimenting with three types of characteristics, n-grams of size {1}, {1,2} and {1,2,3} (we abbreviate the resulting models with bow1, bow2 and bow3 respectively). \u2022 MNB bow - We are also experimenting with the multinomial Naive Bayes method of classification, where Wang and Manning [33] show their ectiveness in many text class tasks. We used scikitlearn (http: / / scikit-learn.org /) implementation of SVM and MNB. \u2022 CNN - We are looking at the CNN model for text class classification, which has shown good results both in the general field [18, 19] and in the biomedical field [28]. \u2022 LSTM - We are also comparing with RNN (LSTM) classi, which is similar to the models used in our models."}, {"heading": "4.4 Model variants", "text": "We evaluate several variants of our models. The first variant is our model architecture, which is the entire model shown in \u00a7 3 minus the attention model. We consider two types of recursive networks, GRU and LSTM, and their bidirectional variants (Bi-GRU and Bi-LSTM). Then we evaluate our complete model, which uses the attention mechanism. When looking at attention, we evaluate both GRU and LSTM as the underlying recursive layer. We shorten these models based on the layers from top to bottom. For example, \"ATT GRU CNN\" corresponds to our attention model with GRU unit, while \"ATT Bi-LSTM CNN\" corresponds to our attention model with bidirectional LSTM in the recurrent layer. Design decisions and hyperparameters were made empirically as follows: We used design decisions and hyperparameter selection: We used the sequence length of 100 words (we used the sequence size of 12 and the GRU)."}, {"heading": "4.5 Results", "text": "First, we look at the problem of identifying cases of harm in the patient reports. That is, we classify a report that shows some signs of harm to the patient (a case of damage) or not (a no-harm case).The main results of our methods of identifying damage are presented in Table 4. Measures are Precision, Recall, F-1 Score for the damage category and the area where the disease occurs. We observe that our attention models (starting with ATT in the table) are the best methods rated by F-1."}, {"heading": "4.6 Analysis", "text": "In order to better evaluate the performance of our system and investigate the errors it makes, we analyze performance on each data set based on each category of event reports. Incident reports are divided into several categories and there are often qualitative differences between narratives in different categories. Tables 6 and 7 show the breakdown of results based on the most common categories in data set 1 and 2. We report the results of the most powerful model variant on each data set (i.e. ATT LSTM CNN for data set 1 and ATT Bi-GRU CNN for data set 2). On data set 1 (Table 6) we observe that the model achieves very high scores in identifying damage in the skin / tissue category with F-1 of 92.9% in identifying damage. Results on some other categories such as Surgury / Procedure, Seclusion Injury, Airway Management and Blood Bank are relatively high."}, {"heading": "1 based on each category. Each data point shows the results in a speci c category as well as the ratio of harm cases in", "text": "The X-axis shows the ratio of the claims in this category. At the bottom left of Figure 5 are the categories with the lowest results in Table 6. The respective ratio of the claims in these categories is 0.009 and 0.027, while the ratio of the claims in skin / tissue (the point at the top right of the figure) is 0.53. We also performed qualitative analyses of the reports in each category by examining the type of incidents in each category. We examine the types of incidents in the best performing category in record 1 skin / tissue and most events relate to pressure ulcers and wounds. On the other hand, looking at the category laboratory / specimens, there are many different types of errors and damages in this category, such as capture problems, documentation problems, labeling problems, filing problems, etc., which are very impracticable in the description, making it a cult for the model to learn all the nuances in this category. This reason, together with the relatively low number of cases of claims we believe would be better with the results in this category in the data cases in the case of the X-axis, the X-axis shows the ratio of claims in this category."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we presented a neural network model for identifying damage in safety narratives associated with clinical care. We used a multi-layered network with revolutionary, recurrent, and soft attention mechanisms. We argued that a revolutionary layer is important to identify the local features and recurrent layer with attention in order to identify the interactions and dependencies along the sequence. We demonstrated that our methods can significantly improve performance over existing methods for identifying damage cases. The impact of the methods and results presented in this paper on patient care is significant. More precise damage identification methods can support the data analysis and reporting process, prevent harm to patients, better prioritize resources to address safety incidents, and then improve overall patient care."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the four anonymous reviewers for their helpful comments and suggestions. We thank NVIDIA Corporation for their support by donating the Titan X Pascal GPU used for this research, which was funded by the Agency for Healthcare Research and Quality (AHRQ), U.S. Department of Health and Human Services under grant / grant number R01 HS023701-02. Opinions expressed in this document are those of the authors and do not necessarily agree with the official position of AHRQ or the U.S. Department of Health and Human Services."}], "references": [{"title": "A survey of text classi cation algorithms", "author": ["Charu C Aggarwal", "ChengXiang Zhai"], "venue": "In Mining text data", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence 35,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A Neural Attention Model for Categorizing Patient Safety Events", "author": ["Arman Cohan", "Allan Fong", "Nazli Goharian", "Raj Ratwani"], "venue": "In Advances in Information Retrieval: 39th European Conference on IR Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Improving deep neural networks for LVCSR using recti ed linear units and dropout", "author": ["George E Dahl", "Tara N Sainath", "Geo rey E Hinton"], "venue": "In ICASSP", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts", "author": ["C\u00edcero Nogueira dos Santos", "Maira Gatti"], "venue": "In COLING", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Finding structure in time", "author": ["Je rey L Elman"], "venue": "Cognitive science 14,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Exploring methods for identifying related patient safety events using structured and unstructured data", "author": ["Allan Fong", "A Zachary Hettinger", "Raj M Ratwani"], "venue": "Journal of biomedical informatics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A Primer on Neural Network Models for Natural Language Processing", "author": ["Yoav Goldberg"], "venue": "Journal of Arti cial Intelligence Research (JAIR)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Alex Graves"], "venue": "Ph.D. thesis, Technische UniversitA\u0302l\u0301at MA\u0302l\u0301unchen", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "A New, Evidence-based Estimate of Patient Harms Associated with Hospital Care", "author": ["John T. James"], "venue": "Journal of Patient Safety", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In ACL", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classi cation", "author": ["Yoon Kim"], "venue": "In EMNLP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Ha ner"], "venue": "Proc. IEEE 86,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Recurrent neural network for text classi cation with multi-task learning", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "The problem with incident reporting", "author": ["Carl Macrae"], "venue": "BMJ Quality & Safety", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Medical error\u00e2\u0102\u0164the third leading cause of death in the US", "author": ["Martin A Makary", "Michael Daniel"], "venue": "Bmj 353 (2016),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Patient safety incident reporting: a qualitative study of thoughts and perceptions of experts 15 years after \u00e2\u0102\u0178 To Err is Human ", "author": ["Imogen Mitchell", "Anne Schuster", "Katherine Smith", "Peter Pronovost", "Albert Wu"], "venue": "BMJ Quality & Safety", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Automated identi cation of extreme-risk events in clinical incident reports", "author": ["Mei-Sing Ong", "Farah Magrabi", "Enrico Coiera"], "venue": "Journal of the American Medical Informatics Association 19,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "On the di culty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Convolutional neural networks for biomedical text classi cation: application in indexing biomedical articles", "author": ["Anthony Rios", "Ramakanth Kavuluru"], "venue": "In ACM-BCB", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing 45,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1997}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classi cation", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classi cation", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In ACL", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Opportunities for incident reporting", "author": ["Huw Williams", "Alison Cooper", "Andrew Carson-Stevens"], "venue": "BMJ Quality & Safety", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Text Classi cation with Topic-based Word Embedding and Convolutional Neural Networks", "author": ["Haotian Xu", "Ming Dong", "Dongxiao Zhu", "Alexander Kotov", "April Idalski Carcone", "Sylvie Naar-King"], "venue": "In ACM-BCB", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Hierarchical Attention Networks for Document Classi cation", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Comparison and combination of several MeSH indexing approaches", "author": ["Antonio Jose Jimeno Yepes", "James G Mork", "Dina Demner-Fushman", "Alan R Aronson"], "venue": "In AMIA annual symposium proceedings,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "Preventable medical errors have been shown to be a major cause of injury and death in the United States [9, 24, 34].", "startOffset": 104, "endOffset": 115}, {"referenceID": 30, "context": "Preventable medical errors have been shown to be a major cause of injury and death in the United States [9, 24, 34].", "startOffset": 104, "endOffset": 115}, {"referenceID": 21, "context": "[24, 30], which translates to an estimated incidence of 210,000 to 400,000 deaths annually [17, 24].", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "[24, 30], which translates to an estimated incidence of 210,000 to 400,000 deaths annually [17, 24].", "startOffset": 91, "endOffset": 99}, {"referenceID": 21, "context": "[24, 30], which translates to an estimated incidence of 210,000 to 400,000 deaths annually [17, 24].", "startOffset": 91, "endOffset": 99}, {"referenceID": 22, "context": "To address these major concerns, healthcare systems have adopted reporting systems in clinical care to help track and trend hazards and errors in patient care [25, 34].", "startOffset": 159, "endOffset": 167}, {"referenceID": 30, "context": "To address these major concerns, healthcare systems have adopted reporting systems in clinical care to help track and trend hazards and errors in patient care [25, 34].", "startOffset": 159, "endOffset": 167}, {"referenceID": 20, "context": "Although reporting systems have been implemented with the goal of improving patient safety and patient care, hospital sta are faced with many challenges in analyzing and understanding these reports [23, 25].", "startOffset": 198, "endOffset": 206}, {"referenceID": 22, "context": "Although reporting systems have been implemented with the goal of improving patient safety and patient care, hospital sta are faced with many challenges in analyzing and understanding these reports [23, 25].", "startOffset": 198, "endOffset": 206}, {"referenceID": 30, "context": "One important aspect in patient care is to identify events that have contributed to or resulted in harm to the patient [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 9, "context": "While there is a growing number of work in categorizing patient safety reports, none has looked at the modeling of general harm across all event types [12, 26].", "startOffset": 151, "endOffset": 159}, {"referenceID": 23, "context": "While there is a growing number of work in categorizing patient safety reports, none has looked at the modeling of general harm across all event types [12, 26].", "startOffset": 151, "endOffset": 159}, {"referenceID": 9, "context": "[12] explored both the unstructured free-text and structured data elements in safety reports to identify and rank similar events.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] explored the similar problem of identifying extreme-risk events in patient safety and incident reports using Naive Bayes and SVM classi ers with bag of words features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Traditional approaches in text classi cation include methods to extract features from text and then use the feature vector as an input to a classi er such as SVM [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 18, "context": "Two of the more widely used neural network architectures have been Convolutional Neural Networks (CNN) [21] and Recurrent Neural Networks (RNN) [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Two of the more widely used neural network architectures have been Convolutional Neural Networks (CNN) [21] and Recurrent Neural Networks (RNN) [11].", "startOffset": 144, "endOffset": 148}, {"referenceID": 5, "context": "[7] were one of the rst to utilize CNNs in many NLP tasks including text classi cation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[21] with adaptations to the NLP domain and showed improvements on several NLP tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Later CNNs were further explored for sentence modeling and classi cation tasks [10, 18, 19].", "startOffset": 79, "endOffset": 91}, {"referenceID": 15, "context": "Later CNNs were further explored for sentence modeling and classi cation tasks [10, 18, 19].", "startOffset": 79, "endOffset": 91}, {"referenceID": 16, "context": "Later CNNs were further explored for sentence modeling and classi cation tasks [10, 18, 19].", "startOffset": 79, "endOffset": 91}, {"referenceID": 33, "context": "These works have mostly used supervised learning frameworks with bag of words features, namedentities, and ontology speci c features [37].", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "More recently, Rios and Kavuluru [28] utilized CNNs for this task; they showed that CNNs are more e ective than feature based methods in biomedical indexing.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "The authors in [6] used a neural network architecture using CNN and RNNs to classify patient safety report.", "startOffset": 15, "endOffset": 18}, {"referenceID": 31, "context": "[35] used a CNN architecture, with multiple sources of word embeddings and evaluated its e ectiveness on the tasks of biomedical literature indexing and clinical note annotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Word emdeddings or distributed representations of words aim to embed (represent) words with dense vectors such that words with similar properties have similar vectors [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 18, "context": "Convolutional Neural Networks (CNNs) [21] have been previously used in sentence modeling and classi cation tasks [18, 19].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "Convolutional Neural Networks (CNNs) [21] have been previously used in sentence modeling and classi cation tasks [18, 19].", "startOffset": 113, "endOffset": 121}, {"referenceID": 16, "context": "Convolutional Neural Networks (CNNs) [21] have been previously used in sentence modeling and classi cation tasks [18, 19].", "startOffset": 113, "endOffset": 121}, {"referenceID": 18, "context": "Similar to convolutional networks for object recognition [21], we use multiple feature maps with di erent lters to capture various aspects of the input sequence.", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "Where h(t) shows the hidden state of the RNN in time step t , x(t) is the input sequence at time step t ,W (h),W (x ), andW (s) are the weights associated with the hidden state, input, and softmax, respectively, and \u0434 is an activation function such as RELU [8].", "startOffset": 257, "endOffset": 260}, {"referenceID": 10, "context": "In sequence modeling tasks, the nal hidden state of the network can represent the whole sequence and can be used for making predictions [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 24, "context": "Training the general formulation of RNNs in practice is di cult due to the exploding and vanishing gradient problems (gradients becoming exceedingly high or become exceedingly close to 0 after only a few timesteps) [27].", "startOffset": 215, "endOffset": 219}, {"referenceID": 13, "context": "Most notable are the Long Short Term Memory (LSTM) [16] and Gated Recurrent Unit (GRU) [5].", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "Most notable are the Long Short Term Memory (LSTM) [16] and Gated Recurrent Unit (GRU) [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 12, "context": "Concretely, we are referring to the formulation of Graves [15] for LSTM.", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "[5] makes each recurrent unit to adaptively capture dependencies of di erent time scales and similar to LSTM, GRU also has gating units that control the ow of information through the computational graph.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] for GRU.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "In order to also capture the backward dependencies and interactions between di erent parts of a sequence, a backward RNN is also trained which can encode the information from the future time steps [14, 29].", "startOffset": 197, "endOffset": 205}, {"referenceID": 26, "context": "In order to also capture the backward dependencies and interactions between di erent parts of a sequence, a backward RNN is also trained which can encode the information from the future time steps [14, 29].", "startOffset": 197, "endOffset": 205}, {"referenceID": 3, "context": "The limitation of using the regular recurrent network for the classi cation task is that the last time step of recurrent network loses some information about the sequence, specially when the sequence length becomes large [5].", "startOffset": 221, "endOffset": 224}, {"referenceID": 27, "context": "While in theory, the last step of the RNN is able to encode all the important information in the entire sequence, in practice it tends to focus more on the more recent time steps [31] and therefore loses some information specially about the earlier time steps.", "startOffset": 179, "endOffset": 183}, {"referenceID": 1, "context": "Inspired by recent work in machine translation [3] and document modeling [36], we propose to address this problem using a soft attention mechanism.", "startOffset": 47, "endOffset": 50}, {"referenceID": 32, "context": "Inspired by recent work in machine translation [3] and document modeling [36], we propose to address this problem using a soft attention mechanism.", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "\u2022 SVM bow - SVM with linear kernel with n-gram bag of words (bow) features [33].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "\u2022 MNB bow - We also experiment with Multinomial Naive Bayes method for classi cation where Wang and Manning [33] show its e ectiveness in many text classi cation tasks.", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "\u2022 CNN - We consider CNN model for text classi cation which has shown good results in both general domain [18, 19] and biomedical domain [28].", "startOffset": 105, "endOffset": 113}, {"referenceID": 16, "context": "\u2022 CNN - We consider CNN model for text classi cation which has shown good results in both general domain [18, 19] and biomedical domain [28].", "startOffset": 105, "endOffset": 113}, {"referenceID": 25, "context": "\u2022 CNN - We consider CNN model for text classi cation which has shown good results in both general domain [18, 19] and biomedical domain [28].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "\u2022 LSTM - We also compare against RNN (LSTM) classi er which is similar to the models used in [22, 32] (see Figure 3).", "startOffset": 93, "endOffset": 101}, {"referenceID": 28, "context": "\u2022 LSTM - We also compare against RNN (LSTM) classi er which is similar to the models used in [22, 32] (see Figure 3).", "startOffset": 93, "endOffset": 101}, {"referenceID": 17, "context": "Adam [20] was used as optimizer and early stopping was applied by monitoring accuracy on the validation set.", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "As far as the baselines, we can see that in general, in terms of F-1 scores, traditional bag of words approaches [33] are not", "startOffset": 113, "endOffset": 117}], "year": 2017, "abstractText": "Preventable medical errors are estimated to be among the leading causes of injury and death in the United States. To prevent such errors, healthcare systems have implemented patient safety and incident reporting systems. These systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care. These reports are narratives in natural language and while they provide detailed information about the situation, it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients. In this work, we present a method based on attentive convolutional and recurrent networks for identifying harm events in patient care and categorize the harm based on its severity level. We demonstrate that our methods can signi cantly improve the performance over existing methods in identifying harm in clinical care.", "creator": "LaTeX with hyperref package"}}}