{"id": "1708.04686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation", "abstract": "Rich and dense human labeled datasets are among the main enabling factors for the recent advance on vision-language understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes --- and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling. We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage.", "histories": [["v1", "Tue, 15 Aug 2017 20:47:02 GMT  (3730kb,D)", "http://arxiv.org/abs/1708.04686v1", "To appear on ICCV 2017"]], "COMMENTS": "To appear on ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["chuang gan", "yandong li", "haoxiang li", "chen sun", "boqing gong"], "accepted": false, "id": "1708.04686"}, "pdf": {"name": "1708.04686.pdf", "metadata": {"source": "CRF", "title": "VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation", "authors": ["Chuang Gan", "Yandong Li", "Haoxiang Li", "Chen Sun", "Boqing Gong"], "emails": [], "sections": [{"heading": null, "text": "We will present the preparatory work to link the instance segmentation provided by COCO to the Q & A in the VQA dataset, and identify the visual questions and segmentation responses (VQS) that have been collected, transferring human monitoring between the previously separate tasks, providing a more effective lever for existing problems, and also opening the door to new research problems and models. In this paper, we will examine two applications of VQS data: supervised attention to VQA and a novel question-focused semantic segmentation task. For the former, we will obtain up-to-date results on the true multiple-choice task in VQA by simply expanding the multi-layered perceptrons with some attention traits learned as explicit monitoring from the segmentation QA links. To put the latter into perspective, we will examine two plausible methods and compare them with a test method that we assume occurs in an inclination."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive by blaming themselves and others, and most of them are able to survive by blaming themselves and others. (...) Most of them are able to survive by blaming themselves and others. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "1.1. Applications of the segmentation-QA links", "text": "We refer to the collected linkages between the COCO segmentations [25] and the QA pairs in the VQA dataset [3] as visual linkages and segmentation responses (VQS). Such linkages transfer human supervision between the hitherto separate tasks, i.e. semantic segmentation and VQA. They enable us to address existing problems more effectively than before and also open the door to new research problems and models for visual language understanding. In this paper, we examine two applications of our VQS dataset: monitored attention to VQA and a novel question-focused semantic segmentation task (QFSS). For the former, we obtain current results on the VQA multiplechoice task by simply adding attention features to the Multilayer Perceptrons (MLP) of [17]."}, {"heading": "1.1.1 Supervised attention for VQA", "text": "The attention scheme is often considered useful for VQA by either visiting certain image regions [48, 46, 45, 26, 24] or modelling object relationships [2, 27]. However, in the absence of explicit attention comments, existing methods opt for latent variables and use indirect cues (e.g. text responses) as a conclusion. As a result, the machine-generated attention cards correlate poorly with human attention cards [5]. This is not surprising, since latent variables hardly correspond to semantic interpretations due to the lack of explicit training signals; similar observations exist in other studies, e.g. object recognition [8], video recognition [11] and word processing [49]. These phenomena underscore the need for explicit links between the visual and text responses that are realized in this work as VQS overall, we show that we can visit the QA through various modeling modes, by means of learning the QA very convincingly."}, {"heading": "1.1.2 Question-focused semantic segmentation (QFSS)", "text": "In addition to monitored attention to better manage VQA, VQS also allows us to explore novel question-oriented semantic segmentation (QFSS). Since VQA only wants text answers, there are potential abbreviations for the learner, e.g. to generate correct answers without accurately explaining the locations and relationships of different visual units. While Visual Grounding (VG) avoids the caveat by providing boundary fields [35, 37, 28, 16] or segmentation [15] across visual audiences, the scope of text expressions in existing VG works is often limited to the visual units present in the image. To bring together the best of VQA and VG, we propose the task of QFSS, whose goal is to produce pixel-by-segmentation to visually answer the questions about images."}, {"heading": "2. Linking image segmentations to text QAs", "text": "In this section we describe in detail how we collect the links between the semantic image segmentation and text questions and answers (QAs). We build our work on the images and instance segmentation masks in COCO [25] and the QAs in the VQA dataset [3]. The COCO images are mainly about everyday scenes that contain common objects in their natural context and take into account complex interactions and relationships between different visual units. To avoid trivial links between the segmentations and QA pairs, we only keep the images that contain at least three instance segmentations in this thesis. The questions in VQA [3] are diverse and cover different parts of an image, different levels of semantic interpretation, as well as common sense and knowledge bases. Next, we elaborate the annotation instructions and provide some analysis of the collected dataset."}, {"heading": "2.1. Annotation instructions", "text": "In fact, it's not that you see yourself as being able to outdo yourself, but that you see yourself as being able to outdo and outdo yourself. In fact, it's not that you outdo each other, but that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other, that you outdo each other. \""}, {"heading": "2.2. Tasks addressed by the participants", "text": "Thanks to the rich catalog of questions collected by Agrawal et al. [3] and the complex visual scenes in COCO [25], participants must analyze the question, understand the visual scene and context, derive the interactions between visual entities, and then pick up the segmentations that answer the questions. In Figure 2 (b), for example, participants must identify the cup in the crowded scene to facilitate the following discussion. Figure 2 shows some typical examples of object recognition. Many questions arise directly from the properties of some objects in the images. In Figure 2 (b), for example, participants are asked to identify the cup in the crowded scene to answer the question \"What color is the coffee cup?.\" Semantic segmentation. For some questions, the visual evidence for the answers is best represented by semantic segmentation. Take Figure 2 (j) and (k), for example: Simple detection of the rider and / bicycle or interaction would be impossible to press."}, {"heading": "2.3. Data statistics", "text": "After collecting the annotations, we remove the question-image pairs for which users have selected the black buttons (full screen) or gray buttons (unknown) to avoid trivial or ambiguous segmentation QA links. In total, we retain 37,868 images, 96,508 questions, 108,537 instance segmentations and 43,725 circumscription fields. Below, we do not distinguish the segmentations from the circumscription fields to facilitate presentation and also for the reason that the surrounding fields are narrow, small and much smaller than the segmentations. Figure 3 counts the distribution of the possible number of instance segments selected per image in response to a question. Over 70% of the questions are answered by a segmentation. On average, each question-image pair has 6.7 candidate segmentations, 1.6 of which are selected by the anchors as visual answers. In Figure 4, we visualize the distribution of the 135 types of questions (Which is the number of the 375 questions we answer by average)."}, {"heading": "3. Applications of VQS", "text": "The user-linked visual questions and segmentations, where the latter answer the former visually, are highly versatile, providing better leverage than before for at least two issues, namely supervised attention to VQA and question-oriented semantic segmentation (QFSS)."}, {"heading": "3.1. Supervised attention for VQA", "text": "To verify this point, we are designing a simple experiment to complement the MLP model in [17]. The extended MLP significantly improves the simple version and leads to state-of-the-art results in the VQA true multiple choice task [3]. Experimental setup. We are conducting experiments with the VQA real multiple choice [3]. The data set contains 248,349 questions for training, 121,512 for validation and 244,302 for the test. Each question has 18 possible answers for candidates and the learning agent must find the correct answer between them. We evaluate our results according to the metric proposed in [3]. MLP for the VQA multiple choice method. Since the VQA multiple choice task provides a solution for the candidates and the learning agent is obliged to find the correct answer between them."}, {"heading": "3.1.1 Augmenting MLP by supervised attention", "text": "We propose to extend the MLP model to include richer feature representations of questions, answers, images and, above all, monitored attention characteristics. Question and answer characteristics xq & xa. For a question or answer, we present them by determining the mean value of the 300D word2vec [30] vectors of constituent words, followed by l2 normalization. This is the same as in [17]. Image characteristics xi. We extract two types of characteristics from an input image: Net [14] pool5 activation and attribute characteristics [44], the latter being the attribute recognition characteristics. We implement an attribute detector by revising the output layer of ResNet. Specifically, we impose a sigmoid function for each attribute and then train the network using binary cross-entropy losses."}, {"heading": "3.1.2 Experimental results", "text": "Table 1 reports on the results of comparing the attention characteristics of advanced MLP with several state-of-the-art methods for the true multiple choice VQA task. We mainly use the Test Dev for comparison. After determining our best individual and ensemble models, we also submit them to the evaluation server to record the results to Test Standard. First, we find that there is an absolute 1.5% improvement over the pure MLP model (MLP + ResNet) by simply extending it with the learned attention characteristics (MLP + ResNet + Atten.) Second, the attribute characteristics for the images are actually quite effective. We achieve a 1.0% improvement over the pure MLP by replacing the image characteristics of ResNet with the attribute characteristics of MLP + Attribute + ResNet (compare the series of MLP + Attributes vs. MLP + ResNet). Nevertheless, by obtaining attention characteristics over the pure MLP, we can still replace the attribute characteristics of MLP + Attribute 1.1."}, {"heading": "3.1.3 What is good supervision for attention in VQA?", "text": "In this section, we contrast the VQS data with the Human Care Maps (HAT) [5] and Bounding Boxes, which are tightly arranged around the segmentations in VQS. Comparative results reported in Table 2 are evaluated on the TestDev dataset of VQA Real Multiple Choice. We see that the segment aitons associated with QAs provide slightly better results than Bounding Boxes, which continue to outperform HAT. These confirm our hypothesis that HAT may be suboptimal for supervised attention learning in VQA, as they typically reveal small portions of objects and contain large portions of the background. However, we believe that it remains interesting to examine VQS for more general attention-based VQA models [48, 46, 45, 26, 24, 2, 27]. In the supplementary materials, we describe the detailed implementation for the ensemble model. We also present Segment Results, which influence the different types of VQA."}, {"heading": "3.2. Question-focused semantic segmentation", "text": "This section examines a new task, question-focused semantic segmentation (QFSS), which is feasible thanks to the collected VQS, which combines two previously separate tasks (i.e. segmentation and VQA). Faced with a question about an image, QFSS expects the learner to provide a visual answer by semantically segmenting the correct visual units from the image. It is designed in a similar way to segmentation from natural speech expressions [15], with possible applications for robotic vision, image editing, etc. To put the new task in perspective, we propose a mask aggregation approach for QFSS, examine a baseline and also examine a method of upper limit, assuming that all instance segmentations are given as oracles in the test phase."}, {"heading": "3.2.1 Mask aggregation for QFSS", "text": "The modeling hypothesis is that the desired output segmentation mask can be composed of high-quality segmentation suggestions. In particular, we use N = 25 segmentation suggestions e1, e2, \u00b7 \u00b7, eN generated by SharpMask [34] against an image. Each suggestion is a binary segmentation mask of the same size as the image. We then swear by a convex combination of these masks E =, i siei as final output in response to a questionable image pair, where the i-th combination coefficient Si is determined by the question Features xq and the representations of the i-th segmentation suggestion. We learn model parameters A by minimizing a loss of two segmentation suggestions selected by the user."}, {"heading": "3.2.2 Experiments on QFSS", "text": "Features. In addition to displaying the questions with the word embed functions xq as in Section 3.1.1, we also test the bag-of-words features. For each instance of segmentation or suggestion, we mask all other pixels in the image with 0 and then extract its features from the last pooling layer of a ResNet-152 [14].Dataset Split. The SharpMask we use is learned from the training set of MS COCO. Therefore, we divide our VQS data in such a way that our test set does not intersect with the training set for SharpMask. Specifically, we use 26,995 images and correspondingly 68,509 questions as a training set. We divide the remaining images and questions into two parts: 5,000 images and related validation questions, and 5,873 images with 14,875 questions as a test set. Results: The definition of the target formation is a working group. The comparison results on QSS, evaluated by Intersection-Union (IOU)."}, {"heading": "4. Conclusion", "text": "In this paper, we propose linking the instance segmentations provided by COCO [25] to the questions and answers in VQA [3]. The collected linkages, called visual questions and segmentation answers (VQA), transfer human monitoring between the individual tasks of semantic segmentation and VQA, allowing us to examine at least two problems with better leverage than before: supervised attention to VQA and a novel question-focused semantic segmentation task. To put it into perspective, we are investigating a basic method and an upper method, assuming that instance segmentations are given only as oracles. Our work is inspired by the observation of the popularity of COCO [25]. We suspect that the existing and seemingly different characteristics about the Chinese systems MCO601 and MCO601 do not just result in different human segments."}, {"heading": "A. Annotation Interface", "text": "Figure 10 shows the user interface for annotations that we used to capture the VQS dataset. Faced with a question about an image, participants are asked to tick the colors of the corresponding segments in addition to selecting the segments to visually answer the question. Participants can also click the \"Add\" button to drag delimiter boxes over the image to answer the question. For more information, see the accompanying slides that we used to train the annotations. For example, VQS vs. VQA-HAT Figure 9 contrasts the human attention cards in VAQHAT [5] with our collected image segments, which are associated by participants with the questions and answers. We note that the HAT maps are roughly compared with the segmentation masks. For example, to answer the question \"What color does the ball have?,\" our VQS dataset will provide a very accurate background for the potential segmentation maps we expect to provide, while the QAT models are not."}, {"heading": "C. The influence of VQS segmentation mask", "text": "The attention characteristics examined in Section 3.1.1 of the main text weigh the characteristic representations of different regions according to the question about the image. The number of regions per image indicates the attention resolutions. The more regions (the higher the resolution) we look at, the more accurate the attention model could be. Of course, too small regions would also lead to trivial solutions, as the visual cues in each region would then be too subtle. In Table 4, we report the results of the VQA Real Multiple Choice on the Test Dev using different resolutions of the segmentation masks. We can observe that higher resolution leads to better VQA results. In a way, this implies the need for accurate segmentation annotations for monitored attention in VQA."}, {"heading": "D. Some implementation details in the VQA", "text": "In our experiments, we use a group of 10 models for the VQA Real Multiple Choice task (see Table 1 of the main text), five of which are trained on the attributes of the images and the other five on the ResNet characteristics. We use the validation set to select the best 10 models and combine them by a convex combination of their decision values. We then test the ensemble on test dev and test standard, or for the VQS experiments, we use the ADAM [21] gradient drop to train the entire network at the learning rate of 0.001 and stack size 16. It takes about a week on a Titan X GPU machine before convergence occurs after 15 epochs. We also report on some additional results in Table 5 for our exploration of the LSTM language embedded in the DeconvNet approach."}], "references": [{"title": "SPICE: Semantic propositional image caption evaluation", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "ECCV, pages 382\u2013398,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "K. Dan"], "venue": "CVPR, 27:55\u201356,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, pages 2425\u20132433,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "CVPR, pages 2422\u20132431,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh", "D. Batra"], "venue": "arXiv preprint arXiv:1606.03556,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, pages 2625\u20132634,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparing automatic evaluation measures for image description", "author": ["D. Elliott", "F. Keller"], "venue": "ACL, 452(457):457,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "T-PAMI, 32(9):1627\u20131645,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "EMNLP,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Stylenet: Generating attractive visual captions with styles", "author": ["C. Gan", "Z. Gan", "X. He", "J. Gao", "L. Deng"], "venue": "CVPR,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "DevNet: A deep event network for multimedia event detection and evidence recounting", "author": ["C. Gan", "N. Wang", "Y. Yang", "D.-Y. Yeung", "A.G. Hauptmann"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2568\u20132577,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic compositional networks for visual captioning", "author": ["Z. Gan", "C. Gan", "X. He", "Y. Pu", "K. Tran", "J. Gao", "L. Carin", "L. Deng"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, pages 2296\u20132304,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Segmentation from natural language expressions", "author": ["R. Hu", "M. Rohrbach", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Natural language object retrieval", "author": ["R. Hu", "H. Xu", "M. Rohrbach", "J. Feng", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 3128\u20133137,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D. Kwak", "M.-O. Heo", "J. Kim", "J.-W. Ha", "B.-T. Zhang"], "venue": "NIPS, pages 361\u2013369,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Hadamard product for low-rank bilinear pooling", "author": ["J.-H. Kim", "K.-W. On", "J. Kim", "J.-W. Ha", "B.-T. Zhang"], "venue": "ICLR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual question answering with question representation update (qru)", "author": ["R. Li", "J. Jia"], "venue": "Advances in Neural Information Processing Systems, pages 4655\u20134663,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Generation and comprehension of unambiguous object descriptions", "author": ["J. Mao", "J. Huang", "A. Toshev", "O. Camburu", "A. Yuille", "K. Murphy"], "venue": "arXiv preprint arXiv:1511.02283,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS, 26:3111\u20133119,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV, pages 1520\u20131528,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P. Hongsuck Seo", "B. Han"], "venue": "CVPR, pages 30\u201338,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "CVPR, pages 4594\u20134602,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to refine object segments", "author": ["P.O. Pinheiro", "T.Y. Lin", "R. Collobert", "P. Doll\u00e1r"], "venue": "ECCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "ICCV, pages 2641\u20132649,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS, pages 2953\u20132961,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["A. Rohrbach", "M. Rohrbach", "R. Hu", "T. Darrell", "B. Schiele"], "venue": "ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR, pages 4613\u20134621,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic concept discovery from parallel text and visual corpora", "author": ["C. Sun", "C. Gan", "R. Nevatia"], "venue": "ICCV,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "CVPR, pages 4566\u20134575,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, pages 3156\u20133164,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep structurepreserving image-text embeddings", "author": ["L. Wang", "Y. Li", "S. Lazebnik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5005\u20135013,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Q. Wu", "C. Shen", "L. Liu", "A. Dick", "A. van den Hengel"], "venue": "In CVPR,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "ECCV,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, pages 2048\u20132057,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning structural svms with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "ICML, pages 1169\u20131176. ACM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling context in referring expressions", "author": ["L. Yu", "P. Poirson", "S. Yang", "A.C. Berg", "T.L. Berg"], "venue": "European Conference on Computer Vision, pages 69\u201385. Springer,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 41, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 28, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 17, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 5, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 39, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 3, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 46, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 11, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 32, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 9, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 6, "context": "jective \u2014 it is hard to evaluate the quality of captions generated by different algorithms [7, 41, 1], and tend to miss subtle details \u2014 in training, the models may be led to capturing the scene-level gist rather than fine-grained entities.", "startOffset": 91, "endOffset": 101}, {"referenceID": 40, "context": "jective \u2014 it is hard to evaluate the quality of captions generated by different algorithms [7, 41, 1], and tend to miss subtle details \u2014 in training, the models may be led to capturing the scene-level gist rather than fine-grained entities.", "startOffset": 91, "endOffset": 101}, {"referenceID": 0, "context": "jective \u2014 it is hard to evaluate the quality of captions generated by different algorithms [7, 41, 1], and tend to miss subtle details \u2014 in training, the models may be led to capturing the scene-level gist rather than fine-grained entities.", "startOffset": 91, "endOffset": 101}, {"referenceID": 2, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 50, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 35, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 12, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 34, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 14, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 36, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 27, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 15, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 42, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 49, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 24, "context": "COCO [25] is especially noticeable among them.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "crowdsourced questions and answers (QAs) about a subset of the COCO images and abstract scenes [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 50, "context": "sociated with bounding boxes in the images [51].", "startOffset": 43, "endOffset": 47}, {"referenceID": 27, "context": "[28] and Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50] have users to give referring expressions that each pinpoints a unique object in an image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The Visual Genome dataset [22] also intersects with COCO in terms of images and provides dense human annotations, especially scene graphs.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "In particular, we focus on linking the segmentations provided by COCO [25] to the QAs in the VQA dataset [3].", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "In particular, we focus on linking the segmentations provided by COCO [25] to the QAs in the VQA dataset [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "have collected some human attention maps for the VQA task [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 50, "context": "While bounding boxes are provided in Visual7W [51] for object mentions in QAs, they do not serve for the purpose of directly answering the questions except for the \u201cpointing\u201d type of questions.", "startOffset": 46, "endOffset": 50}, {"referenceID": 24, "context": "We call the collected links between the COCO segmentations [25] and QA pairs in the VQA dataset [3] visual questions and segmentation answers (VQS).", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "We call the collected links between the COCO segmentations [25] and QA pairs in the VQA dataset [3] visual questions and segmentation answers (VQS).", "startOffset": 96, "endOffset": 99}, {"referenceID": 16, "context": "For the former, we obtain state-of-the-art results on the VQA real multiplechoice task by simply augmenting the multilayer perceptrons (MLP) of [17] with attention features.", "startOffset": 144, "endOffset": 148}, {"referenceID": 47, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 45, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 44, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 25, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 23, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 1, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 151, "endOffset": 158}, {"referenceID": 26, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 151, "endOffset": 158}, {"referenceID": 4, "context": "As a result, the machine-generated attention maps are poorly correlated with human attention maps [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": ", object detection [8], video recognition [11] and text processing [49].", "startOffset": 19, "endOffset": 22}, {"referenceID": 10, "context": ", object detection [8], video recognition [11] and text processing [49].", "startOffset": 42, "endOffset": 46}, {"referenceID": 48, "context": ", object detection [8], video recognition [11] and text processing [49].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "We show that, by supervised learning to attend different image regions using the collected segmentationQA links, we can boost the simple MLP model [17] to very compelling performance on the VQA real multi-choice task.", "startOffset": 147, "endOffset": 151}, {"referenceID": 34, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 36, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 27, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 15, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 14, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "\u2019s work [15] is the most related to QFSS.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Unlike the questions used in this work that are flexible to incorporate commonsense and knowledge bases, the expressive scope of the text phrases in [15] is often limited to the visual entities in the associated images.", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": "We build our work upon the images and instance segmentation masks in COCO [25] and the QAs in the VQA dataset [3].", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "We build our work upon the images and instance segmentation masks in COCO [25] and the QAs in the VQA dataset [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "The questions in VQA [3] are diverse and comprehensively cover various parts of an image, different levels of semantic interpretations, as well as commonsense and knowledge bases.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "[3] and the complex visual scenes in COCO [25], the participants have to parse the question, understand the visual scene and context, infer the interactions between visual entities, and then pick up the segmentations that answer the questions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[3] and the complex visual scenes in COCO [25], the participants have to parse the question, understand the visual scene and context, infer the interactions between visual entities, and then pick up the segmentations that answer the questions.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "After that, we augment the MLP model [17] by the attention features.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "To verify this point, we design a simple experiment to augment the MLP model in [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The augmented MLP significantly improves upon the plain version and gives rise to state-of-the-art results on the VQA real multiple-choice task [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "We conduct experiments on the VQA Real Multiple Choices [3].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "We evaluate our results following the metric suggested in [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "propose to transform the problem to a stack of binary classification problems [17] and solve them by the multilayer perceptrons (MLP) model:", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Two-layer LSTM [3] 62.", "startOffset": 15, "endOffset": 18}, {"referenceID": 37, "context": "1 Region selection [38] 62.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "4 DPPNet [32] 62.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "7 MCB [9] 65.", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "4 \u2212 Co-Attention [26] 65.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "1 MRN [19] 66.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "3 MLB [20] \u2212 68.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "9 MLP + ResNet [17] 67.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "For a question or answer, we represent it by averaging the 300D word2vec [30] vectors of the constituent words, followed by the l2 normalization.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "This is the same as in [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "We extract two types of features from an input image: ResNet [14] pool5 activation and attribute features [44], where the latter is the attribute detection scores.", "startOffset": 61, "endOffset": 65}, {"referenceID": 43, "context": "We extract two types of features from an input image: ResNet [14] pool5 activation and attribute features [44], where the latter is the attribute detection scores.", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "The training data is obtained from the COCO image captions [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "The upper panel of Figure 5 illustrates the process of extracting the attention features, and the bottom panel shows the MLP model [17] augmented with our attention features for the VQA real multiple-choice task.", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "Plain MLP [17] 80.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "HAT [5] 80.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "In this section, we contrast the VQS data to the human attention maps (HAT) [5] and bounding boxes that are placed tightly around the segmentations in VQS.", "startOffset": 76, "endOffset": 79}, {"referenceID": 47, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 45, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 44, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 25, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 23, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 1, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 26, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 14, "context": "It is designed in a way similarly to the segmentation from natural language expressions [15], with possible applications to robot vision, photo editing, etc.", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "In particular, we use N = 25 segmentation proposals e1, e2, \u00b7 \u00b7 \u00b7 , eN generated by SharpMask [34] given an image.", "startOffset": 94, "endOffset": 98}, {"referenceID": 44, "context": ", memory network [45] and stacked attention network [48]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": ", memory network [45] and stacked attention network [48]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Finally, we study a competitive baseline which is motivated by the textconditioned FCN [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "As Figure 7 shows, it contains three components, a convolutional neural network (CNN) [23], a deconvolutional neural network (DeconvNet) [31], and a question embedding to attend the feature maps in CNN.", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "As Figure 7 shows, it contains three components, a convolutional neural network (CNN) [23], a deconvolutional neural network (DeconvNet) [31], and a question embedding to attend the feature maps in CNN.", "startOffset": 137, "endOffset": 141}, {"referenceID": 30, "context": "The convolutional and deconvolutional nets follow the specifications in [31].", "startOffset": 72, "endOffset": 76}, {"referenceID": 38, "context": "Namely, a VGG-16 [39] is trimmed till the last convolutional layer, followed by two fully connected layers, and then mirrored by DeconvNet.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "For each instance segmentation or proposal, we mask out all the other pixels in the image with 0\u2019s and then extract its features from the last pooling layer of a ResNet-152 [14].", "startOffset": 173, "endOffset": 177}, {"referenceID": 24, "context": "In this paper, we propose to link the instance segmentations provided by COCO [25] to the questions and answers in VQA [3].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "In this paper, we propose to link the instance segmentations provided by COCO [25] to the questions and answers in VQA [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 24, "context": "Our work is inspired upon observing the popularity of COCO [25].", "startOffset": 59, "endOffset": 63}], "year": 2017, "abstractText": "Rich and dense human labeled datasets are among the main enabling factors for the recent advance on visionlanguage understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes \u2014 and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling. We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage.", "creator": "LaTeX with hyperref package"}}}