{"id": "1708.04776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network", "abstract": "Nowadays, cross-modal retrieval plays an indispensable role to flexibly find information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval. Different modalities such as image and text have imbalanced and complementary relationships, which contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa. Existing works based on Deep Neural Network (DNN) mostly construct one common space for different modalities to find the latent alignments between them, which lose their exclusive modality-specific characteristics. Different from the existing works, we propose modality-specific cross-modal similarity measurement (MCSM) approach by constructing independent semantic space for each modality, which adopts end-to-end framework to directly generate modality-specific cross-modal similarity without explicit common representation. For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding to utilize the learned attention weights for guiding the fine-grained cross-modal correlation learning, which can capture the imbalanced and complementary relationships between different modalities. Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modality-specific cross-modal similarities to perform cross-modal retrieval. Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset verify the effectiveness of our proposed approach, outperforming 9 state-of-the-art methods.", "histories": [["v1", "Wed, 16 Aug 2017 05:43:54 GMT  (5195kb,D)", "http://arxiv.org/abs/1708.04776v1", "13 pages, submitted to IEEE Transactions on Image Processing"]], "COMMENTS": "13 pages, submitted to IEEE Transactions on Image Processing", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["yuxin peng", "jinwei qi", "yuxin yuan"], "accepted": false, "id": "1708.04776"}, "pdf": {"name": "1708.04776.pdf", "metadata": {"source": "CRF", "title": "Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network", "authors": ["Yuxin Peng", "Jinwei Qi", "Yuxin Yuan"], "emails": ["pengyuxin@pku.edu.cn)."], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. RELATED WORKS", "text": "In this section, the representative cross-modal retrieval methods with common space learning as well as the recent advances in the attention mechanisms in the DNN are briefly discussed."}, {"heading": "A. Common Space Learning for Cross-modal Retrieval", "text": "As a matter of fact, most of them are able to survive themselves by going in search of their own identity, most of them are able to identify themselves, and most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "B. Attention Mechanism", "text": "We mainly introduce two types of attention mechanisms, visual attention and textual attention. [33] propose a task-based visual processing framework for image classification that uses the recursive neural network (RNN) to adaptively select a sequence of regions. Gregor et al. [34] propose a spatial attention mechanism that provides a task-based visual processing framework for image classification that uses recursive neural networks (RNN) to adapt a sequence of regions. Mnih et al. [33] propose a spatial attention mechanism that designs a sequential auto-encoding framework to perform image generation. Yang et al. [35] propose Stacked Attention Networks (SANs) for answering image questions that can locate relevant image regions."}, {"heading": "III. OUR PROPOSED APPROACH", "text": "As shown in Figure 4, our proposed MCSM approach uses modality-specific measurements to construct independent semantic spaces through an end-to-end framework for image or text. In each semantic space, first, a recurring attention network is used to fully exploit the fine-grained modality-specific features; second, an attention-based common embedding is used to capture the unbalanced and complementary relationships between different modalities and perform intermodal correlation learning; and finally, adaptive fusion is proposed to investigate the complementarity between different semantic spaces for intermodal query."}, {"heading": "A. Notation", "text": "The multimodal dataset consists of two parts, the training set and the test set. Likewise, the training data is referred to as Dtr = {Itr} ntrp = 1, with the training data as Ttr = {Itp} ntrp = 1, which has the same number of instances for training. In addition, the training data also has its corresponding semantic category labels, which are called {cIp} ntr = 1 and {cTp} ntrp = 1. Then the test sets are referred to as Dte = {Ite} n."}, {"heading": "C. Text Semantic Space Measurement", "text": "In fact, it is the case that it is a matter of a way in which people are able to determine for themselves what they want and what they want. (...) It is not the case that people are able to identify themselves. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" It is as if. \"(...\" It is as if. \"It is.\" It is as if. \"(...).\" It is as if. \"It is. (...\" It is as if. \"It is.\" It is. \"It is as if.\" It is. \"(...\" It is as if. \"It is.\" It is. \"It is.\" (... \"It is.\" It is as if. \"It is.\" It is as if. \"(...\" It is. \"It is.\" It is. \""}, {"heading": "D. Adaptive Fusion on Different Semantic Spaces", "text": "Inspired by [44], we continue to explore the complementarity between different semantic spaces through adaptive fusion. First, the cross-modal similarity values min-max obtained from different semantic spaces are normalized to [0, 1] or min-max, respectively, which aims to overcome the influence of image-text pairs with too large similarity values and as follow-s.ri (ip, tp) = simi (ip, tp) \u2212 min (simi (i, t) \u2212 min (simi (i, t))), (20) rt (ip, tp) = simt (ip, tp) = simt (ip, tp) \u2212 min (simt (i, t) \u2212 simt (i, t)."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we conduct experiments on three cross-modal datasets, including the widely used Wikipedia and Pascal Sentence datasets, as well as our constructed large-scale XMediaNet dataset, using 9 state-of-the-art methods for comparison to verify the effectiveness of our proposed approach, and presenting comprehensive experimental analyses, including convergence and parameter analysis, as well as baseline experiments to verify the contribution of each component to our approach."}, {"heading": "A. Datasets", "text": "Here we briefly introduce 3 crossmodal datasets used in the experiments, including Wikipedia, Pascal Sentence, and our constructed large format XMediaNet datasets. Each dataset is divided into 3 subsets, namely training sets, test sets, and validation sets. \u2022 Wikipedia dataset [4], as the most commonly used dataset for intermodal retrieval, is selected from \"featured articles\" in Wikipedia2 with 10 most populated categories, including history, biology, and so on. This dataset consists of a total of 2,866 image / text pairs. For a fair and objective comparison, we closely follow the dataset sharing strategy of [13], [14] to divide the datasets into 3 subsets: 2,173 pairs in the training set, 231 pairs in the validation set, and 462 pairs in the test set. \u2022 Pascal Sentence dataset [45] contains datasets, 1 images that will be generated from the 2008 CASPAL development set."}, {"heading": "B. Details of the Network", "text": "And we3http: / / wordnet.princeton.edu / 4http: / / www.flickr.com 5http: / / torch.ch / presents the details of the network including the data preprocessing strategy and network structure as follows. 1) Data preprocessing: For the image, we use the original images in the size of 256 x 256 as inputs. For the text, we convert each word in a document into a 300-dimensional vector of Word2Vec [42] and generate vector sequences as text inputs. The maximum input length is set as the maximum sequence length in the record, and we use zero padding for other sequences below this limit. 2) Recurrent attention network: The recurrent attention network of each semantic space is as three parts, namely convolutional network, long short-term memory (LSTM) and attention network."}, {"heading": "C. Compared Methods", "text": "A total of 9 state-of-the-art methods are compared in the experiments to verify the effectiveness of our proposed approach \u2022 There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47]. While the other 4 methods, Corr-AE [14], DCCA [15], CMDN [13] and Deep-SM [31] are DNN-based methods, the brief introductions of these 9 comparative methods are presented as follows: \u2022 CCA [18] learns project matrices to maximize the correlation between the projected features of different modalities in a common space. \u2022 CFA [21] minimizes the Frobenius standard between the data of different modalities after they have been projected into a common space. \u2022 KCCM uses the kernel function to apply DCM-based characteristics in a higher modal space, CEL-31, and then we learn a common space of DFL. \u2022 KCCMA uses the kernel function to project DCM-13, namely, to project the characteristics of a higher modal space of CEL-31, and then a common space of FL."}, {"heading": "D. Evaluation Metric", "text": "We perform cross-modal retrieval on 3 datasets mentioned above using two types of retrieval tasks that are defined as follows. \u2022 Image Query Text (Image \u2192 Text). Retrieving relevant text instances in the tests set by computed cross-modal similarity, using a query of text. It should be noted that our proposed MCSM approach integrates both common representation learning and distance metrics, which takes original image and text as inputs to directly generate the cross-modal similarity score, while other methods only learn the common representation using handcrafted functions as input. Therefore, all comparison methods of the same CNN functions should be used as input in our proposed approach."}, {"heading": "E. Comparisons with 9 State-of-the-art Methods", "text": "In this part, we present experimental results on our proposed path as well as all the methods compared. Tables I to III show the MAP results of two retrieval tasks and their average results on 3 datasets. We can see that our proposed approach has achieved the best retrieval accuracy compared to other methods. As shown in the table, our proposed approach has improved the average MAP rate from 0.457 to 0.487 percent of the methods mentioned. On the one hand, LGCFL achieves the best retrieval accuracy, which is closer to the CMDN."}, {"heading": "F. Convergence and Parameter Analysis", "text": "First, we conduct convergence experiments with the relatively large XMediaNet dataset, and the curve of the downward trend to the loss value is shown in Figure 8. We can observe that our proposed approach converges within 15K iterations on the XMediaNet dataset on a relatively large scale, demonstrating its efficiency in the training phase. Then, we also conduct parameter experiments on the impact of key parameters, including the learning and margin parameters \u03b1 and \u03b2 in the equations (12), (13), (18) and (19) that are implemented on all three datasets. For the learning rate, we range the value from 1e-2 to 1e-5, and the results are shown in Figure 9, from which we can see that our proposed approach achieves the best accuracy at the learning rate of 1e-4 on all 3 datasets, and the accuracy becomes lower at a higher learning rate. Then, for the margin parameter, we can determine that the margin parameters are equal to 0.0 for each of the results and 0.0 for each of the parameters."}, {"heading": "G. Baseline Comparisons", "text": "In this part, we will conduct two baseline experiments to verify the separate contribution of each component in our proposed MCSM approach. Tables IV and V show the accuracy of our proposed MCSM approach, as well as the baseline approaches in the following two areas. MCSM text means that it is only able to reactivate in the region."}, {"heading": "V. CONCLUSION", "text": "In this paper, we have proposed a modality-specific approach to measuring modality-specific similarity in order to construct independent semantic spaces for different modalities. Firstly, a recurring attention network with attention-based common embedding losses is used to fully model modality-specific characteristics within each modality and to exploit the unbalanced and complementary relationships between different modalities in correlation learning. Secondly, the end-to-end frameworks are implemented in different semantic spaces to directly generate the modality-specific similarity and integrate both common representation learning and distance metric learning for mutual benefit. Thirdly, adaptive fusion is used to explore the complementarity between different semantic spaces. Experiments on three modal datasets, including the widely used Wikipedia and Pascal sentence datasets, as well as our constructed large-scale verified effectiveness of our advanced XNet data set are proposed to verify the effectiveness of our modern data9."}], "references": [{"title": "Neighborhood discriminant hashing for large-scale image retrieval", "author": ["J. Tang", "Z. Li", "M. Wang", "R. Zhao"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 24, no. 9, pp. 2827\u20132840, 2015.  IEEE TRANSACTIONS ON IMAGE PROCESSING  13", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Clip-based similarity measure for querydependent clip retrieval and video summarization", "author": ["Y. Peng", "C.-W. Ngo"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 16, no. 5, pp. 612\u2013627, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "An overview of cross-media retrieval: Concepts, methodologies, benchmarks and challenges", "author": ["Y. Peng", "X. Huang", "Y. Zhao"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACM International Conference on Multimedia (ACM-MM), 2010, pp. 251\u2013260.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning discriminative binary codes for large-scale cross-modal retrieval", "author": ["X. Xu", "F. Shen", "Y. Yang", "H.T. Shen", "X. Li"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 26, no. 5, pp. 2494\u20132507, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Multimodal discriminative binary embedding for large-scale cross-modal retrieval", "author": ["D. Wang", "X. Gao", "X. Wang", "L. He", "B. Yuan"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 25, no. 10, pp. 4540\u2013 4554, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-modal subspace learning via pairwise constraints", "author": ["R. He", "M. Zhang", "L. Wang", "Y. Ji", "Q. Yin"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 24, no. 12, pp. 5543\u20135556, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-view embedding space for modeling internet images, tags, and their semantics", "author": ["Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik"], "venue": "International Journal of Computer Vision (IJCV), vol. 106, no. 2, pp. 210\u2013233, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning cross-media joint representation with sparse and semi-supervised regularization", "author": ["X. Zhai", "Y. Peng", "J. Xiao"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 24, pp. 965\u2013 978, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint feature selection and subspace learning for cross-modal retrieval", "author": ["K. Wang", "R. He", "L. Wang", "W. Wang", "T. Tan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 38, no. 10, pp. 2010\u20132023, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Cross-modal learning to rank via latent joint representation", "author": ["F. Wu", "X. Jiang", "X. Li", "S. Tang", "W. Lu", "Z. Zhang", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 24, no. 5, pp. 1497\u20131509, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning of multimodal representations with random walks on the click graph", "author": ["F. Wu", "X. Lu", "J. Song", "S. Yan", "Z.M. Zhang", "Y. Rui", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 25, no. 2, pp. 630\u2013642, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-media shared representation by hierarchical learning with multiple deep networks", "author": ["Y. Peng", "X. Huang", "J. Qi"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), 2016, pp. 3846\u20133853.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACM International Conference on Multimedia (ACM- MM), 2014, pp. 7\u201316.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep correlation for matching images and text", "author": ["F. Yan", "K. Mikolajczyk"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3441\u20133450.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 2048\u20132057.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2016, pp. 289\u2013297.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, pp. 321\u2013377, 1936.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1936}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J.C. Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 36, no. 3, pp. 521\u2013 535, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-label cross-modal retrieval", "author": ["V. Ranjan", "N. Rasiwasia", "C.V. Jawahar"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015, pp. 4094\u20134102.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimedia content processing through cross-modal association", "author": ["D. Li", "N. Dimitrova", "M. Li", "I.K. Sethi"], "venue": "ACM International Conference on Multimedia (ACM-MM), 2003, pp. 604\u2013611.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Regularization and semisupervised learning on large graphs", "author": ["M. Belkin", "I. Matveeva", "P. Niyogi"], "venue": "Annual Conference on Learning Theory (COLT), 2004, pp. 624\u2013638.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Heterogeneous metric learning with joint graph regularization for cross-media retrieval", "author": ["X. Zhai", "Y. Peng", "J. Xiao"], "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2013, pp. 1198\u20131204.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised crossmedia feature learning with unified patch graph regularization", "author": ["Y. Peng", "X. Zhai", "Y. Zhao", "X. Huang"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 26, no. 3, pp. 583\u2013596, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2012, pp. 1106\u20131114.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-stream multi-class fusion of deep networks for video classification", "author": ["Z. Wu", "Y. Jiang", "X. Wang", "H. Ye", "X. Xue"], "venue": "ACM International Conference on Multimedia (ACM-MM), 2016, pp. 791\u2013800.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML), 2011, pp. 689\u2013696.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning representations for multimodal data with deep belief nets", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "International Conference on Machine Learning (ICML) Workshop, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J.A. Bilmes", "K. Livescu"], "venue": "International Conference on Machine Learning (ICML), 2013, pp. 1247\u20131255.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-modal retrieval with CNN visual features: A new baseline", "author": ["Y. Wei", "Y. Zhao", "C. Lu", "S. Wei", "L. Liu", "Z. Zhu", "S. Yan"], "venue": "IEEE Transactions on Cybernetics (TCYB), vol. 47, no. 2, pp. 449\u2013460, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Cross-modal retrieval via deep and bidirectional representation learning", "author": ["Y. He", "S. Xiang", "C. Kang", "J. Wang", "C. Pan"], "venue": "IEEE Transactions on Multimedia (TMM), vol. 18, no. 7, pp. 1363\u20131377, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2014, pp. 2204\u20132212.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 1462\u20131471.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 21\u201329.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Kocisk\u00fd", "P. Blunsom"], "venue": "International Conference on Learning Representations (ICLR), 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2015, pp. 1693\u20131701.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015, pp. 379\u2013 389.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR), 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-driven visual saliency and attention-based visual question answering", "author": ["Y. Lin", "Z. Pang", "D. Wang", "Y. Zhuang"], "venue": "arXiv preprint arXiv:1702.06700, 2017.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1997}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2013, pp. 3111\u20133119.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1746\u20131751.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Coarse2fine: Two-layer fusion for image retrieval", "author": ["G. Kong", "L. Dong", "W. Dong", "L. Zheng", "Q. Tian"], "venue": "arXiv preprint arXiv:1607.00719, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, 2010, pp. 139\u2013147.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedm\u00e1k", "J. Shawe-Taylor"], "venue": "Neural Computation, vol. 16, no. 12, pp. 2639\u20132664, 2004.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning consistent feature representation for cross-modal multimedia retrieval", "author": ["C. Kang", "S. Xiang", "S. Liao", "C. Xu", "C. Pan"], "venue": "IEEE Transactions on Multimedia (TMM), vol. 17, no. 3, pp. 370\u2013381, 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Different from the traditional single-modal retrieval, such as image retrieval [1] and video retrieval [2], which is limited in providing", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "Different from the traditional single-modal retrieval, such as image retrieval [1] and video retrieval [2], which is limited in providing", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "retrieval results of the same single modality with query, cross-modal retrieval is more flexible and useful to retrieve relevant multimodal information by submitting one query of any modality [3].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "Such methods like [13], [14], [15] attempt to exploit the advantages of DNN in modeling nonlinear correlation to learn common representation with multilayer network.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "Such methods like [13], [14], [15] attempt to exploit the advantages of DNN in modeling nonlinear correlation to learn common representation with multilayer network.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "Such methods like [13], [14], [15] attempt to exploit the advantages of DNN in modeling nonlinear correlation to learn common representation with multilayer network.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Recently, attention mechanism has made great advances in DNN, which allows models to concentrate on the necessary fine-grained parts of visual or textual inputs, and has been successfully applied to various multimodal tasks, such as image caption [16] and visual question answering", "startOffset": 247, "endOffset": 251}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "As indicated in [3], we mainly introduce three categories of existing methods as follows, namely traditional statistical correlation analysis methods, cross-modal graph regularization methods and DNN-based methods.", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "Canonical Correlation Analysis (CCA) [18], as one of the most representative works, is a natural solution to maximize the pairwise correlation between the data of different modalities such as image/text pairs [4].", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "Canonical Correlation Analysis (CCA) [18], as one of the most representative works, is a natural solution to maximize the pairwise correlation between the data of different modalities such as image/text pairs [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 18, "context": "[19] integrate semantic category labels to improve the performance of CCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Multi-view CCA [8] is proposed to construct a third view for modeling the high-level semantics.", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "[20] propose multi-label CCA, which considers the high-level semantic information in the form of multi-label annotations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Besides, Cross-modal Factor Analysis (CFA) [21], as one of the alternative methods, is proposed to minimize the Frobenius norm between the data of different modalities after projecting them into one common space.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "2) Cross-modal graph regularization methods: Graph regularization [22] is widely used to construct a partially labeled graph for semi-supervised learning, which aims to enrich the training set and smooth the solution.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "[23] are the first to integrate graph regularization into cross-modal retrieval and propose Joint Graph Regularized Heterogeneous Metric Learning (JGRHML), which constructs the joint graph regularization term in the learned metric space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Furthermore, Joint Representation Learning (JRL) [9] is proposed to con-", "startOffset": 49, "endOffset": 52}, {"referenceID": 23, "context": "[24] further improve the previous works [9], [23] by constructing a unified hypergraph to learn the common space for up to five modalities, which also utilizes the fine-grained information at the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[24] further improve the previous works [9], [23] by constructing a unified hypergraph to learn the common space for up to five modalities, which also utilizes the fine-grained information at the same time.", "startOffset": 40, "endOffset": 43}, {"referenceID": 22, "context": "[24] further improve the previous works [9], [23] by constructing a unified hypergraph to learn the common space for up to five modalities, which also utilizes the fine-grained information at the same time.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "[10] also adopt multimodal graph regularization term to preserve inter-modality and intra-modality similarity relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "3) DNN-based methods: Deep learning has made great advance in multimodal applications, such as image/video classification [25], [26] and object recognition [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 25, "context": "3) DNN-based methods: Deep learning has made great advance in multimodal applications, such as image/video classification [25], [26] and object recognition [27].", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "3) DNN-based methods: Deep learning has made great advance in multimodal applications, such as image/video classification [25], [26] and object recognition [27].", "startOffset": 156, "endOffset": 160}, {"referenceID": 27, "context": "Bimodal Autoencoders (Bimodal AE) [28] is proposed as an extension of Restricted Boltzmann Machine (RBM) to model multiple modalities by minimizing the reconstruction error.", "startOffset": 34, "endOffset": 38}, {"referenceID": 28, "context": "[29] propose Multimodal Deep Belief Network (Multimodal DBN), which adopts two kinds of DBN for different modalities to model the distribution over their original features, while a joint RBM is adopted on the top of them to model the joint distribution and get the common representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Correspondence Autoencoder (Corr-AE) [14] and Deep Canonical Correlation Analysis (DCCA) [30] also consist of two subnetworks, while Corr-AE jointly models correlation and reconstruction information, and DCCA combines DNN with CCA to maximize the correlation on the top of two subnetworks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "Correspondence Autoencoder (Corr-AE) [14] and Deep Canonical Correlation Analysis (DCCA) [30] also consist of two subnetworks, while Corr-AE jointly models correlation and reconstruction information, and DCCA combines DNN with CCA to maximize the correlation on the top of two subnetworks.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "[13] propose Cross-media Multiple Deep Networks (CMDN) to model the intra-modality and inter-modality correlation in a two-stage learning framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] propose deep-SM to perform deep semantic matching with Convolutional Neural Network (CNN), which exploits the", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] adopt two convolution-based networks to model the matched and mismatched image/text pairs via deep and bidirectional representation learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] propose an attention based task-driven visual processing framework for image classification, which adopts Recurrent Neural Network (RNN) to adaptively select a sequence of regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] propose a spatial attention mechanism, which designs a sequential variational auto-encoding framework to perform image generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] propose Stacked Attention Networks (SANs) for image question answering, which can locate relevant image regions to the question with stacked attention model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] propose a word-by-word neural attention mechanism to reason over entailments of paired words or phrases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] develop a class of attention based deep neural networks, which learn to read and answer", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] propose a fully datadriven approach, which adopts a local attention based model to generate summarization according to the input sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Specifically, the network structure is configured the same as the layers before the last pooling layer (pool5) in 19-layer VGGNet [39].", "startOffset": 130, "endOffset": 134}, {"referenceID": 39, "context": "We compose these regions as a sequence, which are regarded as the results of eye movement when we glance at the image [40].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "Specifically, Long Short Term Memory (LSTM) network [41] is adopted, which is a special kind of RNN, with strong ability to learn long-term dependencies through the memory cell and the update gates as well as preserve previous time-steps information at the same time.", "startOffset": 52, "endOffset": 56}, {"referenceID": 41, "context": "Each word is represented by a k-dimensional vector extracted by Word2Vec [42] model, which is trained on billions of words in Google News and publicly available1.", "startOffset": 73, "endOffset": 77}, {"referenceID": 42, "context": "And a Word CNN is adopted on the input matrix, which has the same configuration with [43].", "startOffset": 85, "endOffset": 89}, {"referenceID": 41, "context": "First, the input text instance tp, which consists of n words, is represented as an n\u00d7k matrix, where each word is also represented as a k-dimensional vector extracted by Word2Vec [42] model.", "startOffset": 179, "endOffset": 183}, {"referenceID": 42, "context": "Following [43], we design fast convolutional networks for text (Word CNN), which is built by several combinations of convolution layer, threshold activation function layer and pooling layer.", "startOffset": 10, "endOffset": 14}, {"referenceID": 40, "context": "Specifically, we also adopt LSTM network [41] to exploit such temporal dependency, which takes a sequence of text fragments as input.", "startOffset": 41, "endOffset": 45}, {"referenceID": 38, "context": "We still need to generate the image representation for each instance ip, which is also extracted from the last fully connected layer (fc7) in 19-layer VGGNet [39] with 4,096 dimensional number, and denoted as q p.", "startOffset": 158, "endOffset": 162}, {"referenceID": 43, "context": "Inspired by [44], we further attempt to explore the complementarity between different semantic spaces by adaptive fusion.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "ent semantic spaces are min-max normalized to [0, 1] respectively, which aims to overcome the influence of image/text pairs with too large similarity scores, and are defined as follows.", "startOffset": 46, "endOffset": 52}, {"referenceID": 3, "context": "\u2022 Wikipedia dataset [4], as the most widely-used dataset for cross-modal retrieval, is selected from \u201cfeatured articles\u201d in Wikipedia2 with 10 most populated categories, including history, biology and so on.", "startOffset": 20, "endOffset": 23}, {"referenceID": 12, "context": "For fair and objective comparison purpose, we exactly follow the dataset partition strategy of [13], [14] to divide the dataset into 3 subsets: 2,173 pairs in training set, 231 pairs in validation set and 462 pairs in testing set.", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "For fair and objective comparison purpose, we exactly follow the dataset partition strategy of [13], [14] to divide the dataset into 3 subsets: 2,173 pairs in training set, 231 pairs in validation set and 462 pairs in testing set.", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "\u2022 Pascal Sentence dataset [45] contains 1,000 images,", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "This dataset is categorized into 20 categories, and also following [13], [14], 800 documents are selected as training set, while 100 documents for testing and 100 documents for validation.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "This dataset is categorized into 20 categories, and also following [13], [14], 800 documents are selected as training set, while 100 documents for testing and 100 documents for validation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "For text, we convert each word in a document into a 300-dimension vector by Word2Vec [42] and generate vector sequences as text inputs.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 76, "endOffset": 80}, {"referenceID": 45, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 97, "endOffset": 100}, {"referenceID": 46, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "While the other 4 methods, Corr-AE [14], DCCA [15], CMDN", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "While the other 4 methods, Corr-AE [14], DCCA [15], CMDN", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "[13] and Deep-SM [31] are DNN-based methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[13] and Deep-SM [31] are DNN-based methods.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "\u2022 CCA [18] learns project matrices to maximize the correlation between the projected features of different modalities in one common space.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "\u2022 CFA [21] minimizes the Frobenius norm between the data of different modalities after projecting them into one", "startOffset": 6, "endOffset": 10}, {"referenceID": 45, "context": "\u2022 KCCA [46] uses kernel function to project the features into a higher-dimensional space, and then learns a common space by CCA.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "\u2022 JRL [9] learns a common space by using semantic information, with semi-supervised regularization and sparse regularization.", "startOffset": 6, "endOffset": 9}, {"referenceID": 46, "context": "\u2022 LGCFL [47] jointly learns basis matrices of different modalities, by using a local group based priori in the", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "\u2022 Corr-AE [14] consists of two autoencoder networks coupled at the code layer to simultaneously model the reconstruction error and correlation loss.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "\u2022 DCCA [15] is a nonlinear extension of CCA.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "\u2022 CMDN [13] adopts multiple deep networks to generate separate representations and learns the common representation with a stacked network.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "\u2022 Deep-SM [31] adopts convolutional neural network to perform deep semantic matching, which fully exploits the strong power of CNN image feature.", "startOffset": 10, "endOffset": 14}, {"referenceID": 38, "context": "of image is extracted from fc7 layer in 19-layer VGGNet [39], while CNN feature of text is extracted by Word CNN with the same configuration of [43].", "startOffset": 56, "endOffset": 60}, {"referenceID": 42, "context": "of image is extracted from fc7 layer in 19-layer VGGNet [39], while CNN feature of text is extracted by Word CNN with the same configuration of [43].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "MAP score considers the ranking of returned retrieval results as well as precision simultaneously, which is extensively adopted in cross-modal retrieval tasks, such as [4], [13].", "startOffset": 168, "endOffset": 171}, {"referenceID": 12, "context": "MAP score considers the ranking of returned retrieval results as well as precision simultaneously, which is extensively adopted in cross-modal retrieval tasks, such as [4], [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 13, "context": "It should be noted that the MAP score is calculated on all returned results, not only top 50 as adopted in [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "487 CMDN [13] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "457 Deep-SM [31] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "450 LGCFL [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "449 JRL [9] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "454 DCCA [15] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "422 Corr-AE [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "436 KCCA [46] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "414 CFA [21] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "318 CCA [18] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "598 CMDN [13] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "535 Deep-SM [31] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "550 LGCFL [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "521 JRL [9] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "534 DCCA [15] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "539 Corr-AE [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "527 KCCA [46] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "467 CFA [21] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "473 CCA [18] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "545 CMDN [13] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "501 Deep-SM [31] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "371 LGCFL [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "475 JRL [9] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "447 DCCA [15] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "429 Corr-AE [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "488 KCCA [46] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "261 CFA [21] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "326 CCA [18] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "Examples of the cross-modal retrieval results on XMediaNet dataset by our proposed MCSM approach as well as compared method CMDN [13].", "startOffset": 129, "endOffset": 133}], "year": 2017, "abstractText": "Nowadays, cross-modal retrieval plays an indispensable role to flexibly find information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval. Different modalities such as image and text have imbalanced and complementary relationships, which contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa. Existing works based on Deep Neural Network (DNN) mostly construct one common space for different modalities to find the latent alignments between them, which lose their exclusive modality-specific characteristics. Different from the existing works, we propose modality-specific cross-modal similarity measurement (MCSM) approach by constructing independent semantic space for each modality, which adopts end-to-end framework to directly generate modalityspecific cross-modal similarity without explicit common representation. For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding to utilize the learned attention weights for guiding the fine-grained crossmodal correlation learning, which can capture the imbalanced and complementary relationships between different modalities. Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modalityspecific cross-modal similarities to perform cross-modal retrieval. Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset verify the effectiveness of our proposed approach, outperforming 9 state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}