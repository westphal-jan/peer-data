{"id": "1708.04782", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "StarCraft II: A New Challenge for Reinforcement Learning", "abstract": "This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.", "histories": [["v1", "Wed, 16 Aug 2017 06:20:52 GMT  (3058kb,D)", "http://arxiv.org/abs/1708.04782v1", "Collaboration between DeepMind &amp; Blizzard. 20 pages, 9 figures, 2 tables"]], "COMMENTS": "Collaboration between DeepMind &amp; Blizzard. 20 pages, 9 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["oriol vinyals", "timo ewalds", "sergey bartunov", "petko georgiev", "alexander sasha vezhnevets", "michelle yeo", "alireza makhzani", "heinrich k\\\"uttler", "john agapiou", "julian schrittwieser", "john quan", "stephen gaffney", "stig petersen", "karen simonyan", "tom schaul", "hado van hasselt", "david silver", "timothy lillicrap", "kevin calderone", "paul keet", "anthony brunasso", "david lawrence", "ers ekermo", "jacob repp", "rodney tsing"], "accepted": false, "id": "1708.04782"}, "pdf": {"name": "1708.04782.pdf", "metadata": {"source": "CRF", "title": "StarCraft II: A New Challenge for Reinforcement Learning", "authors": ["Oriol Vinyals", "Timo Ewalds", "Sergey Bartunov", "Petko Georgiev", "Alexander Sasha Vezhnevets", "Michelle Yeo", "Alireza Makhzani", "Heinrich K\u00fcttler", "John Agapiou", "Julian Schrittwieser", "John Quan", "Stephen Gaffney", "Stig Petersen", "Karen Simonyan", "Tom Schaul", "Hado van Hasselt", "David Silver", "Timothy Lillicrap", "DeepMind Kevin Calderone", "Paul Keet", "Anthony Brunasso", "David Lawrence"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "Computer games offer a compelling solution to the problem of evaluating and comparing different learning and planning approaches to standardized tasks and are an important source of research into artificial intelligence (AI). These games offer several advantages: 1. They have clear objective measures of success; 2. Computer games typically provide rich streams of observation data that are ideal inputs for deep networks; 3. They are externally defined to be difficult and interesting for a human being, ensuring that the challenge itself is not adjusted by the researcher to make the problem easier for the algorithms developed; 4. Games are designed to run anywhere with the same interface and game dynamics, making it easy to share a challenge."}, {"heading": "3 The SC2LE Environment", "text": "The main contribution of our paper is the release of SC2LE, which debunks StarCraft II as a research environment. It consists of three sub-components: a binary Linux version of StarCraft II, the StarCraft II API, and PySC2 (see Figure 1).3https: / / github.com / santiontanon / micrortsThe StarCraft II API4 allows programmatic control of StarCraft II. The API can be used to start a game, receive observations, take action, and replay. This normal game API is available on Windows and Mac OS, but we also offer a limited headless build that runs on Linux, especially for machine learning and distributed use cases. With this API, we have built PySC25, an open source environment optimized for RL agents. PySC2 is a Python environment that envelops the StarCraft II API to facilitate the learning interaction between crafts and StarCraft II."}, {"heading": "3.1 Full Game Description and Reward Structure", "text": "In the full 1v1 game of StarCraft II, two opponents walk on a map that contains resources and other elements such as ramps, bottlenecks and islands. To win a game, a player must: 1. Accumulate resources (minerals and vespengas), 2. Construct production buildings, 3. Amass an army, and 4. Clear all the buildings of the opponent. A game typically lasts from a few minutes to an hour, and early actions in the game (such as which buildings and units are built) have long-term consequences. Players have imperfect information as they see the part of the map in which they have units."}, {"heading": "3.2 Observations", "text": "StarCraft II uses a game engine that renders graphics in 3D. While using the underlying game engine that simulates the entire environment, the StarCraft II API is currently unable to render RGB pixels. Rather, it generates a series of \"feature layers\" that abstract from the RGB images seen4https: / / github.com / s2client-proto 5https: / / github.com / deepmind / pysc2during human play, using the central spatial and graphical concepts of StarCraft II (see Figure 2).Thus, the most important observations come as a set of feature layers rendered to N \u00d7 M pixels (where N and M are configurable, although we always use N = M in our experiments) to represent something specific in the game, for example: unit types, meeting points, owner or visibility."}, {"heading": "3.3 Actions", "text": "We designed the action space to mimic the human interface as closely as possible while maintaining some of the conventions used in other RL environments, such as Atari [4]. Figure 3 shows a brief sequence of actions produced by a player and an agent. Many basic in-game maneuvers are composite actions. Instead of asking the agents to generate these 3 keys / mouse clicks as a sequence of three separate actions that we give them as an atomic composition, they chose the screen, then clicked on a dot on the screen or a mini-map to perform the action."}, {"heading": "3.4 Mini-Games Task Description", "text": "In order to examine elements of the game in isolation, and to provide further fine-grained steps toward the full game, we built several mini-games. These are scenarios based on small cards that were constructed with the purpose of testing a subset of actions and / or game mechanics with a clear reward structure. Unlike the full game, in which the reward is only win / lose / tie, the reward7https: / github.com / deepmind / pysc2 / blob / master / bin / play.pystructure for mini-games requires certain behaviors to reward (as defined in a corresponding.SC2Map file).We encourage the community to build modifications or new mini-games with the powerful StarCraft Map Editor. This allows more than just designing a wide range of minor challenges domains. It allows the exchange of identical setups and ratings with other researchers and ratings."}, {"heading": "3.5 Raw API", "text": "StarCraft II also has a raw API that is similar to the Broodwar API (BWAPI [1]), but without any visual component. In this case, the observations are a list of all visible units on the map, along with the characteristics (unit type, owner, coordinates, health, etc.), but without any visual component. Fog of War still exists, but there is no camera, so you can see all visible units at the same time. This is a simpler and more precise representation, but it does not correspond to the human perception of the game. For comparison against humans, this is called \"cheating,\" as it provides significant additional information.With the raw API, action controllers or groups of units can be selected individually by a unit identifier. There is no need to select individuals or groups of units before action is taken. This allows for much more accurate actions than the human interface allows SCagi, and thus gives the possibility of superhuman behavior over these API, while we do not have RoPI based on the API being used in other cases."}, {"heading": "3.6 Performance", "text": "Observations are made at a speed that depends on several factors: the complexity of the map, the screen resolution, the number of unrendered frames per action, and the number of threads. For complex maps (e.g. full ladder maps), the calculation is dominated by the simulation speed. Measures that are taken less often and allow less rendered frames per action reduce the calculation, but decreasing yields occur relatively quickly, which means that there is little gain over 8 steps per action. Since little time is spent on rendering, higher resolution does not hurt. More instances in parallel threads run quite well. For simpler maps (e.g. CollectMineralShards), the world simulation is fast, so that rendering the observations dominates. In this case, increasing the frames per action and decreasing the resolution can have a big effect than decreasing the resolution per threads per second, which results in approximately 8 \u00d7 64 interprethreads per second, and 64 \u00d7 64 + 64 + 1 interprethreads per second."}, {"heading": "4 Reinforcement Learning: Baseline Agents", "text": "This section provides basic results to calibrate the card difficulty and shows that established RL algorithms can learn useful guidelines, at least in the minigames, but also that many challenges remain. In addition, we provide points for the minigames for two human players: a DeepMind game tester (beginner) and a StarCraft Grandmaster (pro level) (see Table 1).8https: / / github.com / Blizzard / s2client-api 9https: / / github.com / davechurchill / CommandCenter"}, {"heading": "4.1 Learning Algorithm", "text": "The objective of the agent is to maximize the return, e.g. by using a hidden memory state as described below. [21] The parameters of the policy are learned using Asynchronous Advantage Actor Critic (A3C), as demonstrated by Mnih et al. [21], which has shown that the return can be limited to all previous states, e.g. by a hidden memory state as described below. Policy parameters are learned using Asynchronous Advantage Actor Critic (A3C), as demonstrated by Mnih et al. [21], which has shown that the results are state-of-the-art. Gradient of the gradient in different environments. A3C is a political gradient method, which represents an approximate gradient Actor Critic (A3C), as demonstrated by Mnih et al. [21], which has shown that the results are based on the state of the art."}, {"heading": "4.2 Policy Representation", "text": "As described in Section 3, the API exposes actions as a nested list a containing a function name a0 and a set of arguments. Since all arguments, including on-screen pixel coordinates and the minimap, are discrete, naive parameterization of a policy would require 10: 2 million values to determine the common distribution over a, even at low spatial resolution. (2) This representation, if implemented efficiently, is probably easier, since it transforms the problem of selecting a complete action into a sequence of decisions for each argument. In the simple RL baselines reported here, we do a further simplification and use policies that select the function name a0, and all arguments, independently, so."}, {"heading": "4.3 Agent Architectures", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think they are able to survive themselves. \""}, {"heading": "4.4 Results", "text": "The optimization process runs with 64 asynchronous threads using Shared RMS Prop. For each method, we conducted 100 experiments, each using randomly sampled hyperparameters. We use an independent entropy penalty of 10 \u2212 3 for the action function and each action function argument. We trade at a fixed rate every 8 game steps, equivalent to about three actions per second or 180 APM. All experiments were conducted for 600 million steps (or 8 x 600 million game steps).4.1 Full gameFor experiments on the full game, we chose the Abyssal Reef LE card, which was used in online game rankings as well as in professional strategies."}, {"heading": "4.4.2 Mini-Games", "text": "As described in Section 3, you can avoid the complexity of the entire game by defining a series of minigames that focus on specific aspects of the game (see Section 3 for a high-level description of each minigame).The aggregated training results are shown in Figure 6, and the final results with comparisons to human fundamentals can be found in Table 1. A video that introduces our agents is also available at https: / / youtu.be / 6L448yg0Sm0.Overall, fully revolutionary agents performed best in the non-human fundamentals. Slightly surprisingly, the Atari net agent appeared to be a fairly strong competitor in minigames that involve combat, namely FindAndDefeat Zerlings, Defeat Roaches and Defeat ZerlingsAndBanelings. On CollectMineralsAndGas, only the best conventional agents could increase initial resource revenues by producing more units and assigning them to the game."}, {"heading": "5 Supervised Learning from Replays", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5.1 Value Predictions", "text": "The result is a challenging task. Even professional StarCraft II commentators cannot predict the winner despite full access to the game savegame (i.e. they are not restricted by partial observation).The underlying features are desirable because they can solve the challenge of learning from within the game. A well-trained value can emerge from the given state that does not take into account previous observations."}, {"heading": "5.2 Policy Predictions", "text": "The same network that was developed to predict values was designed to predict actions by the user. We refer to this part of the network as the policy, as it can easily be used to play the game. There are many measures that could be applied to imitate networks. We use a simple approach that connects directly to the work of the RL."}, {"heading": "6 Conclusions & Future Work", "text": "This paper presents StarCraft II as a new challenge for a profound reinforcement of learning research. \u2022 We provide details for a freely available Python interface to play the game, as well as human replay data from games collected through Blizzard's official BattleNet ladder. \u2022 With this initial release, we describe monitored learning outcomes based on human replay data for politics and value networks. We also describe results for manageable RL agents on seven mini-games and across the game. We view the minigames primarily as unit tests. That is, an RL agent should be able to achieve human performance on them with relative ease if he is to have a chance of succeeding on the full game. It may be instructive to build additional minigames, but we rate the end result - as the most interesting problem, and hope that it primarily promotes the research that leads to his solution."}, {"heading": "Acknowledgements", "text": "We would like to thank many of Blizzard, especially Tommy Tran, Tyler Plass, Brian Song, Tom van Dijck, and Greg Risselada, the Grandmaster; we would also like to thank the DeepMind team, especially Nal Kalchbrenner, Ali Eslami, Jamey Stevenson, Adam Cain, and our esteemed game testers Amir Sadik & Sarah York; and David Churchill for his early feedback on the Raw API, for building the CommandCenter, and for comments on the manuscript."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and autonomous systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Playing SNES in the retro learning environment", "author": ["Nadav Bhonker", "Shai Rozenberg", "Itay Hubara"], "venue": "arXiv preprint arXiv:1611.02205,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Real-time strategy game competitions", "author": ["Michael Buro", "David Churchill"], "venue": "AI Magazine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on audio, speech, and language processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Global state evaluation in StarCraft", "author": ["Graham Kurtis Stephen Erickson", "Michael Buro"], "venue": "In AIIDE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Learning from demonstrations for real world reinforcement learning", "author": ["Todd Hester", "Matej Vecerik", "Olivier Pietquin", "Marc Lanctot", "Tom Schaul", "Bilal Piot", "Andrew Sendonaris", "Gabriel Dulac-Arnold", "Ian Osband", "John Agapiou"], "venue": "arXiv preprint arXiv:1704.03732,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "A Turing test for computer game bots", "author": ["Philip Hingston"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Classq-l: A q-learning algorithm for adversarial realtime strategy games", "author": ["Ulit Jaidee", "H\u00e9ctor Mu\u00f1oz-Avila"], "venue": "In Eighth Artificial Intelligence and Interactive Digital Entertainment Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning macromanagement in StarCraft from replays using deep learning", "author": ["Niels Justesen", "Sebastian Risi"], "venue": "arXiv preprint arXiv:1707.03743,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Vizdoom: A Doom-based AI research platform for visual reinforcement learning", "author": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Ja\u015bkowski"], "venue": "In Computational Intelligence and Games (CIG),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["Sergey Levine", "Peter Pastor", "Alex Krizhevsky", "Julian Ibarz", "Deirdre Quillen"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Move evaluation in Go using deep convolutional neural networks", "author": ["Chris J Maddison", "Aja Huang", "Ilya Sutskever", "David Silver"], "venue": "arXiv preprint arXiv:1412.6564,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "A survey of real-time strategy game AI research and competition in StarCraft", "author": ["Santiago Ontan\u00f3n", "Gabriel Synnaeve", "Alberto Uriarte", "Florian Richoux", "David Churchill", "Mike Preuss"], "venue": "IEEE Transactions on Computational Intelligence and AI in games,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games", "author": ["Peng Peng", "Quan Yuan", "Ying Wen", "Yaodong Yang", "Zhenkun Tang", "Haitao Long", "Jun Wang"], "venue": "arXiv preprint arXiv:1703.10069,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "The 2014 general video game playing competition", "author": ["Diego Perez", "Spyridon Samothrakis", "Julian Togelius", "Tom Schaul", "Simon Lucas", "Adrien Cou\u00ebtoux", "Jeyull Lee", "Chong-U Lim", "Tommy Thompson"], "venue": "Computational Intelligence and AI in Games,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Data-efficient deep reinforcement learning for dexterous manipulation", "author": ["Ivaylo Popov", "Nicolas Heess", "Timothy Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Yuval Tassa", "Tom Erez", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1704.03073,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "A review of real-time strategy game AI", "author": ["Glen Robertson", "Ian Watson"], "venue": "AI Magazine,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Ms Pac-man versus ghost team CEC 2011 competition", "author": ["Philipp Rohlfshagen", "Simon M Lucas"], "venue": "In Evolutionary Computation (CEC),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei- Fei Li"], "venue": "International Journal of Computer Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["Andrei A Rusu", "Matej Vecerik", "Thomas Roth\u00f6rl", "Nicolas Heess", "Razvan Pascanu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "A video game description language for model-based or interactive learning", "author": ["Tom Schaul"], "venue": "In Conference on Computational Intelligence in Games (IEEE-CIG),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Measuring intelligence through games", "author": ["Tom Schaul", "Julian Togelius", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1109.1314,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Torchcraft: a library for machine learning research on real-time strategy games", "author": ["Gabriel Synnaeve", "Nantas Nardelli", "Alex Auvolat", "Soumith Chintala", "Timoth\u00e9e Lacroix", "Zeming Lin", "Florian Richoux", "Nicolas Usunier"], "venue": "arXiv preprint arXiv:1611.00625,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "ELF: An extensive, lightweight and flexible research platform for real-time strategy games", "author": ["Yuandong Tian", "Qucheng Gong", "Wenling Shang", "Yuxin Wu", "Larry Zitnick"], "venue": "arXiv preprint arXiv:1707.01067,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Elf: An extensive, lightweight and flexible research platform for real-time strategy games", "author": ["Yuandong Tian", "Qucheng Gong", "Wenling Shang", "Yuxin Wu", "Larry Zitnick"], "venue": "arXiv preprint arXiv:1707.01067,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "The 2009 Mario AI competition", "author": ["Julian Togelius", "Sergey Karakovskiy", "Robin Baumgarten"], "venue": "In Evolutionary Computation (CEC),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Episodic exploration for deep deterministic policies for StarCraft micromanagement", "author": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Recent progress in areas such as speech recognition [7], computer vision [16], and natural language processing [38] can be attributed to the resurgence of deep learning [17], which provides a powerful toolkit for non-linear function approximation using neural networks.", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "Recent progress in areas such as speech recognition [7], computer vision [16], and natural language processing [38] can be attributed to the resurgence of deep learning [17], which provides a powerful toolkit for non-linear function approximation using neural networks.", "startOffset": 73, "endOffset": 77}, {"referenceID": 34, "context": "Recent progress in areas such as speech recognition [7], computer vision [16], and natural language processing [38] can be attributed to the resurgence of deep learning [17], which provides a powerful toolkit for non-linear function approximation using neural networks.", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "These techniques have also proven successful in reinforcement learning problems, yielding significant successes in Atari [20], the game of Go [32], three-dimensional virtual environments [3] and simulated robotics domains [18, 29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 28, "context": "These techniques have also proven successful in reinforcement learning problems, yielding significant successes in Atari [20], the game of Go [32], three-dimensional virtual environments [3] and simulated robotics domains [18, 29].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "These techniques have also proven successful in reinforcement learning problems, yielding significant successes in Atari [20], the game of Go [32], three-dimensional virtual environments [3] and simulated robotics domains [18, 29].", "startOffset": 222, "endOffset": 230}, {"referenceID": 25, "context": "These techniques have also proven successful in reinforcement learning problems, yielding significant successes in Atari [20], the game of Go [32], three-dimensional virtual environments [3] and simulated robotics domains [18, 29].", "startOffset": 222, "endOffset": 230}, {"referenceID": 1, "context": "Benchmarks have been critical to measuring and therefore advancing deep learning and reinforcement learning (RL) research [4, 20, 28, 8].", "startOffset": 122, "endOffset": 136}, {"referenceID": 16, "context": "Benchmarks have been critical to measuring and therefore advancing deep learning and reinforcement learning (RL) research [4, 20, 28, 8].", "startOffset": 122, "endOffset": 136}, {"referenceID": 24, "context": "Benchmarks have been critical to measuring and therefore advancing deep learning and reinforcement learning (RL) research [4, 20, 28, 8].", "startOffset": 122, "endOffset": 136}, {"referenceID": 5, "context": "Benchmarks have been critical to measuring and therefore advancing deep learning and reinforcement learning (RL) research [4, 20, 28, 8].", "startOffset": 122, "endOffset": 136}, {"referenceID": 30, "context": "Several environments [1, 34, 33] already exist for reinforcement learning in the original version of StarCraft.", "startOffset": 21, "endOffset": 32}, {"referenceID": 29, "context": "Several environments [1, 34, 33] already exist for reinforcement learning in the original version of StarCraft.", "startOffset": 21, "endOffset": 32}, {"referenceID": 1, "context": "A well known example of games driving reinforcement learning research is the Arcade Learning Environment (ALE [4]), which allows easy and replicable experiments with Atari video games.", "startOffset": 110, "endOffset": 113}, {"referenceID": 27, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 32, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 139, "endOffset": 143}, {"referenceID": 8, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 163, "endOffset": 167}, {"referenceID": 26, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 218, "endOffset": 225}, {"referenceID": 2, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 218, "endOffset": 225}, {"referenceID": 20, "context": "The ALE is a prominent example in a rich tradition of video game benchmarks for AI [31], including Super Mario [36], Ms Pac-Man [27], Doom [14], Unreal Tournament [11], as well as general video game-playing frameworks [30, 5] and competitions [24].", "startOffset": 243, "endOffset": 247}, {"referenceID": 18, "context": "[22] and Robertson & Watson [26] for an overview.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[22] and Robertson & Watson [26] for an overview.", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "The standard API for StarCraft thus far has been BWAPI [1], and related wrappers [33].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "Simplified versions of RTS games have also been developed for AI research, most notably microRTS3 or the more recent ELF [35].", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "Previous work has applied RL approaches to the Wargus RTS game with reduced state and action spaces [12], and learning based agents have also been explored in micromanagement mini-games [23, 37], and learning game outcome or build orders from replay data [9, 13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Previous work has applied RL approaches to the Wargus RTS game with reduced state and action spaces [12], and learning based agents have also been explored in micromanagement mini-games [23, 37], and learning game outcome or build orders from replay data [9, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 33, "context": "Previous work has applied RL approaches to the Wargus RTS game with reduced state and action spaces [12], and learning based agents have also been explored in micromanagement mini-games [23, 37], and learning game outcome or build orders from replay data [9, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 6, "context": "Previous work has applied RL approaches to the Wargus RTS game with reduced state and action spaces [12], and learning based agents have also been explored in micromanagement mini-games [23, 37], and learning game outcome or build orders from replay data [9, 13].", "startOffset": 255, "endOffset": 262}, {"referenceID": 10, "context": "Previous work has applied RL approaches to the Wargus RTS game with reduced state and action spaces [12], and learning based agents have also been explored in micromanagement mini-games [23, 37], and learning game outcome or build orders from replay data [9, 13].", "startOffset": 255, "endOffset": 262}, {"referenceID": 1, "context": "We designed the environment action space to mimic the human interface as closely as possible whilst maintaining some of the conventions employed in other RL environments, such as Atari [4].", "startOffset": 185, "endOffset": 188}, {"referenceID": 29, "context": "This is in contrast to other recent work [33, 23], where the game is accessed on a unit-per-unit basis and actions are individually specified to each unit.", "startOffset": 41, "endOffset": 49}, {"referenceID": 19, "context": "This is in contrast to other recent work [33, 23], where the game is accessed on a unit-per-unit basis and actions are individually specified to each unit.", "startOffset": 41, "endOffset": 49}, {"referenceID": 17, "context": "[21], which was shown to produce state-of-the-art results on a diverse set of environments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "For details we refer the reader to the original paper [21] and the references therein.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "We take established architectures from the literature [20, 21] and adapt them to fit the specifics of the environment, in particular the action space.", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "We take established architectures from the literature [20, 21] and adapt them to fit the specifics of the environment, in particular the action space.", "startOffset": 54, "endOffset": 62}, {"referenceID": 1, "context": "Atari-net Agent The first baseline is a simple adaptation of the architecture successfully used for the Atari [4] benchmark and DeepMind Lab environments [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 17, "context": "It processes screen and minimap feature layers with the same convolutional network as in [21] \u2014 two layers with 16, 32 filters of size 8, 4 and stride 4, 2 respectively.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "The use of supervised data such as replays or human demonstrations has been successful in robotics [2, 25], the game of Go [19, 32], and Atari [10].", "startOffset": 99, "endOffset": 106}, {"referenceID": 21, "context": "The use of supervised data such as replays or human demonstrations has been successful in robotics [2, 25], the game of Go [19, 32], and Atari [10].", "startOffset": 99, "endOffset": 106}, {"referenceID": 15, "context": "The use of supervised data such as replays or human demonstrations has been successful in robotics [2, 25], the game of Go [19, 32], and Atari [10].", "startOffset": 123, "endOffset": 131}, {"referenceID": 28, "context": "The use of supervised data such as replays or human demonstrations has been successful in robotics [2, 25], the game of Go [19, 32], and Atari [10].", "startOffset": 123, "endOffset": 131}, {"referenceID": 7, "context": "The use of supervised data such as replays or human demonstrations has been successful in robotics [2, 25], the game of Go [19, 32], and Atari [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": ", [13]), though not to train a policy over basic actions, but rather to discover build orders.", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "Furthermore, due to partial observability, replays could also be used to study models of uncertainty such as (but not limited to) variational autoencoders [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "This mirrors the results of prior work on StarCraft I [9].", "startOffset": 54, "endOffset": 57}], "year": 2017, "abstractText": "This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the game StarCraft II. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.", "creator": "LaTeX with hyperref package"}}}