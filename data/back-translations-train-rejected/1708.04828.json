{"id": "1708.04828", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs", "abstract": "Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete attributes for each entity. Intuitively, these attributes such as height, price or population count are able to richly characterize entities in knowledge graphs. This additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting. Specifically, we train a neural network for triplet prediction along with a separate network for attribute value regression. Via multi-task learning, we are able to learn representations of entities, relations and attributes that encode information about both tasks. Moreover, such attributes are not only central to many predictive tasks as an information source but also as a prediction target. Therefore, models that are able to encode, incorporate and predict such information in a relational learning context are highly attractive as well. We show that our approach outperforms many state-of-the-art methods for the tasks of relational triplet classification and attribute value prediction.", "histories": [["v1", "Wed, 16 Aug 2017 09:55:15 GMT  (1005kb,D)", "http://arxiv.org/abs/1708.04828v1", "Accepted at CIKM 2017"]], "COMMENTS": "Accepted at CIKM 2017", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["yi tay", "luu anh tuan", "minh c phan", "siu cheung hui"], "accepted": false, "id": "1708.04828"}, "pdf": {"name": "1708.04828.pdf", "metadata": {"source": "CRF", "title": "Multi-Task Neural Network for Non-discrete A\u0082ribute Prediction in Knowledge Graphs", "authors": ["Yi Tay", "Luu Anh Tuan", "Minh C. Phan", "Siu Cheung Hui"], "emails": ["ytay017@e.ntu.edu.sg", "at.luu@i2r.a-star.edu.sg", "phan0005@e.ntu.edu.sg", "asschui@ntu.edu.sg"], "sections": [{"heading": null, "text": "KEYWORDS Knowledge Graphs, Deep Learning, Neural Networks, Multi-Task Learning ACM Reference format: Yi Tay, Luu Anh Tuan, Minh C. Phan, and Siu Cheung Hui. 2017. Multi-Task Neural Network for Non-discrete A ribute Prediction in Knowledge Graphs. In Proceedings of ACM CIKM, Singapore, November 2017 (CIKM '17), 10 pages. DOI: 10.475 / 123 4"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves."}, {"heading": "1.1 Motivation", "text": "First, and secondly, it is possible that the different kinds of information that we have in terms of the way that we present them in terms of the way that we present them in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the way that they and how they are presented in terms of the way that they are presented in terms of the way that they are presented in terms of the"}, {"heading": "1.2 Contributions", "text": "In this paper, we propose an approach to neural networks that elegantly integrates ritual information and enables regression in the context of relational learning. the primary contributions of this paper are the following: \u2022 We propose a novel deep learning architecture for the representation of units, relationships and attributes in KGs. \u2022 By using a common embedding space, we are able to encode ritual information while learning representations. Furthermore, our network also supports the prediction of ritual value that is missing in many methods of relational learning. \u2022 For the first time, we evaluate the ability of many approaches to relational learning such as [4, 27] in the task of predicting ritual values that evaluate the competence and aptitude of representational techniques."}, {"heading": "2 RELATEDWORK", "text": "While traditional relational learning is based on probabilistic, feature engineering or rule mining approaches, it has recently become fashionable to think about knowledge diagrams in latent embedding spaces. The field, also known as deep learning or representation learning, is responsible for many state-of-the-art applications in the fields of NLP and artificial intelligence. Popular latent embedding models can generally again be divided into three categories, namely the neural network approach, the knowledge diagram approach and the factorization approach. Before describing each category, we start with a formal definition of notation used in this paper."}, {"heading": "2.1 Problem Formulation and Notation", "text": "In this section we describe the notation used in this work together with a simple introduction to the problem of relational learning. (2.1.1 Notation.) Let us limit ourselves to a knowledge graph. (E, R) Let us specify a knowledge graph. (E = {e1, e2, e2... e | E |} is the proposition of all entities, and R = {r1, r2... r | R |} is the proposition of all relationships. (E, rk, ej) where two entities are related to each other and can be considered a fact existing in the knowledge graph. (Since a ratification is specific to our model, we present it in later sections. For latent embedding models, entities, and relations ri R o en are presented as real-rated vector or matrix representations.) To facilitate such a design, we are able to use matricesWe, R | n and crafty."}, {"heading": "2.2 Neural Networks for Relational Learning", "text": "In this section we give a brief overview of the existing neural networks (NN), in which the neural networks are suitable for relationship-oriented learning (SGD). \"It is as if the parameters are learned about the stochastic origin.\" (SGD) \"It is as if the neural networks are used for relationship-oriented learning.\" (SGD) \"It is as if the reactionary driving forces are able to grasp the reactionary driving forces (SGD).\" (SGD) \"It is as if the reactionary driving forces in the reactionary world\" (SGD) \"(SGD)\" (SGD) \"is as if the reactionary driving forces in the reactionary world are (SGD)\" (SGD) \"reactionary driving forces in the reactionary world.\" (SGD) \"(SGD)\" it is as if the reactionary driving forces in the reactionary driving forces (SD) are reactionary driving forces in the reactionary world (SD). \""}, {"heading": "2.3 Embedding and Factorization Models", "text": "It is worth noting that there is rich and extensive research on latent embedding models for relational learning. these models are commonly known as translational embedding models and represent improvements over the high-level embedding models proposed by Bordes et al. [17, 22, 31, 36, 37] via this model. On the other hand, the key intuition behind translational embedding models is the concept of the translation principle, i.e. ei + rk \u2212 ej | = 0 for a golden fact.On the other hand, the rarest and early latent embedding models for relational learning are largely based on tensor factorization, which takes advantage of the fact that knowledge graphs inherently follow a 3D structure. Specifically, the general purpose is candecomp (CP) decomposition [5], the tendency to embed [10] in order to insert triplets in a semantic web site."}, {"heading": "2.4 Multi-Task Learning (MTL)", "text": "The key idea of Multi-Task Learning (MTL) is to use the correlation between related tasks to improve performance through parallel learning of tasks. [6] The success of Multi-Task Learning has spurred many multi-task models of neural networks, especially in the area of NLP [8, 13, 23]. While there are other variants of MTL, such as Multi-Task Feature Learning [19], our work focuses on Shared Representative Learning, i.e. the exchange of parameters between two networks so that they result from training on multiple tasks. Much work [13, 23] has shown that multi-task learning can lead to improved performance."}, {"heading": "2.5 Multi-Source Relational Learning", "text": "In many papers it has been suggested to integrate an external source of information to improve representative learning in KGs. Similarly, multi-task learning is used, in which case external information is used to improve learning, but not as an additional task. Therefore, predicting this additional information is not usually supported. Thus, for example, a recent research line is concerned with the common representation of text information with knowledge diagrams [34, 35, 40]. There have also been many extensions of relational learning algorithms to various external information sources such as hierarchical information [39], logical rules [11], schematic information [7], path information [20] and graduation-aware ownership information [15]."}, {"heading": "2.6 Handling Attributes in Knowledge Graphs", "text": "As already mentioned, a widespread problem is that none of the current models has a void to integrate attributes information, especially for non-discrete attributes. Furthermore, a huge problem is that each attribute would require additional matrix factorization, which cannot be practicable. A recent paper, KR-EAR [21], is a translational embedding model that has been proposed to \"model a attribute information.\" Its main idea is that modeling an attribute separately can lead to better relational learning performance. However, its approach deals with relationships that are actually an attribute, such as gender is an attribute that is considered a relationship in most KGs."}, {"heading": "3 MULTI-TASK KNOWLEDGE GRAPH NEURAL NETWORK (MT-KGNN)", "text": "In this section we present MT-KGNN, our novel deep learning architecture for both relational learning and non-discrete ritual prediction on knowledge graphs. Here are two networks in our model, namely the Relational Network (RelNet) and the Ritual Network (A rNet). Figure 1 describes the entire model architecture. Let's start with an introduction to the new notation that refers to the ritual data."}, {"heading": "3.1 Relational Learning with Attributes", "text": "Our work focuses on the use of ribute triplets to both improve the relational learning process and enable regression in a relational learning. e New notation for a knowledge graph can then be formally denoted as B = (E, R, A). Note that ribute triplets are not discrete, i.e. a non-discrete ribute triplet. e New notation for a knowledge graph can then be denoted as B = (E, R, A). We assume that the range of each rib can be derived from the training data. Any value at the test point that exceeds the maximum minutes of normalization is automatically set to 0 or 1."}, {"heading": "3.2 Embedding Layer", "text": "At the time of the training, the inputs to the relationship network [ei, rk, ej, t], where ei, ej, ej, Rn, rk, Rm and t are the goal of the classification, are either 0 or 1. e inputs to the attribute network [ai, vi, aj, vj], where ai, aj, aj, Rl andvi, vj, vj, [0, 1]. For simplicity, we will consider that m = n = l, i.e. all entities, relations and ritual embeddings have the same dimensions. e inputs of our model are discrete, single-engine coded indices, which are passed into an embedded search plane to retrieve the corresponding vector representations."}, {"heading": "3.3 Relational Network (RelNet)", "text": "The RelNet section presents the Relational Network (RelNet) used in our model, which models the structural and relational aspect of the knowledge diagram. RelNet is a simple concatenation of the triplet, which is guided by a nonlinear transformation and ultimately a linear transformation with sigmoid activation. e RelNet component of our model is defined as follows: r el (ei, rk, ej) = \u03c3 (\u00ae w > f (W > d [\u00ae ei; \u00ae ej; \u00ae rk]) + br el) (6), where f tanks the hyperbolic tangent function. w, Rh \u00d7 1 and Wd, R3n \u00b7 h parameters of the network. \u03c3 is the sigmoid function and br el is a scalar distortion. To train RelNet, we minimize the cross-entropy loss function as follows: Lr el = \u2212 N, i = 1 ti logary el (i) (+ 1), which is a common point (\u2212 7)."}, {"heading": "3.4 Attribute Network (AttrNet)", "text": "Similar to RelNet, we form a single-layer network by concatenating the rib and the entities to predict the continuous value, which is a normalized value. However, there are two entities in each relational triplet, namely the head and tail entities. Therefore, there are two sides of the A rNet, which are described as A rNet (le) and A rNet (right), as in Figure 1. e. Motivation to use two networks: Since entities are generally regarded as anti-symmetrical relationships in knowledge diagrams, e.g. entities constantly behave when they are at the head or tail position, we design the network to capture this relational information. In this sense, A rNet optimizes a common loss function of both regression tasks."}, {"heading": "3.5 Multi-Task Optimization and Learning", "text": "In this section, we present a multi-level learning scheme for the training that enables both relational and ritual information. Note that the A rNet input vector includes a ritual learning process that belongs to the head and tail, which not only optimizes the entities contained within the relational triplet, but also enables ritual information."}, {"heading": "3.6 Complexity Analysis", "text": "In this section, we will examine the complexity of our proposed model compared to many other state-of-the-art models. Table 2 describes the spatial and temporal complexity of all models. In addition, we include an estimate of the number of parameters of the YAGO subgraph that we use in our experiments. Our proposed MT-KGNN does not incur large parameter costs compared to the base model ER-MLP. MT-KGNN also shares the embedding of units in RelNet and A rNet. Therefore, the additional costs are derived only from a rib embedding (which is usually much smaller) and additional hidden layer parameters for A rNet. In terms of temporal complexity, our model corresponds to the multiple training of ER-MLP (which depends on the hyperparameter k when AST is used). Similar to spatial complexity, these are even fewer operations compared to models that require square scale operations such as TransR, NTREAL or NTREAL."}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "We are conducting two experiments to evaluate our model: First, we are offering an experimental analysis of our model within the framework of the standard task of the Relational Triplet Classi cation. This is consistent with many studies such as [22, 30]. Second, we are designing another experiment of ritual value prediction to investigate the ability of our model to make regression-like predictions that contribute to the key feature of our model."}, {"heading": "4.1 Datasets", "text": "We use two sets of data in our experiments, constructed from two carefully selected web-scale KGs. \u2022 YAGO [14] is a semantic knowledge base that aggregates data from various sources such as WikiPedia. YAGO contains many famous places and people, which naturally contain many rib values such as size, weight, age, population size, etc. \u2022 Freebase [3] is a widely used benchmark knowledge base for many relational learning tasks. We use the data sets that do not contain rib information. We call our data set YG24K because it contains 24K entities. \u2022 Freebase [3] is a widely used benchmark knowledge base for many relational learning tasks. We use the data set, the Easy Freebase [2], because the public API is no longer available. Due to the large size of the freebase, we extract a domain speci c data set from the Easy Freebase group."}, {"heading": "4.2 Algorithms Compared", "text": "For our experiments, we compare our proposed MT-KGNN approach with many state-of-the-art models. \u2022 CANDECOMP (CP) [10] is a classic tensor factorization approach that evaluates each stage of triplets with s (ei, rk, ej) = ei rk ej. \u2022 RESCAL [27] is a tensor factorization approach based on bilinear tensor products. e scoring function of RESCAL isW > k (\u00ae ej ei). \u2022 TransE [4] is a translational embedding model. A groundbreaking work by Bordes et al., which models relational triplets with | h + r \u2212 t | \u2248 0. \u2022 TransR [22] is an extension of TransE, which proposes the use of matrix projections for model relationship-specific vector spaces."}, {"heading": "4.3 Implementation Details", "text": "We implement all models ourselves in TensorFlow [1]. All algorithms are optimized with the Adam Optimizer [18] with an initial learning rate of 10 \u2212 3 and have set their embedding (entity, relation and, if necessary, a ribute) to 50 dimensions. All algorithms optimize the sigmoid cross-entropy loss function with the exception of TransE and TransR, which we minimize the paired hinge loss as indicated in their original work. For both RESCAL and CP, we consider the adjustment of neural networks [26]. For ER-MLP and MT-KGNN, we set the size of all hidden layers to 100 and drop to d = 0.5 with the Tanh activation function. All models have their entity and relationship embedding to | e | 2 \u2264 1 and | r | 2 \u2264 1. If relation embedding matrices instead of vectors are limited, the norm is limited to 3."}, {"heading": "4.4 Experiment 1 - Relational Triplet Classi cation", "text": "In this experiment, we demonstrate the capability of our model on a standard benchmark for three-dimensional class differences, which is essentially a binary class composition, the benchmark task has been widely adopted in many papers [9, 17, 30]. The task is as follows: Given a triplet of (h, r, t), we classify it as 1 or 0 (true or false). We follow the experimental procedure of [30] obvious. The aim of this experiment is to show that additional ribute information helps in the standard learning tasks. 4.4.1 Assessment methods and metrics. To perform the class composition, we perform negative sampling results according to [30]. For every positive triplet in our train / development / test sets, we include a corrupt triplet. We do this by replacing either the head or the tail entity."}, {"heading": "4.5 Experiment 2 - Attribute Value Prediction", "text": "In this context, it is important that the individual actors in the individual countries show themselves in the most different expressions and expressions, which are reflected in the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions and expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions."}, {"heading": "4.6 Discussion and Analysis", "text": "First, we conduct an ablation study to show the relative effect of RelNet and AST. Second, we extract the rib embedding from MT-KGNN and analyze it. 4.6.1 Ablation studies. Table 6 reports on our FB28K ablation study for rib value prediction. Specifically, we removed RelNet and AST from MT-KGNN to see how much each process contributes to the overall performance of MT-KGNN. Of course, AST would be more important. Therefore, the main investigation relates to the effect of relative triplets on a rib prediction. We observe that there are indeed observable improvements in the use of RelNet. For example, the R2 metric is improved by 17 points, while MAE and RMSE also show significant differences."}, {"heading": "5 CONCLUSION", "text": "We introduced a novel concept of incorporating non-discrete attribute values into relational learning. Non-discrete attributes have traditionally been a challenge to deal with as they do not intuitively feed into the binary nature of KGs. Therefore, our proposed MT-KGNN is a multi-task neural architecture that can elegantly integrate and use this information. It has demonstrated state-of-the-art performance in the relative task of triplet classification and predicting the rib value. In both tasks, we observe that relational and rib information complement each other."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Je\u0082rey Dean", "Ma\u008ahieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geo\u0082rey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wa\u008aenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "Learning on Heterogeneous Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Easy access to the freebase dataset", "author": ["Hannah Bast", "Florian B\u00e4urle", "Bj\u00f6rn Buchhold", "Elmar Hau\u00dfmann"], "venue": "In 23rd International World Wide Web Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Freebase: A Shared Database of Structured General Human Knowledge", "author": ["Kurt D. Bollacker", "Robert P. Cook", "Patrick Tu\u0089s"], "venue": "In Proceedings of the Twenty-Second AAAI Conference on Arti\u0080cial Intelligence, July 22-26,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "PARAFAC: Tutorial and applications", "author": ["Richard Bro"], "venue": "Chemometrics and Intelligent Lab. Syst. 38,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "In Learning to learn", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Typed Tensor Decomposition of Knowledge Bases for Relation Extraction", "author": ["Kai-Wei Chang", "Wen-tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A uni\u0080ed architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Machine Learning, Proceedings of the Twenty-Fi\u0087h International Conference (ICML", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "\u008comas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In \u008ae 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "TripleRank: Ranking Semantic Web Data by Tensor Decomposition", "author": ["\u008comas Franz", "Antje Schultz", "Sergej Sizov", "Ste\u0082en Staab"], "venue": "In \u008ae Semantic Web - ISWC 2009, 8th International Semantic Web Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Jointly Embedding Knowledge Graphs and Logical Rules", "author": ["Shu Guo", "\u008ban Wang", "Lihong Wang", "Bin Wang", "Li Guo"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Distributional vectors encode referential a\u008aributes", "author": ["Abhijeet Gupta", "Gemma Boleda", "Marco Baroni", "Sebastian Pad\u00f3"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "author": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "venue": "CoRR abs/1611.01587", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia: Extended Abstract", "author": ["Johannes Ho\u0082art", "Fabian M. Suchanek", "Klaus Berberich", "Gerhard Weikum"], "venue": "IJCAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "MEmbER: Max-Margin Based Embeddings for Entity Retrieval", "author": ["Shoaib Jameel", "Zied Bouraoui", "Steven Schockaert"], "venue": "In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "A latent factor model for highly multi-relational data", "author": ["Rodolphe Jena\u008aon", "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Multi-Task Model and Feature Joint Learning", "author": ["Ya Li", "Xinmei Tian", "Tongliang Liu", "Dacheng Tao"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Yankai Lin", "Zhiyuan Liu", "Huan-Bo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Knowledge Representation Learning with Entities, A\u008aributes and Relations", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Arti\u0080cial Intelligence, January 25-30,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Recurrent Neural Network for Text Classi\u0080cation with Multi-Task Learning", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning Term Embeddings for Taxonomic Relation Identi\u0080cation Using Dynamic Weighting Neural Network", "author": ["Anh Tuan Luu", "Yi Tay", "Siu Cheung Hui", "See-Kiong Ng"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Je\u0082rey Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proc. IEEE 104,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "A \u008cree-Way Model for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st World Wide Web Conference", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Convolutional Neural Tensor Network Architecture for Community-Based \u008bestion Answering", "author": ["Xipeng Qiu", "Xuanjing Huang"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Non-Parametric Estimation of Multiple Embeddings for Link Prediction on Dynamic Knowledge Graphs", "author": ["Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui"], "venue": "In Proceedings of the \u008airty- First AAAI Conference on Arti\u0080cial Intelligence, February", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "Random Semantic Tensor Ensemble for Scalable Knowledge Graph Link Prediction", "author": ["Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui", "Falk Brauer"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "Learning to Rank \u008bestion Answer Pairs with Holographic Dual LSTM Architecture", "author": ["Yi Tay", "Minh C. Phan", "Anh Tuan Luu", "Siu Cheung Hui"], "venue": "In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Text-enhanced representation learning for knowledge graph", "author": ["Zhigang Wang", "Juanzi Li"], "venue": "In International Joint Conference on Arti\u0080cial Intelligence", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Knowledge Graph and Text Jointly Embedding", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Arti\u0080cial Intelligence, July", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "TransG : A Generative Model for Knowledge Graph Embedding", "author": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "author": ["Qizhe Xie", "Xuezhe Ma", "Zihang Dai", "Eduard H. Hovy"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Representation Learning of Knowledge Graphs with Hierarchical Types", "author": ["Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Aligning Knowledge and Text Embeddings by Entity Descriptions", "author": ["Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 8, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 21, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 29, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 37, "context": "Furthermore, relations in knowledge graphs can be extremely sparse [38] in which relational information might be insu\u0081cient for making solid predictions or recommendations.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "In the Freebase dump Easy Freebase [2], there are already 27 million non-discrete a\u008aribute triplets (\u2248 10%).", "startOffset": 35, "endOffset": 38}, {"referenceID": 27, "context": "As such, there are limited works that a\u008aempt to exploit a\u008aribute information in the se\u008aing of KGs [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "\u2022 For the \u0080rst time, we evaluate the ability of many relational learning approaches such as [4, 27] in the task of a\u008aribute value prediction.", "startOffset": 92, "endOffset": 99}, {"referenceID": 26, "context": "\u2022 For the \u0080rst time, we evaluate the ability of many relational learning approaches such as [4, 27] in the task of a\u008aribute value prediction.", "startOffset": 92, "endOffset": 99}, {"referenceID": 0, "context": "s(ei , rk , ej ) \u2208 [0, 1] (1)", "startOffset": 19, "endOffset": 25}, {"referenceID": 8, "context": "\u008ce \u0080rst approach is the ER-MLP (Entity Relation Multi-Layered Perceptron) which was incepted as part of the Google Knowledge Vault Project [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 29, "context": "\u008ce other approach is the Neural Tensor Network (NTN) [30] which models multiple views of dyadic interactions between entities with tensor products.", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "\u008cis highly expressive neural model was \u0080rst incepted in the \u0080elds of NLP for sentiment analysis and subsequently applied in multiple domains such as knowledge base completion [30] and question answering [29, 33].", "startOffset": 175, "endOffset": 179}, {"referenceID": 28, "context": "\u008cis highly expressive neural model was \u0080rst incepted in the \u0080elds of NLP for sentiment analysis and subsequently applied in multiple domains such as knowledge base completion [30] and question answering [29, 33].", "startOffset": 203, "endOffset": 211}, {"referenceID": 32, "context": "\u008cis highly expressive neural model was \u0080rst incepted in the \u0080elds of NLP for sentiment analysis and subsequently applied in multiple domains such as knowledge base completion [30] and question answering [29, 33].", "startOffset": 203, "endOffset": 211}, {"referenceID": 29, "context": "\u008ce key intuition behind NTN is that it models multiple views of dyadic interactions between entities with a relation-speci\u0080c tensor [30].", "startOffset": 132, "endOffset": 136}, {"referenceID": 3, "context": "\u008cese models are commonly known as translational embedding models and are improvements over the highly in\u0083uential work, TransE [4], proposed by Bordes et al.", "startOffset": 126, "endOffset": 129}, {"referenceID": 16, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 21, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 30, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 35, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 36, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 4, "context": "Speci\u0080cally, the general purpose Candecomp (CP) decomposition [5] was \u0080rst used in [10] to rank triplets in a semantic web se\u008aing.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "Speci\u0080cally, the general purpose Candecomp (CP) decomposition [5] was \u0080rst used in [10] to rank triplets in a semantic web se\u008aing.", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "then proposed RESCAL [16, 28] which is an improved tensor factorization technique based on the bilinear form for relational learning.", "startOffset": 21, "endOffset": 29}, {"referenceID": 27, "context": "then proposed RESCAL [16, 28] which is an improved tensor factorization technique based on the bilinear form for relational learning.", "startOffset": 21, "endOffset": 29}, {"referenceID": 6, "context": "Subsequently, extensions of RESCAL such as TRESCAL [7] and RSTE [32] were proposed.", "startOffset": 51, "endOffset": 54}, {"referenceID": 31, "context": "Subsequently, extensions of RESCAL such as TRESCAL [7] and RSTE [32] were proposed.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "[26], the tensor factorization model RESCAL is casted as a neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 121, "endOffset": 132}, {"referenceID": 12, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 121, "endOffset": 132}, {"referenceID": 22, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 121, "endOffset": 132}, {"referenceID": 18, "context": "While there are other variants of MTL such as multi-task feature learning [19], our work is concerned with shared representation learning, i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "Many works [13, 23] have shown that multi-task learning can lead to improved performance.", "startOffset": 11, "endOffset": 19}, {"referenceID": 22, "context": "Many works [13, 23] have shown that multi-task learning can lead to improved performance.", "startOffset": 11, "endOffset": 19}, {"referenceID": 33, "context": "For example, a recently fashionable line of research is concerned with joint representations of textual information with knowledge graphs [34, 35, 40].", "startOffset": 138, "endOffset": 150}, {"referenceID": 34, "context": "For example, a recently fashionable line of research is concerned with joint representations of textual information with knowledge graphs [34, 35, 40].", "startOffset": 138, "endOffset": 150}, {"referenceID": 39, "context": "For example, a recently fashionable line of research is concerned with joint representations of textual information with knowledge graphs [34, 35, 40].", "startOffset": 138, "endOffset": 150}, {"referenceID": 38, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 164, "endOffset": 168}, {"referenceID": 6, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 189, "endOffset": 192}, {"referenceID": 19, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 14, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 254, "endOffset": 258}, {"referenceID": 27, "context": "In [28], the authors proposed to use a separate matrix factorization to learn a\u008aributes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "A recent work, KR-EAR [21] is a translational embedding model that was proposed to model \u2018a\u008aribute information\u2019.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": ", a non-discrete a\u008aribute triplet, \u03c8i \u2208 \u03c8 is de\u0080ned as (ei ,ak ,v) where ak is an a\u008aribute in A and v is a normalized continuous value from [0, 1].", "startOffset": 140, "endOffset": 146}, {"referenceID": 0, "context": "\u008ce inputs to the A\u008aribute Network are [ai ,vi ,aj ,vj ] where ai ,aj \u2208 Rl andvi ,vj \u2208 [0, 1].", "startOffset": 86, "endOffset": 92}, {"referenceID": 8, "context": "At this point, it is good to note that RelNet remains identical to the ER-MLP model [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "Similar to RelNet, we train a single hidden layer network by concatenation of the a\u008aribute and entity embeddings to predict the continuous value which is a normalized value \u2208 [0, 1].", "startOffset": 175, "endOffset": 181}, {"referenceID": 22, "context": "Similar to many other multitask learning techniques [23], both RelNet and A\u008arNet are trained in an alternating fashion.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "RESCAL [27, 28] Nem + Nrn2 1.", "startOffset": 7, "endOffset": 15}, {"referenceID": 27, "context": "RESCAL [27, 28] Nem + Nrn2 1.", "startOffset": 7, "endOffset": 15}, {"referenceID": 3, "context": "278M (m2 +m)Nt TransE [4] Nem + Nrn 1.", "startOffset": 22, "endOffset": 25}, {"referenceID": 21, "context": "202M Nt TransR [22] Nem + Nr (mn) 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "278M 2mnNt ER-MLP [9] Nem + Nrn + ((2m + n) \u00d7 h) + h 1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 29, "context": "217M (3mh + h)Nt NTN [30] Nem + Nr (n2s + 2ns + 2s) 1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "\u008cis is in concert with many works such as [22, 30].", "startOffset": 42, "endOffset": 50}, {"referenceID": 29, "context": "\u008cis is in concert with many works such as [22, 30].", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "\u2022 YAGO [14] is a semantic knowledge base that aggregates data from various sources including WikiPedia.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "\u2022 Freebase [3] is a widely used benchmark knowledge base for many relational learning tasks.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "We use the dataset dump Easy Freebase [2] since the public API is no longer available.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "\u2022 CANDECOMP (CP) [10] is a classical tensor modeling technique that scores each triplet using s(ei , rk , ej ) = ei rk ej .", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "\u2022 RESCAL [27] is a tensor factorization approach based on bilinear tensor products.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "\u2022 TransE [4] is a translational embedding model.", "startOffset": 9, "endOffset": 12}, {"referenceID": 21, "context": "\u2022 TransR [22] is an extension of TransE that proposes using matrix projection to model relation speci\u0080c vector space.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "\u2022 ER-MLP [9] is the baseline neural network approach that learns representations via a concatenation operator of triplets.", "startOffset": 9, "endOffset": 12}, {"referenceID": 29, "context": "\u2022 Neural Tensor Network (NTN) [30] is a highly expressive model that combines a bilinear tensor product with a MLP.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "We use a se\u008aing of s = 4 following [30] where s is the number of tensor slices.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "We implement all models ourselves in TensorFlow [1].", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "All algorithms are optimized with the Adam optimizer [18] with an initial learning rate of 10\u22123 and have their embeddings (entity, relation and a\u008aribute when applicable) set to 50 dimensions.", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "For both RESCAL and CP, we consider the neural network adaptation [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "\u008cis benchmark task has been widely adopted in many works [9, 17, 30].", "startOffset": 57, "endOffset": 68}, {"referenceID": 16, "context": "\u008cis benchmark task has been widely adopted in many works [9, 17, 30].", "startOffset": 57, "endOffset": 68}, {"referenceID": 29, "context": "\u008cis benchmark task has been widely adopted in many works [9, 17, 30].", "startOffset": 57, "endOffset": 68}, {"referenceID": 29, "context": "We follow the experimental procedure of [30] closely.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "In order to perform classi\u0080cation, we perform negative sampling following [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Note that the usage of supervised classi\u0080ers for evaluating the quality of embeddings has been substantiated in many works [12, 24].", "startOffset": 123, "endOffset": 131}, {"referenceID": 23, "context": "Note that the usage of supervised classi\u0080ers for evaluating the quality of embeddings has been substantiated in many works [12, 24].", "startOffset": 123, "endOffset": 131}, {"referenceID": 0, "context": "\u2022 Random Guess (R-GUESS) randomly generates a value v \u2208 [0, 1] as a prediction.", "startOffset": 56, "endOffset": 62}, {"referenceID": 24, "context": "\u2022 SkipGram [25] or also known as Word2Vec is a highly popular language modeling approach that learns continuous vector representations of words from textual documents.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete a\u008aributes for each entity. Intuitively, these a\u008aributes such as height, price or population count are able to richly characterize entities in knowledge graphs. \u008cis additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete a\u008aribute information in a relational setting. Speci\u0080cally, we train a neural network for triplet prediction along with a separate network for a\u008aribute value regression. Via multi-task learning, we are able to learn representations of entities, relations and a\u008aributes that encode information about both tasks. Moreover, such a\u008aributes are not only central to many predictive tasks as an information source but also as a prediction target. \u008cerefore, models that are able to encode, incorporate and predict such information in a relational learning context are highly a\u008aractive as well. We show that our approach outperforms many state-ofthe-art methods for the tasks of relational triplet classi\u0080cation and a\u008aribute value prediction.", "creator": "LaTeX with hyperref package"}}}