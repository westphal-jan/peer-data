{"id": "1708.04846", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Maximum A Posteriori Inference in Sum-Product Networks", "abstract": "Sum-product networks (SPNs) are a class of probabilistic graphical models that allow tractable marginal inference. However, the maximum a posteriori (MAP) inference in SPNs is NP-hard. We investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we reduce general MAP inference to its special case without evidence and hidden variables; we also show that it is NP-hard to approximate the MAP problem to $2^{n^\\epsilon}$ for fixed $0 \\leq \\epsilon &lt; 1$, where $n$ is the input size. For the algorithmic part, we first present an exact MAP solver that runs reasonably fast and could handle SPNs with up to 1k variables and 150k arcs in our experiments. We then present a new approximate MAP solver with a good balance between speed and accuracy, and our comprehensive experiments on real-world datasets show that it has better overall performance than existing approximate solvers.", "histories": [["v1", "Wed, 16 Aug 2017 11:05:48 GMT  (2848kb,D)", "http://arxiv.org/abs/1708.04846v1", null], ["v2", "Fri, 18 Aug 2017 07:07:01 GMT  (2848kb,D)", "http://arxiv.org/abs/1708.04846v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jun mei", "yong jiang", "kewei tu"], "accepted": false, "id": "1708.04846"}, "pdf": {"name": "1708.04846.pdf", "metadata": {"source": "CRF", "title": "Maximum A Posteriori Inference in Sum-Product Networks", "authors": ["Jun Mei", "Yong Jiang", "Kewei Tu"], "emails": ["meijun@shanghaitech.edu.cn", "jiangyong@shanghaitech.edu.cn", "tukw@shanghaitech.edu.cn"], "sections": [{"heading": null, "text": "For the algorithmic part, we first present an exact MAP solver that runs relatively fast and could handle SPNs with up to 1k variables and 150k arcs in our experiments. Then, we present a new approximate MAP solver with a good balance between speed and accuracy, and our comprehensive experiments with real data sets show that it performs better overall than existing approximate solvers."}, {"heading": "1 Introduction", "text": "SPNs are a class of probable graphical models known for their comprehensible marginal conclusions (Poon and Domingos, 2011). In the previous work, SPNs were mainly used to draw marginal conclusions. On the other hand, although MAP NP conclusions are widely used in many applications in natural language processing, computer vision, speech recognition, etc., MAP conclusions in SPNs have not been extensively studied. Some previous work on MAP conclusions focuses on selective SPNs (Peharz et al., 2014a), also known as determinism in the context of knowledge acquisition (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017). Huang et al. (2006) we presented an exact solver for MAP based on deterministic arithmetic circuits."}, {"heading": "2 Background", "text": "We adapt the notations of Peharz et al. (2015). A random variable is referred to as an uppercase letter, e.g. X, Y. The corresponding lowercase letter x denotes a value that X can take. The set of all values that X can take is referred to as val (X). The corresponding lowercase letter x denotes a composite value. The set of all compound values that X can take is referred to as val (X), i.e. val (X) = \u00d7 Nn = 1val (Xn). The corresponding lowercase letter x denotes a composite value X. The set of all compound values that X can take is referred to as val (X), i.e. val (X) = \u00d7 Nn = 1val (Xn)."}, {"heading": "2.1 Network polynomials", "text": "Darwiche (2003) introduced network polynomials. \u03bbX = x-R denotes the so-called q indicator for X and x. \u03bb denotes a vector that collects all indicators of the X definition 1 (network polynomial). Let \u03a6 be an unnormalized distribution over X with a finite number of values. Network polynomial is defined as asf\u03a6 (\u03bb): = \u2211 x value (X); x value (x); x value (x); X value (x); X value (x); X value (X); X value (X); X value (X); X value (X); X value (X); X value (X); X value (X); X value (X)."}, {"heading": "2.2 Sum-product networks", "text": "SPNs over variables with an infinite number of values are defined as follows: Definition 2 (sum product networks) = Q = C. Let X be variables with an infinite number of values and their indicators. A sum product network S = (G, w) over X is a rooted, directed acyclic graph G = (V, A) with nonnegative parameters w. All leaves of G are indicators and all internal nodes are either sums or products. Denote the amount of children of the node N as ch (N). A sum node S computes a weighted sum S (V, A) with nonnegative parameters wS (S) = [S) wS), CC (CC), where the weight wS, C, w is associated with the arc (S, C)."}, {"heading": "3 Theoretical results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 MAX inference", "text": "We define a specific case of MAP inference without evidence and hidden variables, which we call MAX inference: MAXS: = arg max x-val (X) S (x) (7) We can reduce any MAP problem to a MAX problem in linear time. (Without loss of generality, we assume that the root of an SPN is a sum (otherwise, we can always add a new sum root associated with the old root with weight 1) Faced with a SPN S and an MAP problem with Q, e, H, algorithm 1 modifies S and returns a new SPN called S so that Q), S (q) = S ({q} x {e}) x-val (H), which implies that MAXS variables (Q, e, H).The algorithm runs as follows."}, {"heading": "3.2 Approximation complexity", "text": "It has been shown in the literature that the MAP conclusion is difficult in Bayesian networks (BNs). | Describe the size of a SPN S and a BN B as: | S | and | B |. Theorem 6 in (De Campos, 2011) indicates that for each fixed 0 \u2264 < 1 we can construct an SPN S that represents the same distribution with the size | S | O (| B |) in linear time. See the evidence for Lemma 1 in Appendix A.Theorem 1. For each fixed 0 \u2264; 1 it is NP difficult to represent the same distribution with the size | S | 2 in linear time."}, {"heading": "4 Exact solver", "text": "Since MAP is NP-hard, no efficient exact solver exists. However, with a combination of pruning, heuristic, and optimization techniques, we can make exact conclusions relatively quickly in practice. (In this section, we introduce two processing techniques, one heuristic, and one optimization technology in Algorithm 2. Calculate x = MAXS 1: x \u2190 an initial sample. (Note: the use of any initialization method, for example, random initialization 2: x). (X) 3: Search function (X) 4: X \u2190 a variable with | X \u2190 an initial sample. (X) | 1 means that the value of X is determined if no such X exists, then all variables are determined. (X) is the only element in X: X that we now guarantee 7: for all x values. (X) Consider all possible values of the variable X: X. \""}, {"heading": "5 Approximate solvers", "text": "Thm. 1 states that approximating the MAP conclusion in SPNs is very difficult. In practice, however, it is possible to design approximate solvers with good performance for most data. In this section, we briefly present existing approximate methods and then present a new method. Again, in describing the algorithms, we assume that the MAP problem has been converted to a MAX problem."}, {"heading": "5.1 Existing methods", "text": "It is a parse tree T = (G), which is an SPN where an SPN is (G). It is a parse tree in three steps: First, it changes all sum nodes in the SPN to maximum nodes; second, it calculates the values of all nodes from bottom to top; third, in a recursive top-down manner starting with the root node, it selects the child of each maximum node with the highest value. The selected leaf nodes in the third step represent the approximate MAP solution of BT. We call this method Best Tree because we can show that it actually finds the parse tree of the SPN with the highest value. Tu (2016) showed that any degradable SPN can be considered stochastic context-free and after their work we can define a parse tree of an SPN as a follower."}, {"heading": "5.2 K-Best Tree method", "text": "It can be shown that the set of leaves of a parser tree T (Def. 3) corresponds to a single sample x. We call this relationship T \u0445 x. On the other hand, a sample can correspond to more than one parser tree of the SPN. (Zhao et al., 2016) We say that a sample is ambiguous with respect to an SPN if it corresponds to more than one parser tree of the SPN. We say that an SPN is ambiguous if there are some ambiguous samples with respect to the SPN.) We say that a sample is ambiguous with respect to an SPN if there is more than one parser tree of the SPN origin. We say that an SPN is ambiguous when there are ambiguous samples with respect to the SPN."}, {"heading": "6 Experiments", "text": "We evaluated the MAP solvers using twenty widely used real datasets (collected from applications and data sources such as click logs, plant habitats, cooperative filtering, etc.) of genes and domingos (2013) with variable numbers ranging from 16 to 1556. We used the LearnSPN method (Gens and Domingos, 2013) to obtain one SPN for each dataset. The number of sheets of learned SPNs ranges from 6471 to 2,598,116. Detailed statistics of learned SPNs are presented in Appendix C. We generated MAP problems with different proportions of query (Q), proof (E), and hidden (H) variables. For each dataset and share, we generated 1,000 different MAP problems by randomly dividing the variables into Q / E / H variables. When executing the solvers, we limited the runtime for a vvAP problem by 10 minutes (HEV4) on our HEVR X4 (26VR) X4 experiments."}, {"heading": "6.1 Exact solver", "text": "We evaluated four combinations of the techniques introduced in Section 4: Marginal Checking (MC), Forward Checking (FC), FC with Ording (FC + O) and FC with Ording and Stage (FC + O + S).Figure 2 shows that for each data set and with a Q / E / H ratio of 3 / 3 / 4, the number of times in which each method ran over the 1000 problems within 10 minutes was reached. Results for additional Q / E / H proportions can be found in Appendix D. It is evident that for the records with the smallest variable numbers and SPN sizes, all four methods were completed within ten minutes. On the other data sets, FC clearly beats MC and adding ordering and staging brings further improvements. Our best method, FC + O + S, can be viewed as handling SPNs with up to 1556 variables (\"Ad\") and 147,599 arcs (\"Accidents\")."}, {"heading": "6.2 Approximate solver", "text": "We evaluated all approximate solvers described in section 5, as well as approximate versions of our exact solvers. For BS, we tested beam sizes of 1, 10, and 100. For CBT, we tested K = 10 and 100. We measured the performance of a solver on each data set with each Q / E / H portion based on its runtime and winning number. The winning number is defined as the number of problems where the solver issues a solution with the highest score of all solvers. Since our exact solvers are always algorithms, we also evaluated them as approximate solvers with a 10-minute time budget. Figure 3 shows that for each method and each Q / E / H portion, the average runtime to the winners across all data sets is."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset MC FC FC+O FC+O+S", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Conclusion", "text": "Theoretically, we defined a new inference problem called MAX and presented a linear time reduction from MAP to MAX. This suggests that when studying MAP inference, we can focus on the much simpler MAX problem. We also demonstrated that it is almost impossible to find a practical limit for approximate MAP solutions. Algorithmically, we presented an exact solver based on extensive searching using cutting, heuristics, and optimization techniques, and an approximate solver based on finding the top K-parse trees of the input SPN. Our extensive experiments show that the exact solver is relatively fast and the approximate solver has better overall performance than existing methods."}, {"heading": "A Proof of Lemma 1", "text": "Lemma 1: If we use a tree-structured BN B, we can construct an SPN S that forms the same distribution with the size | | | S | | O (| B |) in linear time.Evidence. As mentioned in (Zhao et al., 2015), there is an algorithm that forms an SPN that represents the same distribution. We adapt its algorithm for tree-structured BNs (Algorithm 4: BN2SPN). If we obtain a tree-structured BN B, we would like to show that | S | O (| B |), where S = BN2SPN (B) stands. We designate the numbers of weights, sum nodes, sum slots, product slots in S as | w | VS |, | AP | O |, and designate the number of parameters and the number of all values of all variables in B # x."}, {"heading": "B Time complexity of KBT", "text": "We analyze the temporal complexity of CBT (algorithm 3). To execute line 5, we first put the best value in each child's multiset into the priority list, and then pop K times into the queue. Whenever we pop a value m, we push the next best value (if any) in the child's multiset of which m belongs into the queue. The size of the queue is | ch (S) |. To execute line 7, we continue to merge the children's multisets in pairs until we have a single multiset left. If we merge two multisets, we first push the product of the best values from the two multisets into a priority list, and then K times. If we push a product m1 x x x x x into the 2 m \u00b2 values, we push it into the 2 x m \u00b2 values, not the 2 m \u00b2 values."}, {"heading": "C Dataset statistics", "text": "The following table shows the statistics of the SPNs learned from the 20 data sets. # Vars denotes the number of variables and # Arcs the number of slurs in the learned SPN."}, {"heading": "D Experimental results with additional Q/E/H proportions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset MC FC FC+O FC+O+S", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset MC FC FC+O FC+O+S", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Language modeling with sum-product networks", "author": ["W.C. Cheng", "S. Kok", "H.V. Pham", "H.L. Chieu", "K.M.A. Chai"], "venue": "In INTERSPEECH,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "On relaxing determinism in arithmetic circuits", "author": ["A. Choi", "A. Darwiche"], "venue": "In ICML,", "citeRegEx": "Choi and Darwiche.,? \\Q2017\\E", "shortCiteRegEx": "Choi and Darwiche.", "year": 2017}, {"title": "Approximation complexity of maximum a posteriori inference in sum-product networks", "author": ["D. Conaty", "D.D. Mau\u00e1", "C.P. de Campos"], "venue": null, "citeRegEx": "Conaty et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Conaty et al\\.", "year": 2017}, {"title": "Decomposable negation normal form", "author": ["A. Darwiche"], "venue": "JACM,", "citeRegEx": "Darwiche.,? \\Q2001\\E", "shortCiteRegEx": "Darwiche.", "year": 2001}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "JACM,", "citeRegEx": "Darwiche.,? \\Q2003\\E", "shortCiteRegEx": "Darwiche.", "year": 2003}, {"title": "A knowledge compilation", "author": ["A. Darwiche", "P. Marquis"], "venue": "map. JAIR,", "citeRegEx": "Darwiche and Marquis.,? \\Q2002\\E", "shortCiteRegEx": "Darwiche and Marquis.", "year": 2002}, {"title": "New complexity results for MAP in Bayesian networks", "author": ["C.P. De Campos"], "venue": "In IJCAI,", "citeRegEx": "Campos.,? \\Q2011\\E", "shortCiteRegEx": "Campos.", "year": 2011}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In NIPS,", "citeRegEx": "Gens and Domingos.,? \\Q2012\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2012}, {"title": "Learning the structure of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In ICML,", "citeRegEx": "Gens and Domingos.,? \\Q2013\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2013}, {"title": "Solving MAP exactly by searching on compiled arithmetic circuits", "author": ["J. Huang", "M. Chavira", "A. Darwiche"], "venue": "In AAAI,", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Learning arithmetic circuits", "author": ["D. Lowd", "P. Domingos"], "venue": "In UAI,", "citeRegEx": "Lowd and Domingos.,? \\Q2008\\E", "shortCiteRegEx": "Lowd and Domingos.", "year": 2008}, {"title": "MAP complexity results and approximation methods", "author": ["J.D. Park"], "venue": "In UAI,", "citeRegEx": "Park.,? \\Q2002\\E", "shortCiteRegEx": "Park.", "year": 2002}, {"title": "Foundations of sum-product networks for probabilistic modeling", "author": ["D.-I.R. Peharz"], "venue": "PhD thesis, Aalborg University,", "citeRegEx": "Peharz.,? \\Q2015\\E", "shortCiteRegEx": "Peharz.", "year": 2015}, {"title": "Learning selective sum-product networks", "author": ["R. Peharz", "R. Gens", "P. Domingos"], "venue": "In ICML Workshop on Learning Tractable Probabilistic Models,", "citeRegEx": "Peharz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2014}, {"title": "Modeling speech with sum-product networks: Application to bandwidth extension", "author": ["R. Peharz", "G. Kapeller", "P. Mowlaee", "F. Pernkopf"], "venue": "In ICASSP,", "citeRegEx": "Peharz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2014}, {"title": "On theoretical properties of sum-product networks", "author": ["R. Peharz", "S. Tschiatschek", "F. Pernkopf", "P.M. Domingos", "B. BioTechMed-Graz"], "venue": "In AISTATS,", "citeRegEx": "Peharz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2015}, {"title": "On the latent variable interpretation in sum-product networks", "author": ["R. Peharz", "R. Gens", "F. Pernkopf", "P. Domingos"], "venue": null, "citeRegEx": "Peharz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2016}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In UAI,", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "Learning sum-product networks with direct and indirect variable interactions", "author": ["A. Rooshenas", "D. Lowd"], "venue": "In ICML,", "citeRegEx": "Rooshenas and Lowd.,? \\Q2014\\E", "shortCiteRegEx": "Rooshenas and Lowd.", "year": 2014}, {"title": "Stochastic And-Or grammars: A unified framework and logic perspective", "author": ["K. Tu"], "venue": "In IJCAI,", "citeRegEx": "Tu.,? \\Q2016\\E", "shortCiteRegEx": "Tu.", "year": 2016}, {"title": "On the relationship between sum-product networks and Bayesian networks", "author": ["H. Zhao", "M. Melibari", "P. Poupart"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "A unified approach for learning the parameters of sum-product networks", "author": ["H. Zhao", "P. Poupart", "G. Gordon"], "venue": "In NIPS,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "SPNs are a class of probabilistic graphical models known for its tractable marginal inference (Poon and Domingos, 2011).", "startOffset": 94, "endOffset": 119}, {"referenceID": 5, "context": ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).", "startOffset": 85, "endOffset": 113}, {"referenceID": 4, "context": ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).", "startOffset": 138, "endOffset": 204}, {"referenceID": 10, "context": ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).", "startOffset": 138, "endOffset": 204}, {"referenceID": 1, "context": ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).", "startOffset": 138, "endOffset": 204}, {"referenceID": 18, "context": "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al.", "startOffset": 79, "endOffset": 136}, {"referenceID": 17, "context": "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b).", "startOffset": 154, "endOffset": 221}, {"referenceID": 0, "context": "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b).", "startOffset": 154, "endOffset": 221}, {"referenceID": 0, "context": ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017). Huang et al. (2006) presented an exact solver for MAP based on deterministic arithmetic circuits.", "startOffset": 180, "endOffset": 226}, {"referenceID": 0, "context": ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017). Huang et al. (2006) presented an exact solver for MAP based on deterministic arithmetic circuits. Peharz et al. (2016) showed that most probable explanation (MPE), a special case of MAP without hidden variables, is tractable on selective SPNs.", "startOffset": 180, "endOffset": 325}, {"referenceID": 0, "context": "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE.", "startOffset": 180, "endOffset": 282}, {"referenceID": 0, "context": "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE. Peharz et al. (2016) showed a different proof based on the NP-hardness results from Bayesian networks.", "startOffset": 180, "endOffset": 362}, {"referenceID": 0, "context": "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE. Peharz et al. (2016) showed a different proof based on the NP-hardness results from Bayesian networks. Conaty et al. (2017) discussed approximation complexity of MAP in SPNs and gave several useful theoretical results.", "startOffset": 180, "endOffset": 465}, {"referenceID": 0, "context": "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE. Peharz et al. (2016) showed a different proof based on the NP-hardness results from Bayesian networks. Conaty et al. (2017) discussed approximation complexity of MAP in SPNs and gave several useful theoretical results. In this paper, we investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we make the following two contributions. First, we define a special MAP inference problem called MAX that has no evidence and hidden variables, and we show that MAP can be reduced to MAX in linear time. This implies that to study MAP we can instead focus on MAX, which has a much simpler form. Second, we show that it is NP-hard to approximate the MAP problem to 2 fox fixed 0 \u2264 < 1, where n is the input size. This result is similar to a theorem proved by Conaty et al. (2017), but we use a proof strategy that is arguably simpler than theirs.", "startOffset": 180, "endOffset": 1168}, {"referenceID": 12, "context": "We adapt the notations from Peharz et al. (2015). A random variable is denoted as an upper-case letter, e.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "1 Network polynomials Darwiche (2003) introduced network polynomials.", "startOffset": 22, "endOffset": 38}, {"referenceID": 5, "context": "Using the terminology of knowledge compilation (Darwiche and Marquis, 2002) and negation normal forms (Darwiche, 2001), the algorithm performs conditioning on the evidence variables, projects the SPN onto the query variables, and then makes simplifications to the SPN structure.", "startOffset": 47, "endOffset": 75}, {"referenceID": 3, "context": "Using the terminology of knowledge compilation (Darwiche and Marquis, 2002) and negation normal forms (Darwiche, 2001), the algorithm performs conditioning on the evidence variables, projects the SPN onto the query variables, and then makes simplifications to the SPN structure.", "startOffset": 102, "endOffset": 118}, {"referenceID": 2, "context": "Note that in parallel to our work, Conaty et al. (2017) gave a similar result to Thm.", "startOffset": 35, "endOffset": 56}, {"referenceID": 17, "context": "Best Tree (BT) BT, first used by Poon and Domingos (2011), runs in three steps: first, it changes all the sum nodes in the SPN to max nodes; second, it calculates the values of all the nodes from bottom up; third, in a recursive top-down manner starting from the root node, it selects the child of each max node with the largest value.", "startOffset": 33, "endOffset": 58}, {"referenceID": 17, "context": "Best Tree (BT) BT, first used by Poon and Domingos (2011), runs in three steps: first, it changes all the sum nodes in the SPN to max nodes; second, it calculates the values of all the nodes from bottom up; third, in a recursive top-down manner starting from the root node, it selects the child of each max node with the largest value. The selected leaf nodes in the third step represent the approximate MAP solution of BT. We name this method Best Tree because we can show that it actually finds the parse tree of the SPN with the largest value. Tu (2016) showed that any decomposable SPN can be seen as a stochastic context-free And-Or grammar, and following their work we can define a parse tree of an SPN as follows.", "startOffset": 33, "endOffset": 557}, {"referenceID": 21, "context": ", induced trees in (Zhao et al., 2016).", "startOffset": 19, "endOffset": 38}, {"referenceID": 15, "context": "We name this method Normalized Greedy Selection because it can be seen as greedily constructing a parse tree in a recursive top-down manner by selecting for each sum node the child with the largest weight in the locally normalized SPN (Peharz et al., 2015).", "startOffset": 235, "endOffset": 256}, {"referenceID": 12, "context": "Normalized Greedy Selection (NG) NG was also used first by Poon and Domingos (2011). It is very similar to BT except that in the first step, NG does not change sum nodes to max nodes.", "startOffset": 59, "endOffset": 84}, {"referenceID": 2, "context": "Argmax-Product (AMAP) AMAP was proposed by Conaty et al. (2017). It does |ch(S)| times bottom-up evaluation on every sum S in the SPN, so it has quadratic time complexity, while BT and NG both have linear time complexity.", "startOffset": 43, "endOffset": 64}, {"referenceID": 11, "context": "Beam Search (BS) Hill climbing has been used in MAP inference of arithmetic circuits (Park, 2002; Darwiche, 2003), a type of models closely related to SPNs.", "startOffset": 85, "endOffset": 113}, {"referenceID": 4, "context": "Beam Search (BS) Hill climbing has been used in MAP inference of arithmetic circuits (Park, 2002; Darwiche, 2003), a type of models closely related to SPNs.", "startOffset": 85, "endOffset": 113}, {"referenceID": 21, "context": "Furthermore, S(x) = \u2211 T \u223cx T (x) (Zhao et al., 2016).", "startOffset": 33, "endOffset": 52}, {"referenceID": 16, "context": "It is easy to show that BT finds the exact solution to the MAX problem if the SPN is unambiguous (Peharz et al., 2016).", "startOffset": 97, "endOffset": 118}, {"referenceID": 7, "context": ") from Gens and Domingos (2013), with variable numbers ranging from 16 to 1556.", "startOffset": 7, "endOffset": 32}, {"referenceID": 8, "context": "(Gens and Domingos, 2013) to obtain an SPN for each dataset.", "startOffset": 0, "endOffset": 25}], "year": 2017, "abstractText": "Sum-product networks (SPNs) are a class of probabilistic graphical models that allow tractable marginal inference. However, the maximum a posteriori (MAP) inference in SPNs is NP-hard. We investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we reduce general MAP inference to its special case without evidence and hidden variables; we also show that it is NP-hard to approximate the MAP problem to 2 for fixed 0 \u2264 < 1, where n is the input size. For the algorithmic part, we first present an exact MAP solver that runs reasonably fast and could handle SPNs with up to 1k variables and 150k arcs in our experiments. We then present a new approximate MAP solver with a good balance between speed and accuracy, and our comprehensive experiments on real-world datasets show that it has better overall performance than existing approximate solvers.", "creator": "LaTeX with hyperref package"}}}