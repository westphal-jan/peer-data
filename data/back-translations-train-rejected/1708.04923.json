{"id": "1708.04923", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "mAnI: Movie Amalgamation using Neural Imitation", "abstract": "Cross-modal data retrieval has been the basis of various creative tasks performed by Artificial Intelligence (AI). One such highly challenging task for AI is to convert a book into its corresponding movie, which most of the creative film makers do as of today. In this research, we take the first step towards it by visualizing the content of a book using its corresponding movie visuals. Given a set of sentences from a book or even a fan-fiction written in the same universe, we employ deep learning models to visualize the input by stitching together relevant frames from the movie. We studied and compared three different types of setting to match the book with the movie content: (i) Dialog model: using only the dialog from the movie, (ii) Visual model: using only the visual content from the movie, and (iii) Hybrid model: using the dialog and the visual content from the movie. Experiments on the publicly available MovieBook dataset shows the effectiveness of the proposed models.", "histories": [["v1", "Wed, 16 Aug 2017 15:12:20 GMT  (1878kb,D)", "http://arxiv.org/abs/1708.04923v1", "Accepted in ML4Creativity workshop in KDD 2017. Preprint"]], "COMMENTS": "Accepted in ML4Creativity workshop in KDD 2017. Preprint", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["naveen panwar", "shreya khare", "neelamadhav gantayat", "rahul aralikatte", "senthil mani", "anush sankaran"], "accepted": false, "id": "1708.04923"}, "pdf": {"name": "1708.04923.pdf", "metadata": {"source": "META", "title": "mAnI: Movie Amalgamation using Neural Imitation", "authors": ["Naveen Panwar", "Shreya Khare", "Neelamadhav Gantayat", "Rahul Aralika\u008ae", "Senthil Mani", "Anush Sankaran"], "emails": ["naveen.panwar@in.ibm.com", "shkhare4@in.ibm.com", "neelamadhav@in.ibm.com", "rahul.a.r@in.ibm.com", "sentmani@in.ibm.com", "anussank@in.ibm.com"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computingmethodologies \u2192 Cognitive Science; Learning latent representations; KEYWORDS creative AI, multi-modal learning, deep learning ACM Reference format: Naveen Panwar, Shreya Khare, Neelamadhav Gantayat, Rahul Aralika e, Senthil Mani, Anush Sankaran. 2017. mAnI: Movie Amalgamation using Neural Imitation. In Proceedings of Workshop on Machine Learning for Creativity, SIGKDD, Nova Scotia, Canada, August 2017 (ML4Creativity '17), 8 pages. DOI:"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a purely mental game, in which the aim is to find a solution with which to identify."}, {"heading": "2 LITERATURE STUDY", "text": "In fact, the majority of people who are able to sit in the world feel squeezed into the world in which they are unable to live, in which they want to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3 DATASET", "text": "The MovieBook dataset, based on the work of Zhu et al. [18], is of the utmost relevance to our problem. The dataset contains visual clips (roughly a few seconds) from movies, corresponding dialogue text (SRT) for the visual clips, and small sections of book text (approximately 3-10 lines) for 11 different books. A manual alignment is available for a portion of each book, and each alignment is performed using one of the three keywords: (i) visual keyword based on the film clip, (ii) dialogue based on the dialogue spoken during the clip, and (ii) audio keyword based on the audio during this clips. e-properties of this dataset are shown in Table 1. From the collection of 11 book-movie pairs, there are a total of 29, 436 book paragraphs, 19, 985 film recordings, and 16, 909 sets in dialogue subtitles."}, {"heading": "4 PROPOSED APPROACH", "text": "The overall proposed approach consists of three different models and is illustrated in Figure 3. The individual steps and their training sequence are explained in detail in this section."}, {"heading": "4.1 Book Sentence and Movie Dialog Representation", "text": "For each part of the book or a dialogue snippet, a sentence representation model is learned using skip vectors [4], one of the most advanced models for unattended learning of text sequences. The a RNN model encodes the si sentence and two decoders a blank to foresee the si \u2212 1 and si + 1 sentences, depending on the encoding, as shown in Figure 4. Such a model requires tuples of three sentences and can be trained in an unattended manner. Kiros et al. [4] also show that a generic sentence representation model trained on a huge body of books can be used directly in eight different applications without the need for coordination or task adaptation to the previous model of the skipable book."}, {"heading": "4.2 Movie Video Representation", "text": "Presenting videos as embedded vector representation is well researched. In this research, we want to describe a film clip in text so that semantic similarity to the book excerpt could be computed.Captions and video caption techniques could generate a single set of caption for an image or video. Recently, Neural Storyteller 2 conditioned the caption of an RNN on generate2h ps: / / github.com / ryankiros / Neural Storytellera longer history to explain a single image. In this study, we explain the Neural Storyteller style of the model to generate a longer story for a video than for a single image. In the face of a video image, a caption is generated for each image using an encoder decoder tellera model, as suggested in [10]. Conditioned on the combined frame captions, an RNN decoder is a story that explains the entire video material."}, {"heading": "4.3 Extraction through Dialog Model", "text": "In this model, a similarity metric is learned between the book sets and only the dialogue (SRT) in the video, without using the visual content of the film. In view of a book set and a dialogue, their respective representations are calculated using the skipped model. As proposed in [9], \u00ae b. \u00ae d and abs (\u00ae b \u2212 \u00ae d) are calculated and concatenated, and a regression-based semantic similarity model is trained on these representations. [9] Here, the regression model is binary, with the input pair predicted as {Match, Non-Match}. During the test phase, its skipped imaginary representation is calculated for a given book set or a random subject substitution, and semantic relevance is compressed against all available dialogue sets. A list of these dialogue sets above a threshold, t, is shortlisted as relevant film parts that explain the input set."}, {"heading": "4.4 Extraction through Visual Model", "text": "In this model, only the book sets and the video clips are used, while the dialog sets are not used. In a given video clip, a story explaining that video clip is automatically generated using the approach suggested in Section 4.2. In the auto-extracted story, a skipped presentation is extracted, so that both the book set and the video clips are in the same feature area. In this room, the similarity class can be trained and tested in a similar way as explained in Section 4.3."}, {"heading": "4.5 Extraction through Hybrid Model", "text": "To match a book with the corresponding video clip, we use both the video information and the dialog information in this hybrid model. For a given book set, the similarity value for all dialog sets is determined using the dialog model explained in Section 4.3, and the similarity value for all video clips is determined using the visual model explained in Section 4.4. Between the two lists of obtained similarity values, a sum merge is performed, and a threshold is set on the merged score."}, {"heading": "5 EXPERIMENTAL STUDY", "text": "It is only possible to explain the semantic similarity of the model in Section 4.3. Total data is divided between 60% for training, 20% for validation and 20% for testing. To compare with the proposed similarity model, a cosmic distance is used on the basis of the similarity of Tai et al. [9] and on the basis of SICK data."}, {"heading": "6 CONCLUSION AND FUTUREWORK", "text": "We have developed three models to retrieve semantically similar film content from a book snippet: (i) a dialogue model that uses only the dialogue content from the movie, (ii) a visual model that uses only the visual content from the movie, and (iii) a hybrid model that combines both the visual and the dialogue content from the movie. A frame-conditioned LSTM-based decoder is used to generate a single story that explains a movie snippet. Experimental results from the publicly available MovieBook dataset show the effectiveness of the proposed hybrid model, which provides about 80% retrieval accuracy. In the future, we plan to expand this approach by creating creatively animated images and video snippet explaining a book snippet [7]."}], "references": [{"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Video Retrieval System", "author": ["Murray Campbell", "Alexander Haubold", "Ming Liu", "Apostol Natsev", "John R Smith", "Jelena Tesic", "Lexing Xie", "Rong Yan", "Jun Yang"], "venue": "IBM Research TRECVID-", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Je\u0082rey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": "In Proceedings of the IEEE conference on computer vision and pa\u0088ern recognition", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["\u008boc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Contentbased multimedia information retrieval: State of the art and challenges", "author": ["Michael S Lew", "Nicu Sebe", "Chabane Djeraba", "Ramesh Jain"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Don\u2019t just listen, use your imagination: Leveraging visual common sense for non-visual tasks", "author": ["Xiao Lin", "Devi Parikh"], "venue": "In International Conference on Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["Josef Sivic", "AndrewZisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Book2movie: Aligning video scenes with book chapters", "author": ["Makarand Tapaswi", "Martin Bauml", "Rainer Stiefelhagen"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A Uni\u0080ed Framework for Tracking Based Text Detection and Recognition from Web Videos", "author": ["Shu Tian", "Xu-Cheng Yin", "Ya Su", "Hong-Wei Hao"], "venue": "IEEE Transactions on Pa\u0088ern Analysis and Machine Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Learning common sense through visual abstraction", "author": ["Ramakrishna Vedantam", "Xiao Lin", "Tanmay Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Je\u0082rey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In International Conference on Computer", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Uni\u0080ed Framework", "author": ["Ran Xu", "Caiming Xiong", "Wei Chen", "Jason J Corso"], "venue": "In AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Content based lecture video retrieval using speech and video text information", "author": ["Haojin Yang", "Christoph Meinel"], "venue": "IEEE Transactions on Learning Technologies 7,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Beyond short snippets: Deep networks for video classi\u0080cation", "author": ["Joe Yue-Hei Ng", "Ma\u008ahew Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici"], "venue": "In Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In IEEE International Conference on Computer Vision", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "\u008ce publicly available MovieBook [18] dataset contains manually de\u0080ned alignment of 11 movies with their corresponding books.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "\u0087e visuals are obtained from the MovieBook dataset [18].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "\u0087e visuals are obtained from the MovieBook dataset [18].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "Table 1: Statistics for MovieBook dataset [18] with ground-truth for alignment between books and their movie releases.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Textual content and concept based video retrieval has been well explored in the literature [2, 6, 8].", "startOffset": 91, "endOffset": 100}, {"referenceID": 4, "context": "Textual content and concept based video retrieval has been well explored in the literature [2, 6, 8].", "startOffset": 91, "endOffset": 100}, {"referenceID": 6, "context": "Textual content and concept based video retrieval has been well explored in the literature [2, 6, 8].", "startOffset": 91, "endOffset": 100}, {"referenceID": 14, "context": "Yang and Meinel [16] used Optical Character Recognition (OCR) and Automatic Speech Recognition (ASR) to transcribe content from video lectures and perform querying over the extracted content.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "[12] further extended this by tracking textual content across the frames in a video for be\u008aer content generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] learnt a joint text-video embedding model built over independently learnt deep models of language semantic understanding and video embedding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] considered each frame of a video as a word in a sentence and learnt an LSTM network to temporally embed the video.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed a Long-Term Recurrent Convolutional Network (LRCN) model for conditionally embedding the video based on the task to be performed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] learnt a sequence to sequence model to encode a video frame sequence using an LSTM network and decode its corresponding caption using a conditional LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "For understanding large pieces of text, Le and Mikolov [5] extended a word representation word2vec to learn paragraph and document level representation.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "[1] proposed a simple method of averaging the word embeddings over a sentence and modifying it using PCA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] further proposed Book2Movie which aims to align book chapters to its corresponding movie scenes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] aligning books and movies at sentence level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18], the MovieBook dataset is the highly relevant for our problem statement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Given a video frame, an image caption is generated for every frame using an encoder-decoder model as proposed in [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "As proposed in [9], \u00ae b .", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Over these representations, a regression based semantic similarity model is trained [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "[9] and trained on SICK dataset, are used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In future, we plan to extend this approach by creatively generating animated images and video snippets that explains a book snippets [7] [13].", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "In future, we plan to extend this approach by creatively generating animated images and video snippets that explains a book snippets [7] [13].", "startOffset": 137, "endOffset": 141}], "year": 2017, "abstractText": "Cross-modal data retrieval has been the basis of various creative tasks performed by Arti\u0080cial Intelligence (AI). One such highly challenging task for AI is to convert a book into its corresponding movie, which most of the creative \u0080lm makers do as of today. In this research, we take the \u0080rst step towards it by visualizing the content of a book using its corresponding movie visuals. Given a set of sentences from a book or even a fan-\u0080ction wri\u008aen in the same universe, we employ deep learning models to visualize the input by stitching together relevant frames from the movie. We studied and compared three di\u0082erent types of se\u008aing to match the book with the movie content: (i) Dialog model: using only the dialog from the movie, (ii) Visual model: using only the visual content from the movie, and (iii) Hybrid model: using the dialog and the visual content from the movie. Experiments on the publicly available MovieBook dataset shows the e\u0082ectiveness of the proposed models.", "creator": "LaTeX with hyperref package"}}}