{"id": "1708.04968", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Fault in your stars: An Analysis of Android App Reviews", "abstract": "Mobile app distribution platforms such as Google Play Store allow users to share their feedback about downloaded apps in the form of a review comment and a corresponding star rating. Typically, the star rating ranges from one to five stars, with one star denoting a high sense of dissatisfaction with the app and five stars denoting a high sense of satisfaction.", "histories": [["v1", "Wed, 16 Aug 2017 16:37:52 GMT  (136kb,D)", "http://arxiv.org/abs/1708.04968v1", "Under review in CoDS-COMAD 2018. Preprint"]], "COMMENTS": "Under review in CoDS-COMAD 2018. Preprint", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["rahul aralikatte", "giriprasad sridhara", "neelamadhav gantayat", "senthil mani"], "accepted": false, "id": "1708.04968"}, "pdf": {"name": "1708.04968.pdf", "metadata": {"source": "META", "title": "Fault in your stars: An Analysis of Android App Reviews", "authors": ["Rahul Aralika\u008ae", "Giriprasad Sridhara", "Neelamadhav Gantayat", "Senthil Mani"], "emails": ["rahul.a.r@in.ibm.com", "girisrid@in.ibm.com", "neelamadhav@in.ibm.com", "sentmani@in.ibm.com"], "sections": [{"heading": null, "text": "Unfortunately, a user's rating does not match the opinion expressed in the review. Consider, for example, the following rating for the Facebook app on Android: \"Awesome App.\" It is reasonable to assume that the rating for this review is five stars, but the actual rating is one star! Such inconsistent ratings can result in a rating (or rating) of an app that a user can download, as users typically view the average star ratings while making a decision about downloading an app. In addition, app developers receive distorted feedback about the application that does not represent reality. Of particular importance is the rating for small apps with a few thousand downloads, as even a small number of non-matching reviews can drastically lower the average rating. In this work, we conducted a study on this review problem, in which we examined the issue of match between the rating and accuracy of our mobile apps. We manually examined 8600 apps from 810 popular reviews and found that Android were not consistent with the rating file."}, {"heading": "1 INTRODUCTION", "text": "In fact, we are able to outdo and outdo ourselves, \"he said in an interview with the Deutsche Presse-Agentur.\" I am very happy that it has come to this point, \"he said.\" I am very happy that it has come to this point, \"he said in an interview with the Deutsche Presse-Agentur.\" I am very happy that it has come to this point, \"he said."}, {"heading": "2 MOTIVATION", "text": "In this section, we use two methods to motivate our work: \u2022 A survey of end users and developers of Android apps \u2022 A manual comment on reviews of popular Android apps"}, {"heading": "2.1 Motivating Survey", "text": "In our survey, we primarily wanted to know whether users believed that a mobile app would update the rating of stars and the related rating texts shouldmatch, whether an automated system for detecting misdirected ratings would be useful, and whether users would update the star rating when updating the rating text. We hosted the survey questions on Google forms and posted the link on various platforms such as Android forums, mailing lists, bulletin boards at two leading universities in our country and in our organization. No remuneration was paid to any of the respondents. We were not informed of our hypothesis on rating errors. The survey had two branches based on the respondent being an Android app developer or just an end user. The developers had seven questions, while the developers had four questions."}, {"heading": "2.2 Manual Annotation of Reviews", "text": "This year it is more than ever before."}, {"heading": "2.3 Sentiment Analysis Based Rating Prediction is insu cient", "text": "Natural Language Processing, Sentiment Analysis Research is about the automatic analysis of mood expressed in a sentence. Normally, the sentence analyzed is categorized into one of 5 categories, namely a highly negative, negative, neutral, positive and very positive. Intuitively, it seems that you can use sensation techniques on review texts to obtain a category as highly positive and map it to a numerical star rating, 5. However, this approach does not work in practice as described below. Sentiment Score Calculation: For each review in our set of 8600 reviews, we first apply a toolkit from the Stanford NLP to obtain individual sentences. We then applied sentiment analysis to each sentence and calculated the general average sentiment as follows: E ve sentiment categories were mapped on an ordinary scale of 1 to 5, with the category being highly negative to 1 and the category highly positive to 5."}, {"heading": "3 APPROACH", "text": "In this section, we describe our approach to automatically detecting evaluation errors. We use three different approaches, two based on traditional machine learning and one based on deep learning. a manually annotated set of 8,600 reviews from 10 popular apps served as training data for these approaches."}, {"heading": "3.1 Machine Learning", "text": "In fact, most of us are able to survive ourselves without being able to survive ourselves, \"he said in an interview with the Deutsche Presse-Agentur.\" It's not as if we are able to save the world, \"he told the Deutsche Presse-Agentur.\" But it's not that we are able to fix the world, \"he told the Deutsche Presse-Agentur.\" But it's not that we are able to get the world under control. \""}, {"heading": "3.2 Deep Learning", "text": "It is about the question of whether it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about which it is about a way and in which it is about a way, and in which it is about which it is about a way and in which it is about which it is about a way, and in which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which is about which it is about which is about which it is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is"}, {"heading": "4 EVALUATION", "text": "In this section, we describe our review. We designed our review to answer the following research questions (RQ): RQ1: Accuracy of manually annotated data: How accurate is the accuracy of our machine learning and deep learning techniques with manually annotated data? RQ2: Accuracy of wild data: With what accuracy are we able to automatically predict the star rating of a given review text? RQ3: Mismatch Prevalence: How common are the similarities between popular mobile apps for Android?"}, {"heading": "4.1 Metric", "text": "As explained in section 2.2, we categorized the ratings 5-4 as good, rating 3 as neutral and rating 1-2 as bad. We considered our prediction to be accurate because the predicted rating and correct rating fell into the same category. Therefore, our accuracy is given by Equation 8, in which Pi is the predicted rating category, ci is the correct rating category and n is the total number of ratings in the rating data. Note that \"precision / callback\" etc. was not used, as they only make sense if there are several correct answers and a subset of them is returned by an approach. Here, we have only one binary answer as a result (i.e. match / inmatch) and therefore firmly believe that \"accuracy\" is the most appropriate measurement.Accuracy = 1 n \u0432i zi {zi = 1 i f pi = 0 i f pi, ci (8)"}, {"heading": "4.2 RQ1: Accuracy on manually annotated data", "text": "In this context, it should be noted that this is a mere formality."}, {"heading": "4.3 RQ2: Accuracy on data from the wild", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "4.4 RQ3: Mismatch Prevalence", "text": "We answer the question of how common the review ring miscalculations are in terms of the number of reviews in an app. Unfortunately, Google Play Store is not able to download all the reviews for an app (unless it is an app developer), so the next option is to browse the Google Play Store and retrieve the reviews."}, {"heading": "4.5 Replication Package", "text": "Our tool is available at h p: / / mismatch.mybluemix.net, where answers to user surveys, data sets for manual annotations and evaluations are also provided. Furthermore, model parameters for the DCNN are mentioned in detail."}, {"heading": "4.6 reats to Validity", "text": "Our study focuses on Android applications with reviews from the Google Play Store and therefore could not be generalized to other distribution platforms such as the Apple Store. Due to the limitations Google Play Store imposes on downloading all reviews of an app, we inevitably had to evaluate a smaller subset of the most recent 82,234 reviews from 10 popular Android apps. Our results cannot be generalized to all reviews, especially if the review text contains terms that were not previously seen by the model. To mitigate this, we have downloaded as many reviews as possible and plan to evaluate our approach to reviews from the Apple Store in the future. User surveys are generally vulnerable to various threats [6], such as being unrepresentative, having bias and idealistic responses (what people in a survey say can be read from what they actually do in practice). To mitigate these threats, we have tried to ensure that we publish responses from a sample of a representative survey, using many observations across many platforms."}, {"heading": "5 IMPLICATIONS OF OURWORK", "text": "We believe that this is a fundamental work and can be used in several previous research papers such as [2, 16]. Previous research using ratings and ratings assumes that the ratings and ratings are consistent and can therefore be used as they are (i.e., the average rating really represents the experience of end users; a low star rating implies a negative opinion, etc.) Our work suggests that we need to be more careful when dealing with ratings and ratings. (For example, a heuristic to automatically assign negative ratings could assume a low star rating of 1 or 2. However, this heuristic might not be very accurate and would overlook a number of negative ratings that were incorrectly rated with 4 or 5 stars.) In addition, as shown in Table 11, rating by an incongruence of ratings will affect the overall average rating of an app, and this may affect research that uses an average rating [16] to determine the success of a download, with an average correlation between an average rating and strong downloads."}, {"heading": "6 RELATEDWORK", "text": "By and large, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify errors and feature requests [2, 8, 11, 25]. To help developers prioritize devices for testing their app, [12] they examined ratings from different devices for the same app and found that some devices give significantly lower ratings. [4] Dave et al. use information techniques to differentiate between positive and negative product ratings, stating that the performance of their method is due to rating inconsistencies that they label as \"similar qualitative descriptions.\" In the most extreme cases, reviewers do not understand the rating system and give a 1 instead of a 5. \"Our work, on the other hand, deals with the rating problem of inconsistency inconsistency. Fu et al. [7] try to understand why people do not like an app."}, {"heading": "7 CONCLUSION", "text": "We conducted a survey of end users and developers of Android apps. Responses to surveys suggest that: (1) review text and corresponding star ratings should match; (2) it is useful to have an automated system to detect mismatches; (3) end users do not update star ratings when updating their review text; (4) developers believe there are mismatches and general app development.We manually analyzed 8,600 reviews from 10 mobile apps available for Android. These apps include Facebook, Gmail and other popular apps. We found that about 20% of reviews had reviews that did not match the review text. Furthermore, our study suggested that manual analysis of reviews to detect inconsistent reviews can be lengthy and time-consuming, justifying automated solutions."}], "references": [{"title": "Beyond kappa: A review of interrater agreement measures", "author": ["Mousumi Banerjee", "Michelle Capozzoli", "Laura McSweeney", "Debajyoti Sinha"], "venue": "Canadian Journal of Statistics 27,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "AR-miner: Mining Informative Reviews for Developers from Mobile App Marketplace", "author": ["Ning Chen", "Jialiu Lin", "Steven C.H. Hoi", "Xiaokui Xiao", "Boshen Zhang"], "venue": "In Proceedings of the 36th International Conference on So\u0087ware Engineering (ICSE 2014). ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Mining the Peanut Gallery: Opinion Extraction and Semantic Classi\u0080cation of Product Reviews", "author": ["Kushal Dave", "Steve Lawrence", "David M. Pennock"], "venue": "In Proceedings of the 12th International Conference on World Wide Web (WWW \u201903)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Mining the Peanut Gallery: Opinion Extraction and Semantic Classi\u0080cation of Product Reviews", "author": ["Kushal Dave", "Steve Lawrence", "David M. Pennock"], "venue": "In Proceedings of the 12th International Conference on World Wide Web (WWW \u201903)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Selecting Empirical Methods for So\u0089ware Engineering Research", "author": ["Steve Easterbrook", "Janice Singer", "Margaret-Anne Storey", "Daniela Damian"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Why People Hate Your App: Making Sense of User Feedback in a Mobile App Store", "author": ["Bin Fu", "Jialiu Lin", "Lei Li", "Christos Faloutsos", "Jason Hong", "Norman Sadeh"], "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \u201913)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Analysis of User Comments: An Approach for So\u0089ware Requirements Evolution", "author": ["Laura V. Galvis Carre\u00f1o", "Kristina Winbladh"], "venue": "In Proceedings of the  2013 International Conference on So\u0087ware Engineering (ICSE", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "App store mining and analysis: MSR for app stores", "author": ["M. Harman", "Y. Jia", "Y. Zhang"], "venue": "In Mining So\u0087ware Repositories (MSR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Retrieving and Analyzing Mobile Apps Feature Requests from Online Reviews", "author": ["Claudia Iacob", "Rachel Harrison"], "venue": "In Proceedings of the 10th Working Conference on Mining So\u0087ware Repositories (MSR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Prioritizing the Devices to Test Your App on: A Case Study of Android Game Apps", "author": ["Hammad Khalid", "Meiyappan Nagappan", "Emad Shihab", "Ahmed E. Hassan"], "venue": "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of So\u0087ware Engineering (FSE 2014). ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classi\u0080cation", "author": ["Yoon Kim"], "venue": "CoRR abs/1408.5882", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Comparison of Learning Algorithms for Handwri\u008aen Digit Recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bo\u008aou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Mller", "E. Sckinger", "P. Simard", "V. Vapnik"], "venue": "In INTERNATIONAL CONFERENCE ON ARTIFICIAL NEURAL NETWORKS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "API Change and Fault Proneness: A \u008creat to the Success of Android Apps", "author": ["Mario Linares-V\u00e1squez", "Gabriele Bavota", "Carlos Bernal-C\u00e1rdenas", "Massimiliano Di Penta", "Rocco Oliveto", "Denys Poshyvanyk"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of So\u0087ware Engineering (ESEC/FSE", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Tree-based Convolution for Sentence Modeling", "author": ["Mingbo Ma", "Liang Huang", "Bowen Zhou", "Bing Xiang"], "venue": "CoRR abs/1507.01839", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "\u008ce Stanford CoreNLP Natural Language Processing Toolkit. In Association for Computational Linguistics (ACL) System Demonstrations", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "E\u0081cient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Je\u0082rey Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Machine Learning (1 ed.)", "author": ["\u008comas M. Mitchell"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Statistical-Mechanical Analysis of Pre-training and Fine Tuning in Deep Learning", "author": ["M. Ohzeki"], "venue": "Journal of the Physical Society of Japan", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Je\u0082rey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP). 1532\u20131543", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship A\u008aribution", "author": ["S. Ruder", "P. Gha\u0082ari", "J.G. Breslin"], "venue": "ArXiv e-prints (Sept. 2016)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Statistical Genomics: Methods and Protocols. Springer, New York, NY, Chapter Introducing Machine Learning Concepts with WEKA, 353\u2013378", "author": ["Tony C. Smith", "Eibe Frank"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Mining User Opinions in Mobile App Reviews: A Keyword-Based Approach (T)", "author": ["Phong Minh Vu", "Tam \u008ce Nguyen", "Hung Viet Pham", "Tung \u008canh Nguyen"], "venue": "In Proceedings of the 2015 30th IEEE/ACM International Conference on Automated So\u0087ware Engineering (ASE) (ASE \u201915)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Latent Aspect Rating Analysis Without Aspect Keyword Supervision", "author": ["Hongning Wang", "Yue Lu", "ChengXiang Zhai"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \u201911)", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Empirical Evaluation of Recti\u0080ed Activations in Convolutional Network", "author": ["Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li"], "venue": "CoRR abs/1505.00853", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Ma\u008ahew D. Zeiler"], "venue": "CoRR abs/1212.5701", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "\u008ce review comments and star ratings are very important as studies and our survey show that users typically download an app based on these factors [9].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "However, a study [3] and our investigations suggest that o\u0089en the star rating given by a user is not consistent with the opinion expressed in the review comment.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Review rating mismatches can occur due to a variety of reasons; one reason could be that novice end users may simply be confused about the di\u0082erence between one and \u0080ve stars [3].", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "One issue with surveys is that \u201cwhat people say\u201d could be di\u0082erent from \u201cwhat people do\u201d [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "To overcome such issues, typically, a triangulation approach is used to con\u0080rm a survey\u2019s \u0080ndings [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "We measured the agreement among the annotators through the Fleiss\u2019s Kappa, a standard inter-annotator agreeent measure when there are multiple annotators [1].", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "7 indicating a substantial level of agreement [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 15, "context": "Sentiment Score Calculation: For each review in our set of 8600 reviews, we \u0080rst applied a natural language tokenizer from the Stanford NLP toolkit [18] to obtain individual sentences.", "startOffset": 148, "endOffset": 152}, {"referenceID": 17, "context": "Due to space constraints, we do not provide details of these classi\u0080ers, but interested readers can refer to [20] for more details.", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "RNNs with LSTM units [10] have become the defacto standard for unsupervised extraction of features from text.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "However, recently CNNs have been used to get state of the art results on problems involving small pieces of text [23].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "CNN [15] is a type of feedforward arti\u0080cial neural network whose simplest form consists of 2 types of layers; the convolutional layer and the pooling layer.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "Let xi \u2208 Rd be a d-dimensional representation of a word (can either be a one-hot representation or word2vec [19]).", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "DCNN is similar to the model proposed in [13], but it also considers the ancestor and sibling words in a sentence\u2019s dependency tree.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "where \u03c6 is a non-linearity such as tanh or ReLu [27].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": ": When using CNNs with text data, we use max-over-time pooling [13] to get the maximum activation over the feature map c .", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "\u008cis is also called \u2018max-over-tree\u2019 pooling [17].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "\u008ce model we built is similar to what is done in [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "95 which was decayed using adadelta update rule [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "8 [24].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "We then use 100 dimensional glove vectors [22] pre-trained on Wikipedia-14 and Gigaword 5 datasets to represent each word in our vocabulary.", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": ", what would be the accuracy of our approach when applied to non Android app reviews? We gathered 1000 random product reviews apiece from the publicly available Amazon MP3 and Trip Advisor datasets [26] and applied our DCNN model to obtain star ratings for these reviews.", "startOffset": 198, "endOffset": 202}, {"referenceID": 18, "context": "\u008cis shows that, our model works well on di\u0082erent domains even without any \u0080ne tuning [21] and being trained on a relatively small dataset of an entirely di\u0082erent domain (8600 Android app reviews).", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "User surveys are generally prone to various threats[6] such as being unrepresentative, exhibiting bias and idealistic responses (what people say in a survey can be di\u0082erent from what they actually do in practice).", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "We believe this is a foundational work and can be used in several prior research works such as [2, 16].", "startOffset": 95, "endOffset": 102}, {"referenceID": 13, "context": "We believe this is a foundational work and can be used in several prior research works such as [2, 16].", "startOffset": 95, "endOffset": 102}, {"referenceID": 13, "context": "Further as shown in Table 11, review-rating mismatch will a\u0082ect the overall average rating of an app and this can impact research that uses average rating [16] to determine success of an app.", "startOffset": 155, "endOffset": 159}, {"referenceID": 7, "context": "\u008cere is a strong correlation between average rating and downloads [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 6, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 9, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 22, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 10, "context": "To help developers prioritize the devices to test their app, [12] examined reviews from di\u0082erent devices for the same app and found that some devices gave signi\u0080cantly lower ratings.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "[4] use information retrieval techniques to distinguish between positive and negative product reviews.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] try to understand why people might dislike an app.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Mobile app distribution platforms such as Google Play Store allow users to share their feedback about downloaded apps in the form of a review comment and a corresponding star rating. Typically, the star rating ranges from one to \u0080ve stars, with one star denoting a high sense of dissatisfaction with the app and \u0080ve stars denoting a high sense of satisfaction. Unfortunately, due to a variety of reasons, o\u0089en the star rating provided by a user is inconsistent with the opinion expressed in the review. For example, consider the following review for the Facebook App on Android; \u201cAwesome App\u201d. One would reasonably expect the rating for this review to be \u0080ve stars, but the actual rating is one star! Such inconsistent ratings can lead to a de\u0083ated (or in\u0083ated) overall average rating of an app which can a\u0082ect user downloads, as typically users look at the average star ratings while making a decision on downloading an app. Also, the app developers receive a biased feedback about the application that does not represent ground reality. \u008cis is especially signi\u0080cant for small apps with a few thousand downloads as even a small number of mismatched reviews can bring down the average rating drastically. In this paper, we conducted a study on this review-rating mismatch problem. We manually examined 8600 reviews from 10 popular Android apps and found that 20% of the ratings in our dataset were inconsistent with the review. Further, we developed three systems; two of which were based on traditional machine learning and one on deep learning to automatically identify reviews whose rating did not match with the opinion expressed in the review. Our deep learning system performed the best and had an accuracy of 92% in identifying the correct star rating to be associated with a given review. In another evaluation, we asked 23 end users to write reviews for any 5 apps that they had used recently. We got 115 reviews from 66 di\u0082erent mobile apps. Our deep learning system had an accuracy of 87%. Further, our study suggests that this problem is quite prevalent among apps. Across the ten apps used in our study, the mismatch percentage ranged from 16% to 26%.", "creator": "LaTeX with hyperref package"}}}