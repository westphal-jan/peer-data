{"id": "1708.04983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Visualizing and Exploring Dynamic High-Dimensional Datasets with LION-tSNE", "abstract": "T-distributed stochastic neighbor embedding (tSNE) is a popular and prize-winning approach for dimensionality reduction and visualizing high-dimensional data. However, tSNE is non-parametric: once visualization is built, tSNE is not designed to incorporate additional data into existing representation. It highly limits the applicability of tSNE to the scenarios where data are added or updated over time (like dashboards or series of data snapshots).", "histories": [["v1", "Wed, 16 Aug 2017 17:17:56 GMT  (1630kb,D)", "http://arxiv.org/abs/1708.04983v1", "44 pages, 24 figures, 7 tables, planned for submission"]], "COMMENTS": "44 pages, 24 figures, 7 tables, planned for submission", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andrey boytsov", "francois fouquet", "thomas hartmann", "yves letraon"], "accepted": false, "id": "1708.04983"}, "pdf": {"name": "1708.04983.pdf", "metadata": {"source": "CRF", "title": "Visualizing and Exploring Dynamic High-Dimensional Datasets with LION-tSNE", "authors": ["Andrey Boytsov", "Francois Fouquet", "Thomas Hartmann"], "emails": ["name.surname@uni.lu"], "sections": [{"heading": null, "text": "In this paper, we propose LION-tSNE (Local Interpolation with Outlier coNtrol) - a novel approach to integrating new data into the tSNE representation. LION-tSNE is based on local interpolation near training data, outlier detection, and a special outlier mapping algorithm. We show that the LION-tSNE method is robust for outliers as well as for new samples from existing clusters. We also discuss several possible improvements for specific cases. We compare LION-tSNE with a comprehensive list of possible benchmark approaches that include multiple interpolation techniques, gradient descend for new data, and neural network adjustments. Keywords: tSNE, visualization, exploration, dimensionality reduction, embedding, interpolation, inverse distance weighting, approximation, gradient descend, neural networks."}, {"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is not a country, but a country in which it is a country, a country in which it is a city and a country, a country, a city and a country, a country and a country, a country and a country, a country, a country, a country, a city and a country, a country, a country, a country and a country, a country and a country, a country and a country, a country, a country and a country, a country, a country, a country and a country, a country, a country and a country, a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country,"}, {"heading": "2 T-Distributed Stochastic Neighbor Embedding", "text": "It is only a brief explanation of the SNtE algorithms. Please, refer to the asymmetric nature of the Kullback-Leibler divergence."}, {"heading": "3 Related Work", "text": "This year it is more than ever before."}, {"heading": "4 LION-tSNE Algorithm", "text": "It is only a matter of time before there is a solution to this problem until there is an embedding, and the influence of distant points can significantly reduce performance. This problem is applicable to all benchmark methods, while it is mainly visible in the inverse distance evaluation of interpolation. - One solution to this problem is to ensure that the embedding of new examples should only depend on within a certain radius.Outlier detection and localization can, of course, lead to a single approach: we can use local interpolation when we have a certain radius.Outlier detection and locality within a certain radius.Outlier detection and locality can, of course, combine in a single approach: we can use local interpolation when we use a new sample x."}, {"heading": "4.1 Outlier Placement", "text": "The main objective of the outliers is the following: If a new sample x is an outlier, its mapping y should be perceived as an outlier in the visualization. (To place an outlier, we must find a position for y so that there are no neighbors in a given radius.) The use of something like y ymax for each dimension of the y cells is not an option, while a new sample too far away from existing values can make visualization more difficult. Radius ry is a parameter of the algorithm. It can set outlier points too far from the existing area and reduce the readability of the visualization, while it can make clusters and outliers indistinguishable. Radius ry can be based on ryNN - some percentiles of the distribution of the closest neighboring distances in the y space, e.g. 95 or 99. Percentiles depending on the percentage of original training points we assume to be outliers; or Nrymax can be set to better distance between neighbors."}, {"heading": "4.2 Selecting Power Parameter", "text": "In fact, the fact is that most of us are able to outdo ourselves. (...) It is only a matter of time before such a situation occurs. (...) It is a matter of time before such a situation occurs. (...) It is a matter of time before such a situation occurs. (...) It is a matter of time before it occurs. (...) It is a matter of time before it occurs. (...) It is a matter of time before it occurs. (...) It is a matter of time before it occurs. (...) It is a matter of time before it occurs. (...) It is a matter of time before it occurs. (...) It is a matter of time before it occurs. (...) It is a matter of time until it occurs. (...) It is a matter of time until it occurs. (...) It is a matter of time until it occurs. (...) It is a matter of time until it occurs. (...) It is a matter of time until it occurs."}, {"heading": "5 Benchmark Methods", "text": "There are several important approaches to solving this problem. (One possible approach is to use a new sample formula for the training matrix X, and then run on until a new minimum cost is reached.) The option is to use old values of the training matrix again to find new embedding methods that correspond to the last rate. (2) The option is to maintain the old values of the training matrix or recalculate them. (3) We will find new embedding methods by minimizing the KL divergence (formula 6). In the order of the original yi points to stay in place, the gradient KL (P-Q) / will be shortened."}, {"heading": "1 hidden layers, 500 nodes; tanh activation unit; no regularization. It", "text": "While it is probably not the best model selection, it will be interesting to use for benchmark benchmarks. (For most generic neural network configurations, we have purposely used most generic network configurations suitable for each type of dataset.) Other specialized solutions can be selected for specific cases. (For example, if input x is an image, we can determine the correct neural networks for all datasetries [16]. In addition, we have decided to compare LION-tSNE with several methods described in related work. (One of them is kernelized tSNE, developed by Gisbrecht et. al [11] The mapping is interpolated using function 11. The parameters are determined as a distance from xi to the nearest neighbors, then multiplied by some coefficients that the system can be efficient."}, {"heading": "6 Evaluation", "text": "To be useful, the mapping method should maintain the local structure of the data, just like the original tSNE representation. There are two aspects of preserving the structure when adding new data: \u2022 If a new data point x belongs to a structure in the original space and this structure is successfully represented in tSNE, then the point x should belong to the embedding of this structure. For example, if a new handwritten number is added to visualize MNIST records, its embedding should also belong to the cluster of the same digits. We will show that most interpolation and approximation methods handle this problem relatively well. \u2022 If a new data point x does not belong to any structure, its embedding should not belong to any structure in the y space. For example, if we try to add a data point with noise to the visualization of MNIST records, the embedding of a new point should not belong to any cluster of handwritten digits. \u2022 If a new data point x does not belong to any structure in the y space, its embedding should not belong to any structure in the y space."}, {"heading": "6.1 Cluster attribution test", "text": "rE \"s tis rf\u00fc rf\u00fc ide rf\u00fc ide rf\u00fc rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu"}, {"heading": "6.2 Outlier Test", "text": "Dre rf\u00fc nde eeisrrVnlrteeerterrrrr rf\u00fc ide eeisrteeVnlrrrrrteeoiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui"}, {"heading": "6.3 Increasing Data Set Size and Dimensionality", "text": "This section aims to evaluate the basic truth for LION-tSNE when increasing the size and dimensionality of the dataset."}, {"heading": "6.4 Complexity Analysis", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the"}, {"heading": "7 Discussion", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "8 Conclusion and Future Work", "text": "Some possible improvements and future work possibilities have already been mentioned in Section 7. However, there are several important future work directions that have not been mentioned before, and these can have a big impact on the performance of LION-tSNE. Perhaps the most effective future work direction is the use of the fast search for the nearest neighbors. The simple approach uses O (N) comparisons to determine neighbors in a certain radius around the new sample x. It can be a problem if N is large. Finding the nearest neighbors in a certain fixed radius is a recognized problem for itself [3]. Also, finding the next fixed radius can be done with fast K search techniques for the nearest neighbors [8] with sufficient number of neighbors and additional verification if the distance is small enough. Another option is the search for the orthogonal radius [4], i.e. the search for neighbors in a box rather than in a sphere. And of course, applying LION-tSNE to a wide variety of practical tasks can help identify further scope for improving the algorithm."}, {"heading": "Acknowledgments", "text": "This research was supported by BGL BNP Paribas and the Alphonse Weicker Foundation. We thank Anne Goujon and Fabio Nozza of BGL BNP Paribas for their support."}], "references": [{"title": "Data-driven identification of prognostic tumor subpopulations using spatially mapped t-SNE of mass spectrometry imaging data", "author": ["Walid M. Abdelmoula", "Benjamin Balluff", "Sonja Englert", "Jouke Dijkstra", "Marcel J.T. Reinders", "Axel Walch", "Liam A. McDonnell", "Boudewijn P.F. Lelieveldt"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A Survey of Techniques for Fixed Radius Near Neighbor Searching", "author": ["Jon L Bentley"], "venue": "Technical report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Computational Geometry: Algorithms and Applications", "author": ["Mark de Berg", "Otfried Cheong", "Marc van Kreveld", "Mark Overmars"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Radial Basis Functions: Theory and Implementations", "author": ["Martin D. Buhmann"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Gait recognition based on DWT and t-SNE", "author": ["Linlin Che", "Yinghui Kong"], "venue": "In Third International Conference on Cyberspace Technology (CCT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "The Use of Faces to Represent Points in K-Dimensional Space Graphically", "author": ["Herman Chernoff"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1973}, {"title": "k-Nearest neighbour classifiers", "author": ["Padraig Cunningham", "Sarah Jane Delany"], "venue": "Multiple Classifier Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Dimensionality reduction techniques to analyze heating systems in buildings", "author": ["Manuel Dom\u00ednguez", "Seraf\u00edn Alonso", "Antonio Mor\u00e1n", "Miguel A. Prada", "Juan J. Fuertes"], "venue": "Information Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Out-of-sample kernel extensions for nonparametric dimensionality reduction", "author": ["Andrej Gisbrecht", "Wouter Lueks", "Bassam Mokbel", "Barbara Hammer"], "venue": "ESANN", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Parametric nonlinear dimensionality reduction using kernel t-SNE", "author": ["Andrej Gisbrecht", "Alexander Schulz", "Barbara Hammer"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Neural Networks and Learning Machines", "author": ["Simon S. Haykin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Stochastic Neighbor Embedding", "author": ["Geoffrey E Hinton", "Sam T. Roweis"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["Harold Hotelling"], "venue": "Journal of educational psychology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1933}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Application of t-SNE to Human Genetic Data. bioRxiv", "author": ["Wentian Li", "Jane E. Cerise", "Yaning Yang", "Henry Han"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Learning a Parametric Embedding by Preserving Local Structure", "author": ["Laurens Maaten"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Visualizing Data using t- SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Analysis of Electricity Consumption Profiles by Means of Dimensionality Reduction Techniques", "author": ["Antonio Mor\u00e1n", "Juan J. Fuertes", "Miguel A. Prada", "Seraf\u00edn Alonso", "Pablo Barrientos", "Ignacio D\u00edaz"], "venue": "In Engineering Applications of Neural Networks, Communications in Computer and Information Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "m- TSNE: A Framework for Visualizing High-Dimensional Multivariate Time Series. In VAHC2016 Workshop on Visual Analytics in Healthcare in conjunction with AMIA", "author": ["Minh Nguyen", "Sanjay Purushotham", "Hien To", "Cyrus Shahabi"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "author": ["N. Pezzotti", "B.P.F. Lelieveldt", "L. v d Maaten", "T. H\u00f6llt", "E. Eisemann", "A. Vilanova"], "venue": "IEEE Transactions on Visualization and Computer Graphics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Visualizing time-dependent data using dynamic t-SNE", "author": ["Paulo E. Rauber", "Alexandre X. Falc\u00e3o", "Alexandru C. Telea"], "venue": "Proc. EuroVis Short Papers,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "A Nonlinear Mapping for Data Structure Analysis", "author": ["J.W. Sammon"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1969}, {"title": "A Two-dimensional Interpolation Function for Irregularlyspaced Data", "author": ["Donald Shepard"], "venue": "In Proceedings of the 1968 23rd ACM National Conference,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1968}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1958}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science (New York, N.Y.),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}], "referenceMentions": [{"referenceID": 13, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 123, "endOffset": 127}, {"referenceID": 5, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 24, "context": "in the data: those can be inter-point distances [27], outliers [7], linear subspaces [15], and more.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "in the data: those can be inter-point distances [27], outliers [7], linear subspaces [15], and more.", "startOffset": 63, "endOffset": 66}, {"referenceID": 13, "context": "in the data: those can be inter-point distances [27], outliers [7], linear subspaces [15], and more.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "T-distributed stochastic neighbor embedding (tSNE) [20] is a very popular prize-winning [1] algorithm for dimensionality reduction.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 113, "endOffset": 117}, {"referenceID": 4, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 130, "endOffset": 133}, {"referenceID": 18, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 162, "endOffset": 166}, {"referenceID": 7, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 184, "endOffset": 187}, {"referenceID": 0, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 210, "endOffset": 213}, {"referenceID": 16, "context": "While there were some attempts to extend tSNE to incorporate new data [19, 11], a large amount of possible approaches remains unexplored, and there was no attempt to create a systematic comparative study.", "startOffset": 70, "endOffset": 78}, {"referenceID": 9, "context": "While there were some attempts to extend tSNE to incorporate new data [19, 11], a large amount of possible approaches remains unexplored, and there was no attempt to create a systematic comparative study.", "startOffset": 70, "endOffset": 78}, {"referenceID": 17, "context": "Hinton [20].", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "TSNE itself is based on SNE algorithm [14] by G.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "Please, refer to original articles [20] and [14] for more details and explanations.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Please, refer to original articles [20] and [14] for more details and explanations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Among many other use cases, MNIST was used as one of the benchmarks for the original tSNE algorithm [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "Then the variances sigmai can be calculated using root finding techniques (the authors of tSNE [20] suggested binary search).", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "The authors of original tSNE algorithm [20] suggest several improvements for gradient descent to avoid poor local optima.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "van der Maaten [19], the author of the original tSNE algorithm, addressed the problem of building f : X \u2192 Y representation for tSNE.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "al [11] proposed kernelized tSNE - parametric tSNE based on normalized Gaussian kernels.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "This approach is close to RBF interpolation [5] and also has some resemblance to IDW interpolation [28].", "startOffset": 44, "endOffset": 47}, {"referenceID": 25, "context": "This approach is close to RBF interpolation [5] and also has some resemblance to IDW interpolation [28].", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Earlier version of the same method was also proposed in [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "[24] proposed A-tSNE - an improvement over tSNE for progressive visual analytics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] also included procedure for removing and adding point to tSNE based", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] proposed mTSNE algorithm, where the authors created single visualization for the entire set of time series, using time series distance metrics to produce P matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "For local interpolation here we use inverse distance weighting (IDW) [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "RBF interpolation [5] and some approximation methods are plausible substitutes of IDW in LION-tSNE.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "Buhmann [5] provides a thorough introduction to radial basis functions.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "it accepts multidimensional input, but depends only on distance between the input argument and some reference point [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Coefficients \u03bbi can be found by solving the system of linear equations (SLE) 10, and under some conditions invertibility of SLE matrix is guaranteed [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 10, "context": "For that purpose neural networks [12] are a natural choice.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "\u2022 For activation function we tested rectified linear units (ReLu, see [22]) ReLu(x) = max(0, x) and hyperbolic tangent tanh(x) = e x\u2212e\u2212x ex+e\u2212x .", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "\u2022 In order to avoid overfitting, we used dropout regularization [29].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "For example, if input x is an image, convolutional neural networks [16] might be fitting choice.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "al [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Also we compare LION-tSNE with parametric tSNE approach described by van der Maaten [19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "RBM are undirected multilayer models used, among other applications, as the earliest deep learning models (see [13]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "al [11] with K equal to 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "\u2022 Parametric tSNE as described by van der Maaten [19], using 60000 samples of non-processed 784-dimensional tSNE and 3 hidden layers of restricted Boltzmann machines of 500, 500 and 200 nodes per layer respectively.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "Kernelized tSNE described in [11] showed near-baseline accuracy for properly chosen K parameters.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "Kernelized tSNE [11] showed excellent accuracy on previous test, but in this test it placed most outliers firmly within existing clusters (see figure 20).", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "One more prominent method, to which LION-tSNE was not yet compared, is parametric tSNE [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "RBMs are the earliest version of deep learning models [13], and they often require a lot of data to be trained.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "The next section describes application of LIONtSNE to larger and higher-dimensional datasets using parametric tSNE [19] as", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "This layer configuration was used for evaluation by parametric tSNE author [19], source code of parametric tSNE and the training/test dataset was made available by the original algorithm author.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "Straightforward implementation of tSNE algorithm is has quadratic complexity in number of points [20] (if dimensionality is taken into account, O(NK)), and it is the upper bound of repeated gradient descent complexity.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "Neural network approximation and restricted Boltzmann machines for parametric tSNE [19] require upfront training using backpropagation.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "- Using tSNE time series visualization approaches (like points on a single tSNE plot [23] or like a series of plots [25]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "- Using tSNE time series visualization approaches (like points on a single tSNE plot [23] or like a series of plots [25]).", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Search for nearest neighbors in a certain fixed radius is a recognized problem on its own [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "Also fixed radius nearest neighbors search can be approximated with fast K nearest neighbors search techniques [8] with sufficient number of neighbors and additional check whether the distance is small enough.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Another option is to use orthogonal range search [4], i.", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "T-distributed stochastic neighbor embedding (tSNE) is a popular and prize-winning approach for dimensionality reduction and visualizing highdimensional data. However, tSNE is non-parametric: once visualization is built, tSNE is not designed to incorporate additional data into existing representation. It highly limits the applicability of tSNE to the scenarios where data are added or updated over time (like dashboards or series of data snapshots). In this paper we propose, analyze and evaluate LION-tSNE (Local Interpolation with Outlier coNtrol) a novel approach for incorporating new data into tSNE representation. LION-tSNE is based on local interpolation in the vicinity of training data, outlier detection and a special outlier mapping algorithm. We show that LION-tSNE method is robust both to outliers and to new samples from existing clusters. We also discuss multiple possible improvements for special cases. We compare LION-tSNE to a comprehensive list of possible benchmark approaches that include multiple interpolation techniques, gradient descent for new data, and neural network approximation.", "creator": "LaTeX with hyperref package"}}}