{"id": "1708.05045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding", "abstract": "Entity alignment is the task of finding entities in two knowledge bases (KBs) that represent the same real-world object. When facing KBs in different natural languages, conventional cross-lingual entity alignment methods rely on machine translation to eliminate the language barriers. These approaches often suffer from the uneven quality of translations between languages. While recent embedding-based techniques encode entities and relationships in KBs and do not need machine translation for cross-lingual entity alignment, a significant number of attributes remain largely unexplored. In this paper, we propose a joint attribute-preserving embedding model for cross-lingual entity alignment. It jointly embeds the structures of two KBs into a unified vector space and further refines it by leveraging attribute correlations in the KBs. Our experimental results on real-world datasets show that this approach significantly outperforms the state-of-the-art embedding approaches for cross-lingual entity alignment and could be complemented with methods based on machine translation.", "histories": [["v1", "Wed, 16 Aug 2017 19:30:17 GMT  (898kb)", "http://arxiv.org/abs/1708.05045v1", null], ["v2", "Tue, 26 Sep 2017 02:06:08 GMT  (898kb)", "http://arxiv.org/abs/1708.05045v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.DB", "authors": ["zequn sun", "wei hu", "chengkai li"], "accepted": false, "id": "1708.05045"}, "pdf": {"name": "1708.05045.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding", "authors": ["Zequn Sun", "Wei Hu", "Chengkai Li"], "emails": ["zqsun.nju@gmail.com,", "whu@nju.edu.cn", "cli@uta.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.05 045v 1 [cs.C L] 1Keywords: lingual entity alignment, knowledge data embedding, common attribute preserving embedding"}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Related Work", "text": "We divide this work into two parts: the embedding of KB and the linguistic orientation of KB, which we will discuss later in this section."}, {"heading": "2.1 KB Embedding", "text": "TransE [1], the pioneer of translation-based methods, interprets a relationship vector as a translation from the head-to-entity vector to its tail-to-entity vector. In other words, if a triple relationship (h, r, t) persists, h + r \u2248 t is to be expected. TransE has demonstrated its great ability to model 1-to-1 relationships and has achieved promising results for the completion of KB. To further improve TransE, later work has been proposed, including TransH [22] and TransR [15]. In addition, there are some non-translation-based approaches to embedding KB [2,18,20]. In addition, several studies use knowledge in KB to improve embedding. Krompa\u00df et al. [13] added that type limitations on KB [embedding models and improving their performance on linkage prediction."}, {"heading": "2.2 Cross-lingual KB Alignment", "text": "Existing work on the translingual alignment of KB generally falls into two categories: translingual ontology matching and translingual alignment of entities. For translingual ontology matching, Fu et al. [8,9] presented a generic framework that uses machine translation tools to translate labels into the same language, and uses monolingual ontological matching methods to find mappings. Spohr et al. [21] used translation-based label similarities and ontology structures as characteristics for learning translingual mapping functions through machine learning techniques (e.g. SVM). In all of these works, machine translation is an integral component. For the translingual alignment of entities, MTransE [5] integrates KB structures into language-specific vector spaces and designs five alignment models to achieve translations between Bs in different languages with the alignment of sound."}, {"heading": "3 Cross-lingual Entity Alignment via KB Embedding", "text": "In this section, we will first present notations and the general framework of our common attribute-conserving embedding model. We will then go into the technical details of the model and discuss several key design issues. We will use lowercase bold letters to denote the vector representations of the corresponding terms, e.g. (h, r, t) the vector representation of triple (h, r, t). We will use bold capital letters to denote matrices, and we will use uppercase letters to denote various KBs. For example, E (1) denotes the representation matrix for units in KB1 in which each line is an entity vector e (1)."}, {"heading": "3.1 Overview", "text": "The framework of our common attribute embedding model is shown in the figure. As two KBs designated by KB1 and KB2 are embedded in different natural languages and some pre-aligned entities.According to TransE [1], we interpret a relationship as a translation from the head to characterize the structural information of KB2 and expect the latently aligned entities to be embedded nearby."}, {"heading": "3.2 Structure Embedding", "text": "The goal of SE is to model the geometric structures of two CBs and to learn approximate representations for latently aligned triplets. Formally, we expect a triple difference (h, r, t). To measure the plausibility of tr, we define the score function f (tr) = 0 h + r \u2212 t \u00b2 2. We prefer a lower value of f (tr) and want to minimize it for each triplex relationship. Figure 2 gives an example of how the geometric structures of two CBs with seed alignment. In phase (1) we initialize all vectors randomly and let each pair in seed alignment build overlaps to show the triplex relationship intuitively in the figure, we consider a unit as a point in vector space and motion relationships to start from their heads."}, {"heading": "3.3 Attribute Embedding and Entity Similarity Calculation", "text": "We list a number of attributes that are correlated when used together to describe a unit. For example: attributes length, latitude and place name are correlated because they are commonly used to describe a place. Furthermore, we want to assign a higher correlation to the pair of longitude and latitude because they have the same range. We use seed pairs to establish correlations between cross-border attributes. To capture the correlations of attributes, we consider the attributes of e (1) to be correlated for each attribute of e (2), and vice versa. We expect attributes with high correlations to be embedded. To capture the correlations of attributes, AE borrows the idea of Skipgram [16], a very popular model that learns word embedding by predicting the context of a word itself."}, {"heading": "3.4 Joint Attribute-Preserving Embedding", "text": "Inspired by [25], we use matrices of pairwise similarities between entities as monitored information and minimize the following objective function: OS = E (1) SE \u2212 S (1,2) E (2) SE \u2212 2F + \u03b2 (1) SE \u2212 S (1) E (1) SE \u2212 2 F + E (2) E (2) E (2) SE \u2212 2 F), (5) where \u03b2 is a hyperparameter that balances similarities between KBs and their internal similarities. ESE-R \u00b7 d denotes the matrix of entity vectors for one KB in SE with each row an entity vector. S (1,2) E (2) SE calculates latent vectors of entities in KB1 by accumulating vectors of entities in KB2 based on their similarities."}, {"heading": "3.5 Discussions", "text": "We discuss and analyze our common attributes, which are based on the ranking, in the following areas: \"It is not like if.\" \"It is not like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"\" It is. \"\" \"It is.\" \"\" It is like. \"\" It is. \"\" It is like if. \"\" \"It is like if.\" \"\" \"It is like if.\" \"\" \"It is like if.\" \"\" \"It is.\" \"\" It is. \"\" \"It is.\" \"\" It is. \"\" \"It is.\" \"\" It is. \"\" \"\" It is. \"\" \"\" It. \"\" \"\" \"It...................\" \"\" \"\" \"It.......\" \"\" \"\" \"It..\" \"\" \"\" It. \"\" \"\" \"It.\" \"\" \"\" \"It..\" \"\" \"\" \"It..\" \"\" \"\" \"\" \"It...\" \"\" \"\" \"\" \"\" It........... \"\" \"\" \"\" \"\" It............. \"\" \"\" \"\" \"\" \"\" \"It..............\" \"\" \"\" \"\" \"\" \".......\" \"\" \"\" \"It.............\" \"\" \"\" \"\" \"\" \"\"......... \"\" \"\" \"\" \"\"... \"\" \"\" \"\" \"\" \".......\" \"\" \"\" \"\" \"...........\" \"\" \"\" \"\"... \"\" \"\" \".............\" \"\" \"\" \"\" \"\" \"\" \"\" \"..........\""}, {"heading": "4 Evaluation", "text": "In this section we report on our experiments and results with cross-language data sets from the real world. We developed our approach called JAPE using TensorFlow4 - a very popular open source software library for numerical calculations. Our experiments were conducted on a personal workstation with an Intel Xeon E3 3.3 GHz CPU and 128 GB of memory. Data sets, source code and experimental results are available on this website5."}, {"heading": "4.1 Datasets", "text": "We selected DBpedia (2016-04) to build three lingual datasets. DBpedia is a large multilingual KB containing interlanguage links (ILLs) from entities with English versions to those in other languages. In our experiments, we extracted 15,000 ILLs with popular entities from English, Chinese, Japanese, and French, and considered them to be our reference alignment (i.e. gold standards). Our strategy for extracting datasets is to randomly select an ILL pair. The entities involved have extracted at least four relationship triples and then relationships and attribute infobox triples for selected entities. Statistics of the three datasets are listed in Table 1, which indicates that the number of entities involved in each language is much greater than 15,000, and attribute triples contribute to a significant share of the datasets.4 soft: https / https / / sortensorte.APgips / httg.nflor.com /.nflor.com"}, {"heading": "4.2 Comparative Approaches", "text": "As already mentioned, JE [11] and MTransE [5] are two representative, embedded methods for aligning units. In our experiments, we have made our best efforts to implement the two models, as they currently do not publish source code or software, and we performed them using the above data sets as comparative approaches. MTransE has five variants in its alignment model in particular, the fourth of which performs best according to the experiments of its authors. Therefore, we chose this variant to represent MTransE. We followed the implementation details in [5,11] and added other unreported details with careful consideration. For example, we added a strong orthogonal restriction to the linear transformation matrix in MTransE to ensure invertibility, because we found that it yields better results. For JAPE, we matched various parameter values and set d = 75, \u03b1 = 0.1, \u03b2 = 0.05, GP = 0.05 for the best learning rates of iSE and iSE."}, {"heading": "4.3 Evaluation Metrics", "text": "In accordance with conventions [1,5,11], we used Hits @ k and Mean to evaluate the performance of the three approaches. Hits @ k measures the proportion of correctly aligned units at the top k, while Mean calculates the mean of these ranks. A higher Hits @ k and a lower mean indicate better performance. It is noteworthy that the optimal Hits @ k and Mean in all three approaches do not usually come from the same era. To make a fair comparison, we did not specify the number of epochs, but used early interruptions to avoid over-training, and the training process is stopped as long as the change ratio of Mean is less than 0.0005. Furthermore, training AE on each record takes 100 epochs."}, {"heading": "4.4 Experimental Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we presented a common attribute-preserving embedding model for cross-lingual entity alignment. We proposed structure embedding and attribute embedding to illustrate the relationship structures and attribute correlations of KBs and to learn approximate embedding for latent aligned units. Our experiments with real data sets showed that our approach achieved better results than two state-of-the-art embedding approaches and could be supplemented by conventional methods based on machine translation. In future work, we look forward to improving our approach in several aspects. Firstly, structure embedding suffered from multilingual mapping relationships, so we plan to extend it by cross-lingual hyperplane projection. Secondly, we would like to incorporate our attribute embedding of discarded attribute values due to their diversity and cross-linguality, which we would like to evaluate lingual word embedding techniques from Wikipedia 66."}], "references": [{"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "NIPS. pp. 2787\u20132795", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "AAAI. pp. 301\u2013306", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "String similarity metrics for ontology alignment", "author": ["M. Cheatham", "P. Hitzler"], "venue": "Alani, H., et al. (eds.) ISWC. pp. 294\u2013309", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["D. Chen", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "arXiv:1301.3618", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-lingual knowledge graph embeddings for cross-lingual knowledge alignment", "author": ["M. Chen", "Y. Tian", "M. Yang", "C. Zaniolo"], "venue": "IJCAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12(7), 2121\u2013 2159", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "English-chinese knowledge base translation with neural network", "author": ["X. Feng", "D. Tang", "B. Qin", "T. Liu"], "venue": "COLING. pp. 2935\u20132944", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-lingual ontology mapping \u2013 an investigation of the impact of machine translation", "author": ["B. Fu", "R. Brennan", "D. O\u2019Sullivan"], "venue": "G\u00f3mez-P\u00e9rez, A., et al. (eds.) ASWC. pp. 1\u201315", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Cross-lingual ontology mapping and its use on the multilingual semantic web", "author": ["B. Fu", "R. Brennan", "D. O\u2019Sullivan"], "venue": "WWW Workshop on Multilingual Semantic Web. pp. 13\u201320", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Entity matching on web tables : a table embeddings approach for blocking", "author": ["A.L. Gentile", "P. Ristoski", "S. Eckel", "D. Ritze", "H. Paulheim"], "venue": "EDBT. pp. 510\u2013513", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "A joint embedding method for entity alignment of knowledge bases", "author": ["Y. Hao", "Y. Zhang", "S. He", "K. Liu", "J. Zhao"], "venue": "Chen, H., et al. (eds.) CCKS. pp. 3\u201314", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving neural knowledge base completion with cross-lingual projections", "author": ["P. Klein", "S.P. Ponzetto", "G. Glava\u0161"], "venue": "EACL. pp. 516\u2013522", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Type-constrained representation learning in knowledge graphs", "author": ["D. Krompa\u00df", "S. Baier", "V. Tresp"], "venue": "Arenas, M., et al. (eds.) ISWC. pp. 640\u2013655", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge representation learning with entities, attributes and relations", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "IJCAI. pp. 2866\u20132872", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "AAAI. pp. 2181\u20132187", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv:1301.3781", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS. pp. 3111\u20133119", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H. Kriegel"], "venue": "ICML. pp. 809\u2013816", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Rdf2vec: RDF graph embeddings for data mining", "author": ["P. Ristoski", "H. Paulheim"], "venue": "Groth, P., et al. (eds.) ISWC. pp. 498\u2013514", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS. pp. 926\u2013934", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A machine learning approach to multilingual and cross-lingual ontology matching", "author": ["D. Spohr", "L. Hollink", "P. Cimiano"], "venue": "Aroyo, L., et al. (eds.) ISWC. pp. 665\u2013680", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "AAAI. pp. 1112\u20131119", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-lingual knowledge linking across wiki knowledge bases", "author": ["Z. Wang", "J. Li", "Z. Wang", "J. Tang"], "venue": "WWW. pp. 459\u2013468", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Normalized word embedding and orthogonal transform for bilingual word translation", "author": ["C. Xing", "D. Wang", "C. Liu", "Y. Lin"], "venue": "HLT-NAACL. pp. 1006\u20131011", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["W.Y. Zou", "R. Socher", "D.M. Cer", "C.D. Manning"], "venue": "EMNLP. pp. 1393\u20131398", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 20, "context": "[21] argued that the quality of alignment in cross-lingual scenarios heavily depends on the quality of translations between multiple languages.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 57, "endOffset": 66}, {"referenceID": 14, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 57, "endOffset": 66}, {"referenceID": 21, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 57, "endOffset": 66}, {"referenceID": 4, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 158, "endOffset": 164}, {"referenceID": 10, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 158, "endOffset": 164}, {"referenceID": 4, "context": "However, as discovered in [5], the existing alignment between cross-lingual KBs usually accounts for a small proportion.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "TransE [1], the pioneer of translation-based methods, interprets a relationship vector as the translation from the head entity vector to its tail entity vector.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "To further improve TransE, later work including TransH [22] and TransR [15] was proposed.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "To further improve TransE, later work including TransH [22] and TransR [15] was proposed.", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "Additionally, there exist a few non-translation-based approaches to KB embedding [2,18,20].", "startOffset": 81, "endOffset": 90}, {"referenceID": 17, "context": "Additionally, there exist a few non-translation-based approaches to KB embedding [2,18,20].", "startOffset": 81, "endOffset": 90}, {"referenceID": 19, "context": "Additionally, there exist a few non-translation-based approaches to KB embedding [2,18,20].", "startOffset": 81, "endOffset": 90}, {"referenceID": 12, "context": "[13] added type constraints to KB embedding models and enhanced their performance on link prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "KR-EAR [14] embeds attributes additionally by modeling attribute correlations and obtains good results", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "RDF2Vec [19] uses local information of KB structures to generate sequences of entities and employs language modeling approaches to learn entity embeddings for machine learning tasks.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "For cross-lingual tasks, [12] extends NTNKBC [4] for cross-lingual KB completion.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "For cross-lingual tasks, [12] extends NTNKBC [4] for cross-lingual KB completion.", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "[7] uses a neural network approach that translates English KBs into Chinese to expand Chinese KBs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8,9] presented a generic framework, which utilizes machine translation tools to translate labels to the same language and uses monolingual ontology matching methods to find mappings.", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "[8,9] presented a generic framework, which utilizes machine translation tools to translate labels to the same language and uses monolingual ontology matching methods to find mappings.", "startOffset": 0, "endOffset": 5}, {"referenceID": 20, "context": "[21] leveraged translation-based label similarities and ontology structures as features for learning cross-lingual mapping functions by machine learning techniques (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "For cross-lingual entity alignment, MTransE [5] incorporates TransE to encode KB structures into language-specific vector spaces and designs five alignment models to learn translation between KBs in different languages with seed alignment.", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "JE [11] utilizes TransE to embed different KBs into a unified space with the aim that each seed alignment has similar embeddings, which is extensible to the cross-lingual scenario.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[23] proposed a graph model, which only leverages language-independent features (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] exploited embedding-based methods for aligning entities in Web tables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Following TransE [1], we interpret a relationship as the translation from the head entity to the tail entity, to characterize the structure information of KBs.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "corrupted triples), which have been widely used in translation-based embedding models [1,15,22], are also valuable to SE.", "startOffset": 86, "endOffset": 95}, {"referenceID": 14, "context": "corrupted triples), which have been widely used in translation-based embedding models [1,15,22], are also valuable to SE.", "startOffset": 86, "endOffset": 95}, {"referenceID": 21, "context": "corrupted triples), which have been widely used in translation-based embedding models [1,15,22], are also valuable to SE.", "startOffset": 86, "endOffset": 95}, {"referenceID": 0, "context": "\u03b1 is a ratio hyper-parameter that weights positive and negative triples and its range is [0, 1].", "startOffset": 89, "endOffset": 95}, {"referenceID": 15, "context": "To capture the correlations of attributes, AE borrows the idea from Skipgram [16], a very popular model that learns word embeddings by predicting the context of a word given the word itself.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "To prevent all the vectors from having the same value, we adopt the negative sampling approach [17] to efficiently parameterize Eq.", "startOffset": 95, "endOffset": 99}, {"referenceID": 24, "context": "Inspired by [25], we use the matrices of pairwise similarities between entities as supervised information and minimize the following objective function:", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "(1)) does not follow the margin-based ranking loss function below, which is used by many previous KB embedding models [1]:", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "In contrast, JE [11] uses the margin-based ranking loss from TransE [1], while MTransE [5] does not have this as it does not use negative triples.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "In contrast, JE [11] uses the margin-based ranking loss from TransE [1], while MTransE [5] does not have this as it does not use negative triples.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "In contrast, JE [11] uses the margin-based ranking loss from TransE [1], while MTransE [5] does not have this as it does not use negative triples.", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "(2) and (6) with a gradient descent optimization algorithm called AdaGrad [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 23, "context": "(3) and the cosine similarity to measure embeddings [24].", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "The structure embedding belongs to the translation-based embedding models, which have already been proved to be capable of learning embeddings at large scale [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "As aforementioned, JE [11] and MTransE [5] are two representative embeddingbased methods for entity alignment.", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "As aforementioned, JE [11] and MTransE [5] are two representative embeddingbased methods for entity alignment.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "We followed the implementation details reported in [5,11] and complemented other unreported details with careful consideration.", "startOffset": 51, "endOffset": 57}, {"referenceID": 10, "context": "We followed the implementation details reported in [5,11] and complemented other unreported details with careful consideration.", "startOffset": 51, "endOffset": 57}, {"referenceID": 0, "context": "Following the conventions [1,5,11], we used Hits@k and Mean to assess the performance of the three approaches.", "startOffset": 26, "endOffset": 34}, {"referenceID": 4, "context": "Following the conventions [1,5,11], we used Hits@k and Mean to assess the performance of the three approaches.", "startOffset": 26, "endOffset": 34}, {"referenceID": 10, "context": "Following the conventions [1,5,11], we used Hits@k and Mean to assess the performance of the three approaches.", "startOffset": 26, "endOffset": 34}, {"referenceID": 8, "context": "Combination with Machine Translation Since machine translation is often used in cross-lingual ontology matching [9,21], we designed a machine translation based approach that employs Google Translate to translate the labels of entities in one KB and computes similarities between the translations and the labels of entities in the other KB.", "startOffset": 112, "endOffset": 118}, {"referenceID": 20, "context": "Combination with Machine Translation Since machine translation is often used in cross-lingual ontology matching [9,21], we designed a machine translation based approach that employs Google Translate to translate the labels of entities in one KB and computes similarities between the translations and the labels of entities in the other KB.", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": "For similarity measurement, we chose Levenshtein distance because of its popularity in ontology matching [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 21, "context": "We think that the reasons are twofold: (i) DBP100K contains quite a few \u201csparse\u201d entities involved in a very limited number of triples, which affect embedding the structure information of KBs; and (ii) as the number of latent aligned entities in DBP100K are several times larger than DBP15K, the TransEbased models suffer from the increased occurrence of multi-mapping relations as explained in [22].", "startOffset": 395, "endOffset": 399}], "year": 2017, "abstractText": "Entity alignment is the task of finding entities in two knowledge bases (KBs) that represent the same real-world object. When facing KBs in different natural languages, conventional cross-lingual entity alignment methods rely on machine translation to eliminate the language barriers. These approaches often suffer from the uneven quality of translations between languages. While recent embedding-based techniques encode entities and relationships in KBs and do not need machine translation for cross-lingual entity alignment, a significant number of attributes remain largely unexplored. In this paper, we propose a joint attribute-preserving embedding model for cross-lingual entity alignment. It jointly embeds the structures of two KBs into a unified vector space and further refines it by leveraging attribute correlations in the KBs. Our experimental results on real-world datasets show that this approach significantly outperforms the state-of-the-art embedding approaches for cross-lingual entity alignment and could be complemented with methods based on machine translation.", "creator": "LaTeX with hyperref package"}}}