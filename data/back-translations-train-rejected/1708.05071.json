{"id": "1708.05071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Learning spectro-temporal features with 3D CNNs for speech emotion recognition", "abstract": "In this paper, we propose to use deep 3-dimensional convolutional networks (3D CNNs) in order to address the challenge of modelling spectro-temporal dynamics for speech emotion recognition (SER). Compared to a hybrid of Convolutional Neural Network and Long-Short-Term-Memory (CNN-LSTM), our proposed 3D CNNs simultaneously extract short-term and long-term spectral features with a moderate number of parameters. We evaluated our proposed and other state-of-the-art methods in a speaker-independent manner using aggregated corpora that give a large and diverse set of speakers. We found that 1) shallow temporal and moderately deep spectral kernels of a homogeneous architecture are optimal for the task; and 2) our 3D CNNs are more effective for spectro-temporal feature learning compared to other methods. Finally, we visualised the feature space obtained with our proposed method using t-distributed stochastic neighbour embedding (T-SNE) and could observe distinct clusters of emotions.", "histories": [["v1", "Mon, 14 Aug 2017 17:32:06 GMT  (456kb,D)", "http://arxiv.org/abs/1708.05071v1", "ACII, 2017, San Antonio"]], "COMMENTS": "ACII, 2017, San Antonio", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["jaebok kim", "khiet p truong", "gwenn englebienne", "vanessa evers"], "accepted": false, "id": "1708.05071"}, "pdf": {"name": "1708.05071.pdf", "metadata": {"source": "CRF", "title": "Learning spectro-temporal features with 3D CNNs for speech emotion recognition", "authors": ["Jaebok Kim", "Khiet P. Truong", "Gwenn Englebienne", "Vanessa Evers"], "emails": ["v.evers}@utwente.nl"], "sections": [{"heading": null, "text": "In recent years, it has become clear that individual countries are countries where most people are unable to integrate, including those countries where most people are able to integrate and where most of them are unable to integrate, as well as those countries where most people are able to integrate and integrate."}], "references": [{"title": "Speech emotion recognition using deep neural network and extreme learning machine", "author": ["I.T. Kun Han", "Dong Yu"], "venue": "Proceedings of INTERSPEECH, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "An experimental study of speech emotion recognition based on deep convolutional neural networks", "author": ["W. Zheng", "J. Yu", "Y. Zou"], "venue": "Proceedings of International Conference on Affective Computing and Intelligent Interaction. IEEE, 2015, pp. 827\u2013831.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning salient features for speech emotion recognition using convolutional neural networks", "author": ["Q. Mao", "M. Dong", "Z. Huang", "Y. Zhan"], "venue": "IEEE Transactions on Multimedia, vol. 16, no. 8, pp. 2203\u20132213, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "S. Zafeiriou"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5200\u20135204.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "High-level feature representation using recurrent neural network for speech emotion recognition", "author": ["J. Lee", "I. Tashev"], "venue": "Proceedings of INTERSPEECH, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1828}, {"title": "Representation learning for speech emotion recognition", "author": ["S. Ghosh", "E. Laksana", "L.-P. Morency", "S. Scherer"], "venue": "Proceedings of INTERSPEECH, 2016, pp. 3603\u20133607.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 1, pp. 221\u2013231, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 4489\u20134497.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, no. 10, p. 1995, 1995.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Towards speech emotion recognition \u201cin the wild\u201d using aggregated corpora and deep multi-task learning", "author": ["J. Kim", "G. Englebienne", "K.P. Truong", "V. Evers"], "venue": "Proceedings of INTERSPEECH, 2017, p. To be appeared.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep temporal models using identity skip-connections for speech emotion recognition", "author": ["\u2014\u2014"], "venue": "Proceedings of ACM Multimedia, 2017, p. To be appeared.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4580\u20134584.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Emotional prosody speech and transcripts", "author": ["M. Liberman", "K. Davis", "M. Grossman", "N. Martey", "J. Bell"], "venue": "Linguistic Data Consortium, Philadelphia, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "The enterface\u201905 audiovisual emotion database", "author": ["O. Martin", "I. Kotsia", "B. Macq", "I. Pitas"], "venue": "22nd International Conference on Data Engineering Workshops (ICDEW\u201906). IEEE, 2006, pp. 8\u20138.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "A database of german emotional speech.", "author": ["F. Burkhardt", "A. Paeschke", "M. Rolfes", "W.F. Sendlmeier", "B. Weiss"], "venue": "Proceedings of INTER- SPEECH,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "You stupid tin box-children interacting with the aibo robot: A cross-linguistic emotional speech corpus.", "author": ["A. Batliner", "C. Hacker", "S. Steidl", "E. N\u00f6th", "S. D\u2019Arcy", "M.J. Russell", "M. Wong"], "venue": "Proceedings of LREC,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Iemocap: Interactive emotional dyadic motion capture database", "author": ["C. Busso", "M. Bulut", "C.-C. Lee", "A. Kazemzadeh", "E. Mower", "S. Kim", "J.N. Chang", "S. Lee", "S.S. Narayanan"], "venue": "Language resources and evaluation, vol. 42, no. 4, pp. 335\u2013359, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "The semaine corpus of emotionally coloured character interactions", "author": ["G. McKeown", "M.F. Valstar", "R. Cowie", "M. Pantic"], "venue": "Proceedings of IEEE International Conference on Multimedia and Expo (ICME), 2010, pp. 1079\u20131084.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions", "author": ["F. Ringeval", "A. Sonderegger", "J. Sauer", "D. Lalanne"], "venue": "Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on. IEEE, 2013, pp. 1\u20138.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "feeltrace\u2019: An instrument for recording perceived emotion in real time", "author": ["R. Cowie", "E. Douglas-Cowie", "S. Savvidou*", "E. McMahon", "M. Sawey", "M. Schr\u00f6der"], "venue": "ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion, 2000, pp. 19\u201324.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic early stopping using cross validation: quantifying the criteria", "author": ["L. Prechelt"], "venue": "Neural Networks, vol. 11, no. 4, pp. 761\u2013767, 1998.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Emotion classification via utterancelevel dynamics: A pattern-based approach to characterizing affective expressions", "author": ["Y. Kim", "E.M. Provost"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 3677\u20133681.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Residual networks behave like ensembles of relatively shallow networks", "author": ["A. Veit", "M.J. Wilber", "S. Belongie"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 550\u2013558.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Recently, deep learning methods such as Fullyconnected Neural Networks (FCN) [1], Convolutional Neural Networks (CNN) [2], [3], and Long Short-Term Memory (LSTM) [4], [5] have shown considerable improvements of performance in speech emotion recognition (SER).", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "Introduction Recently, deep learning methods such as Fullyconnected Neural Networks (FCN) [1], Convolutional Neural Networks (CNN) [2], [3], and Long Short-Term Memory (LSTM) [4], [5] have shown considerable improvements of performance in speech emotion recognition (SER).", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Introduction Recently, deep learning methods such as Fullyconnected Neural Networks (FCN) [1], Convolutional Neural Networks (CNN) [2], [3], and Long Short-Term Memory (LSTM) [4], [5] have shown considerable improvements of performance in speech emotion recognition (SER).", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "Introduction Recently, deep learning methods such as Fullyconnected Neural Networks (FCN) [1], Convolutional Neural Networks (CNN) [2], [3], and Long Short-Term Memory (LSTM) [4], [5] have shown considerable improvements of performance in speech emotion recognition (SER).", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "Introduction Recently, deep learning methods such as Fullyconnected Neural Networks (FCN) [1], Convolutional Neural Networks (CNN) [2], [3], and Long Short-Term Memory (LSTM) [4], [5] have shown considerable improvements of performance in speech emotion recognition (SER).", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "As a potential way to improve performance, representation learning has been used to build high-level features from lowlevel features through several layers [2], [3], [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "As a potential way to improve performance, representation learning has been used to build high-level features from lowlevel features through several layers [2], [3], [6].", "startOffset": 161, "endOffset": 164}, {"referenceID": 5, "context": "As a potential way to improve performance, representation learning has been used to build high-level features from lowlevel features through several layers [2], [3], [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 6, "context": "However, learning sequential structures of spectrogram representations appeared to be still challenging [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "CNN-based methods have been investigated to this end [2], [3], [4], [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "CNN-based methods have been investigated to this end [2], [3], [4], [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "CNN-based methods have been investigated to this end [2], [3], [4], [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "3D CNNs are able to extract spatio-temporal features in a seamless way and have shown promising performances in computer vision tasks [9], [10].", "startOffset": 134, "endOffset": 137}, {"referenceID": 8, "context": "3D CNNs are able to extract spatio-temporal features in a seamless way and have shown promising performances in computer vision tasks [9], [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "Representation learning offers a partial and potential solution by extracting high-level features from lowlevel features through a composition of multiple non-linear transformations [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "For example, CNN can extract abstract features in a more explicit way via a pooling mechanism [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "For example, in [1], [5], [12], high-level features obtained from off-the-shelf features outperformed conventional methods.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "For example, in [1], [5], [12], high-level features obtained from off-the-shelf features outperformed conventional methods.", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "For example, in [1], [5], [12], high-level features obtained from off-the-shelf features outperformed conventional methods.", "startOffset": 26, "endOffset": 30}, {"referenceID": 6, "context": "However, representation learning using log-spectrogram features did not outperform that of using off-the-shelf features - learning such a complex sequential structure of emotional speech appeared to be hard for representation learning [7].", "startOffset": 235, "endOffset": 238}, {"referenceID": 9, "context": "with distortions and variations [11].", "startOffset": 32, "endOffset": 36}, {"referenceID": 1, "context": "For one dimensional (D) discrete data, it is defined as: (f \u2217 g)(t) = T \u2211 k=\u2212T f(t\u2212 k) \u00b7 g(k) (1) CNN-based methods using low-level features were proposed and outperformed off-the-shelf feature-based methods [2], [3], [4], [8], [13].", "startOffset": 208, "endOffset": 211}, {"referenceID": 2, "context": "For one dimensional (D) discrete data, it is defined as: (f \u2217 g)(t) = T \u2211 k=\u2212T f(t\u2212 k) \u00b7 g(k) (1) CNN-based methods using low-level features were proposed and outperformed off-the-shelf feature-based methods [2], [3], [4], [8], [13].", "startOffset": 213, "endOffset": 216}, {"referenceID": 3, "context": "For one dimensional (D) discrete data, it is defined as: (f \u2217 g)(t) = T \u2211 k=\u2212T f(t\u2212 k) \u00b7 g(k) (1) CNN-based methods using low-level features were proposed and outperformed off-the-shelf feature-based methods [2], [3], [4], [8], [13].", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "For one dimensional (D) discrete data, it is defined as: (f \u2217 g)(t) = T \u2211 k=\u2212T f(t\u2212 k) \u00b7 g(k) (1) CNN-based methods using low-level features were proposed and outperformed off-the-shelf feature-based methods [2], [3], [4], [8], [13].", "startOffset": 228, "endOffset": 232}, {"referenceID": 1, "context": "In [2], [3], [8] 2D feature maps were composed of spectrogram features with a fine resolution.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [2], [3], [8] 2D feature maps were composed of spectrogram features with a fine resolution.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Instead, LSTM should be followed to model temporal dependencies [4], [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "Moreover, temporal convolutions can extract spectral features from raw wave signals and capture long-term dependencies [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 12, "context": "Lastly, CNN-LSTM-DNN was proposed to address frequency variations in spectral domain, long-term dependencies, separation in utterance-level feature space for the task of speech recognition [14].", "startOffset": 189, "endOffset": 193}, {"referenceID": 7, "context": "Without these complex memory mechanisms, 3D CNNs could learn temporal features [9], [10].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Without these complex memory mechanisms, 3D CNNs could learn temporal features [9], [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "In [9], [10], a series of human\u2019s motion was modelled by 3D CNNs, it empirically turned out that 3D CNNs are not only effective but also efficient to capture spatio-temporal features.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [9], [10], a series of human\u2019s motion was modelled by 3D CNNs, it empirically turned out that 3D CNNs are not only effective but also efficient to capture spatio-temporal features.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Data We select seven representative corpora: LDC Emotional Prosody [15], eNTERFACE [16], EMODB [17] FAU-aibo emotion corpus [18], IEMOCAP [19], SEMAINE [20], and RECOLA [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Data We select seven representative corpora: LDC Emotional Prosody [15], eNTERFACE [16], EMODB [17] FAU-aibo emotion corpus [18], IEMOCAP [19], SEMAINE [20], and RECOLA [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "Data We select seven representative corpora: LDC Emotional Prosody [15], eNTERFACE [16], EMODB [17] FAU-aibo emotion corpus [18], IEMOCAP [19], SEMAINE [20], and RECOLA [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "Data We select seven representative corpora: LDC Emotional Prosody [15], eNTERFACE [16], EMODB [17] FAU-aibo emotion corpus [18], IEMOCAP [19], SEMAINE [20], and RECOLA [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "Data We select seven representative corpora: LDC Emotional Prosody [15], eNTERFACE [16], EMODB [17] FAU-aibo emotion corpus [18], IEMOCAP [19], SEMAINE [20], and RECOLA [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "Data We select seven representative corpora: LDC Emotional Prosody [15], eNTERFACE [16], EMODB [17] FAU-aibo emotion corpus [18], IEMOCAP [19], SEMAINE [20], and RECOLA [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "Data We select seven representative corpora: LDC Emotional Prosody [15], eNTERFACE [16], EMODB [17] FAU-aibo emotion corpus [18], IEMOCAP [19], SEMAINE [20], and RECOLA [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 20, "context": "To map the continuous labels into the four discrete categories, we use the landmarks of the valence and arousal dimensions as provided in FEELTRACE [22].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "Method While the previous methods [8], [14] learn spectral features and the temporal dependencies via the augmentation of CNN and LSTM, our proposed method is designed to learn spectro-temporal features simultaneously as depicted in Figure 1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Based on the previous finding [10], we adopt a homogeneous architecture, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "However, to preserve the spectro-temporal features at early phases [10], we do not pool outputs of the first convolutional layers.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Moreover, it requires statistical functionals at the softmax layer and a proceeding classifier such as Extreme Learning Machine (ELM) as proposed in [1], [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "Moreover, it requires statistical functionals at the softmax layer and a proceeding classifier such as Extreme Learning Machine (ELM) as proposed in [1], [5].", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "We follow the same set-up of the functionals and ELM proposed in [1], [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "We follow the same set-up of the functionals and ELM proposed in [1], [5].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Training a linear classifier on features from the top fully-connected layer of 3D-CNN-DNN can be an effective approach [10] but it may bring similar effects of 3D-CNNDNN-ELM.", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "BEST, WORST, AND MEAN PERFORMANCE (UA) BY VARYING RESOLUTIONS (L X S X T) OF 3D KERNELS As a common set-up, we use Adam method [23] with a minibatch of 128 samples, and a fixed learning rate of 3 \u00b7 10\u22123.", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "To prevent over-fitting, we use early-stopping [24] (the maximum number of epoch: 20) and dropout [25] (p = .", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "To prevent over-fitting, we use early-stopping [24] (the maximum number of epoch: 20) and dropout [25] (p = .", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Features Methods Configuration of layers Parameters (K) UA off-the-shelf DNN-ELM [1] 3 x FCN (256) + ELM 141 .", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "05 LSTM-ELM [5] 2 x LSTM (128) + ELM 214 .", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "11 raw waveform 1D-CNN-LSTM [4] CNN (80) + CNN (800) + 2 x LSTM (128) 1583 .", "startOffset": 28, "endOffset": 31}, {"referenceID": 12, "context": "03 2D-CNN-LSTM-DNN [14] 2 x CNN (2 x 2) + 2 x LSTM (128) + 2 x FCN (512) 595 .", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "01 3D-CNN-DNN (proposed) 2 x CNN (2 x 2 x [2, 32, 128]) + 2 x FCN (512) 792\u2212 810 .", "startOffset": 42, "endOffset": 54}, {"referenceID": 1, "context": "03 3D-CNN-DNN-ELM (proposed) 2 x CNN (2 x 2 x [2, 32, 128]) + 2 x FCN (512) 527\u2212 548 .", "startOffset": 46, "endOffset": 58}, {"referenceID": 0, "context": "Compared to state-of-the-art We compare our proposed methods to state-of-theart methods: DNN-ELM [1], LSTM-ELM [5], 1D-CNNLSTM [4], 2D-CNN-LSTM [8], and 2D-CNN-LSTMDNN [14].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "Compared to state-of-the-art We compare our proposed methods to state-of-theart methods: DNN-ELM [1], LSTM-ELM [5], 1D-CNNLSTM [4], 2D-CNN-LSTM [8], and 2D-CNN-LSTMDNN [14].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "Compared to state-of-the-art We compare our proposed methods to state-of-theart methods: DNN-ELM [1], LSTM-ELM [5], 1D-CNNLSTM [4], 2D-CNN-LSTM [8], and 2D-CNN-LSTMDNN [14].", "startOffset": 127, "endOffset": 130}, {"referenceID": 12, "context": "Compared to state-of-the-art We compare our proposed methods to state-of-theart methods: DNN-ELM [1], LSTM-ELM [5], 1D-CNNLSTM [4], 2D-CNN-LSTM [8], and 2D-CNN-LSTMDNN [14].", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "DNN-ELM [1] and LSTM-ELM [5].", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "DNN-ELM [1] and LSTM-ELM [5].", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "Statistical functionals to extract utterancelevel features and a proceeding Extreme Learning Machine (ELM) have the same setting in [1], [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Statistical functionals to extract utterancelevel features and a proceeding Extreme Learning Machine (ELM) have the same setting in [1], [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "1D-CNN-LSTM [4].", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "2D-CNN-LSTM [8] and 2D-CNN-LSTM-DNN [14].", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "2 x 2 x [2, 32, 128] is short for these three shapes of kernels.", "startOffset": 8, "endOffset": 20}, {"referenceID": 6, "context": "While emotional classes can be directly learned from spectrogram features [7], outperforming off-the-shelf features is still challenging.", "startOffset": 74, "endOffset": 77}, {"referenceID": 24, "context": "Because of the complexity of LSTM, the depth is limited to two, and it might not be sufficiently deep to learn the complicated sequential structure of emotional speech [27].", "startOffset": 168, "endOffset": 172}, {"referenceID": 25, "context": "To this end, we visualise utterancelevel representations of one test set of our cross-validation by using t-distributed stochastic neighbour embedding (TSNE) [28].", "startOffset": 158, "endOffset": 162}], "year": 2017, "abstractText": "Abstract\u2014In this paper, we propose to use deep 3-dimensional convolutional networks (3D CNNs) in order to address the challenge of modelling spectro-temporal dynamics for speech emotion recognition (SER). Compared to a hybrid of Convolutional Neural Network and Long-Short-Term-Memory (CNNLSTM), our proposed 3D CNNs simultaneously extract shortterm and long-term spectral features with a moderate number of parameters. We evaluated our proposed and other stateof-the-art methods in a speaker-independent manner using aggregated corpora that give a large and diverse set of speakers. We found that 1) shallow temporal and moderately deep spectral kernels of a homogeneous architecture are optimal for the task; and 2) our 3D CNNs are more effective for spectrotemporal feature learning compared to other methods. Finally, we visualised the feature space obtained with our proposed method using t-distributed stochastic neighbour embedding (TSNE) and could observe distinct clusters of emotions.", "creator": "LaTeX with hyperref package"}}}