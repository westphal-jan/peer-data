{"id": "1708.05296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "A Survey of Parallel A*", "abstract": "A* is a best-first search algorithm for finding optimal-cost paths in graphs. A* benefits significantly from parallelism because in many applications, A* is limited by memory usage, so distributed memory implementations of A* that use all of the aggregate memory on the cluster enable problems that can not be solved by serial, single-machine implementations to be solved. We survey approaches to parallel A*, focusing on decentralized approaches to A* which partition the state space among processors. We also survey approaches to parallel, limited-memory variants of A* such as parallel IDA*.", "histories": [["v1", "Wed, 16 Aug 2017 01:45:40 GMT  (495kb,D)", "http://arxiv.org/abs/1708.05296v1", "arXiv admin note: text overlap witharXiv:1201.3204"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1201.3204", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alex fukunaga", "adi botea", "yuu jinnai", "akihiro kishimoto"], "accepted": false, "id": "1708.05296"}, "pdf": {"name": "1708.05296.pdf", "metadata": {"source": "CRF", "title": "A Survey of Parallel A*", "authors": ["Alex Fukunaga", "Adi Botea", "Yuu Jinnai", "Akihiro Kishimoto"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to abide by the rules that they apply in practice."}, {"heading": "2 Preliminaries: Review of A*", "text": "This section provides preliminary and background material for the rest of this paper. We first formally define the problem of space search, and then we present the problem of A * search. (D) The formal definitions presented below are from Edelkamp and Schroedl's textbook on heuristic search (D). Definition 1 (D): A state problem P = (S, A, s0, T) is defined by a series of states S, an initial state s0, a set of target states T, and a finite series of actions A = a1,... am, where each ai: S \u2192 S transforms one state into another state.For the sliding tile puzzle, the state formulation consists of states S, where each state corresponds to a unique configuration of tiles, s0 is the predetermined initial configuration, T is a singleton whose only member is the configuration with the tiles in the correct order between tiles, A and the transitions between tiles."}, {"heading": "2.1 The A* Algorithm", "text": "Most of the parallel search algorithms presented in this chapter are based on the serial algorithm A * [23]. A * is a best-first search algorithm whose pseudo-code is represented in algorithm 1. A * holds two sets of nodes, called the OPEN list and the CLOSED list. CLOSED list is the set of advanced nodes that refers to the generation of their successors. The OPEN list contains the nodes that have been generated and are waiting to be expanded. At each iteration of the main loop shown in the pseudocode, A * selects a node from the OPEN list for expansion, with the smallest f value. The f value of a node n is defined as f (n) = g (n) + h (n). The g (n) value is the cost of the best known path from root 0 to the current node."}, {"heading": "3 Parallel Best-First Search Algorithms", "text": "The parallelization of A * heuristic search is important for two reasons: First, effective parallelization is necessary to achieve good acceleration on multi-core processors. However, in the case of parallelization on a cluster that consists of many machines, parallelization offers another advantage that is at least as important as acceleration that increases the total memory. A * memory usage increases continuously during the run, as all extended nodes must remain in memory to ensure the solidity (solution optimality) and completeness of the algorithm. Running A * on a cluster of machines in parallel A * makes all the total memory of the cluster available. This allows A * to solve problems in parallel that would not be solvable at all on a single machine (using the same heuristic function), providing a fundamental advantage of parallelization of A * and making parallelization of A * an even more pressing concern."}, {"heading": "3.1 Parallel Overheads", "text": "Efficient implementation of parallel search algorithms is challenging due to several types of overhead. Search Overhead (SO) occurs when a parallel implementation of a search algorithm expands (or generates) more states than a serial implementation. The main cause of search overhead is the division of the search space between processors, which has the side effect of limiting access to non-local information. For example, sequential A * can be terminated immediately after a solution is found because it is guaranteed to be optimal. However, if a parallel A * algorithm finds a (first) solution for a processor, it is not necessarily a globally optimal solution. A better solution that uses nodes processed in another processor may exist. Synchronization effort is the time wasted when some processors have to wait for the other synchronization points to reach."}, {"heading": "3.2 Centralized Parallel A*", "text": "Algorithms such as breadth-first or best-first search (including A *) use an open list that stores the set of states that have been generated but have not yet been expanded. In an early study, we first took a centralized approach to parallel A *.Algorithm 2: Simple Parallel A * (SPA *) 1 extremely extensive approaches to parallelizing best-first search, based on the way the use and maintenance of the open list was paralleled; 2 Initialize Lock lo, li; 3 Initialize incumbent.cost =. 4 Parallel, on each thread, execute 5-32; 5 while TerminateDetection () 6 do if OPENshared = success or smallest f (n) value of n OPENshared powers.cost then 7 Continue; 8 AcquireLock (lo)."}, {"heading": "3.3 Decentralized Parallel A*", "text": "As described above, SPA * suffers from a heavy synchronization effort due to the necessity of constantly having to access split open / closed lists (= 17). (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) (SO) SO (SO) (SO) SO (SO) (SO) SO (SO) (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) SO (SO) (SO) SO (SO) SO (SO) (SO) SO (SO) (SO) SO (SO) (SO) (SO) (SO) SO (SO) (SO) (SO) (SO) (SO) (SO) SO (SO) (SO) (SO) (SO) (SO) (SO) SO (SO) (SO) (SO) (SO) SO (SO) (SO) (SO) (SO (SO) (SO) (SO) (SO) (SO) (SO) (SO) (SO) (SO) SO (SO) (SO (SO) (SO) (SO) (SO) (SO) (SO) (SO) (SO) (SO) (SO (SO) (SO) (SO) (SO) (SO) (SO) (SO) ("}, {"heading": "3.3.1 Termination Detection in Decentralized Parallel Search", "text": "In a decentralized parallel A *, when a solution is discovered, there is no guarantee at this point that the solution is optimal. [46] When a processor discovers a locally optimal solution, the processor sends its costs. The search cannot be stopped until all processors have proven that there is no solution with better costs. To terminate a decentralized parallel A * correctly, it is not enough to check the local open list for each processor. We also need to make sure that there is no message on the way to a processor that could lead to a better solution. Various termination algorithms exist. A commonly used method is Matter [51]. Matter's method is based on counting the messages sent and received messages. If all processors were able to count at the same time, it would be trivial to determine whether a message is still on the way. In reality, different processors will report Pi their sent and received counters."}, {"heading": "4 Hash-Based Decentralized A*", "text": "A decentralized A * approach addressing both load balancing and duplicate recognition functions assigns a unique processor to each search node after a hash function. That is, in Figure 3, line 31, ComputeRecipient (n \") is implemented by hashing, i.e., ComputeRecipient (n\") = hash (n \") mod numprocessors, which maps each state to exactly one processor that\" owns \"the state. If the hash keys are uniformly distributed among the processors, the time in which each state is equal, then the load distribution is achieved. In addition, the duplicate recognition is performed by the\" owner \"state - states that are already duplicated in the local OPEN / CLOSED lists, and by definition, nodes cannot be expanded by a non-owner processor."}, {"heading": "4.1 Hash Distributed A*", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Decentralized Search Using Structure-based Search Space", "text": "In fact, most of them will be able to feel as if they are able to play by the rules, and that they will be able to play by the rules that they are able to play by the rules."}, {"heading": "6 Hash Functions for Hash-Based Decentralized Work Dis-", "text": "The performance of hash-based decentralized A * algorithms in Section 4 depends entirely on the properties of the hash function. However, early work on hash-based decentralized A * did not provide an empirical assessment of the hash functions of candidates, and the importance of choosing the hash function was not fully understood or appreciated. Recent work has examined the performance characteristics and trade-offs between different hash strategies, which has led to a much better understanding of earlier hash strategies, as well as new hash strategies that combine earlier methods to achieve superior performance [31, 33]. In this section, we first classify and review various hash functions proposed for hash-based distributed A * (Sections 6.1-6.6), and then present an evaluation of some functions in the moving puzzle benchmark domain (Section 6.7)."}, {"heading": "6.1 Multiplicative Hashing", "text": "The H (\u03ba) multiplication method is a widely used hash method that has been observed to hash a random key for P slots with almost equal probability [11]. Multiplicative hash M (s) uses this function to achieve good load balancing of nodes between processors [50]: M (s) = H (\u0432 (s), (1) H (\u0443) = bp (\u0445 \u00b7 A \u2212 b\u0443 \u00b7 Ac) c, (2) where the key is derived from the state s, p is the number of processors, and A is a parameter in the range [0, 1). Typically, A = (\u221a 5 \u2212 1) / 2 (the golden ratio) is used, as the hash function is known to work well with this value of A [39]."}, {"heading": "6.2 Zobrist Hashing", "text": "Since the distribution of work in HDA * is entirely determined by a global hash function, the choice of the hash function is decisive for its performance. Kishimoto et al. [37, 36] stated that it is desirable to use a hash function that distributes the nodes evenly across the processors, and used the zobrist hash function described below [78]. The zobrist hash value of a state, Z (s), is calculated as follows. For simplicity's sake, let us assume that s is represented as an array of n propositions, s = (x0, x1,..., xn). Let R be a table containing pre-initialized bitstrings: Z (s): = R [x0] xor [x1] xor \u00b7 xor R [xn] (3). In the rest of the paper, we refer to the original version of HDA * by Kishoto et al [37], which is used as a hash, Zobrist."}, {"heading": "6.3 Operator-Based Zobrist hashing", "text": "Assuming it is an ideal implementation that distributes nodes evenly across all threads, each node generated is likely to be sent 1 \u2212 1 # threads to another thread. Therefore, at 16 threads, > 90% of the nodes are sent to other threads, resulting in communication costs for the vast majority of node generations. Operator-based zobrist hashing (OZHDA *) [32] partially solves this problem by manipulating the random bit strings in R, the table used to calculate zobrist hash values, so that for some selected states S there are some operators A (s) for s-S, so that the successors of s generated when A (s) are applied to s are guaranteed to have the same zobrist hash value that ensures that they are significantly increased compared to HDA search results, whether the processor s are higher than the DA * and the overDA * in search results."}, {"heading": "6.4 Abstraction", "text": "To minimize the communication overhead in HDA *, Burns et al. [4] have proposed AHDA * to use the abstraction strategy to project nodes in the state space to abstract states. A hash-based work distribution function can then be applied to the projected state. Burns et al. [4] \"s AHDA * implementation assigns abstract states to the processors by using a perfect hashing and a modulus operator. Thus, nodes projected to the same abstract state are assigned to the same thread. If the abstraction function is defined in such a way that children of nodes n are normally in the same abstract state as n, then the communication overhead is minimized. The disadvantage of this method is that it focuses exclusively on minimizing communication overhead, and there is no mechanism for balancing loads."}, {"heading": "6.5 Abstract Zobrist Hashing", "text": "Both search and communication have a significant impact on the performance of HDA *, and methods that address only one of these commonalities are inadequate. ZHDA *, which uses zobrist hashing, assigns nodes evenly to the processors by achieving near-perfect load balance, but this results in significant search expenditure, as all productive search functions can be performed on one node, while all other nodes search for unproductive nodes that are not expanded by A *. Thus, we need a more balanced approach to both search and communication."}, {"heading": "6.6 Hyperplane Work Distribution", "text": "HDA suffers significantly from increased search effort in the Multiple Sequence Aligment (MSA) domain, whose search space is a directed acyclic graph with non-uniform edge costs [40]. The increased search effort is caused by reopening the nodes in the closed list to ensure optimum resolution, but even with a consistent heuristic method, HDA * may have to reopen a node because HDA * selects the best node in its local open list, which is not necessarily the best node globally. On the other hand, A *, using the consistent heuristic method, will never reopen the nodes in the closed list. Figure 4 illustrates an example of HDA * s disadvantage, which P1 owns states, and P2 owns state b. P1 is likely to expand d via path \u2192 c \u2192 c \u2192 d \u2192 d, since P1 does not transmit, c, and d to P2, while b must be explicated."}, {"heading": "6.7 Empirical Comparison of Hash Functions", "text": "To illustrate the scaling behavior of the various hash functions examined in this section, we evaluated the performance of the following parallel A * algorithms on the 15 puzzle. See [33] for a more detailed comparison \u2022 AZHDA *: HDA * using the abstract Zobrist hashings [31] \u2022 ZHDA *: HDA * using the Zobrist hashings [36] \u2022 AHDA *: HDA * using the abstract work distribution [4] \u2022 SafePBNF: [4] \u2022 HDA * + GOHA: HDA * using the multiplicative hashings, a hash function proposed in [50] \u2022 Randomized strategy: Nodes are sent to random cores (duplicate nodes are not guaranteed to be transferred to the same core) [47, 35] \u2022 Simple Parallel A * (centralized, single OPEN list) [30] This experiment was carried out on an Intel X26GB 2.2 GHe50 *."}, {"heading": "6.8 Domain-Independent, Automatic Generation of Hash Functions", "text": "The hash methods described above are domain-independent methods that can be applied to a wide range of problems. Although specific implementations of hash functions can be handcrafted for a specific problem, as in the sliding puzzle example above, it is possible to fully automate this process if a formal model of a domain (such as PDDL / SAS + for classic planning) is available. For every possible mapping of value k to variables vi in an SAS + representation, e.g. vi = k, there is a binary proposition xi, k (i.e. the corresponding STRIPS projection threshold [2] is simple [36]. For every possible mapping of value k to variables vi in an SAS + representation, e.g. vi = k, there is a binary proposition xi, k (i.e., the corresponding STRIPS projection threshold for representation). Each such proposition xi, k is a feature to which a randomly generated bit sequence is assigned."}, {"heading": "6.9 Hash-Based Work Distribution in Model Checking", "text": "While this paper focuses on the parallel inventory (more precisely, parallel A *) applied to standard AI search domains, including domain-independent planning and the sliding-tile puzzle, the distributed search, including hash-based work distribution, has also been extensively examined by the Parallel Model Testing Community. Parallel verification tasks are addressed, which include an exhaustive list of all available states in a state area and implement a hash-based work distribution scheme in which each state is assigned to a unique owner-processor. Kumar and Mercer [45] present a burden-balancing technique as an alternative to the hash-based work distribution implemented in Muri. The Eddy Murphi Model Checker specializes in the tasks of the processors, defining two threads for each processing node. The worker-thread performs state processing (e.g. through the expansion of state communication, during the other states)."}, {"heading": "7 Parallel Portfolios using A*", "text": "An algorithm portfolio [29] is frequently used and parallelized in other areas, such as the ManySAT solver [22] to the SAT solution and ArvandHerd [68] to satisfy planning. This approach performs a number of different search algorithms in parallel. However, processors execute the search algorithms largely independently, but can regularly exchange important information with others. A long-term distribution is often observed in the runtime distribution of the search algorithms [21]. The algorithm portfolio attempts to exploit such search behavior by using a variety of algorithms that potentially overlap, but examine different parts of the search space. Gearing [38], which is a simple version of the algorithm portfolio, performs the search simultaneously with different parameter settings. Valenzano et al. apply parallel gearing [69] to the weighted versions of IDA * [41], RBFS [42], PNF] an optimal search on parallweight values."}, {"heading": "8 Parallel, Limited-Memory A* (Parallel IDA*, TDS, PRA*)", "text": "In problem areas where the node generation rate of A * is high, the amount of memory available previously becomes a significant constraint, as A * memory can be exhausted and terminated before searching. A * -based schedulers for domain-independent, classical scheduling can consume 106 \u2212 107 bytes per second. Highly optimized approaches to specific domains, such as moving tiles, can generate over 106 \u2212 107 nodes per second, meaning that A * -based scheduling can consume 106 \u2212 107 bytes per second."}, {"heading": "8.1 Transposition Table-Driven Scheduling (TDS)", "text": "Transposition-Table-Driven work scheduling (TDS) [63, 61] is a distributed-memory, parallel IDA * algorithm that uses a distributed transposition table. Similar to HDA *, TDS splits the transposition table (TT) across processors and distributes the work asynchronously using a Zobrist-based state hash function. In this way, TDS effectively distributes the large amount of distributed memory to the TT and uses the TT to efficiently detect and truncate duplicate states that arrive at the processor. Distributed TT implementation uses the Zobrist hash function to map states on processors. TDS initiates parallelism within each iteration and synchronizes between iterations. In a simple implementation of the TDS, processors must exchange messages that convey lower limits backwards."}, {"heading": "8.2 Work Stealing for IDA*", "text": "Work theft is a standard approach to partitioning the search space and is used in particular for parallelizing the depth search in shared memory environments. In work theft, each processor maintains a local work queue. When creating a new state, the processor places that state in its local queue. If the processor has no work in its queue, it \"steals\" work from the queue of a busy processor. Strategies for selecting a processor to steal the work and determine the workload are extensively studied (e.g. [58, 15, 17]. However, work theft suffers from performance deterioration in domains where duplicate states play an important role. Romein et al. implemented work theft for IDA * with transposition tables and comparative strategies that are compared to TDS in a distributed storage environment. They showed that TDS was up to 12.6 times faster than work theft in a puzzle domain."}, {"heading": "8.3 Parallel Window Search", "text": "Another approach to parallelizing IDA * is the parallel window search [57], in which each processor searches from the same root node but is assigned a different boundary - i.e., each processor is assigned a different, independent IDA iteration *. When a processor finishes an iteration, it is assigned the next highest boundary that has not yet been assigned to a processor. The first solution found by a parallel window IDA * is not necessarily optimal, but if, after finding a solution in the processor, we wait with boundary b until all processors with a binding of less than b are ready, then the optimal solution found is guaranteed."}, {"heading": "8.4 Parallel Retracting A* (PRA*)", "text": "Parallel Retracting A * (PRA *) [14] simultaneously addresses the problems of work distribution and dual state detection. In PRA *, each processor maintains its own open and closed lists. A hash function maps each state to exactly one processor that \"owns\" the state (as mentioned in Section 4, the hash function used in PRA * was not specified in [14]. When creating a state, PRA * distributes it to the corresponding owner. If the hash keys are distributed evenly across the processors, load balancing is achieved. After receiving states, PRA * has the advantage that the recognition of duplicates at the target processor can be performed efficiently and locally. While PRA * integrated the idea of hash-based work distribution, PRA * differs significantly from a parallel A * by being a parallel version of Retracting A * (RA)."}, {"heading": "9 Parallel A* in Cloud Environments with Practically Un-", "text": "In recent years, the number of those who are able to survive has multiplied; in recent years, the number of those who are able to survive has multiplied; in recent years, the number of those who are able to survive has multiplied."}, {"heading": "9.1 Iterative Allocation Strategy", "text": "A scalable, ravenous algorithm is an algorithm that can run on any number of processors, and its memory consumption increases as it continues to run. HDA * is an example of scalable, ravenous algorithm development when the previous iteration failed. We are looking for a policy that seeks to minimize the total cost. Two realistic assumptions that allow formal analysis are the following: All HOUSES used by IA are identical hardware configurations; second, when a problem is solved on i HOUSES, it is solved on j > i HOUSES (monotonicity)."}, {"heading": "10 Parallel A* and IDA* on Graphics Processing Units", "text": "In fact, the fact is that we are going to be able to be in a position, and that we are going to be able to be able to put ourselves in a position to be in the lead, \"he said."}, {"heading": "11 Other Approaches", "text": "An alternative to partitioning the search space among processors is the parallelization of the calculation performed while processing a single search node (cf. [7, 8]). The Operator Distribution Method for parallel Planning (ODMP) [71] parallelizes the calculation on each node. In ODMP, there is a single control thread and several planning threads. The control thread is responsible for initializing and maintaining the current search state. At each step of the control thread main loop, it generates the applicable operators, inserts them into an operator pool and activates the planning threads. Each planning thread independently extracts an operator from this common operator pool, computes the generated actions, generates the resulting states, evaluates the states with the heuristic function and stores the new state and its heuristic value in a global agenda data structure. After the operator thread is empty, the state extracts the best agenda control."}, {"heading": "Acknowledgements", "text": "This work was partially supported by JSPS KAKENHI grants 25330253 and 17K00296."}], "references": [{"title": "On transposition tables for single-agent search and planning: Summary of results", "author": ["Y. Akagi", "A. Kishimoto", "A. Fukunaga"], "venue": "Proceedings of the 3rd Symposium on Combinatorial Search (SOCS), pp", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Complexity results for SAS+ planning", "author": ["C. B\u00e4ckstr\u00f6m", "B. Nebel"], "venue": "Computational Intelligence 11(4),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "GPU accelerated pathfinding", "author": ["A. Bleiweiss"], "venue": "Proceedings of the EUROGRAPHICS/ACM SIG- GRAPH Conference on Graphics Hardware", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Best-first heuristic search for multicore machines", "author": ["E. Burns", "S. Lemons", "W. Ruml", "R. Zhou"], "venue": "Journal of Artificial Intelligence Research (JAIR)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Best-first heuristic search for multi-core machines", "author": ["E. Burns", "S. Lemons", "R. Zhou", "W. Ruml"], "venue": "Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Implementing fast heuristic search code", "author": ["E.A. Burns", "M. Hatem", "M.J. Leighton", "W. Ruml"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "On the parallelization of UCT", "author": ["T. Cazenave", "N. Jouandeau"], "venue": "Proceedings of Computers and Games CG-08, LNCS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Heuristic search in restricted memory", "author": ["P. Chakrabarti", "S. Ghose", "A. Acharya", "S. de Sarkar"], "venue": "Artificial Intelligence 41(2),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Adaptive parallel iterative deepening search", "author": ["D. Cook", "R. Varnell"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Introduction to Algorithms, Second Edition", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "The MIT Press", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Scalable load balancing strategies for parallel A* algorithms", "author": ["S. Dutt", "N. Mahapatra"], "venue": "Journal of parallel and distributed computing 22,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Heuristic Search: Theory and Applications", "author": ["S. Edelkamp", "S. Schroedl"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "PRA\u2217: Massively parallel heuristic search", "author": ["M. Evett", "J. Hendler", "A. Mahanti", "D. Nau"], "venue": "Journal of Parallel and Distributed Computing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Spielbaumsuche auf massiv parallelen systemen", "author": ["R. Feldmann"], "venue": "Ph.D. thesis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "Kbfs: K-best-first search", "author": ["A. Felner", "S. Kraus", "R.E. Korf"], "venue": "Annals of Mathematics and Artificial Intelligence", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "The implementation of the cilk-5 multithreaded language", "author": ["M. Frigo", "C.E. Leiserson", "K.H. Randall"], "venue": "ACM SIGPLAN Conferences on Programming Language Design and Implementation", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Iterative resource allocation for memory intensive parallel search algorithms on clouds, grids, and shared clusters", "author": ["A. Fukunaga", "A. Kishimoto", "A. Botea"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Limited discrepancy beam search", "author": ["D. Furcy", "S. Koenig"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, pp", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Parallel state space construction for model-checking", "author": ["H. Garavel", "R. Mateescu", "I.M. Smarandache"], "venue": "Proceedings of the 8th International SPIN Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Heavy-tailed phenomena in satisfiability and constraint satisfaction problems", "author": ["C. Gomes", "B. Selman", "N. Crato", "H. Kautz"], "venue": "Journal of Automated Reasoning 24(1-2),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "ManySAT: a parallel SAT solver", "author": ["Y. Hamadi", "S. Jabbour", "L. Sais"], "venue": "Journal on Satisfiability, Boolean Modeling and Computation", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P. Hart", "N. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on System Sciences and Cybernetics SSC-4(2),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1968}, {"title": "The Fast Downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research 26,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Achieving scalability in parallel reachability analysis of very large circuits", "author": ["T. Heyman", "D. Geist", "O. Grumberg", "A. Schuster"], "venue": "Proceedings 12th International Conference on Computer Aided Verification,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "The design of a multicore extension of the SPIN model checker", "author": ["G.J. Holzmann", "D. Bo\u015dna\u0109ki"], "venue": "IEEE Transactions on Software Engineering", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Block-parallel ida* for gpus", "author": ["S. Horie", "A.S. Fukunaga"], "venue": "Proceedings of the Tenth International Symposium on Combinatorial Search, Edited by Alex Fukunaga and Akihiro Kishimoto,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "An economics approach to hard computational problems", "author": ["B. Huberman", "R. Lukose", "T. Hogg"], "venue": "Science 275(5296),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1997}, {"title": "Parallel A* and AO* algorithms: An optimality criterion and performance evaluation", "author": ["K. Irani", "Y. Shih"], "venue": "In: International Conference on Parallel Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1986}, {"title": "Abstract Zobrist hashing: An efficient work distribution method for parallel best-first search", "author": ["Y. Jinnai", "A. Fukunaga"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Automated creation of efficient work distribution functions for parallel best-first search", "author": ["Y. Jinnai", "A. Fukunaga"], "venue": "Proc. ICAPS", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "On work distribution functions for parallel best-first search", "author": ["Y. Jinnai", "A. Fukunaga"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "A randomized parallel branch-and-bound procedure", "author": ["R. Karp", "Y. Zhang"], "venue": "Proceedings of the 20th ACM Symposium on Theory of Computing (STOC), pp", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1988}, {"title": "Randomized parallel algorithms for backtrack search and branch-and-bound computation", "author": ["R. Karp", "Y. Zhang"], "venue": "Journal of the Association for Computing Machinery", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1993}, {"title": "Evaluation of a simple, scalable, parallel best-first search strategy", "author": ["A. Kishimoto", "A. Fukunaga", "A. Botea"], "venue": "Artificial Intelligence 195,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Scalable, parallel best-first search for optimal sequential planning", "author": ["A. Kishimoto", "A.S. Fukunaga", "A. Botea"], "venue": "Proc. ICAPS, pp", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Are many reactive agents better than a few deliberative ones", "author": ["K. Knight"], "venue": "Proceedings of the 13th International Joint Conference on Artificial Intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1993}, {"title": "Sorting and Searching", "author": ["D.E. Knuth"], "venue": "The Art of Computer Programming,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1973}, {"title": "Evaluations of Hash Distributed A* in optimal sequence alignment", "author": ["Y. Kobayashi", "A. Kishimoto", "O. Watanabe"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Depth-first iterative deepening: An optimal admissible tree search", "author": ["R. Korf"], "venue": "Artificial Intelligence 97,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1985}, {"title": "Linear-Space Best-First Search", "author": ["R. Korf"], "venue": "Artificial Intelligence 62(1),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1993}, {"title": "Disjoint pattern database heuristics", "author": ["R.E. Korf", "A. Felner"], "venue": "Artificial Intelligence 134(1-2),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Divide-and-conquer frontier search applied to optimal sequence alignment", "author": ["R.E. Korf", "W. Zhang"], "venue": "Proceedings of the 17th National Conference on Artificial Intelligence", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2000}, {"title": "Load balancing parallel explicit state model checking", "author": ["R. Kumar", "E.G. Mercer"], "venue": "Electronic Notes in Theoretical Computer Science", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Parallel best-first search of state-space graphs: A summary of results", "author": ["V. Kumar", "K. Ramesh", "V.N. Rao"], "venue": "Proceedings of the 7th National Conference on Artificial Intelligence", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1988}, {"title": "Parallel best-first search of state-space graphs: A summary of results", "author": ["V. Kumar", "K. Ramesh", "V.N. Rao"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1988}, {"title": "Distributed-memory model checking with SPIN. In: Theoretical and Practical Aspects of SPIN Model Checking, 5th and 6th International SPIN Workshops", "author": ["F. Lerda", "R. Sisto"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1999}, {"title": "A SIMD approach to parallel heuristic search", "author": ["A. Mahanti", "C. Daniels"], "venue": "Artificial Intelligence", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1993}, {"title": "Scalable global and local hashing strategies for duplicate pruning in parallel A* graph search", "author": ["N. Mahapatra", "S. Dutt"], "venue": "IEEE Transactions on Parallel and Distributed Systems", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1997}, {"title": "Algorithms for distributed termination detection", "author": ["F. Mattern"], "venue": "Distributed Computing 2(3),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1987}, {"title": "Parallel and distributed model checking in Eddy", "author": ["I. Melatti", "R. Palmer", "G. Sawaya", "Y. Yang", "R.M. Kirby", "G. Gopalakrishnan"], "venue": "International Journal on Software Tools for Technology Transfer 11(1),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "Sequential and parallel algorithms for frontier A* with delayed duplicate detection", "author": ["R. Niewiadomski", "J.N. Amaral", "R.C. Holte"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2006}, {"title": "Heuristics - Intelligent Search Strategies for Computer Problem Solving", "author": ["J. Pearl"], "venue": "Addison\u2013Wesley", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1984}, {"title": "PA*SE: Parallel A* for slow expansions", "author": ["M. Phillips", "M. Likhachev", "S. Koenig"], "venue": "In: Proc. ICAPS", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "Depth-first heuristic search on a SIMD machine", "author": ["C. Powley", "C. Ferguson", "R. Korf"], "venue": "Artificial Intelligence", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1993}, {"title": "Single-agent parallel window search", "author": ["C. Powley", "R. Korf"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 13(5),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1991}, {"title": "Parallel depth-first search on multiprocessors part I: Implementation", "author": ["V.N. Rao", "V. Kumar"], "venue": "International Journal of Parallel Programming", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1987}, {"title": "Enhanced iterative-deepening search", "author": ["A. Reinefeld", "T. Marsland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 16(7),", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1994}, {"title": "Wide-area transposition-driven scheduling", "author": ["J.W. Romein", "H.E. Bal"], "venue": "Proceedings of the 10th IEEE International Symposium on High Performance Distributed Computing,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2001}, {"title": "A performance analysis of transposition-tabledriven work scheduling in distributed search", "author": ["J.W. Romein", "H.E. Bal", "J. Schaeffer", "A. Plaat"], "venue": "IEEE Transactions on Parallel and Distributed Systems", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2002}, {"title": "Transposition table driven work scheduling in distributed search", "author": ["J.W. Romein", "A. Plaat", "H.E. Bal", "J. Schaeffer"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1999}, {"title": "Transposition table driven work scheduling in distributed search", "author": ["J.W. Romein", "A. Plaat", "H.E. Bal", "J. Schaeffer"], "venue": "Proceedings of the National Conference on Artificial Intelligence", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1999}, {"title": "Efficient memory-bounded search methods", "author": ["S. Russell"], "venue": "In: Proc. ECAI", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 1992}, {"title": "Parallelizing the Murphi verifier", "author": ["U. Stern", "D.L. Dill"], "venue": "Proceedings of the 9th International Conference on Computed Aided Verification,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1997}, {"title": "Parallelizing the Murphi verifier", "author": ["U. Stern", "D.L. Dill"], "venue": "Formal Methods in System Design 18(2),", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2001}, {"title": "Exploiting the computational power of the graphics card: Optimal state space planning on the GPU", "author": ["D. Sulewski", "S. Edelkamp", "P. Kissmann"], "venue": "Proceedings of the 21st International Conference on Automated Planning and Scheduling,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2011}, {"title": "Arvandherd: Parallel planning with a portfolio", "author": ["R. Valenzano", "H. Nakhost", "M. M\u00fcller", "J. Schaeffer", "N. Sturtevant"], "venue": "Proceedings of the 20th European Conference on Artificial Intelligence,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2012}, {"title": "Simultaneously searching with multiple settings: An alternative to parameter tuning for suboptimal single-agent search algorithms", "author": ["R. Valenzano", "N. Sturtevant", "J. Schaeffer", "K. Buro", "A. Kishimoto"], "venue": "Proceedings of the 20th International Conference on Automated Planning and Scheduling,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2010}, {"title": "Adaptive k-parallel best-first search: A simple but efficient algorithm for multi-core domain-independent planning", "author": ["V. Vidal", "L. Bordeaux", "Y. Hamadi"], "venue": "Proceedings of the 3rd Symposium on Combinatorial Search", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2010}, {"title": "Parallel planning via the distribution of operators", "author": ["D. Vrakas", "I. Refanidis", "I. Vlahavas"], "venue": "Journal of Experimental and Theoretical Artificial Intelligence", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2001}, {"title": "Domain-independent structured duplicate detection", "author": ["R. Zhou", "E. Hansen"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2006}, {"title": "Structured duplicate detection in external-memory graph search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2004}, {"title": "Breadth-first heuristic search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Artificial Intelligence 170(4),", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2006}, {"title": "Domain-independent structured duplicate detection", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI), pp", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2006}, {"title": "Parallel structured duplicate detection", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI), pp", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2007}, {"title": "Massively parallel A* search on a GPU", "author": ["Y. Zhou", "J. Zeng"], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI), pp", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "A new hashing method with application for game playing", "author": ["A.L. Zobrist"], "venue": "reprinted in International Computer Chess Association Journal (ICCA)", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1970}], "referenceMentions": [{"referenceID": 21, "context": "First, in Section 2, we give a formal definition of state-space search, and review the A* algorithm [23], which is the standard, baseline approach for optimally solving state-space search problems.", "startOffset": 100, "endOffset": 104}, {"referenceID": 39, "context": "Limited-memory variants of A* such as IDA* [41] overcome this limitation (at the cost of some search efficiency).", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "The formal definitions presented below are adapted from Edelkamp and Schroedl\u2019s textbook on heuristic search [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Most of the parallel state-space search algorithms presented in this chapter are based on the serial algorithm A* [23].", "startOffset": 114, "endOffset": 118}, {"referenceID": 52, "context": "A* is complete on both finite and infinite graphs [54].", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "If h is an admissible function, then A* using h is admissible [23].", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "It allows us to evaluate the efficiency of a parallel search algorithm, such as HDA* [37], even in very difficult instances where serial A* fails and therefore a direct comparison of the node expansions between HDA* and A* is not possible.", "startOffset": 85, "endOffset": 89}, {"referenceID": 34, "context": "If the fraction is close to 1, then the instance at hand is solved quite efficiently [36].", "startOffset": 85, "endOffset": 89}, {"referenceID": 44, "context": "In an early study, Kumar, Ramesh, and Rao [46] identified two broad approaches to parallelizing best-first search, based on how the usage and maintenance of the open list was parallelized.", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "The most straightforward way to parallelize A* on a shared-memory, multi-core machine is Simple Parallel A* (SPA*) [30], shown in Algorithm 2.", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "However, concurrent access to the shared open list becomes a bottleneck, even if lock-free data structures are used [4] \u2013 in fact, for problems with", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "fast node generation rates, SPA* exhibits runtimes that are slower than single-threaded A* [4].", "startOffset": 91, "endOffset": 94}, {"referenceID": 68, "context": "[70] propose Parallel K-Best First Search, a multi-core version of the K-BFS algorithm [16], a satisficing (non-admissible) best-first search variant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[70] propose Parallel K-Best First Search, a multi-core version of the K-BFS algorithm [16], a satisficing (non-admissible) best-first search variant.", "startOffset": 87, "endOffset": 91}, {"referenceID": 53, "context": "have proposed PA*SE, a mechanism for reducing node re-expansions in SPA* [55] that only expands nodes when their g-values are optimal, ensuring that nodes are not re-expanded.", "startOffset": 73, "endOffset": 77}, {"referenceID": 45, "context": "Kumar, Ramesh and Rao [47], as well as Karp and Zhang [34, 35] proposed a random work allocation strategy, where newly generated states are sent to random processors, i.", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "Kumar, Ramesh and Rao [47], as well as Karp and Zhang [34, 35] proposed a random work allocation strategy, where newly generated states are sent to random processors, i.", "startOffset": 54, "endOffset": 62}, {"referenceID": 33, "context": "Kumar, Ramesh and Rao [47], as well as Karp and Zhang [34, 35] proposed a random work allocation strategy, where newly generated states are sent to random processors, i.", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 42, "context": ", [44, 72]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 70, "context": ", [44, 72]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 44, "context": "In a decentralized parallel A*, when a solution is discovered, there is no guarantee at that time that the solution is optimal [46].", "startOffset": 127, "endOffset": 131}, {"referenceID": 49, "context": "A commonly used method is by Mattern [51].", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "[14], a limited-memory best-first search algorithm for a massively parallel SIMD machine (see Section 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "It was then used in a parallelization of SEQ A*, a variant of A* that performs partial expansion of states, on a hypercube by Mahapatra and Dutt [50], who called the technique Global Hashing (GOHA).", "startOffset": 145, "endOffset": 149}, {"referenceID": 60, "context": "or Mahapatra and Dutt, as their work encompassed significantly more than this work distribution mechanism Transposition-Table-Driven Work Scheduling (TDS) [62] is a distributedmemory, parallel IDA* with hash-based work distribution (see Section 8.", "startOffset": 155, "endOffset": 159}, {"referenceID": 35, "context": "Kishimoto, Fukunaga, and Botea reopened investigation into hash-based work distribution for A* by implementing HDA*, a straightforward application of hash-based work distribution to A*, showing that it scaled quite well on both multi-core machines and large-scale clusters [37, 36].", "startOffset": 273, "endOffset": 281}, {"referenceID": 34, "context": "Kishimoto, Fukunaga, and Botea reopened investigation into hash-based work distribution for A* by implementing HDA*, a straightforward application of hash-based work distribution to A*, showing that it scaled quite well on both multi-core machines and large-scale clusters [37, 36].", "startOffset": 273, "endOffset": 281}, {"referenceID": 76, "context": "1), HDA* used the Zobrist hash function [78].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "Recently, Jinnai and Fukunaga compared hash distribution functions that have been used in the literature, showing that the Zobrist hash function as well as Abstract Zobrist hashing, an improved version of the Zobrist function, significantly outperforms other hash functions which have been used in the literature [31].", "startOffset": 313, "endOffset": 317}, {"referenceID": 4, "context": "This results in significant synchronization overhead \u2013 for example, it was observed in [5] that a straightforward implementation of PRA* exhibited extremely poor performance on the Grid search problem, and multi-core performance for up to 8 cores was consistently slower than sequential A*.", "startOffset": 87, "endOffset": 90}, {"referenceID": 23, "context": "2Even if the heuristic function [25] is consistent, parallel A* search may sometimes have to reopen a state saved in the closed list.", "startOffset": 32, "endOffset": 36}, {"referenceID": 61, "context": "[63] showed that this communication overhead could be overcome by packing multiple states with the same destination into a single message.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "In [37, 36], 100 states are packed into each message on a commodity cluster using more than 16 CPU cores and a HPC cluster, while 10 states are packed on the commodity cluster using fewer than 16 cores.", "startOffset": 3, "endOffset": 11}, {"referenceID": 34, "context": "In [37, 36], 100 states are packed into each message on a commodity cluster using more than 16 CPU cores and a HPC cluster, while 10 states are packed on the commodity cluster using fewer than 16 cores.", "startOffset": 3, "endOffset": 11}, {"referenceID": 52, "context": "While the use of abstractions as the basis for heuristic functions has a long history [54], the use of abstractions as a mechanism for partitioning search states originated in Structured Duplicate Detection (SDD), an external memory search which stores explored states on disk [73].", "startOffset": 86, "endOffset": 90}, {"referenceID": 71, "context": "While the use of abstractions as the basis for heuristic functions has a long history [54], the use of abstractions as a mechanism for partitioning search states originated in Structured Duplicate Detection (SDD), an external memory search which stores explored states on disk [73].", "startOffset": 277, "endOffset": 281}, {"referenceID": 74, "context": "Parallel Structured Duplicate Detection (PSDD) is a parallel search algorithm that exploits n-blocks to address both synchronization overhead and communication overhead [76].", "startOffset": 169, "endOffset": 173}, {"referenceID": 72, "context": "While PSDD uses disjoint duplicate detection scopes to parallelize breadth-first heuristic search [74], Parallel Best-NBlock-First (PBNF) [4] extends PSDD to best-first search on multi-core machines by ensuring that n-blocks with the best current f -values are assigned to processors.", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "While PSDD uses disjoint duplicate detection scopes to parallelize breadth-first heuristic search [74], Parallel Best-NBlock-First (PBNF) [4] extends PSDD to best-first search on multi-core machines by ensuring that n-blocks with the best current f -values are assigned to processors.", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "proposed SafePBNF, a livelock-free version of PBNF [4].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "[4] also proposed AHDA*, a variant of HDA* using an abstraction-based node distribution function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "Recent work has investigated the performance characteristics and tradeoffs among various hashing strategies, resulting in a significantly better understanding of previous hashing strategies, as well as new hashing strategies that combine previous methods in order to obtain superior performance [31, 33].", "startOffset": 295, "endOffset": 303}, {"referenceID": 31, "context": "Recent work has investigated the performance characteristics and tradeoffs among various hashing strategies, resulting in a significantly better understanding of previous hashing strategies, as well as new hashing strategies that combine previous methods in order to obtain superior performance [31, 33].", "startOffset": 295, "endOffset": 303}, {"referenceID": 9, "context": "The multiplication method H(\u03ba) is a widely used hashing method that has been observed to hash a random key to P slots with almost equal likelihood [11].", "startOffset": 147, "endOffset": 151}, {"referenceID": 48, "context": "Multiplicative hashing M(s) uses this function to achieve good load balancing of nodes among processors [50]:", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "Typically A = ( \u221a 5 \u2212 1)/2 (the golden ratio) is used since the hash function is known to work well with this value of A [39].", "startOffset": 121, "endOffset": 125}, {"referenceID": 35, "context": "[37, 36] noted that it is desirable to use a hash function that uniformly distributed nodes among processors, and used the Zobrist hash function [78], described below.", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "[37, 36] noted that it is desirable to use a hash function that uniformly distributed nodes among processors, and used the Zobrist hash function [78], described below.", "startOffset": 0, "endOffset": 8}, {"referenceID": 76, "context": "[37, 36] noted that it is desirable to use a hash function that uniformly distributed nodes among processors, and used the Zobrist hash function [78], described below.", "startOffset": 145, "endOffset": 149}, {"referenceID": 35, "context": "[37, 36], which used Zobrist hashing, as ZHDA* or HDA\u2217[Z ].", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "[37, 36], which used Zobrist hashing, as ZHDA* or HDA\u2217[Z ].", "startOffset": 0, "endOffset": 8}, {"referenceID": 30, "context": "Operator-based Zobrist hashing (OZHDA*) [32] partially addresses this problem by manipulating the random bit strings in R, the table used to compute Zobrist hash values, such that for some selected states S, there are some operators A(s) for s \u2208 S such that the successors of s that are generated when a \u2208 A(s) is applied to s are guaranteed to have the same Zobrist hash value as s, which ensures that they are assigned to the same processor as s.", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "Jinnai and Fukunaga [32] showed that OZHDA* significantly reduces communication overhead compared to Zobrist hashing [32].", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "Jinnai and Fukunaga [32] showed that OZHDA* significantly reduces communication overhead compared to Zobrist hashing [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "[4] proposed AHDA*, which uses abstraction based node assignment.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "PBNF [4] and PSDD [76].", "startOffset": 5, "endOffset": 8}, {"referenceID": 74, "context": "PBNF [4] and PSDD [76].", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "[4] assigns abstract states to processors using a perfect hashing and a modulus operator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] implemented the hashing strategy using a perfect hashing and a modulus operator, and an abstraction strategy following the construction for SDD [75] (for domain-independent planning), or a hand-crafted abstraction (for the sliding-tile puzzle and grid path-finding domains).", "startOffset": 0, "endOffset": 3}, {"referenceID": 73, "context": "[4] implemented the hashing strategy using a perfect hashing and a modulus operator, and an abstraction strategy following the construction for SDD [75] (for domain-independent planning), or a hand-crafted abstraction (for the sliding-tile puzzle and grid path-finding domains).", "startOffset": 148, "endOffset": 152}, {"referenceID": 30, "context": "Jinnai and Fukunaga showed that AHDA* with a static Nmax threshold performed poorly for a benchmark set with varying difficulty because a fixed size abstract graph results in very poor load balance, and proposed Dynamic AHDA* (DAHDA*), which dynamically sets the size of the abstract graph according to the number of features (the state space size is exponential in the number of features) [32].", "startOffset": 390, "endOffset": 394}, {"referenceID": 38, "context": "HDA* suffers significantly from increased search overhead in the multiple sequence alignment (MSA) domain whose search space is a directed acyclic graph with non-uniform edge costs [40].", "startOffset": 181, "endOffset": 185}, {"referenceID": 38, "context": "Choosing a good value for d is important for achieving satisfactory parallel performance (see [40] for details).", "startOffset": 94, "endOffset": 98}, {"referenceID": 48, "context": "LOHA [50] distributes work with a hash function taking into account locality for the Traveling Salesperson Problem where the search space is represented as a levelized graph.", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "See [33] for a more detailed comparisons", "startOffset": 4, "endOffset": 8}, {"referenceID": 29, "context": "\u2022 AZHDA*: HDA* using Abstract Zobrist hashing [31] \u2022 ZHDA*: HDA* using Zobrist hashing [36] \u2022 AHDA*: HDA* using abstraction based work distribution [4]", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "\u2022 AZHDA*: HDA* using Abstract Zobrist hashing [31] \u2022 ZHDA*: HDA* using Zobrist hashing [36] \u2022 AHDA*: HDA* using abstraction based work distribution [4]", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "\u2022 AZHDA*: HDA* using Abstract Zobrist hashing [31] \u2022 ZHDA*: HDA* using Zobrist hashing [36] \u2022 AHDA*: HDA* using abstraction based work distribution [4]", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "\u2022 SafePBNF: [4] \u2022 HDA*+GOHA: HDA* using multiplicative hashing, a hash function proposed in [50] \u2022 Randomized strategy: nodes are sent to random cores (duplicate nodes are not guaranteed to be sent to the same core) [47, 35]", "startOffset": 12, "endOffset": 15}, {"referenceID": 48, "context": "\u2022 SafePBNF: [4] \u2022 HDA*+GOHA: HDA* using multiplicative hashing, a hash function proposed in [50] \u2022 Randomized strategy: nodes are sent to random cores (duplicate nodes are not guaranteed to be sent to the same core) [47, 35]", "startOffset": 92, "endOffset": 96}, {"referenceID": 45, "context": "\u2022 SafePBNF: [4] \u2022 HDA*+GOHA: HDA* using multiplicative hashing, a hash function proposed in [50] \u2022 Randomized strategy: nodes are sent to random cores (duplicate nodes are not guaranteed to be sent to the same core) [47, 35]", "startOffset": 216, "endOffset": 224}, {"referenceID": 33, "context": "\u2022 SafePBNF: [4] \u2022 HDA*+GOHA: HDA* using multiplicative hashing, a hash function proposed in [50] \u2022 Randomized strategy: nodes are sent to random cores (duplicate nodes are not guaranteed to be sent to the same core) [47, 35]", "startOffset": 216, "endOffset": 224}, {"referenceID": 28, "context": "\u2022 Simple Parallel A* (centralized, single OPEN list) [30]", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "The code for the experiment (based on the code by [4]) is available at https://github.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "Following [4], we implemented open list using a binary heap.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "For example, for ZHDA*, domain-independent feature generation for classical planning problems represented in the SAS+ representation [2] is straightforward [36].", "startOffset": 133, "endOffset": 136}, {"referenceID": 34, "context": "For example, for ZHDA*, domain-independent feature generation for classical planning problems represented in the SAS+ representation [2] is straightforward [36].", "startOffset": 156, "endOffset": 160}, {"referenceID": 3, "context": "[4] used the greedy abstraction algorithms by Zhou and Hansen [75] to select the subset of features [4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 73, "context": "[4] used the greedy abstraction algorithms by Zhou and Hansen [75] to select the subset of features [4].", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "[4] used the greedy abstraction algorithms by Zhou and Hansen [75] to select the subset of features [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 30, "context": "Methods based on the domain-transition graph are proposed in [32, 33].", "startOffset": 61, "endOffset": 69}, {"referenceID": 31, "context": "Methods based on the domain-transition graph are proposed in [32, 33].", "startOffset": 61, "endOffset": 69}, {"referenceID": 63, "context": "Parallel Mur\u03c6 [65, 66] addresses verification tasks that involve exhaustively enumerating all reachable states in a state space, and implements a hash-based work distribution schema where each state is assigned to a unique owner processor.", "startOffset": 14, "endOffset": 22}, {"referenceID": 64, "context": "Parallel Mur\u03c6 [65, 66] addresses verification tasks that involve exhaustively enumerating all reachable states in a state space, and implements a hash-based work distribution schema where each state is assigned to a unique owner processor.", "startOffset": 14, "endOffset": 22}, {"referenceID": 43, "context": "Kumar and Mercer [45] present a load balancing technique as an alternative to the hash-based work distribution implemented in Mur\u03c6.", "startOffset": 17, "endOffset": 21}, {"referenceID": 50, "context": "The Eddy Murphi model checker [52] specializes processors\u2019 tasks, defining two threads for each processing node.", "startOffset": 30, "endOffset": 34}, {"referenceID": 46, "context": "Lerda and Sisto parallelized the SPIN model checker to increase the availability of memory resources [48].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "Holzmann and Bo\u015dna\u0109ki [27]", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "[20] use hashbased work distribution to convert an implicitly defined model-checking state space into an explicit file representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Symbolic parallel model checking has been addressed in [26].", "startOffset": 55, "endOffset": 59}, {"referenceID": 63, "context": ", [65, 66, 48, 20]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 64, "context": ", [65, 66, 48, 20]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 46, "context": ", [65, 66, 48, 20]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 18, "context": ", [65, 66, 48, 20]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 27, "context": "An algorithm portfolio [29] is often employed and parallelized in other domains, such as the ManySAT solver [22] for SAT solving and ArvandHerd [68] for satisficing planning.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "An algorithm portfolio [29] is often employed and parallelized in other domains, such as the ManySAT solver [22] for SAT solving and ArvandHerd [68] for satisficing planning.", "startOffset": 108, "endOffset": 112}, {"referenceID": 66, "context": "An algorithm portfolio [29] is often employed and parallelized in other domains, such as the ManySAT solver [22] for SAT solving and ArvandHerd [68] for satisficing planning.", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "A long-tailed distribution is often observed in the runtime distribution of the search algorithms [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 36, "context": "Dovetailing [38], which is a simple version of the algorithm portfolio, performs search simultaneously with different parameter settings.", "startOffset": 12, "endOffset": 16}, {"referenceID": 67, "context": "apply parallel dovetailing [69] to the weighted versions of IDA* [41], RBFS [42], A*, PBNF [4], as well as BULB [19], a suboptimal heuristic search.", "startOffset": 27, "endOffset": 31}, {"referenceID": 39, "context": "apply parallel dovetailing [69] to the weighted versions of IDA* [41], RBFS [42], A*, PBNF [4], as well as BULB [19], a suboptimal heuristic search.", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "apply parallel dovetailing [69] to the weighted versions of IDA* [41], RBFS [42], A*, PBNF [4], as well as BULB [19], a suboptimal heuristic search.", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "apply parallel dovetailing [69] to the weighted versions of IDA* [41], RBFS [42], A*, PBNF [4], as well as BULB [19], a suboptimal heuristic search.", "startOffset": 91, "endOffset": 94}, {"referenceID": 17, "context": "apply parallel dovetailing [69] to the weighted versions of IDA* [41], RBFS [42], A*, PBNF [4], as well as BULB [19], a suboptimal heuristic search.", "startOffset": 112, "endOffset": 116}, {"referenceID": 67, "context": "[69] indicate that parallel dovetailing often yields good speedups and solves additional problem instances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "A*-based planners for domain-independent, classical planning such as Fast Downward [24] generate between 10\u221210 nodes per second on standard International Planning Competition benchmark domains.", "startOffset": 83, "endOffset": 87}, {"referenceID": 5, "context": "Highly optimized solvers for specific domains such as the sliding-tile puzzle can generate over 10\u221210 nodes per second [6], consuming memory even faster.", "startOffset": 119, "endOffset": 122}, {"referenceID": 39, "context": "A* (IDA*) [41].", "startOffset": 10, "endOffset": 14}, {"referenceID": 39, "context": "Although each iteration revisits all of the nodes visited on all of the previous iterations, many search spaces have the property that the runtime of iterative deepening is dominated by the search performed in the last few iterations, so the overhead of repeating the work done in past iterations is relatively small as a fraction of the total search effort [41].", "startOffset": 358, "endOffset": 362}, {"referenceID": 57, "context": "To alleviate this problem with standard IDA*, a transposition table, which is a cache of lower bounds on the solution cost achievable for previously visited states, can be added so that search is pruned if a search reaches a previously visited state and it can be proven that the pruning does not result in loss of optimality [59, 1].", "startOffset": 326, "endOffset": 333}, {"referenceID": 0, "context": "To alleviate this problem with standard IDA*, a transposition table, which is a cache of lower bounds on the solution cost achievable for previously visited states, can be added so that search is pruned if a search reaches a previously visited state and it can be proven that the pruning does not result in loss of optimality [59, 1].", "startOffset": 326, "endOffset": 333}, {"referenceID": 7, "context": "Other limited-memory A* variants include MA* [9], SMA* [64], and recursive best-first search [42].", "startOffset": 45, "endOffset": 48}, {"referenceID": 62, "context": "Other limited-memory A* variants include MA* [9], SMA* [64], and recursive best-first search [42].", "startOffset": 55, "endOffset": 59}, {"referenceID": 40, "context": "Other limited-memory A* variants include MA* [9], SMA* [64], and recursive best-first search [42].", "startOffset": 93, "endOffset": 97}, {"referenceID": 61, "context": "Transposition-Table-Driven work scheduling (TDS) [63, 61] is a distributed-memory, parallel IDA* algorithm that uses a distributed transposition table.", "startOffset": 49, "endOffset": 57}, {"referenceID": 59, "context": "Transposition-Table-Driven work scheduling (TDS) [63, 61] is a distributed-memory, parallel IDA* algorithm that uses a distributed transposition table.", "startOffset": 49, "endOffset": 57}, {"referenceID": 61, "context": "eliminates such a back-propagation procedure, thus reducing communication overhead in exchange for giving up the use of more informed lower bounds (see [63, 61] for details).", "startOffset": 152, "endOffset": 160}, {"referenceID": 59, "context": "eliminates such a back-propagation procedure, thus reducing communication overhead in exchange for giving up the use of more informed lower bounds (see [63, 61] for details).", "startOffset": 152, "endOffset": 160}, {"referenceID": 61, "context": "[63, 61] showed that TDS exhibits a very low (sometimes negative) search overhead and yields significant (sometimes super-linear) speedups in solving puzzles on a distributed-memory machine, compared to a sequential IDA* that runs on a single computational node with limited RAM capacity.", "startOffset": 0, "endOffset": 8}, {"referenceID": 59, "context": "[63, 61] showed that TDS exhibits a very low (sometimes negative) search overhead and yields significant (sometimes super-linear) speedups in solving puzzles on a distributed-memory machine, compared to a sequential IDA* that runs on a single computational node with limited RAM capacity.", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "showed that HDA* was consistently faster than TDS but sometimes terminates its execution due to memory exhaustion [36].", "startOffset": 114, "endOffset": 118}, {"referenceID": 56, "context": "[58, 15, 17]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[58, 15, 17]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "[58, 15, 17]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 59, "context": "9 times faster than work stealing in puzzle solving domains [61].", "startOffset": 60, "endOffset": 64}, {"referenceID": 58, "context": "Therefore, Romein and Bal combined TDS with work stealing [60] in a grid environment where the communication latency is high between PC clusters and is low within each cluster.", "startOffset": 58, "endOffset": 62}, {"referenceID": 54, "context": "Variants of work stealing-based IDA* for Single Instruction, Multiple Data (SIMD) architecture machines have also been studied [56, 49].", "startOffset": 127, "endOffset": 135}, {"referenceID": 47, "context": "Variants of work stealing-based IDA* for Single Instruction, Multiple Data (SIMD) architecture machines have also been studied [56, 49].", "startOffset": 127, "endOffset": 135}, {"referenceID": 55, "context": "Another approach to parallelizing IDA* is parallel-window search [57], where each processor searches from the same root node, but is assigned a different bound \u2013 that is, each processor is assigned a different, independent iteration of IDA*.", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "Parallel Retracting A* (PRA*) [14] simultaneously addresses the problems of work distribution and duplicate state detection.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "A hash function maps each state to exactly one processor which \u201cowns\u201d the state (as mentioned in Section 4, the hash function used in PRA* was not specified in [14]).", "startOffset": 160, "endOffset": 164}, {"referenceID": 12, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of Retracting A* (RA)* [14], a limited-memory search algorithm closely related to MA* [9] and SMA* [64].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of Retracting A* (RA)* [14], a limited-memory search algorithm closely related to MA* [9] and SMA* [64].", "startOffset": 231, "endOffset": 234}, {"referenceID": 62, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of Retracting A* (RA)* [14], a limited-memory search algorithm closely related to MA* [9] and SMA* [64].", "startOffset": 244, "endOffset": 248}, {"referenceID": 12, "context": "On the other hand, the implementation of this retraction mechanism in [14] incurs a significant synchronization overhead: when a processor P generates a new state s and sends it to the destination processor Q, P blocks and waits for Q to confirm that s has been successfully received and stored (or whether the send operation failed due to memory exhaustion at the destination processor).", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "The iterative allocation (IA) strategy [18] repeatedly runs a ravenous algorithm a until the problem is solved.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "A particularly simple but useful strategy is the Geometric (b) Strategy, which was analyzed and evaluated by [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "This observation was experimentlly validated in [18] for domain-independent planning benchmarks and sequence alignment benchmarks.", "startOffset": 48, "endOffset": 52}, {"referenceID": 34, "context": "In addition, HDA* has been observed to exhaust memory within 20 minutes on every planning and 24-puzzle problem studied in [36].", "startOffset": 123, "endOffset": 127}, {"referenceID": 75, "context": "Zhou and Zeng propose a GPU-based A* algorithm using many (thousands) of parallel priority queues (OPEN lists) [77].", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "Horie and Fukunaga developed Block-Parallel IDA* (BPIDA*) [28], a parallel version of IDA* [41] for the GPU.", "startOffset": 58, "endOffset": 62}, {"referenceID": 39, "context": "Horie and Fukunaga developed Block-Parallel IDA* (BPIDA*) [28], a parallel version of IDA* [41] for the GPU.", "startOffset": 91, "endOffset": 95}, {"referenceID": 54, "context": "Although the single instruction, multi-thread architecture used in NVIDIA GPUs is somewhat similar to earlier SIMD architectures, Horie and Fukunaga found that simply porting earlier SIMD IDA* approaches [56, 49] to the GPU results in extremely poor performance due to warp divergence and load balancing overheads.", "startOffset": 204, "endOffset": 212}, {"referenceID": 47, "context": "Although the single instruction, multi-thread architecture used in NVIDIA GPUs is somewhat similar to earlier SIMD architectures, Horie and Fukunaga found that simply porting earlier SIMD IDA* approaches [56, 49] to the GPU results in extremely poor performance due to warp divergence and load balancing overheads.", "startOffset": 204, "endOffset": 212}, {"referenceID": 41, "context": ", pattern databases [43]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 57, "context": "Using such memoryintensive heuristics (as well as other memory-intensive methods such as a transposition tables [59]) on the GPU will require using the global memory and is a direction for future work.", "startOffset": 112, "endOffset": 116}, {"referenceID": 65, "context": "One instance of such a hybrid GPU/CPU based approach is for best-fist search with a blind heuristic by Sulweski et al [67].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "Finally, a different application of many-core GPU architectures is for multi-agent search, where each core executes an independent A* search for each agent in the simulation environment [3].", "startOffset": 186, "endOffset": 189}, {"referenceID": 6, "context": ", [7, 8]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 69, "context": "The Operator Distribution Method for parallel Planning (ODMP) [71] parallelizes the computation at each node.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "The EUREKA system [10] used machine learning to automatically configure parallel IDA* for various problems (including nonlinear planning) and machine architectures.", "startOffset": 18, "endOffset": 22}, {"referenceID": 51, "context": "[53] propose PFA*-DDD, a parallel version of Frontier A* with Delayed Duplicate Detection.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "A* is a best-first search algorithm for finding optimal-cost paths in graphs. A* benefits significantly from parallelism because in many applications, A* is limited by memory usage, so distributed memory implementations of A* that use all of the aggregate memory on the cluster enable problems that can not be solved by serial, single-machine implementations to be solved. We survey approaches to parallel A*, focusing on decentralized approaches to A* which partition the state space among processors. We also survey approaches to parallel, limited-memory variants of A* such as parallel IDA*.", "creator": "LaTeX with hyperref package"}}}