{"id": "1708.05521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "abstract": "In this paper we describe a deep learning system that has been designed and built for the WASSA 2017 Emotion Intensity Shared Task. We introduce a representation learning approach based on inner attention on top of an RNN. Results show that our model offers good capabilities and is able to successfully identify emotion-bearing words to predict intensity without leveraging on lexicons, obtaining the 13th place among 22 shared task competitors.", "histories": [["v1", "Fri, 18 Aug 2017 06:59:16 GMT  (64kb,D)", "http://arxiv.org/abs/1708.05521v1", "WASSA 2017 Shared Task on Emotion Intensity"]], "COMMENTS": "WASSA 2017 Shared Task on Emotion Intensity", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["edison marrese-taylor", "yutaka matsuo"], "accepted": false, "id": "1708.05521"}, "pdf": {"name": "1708.05521.pdf", "metadata": {"source": "CRF", "title": "EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "authors": ["Edison Marrese-Taylor", "Yutaka Matsuo"], "emails": ["emarrese@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "Place among 22 competitors."}, {"heading": "1 Introduction", "text": "Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge and convey emotions, feelings and attitudes that can be studied to understand the expression of emotions in the text and the associated social phenomena. When studying emotions in the text, it is generally useful to characterize the emotional charge of a passage by their words. Some words have an impact as a core component of their meaning. For example, depressed and wistful denote a certain level of sadness and are therefore associated with sadness. On the other hand, some words are associated with affects, although they do not designate any influence. For example, failure and death describe concepts that are normally accompanied by sadness and thus determine a certain level of sadness. When analyzing the emotional content in the text, Mozart's tasks are always presented almost as classification tasks, which are often useful in identifying emotions."}, {"heading": "2 Proposed Approach", "text": "Our work relates to deep learning techniques for the recognition of emotions in images (Dhall et al., 2015) and videos (Ebrahimi Kahou et al., 2015), as well as to the classification of emotions (Lakomkin et al., 2017). Our work also relates to Liu and Lane (2016), who introduced attention to RNar Xiv: 170 8.05 521v 1 [cs.C L] 18 August 201 7for filling slots in Natural Language Understanding. Since in the task the input-output alignment is explicit, they investigated how alignment can best be used in encoder decoder models, which suggest that the attention mechanisms are helpful. EmoAtt is based on a bi-directional RNN that includes an embedded input sequence x = {x1,..., xn} and returns a list of hidden vectors that capture the context."}, {"heading": "3 Experimental Setup", "text": "We also use tweets for four emotions: joy, sadness, fear and anger. These have been commented on with Best-Worst Scaling (BWS) to get very reliable results (Kiritchenko and Mohammad, 2016). We experimented with a vocabulary of 1.2 M. To prepare the data, we used Twokenizer (Gimpel et al., 2011), which basically provides a set of curated rules for dividing the tweets into tokens. We use Tweeboparser (Owoputi et al.) to get the POS tags for each tweet."}, {"heading": "4 Results and Discussion", "text": "In this section, we report on the results of the experiments we have conducted to test our proposed model. In general, as Table 2 shows, our intra-sentence attention RNN was able to surpass the Weka baseline (Mohammad and Bravo-Marquez, 2017a) 2github.com / epochx / emoatton's development record by a solid margin. Furthermore, the model succeeds in doing so without additional resources, except for pre-trained word embeddings. However, these results are reversed for the test record, where our model performs worse than the baseline. This shows that the model is unable to generalize well what we are doing for the lack of semantic information due to the vocabulary gap we have observed between the data sets and the GloVe embeddings. To confirm the usefulness of our binary properties, we have conducted an ablation experiment and trained our best models without actually seeing our results in relation to the table 3."}, {"heading": "4.1 Anger Dataset", "text": "For the Fury dataset, our experiments showed that GloVe dimension embedding outperformed 50 others and achieved an average gain of 0.066 correlation to size 25 embedding and 0.021 for size 100 embedding. However, the first of these values was significant, with a p-value of 3.86 \u00d7 10 \u2212 5. Regarding the hidden size of the RNN, we could not detect any statistical difference between the tested values."}, {"heading": "4.2 Joy Dataset", "text": "In the Joy dataset, our experiments showed that GloVe vectors of the dimension 50 performed better than others, in this case with an average correlation gain of 0.052 (p = 5.6 x 10 \u2212 2) over embedded size 100 and 0.062 (p = 3.1 x 10 \u2212 2) over 50 hidden units. Regarding the hidden size of the RNN, we observed that 100 hidden units performed better in our experiments, with an average absolute gain of 0.052 (p = 6.5 x 10 \u2212 2) over 50 hidden units. Compared to models with 200 hidden units, the difference in performance was not statistically significant."}, {"heading": "4.3 Fear Dataset", "text": "With respect to the fear data set, we again observed that size 50 embedding yielded the best results, with average gains of 0.12 (p = 7 \u00b7 10 \u2212 4) and 0.11 (p = 1.9 \u00b7 10 \u2212 3) for size 25 and 100, respectively. With regard to the size of the RNN-hidden state, our experiments showed that the use of 100 hidden units yielded the best results, with average absolute gains of 0.117 (p = 9 \u00b7 10 \u2212 4) and 0.108 (p = 0.002.4 \u00b7 10 \u2212 3) over size 50 and 200, respectively."}, {"heading": "4.4 Sadness Dataset", "text": "Finally, in the sadness datasets, we experimentally observed that the use of embedding 50 yielded the best results, with a statistically significant average gain of 0.092 correlation points (p = 1.3 \u00d7 10 \u2212 3) over size 25. The results were statistically equivalent for size 100. We also observed that the use of 50 or 100 hidden units for the RNN yielded statistically equivalent results, while both performed better than using a hidden size of 200."}, {"heading": "5 Conclusions", "text": "In this paper, we introduced an intra-sentence attention RNN for emotional intensity that we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model uses no external information except for pre-trained embedding and is capable of surpassing the Weka baseline for the development set, but not for the test set. In the shared task, it ranked 13th out of 22 competitors."}], "references": [{"title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015", "author": ["Abhinav Dhall", "O.V. Ramana Murthy", "Roland Goecke", "Jyoti Joshi", "Tom Gedeon."], "venue": "Proceedings of the 2015 ACM on International Con-", "citeRegEx": "Dhall et al\\.,? 2015", "shortCiteRegEx": "Dhall et al\\.", "year": 2015}, {"title": "Recurrent neural networks for emotion recognition in video", "author": ["Samira Ebrahimi Kahou", "Vincent Michalski", "Kishore Konda", "Roland Memisevic", "Christopher Pal."], "venue": "Proceedings of the 2015 ACM on International Confer-", "citeRegEx": "Kahou et al\\.,? 2015", "shortCiteRegEx": "Kahou et al\\.", "year": 2015}, {"title": "Part-ofspeech tagging for twitter: Annotation", "author": ["Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Capturing reliable fine-grained sentiment associations by crowdsourcing and best\u2013worst scaling", "author": ["Svetlana Kiritchenko", "Saif M. Mohammad."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Kiritchenko and Mohammad.,? 2016", "shortCiteRegEx": "Kiritchenko and Mohammad.", "year": 2016}, {"title": "Automatically augmenting an emotion dataset improves classification using audio", "author": ["Egor Lakomkin", "Cornelius Weber", "Stefan Wermter."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-", "citeRegEx": "Lakomkin et al\\.,? 2017", "shortCiteRegEx": "Lakomkin et al\\.", "year": 2017}, {"title": "AttentionBased Recurrent Neural Network Models for Joint Intent Detection and Slot Filling", "author": ["Bing Liu", "Ian Lane."], "venue": "Interspeech 2016. pages 685\u2013689. https://doi.org/10.21437/Interspeech.2016-1352.", "citeRegEx": "Liu and Lane.,? 2016", "shortCiteRegEx": "Liu and Lane.", "year": 2016}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["Grgoire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio."], "venue": "INTERSPEECH. pages 3771 \u20133775.", "citeRegEx": "Mesnil et al\\.,? 2013", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Emotion intensities in tweets", "author": ["Saif M. Mohammad", "Felipe Bravo-Marquez."], "venue": "Proceedings of the", "citeRegEx": "Mohammad and Bravo.Marquez.,? 2017a", "shortCiteRegEx": "Mohammad and Bravo.Marquez.", "year": 2017}, {"title": "WASSA-2017 Shared Task on Emotion Intensity", "author": ["Saif M. Mohammad", "Felipe Bravo-Marquez."], "venue": "Proceedings of the EMNLP 2017 Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media (WASSA). Copen-", "citeRegEx": "Mohammad and Bravo.Marquez.,? 2017b", "shortCiteRegEx": "Mohammad and Bravo.Marquez.", "year": 2017}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems. pages 2773\u2013 2781. http://papers.nips.cc/paper/5635-grammar-", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "To this end, the WASSA-2017 Shared Task on Emotion Intensity (Mohammad and Bravo-Marquez, 2017b) represents the first", "startOffset": 61, "endOffset": 96}, {"referenceID": 0, "context": "Our work is related to deep learning techniques for emotion recognition in images (Dhall et al., 2015) and videos (Ebrahimi Kahou et al.", "startOffset": 82, "endOffset": 102}, {"referenceID": 4, "context": ", 2015), as well as and emotion classification (Lakomkin et al., 2017).", "startOffset": 47, "endOffset": 70}, {"referenceID": 0, "context": "Our work is related to deep learning techniques for emotion recognition in images (Dhall et al., 2015) and videos (Ebrahimi Kahou et al., 2015), as well as and emotion classification (Lakomkin et al., 2017). Our work is also related to Liu and Lane (2016), who introduced an attention RNN ar X iv :1 70 8.", "startOffset": 83, "endOffset": 256}, {"referenceID": 6, "context": "To improve the capabilities of the RNN to capture short-term temporal dependencies (Mesnil et al., 2013), we define the following:", "startOffset": 83, "endOffset": 104}, {"referenceID": 11, "context": "tional component in a fashion similar to Vinyals et al. (2015). Formally,", "startOffset": 41, "endOffset": 63}, {"referenceID": 7, "context": "To test our model, we experiment using the training, validation and test datasets provided for the shared task (Mohammad and Bravo-Marquez, 2017a), which include tweets for four emotions: joy, sadness, fear, and anger.", "startOffset": 111, "endOffset": 146}, {"referenceID": 3, "context": "These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores (Kiritchenko and Mohammad, 2016).", "startOffset": 83, "endOffset": 115}, {"referenceID": 10, "context": "We experimented with GloVe1 (Pennington et al., 2014) as pre-trained word embedding vectors, for sizes 25, 50 and 100.", "startOffset": 28, "endOffset": 53}, {"referenceID": 2, "context": "To pre-process the data, we used Twokenizer (Gimpel et al., 2011), which basically provides a set of curated rules to split", "startOffset": 44, "endOffset": 65}, {"referenceID": 9, "context": "We also use Tweeboparser (Owoputi et al., 2013) to get the POS-tags for each tweet.", "startOffset": 25, "endOffset": 47}, {"referenceID": 7, "context": "In general, as Table 2 shows, our intra-sentence attention RNN was able to outperform the Weka baseline (Mohammad and Bravo-Marquez, 2017a)", "startOffset": 104, "endOffset": 139}], "year": 2017, "abstractText": "In this paper we describe a deep learning system that has been designed and built for the WASSA 2017 Emotion Intensity Shared Task. We introduce a representation learning approach based on inner attention on top of an RNN. Results show that our model offers good capabilities and is able to successfully identify emotionbearing words to predict intensity without leveraging on lexicons, obtaining the 13th place among 22 shared task competitors.", "creator": "LaTeX with hyperref package"}}}