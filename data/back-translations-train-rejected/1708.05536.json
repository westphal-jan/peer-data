{"id": "1708.05536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution", "abstract": "Recent applications of neural language models have led to an increased interest in the automatic generation of natural language. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles. Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text. In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.", "histories": [["v1", "Fri, 18 Aug 2017 08:43:52 GMT  (192kb,D)", "http://arxiv.org/abs/1708.05536v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["e manjavacas", "j de gussem", "w daelemans", "m kestemont"], "accepted": false, "id": "1708.05536"}, "pdf": {"name": "1708.05536.pdf", "metadata": {"source": "CRF", "title": "Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution", "authors": ["Enrique Manjavacas", "Jeroen De Gussem", "Walter Daelemans", "Mike Kestemont"], "emails": ["firstname@uantwerpen.be", "lastname@uantwerpen.be", "jedgusse.degussem@ugent.be"], "sections": [{"heading": "1 Introduction", "text": "In his policy paper \"Computing Machinery and Intelligence,\" he quoted Jefferson's \"The Mind of Mechanical Man\" (1949): \"Only when a machine can write a sonnet or write a concert because of the thoughts and emotions felt, and not because of the accidental dropping of symbols, could we agree that the machine corresponds to the brain.\" What is striking is that these early pioneers of modern AI regard the conscious creation of literature as an important milestone on the long road to universal AI."}, {"heading": "2 Character-Level Text Generation", "text": "\"We are approaching the task of text generation using the character-level model (LM). In short, an LM is a probabilistic model of linguistic sequences that in each step in a sequence have generated a probability distribution beyond the vocabulary,\" continues the author, \"both the manner and the manner in which the individual sentences are defined in relation to the scope of the model.\" The length of the prefix sequence is taken into account to take into account the output distribution in each step. \"The extension, a generative model of sentences is defined by the following equation: P (w1, w2, wS) = n\" n. \""}, {"heading": "2.1 Ngram-based Language Model", "text": "In the current study, we compare two widely used LM architectures - an ngram-based LM (NGLM) and a recursive neural network-based LM (RNNLM). An NGLM is basically a conditional probability table for Equation 1, which is estimated on the basis of the counting data for Ngrams of a given length n. Typically, NGLMs suffer from a problem of data sparseness, since for a sufficiently large value of n many possible prefixes are not observed in the training data and the corresponding probability distribution is missing. To mitigate the problem of sparseness, two techniques - smoothing and backoff models - can be used that either reserve a certain probability mass and distribute it evenly over unnoticed Ngrams (smoothing) or use a subordinate model to allow approximation to the conditional distribution of an unobserved Ngram (off models)."}, {"heading": "2.2 RNN-based Language Model", "text": "The RNLM retrieve its associated embedding wt by wt = Wxt is where the reactionary neural networks (RNN) are the hidden information flows during sequencer processing (mikolov et al., 2010).As shown in (Bengio et al., 2003), in a particular step it is t, an RNLM (Elman, 1990) (i) first computes a hidden activation in the previous step ht \u2212 1, and (iii) it projects the hidden activation onto a space of dimensionality equal to the vocabulary size V, followed by a softmax function that turns the output vector into a valid probability distribution."}, {"heading": "3 Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Design", "text": "It's not the first time the actress has been linked to a number of high-profile men, having been linked to a string of high-profile romances in the past, including a number of high-profile romances, including with model Cara Delevingne, who she was spotted kissing on the lips at the MTV Europe Music Awards in Cannes last month."}, {"heading": "3.2 Language Model Architectures for Text Generation", "text": "For the current experiments, we generate 20 documents of 5000 words each, based on a value of 1 and a value estimated on each author's dataset. For the RNNLM, we reset the seed (parameter s) per 10 successfully generated sentences, while for the NGLM, we do it after each sentence. This asymmetry is motivated by the fact that the power distribution of an NGLM in each step is much more skewed and therefore sentences generated from the same seed tend to be much less variable. For the model fitting, we set the NGLM order to 6, which, on a subjective evaluation, represents a sufficiently large value for the comparatively small size of the datasets. For the RNLM models, the following parameter settings were selected."}, {"heading": "3.3 Attribution as Classification", "text": "For the AA classification, as described in the experimental setup of Section 3, we use a linear SVM classifier (Diederich, 2003) as a style marker to determine authorship. Note that the feature extraction of Ngrams in the order of 2 to 4 could have important effects, since NGLM training would fully focus on capturing this particular distribution, while the more expressive RNLM can model complete sequences. Furthermore, we do not use word-level characteristics such as word grams or POS tags, as this would introduce further asymmetry in the comparison, since the RNNLM can generate invisible words, whereas the NGLM cannot. Model accuracy of the SVM is refined by searching for different value ranges for the SVM parameters. The number of features is set to a range of 30,000 to 5,000, with the NLM each having more than 10,000 features in the order of SVM available."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Examples of Generated Language", "text": "What follows are two brief excerpts from the respective editions of a NGLM and RNNLM, which aim at Augustine (A.H.) (most prolific author of the dataset, see Table 1), which gives an anecdotal intuition about how the output of these language models differs. Ngram-based LM (\u03b1) (1) * Sed But uis you wish uenire: to come quod seit postridie, tomorrowascensiones ascensiones ora honored pastorem, the shepherd, nec and not sane completely reipublicos republican idem same testis testimony and also implement sit tamen nevertheless mentiendum to be deceived. They will be enriched.RNN-based LM (\u03b1) (2).Et And idam the same (?) precepti, commandment, siue be itad to the feeling noui: I do not know enough to do it."}, {"heading": "4.2 Attribution results", "text": "The results of the mapping experiments are presented in Table 1 in terms of retrieval, precision and F1 values, and the distributions are visualized in Fig. 3. We focus our discussion on the macro averaged F1 values, although it should not be forgotten that the values of individual authors vary considerably (see Fig. 3). In terms of authentic data, the classification of \u03b1 on the basis of \u03c9 is somewhat more difficult than the reverse direction, which seems to be a negligible directional artifact. Interestingly, if we use the generated data as training material to classify authentic material < \u03b1, \u03c9 >, we see that the F1 values for both LMs decrease significantly, although the NGLM appears more robust in this respect. Interestingly, the decrease is much less significant for the opposite situation in which we train on authentic material and classify the generated material < LT, as well as LT >. This indicates that enough of this text will be retained by the author to include generic information in the \u03b1."}, {"heading": "4.3 Discussion", "text": "To understand the difference in behavior between the two LMs, it is useful to examine Fig. 4. Here, we use a Principal Components Analysis (Binongo and Smith, 1999) to represent 2,500-word samples for three highly productive authors (Augustine of Hippo, Honorius of Autun, and Gregory the Great) using the 150 most common Ngrams. We include a mixture of authentic data and generated alpha data for each author by comparing the NGLM and the RNNLM. The graphs show that NGLM produces alpha samples that are very close to the authentic data, while the texts produced by the RNNLM follow a distinctly different distribution from the RNLM keywords. This difference is, for example, very exaggerated for Augustine. As can be expected from the observation in H.DAding, the NGLM 3.3 produces data that remains very close to the original input."}, {"heading": "5 Conclusion", "text": "Our preliminary results confirm that the data generated by a traditional NGLM are relatively \"boring\" and \"conservative,\" in the sense that they remain relatively close to the local distribution of the source data on which they were trained. Conceptually, the RNLM has a clear advantage in terms of expressiveness and capacity over the NGLM. In practice, insufficient RNLM results in more incomplete examples, which explains why the NGLM outperforms the RNLM when the classifier is limited to the data generated (< \u03b1, \u03c9 > and < \u03c9, \u03b1 >). At the same time, the training data augmentation setup (< \u03b1 + \u03b1) shows that the NGLM generated data contributes comparatively little to the data generated, but NGLM generated data does relatively little to the authentic data - the reproduction of a subset of the original characteristic distribution, as shown in Figure 5 - the NGLM contribution increases the result generated in terms of the NRM classification."}, {"heading": "A Author names with abbreviations", "text": "Abbrv Author H.S Hieronymus Stridonensis G.I Gregorius I A.H Augustinus Hipponensis A.M Ambrosius Mediolanensis B Beda H.C Hildebertus Cenomanensis H.d.S.V Hugo de S- Victore R.T Rupertus Tuitiensis W.S Walafridus Strabo T Tertullianus P.D Petrus Damianus H.A Honorius Augustodunensis H.R Hincmarus Rhemensis B.C Bernardus Claraevallensis A Alcuinus R.M Rabanus Maurus A.C Anselmus Cantuariensis R.S.V Richardus S- Victoris"}], "references": [{"title": "Pascal Vincent", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme"], "venue": "and Christian Janvin.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Wilfrid A", "author": ["M Jos\u00e9 Nilo G. Binongo"], "venue": "Smith.", "citeRegEx": "Binongo and Smith1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Do androids dream of cooking? https://gist.github.com/ nylki/1efbaa36635956d35bcc", "author": ["Tom Brewe"], "venue": null, "citeRegEx": "Brewe.,? \\Q2015\\E", "shortCiteRegEx": "Brewe.", "year": 2015}, {"title": "Authorship attribution with support vector machines", "author": ["Joachim Diederich"], "venue": "Applied Intelligence,", "citeRegEx": "Diederich.,? \\Q2003\\E", "shortCiteRegEx": "Diederich.", "year": 2003}, {"title": "Deeper delta across genres and languages: Do we really need the most frequent words", "author": ["Eder", "Rybicki2011] Maciej Eder", "Jan Rybicki"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Eder et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eder et al\\.", "year": 2011}, {"title": "Taking stylometry to the limits: Benchmark study on 5,281 texts from \"patrologia latina", "author": ["Maciej Eder"], "venue": "In Digital Humanities 2015: Conference Abstracts,", "citeRegEx": "Eder.,? \\Q2015\\E", "shortCiteRegEx": "Eder.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Mathew Johnson", "author": ["Liang Feynman", "Mark Gotham", "Marcin Tomczak"], "venue": "and Jaimie Shotton.", "citeRegEx": "Feynman et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems,", "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "The mind of mechanical man", "author": ["Geoffrey Jefferson"], "venue": "British Medical Journal,", "citeRegEx": "Jefferson.,? \\Q1949\\E", "shortCiteRegEx": "Jefferson.", "year": 1949}, {"title": "The unreasonable effectiveness of recurrent neural networks. http://karpathy.github.io/ 2015/05/21/rnn-effectiveness", "author": ["Andrej Karpathy"], "venue": null, "citeRegEx": "Karpathy.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy.", "year": 2015}, {"title": "Function words in authorship attribution. from black magic to theory", "author": ["Mike Kestemont"], "venue": "In Proceedings of the 3rd Workshop on Computational Linguistics for Literature,", "citeRegEx": "Kestemont.,? \\Q2014\\E", "shortCiteRegEx": "Kestemont.", "year": 2014}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Emmanuel Dupoux", "author": ["Tal Linzen"], "venue": "and Yoav Goldberg.", "citeRegEx": "Linzen et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Continuity through appropriation", "author": ["Yanick Maes"], "venue": "Latinitas Perennis. Volume II: Appropriation and Latin Literature,", "citeRegEx": "Maes.,? \\Q2009\\E", "shortCiteRegEx": "Maes.", "year": 2009}, {"title": "Tapani Raiko", "author": ["Eric Malmi", "Pyry Takala", "Hannu Toivonen"], "venue": "and Aristides Gionis.", "citeRegEx": "Malmi et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Co-training and self-training for word sense disambiguation", "author": ["Rada Mihalcea"], "venue": "In CoNLL,", "citeRegEx": "Mihalcea.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea.", "year": 2004}, {"title": "Jan Cernock\u00fd", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget"], "venue": "and Sanjeev Khudanpur.", "citeRegEx": "Mikolov et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Alexey Romanov", "author": ["Peter Potash"], "venue": "and Anna Rumshisky.", "citeRegEx": "Potash et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Manuel Montes", "author": ["Upendra Sapkota", "Steven Bethard"], "venue": "and Thamar Solorio.", "citeRegEx": "Sapkota et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of modern authorship attribution methods", "author": ["Efstathios Stamatatos"], "venue": "Journal of the American Society For Information Science and Technology,", "citeRegEx": "Stamatatos.,? \\Q2009\\E", "shortCiteRegEx": "Stamatatos.", "year": 2009}, {"title": "On the robustness of authorship attribution based on character n-gram features", "author": ["Efstathios Stamatatos"], "venue": "Journal of Law and Policy,", "citeRegEx": "Stamatatos.,? \\Q2013\\E", "shortCiteRegEx": "Stamatatos.", "year": 2013}, {"title": "Ming Zhang", "author": ["Jian Tang", "Yifan Yang", "Sam Carton"], "venue": "and Qiaozhu Mei.", "citeRegEx": "Tang et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ilya Sutskever", "author": ["Wojciech Zaremba"], "venue": "and Oriol Vinyals.", "citeRegEx": "Zaremba et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Recent applications of neural language models have led to an increased interest in the automatic generation of natural language. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles. Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text. In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.", "creator": "LaTeX with hyperref package"}}}