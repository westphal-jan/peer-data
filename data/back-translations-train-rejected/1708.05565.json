{"id": "1708.05565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online Auctions", "abstract": "We present LADDER, the first deep reinforcement learning agent that can successfully learn control policies for large-scale real-world problems directly from raw inputs composed of high-level semantic information. The agent is based on an asynchronous stochastic variant of DQN (Deep Q Network) named DASQN. The inputs of the agent are plain-text descriptions of states of a game of incomplete information, i.e. real-time large scale online auctions, and the rewards are auction profits of very large scale. We apply the agent to an essential portion of JD's online RTB (real-time bidding) advertising business and find that it easily beats the former state-of-the-art bidding policy that had been carefully engineered and calibrated by human experts: during JD.com's June 18th anniversary sale, the agent increased the company's ads revenue from the portion by more than 50%, while the advertisers' ROI (return on investment) also improved significantly.", "histories": [["v1", "Fri, 18 Aug 2017 11:25:30 GMT  (1073kb)", "http://arxiv.org/abs/1708.05565v1", "8 pages, 12 figures"], ["v2", "Fri, 1 Sep 2017 14:05:09 GMT  (1073kb)", "http://arxiv.org/abs/1708.05565v2", "8 pages, 12 figures"]], "COMMENTS": "8 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.GT", "authors": ["yu wang", "jiayi liu", "yuxiang liu", "jun hao", "yang he", "jinghe hu", "weipeng p yan", "mantian li"], "accepted": false, "id": "1708.05565"}, "pdf": {"name": "1708.05565.pdf", "metadata": {"source": "CRF", "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online Auctions", "authors": ["Yu Wang", "Jiayi Liu", "Yuxiang Liu", "Jun Hao", "Yang He", "Jinghe Hu", "Weipeng Yan", "Mantian Li"], "emails": ["limantian}@jd.com"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "Background", "text": "In this paper, we consider the auction environment as. After a while, the broker receives a real number of, \".\""}, {"heading": "Related Work", "text": "(Mnih et al. 2015) suggested DQN, which combined RL and CNN and learned directly from screen pixels and above average human experts in Atari 2600 games. (Gu et al. 2016) improved the method with a new network architecture Q = universal auction candidates from the 2016 Alphabet Auction) suggested Double DQN to address the overestimated problem in DQN.POMDPs were well studied in (Jaakkola, Singh, and Jordan 1995), (Kaelbling, Littman, and Cassandra 1998), and (Monahan 1982). (Hausknecht and Stone 2015) Atari modelled 2600 games as POMDPs by replacing a complete connection in DQopps. All these algorithms are sequential in that they can only act once after each step of SGD, which is not acceptable in our application scenario. (Mnih et al. 2016) presents A3C and other synchronized RQ algorithms to perform step-Q algorithms."}, {"heading": "Modeling", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "Deep Asynchronous Stochastic Q-learning", "text": "RL algorithms are inherently sequential, although A3C and other algorithms in (Mnih et al. 2016) made it possible to trade an entire episode between each training step, but they are sequential because the two processes of acting and training in these algorithms are still serially exe-cuted, which is unacceptable for an online DSP service that has to respond to each of the huge auction steps in several milliseconds. From this perspective, training during serving is absolutely impracticable, needless to say that it requires a hundred times more servers, which is uneconomical.It is distinguishable that we solve this problem by introducing a fully decoupled parallel mechanism that leads to a completely asynchronous RL algorithm in which all three processes (learning from the environment, acting in the environment, and observing the environment) run simultaneously without waiting for each other."}, {"heading": "Data Augmentation and the Loss", "text": "Suppose we have a stochastic transition (,,) as discussed above, given the ownership of GSP auctions and the definition of and, we have a deduction that any bid over the auction would win and any bid under would lose the auction. Given the deduction, for all \""}, {"heading": "Experimental Results", "text": "LADDER's experiments have been conducted at four major publishers, which account for a significant portion of JD.com's DSP business revenue. We are trying to improve both publishers \"revenue and profits in the experiments."}, {"heading": "Experiment Setup", "text": "We train the algorithm 1 on 4 Tesla K80 GPUs and operate it on 24 PC servers (Fig. 3) with a powerful C + + sparse volumes. We use RMSProp to optimize the loss in the formula (4) with a learning rate of 5. The use of greedy people was limited to an \u03b5 of 1 to minimize the negative impact on the business. The training was split into 2 phases: imitation. We filled the experience memory with data generated by the ECPM policy of the hybrid system. At this point, before enough self-generated data enters the memory, LADDER is learning the ECPM policy. In this cold start phase, LADDER interacts little with the environment and thus ensures that losses are under control.Introspection. After several hours of imitation (the actual time required depends on the procedure chosen), LADDER begins to learn from data generated by its own policies."}, {"heading": "Evaluation", "text": "In May 2017, we evaluated LADDER using the online A / B test in an overlapping experimental system similar to (Tang et al. 2010) and considered the ECPM policy as a baseline. Initially, LADDER offered 10% of the auctions per day and the remaining 90% ran the baseline. We started LADDER at 90% of the auctions on day 8 and hold the remaining 10% as a holdback, which executed the baseline policy for months for reasons of scientific rigor. The experimental system resulted in a proportional normalization of all experiments to facilitate comparison. Figure 4 shows the performance comparisons between the loader and baseline. We normalized all data in the range [0.1] for privacy. Figure 4 (a) shows the reward comparison (gains), where we can see that LADDER suffered huge losses on the first day in the imitation phase because it tended to offer all requests for exploration."}, {"heading": "Exploration and Exploitation", "text": "To maximize revenue, we set the bet value to 0.6. As Figure 6 shows, revenue decreases while rewards and CTR increase, meaning that the agent tends to be less aggressive when we lower it from 0.6 to 0.55."}, {"heading": "Visualization", "text": "To find out if LADDER is able to understand highly semantic information embedded in the plaintext input, we use t-SNE to visualize the results of the hidden layer."}, {"heading": "Multiple Games in a Single Model", "text": "As already mentioned, LADDER serves very different publishers at the same time (also known as different auction games). Although it is a challenge, Figure 5 shows that LADDER has successfully learned the differences between publishers from the plaintext entries, exceeding the baseline in revenue and CTR for each rated publisher. To technically verify how well LADDER can differentiate between different publishers, we use publisher types as labels and visualize 1000,000 random samples in Figure 7 (a). As expected, all 4 publishers are perfectly divided into separate clusters."}, {"heading": "Complex Semantic", "text": "In Figure 7 (a), the samples from the same publisher are dispersed into several clusters. In fact, LADDER's semantics are much more complex than those of the publisher IDs. For further analysis, we only visualize the data from Publisher 1 from the same 1000,000 samples. As Figure 7 (b) shows, LADDER has learned fairly complex conditions from the plaintext inputs that are indispensable for an auction. For example, SKUs supplied by JD Logistic Network (JDLN) may be more attractive to a cold start user, as JDLN is known to have superior user experience. As shown in the left part of Figure 7 (b), LADDER recognizes these situations."}, {"heading": "Future Work", "text": "Given that ADXs mimic exchanges, the use of LADDER in quantitative trading is also of great interest and challenge. The referral system is a domain that is similar to online advertising, so our approach in this area should work with a domain-specific loss function. What we are working on is the use of LADDER not only for bidding, but also in the ranking phase of online advertising, which can also bring significant business benefits."}, {"heading": "Conclusions", "text": "Our goal is to develop a human-level agent capable of not only saving manpower while performing as well or even better than humans, but also understanding an auction situation directly from a simple text description, as human experts do. As a result, LADDER achieves the goal by easily outperforming the existing industrial state-of-the-art solution in A / B tests, meaning that it has fully utilized the high-level semantic information in the auction game without sophisticated feature engineering and responds immediately to the changing auction environment. We also introduce DASQN, an asynchronous stochastic Q network that fully decouples the learning, observation, and action sequences in Q-learning, greatly improving its runtime performance and enabling the algorithm to solve large-scale real-world problems."}], "references": [{"title": "Searching for solutions in games and artificial intelligence", "author": ["L.V. Allis"], "venue": null, "citeRegEx": "Allis,? \\Q1994\\E", "shortCiteRegEx": "Allis", "year": 1994}, {"title": "Realtime bidding algorithms for performance-based display ad allocation", "author": ["Y. Chen", "P. Berkhin", "B. Anderson", "N.R. Devanur"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Wide & deep learning for recommender systems", "author": ["H.T. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir"], "venue": "Proceedings of the 1st Workshop on Deep Learning for Recommender Systems", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "A primer in game theory. Harvester Wheatsheaf", "author": ["R. Gibbons"], "venue": null, "citeRegEx": "Gibbons,? \\Q1992\\E", "shortCiteRegEx": "Gibbons", "year": 1992}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "International Conference on Machine Learning.,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Deep recurrent q-learning for partially observable mdps. CoRR, abs/1507.06527", "author": ["M. Hausknecht", "P. Stone"], "venue": null, "citeRegEx": "Hausknecht and Stone,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone", "year": 2015}, {"title": "Reinforcement learning algorithm for partially observable Markov decision problems", "author": ["T. Jaakkola", "S.P. Singh", "M.I. Jordan"], "venue": "Advances in neural information processing systems.,", "citeRegEx": "Jaakkola et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1995}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence.,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Ad click prediction: a view from the trenches", "author": ["H.B. McMahan", "G. Holt", "D. Sculley", "M. Young", "D. Ebner", "J. Grady", "L. Nie", "T. Phillips", "E. Davydov", "D. Golovin"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "McMahan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning.,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "State of the art\u2014a survey of partially observable Markov decision processes: theory, models, and algorithms", "author": ["G.E. Monahan"], "venue": "Management Science.,", "citeRegEx": "Monahan,? \\Q1982\\E", "shortCiteRegEx": "Monahan", "year": 1982}, {"title": "Optimal auction design", "author": ["R.B. Myerson"], "venue": "Mathematics of operations research.,", "citeRegEx": "Myerson,? \\Q1981\\E", "shortCiteRegEx": "Myerson", "year": 1981}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "arXiv preprint arXiv:1506.08941", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST).,", "citeRegEx": "Rendle,? \\Q2012\\E", "shortCiteRegEx": "Rendle", "year": 2012}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke", "A.A. Alemi"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2017}, {"title": "Overlapping experiment infrastructure: More, better, faster experimentation", "author": ["D. Tang", "A. Agarwal", "D. O\u2019Brien", "M. Meyer"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "Tang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2010}, {"title": "Deep Reinforcement Learning with Double Q-Learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "T. Schaul", "M. Hessel", "H. Van Hasselt", "M. Lanctot", "N. De Freitas"], "venue": "arXiv preprint arXiv:1511.06581", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Real-time bidding for online advertising: measurement and analysis", "author": ["S. Yuan", "J. Wang", "X. Zhao"], "venue": "Proceedings of the Seventh International Workshop on Data Mining for Online Advertising. ACM,", "citeRegEx": "Yuan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2013}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in neural information processing systems.,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Researchers have made great progress recently in learning to control agents directly from raw high-dimensional sensory inputs like vision in domains such as Atari 2600 games (Mnih et al. 2015), where reinforcement learning (RL) agents have human-level performance.", "startOffset": 174, "endOffset": 192}, {"referenceID": 12, "context": "Obviously, the process of many DSPs/ADXs bidding for an ad offer is an auction game (Myerson 1981) of incomplete information.", "startOffset": 84, "endOffset": 98}, {"referenceID": 8, "context": "However, the online ads industry just ignores this fact and considers RTB a solved problem: all existing DSPs model auction games as supervised learning (SL) problems by predicting the click through rate (CTR) (McMahan et al. 2013) or conversion rate (CVR) (Yuan, Wang, and Zhao 2013) of ads and using effective cost per mille (ECPM) as bids (Chen et al.", "startOffset": 210, "endOffset": 231}, {"referenceID": 1, "context": "2013) or conversion rate (CVR) (Yuan, Wang, and Zhao 2013) of ads and using effective cost per mille (ECPM) as bids (Chen et al. 2011).", "startOffset": 116, "endOffset": 134}, {"referenceID": 8, "context": "com started its DSP business in 2014, at first we employed the industry state-of-the-art approach of ECPM bidding with a calibrated CTR model (McMahan et al. 2013) as depicted in Figure 2.", "startOffset": 142, "endOffset": 163}, {"referenceID": 15, "context": "For comparison, the solution space of the game of Go is about 10 (Allis and others 1994; Silver et al. 2016).", "startOffset": 65, "endOffset": 108}, {"referenceID": 15, "context": "That\u2019s very different from Atari games, text-based games (Narasimhan, Kulkarni, and Barzilay 2015) and the game of Go (Silver et al. 2016).", "startOffset": 118, "endOffset": 138}, {"referenceID": 2, "context": "Although sophisticated feature engineering can utilize these information in a model like wide and deep models (Cheng et al. 2016) or factorization machines (Rendle 2012) as is already in place in the hybrid system, taking into account JD\u2019s scale, such models will be of billions of features and therefore too heavy to react instantly to the rapidly varying auction environment, leading to poor performance.", "startOffset": 110, "endOffset": 129}, {"referenceID": 14, "context": "2016) or factorization machines (Rendle 2012) as is already in place in the hybrid system, taking into account JD\u2019s scale, such models will be of billions of features and therefore too heavy to react instantly to the rapidly varying auction environment, leading to poor performance.", "startOffset": 32, "endOffset": 45}, {"referenceID": 9, "context": "Q-learning and its variants especially DQN (Mnih et al. 2015) learns a value function Q(s, a; \u03b8) which indicates the future rewards since current state and derives a policy \u03c0(s, a) = argmax Q(s, a; \u03b8), a \u2208 {1,2, .", "startOffset": 43, "endOffset": 61}, {"referenceID": 9, "context": "(Mnih et al. 2015) proposed DQN which combined RL and CNN and learned directly from screen pixels and outperformed human experts in Atari 2600 games.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "(Gu et al. 2016) improved the method with a new network architec-", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "POMDPs were well studied in (Jaakkola, Singh, and Jordan 1995), (Kaelbling, Littman, and Cassandra 1998), and (Monahan 1982).", "startOffset": 110, "endOffset": 124}, {"referenceID": 5, "context": "(Hausknecht and Stone 2015) modeled Atari 2600 games as POMDPs by replacing a full connection lay in DQN by an LSTM.", "startOffset": 0, "endOffset": 27}, {"referenceID": 10, "context": "(Mnih et al. 2016) presented A3C and n-step Q-learning among other asynchronous algorithms which decoupled RL algorithms to some extent in that agents could act n steps between 2 training steps, as well as could learn from several copies of the same game at the same time.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "(Silver et al. 2016) applied RL, CNNs and Monte Carlo Tree Search to the game of Go and their agent namely AlphaGo beat the top human experts in an open competition.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "The auction game is a typical game of incomplete information where each DSP is a player (Gibbons 1992).", "startOffset": 88, "endOffset": 102}, {"referenceID": 8, "context": "We tried to calibrate Q to a click through rate (CTR ) as depicted in (McMahan et al. 2013), except that we used a factorization machine (Rendle 2012) instead of Poisson regression for calibration.", "startOffset": 70, "endOffset": 91}, {"referenceID": 14, "context": "2013), except that we used a factorization machine (Rendle 2012) instead of Poisson regression for calibration.", "startOffset": 51, "endOffset": 64}, {"referenceID": 2, "context": "Our ranking model has a structure similar to wide and deep models (Cheng et al. 2016) with billions of weights and tens of gigabyte of memory and disk space requirements, meaning Q can hardly react to the rapidly changing auction environment without delay because the model is too huge to update in time, leading us to design a real-time impression-click data stream for online learning of the calibration CTR model.", "startOffset": 66, "endOffset": 85}, {"referenceID": 16, "context": "Also, we use a traditional architecture rather than the state-of-the-art Inception networks or ResNets (Szegedy et al. 2017) for the same reason.", "startOffset": 103, "endOffset": 124}, {"referenceID": 10, "context": "RL algorithms are inherently sequential, though A3C and other algorithms in (Mnih et al. 2016) made it possible to act an entire episode between each training step, they are still sequential in nature because the two processes of acting and training in those algorithms are still serially exe-", "startOffset": 76, "endOffset": 94}, {"referenceID": 9, "context": "However, training independent agents for different games as (Mnih et al. 2015) will make more states unobservable.", "startOffset": 60, "endOffset": 78}, {"referenceID": 19, "context": "Although we use DQN in this paper, Double DQN and Dueling Double DQN (Wang et al. 2015) can be naturally incorporated in out algorithm.", "startOffset": 69, "endOffset": 87}, {"referenceID": 17, "context": "In May 2017, we evaluated LADDER with online A/B test in an overlapping experiment system similar to (Tang et al. 2010) and regarded the ECPM policy as baseline.", "startOffset": 101, "endOffset": 119}], "year": 2017, "abstractText": "We present LADDER, the first deep reinforcement learning agent that can successfully learn control policies for largescale real-world problems directly from raw inputs composed of high-level semantic information. The agent is based on an asynchronous stochastic variant of DQN (Deep Q Network) named DASQN. The inputs of the agent are plain-text descriptions of states of a game of incomplete information, i.e. real-time large scale online auctions, and the rewards are auction profits of very large scale. We apply the agent to an essential portion of JD\u2019s online RTB (real-time bidding) advertising business and find that it easily beats the former state-of-the-art bidding policy that had been carefully engineered and calibrated by human experts: during JD.com\u2019s June 18th anniversary sale, the agent increased the company\u2019s ads revenue from the portion by more than 50%, while the advertisers\u2019 ROI (return on investment) also improved significantly.", "creator": null}}}