{"id": "1708.05592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Future Word Contexts in Neural Network Language Models", "abstract": "Recently, bidirectional recurrent network language models (bi-RNNLMs) have been shown to outperform standard, unidirectional, recurrent neural network language models (uni-RNNLMs) on a range of speech recognition tasks. This indicates that future word context information beyond the word history can be useful. However, bi-RNNLMs pose a number of challenges as they make use of the complete previous and future word context information. This impacts both training efficiency and their use within a lattice rescoring framework. In this paper these issues are addressed by proposing a novel neural network structure, succeeding word RNNLMs (su-RNNLMs). Instead of using a recurrent unit to capture the complete future word contexts, a feedforward unit is used to model a finite number of succeeding, future, words. This model can be trained much more efficiently than bi-RNNLMs and can also be used for lattice rescoring. Experimental results on a meeting transcription task (AMI) show the proposed model consistently outperformed uni-RNNLMs and yield only a slight degradation compared to bi-RNNLMs in N-best rescoring. Additionally, performance improvements can be obtained using lattice rescoring and subsequent confusion network decoding.", "histories": [["v1", "Fri, 18 Aug 2017 13:11:22 GMT  (617kb,D)", "http://arxiv.org/abs/1708.05592v1", "Submitted to ASRU2017"]], "COMMENTS": "Submitted to ASRU2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xie chen", "xunying liu", "anton ragni", "yu wang", "mark gales"], "accepted": false, "id": "1708.05592"}, "pdf": {"name": "1708.05592.pdf", "metadata": {"source": "CRF", "title": "FUTURE WORD CONTEXTS IN NEURAL NETWORK LANGUAGE MODELS", "authors": ["X. Chen", "X. Liu", "A. Ragni", "Y. Wang", "M.J.F. Gales"], "emails": ["xc257@eng.cam.ac.uk,", "ar527@eng.cam.ac.uk,", "yw396@eng.cam.ac.uk,", "mjfg@eng.cam.ac.uk,", "xyliu@se.cuhk.edu.hk"], "sections": [{"heading": null, "text": "Index Terms - Bidirectional Recurring Neural Network, Speech Model, Consecutive Words, Speech Recognition"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of us are able to move to another world where we are able to integrate, \"he said in an interview with the New York Times."}, {"heading": "2. UNI- AND BI-DIRECTIONAL RNNLMS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Unidirectional RNNLMs", "text": "Unlike previous NNLMs, where only the modelling of the previous n \u2212 1 words = more complex units [1] can be used, the recursive NNLM [13] is the complete non-ar Xiv: 170 8.05 592v 1 [cs.C L] 18 Aug 201 7 abbreviated history wt \u2212 11 = w1, w2,..., wt \u2212 1 for word wt-K Encryption of the previous word wt \u2212 1 and a continuous vector ht \u2212 2 as a compact representation of the remaining context wt \u2212 21. Figure 1 shows an example of this unidirectional RNLM (unidirectional RNLM) encryption. The newest word wt \u2212 1 is used as input and projection into a low-dimensional, continuous, space via a linear projection layer. A recursive hidden layer is used after this projection layer."}, {"heading": "2.2. Bidirectional RNNLMs", "text": "\"Unlike uni RNLMs,\" he says, \"it is not possible to estimate the probability of a current word P (wt \u2212 11, wLt + 1). (wLt \u2212 11, wLt + 1) While h + 1 is another vector to encode the future information, wLt + 1 is the future context vector. This future context vector is compressed from the next word wt \u2212 1 and the previous future context vector h + 2, which contains information about wLt + 2. The constellation of wLt \u2212 1 and h \u2212 1 is then fed into the output level to calculate the output reliability. (wLt + 2) The constellation of wLt \u2212 1 and h \u2212 1 is then fed into the output reliability."}, {"heading": "3. RNNLMS WITH SUCCEEDING WORDS", "text": "As discussed above, bi-RNNLMs are slow to train and difficult to use in lattice rescoring. To address these problems, this paper proposes a novel structure, su-RNNLM, to integrate future context information. the model structure is illustrated in Figure 3. In the same way as bi-RNNLMs, the prehistory wt \u2212 11 is modeled with recurring units (e.g. LSTM, GRU). However, instead of modeling the complete future context information, wLt + 1, using recursive units, feedforward units are used to capture an infinite number of successful words, wt + kt + 1. The softmax function is re-applied to the output layer to obtain the probability of the current word imperimperimperation (wt | wt \u2212 11, w + k \u2212 Nk \u2212 1) imperperimperation \u2212 imperimperimperation \u2212 imperimperimperimation \u2212 imperimperimperimation \u2212 imperimperimperimperimation \u2212 imperimperimperimperimation \u2212 imperimperimperimperimperimation \u2212 imperimperimperimp \u2212 imperimperimp \u2212 imperimperimperimperimperimperimperimp \u2212 imperimperimperimperimperimperimperimperimation \u2212 imperimperimperimperimation \u2212 imperimperimperimperimperimperimperimperimation \u2212 imperimperimperimperimation \u2212 imperimperimperimperimperimperimperimperimperimation \u2212 imperimperimperimperimperimation \u2212 imperimperimperimperimperimperimation \u2212 imperimperimperimperimperimperimation \u2212 imperimperimation \u2212 imperimperimperimperimperimperimperimperimation \u2212 imperimperimp \u2212 imperimperimperimperimperimperimperimperimation \u2212 imperimperimperimperimperimation \u2212 imperimperimperimperimperimperimperimperimperim"}, {"heading": "4. LATTICE RESCORING", "text": "As mentioned in Section 2.2, N-best rescoring has previously been used for bi-RNNLMs. However, it is not practical to use N-best rescoring for rescoring and generating grids, as both the complete previous and future context information is required. However, grids are very useful in many applications, such as estimating the confidence value [9], keyword search [10], and decoding confusion networks [11]. In contrast, su-RNNLMs require a fixed number of consecutive words instead of the complete future context information. Figure 3 shows su-RNNLMs as a combination of uni-RNNLMs for history information and feedforward NNLMs for future context information."}, {"heading": "4.1. Lattice rescoring of uni-RNNLMs", "text": "In this paper, the n-gram approximation [8] is used on the basis of the uni-RNLM grid rescoring approach. If one considers the merging of two paths, the two paths are considered \"equivalent\" and can be merged, as illustrated in Figure 5 for the start node of wordw4. History information from the best path is retained for the following RNLM probability calculation, and the history of all other paths are discarded, for example, the path (w0, w2, w3) is retained and the other path (w1, w2, w3) is discarded in the face of the arc w4. There are two types of approximation involved for uni-RNLM grid rescoring, which are the merge and cache approximation. Merge approximation controls the merge of two paths merging \u2212 all paths \u2212 the same path, [8] the first one of which can be achieved in the same path and the other."}, {"heading": "4.2. Lattice rescoring of su-RNNLMs", "text": "In order to handle the following words correctly, paths are merged only if the following words are identical. In this way, the path extension is carried out in both directions. Two arbitrary paths with the same consecutive words and n \u2212 1 previous words are merged. Figure 4 shows a part of an example grid created by a 2 gram LM. To convert the uni RNLM grid with a 3 gram approximation, the grey shaded node in Figure 4 must be duplicated, because the word w3 has two different 3 gram histories, which are (w0, w2) and (w1, w2) respectively. Figure 5 shows the grid after converting a uni gram LNLM with a 3 gram approximation. To use su RNLMs for a distinguishable word, a larger NLat can be taken into account with a subsequent Ngram node."}, {"heading": "5. LANGUAGE MODEL INTERPOLATION", "text": "For unidirectional speech models such as the n-gram model and uniRNNLMs, the word probabilities are normally combined with linear interpolation, where Pu (wt | wt \u2212 11) = (11) (1 \u2212 \u03bb1) Pn (wt | wt \u2212 11) + \u03bb1Pr (wt | w t \u2212 1), where Pn and Pr are the probabilities of n-gram and uni-RNN LMs, respectively, \u03bb1 being the interpolation weight of uni RNLMs. However, it is not valid to combine uni LMs (e.g. unidirectional n-gram LMs or RNLMs) and bi-LMs (or su-LMs) with linear interpolation, which is normal (or su-LMs) in equation 5 due to the normalization term required for bi-LMs (or su-LMs)."}, {"heading": "6. EXPERIMENTS", "text": "In fact, it is a way in which people move in the most diverse areas of life in which they move, and in which people find themselves in the most diverse life and work environments in the world in which they live. \"nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; hics,\" he says, \"is\" as he says, \"as he says\" bsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp nbsp; nbsp; nbsp; nbsp nbsp; nbsp; nbsp; nbsp nbsp n"}, {"heading": "7. CONCLUSIONS", "text": "This paper examines the use of future context information on neural network speech models. Instead of using a recursive unit to capture all future information, a feedback unit was used to model a limited number of consecutive words. Existing training and rescoring algorithms for uni RNNLMs are extended to the proposed su RNLMs. Experimental results show that su RNNLMs performed slightly worse than bi-RNNLMs, but at a much faster training speed. In addition, additional performance improvements can be achieved through the rescoring of grids and subsequent decoding of confusion networks."}, {"heading": "8. REFERENCES", "text": "[1] Stanley Chen and Joshua Goodman, \"An empirical study of smoothing networks for language modeling,\" Computer Speech & Language, vol. 13, no. 4, pp. 359-393, 1999. [2] Yoshua Bengio, Re-jean Ducharme, Pascal Vincent, and Christian Jauvin, \"A neural probabilistic language model,\" Journal of Machine Learning Research, vol. 3, pp. 1137-1155, 2003. [3] Tomas Mikolov, Martin Karafia \"t, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur,\" Recurrent neural network based language model, \"in Proc. ISCA INTERSPEECH, 2010. [4] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and offrey Zweig.\""}], "references": [{"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley Chen", "Joshua Goodman"], "venue": "Computer Speech & Language, vol. 13, no. 4, pp. 359\u2013393, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Proc. ISCA INTERSPEECH, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Achieving human parity in conversational speech recognition", "author": ["Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1610.05256, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting the succeeding words in recurrent neural network language models", "author": ["Yangyang Shi", "Martha Larson", "Pascal Wiggers", "Catholijn Jonker"], "venue": "Proc. ICSA INTERSPEECH, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Bidirectional recurrent neural network language models for automatic speech recognition", "author": ["Ebru Arisoy", "Abhinav Sethy", "Bhuvana Ramabhadran", "Stanley Chen"], "venue": "Proc. ICASSP. IEEE, 2015, pp. 5421\u20135425.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating bidirectional recurrent neural network language models for speech recognition", "author": ["Xie Chen", "Anton Ragni", "Xunying Liu", "Mark Gales"], "venue": "Proc. ICSA INTERSPEECH, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Two efficient lattice rescoring methods using recurrent neural network language models", "author": ["Xunying Liu", "Xie Chen", "Yongqiang Wang", "Mark Gales", "Phil Woodland"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1438\u20131449, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Confidence measures for large vocabulary continuous speech recognition", "author": ["Frank Wessel", "Ralf Schluter", "Klaus Macherey", "Hermann Ney"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 3, pp. 288\u2013298, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Recurrent neural network language models for keyword search", "author": ["Xie Chen", "Anton Ragni", "Jake Vasilakes", "Xunying Liu", "Kate Knill", "Mark Gales"], "venue": "Proc. ICASSP. IEEE, 2017, pp. 5775\u20135779.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Finding consensus in speech recognition: word error minimization and other applications of confusion networks", "author": ["Lidia Mangu", "Eric Brill", "Andreas Stolcke"], "venue": "Computer Speech & Language, vol. 14, no. 4, pp. 373\u2013400, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient training and evaluation of recurrent neural network language models for automatic speech recognition", "author": ["Xie Chen", "Xunying Liu", "Yongqiang Wang", "Mark Gales", "Phil Woodland"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech & Language, vol. 21, no. 3, pp. 492\u2013518, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Improved neural network based language modelling and adaptation", "author": ["Junho Park", "Xunying Liu", "Mark Gales", "Phil Woodland"], "venue": "Proc. ISCA INTERSPEECH, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "The dawn of statistical asr and mt", "author": ["Frederick Jelinek"], "venue": "Computational Linguistics, vol. 35, no. 4, pp. 483\u2013494, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schluter"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 3, pp. 517\u2013529, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The AMI meeting corpus: A preannouncement", "author": ["Jean Carletta"], "venue": "Machine learning for multimodal interaction, pp. 28\u201339. Springer, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "The Kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz"], "venue": "ASRU, IEEE Workshop on, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["Karel Vesel\u1ef3", "Arnab Ghoshal", "Luk\u00e1s Burget", "Daniel Povey"], "venue": "Proc. ICSA INTERSPEECH, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["Mark Gales"], "venue": "Computer Speech & Language, vol. 12, no. 2, pp. 75\u201398, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "CUED-RNNLM an open-source toolkit for efficient training and evaluation of recurrent neural network language models", "author": ["Xie Chen", "Xunying Liu", "Mark Gales", "Phil Woodland"], "venue": "Proc. ICASSP. IEEE, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "n-gram LMs [1] and neural network based language mdoels (NNLMs) [2, 3] are two widely used language models.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "n-gram LMs [1] and neural network based language mdoels (NNLMs) [2, 3] are two widely used language models.", "startOffset": 64, "endOffset": 70}, {"referenceID": 2, "context": "n-gram LMs [1] and neural network based language mdoels (NNLMs) [2, 3] are two widely used language models.", "startOffset": 64, "endOffset": 70}, {"referenceID": 1, "context": "This n-gram assumption can also be used to construct a n-gram feedforward NNLMs [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "Individual forward and backward RNNLMs can be built, and these two LMs combined with a log-linear interpolation [4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "In [5], succeeding words were incorporated into RNNLM within a Maximum Entropy framework.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "[6] investigated the use of bidirectional RNNLMs (bi-RNNLMs) for speech recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "More recently, bi-RNNLMs can produce consistent, and significant, performance improvements over unidirectional RNNLMs (uni-RNNLMs) on a range of speech recognition tasks [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "This means that the form of approximation used for uni-RNNLMs [8] is not suitable to apply.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Hence, N-best rescoring is normally used [5, 6, 7].", "startOffset": 41, "endOffset": 50}, {"referenceID": 5, "context": "Hence, N-best rescoring is normally used [5, 6, 7].", "startOffset": 41, "endOffset": 50}, {"referenceID": 6, "context": "Hence, N-best rescoring is normally used [5, 6, 7].", "startOffset": 41, "endOffset": 50}, {"referenceID": 8, "context": "Lattices can be used for a wide range of downstream applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "Lattices can be used for a wide range of downstream applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "Lattices can be used for a wide range of downstream applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 11, "context": "This allows existing efficient training [12] and lattice rescoring [8] algorithms developed for uni-RNNLMs to be extended to the proposed su-RNNLMs.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "This allows existing efficient training [12] and lattice rescoring [8] algorithms developed for uni-RNNLMs to be extended to the proposed su-RNNLMs.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "In contrast to feedforward NNLMs, where only modeling the previous n \u2212 1 words, recurrent NNLMs [13] represent the full nonar X iv :1 70 8.", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "The form of the recurrent layer can be based on a standard sigmoid based recurrent unit, with sigmoid activations [3], or more complicated forms such as gated recurrent unit (GRU) [14] and long short-term memory (LSTM) units [15].", "startOffset": 114, "endOffset": 117}, {"referenceID": 13, "context": "The form of the recurrent layer can be based on a standard sigmoid based recurrent unit, with sigmoid activations [3], or more complicated forms such as gated recurrent unit (GRU) [14] and long short-term memory (LSTM) units [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "The form of the recurrent layer can be based on a standard sigmoid based recurrent unit, with sigmoid activations [3], or more complicated forms such as gated recurrent unit (GRU) [14] and long short-term memory (LSTM) units [15].", "startOffset": 225, "endOffset": 229}, {"referenceID": 15, "context": "An additional node is often added at the output layer to model the probability mass of out-of-shortlist (OOS) words to speed up softmax computation by limiting vocabulary size [16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "According to the definition in [17], the perplexity can be computed based on sentence probability with,", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "minibatch) mode [12].", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "length [12].", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Lattice rescoring is also possible by introducing approximations [8] to control merging and expansion of different paths in lattice.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "In order to achieve good performance for speech recognition, [7] proposed an additional smoothing of the bi-RNNLM probability at test time.", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "First, N-best rescoring is normally used for speech recognition [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "In [6], all sentences in the training corpus were concatenated together to form a single sequence to facilitate minibatch based training.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], the bi-RNNLMs were trained in a more consistent fashion.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "This is similar to the zero padding of the feedforward forward NNLMs at the beginning of each sentence [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "As the number of succeeding words is finite and fixed for each word, its succeeding words can be organized as a n-gram future context and used for minibatch mode training as in feedforward NNLMs [13].", "startOffset": 195, "endOffset": 199}, {"referenceID": 11, "context": "Su-RNNLMs can then be trained efficiently in a similar fashion to uni-RNNLMs in a spliced sentence bunch mode [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "Lattice rescoring with feedforward NNLMs is straightforward [13] whereas approximations are required for uni-RNNLMs lattice rescoring [8, 18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "Lattice rescoring with feedforward NNLMs is straightforward [13] whereas approximations are required for uni-RNNLMs lattice rescoring [8, 18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 17, "context": "Lattice rescoring with feedforward NNLMs is straightforward [13] whereas approximations are required for uni-RNNLMs lattice rescoring [8, 18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 8, "context": "However, lattices are very useful in many applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "However, lattices are very useful in many applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "However, lattices are very useful in many applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 7, "context": "In this paper, the n-gram approximation [8] based approach is used for uni-RNNLMs lattice rescoring.", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "In [8], the first path reaching the node was kept and all other paths with the same n-gram history were discarded irrespective of the associated scores.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [8], RNNLM probabilities were cached based on the previous n\u2212 1 words, which is referred as cache approximation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In order to avoid this inaccuracy yet maintain the efficiency, the cache approximation used in [8] is improved by adopting the complete history as key for caching RNNLM probabilities.", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "Both modifications yielt small but consistent improvements over [8] on a range of tasks.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "As described in [7], uni-LMs can be log-linearly interpolated with bi-LMs for speech recognition using,", "startOffset": 16, "endOffset": 19}, {"referenceID": 18, "context": "Experiments were conducted using the AMI IHM meeting corpus [19] to evaluated the speech recognition performance of various language models.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "The Kaldi acoustic model training recipe [20] featuring sequence training [21] was applied for deep neural network (DNN) training.", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "The Kaldi acoustic model training recipe [20] featuring sequence training [21] was applied for deep neural network (DNN) training.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "CMLLR transformed MFCC features [22] were used as the input and 4000 clustered context dependent states were used as targets.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "An extended version of CUED-RNNLM [23] was developed for the training of uni-RNNLMs, bi-RNNLMs and su-RNNLMs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "7 as suggested in [7].", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "The 3-gram approximation was applied for the history merging of uni-RNNLMs and su-RNNLMs during lattice rescoring and generation [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "However, the training of su-RNNLMs is much faster than bi-RNNLMs as it is difficult to parallelise the training of bi-RNNLMs efficiently [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "uk/projects/cued-rnnlm/ 3N-best list can be converted to lattice and CN decoding then can be applied, but it requires a much larger N-best list, such as 10K used in [8].", "startOffset": 165, "endOffset": 168}], "year": 2017, "abstractText": "Recently, bidirectional recurrent network language models (biRNNLMs) have been shown to outperform standard, unidirectional, recurrent neural network language models (uni-RNNLMs) on a range of speech recognition tasks. This indicates that future word context information beyond the word history can be useful. However, bi-RNNLMs pose a number of challenges as they make use of the complete previous and future word context information. This impacts both training efficiency and their use within a lattice rescoring framework. In this paper these issues are addressed by proposing a novel neural network structure, succeeding word RNNLMs (suRNNLMs). Instead of using a recurrent unit to capture the complete future word contexts, a feedforward unit is used to model a finite number of succeeding, future, words. This model can be trained much more efficiently than bi-RNNLMs and can also be used for lattice rescoring. Experimental results on a meeting transcription task (AMI) show the proposed model consistently outperformed uni-RNNLMs and yield only a slight degradation compared to bi-RNNLMs in N-best rescoring. Additionally, performance improvements can be obtained using lattice rescoring and subsequent confusion network decoding.", "creator": "LaTeX with hyperref package"}}}