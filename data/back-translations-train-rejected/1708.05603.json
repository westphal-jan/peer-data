{"id": "1708.05603", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Nonnegative Restricted Boltzmann Machines for Parts-based Representations Discovery and Predictive Model Stabilization", "abstract": "The success of any machine learning system depends critically on effective representations of data. In many cases, it is desirable that a representation scheme uncovers the parts-based, additive nature of the data. Of current representation learning schemes, restricted Boltzmann machines (RBMs) have proved to be highly effective in unsupervised settings. However, when it comes to parts-based discovery, RBMs do not usually produce satisfactory results. We enhance such capacity of RBMs by introducing nonnegativity into the model weights, resulting in a variant called nonnegative restricted Boltzmann machine (NRBM). The NRBM produces not only controllable decomposition of data into interpretable parts but also offers a way to estimate the intrinsic nonlinear dimensionality of data, and helps to stabilize linear predictive models. We demonstrate the capacity of our model on applications such as handwritten digit recognition, face recognition, document classification and patient readmission prognosis. The decomposition quality on images is comparable with or better than what produced by the nonnegative matrix factorization (NMF), and the thematic features uncovered from text are qualitatively interpretable in a similar manner to that of the latent Dirichlet allocation (LDA). The stability performance of feature selection on medical data is better than RBM and competitive with NMF. The learned features, when used for classification, are more discriminative than those discovered by both NMF and LDA and comparable with those by RBM.", "histories": [["v1", "Fri, 18 Aug 2017 13:34:18 GMT  (1243kb,D)", "http://arxiv.org/abs/1708.05603v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tu dinh nguyen", "truyen tran", "dinh phung", "svetha venkatesh"], "accepted": false, "id": "1708.05603"}, "pdf": {"name": "1708.05603.pdf", "metadata": {"source": "CRF", "title": "Nonnegative Restricted Boltzmann Machines for Parts-based Representations Discovery and Predictive Model Stabilization", "authors": ["Tu Dinh Nguyen", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh", "Dinh Nguyen"], "emails": ["tu.nguyen@deakin.edu.au."], "sections": [{"heading": null, "text": "Keywords parts-based representation \u00b7 nonnegative \u00b7 restricted Boltzmann machines \u00b7 learning representation \u00b7 semantic features \u00b7 linear predictive model \u00b7 stability.Tu Dinh Nguyen, Truyen Tran, Dinh Phung, Svetha Venkatesh Center for Pattern Recognition and Data Analytics, Deakin University, Australia. Appropriate e-mail: tu.nguyen @ deakin.edu.au.ar Xiv: 170 8.05 603v 1 [cs.L G] 18 Aug 2"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Preliminaries", "text": "It is not only the way in which the hidden units cannot capture the hidden factors in the observations, but also the way in which the hidden units are linked by their connection quantities. A graphical representation of the RBM is given in Fig. 2. As a matter of convention in the RBM literature, we will use the terms \"unity\" and \"random variable.\" 2.1 Model of representationLet v denote the set of visible variables: v = [1, v2, vN] > [2, vN] and h indicate the set of hidden variables."}, {"heading": "3 Nonnegative restricted Boltzmann machine", "text": "s ability to estimate the intrinsic dimensionality of the data and to stabilize the linear predictive models. - The derivation of part-based representation begins with the connecting weights of the standard RBM. - In the RBM, two layers are connected to each other, which form a weight matrix W = [wnk] N \u00b7 K, in which wnk is the strength of association between the hidden unit and the visible unit n. - The column vector w \u00b7 k is the learned filter of the hidden unit k. - Illustrations of this column must be economical, e.g. only a small portion of the entries are non-zeros. - Recall that the activation of this hidden unit is also known as fire rate in the neural network language."}, {"heading": "4 Stabilizing Linear Predictive Models", "text": "RE \"s guide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green. RE\" s guide for the green for the green for the green"}, {"heading": "5 Experiments", "text": "In fact, most of us are able to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "5.1.1 Decomposing images into parts-based representations", "text": "We now show that the non-negative constraints allow the NRBM to create meaningful, part-based reception fields. Fig. 4 shows the 100 filters learned from the MNIST images. It is evident that basic structures of handwritten digits such as strokes and dots are discovered by both the RBM and the NRBM. However, the characteristics learned by the NRBM in Fig. 4b are easier to visually interpret, while the characteristics learned by the RBM in Fig. 4a are visually interpretable along the line with classical NMF [35] (Fig. 5b), whereas the RBM produces global facial structures (eyes, mouth, nose, eyebrows, etc.) revealed by the NRBM (Fig. 5a)."}, {"heading": "5.1.2 Dead factors and dimensionality estimation", "text": "We are now examining NRBM's ability to estimate the intrinsic dimensionality of the data, as described in Section 3.2. We find that by \"dimensionality\" we mean roughly the degree of variation, not just the dimension of the data volume. This is because our latent factors are discrete binary variables and can therefore be less flexible than real coefficients. To this end, we calculate the number of dead or unused hidden units. The hidden unit k is declared \"dead\" if the normalized '1 standard of its coupling weight vector is less than a threshold. In Fig. 7, the number of hidden units used is plotted against the total number of hidden units K by taking the average over a series of thresholds (corresponding to 0.01; 0.02;... 0.06), which in this case, however, do not cause any dead units."}, {"heading": "5.1.3 Semantic features discovering on text data", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5.3.1 Temporal validation", "text": "We derive the cohort in training and test data to validate the predictiveness of our proposed model. Two problems that need to be addressed during this time2 Note that the t-SNE does not cluster, but merely reduces the dimensionality of visualization in 2D while still trying to preserve the local characteristics of the data.3 Gaussian SVM has not performed well. The splitting process is: learning the past and predicting the future; ensuring training and test kits are completely separate. At this point, we use a time control point to divide the data into two parts. Specifically, we collect approvals that have discharge data before September 2010 to form the training set and then for testing. Next, we specify the set of unique patients in the training set. We then remove all approvals of such patients in the test set to ensure no overlap between two sets. Finally, we obtain 1 unique test data with 360 patients and 415 in the training model."}, {"heading": "5.3.2 Evaluation protocol", "text": "The Jaccard index, also known as the Jaccard similarity coefficient, of course takes into account both similarity and diversity in order to measure how two sets of characteristics relate. Consistency index supports the selection of characteristics in obtaining several desirable properties, i.e. monotonicity, limits and correction for chances. We trained our proposed model by running M = 10 bootstraps and obtained a list of characteristic sets S = {S1, S2,..., SM} in which Si is a subset of the original set of characteristic sets. Note that the cardinalities are: | Si | T and | v | K with the condition: T \u2264 K. Taking into account a pair of subgroups Si and Sj, the pairwiseconsistency index C (Si, Sj): The pairwiseconsistency index C (Sj, Sj) is defined as C (Sj, Sj) \u2212 Sj (Sj \u2212 T)."}, {"heading": "5.3.3 Results", "text": "This year it is more than ever before."}, {"heading": "7 Conclusion", "text": "In summary, this work introduces a novel variant of the powerful restricted Boltzmann machine, called non-negative RBM (NRBM), where the mapping weights do not necessarily have to be negative. This gives the NRBM the new ability to detect interpretable part-based representations, semantically plausible high-quality features for additive data such as images and texts. Our proposed method can also stabilize linear prediction models in feature selection tasks for high-dimensional medical data. Furthermore, the NRBM can be used to detect the intrinsic dimensionality of the data, which is not seen in the standard RBM. This is because the latent factors \"compete\" with each other under the non-negativity limitation in order to best represent data, which leads to some unused factors. At the same time, the NRBM retains the intrinsic dimensionality of the data, the almost full strength of the RNRM representation, the MNRF representation, the non-discriminatory representation of the NRF and the non-NRF representation, where the NRF representation is well divided."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The success of any machine learning system depends critically on<lb>effective representations of data. In many cases, it is desirable that a represen-<lb>tation scheme uncovers the parts-based, additive nature of the data. Of current<lb>representation learning schemes, restricted Boltzmann machines (RBMs) have<lb>proved to be highly effective in unsupervised settings. However, when it comes<lb>to parts-based discovery, RBMs do not usually produce satisfactory results. We<lb>enhance such capacity of RBMs by introducing nonnegativity into the model<lb>weights, resulting in a variant called nonnegative restricted Boltzmann machine<lb>(NRBM). The NRBM produces not only controllable decomposition of data<lb>into interpretable parts but also offers a way to estimate the intrinsic nonlin-<lb>ear dimensionality of data, and helps to stabilize linear predictive models. We<lb>demonstrate the capacity of our model on applications such as handwritten<lb>digit recognition, face recognition, document classification and patient read-<lb>mission prognosis. The decomposition quality on images is comparable with or<lb>better than what produced by the nonnegative matrix factorization (NMF),<lb>and the thematic features uncovered from text are qualitatively interpretable<lb>in a similar manner to that of the latent Dirichlet allocation (LDA). The sta-<lb>bility performance of feature selection on medical data is better than RBM<lb>and competitive with NMF. The learned features, when used for classifica-<lb>tion, are more discriminative than those discovered by both NMF and LDA<lb>and comparable with those by RBM.", "creator": "LaTeX with hyperref package"}}}