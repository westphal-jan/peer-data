{"id": "1708.05604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Accelerating recurrent neural network training using sequence bucketing and multi-GPU data parallelization", "abstract": "An efficient algorithm for recurrent neural network training is presented. The approach increases the training speed for tasks where a length of the input sequence may vary significantly. The proposed approach is based on the optimal batch bucketing by input sequence length and data parallelization on multiple graphical processing units. The baseline training performance without sequence bucketing is compared with the proposed solution for a different number of buckets. An example is given for the online handwriting recognition task using an LSTM recurrent neural network. The evaluation is performed in terms of the wall clock time, number of epochs, and validation loss value.", "histories": [["v1", "Fri, 18 Aug 2017 13:36:30 GMT  (981kb)", "http://arxiv.org/abs/1708.05604v1", "4 pages, 5 figures, Comments, 2016 IEEE First International Conference on Data Stream Mining &amp; Processing (DSMP), Lviv, 2016"]], "COMMENTS": "4 pages, 5 figures, Comments, 2016 IEEE First International Conference on Data Stream Mining &amp; Processing (DSMP), Lviv, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["viacheslav khomenko", "oleg shyshkov", "olga radyvonenko", "kostiantyn bokhan samsung r&d institute ukraine srk)"], "accepted": false, "id": "1708.05604"}, "pdf": {"name": "1708.05604.pdf", "metadata": {"source": "META", "title": "Accelerating Recurrent Neural Network Training", "authors": ["Viacheslav Khomenko", "Oleg Shyshkov", "Olga Radyvonenko", "Kostiantyn Bokhan"], "emails": ["v.khomenko@samsung.com"], "sections": [{"heading": null, "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "A. Sequence bucketing algorithm", "text": "The empirical distribution of the input sequence lengths and an example of clusterization for the number of buckets Q = 6 are shown in Fig. 2.Labeling can be described as an optimization problem. Let}, {21 nsssS be the set of sequencesand ii sl is the length of sequence i. Each GPU sequence in a mini-batch can be described in a synchronized parallel manner so that the processing time of a mini-batch},..., {21 ksssB is proportional to iki lO,,1max and processing time of the entire set is expressed as: ini lknOST,,1max (1) The minimum and maximum processing time in a mini-batch could be very different if the sequences were randomly mixed before the splitting."}, {"heading": "B. Parallel training of recurrent neural networks", "text": "For the parallel training of the RNNs on the GPU, we propose the following algorithm, which allows massive parallel model training and can be scaled up to a large number of GPUs: 1. Initialize basic parameters of the model. 2. Create Q models (for optimal input sequence lengths) and serialize models for storage, for example the file system. Parameters of each model are randomly initialized. 3. Generate training data (1st epoch) or regenerate training data by remixing parts of the data for the following epochs. The number of data portions corresponds to the number of GPUs. This is possible on the assumption that the amount of training data is sufficient [9]. 4. Individual training processes arise for each of the training data portions. The process iterates over the minibatches in the data portion. The batches are formed taking into account the input sequence lengths. The parameter update rule of the individual model is ELTA 21 [IVD]."}, {"heading": "A. Experimental setup", "text": "The raw data contains 1 Gb samples of Afrikaans and English in binary form; the data set was collected on Samsung Galaxy S-Note devices with stylus input; the validation set is composed of 5% randomly selected samples of varying lengths; the data set contains text labels that serve as a reference to output sequences; however, these labels are not explicitly aligned to handline input sequences; for each era, the system was first fed with shorter sequences (first buckets), and then gradually the bucket count increased; the LSTM model was evaluated with CTC cost functions using Theano [22] and Lasagne [23] frameworks.The recurring training for neural network models was evaluated on a rack of 6 NVidia Tesla K40m GPUs."}, {"heading": "B. Model training", "text": "The validation loss as a function of the wall clock time and epoch number is given in Fig. 3; the best acceleration with minimum validation loss was achieved for Q = 3.Regarding the wall clock time, the system without bucketing (purely random distribution of sequences on minibatches) has the longest epoch time (4 hours per epoch, Fig. 4). The epoch time decreases as Q increases. For the value of Q = 3 the speed is up to factor 4.From the comparison we can observe the influence of the sequence bucket on the training speed and losses. We observed a faster convergence of validation loss especially at the beginning of training. The validation loss as a function of the wall clock time is presented in Fig. 5a, and as a function of the epoch number in training."}], "references": [{"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural computation, vol. 12(10), pp. 2451\u20132471, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint, arXiv:1412.3555, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["A. Graves", "M. Liwicki", "H. Bunke", "J. Schmidhuber", "S. Fern\u00e1ndez"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 577\u2013584.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Combination of global and local contexts for text/non-text classification in heterogeneous online handwritten documents", "author": ["T. Van Phan", "M. Nakagawa"], "venue": "Pattern Recognition, vol. 51, 2016, pp. 112\u2013124.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["O. Irsoy", "C. Cardie"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, October, pp. 720\u2013728.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convex net: A scalable architecture for speech pattern classification", "author": ["D. Yu", "L. Deng"], "venue": "Proceedings of Interspeech, 2011, pp. 2285\u20132288.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for lowlatency speech synthesis", "author": ["H. Zen", "H. Sak"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference, 2015, pp. 4470\u20134474.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Speed Up of Recurrent Neural Network Language Models With Sentence Independent Subsampling Stochastic Gradient Descent", "author": ["Y. Shi", "M.-Y. Hwang", "K. Yao", "M. Larson"], "venue": "INTERSPEECH, 2013. pp. 1203\u20131207.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerating recurrent neural network training via two stage classes and parallelization", "author": ["H. Zhiheng"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A. Smola", "L. Li"], "venue": "Advances in Neural Information Processing Systems, vol. 23, 2010, pp. 2595\u20132603.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies", "author": ["S. Ji", "S.V.N. Vishwanathan", "N.N. Satish", "M.J. Anderson", "P. Dubey"], "venue": "arXiv preprint, arXiv:1511.06909, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference, pp. 285\u2013290.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert"], "venue": "In Proceedings of the 26th annual international conference on machine learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez"], "venue": "Proceedings of the International Conference on Machine Learning, ICML-2006, pp. 369\u2013376.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM", "author": ["J.S. Garofolo"], "venue": "National Institute of Standards and Technology, NISTIR 4930, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Unipen project of on-line data exchange and recognizer benchmarks", "author": ["I. Guyon"], "venue": "Pattern Recognition, vol. 2- Conference B: Computer Vision & Image Processing, Proceedings of the 12th IAPR International. Conference on, vol. 2, IEEE, 1994, pp. 29\u201333.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "IAMonDo-database: an online handwritten document database with non-uniform contents", "author": ["E. Inderm\u00fchle", "M. Liwicki", "H. Bunke"], "venue": "Proceedings of the 9th IAPR International Workshop on Document Analysis Systems, ACM, 2010, pp. 97\u2013104.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent Memory Network for Language Modeling.", "author": ["K. Tran", "A. Bisazza", "C. Monz"], "venue": "arXiv preprint,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1223\u20131231.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint, arXiv:1212.5701, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["T.T.D. Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "A. Belopolsky"], "venue": "arXiv preprint, arXiv:1605.02688, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Long Short-Term Memory (LSTM) [1] RNNs perform better on tasks involving long time lags compared to traditional RNNs.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "Gated Recurrent Unit (GRU) networks [2] have similar ideology to an LSTM, but they speed up training due to architectural simplifications.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 195, "endOffset": 198}, {"referenceID": 5, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 6, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 237, "endOffset": 240}, {"referenceID": 7, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 260, "endOffset": 263}, {"referenceID": 7, "context": "The problem of accelerating the RNN mini-batch gradient descent training is widely discussed in the literature in the last years [8, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 8, "context": "The problem of accelerating the RNN mini-batch gradient descent training is widely discussed in the literature in the last years [8, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": ") to improve convergence of model training [8].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "Training parallelization and a two-stage network structure for RNN [9] allow to speed-up training.", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "The BlackOut [11] approach allows to accelerate training for even larger vocabularies (10 outputs).", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "It relies on weighted sampling strategy, employs a discriminative training loss and is applied only to the softmax output layer, in contrast to DropOut [12], which is typically applied to the input and hidden layers.", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "In its turn, the curriculum learning [13] consists in organizing training samples in a meaningful way rather than in purely random order.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "It improves LSTM training on the program evaluation and memorization tasks [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "handwriting, speech or gesture recognition), where noisy real-valued input sequences are annotated with non-aligned strings [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "Most of benchmark datasets for perceptual machine learning tasks (TIMIT [16], UNIPEN [17], IAMonDo [18]) contain recordings of different length.", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "Most of benchmark datasets for perceptual machine learning tasks (TIMIT [16], UNIPEN [17], IAMonDo [18]) contain recordings of different length.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "Most of benchmark datasets for perceptual machine learning tasks (TIMIT [16], UNIPEN [17], IAMonDo [18]) contain recordings of different length.", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "Batch grouping algorithms could be useful for organizing training data [19, 20].", "startOffset": 71, "endOffset": 79}, {"referenceID": 19, "context": "Batch grouping algorithms could be useful for organizing training data [19, 20].", "startOffset": 71, "endOffset": 79}, {"referenceID": 8, "context": "This is possible under the assumption that the amount of training data is sufficient [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "The parameter update rule of the individual model is ADADELTA [21].", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "The aggregation of parameters is done in the main process according to the model update rule proposed in [9].", "startOffset": 105, "endOffset": 108}, {"referenceID": 21, "context": "The LSTM model was trained with CTC cost function using Theano [22] and Lasagne [23] frameworks.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "An efficient algorithm for recurrent neural network training is presented. The approach increases the training speed for tasks where a length of the input sequence may vary significantly. The proposed approach is based on the optimal batch bucketing by input sequence length and data parallelization on multiple graphical processing units. The baseline training performance without sequence bucketing is compared with the proposed solution for a different number of buckets. An example is given for the online handwriting recognition task using an LSTM recurrent neural network. The evaluation is performed in terms of the wall clock time, number of epochs, and validation loss value.", "creator": "Microsoft\u00ae Word 2016"}}}