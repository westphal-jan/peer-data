{"id": "1708.05655", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Multi-objective Contextual Multi-armed Bandit Problem with a Dominant Objective", "abstract": "In this paper, we propose a new multi-objective contextual multi-armed bandit problem with two objectives, where one of the objectives dominates the other objective. Unlike single-objective bandit problems in which the learner obtains a random scalar reward for each arm it selects, in the proposed problem, the learner obtains a random reward vector, where each component of the reward vector corresponds to one of the objectives and the distribution of the reward depends on the context that is provided to the learner at the beginning of each round. We call this problem contextual multi-armed bandit with a dominant objective (CMAB-DO). In CMAB-DO, the goal of the learner is to maximize its total reward in the non-dominant objective while ensuring that it maximizes its total reward in the dominant objective. In this case, the optimal arm given a context is the one that maximizes the expected reward in the non-dominant objective among all arms that maximize the expected reward in the dominant objective. First, we show that the optimal arm lies in the Pareto front. Then, we propose the multi-objective contextual multi-armed bandit algorithm (MOC-MAB), and define two performance measures: the 2-dimensional (2D) regret and the Pareto regret. We show that both the 2D regret and the Pareto regret of MOC-MAB are sublinear in the number of rounds. We also compare the performance of the proposed algorithm with other state-of-the-art methods in synthetic and real-world datasets. The proposed model and the algorithm have a wide range of real-world applications that involve multiple and possibly conflicting objectives ranging from wireless communication to medical diagnosis and recommender systems.", "histories": [["v1", "Fri, 18 Aug 2017 15:41:58 GMT  (512kb,D)", "http://arxiv.org/abs/1708.05655v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cem tekin", "eralp turgay"], "accepted": false, "id": "1708.05655"}, "pdf": {"name": "1708.05655.pdf", "metadata": {"source": "CRF", "title": "Multi-objective Contextual Multi-armed Bandit Problem with a Dominant Objective", "authors": ["Cem Tekin", "Eralp Turgay"], "emails": ["cemtekin@ee.bilkent.edu.tr,", "turgay@ee.bilkent.edu.tr."], "sections": [{"heading": null, "text": "Index terms - online learning, contextual bandits, multi-objective bandits, dominant goal, multi-dimensional regret, pareto regret.I. INTRODUCTION With the rapid increase in the generation speed of streaming data, online learning methods are becoming increasingly valuable for sequential decision-making problems. Many of these problems, including referral systems [1], medical screening [3], cognitive radio networks [4], and wireless network surveillance [6] can involve multiple and possibly conflicting objectives. In this work, we propose a multi-objective contextual bandit problem with dominant and non-dominant objectives. For this problem, we construct a multi-objective contextual bandit algorithm called MOC-MAB, which maximizes the long-term reward of the non-dominant objective conditions to the fact that it maximizes the long-term reward of the dominant objective objectives."}, {"heading": "II. RELATED WORK", "text": "In the last ten years, many variants of the classic multi-armed problem have been introduced (see [11] for a comprehensive discussion).Two notable examples are context-dependent bandits [9], [12], [13] and multi-objective bandits [7].While these examples have been examined separately in previous work, in this paper we aim to merge context-dependent bandits and multi-objective bandits. Below, we discuss the related work on the classic multi-armed bandit problem, context-dependent bandits and multi-objective bandits. The differences between our work and related work are summarized in Table I."}, {"heading": "A. The Classical Multi-armed Bandit Problem", "text": "The classic multi-armed bandit problem concerns K-weapons with an unknown distribution of rewards. The learner selects weapons one after the other and observes loud reward patterns from the selected weapons. The goal of the learner is to use the knowledge gained from these observations to maximize his long-term rewards. To do this, the learner must identify weapons with high rewards without wasting too much time with weapons with low rewards. Finally, he must use the balance between exploration and exploitation.A by technical analysis of the classic multi-armed bandit problem is given in [14], where it is shown that O (log T) regret is asymptotically achieved through index policies that use upper confidence limits (UCBs) for the rewards. This result is narrow in the sense that there is an asymptotic lower limit. Later, in [15] it is shown that it is possible to achieve O (log T regret) by using the means of index constructing the rewards."}, {"heading": "B. The Contextual Bandit Problem", "text": "The context, of course, arises in many practical applications such as social referral systems [18], medical diagnoses [19] and big data stream mining [20]. Existing work on contextual bandits can be divided into three categories based on how the contexts arrive and how they relate to the rewards of the arm. However, the first category assumes the existence of similarity information (usually in the form of a metric) relating to the expected reward of an arm as a function of the context. In this category, no statistical assumptions are made about how the contexts arrive. However, given a particular context, the rewards of the arm come from a fixed distribution parameterized by the context.This problem is taken into account."}, {"heading": "C. The Multi-objective Bandit Problem", "text": "Since the rewards are no longer scalar, the definition of a yardstick for comparing the learner with others becomes opaque. Existing work on multi-objective bandits can be divided into two categories: Pareto approach and scalarized regret. 3In the Pareto approach, the main idea is to estimate the Pareto front, which consists of the arms that are not dominated by any other arm. Dominance relationship is defined in such a way that if the expected reward of one arm a is greater than the expected reward of another arm a in at least one goal, and the expected reward of the arm a is not greater than the expected reward of the arm a in any other goal, then the arm a dominates the arm a. This approach is proposed in [7], and a learning algorithm called Pareto-UCB1, which achieves O (log T) Pareto regret. Essentially, this algorithm calculates UCB index for each target algorithm, then we estimate and pareto algorithm."}, {"heading": "III. PROBLEM DESCRIPTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. System Model", "text": "The system works in a sequence of rounds indexed by the number of individual terms. At the beginning of round t, the learner will observe a d-dimensional context designated by xt. Without loss of generality, we assume that xt lies in context space X: = [0, 1]. After consideration, the learner selects an arm from a finite set A and then looks at a two-dimensional random reward rt = (r1t, r 2 t), which is both on xt and on. Here, r1t and r 2 t denote the rewards in the dominant and non-dominant objective, respectively, they are given by r1t = \u00b5 1 on (xt) and r 2 t = \u00b5 2 on (xt) + 2 t, where the reward is represented in the dominant and non-dominant objective."}, {"heading": "B. Definitions of the 2D Regret and the Pareto Regret", "text": "The goal of the learner is to compete with an oracle that knows the expected rewards of the arms for each context and selects the optimal arm taking into account the current context. Therefore, after round T, the learner's 2D regret becomes a tuple (Reg1 (T), Reg2 (T), whereas Regi (T): = T \u2211 t = 1 microgram (xt) \u2212 T \u2211 t = 1 microgram (xt), i \u01921, 2 (1) for an arbitrary sequence of contexts x1,.., xT. Another interesting measure of performance is the Pareto regret [7], which measures the loss of the learner in relation to the arms in the Pareto front. To define the Pareto regret, we first define the Pareto suboptimism gap (PSG).Definition 2 (PSG of an arm).Definition of an arm in relation to a given context in the Pareto regret (the Pareto Regret)."}, {"heading": "C. Applications of CMAB-DO", "text": "In this subsection, we describe two possible applications of CMAB-DO.1) Multi-channel communication: Consider a multi-channel communication scenario in which a user selects a channel Q-Q and a transmission rate R-R in each round after having received context xt: = {SNRQ, t} Q-Q, where SNRQ, t is the signal-to-noise ratio of channel Q in round. In this setup, each arm corresponds to a transmission rate channel pair that is related to aR, Q. Consequently, the group of arms A = R \u00d7 Q. When the user has completed his transmission at the end of round t, he receives a two-dimensional reward in which the dominant arm is related to throughput and the non-dominant arm to reliability. Here, r2t, 0, 1} where 0 and 1 correspond to this objective diagnosis of a failed and successful transmission. In addition, the success rate of aR, Q, Q = 1 \u2212 pout \u2212 pr (Q) is the probability (Q) of the patient (Q \u2212 xt), Q \u2212 Q (1) is the patient's (Q."}, {"heading": "IV. THE LEARNING ALGORITHM", "text": "We introduce MOC-MAB in this section. Its pseudo-code is Uncertainty reported in algorithm 1 = Uncertainty reported. (MOC-MAB uniformly divided into md hypercubes with edge lengths of 1 / 2. This division is called by P. For each p-P and a-A there will be a reward: (i) a count Na, p counting the number of contexts selected before the current round, (ii) the number of rewards obtained before the current round, with the context in p and arm a selected, i.e., p) the number of contexts set in p and p-2 a, p for the dominant and non-dominant targets. The idea behind the split is to use the similarity of the rewards given in Assumption 1 to jointly learn for groups of similar contexts. Basically, if the number of sets in the partition is small, the number of past samples is large, the sample specific group is large."}, {"heading": "V. REGRET ANALYSIS", "text": "In this section, we demonstrate that both the 2D regret and the pareto regret of MOC-MAB (s) sublinear functions of T (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s (s) (s) (s) (s) (s) (s) (s (s) (s) (s) (s) (s (s) (s) (s) (s (s) (s) (s (s) (s (s) (s) (s) (s) (s (s) (s) (s) (s (s (s) (s (s) (s) (s (s) (s) (s) (s (s (s) (s) (s) (s) (s (s) (s) (s (s (s) (s) (s) (s (s) (s) (s) (s) (s) (s (s) (s) (s (s) (s) (s (s) (s) (s) (s) (s) (s, s (s"}, {"heading": "VI. LEARNING UNDER PERIODICALLY CHANGING REWARD DISTRIBUTIONS", "text": "In many practical cases, the reward distribution of an arm changes over time even under the same context. In a referral system, for example, the probability that a user clicks on an ad varies with the time of day, but the pattern of change can be periodic on a daily basis and this can be detected by the system. In addition, this change is usually gradual over time. In this section, we will expand MOC-MAB to deal with such settings."}, {"heading": "A. Experiment 1", "text": "In this experiment, we evaluate the performance of MOCMAB on the breast cancer dataset from the UCI Machine Learning Repository [30]. The dataset contains 569 instances and 30 characteristics such as radius, texture, circumference, surface, smoothness, compactness, etc. 357 instances are labeled benign, the others are labeled malignant. First, we apply PCA to reduce the dimension of the feature vector to 3. We use these characteristics throughout the rest of the experiment. The benchmarks we compare MOC-MAB are support vectors, multi-layered perceptrons, and logistical regression. For each run, the dataset is randomly divided into 284 training and 285 testing instances. All reported results are averaged over 50 runs."}, {"heading": "B. Experiment 2", "text": "In this experiment, we use the data set and competition algorithms of Experiment 1. The test phase size is set at T = 105. The purpose of this experiment is to highlight the advantage of online learning over offline learning when the distribution of training and test sets is different. To this end, offline benchmarks are trained with training sets that have different favorable patient rates. Figure 2 shows the error rate at T = 105 on average over 10 runs. As expected, the error rate of offline methods strongly depends on the breakdown of the training set and is therefore much worse than MOC-MAB."}, {"heading": "C. Experiment 3", "text": "In this case, the two candidates are people who are able to position themselves at the top of the party."}, {"heading": "VIII. CONCLUSION", "text": "In accordance with this definition, we propose two performance indicators: 2D regret (which is multi-dimensional) and Pareto regret (which is scalar), then we propose an online learning algorithm called MOCMAB and show that it achieves sublinear 2D regret and Pareto regret. To the best of our knowledge and belief, we first consider in our work a multi-objective contextual bandit problem where the expected rewards and contexts are interrelated through similarity information, and we evaluate the performance of MOC-MAB on both real and synthetic data sets, comparing it with offline methods and other bandit algorithms. Our results show that MOC-MAB outperforms its competitors who are not specifically designed to solve problems with dominant and non-dominant targets."}, {"heading": "APPENDIX A OOF OF LEMMA 1", "text": "From the definitions of Lia, p (t), U i (a), p (t), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (a), i (b), i (b), p (v), p (v), p (p), p (t), p (p (p), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (p), p (p), p (p), p (p), p (t), p (p, p, p (t), p (p, p, p, p, p (t), p (p, p, p, p, p, t, p, p, p (t), p (p, p, p), p (t), p (p, p, p, p, p, p (t), p, p, p (t), p (p, p, p, p, p, p, p, p, p, p (t), p (t), p (p, p, p, p, p, p, p (t), p (t, p, p, p, p, p, p, p, p (t), p (t, p, p, p, p, p, p, p (t), p (t), p (t, p (t, p, p, p, p, p (t), p (t), p (t, p (t, p, p, p, p, p, p, p (t), p (t), p (t, p (t, p, p, p, p, p, p (t, p, p, p, p, p, p, p, p, p, p), p (t, p (t), p, p (t, p, p"}, {"heading": "APPENDIX B PROOF OF LEMMA 5", "text": "Leave Ta, p: = {1 \u2264 l \u2264 Np (t): a \u0439p (l) = a \u00b2 and T \u00b2 a \u00b2, p: = {l \u00b2 Ta, p: n \u00b2 a, p (l) \u2265 1} With Lemma 2 we have Reg1p (t) = a \u00b2, p (U1a), p (l) \u2212 l (l) \u2212 p (l) \u2212 p (l) + 2 (\u03b2 + 2) v) + | C1max = a \u00b2 l \u00b2, p (U1a \u00b2 p (l), p (l) \u2212 p (l), p (l) + 2 (\u03b2 + 2) v) + | A | a \u00b2 l \u00b2, p (T), p (U1a \u00b2 p (l) p (l), p (A), p (A, A, A, A, A, A (p A, A, A), p (p A), p (p A), p (A), p (p, A, A (p), A (p, A), P (A, A), P (A, A), P (p, A), A (p, A, A, A (p), A (p, A), A (p, A, A, A, A, A, A (p), A (p, A), A (p, A, A, A, A, A, A (p), A (p, A), A (p, A, A, A, A, A (p, A), A (p, A, A (p, A), A (p, A), A (p, A (p, A), A (p, A), A (p, A, A (p, A), A (p, A, A (p, A), A (p, A), A (p, A (p, A), A (p, A, A (p, A), A (p, A), A (p, A (p, A), A (p, A (p, A), A (p (p, A), A (p, A), A (p (p, A), A (p, A),"}, {"heading": "APPENDIX C PROOF OF LEMMA 6", "text": "The result of Lemma 4 gives the contribution to the regret of the non-dominant goal in rounds for which u-a-p (t), p (t) > \u03b2v is limited by C2max | A | (2Am, T \u03b22v2 + 1). (21) Leave T 2a, p: = {l \u2264 Np (t): a-p (t) = a and N-p (l) \u2265 2Am, T / (\u03b22v2). By Lemma 3 we have an a-p (l). (l) l-p (t), p-p (l) \u2212 p (l) \u2264 2Am, T / (\u03b22v2)."}, {"heading": "APPENDIX D", "text": "CONCENTRATION INEQUALITY [28], [31] Consider an arm a for which the rewards of the lens i are generated by a process {Ria (t)} Tt = 1 with \u00b5ia = E [Ria (t)] where the noise is Ria (t) \u2212 \u00b5ia conditionally 1-sub-gauss. Let Na (T) \u2265 1 indicate the number of times a is selected until the end of time T. Let us say that p = 1 I (a (t) = a) R i (t) / Na (T)."}], "references": [{"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire"], "venue": "Proc. 19th International Conference on World Wide Web, 2010, pp. 661\u2013670.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Personalized course sequence recommendations", "author": ["Jie Xu", "Tianwei Xing", "Mihaela Van Der Schaar"], "venue": "IEEE Trans. Signal Process., vol. 64, no. 20, pp. 5340\u20135352, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Using contextual learning to improve diagnostic accuracy: Application in breast cancer screening", "author": ["Linqi Song", "William Hsu", "Jie Xu", "Mihaela van der Schaar"], "venue": "IEEE J. Biomed. Health Inform., vol. 20, no. 3, pp. 902\u2013914, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiuser channel allocations in cognitive radio networks: A combinatorial multiarmed bandit formulation", "author": ["Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain"], "venue": "Proc. IEEE Symposium on New Frontiers in Dynamic Spectrum, 2010, pp. 1\u20139.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed stochastic online learning policies for opportunistic spectrum access", "author": ["Yi Gai", "Bhaskar Krishnamachari"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 23, pp. 6184\u20136193, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequential learning for multi-channel wireless network monitoring with channel switching costs", "author": ["Thanh Le", "Csaba Szepesvari", "Rong Zheng"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 22, pp. 5919\u20135929, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Designing multi-objective multiarmed bandits algorithms: A study", "author": ["Madalina M Drugan", "Ann Nowe"], "venue": "Proc. International Joint Conference on Neural Networks (IJCNN), 2013, pp. 1\u20138.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Contextual multi-armed bandits", "author": ["Tyler Lu", "D\u00e1vid P\u00e1l", "Martin P\u00e1l"], "venue": "Proc. AISTATS, 2010, pp. 485\u2013492.  13", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Contextual bandits with similarity information", "author": ["Aleksandrs Slivkins"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 2533\u20132568, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Li", "Lev Reyzin", "Robert E Schapire"], "venue": "Proc. AISTATS, 2011, pp. 208\u2013214.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 1, pp. 1\u2013122, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["John Langford", "Tong Zhang"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 20, pp. 1096\u20131103, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert Schapire"], "venue": "Proc. International Conference on Machine Learning (ICML), 2014, pp. 1638\u20131646.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze L Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics, vol. 6, pp. 4\u201322, 1985.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1985}, {"title": "Sample mean based index policies with O(logn) regret for the multi-armed bandit problem", "author": ["Rajeev Agrawal"], "venue": "Advances in Applied Probability, vol. 27, no. 4, pp. 1054\u20131078, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Aur\u00e9lien Garivier", "Olivier Capp\u00e9"], "venue": "Proc. 24th Annual Conference on Learning Theory (COLT), 2011, pp. 359\u2013376.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed online learning in social recommender systems", "author": ["Cem Tekin", "Simpson Zhang", "Mihaela van der Schaar"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 8, no. 4, pp. 638\u2013652, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive ensemble learning with confidence bounds", "author": ["Cem Tekin", "Jinsung Yoon", "Mihaela van der Schaar"], "venue": "IEEE Trans. Signal Process., vol. 65, no. 4, pp. 888\u2013903, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed online learning via cooperative contextual bandits", "author": ["Cem Tekin", "Mihaela van der Schaar"], "venue": "IEEE Trans. Signal Process., vol. 63, no. 14, pp. 3700\u20133714, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "RELEAF: An algorithm for learning and exploiting relevance", "author": ["Cem Tekin", "Mihaela van der Schaar"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 9, no. 4, pp. 716\u2013727, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of kernelised contextual bandits", "author": ["Michal Valko", "Nathan Korda", "R\u00e9mi Munos", "Ilias Flaounas", "Nello Cristianini"], "venue": "Proc. 29th Conference on Uncertainty in Artificial Intelligence (UAI), 2013, pp. 654\u2013663.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "Proc. 27th Conference on Uncertainty in Artificial Intelligence (UAI), 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge gradient for multi-objective multi-armed bandit algorithms", "author": ["Saba Q Yahyaa", "Madalina M Drugan", "Bernard Manderick"], "venue": "Proc. ICAART (1), 2014, pp. 74\u201383.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Thompson sampling for multiobjective multi-armed bandits problem", "author": ["Saba Q Yahyaa", "Bernard Manderick"], "venue": "Proc. European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. 47\u201352.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Annealing-Pareto multi-objective multi-armed bandit algorithm", "author": ["Saba Q Yahyaa", "Madalina M Drugan", "Bernard Manderick"], "venue": "Proc. IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2014, pp. 1\u20138.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalarization based Pareto optimal set of arms identification algorithms", "author": ["Madalina M Drugan", "Ann Now\u00e9"], "venue": "Proc. International Joint Conference on Neural Networks (IJCNN), 2014, pp. 2690\u20132697.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), 2011, pp. 2312\u20132320.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "How to use expert advice", "author": ["Nicolo Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P Helmbold", "Robert E Schapire", "Manfred K Warmuth"], "venue": "Journal of the ACM (JACM), vol. 44, no. 3, pp. 427\u2013485, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to optimize via posterior sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Mathematics of Operations Research, vol. 39, no. 4, pp. 1221\u20131243, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "For this, we extend the Pareto regret proposed in [7] to take into account the dependence of the Pareto front on the context.", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "Then, we argue that it is possible to make the Pareto regret of MOC-MAB \u00d5(T ) by adjusting its parameters, such that the Pareto regret becomes order optimal up to a logarithmic factor [8], but this comes at an expense of making the regret in the non-dominant objective of MOC-MAB linear in the number of rounds.", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "RELATED WORK In the past decade, many variants of the classical multiarmed problem have been introduced (see [11] for a comprehensive discussion).", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 13, "context": "A through technical analysis of the classical multi-armed bandit problem is given in [14], where it is shown that O(log T ) regret is achieved asymptotically by index policies that use upper confidence bounds (UCBs) for the rewards.", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "Later on, it is shown in [15] that it is possible to achieve O(log T ) regret by using index policies constructed using the sample means of the arm rewards.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "The first finitetime logarithmic regret bound is given in [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "This line of research has been followed by many others, and new algorithms with tighter regret bounds have been proposed [17].", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "The context naturally arises in many practical applications such as social recommender systems [18], medical diagnosis [19] and big data stream mining [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "The context naturally arises in many practical applications such as social recommender systems [18], medical diagnosis [19] and big data stream mining [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The context naturally arises in many practical applications such as social recommender systems [18], medical diagnosis [19] and big data stream mining [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "This problem is considered in [8], and the Query-AdClustering algorithm that achieves O(T 1\u22121/(2+dc)+ ) regret for any > 0 is proposed, where dc is the covering dimension of the similarity space.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "A parallel work [9] proposes the contextual zooming algorithm which partitions the similarity space non-uniformly, according to both sampling frequency and rewards obtained from different regions of the similarity space.", "startOffset": 16, "endOffset": 19}, {"referenceID": 20, "context": "This issue is addressed in [21], where it is assumed that the arm rewards depend on an unknown subset of the contexts, and it is shown that the regret in this case only depends on the number of relevant context dimensions.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "[1] proposed the LinUCB algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], and is shown to achieve \u00d5( \u221a Td) regret, where d is the dimension of the context.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] mixed LinUCB and SupLinUCB with kernel functions and proposed an algorithm whose regret is \u00d5( \u221a T d\u0303) where d\u0303 is the effective dimension of the kernel feature space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposed the epoch greedy algorithm with O(T ) regret and later works [13], [23] proposed more efficient learning algorithms with \u00d5(T ) regret.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] proposed the epoch greedy algorithm with O(T ) regret and later works [13], [23] proposed more efficient learning algorithms with \u00d5(T ) regret.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "[12] proposed the epoch greedy algorithm with O(T ) regret and later works [13], [23] proposed more efficient learning algorithms with \u00d5(T ) regret.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 73, "endOffset": 76}, {"referenceID": 9, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 201, "endOffset": 204}, {"referenceID": 6, "context": "This approach is proposed in [7], and a learning algorithm called Pareto-UCB1 that achieves O(log T ) Pareto regret is proposed.", "startOffset": 29, "endOffset": 32}, {"referenceID": 23, "context": "A modified version of this algorithm where the indices depend on both the estimated mean and the estimated standard deviation is proposed in [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "Numerous other variants are also considered in prior works, including the Pareto Thompson sampling algorithm in [25] and the Annealing Pareto algorithm in [26].", "startOffset": 112, "endOffset": 116}, {"referenceID": 25, "context": "Numerous other variants are also considered in prior works, including the Pareto Thompson sampling algorithm in [25] and the Annealing Pareto algorithm in [26].", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "On the other hand, in the scalarized approach [7], [27], a random weight is assigned to each objective at each round, from which for each arm a weighted sum of the indices of the objectives are calculated.", "startOffset": 46, "endOffset": 49}, {"referenceID": 26, "context": "On the other hand, in the scalarized approach [7], [27], a random weight is assigned to each objective at each round, from which for each arm a weighted sum of the indices of the objectives are calculated.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "For instance, Scalarized UCB1 in [7] achieves O(S\u2032 log(T/S\u2032)) scalarized regret where S\u2032 is the number of scalarization functions used by the algorithm.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "Without loss of generality, we assume that xt lies in the context space X := [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 7, "context": "We assume that the expected rewards are H\u00f6lder continuous in the context, which is a common assumption in the contextual bandit literature [8], [19], [20].", "startOffset": 139, "endOffset": 142}, {"referenceID": 18, "context": "We assume that the expected rewards are H\u00f6lder continuous in the context, which is a common assumption in the contextual bandit literature [8], [19], [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "We assume that the expected rewards are H\u00f6lder continuous in the context, which is a common assumption in the contextual bandit literature [8], [19], [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 27, "context": "1Examples of 1-sub-Gaussian distributions include the Gaussian distribution with zero mean and unit variance, and any distribution defined over an interval of length 2 with zero mean [28].", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "Another interesting performance measure is the Pareto regret [7], which measures the loss of the learner with respect to arms in the Pareto front.", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "See [29] and [19] for a discussion on the doubling-trick.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "See [29] and [19] for a discussion on the doubling-trick.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "To prove the lemma above, we use the concentration inequality given in Lemma 6 in [28] to bound the probability of UCa,p.", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "To overcome this problem, we use the sandwich technique proposed in [19] in order to bound the rewards sampled from actual context arrivals between the rewards sampled from two specific processes that are related to the original process, where each process has a fixed mean value.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "This will make the Pareto regret \u00d5(T ) (which matches with the lower bound in [8] for the single-objective contextual bandit problem with similarity information up to a logaritmic factor) but will also make the regret in the non-dominant objective linear.", "startOffset": 78, "endOffset": 81}, {"referenceID": 29, "context": "Experiment 1 In this experiment, we evaluate the performance of MOCMAB on the breast cancer dataset from the UCI Machine Learning Repository [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 0, "context": "We take X = [0, 1] and assume that the context at each time 1 2 3 4 5 6 7 8 9 10 Test size 104 0.", "startOffset": 12, "endOffset": 18}, {"referenceID": 6, "context": "We compare MOC-MAB with the following bandit algorithms: Pareto UCB1 (P-UCB1): This is the Empirical Pareto UCB1 algorithm proposed in [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "Scalarized UCB1 (S-UCB1): This is the Scalarized Multiobjective UCB1 algorithm proposed in [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "Contextual Dominant UCB1 (CD-UCB1): This is the contextual version of UCB1 [16], which partitions the context space in the same way as MOC-MAB does, and uses a different instance of UCB1 in each set of the partition.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "For S-UCB1 and CS-UCB1, the weights of the linear scalarization functions are chosen as [1, 0], [0.", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": "5] and [0, 1].", "startOffset": 7, "endOffset": 13}, {"referenceID": 27, "context": "APPENDIX D CONCENTRATION INEQUALITY [28], [31] Consider an arm a for which the rewards of objective i are generated by a process {R a(t)}t=1 with \u03bca = E[R a(t)], where the noise R a(t) \u2212 \u03bca is conditionally 1-sub-Gaussian.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "APPENDIX D CONCENTRATION INEQUALITY [28], [31] Consider an arm a for which the rewards of objective i are generated by a process {R a(t)}t=1 with \u03bca = E[R a(t)], where the noise R a(t) \u2212 \u03bca is conditionally 1-sub-Gaussian.", "startOffset": 42, "endOffset": 46}], "year": 2017, "abstractText": "In this paper, we propose a new multi-objective contextual multi-armed bandit problem with two objectives, where one of the objectives dominates the other objective. Unlike single-objective bandit problems in which the learner obtains a random scalar reward for each arm it selects, in the proposed problem, the learner obtains a random reward vector, where each component of the reward vector corresponds to one of the objectives and the distribution of the reward depends on the context that is provided to the learner at the beginning of each round. We call this problem contextual multi-armed bandit with a dominant objective (CMAB-DO). In CMAB-DO, the goal of the learner is to maximize its total reward in the non-dominant objective while ensuring that it maximizes its total reward in the dominant objective. In this case, the optimal arm given a context is the one that maximizes the expected reward in the non-dominant objective among all arms that maximize the expected reward in the dominant objective. First, we show that the optimal arm lies in the Pareto front. Then, we propose the multi-objective contextual multi-armed bandit algorithm (MOCMAB), and define two performance measures: the 2-dimensional (2D) regret and the Pareto regret. We show that both the 2D regret and the Pareto regret of MOC-MAB are sublinear in the number of rounds. We also compare the performance of the proposed algorithm with other state-of-the-art methods in synthetic and real-world datasets. The proposed model and the algorithm have a wide range of real-world applications that involve multiple and possibly conflicting objectives ranging from wireless communication to medical diagnosis and recommender systems.", "creator": "LaTeX with hyperref package"}}}