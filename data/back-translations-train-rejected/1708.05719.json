{"id": "1708.05719", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Cross-Lingual Dependency Parsing for Closely Related Languages - Helsinki's Submission to VarDial 2017", "abstract": "This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, Slovak seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the fully supervised models trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the baseline are rather modest. Norwegian works best with information coming from Swedish whereas Danish contributes surprisingly little.", "histories": [["v1", "Fri, 18 Aug 2017 18:00:05 GMT  (52kb,D)", "http://arxiv.org/abs/1708.05719v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["j\\\"org tiedemann"], "accepted": false, "id": "1708.05719"}, "pdf": {"name": "1708.05719.pdf", "metadata": {"source": "CRF", "title": "Cross-Lingual Dependency Parsing for Closely Related Languages \u2013 Helsinki\u2019s Submission to VarDial 2017", "authors": ["J\u00f6rg Tiedemann"], "emails": ["first.lastname@helsinki.fi"], "sections": [{"heading": "1 Introduction", "text": "Various approaches have been proposed in the literature, which can mainly be divided into data transfer (i.e. annotation projection, e.g. (Hwa et al., 2005) and model transfer approaches (e.g. delexicalized models such as (McDonald et al., 2013)). In this paper, we will focus on data transfer using annotation projection and machine translation to translate source-language tree trunks used as training data for dependency savers in the target language. Our previous work has shown that these techniques are quite robust and perform better than simple transfer models based on delexicalized parsers (Tiedemann and Agic, 2016), especially in real-world test cases where parts of the language (PoS) are used instead of using them as gold standard annotation."}, {"heading": "2 Methodology and Data", "text": "Our translation is based on previous work and essentially uses models and techniques projected by (Hwa et al., 2005; Tiedemann, 2014; Dumdemann et al., 2014). We have used very different methods and parameters, which we have tested using the development data provided by VarDial 2017. All our results have therefore been closely measured against the data sets with predicted PoS labels. In particular, we have used three different cross-slingual models, which evaluate different types of different methods and parameters: annotation and projection via wordaligned parallel corporations provided by the workshop organisers."}, {"heading": "3 Results", "text": "We have looked at all the language pairs from the VarDial campaign and here we present the relevant results from our experiments. First, we must mention that we have created new baselines to have fair comparisons of the translingual models in terms of baseline approaches; the same table also summarizes our basic results for all language pairs, using the three approaches to data transfer introduced in the previous section; all projections are made in a collapsed dummy mode as described above; the first observation is that all translingual models exceed the delexications; this is, at least in terms of self-assurance and motivation for further developments, the translation and translation of the language; another observation is that Croatia is surprisingly hard to improve the comparison with the cross-lingual model, which is true."}, {"heading": "4 Conclusions", "text": "Our experiments show the use of annotation projection and tree bank translation techniques. The models perform well, especially for the Slovak language, which even exceeds the fully monitored \"upper boundary model.\" In this paper, we discussed the use of target language markers alongside the annotation projection, concluding that adding morphological information is almost always useful. We observe a large gap between LAS and UAS, which would require more in-depth investigation, one possible reason being the use of language-specific dependency designations that are not available from the projection. In fact, however, we doubt this explanation when considering the success of the winning team. LAS has not suffered as much in their results. Some surprising results could also be seen, for example the fact that Danish does not work as well as a source of Norwegian Swedish. This cannot be explained in terms of linguistic reasons, but has to indicate unexpected annotation differences or possibly a greater unequal weight of domes."}], "references": [{"title": "Top Accuracy and Fast Dependency Parsing is not a Contradiction", "author": ["Bernd Bohnet"], "venue": "In Proceedings of COLING,", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Clara Cabezas", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg"], "venue": "and Okan Kolak.", "citeRegEx": "Hwa et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust morphological tagging with word representations", "author": ["M\u00fcller", "Sch\u00fctze2015] Thomas M\u00fcller", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of NAACL", "citeRegEx": "M\u00fcller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2015}, {"title": "Natalia Silveira", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo"], "venue": "et al.", "citeRegEx": "Nivre et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "David Marec\u0306ek", "author": ["Rudolf Rosa", "Daniel Zeman"], "venue": "and Zden\u0115k Z\u0306abokrtsk\u00fd.", "citeRegEx": "Rosa et al.2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Jan Haji\u010d", "author": ["Milan Straka"], "venue": "and Strakov\u00e1.", "citeRegEx": "Straka et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "\u017deljko Agi\u0107", "author": ["J\u00f6rg Tiedemann"], "venue": "and Joakim Nivre.", "citeRegEx": "Tiedemann et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Rediscovering annotation projection for cross-lingual parser induction", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of COLING", "citeRegEx": "Tiedemann.,? \\Q2014\\E", "shortCiteRegEx": "Tiedemann.", "year": 2014}, {"title": "Crosslingual dependency parsing with universal dependencies and predicted PoS labels", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the Third International Conference on Dependency Linguistics (Depling", "citeRegEx": "Tiedemann.,? \\Q2015\\E", "shortCiteRegEx": "Tiedemann.", "year": 2015}, {"title": "Improving the cross-lingual projection of syntactic dependencies", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA", "citeRegEx": "Tiedemann.,? \\Q2015\\E", "shortCiteRegEx": "Tiedemann.", "year": 2015}, {"title": "2017", "author": ["Marcos Zampieri", "Shervin Malmasi", "Nikola Ljube\u0161i\u0107", "Preslav Nakov", "Ahmed Ali", "J\u00f6rg Tiedemann", "Yves Scherrer", "No\u00ebmi Aepli"], "venue": "Findings of the VarDial Evaluation Campaign", "citeRegEx": "Zampieri et al.2017", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [], "year": 2017, "abstractText": "This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, Slovak seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the fully supervised models trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the baseline are rather modest. Norwegian works best with information coming from Swedish whereas Danish contributes sur-", "creator": "LaTeX with hyperref package"}}}