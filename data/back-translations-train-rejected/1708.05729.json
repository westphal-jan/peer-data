{"id": "1708.05729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Neural machine translation for low-resource languages", "abstract": "Neural machine translation (NMT) approaches have improved the state of the art in many machine translation settings over the last couple of years, but they require large amounts of training data to produce sensible output. We demonstrate that NMT can be used for low-resource languages as well, by introducing more local dependencies and using word alignments to learn sentence reordering during translation. In addition to our novel model, we also present an empirical evaluation of low-resource phrase-based statistical machine translation (SMT) and NMT to investigate the lower limits of the respective technologies. We find that while SMT remains the best option for low-resource settings, our method can produce acceptable translations with only 70000 tokens of training data, a level where the baseline NMT system fails completely.", "histories": [["v1", "Fri, 18 Aug 2017 18:16:23 GMT  (16kb)", "http://arxiv.org/abs/1708.05729v1", "rejected from EMNLP 2017"]], "COMMENTS": "rejected from EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert \\\"ostling", "j\\\"org tiedemann"], "accepted": false, "id": "1708.05729"}, "pdf": {"name": "1708.05729.pdf", "metadata": {"source": "CRF", "title": "Neural machine translation for low-resource languages", "authors": ["Robert \u00d6stling"], "emails": ["robert@ling.su.se,", "jorg.tiedemann@helsinki.fi"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.05 729v 1 [cs.C L] 18 Aug 201 7proaches have improved the state of the art in many machine translation settings in recent years, but they require large amounts of training data to produce meaningful results. We show that NMT can also be used for languages with low resources by introducing more local dependencies and using word alignments to learn sentence orders during translation. In addition to our novel model, we also present an empirical evaluation of resource-poor phrase-based statistical machine translation (SMT) and NMT to examine the lower limits of the respective technologies. We note that while SMT remains the best option for settings with low resources, our method can produce acceptable translations with only 70,000 tokens of training data, a level at which the basic NMT system completely fails."}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT) has made rapid progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016) and has become a serious alternative to phrase-based statistical machine translation (Koehn et al., 2003). Most of the literature to date performs empirical evaluations using training data in the order of millions to tens of millions of parallel sentence pairs. In contrast, we want to see how low you can press the requirements for training data for the translation of neural machines.1 To our knowledge, there is no prior systematic treatment of the1The code of our implementation is available at http: / / www.example.com (redacted for verification, code filed as nmt.tgz in the review system). Zoph et al. (2016) has researched the resource-poor NMT, but assumes the existence of large amounts of data from related languages."}, {"heading": "2 Low-resource model", "text": "In fact, it is a matter of a way in which people move in the most diverse areas of life: from the world of work to the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the world of work, from the world of work to the people, from the environment to the people, from the environment to the people to the people, from the people to the people, from the environment to the people, from the environment to the people to the people, from the people to the people, from the people to the people to the people, from the people to the people, from the environment to the people to the people, from the people to the people, from the people to the people to the people, from the working environment to the people to the people, from the working environment to the people to the working world, from the working environment to the working environment to the working world, from the working world to the working world, from the working world to the working world to the working world, from the working world to the people, from the working environment to the people to the people, from the people to the people to the people, from the environment to the people to the people, from the environment to the people to the people to the people to the people, from the environment to the people to the environment, from the people to the people to the people to the environment, from the environment to the people to the people to the environment, from the people to the people to the environment, from the people to the environment to the people to the people to the environment, from the people to the people to the environment, from the people to the environment to the people to the people to the environment, from the environment to the people to the people to the environment, from the environment to the people to the environment to the people to the people to the environment, from the people to the people to the environment, from the environment to the people to"}, {"heading": "3 Training", "text": "Since our intended application is the translation of resource-poor languages, we rely on word alignments to ensure monitoring of the reordering model. We use the EFMARAL aligner (O \ufffd stling and Tiedemann, 2016), which uses a Bayesian model with priors that delivers good results even for rather small corpora. Hence, we obtain estimates of the alignment probabilities Pf (ai = j) and Pb (aj = i) in the forward and backward directions, or in the series of consistent word alignments (i, j). Our model requires a sequence of source marks and a sequence of targets of equal length. We extract this by first finding the most confident 1-to-1 word alignments, that is, the series of consistent pairs (i, j) with maximum extension (i, j) Pf (ai = j) \u00b7 Pb (aj = i). Then we use the source phrase as the same source point so that it has the final length of the school paragraph."}, {"heading": "4 Baseline systems", "text": "In addition to our proposed model, we use two public translation systems: Moses (Koehn et al., 2007) for phrase-based statistical machine translation (SMT) and HNMT3 for neural machine translation (NMT). For comparison, we use the same word alignment method (O \ufffd stling and Tiedemann, 2016) with Moses as with our proposed model (see Section 3). For the SMT system, we use 5-gram modified Kneser-Ney language models estimated with KenLM (Heafield et al., 2013). We symmetrize the word alignments with GROW-DIAG-FINAL heuristics. Otherwise, default settings for phrase extraction and estimation of translation probabilities and lexic weights are used. Parameters are matched with MERT (Och., 2003) with 200-best lists. The base model NMT-System, NHT-Code, with cognitive attention (cognitive translation) and Lugnition-based structures are used."}, {"heading": "5 Data", "text": "The most commonly translated publicly available parallel text is the Bible, which was previously used for multilingual NLPs (e.g. Jarowsky et al., 2001; Agic \u0301 et al., 2015). In addition, the journal Watchtower is also publicly available and is translated into a large number of languages. Although it generally contains less text than the Bible, Agic \u0301 et al. (2016) have shown that their more modern style and more consistent translations can outweigh this disadvantage for tasks outside the domain. Bible and Watchtower texts are quite similar, so we also rate data from the WMT domain (newstest2016 for Czech and German, newstest2008 for French and Spanish), which are four languages that occur in all three data sets, and we use them in all experiments with English as the target language. The Watchtower texts are the shortest, having removed 1000 random sentences each for development and testing."}, {"heading": "6 Results", "text": "Table 3 summarizes the results of our evaluation, and Table 2 shows some sample translations. We use the BLEU metric for the evaluation (Papineni et al., 2002). In summary, it is clear that SMT remains the best method for low-resource machine translation, but that current methods, given the parallel data available for low-resource languages, are unable to generate acceptable general machine translation systems. Our model manages to narrow the gap between phrase-based and neural machine translation. 4The reason we did not simply use the New Testament is that it consists largely of four redundant Gospels, making it difficult to use for machine translation evaluation. BLEU values of 9-17% (within the domain) were determined using only about 70,000 characters of training data, a state in which the traditional NMT system cannot produce any meaningful output at all."}, {"heading": "7 Discussion", "text": "We have identified a possible path to better neural machine translation for low-resource languages, where we cannot accept data beyond a small parallel text. In our assessment, we see that it exceeds a standard NMT baseline, but is currently no better than the SMT system. In the future, we hope to use the insights gained from this work to further explore the possibility of making NMT models perform better under extreme data sparseness. In particular, we would like to examine models that retain more of the characteristic fluidity of NMT while ensuring that appropriateness does not suffer too much when the data is sparse."}], "references": [{"title": "If all you have is a bit of the bible: Learning pos taggers for truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Dirk Hovy", "Anders S\u00f8gaard."], "venue": "Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Agi\u0107 et al\\.,? 2015", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2015}, {"title": "Multilingual projection for parsing truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "H\u00e9ctor Martnez Alonso", "Natalie Schluter", "Anders S\u00f8gaard."], "venue": "Transactions of the Association for Computational Linguistics 4:301\u2013", "citeRegEx": "Agi\u0107 et al\\.,? 2016", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scalable modified Kneser-Ney languagemodel estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of ACL. pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Vol-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010. pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of ACL. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Efficient word alignment with Markov Chain Monte Carlo", "author": ["Robert \u00d6stling", "J\u00f6rg Tiedemann."], "venue": "Prague Bulletin of Mathematical Linguistics 106:125\u2013146.", "citeRegEx": "\u00d6stling and Tiedemann.,? 2016", "shortCiteRegEx": "\u00d6stling and Tiedemann.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "INTERSPEECH 2012. pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton."], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Deniz Yuret", "Jonathan May", "Kevin Knight"], "venue": "Stroudsburg, PA, USA,", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation (NMT) has made rapid progress over the last few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016),", "startOffset": 81, "endOffset": 145}, {"referenceID": 2, "context": "Neural machine translation (NMT) has made rapid progress over the last few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016),", "startOffset": 81, "endOffset": 145}, {"referenceID": 5, "context": "emerging as a serious alternative to phrasebased statistical machine translation (Koehn et al., 2003).", "startOffset": 81, "endOffset": 101}, {"referenceID": 14, "context": "Zoph et al. (2016) did explore low-", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "The current de-facto standard approach to NMT (Bahdanau et al., 2014) has two components: a target-side RNN language model (Mikolov et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 7, "context": ", 2014) has two components: a target-side RNN language model (Mikolov et al., 2010; Sundermeyer et al., 2012), and an encoder plus attention mechanism that is used to condition on the source sentence.", "startOffset": 61, "endOffset": 109}, {"referenceID": 11, "context": ", 2014) has two components: a target-side RNN language model (Mikolov et al., 2010; Sundermeyer et al., 2012), and an encoder plus attention mechanism that is used to condition on the source sentence.", "startOffset": 61, "endOffset": 109}, {"referenceID": 6, "context": "Thus we use the same two-level source sentence encoding scheme as Luong and Manning (2016) except that we do not use separate embeddings for common words.", "startOffset": 66, "endOffset": 91}, {"referenceID": 9, "context": "We use the EFMARAL aligner (\u00d6stling and Tiedemann, 2016), which uses a", "startOffset": 27, "endOffset": 56}, {"referenceID": 13, "context": "For this we use the Chainer library (Tokui et al., 2015), with Adam (Kingma and Ba, 2015) for optimization.", "startOffset": 36, "endOffset": 56}, {"referenceID": 4, "context": ", 2015), with Adam (Kingma and Ba, 2015) for optimization.", "startOffset": 19, "endOffset": 40}, {"referenceID": 9, "context": "For comparability, we use the same word alignment method (\u00d6stling and Tiedemann, 2016) withMoses as with our proposed model (see Section 3).", "startOffset": 57, "endOffset": 86}, {"referenceID": 3, "context": "For the SMT system, we use 5-gram modified Kneser-Ney language models estimated with KenLM (Heafield et al., 2013).", "startOffset": 91, "endOffset": 114}, {"referenceID": 8, "context": "Parameters are tuned using MERT (Och, 2003) with 200-best lists.", "startOffset": 32, "endOffset": 43}, {"referenceID": 6, "context": "The baseline NMT system, HNMT, uses standard attention-based translation with the hybrid source encoder architecture of Luong and Manning (2016) and a characterbased decoder.", "startOffset": 120, "endOffset": 145}, {"referenceID": 0, "context": "allel text is the Bible, which has been used previously for multilingual NLP (e.g. Yarowsky et al., 2001; Agi\u0107 et al., 2015).", "startOffset": 77, "endOffset": 124}, {"referenceID": 0, "context": "Although generally containing less text than the Bible, Agi\u0107 et al. (2016) showed that its more modern style and consistent translations can outweigh this disadvantage for out-of-domain tasks.", "startOffset": 56, "endOffset": 75}, {"referenceID": 10, "context": "For evaluation we use the BLEU metric (Papineni et al., 2002).", "startOffset": 38, "endOffset": 61}], "year": 2017, "abstractText": "Neural machine translation (NMT) approaches have improved the state of the art in many machine translation settings over the last couple of years, but they require large amounts of training data to produce sensible output. We demonstrate that NMT can be used for low-resource languages as well, by introducing more local dependencies and using word alignments to learn sentence reordering during translation. In addition to our novel model, we also present an empirical evaluation of low-resource phrase-based statistical machine translation (SMT) and NMT to investigate the lower limits of the respective technologies. We find that while SMT remains the best option for low-resource settings, our method can produce acceptable translations with only 70 000 tokens of training data, a level where the baseline NMT system fails completely.", "creator": "LaTeX with hyperref package"}}}