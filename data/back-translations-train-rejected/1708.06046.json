{"id": "1708.06046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "nuts-flow/ml: data pre-processing for deep learning", "abstract": "Data preprocessing is a fundamental part of any machine learning application and frequently the most time-consuming aspect when developing a machine learning solution. Preprocessing for deep learning is characterized by pipelines that lazily load data and perform data transformation, augmentation, batching and logging. Many of these functions are common across applications but require different arrangements for training, testing or inference. Here we introduce a novel software framework named nuts-flow/ml that encapsulates common preprocessing operations as components, which can be flexibly arranged to rapidly construct efficient preprocessing pipelines for deep learning.", "histories": [["v1", "Mon, 21 Aug 2017 01:28:37 GMT  (60kb,D)", "http://arxiv.org/abs/1708.06046v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SE", "authors": ["s maetschke", "r tennakoon", "c vecchiola", "r garnavi"], "accepted": false, "id": "1708.06046"}, "pdf": {"name": "1708.06046.pdf", "metadata": {"source": "CRF", "title": "nuts-flow/ml : data pre-processing for deep learning", "authors": ["S. Maetschke", "R. Tennakoon", "C. Vecchiola", "R. Garnavi"], "emails": ["stefanrm@au1.ibm.com", "christian.vecchiola@au1.ibm.com", "rahilgar@au1.ibm.com", "ruwan.tennakoon@rmit.edu.au7"], "sections": [{"heading": "1 Introduction", "text": "The large number of parameters to be optimized during the training requires large amounts of data and hardware accelerators, such as graduate processes (GPUs) or dedicated hardware, such as the Tensor Processing Unit (TPU). Training is typically performed via the Stochastic Gradient Descent (SGD) or variations thereon. Network weights are based on small batches (mini-batches) of data sets. Deep Learning Frameworks [2] consist of up to three layers of software (see Figure 1)."}, {"heading": "Deep Learning APIs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Keras, TFLearn, Blocks&Fuel, ...", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Computational Graphs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Theano, Tensorflow", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Hardware Abstractions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CUDA, GpuArray", "text": "Data pre-processing for deep learning is particularly difficult due to the following features: 1) training data cannot be fully loaded into memory and must be processed sluggishly; 2) the training set is often enriched by random augmentation; 3) training is based on batches; and 4) some data pre-processing is done on the CPU, but training and conclusions are performed by the GPU. In the following sections, we first discuss the pre-processing capabilities of existing deep learning frameworks, then describe the basics of data pre-processing pipelines, and finally introduce the novel data pre-processing framework Nuts-flow / ml before concluding."}, {"heading": "2 Background", "text": "The existing learning frameworks focus on the definition and formation of artificial neural networks, but offer little or very limited support for data preprocessing. They are largely based on NumPy [19], Scikit [25], Pandas [16] and similar deep-learning agnostic libraries designed for in-memory processing and have limited support for lazy data flows as a prerequisite for deep learning. (In the following we will discuss Keras, TFLearn, Tensorflow, Caffe and Fuel in more details.Keras provides an ImageDataGenerator for standard augmentations such as rotation or flipping of images, but is not easily extendable to other augmentations, neither supports patch generation nor allows the application of the same random augmentation to two images at the same time, e.g. image and mask, which is a common requirement for segmentation tasks."}, {"heading": "3 Data processing pipelines", "text": "The main advantage and purpose of a processing pipeline compared to other programmatic constructs is the rotten evaluation of data. Only a subset of the total volume of data occupies memory at the same time, and data is processed when required, enabling efficient processing of data that is too large for a computer's main memory, including infinitely large volumes.Data processing pipelines, especially for deep learning, are characterized by a common sequence of steps that can be summarized in a canonical pipeline described in the next section."}, {"heading": "3.1 Canonical Pipeline", "text": "These common processing steps can be represented in a Canonical pipeline (see Figure 3). For convenience, the following more detailed descriptions focus on vision tasks and image data, but any deep-learning application with data too large to fit into memory such as video, audio, or large text documents benefits from similar functionality. The reader component of the pipeline reads sample data, such as paths to image files and class labels, stored in text files, pandas tables, or databases. The sample set is then divided by a splitter into training, validation, and test folds. As the training data cannot be fully loaded into memory, images or other blocks of data loaded by a loader are automatically loaded."}, {"heading": "3.2 Iterators and itertools", "text": "A common design pattern for implementing lazy evaluation is the iterator [14], which ensures that data is processed only on demand. Pipelines in nuts-flow / ml are implemented as chains of iterators. Here, we first discuss Python's iterator construct and its iterator library before using them specifically in the architecture of nuts-flow / ml.Iterators in the Python programming language (version 2.7) are classes that expose a next () 1 method. Each invocation of the next () returns the next element of the iterator. Thus, the following class implements an iterator that even generates numbers."}, {"heading": "4.1 nuts-flow", "text": "Before discussing the details of the nut flow, we would like to motivate its design and syntax selection by reimplementing the simple itertools pipeline shown above and comparing it with the corresponding implementation of the nut flow (imports are omitted): 1 # Itertools 2 List (islice (ifilter (lambda x: x > 5, xrange (10), 3))) 1 Python 3.x replaces the next () with _ _ next _ _ ().1 # Nut flow 2 range (10) > > Filters (_ > 5) > Take (3) > > Collect () Both implementations return exactly the same result: a list of numbers [6, 7, 8]."}, {"heading": "4.1.1 Architecture", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.2 nuts-ml", "text": "nuts-ml is based on nuts-flow, but adds nuts for image preprocessing and machine learning, while nuts-flow is an extension of Python's itertools library for implementing generic data flows. Figure 4 shows the entire architecture of the library."}, {"heading": "Python iterators", "text": "This year is the highest in the history of the country."}, {"heading": "5 Conclusion", "text": "Data pre-processing is an essential part of any deep learning application, but existing APIs or libraries for pre-processing are limited in functionality and often difficult to expand. Here, we have introduced a novel software framework nuts-flow / ml that includes common pre-processing functions as components that can be flexibly arranged to quickly build efficient data pre-processing pipelines. Both libraries are freely available under the Apache 2 licenses on github [20, 23].Future work will focus on extending the library to include pre-processing functions for data types other than images such as audio, video and text. We also intend to provide additional network wrappers for other deep learning frameworks beyond keras, with theano or tensor flow backend and lasagna that are already supported. 3 That this is an image file that is not expensive but does not contain any compatible samples."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "author": ["M. Abadi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning", "author": ["S. Bahrampour", "N. Ramakrishnan", "L. Schott", "M. Shah"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Torch7: A Matlab-like Environment for Machine Learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "http://publications.idiap.ch/downloads/papers/2011/Collobert_", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) (ICCV \u201915)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia (MM \u201914)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "In-Datacenter Performance Analysis of a Tensor Processing Unit", "author": ["N.P. Jouppi"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Mastering Python Design Patterns", "author": ["S. Kasampalis"], "venue": "Packt Publishing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proceedings of the 25th International Conference on Neural Information Processing Systems (NIPS\u201912),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "pandas: a Foundational Python Library for Data Analysis and Statistics", "author": ["W. McKinney"], "venue": "PyHPC", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["B. Merri\u00ebnboer"], "venue": "https://arxiv.org/", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["F. Pedregosa"], "venue": "Journal of Machine Learning Research. no", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "An overview of gradient descent optimization algorithms", "author": ["S. Ruder"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "TF.Learn: TensorFlow\u2019s High-level Module for Distributed Machine Learning", "author": ["Y. Tang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Apache Spark: a unified engine for big data processing", "author": ["M Zaharia et. al"], "venue": "Commun. ACM 59,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Human level performance and beyond have been achieved in many vision and other machine learning applications due to deep learning [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 7, "context": "Deep learning describes a class of artificial neural networks characterized by many layers and weights [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "The large number of parameters to be optimized during training requires large amounts of data and hardware accelerators such as Graphical Processing Units (GPUs) or dedicated hardware such as the Tensor Processing Unit (TPU) [13] for tensor computations.", "startOffset": 225, "endOffset": 229}, {"referenceID": 11, "context": "Training is typically performed via Stochastic Gradient Descent (SGD) or variants thereof [27] which adjust network weights based on small batches (mini-batches) of data.", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "Deep learning frameworks [2] are composed of up to three software layers (see Figure 1).", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "Theano [29] and Tensorflow [1] are examples of such libraries or backends.", "startOffset": 27, "endOffset": 30}, {"referenceID": 12, "context": "For instance, Keras [4], TFLearn [28], Blocks&Fuel [17] and Caffe [12] are libraries with Python APIs used for this purpose.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "For instance, Keras [4], TFLearn [28], Blocks&Fuel [17] and Caffe [12] are libraries with Python APIs used for this purpose.", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "For instance, Keras [4], TFLearn [28], Blocks&Fuel [17] and Caffe [12] are libraries with Python APIs used for this purpose.", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "Torch [5] based on Lua, Mocha [18] based on Julia, and Deeplearing4J [8] based on Java are common non-Python alternatives.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "They largely rely on NumPy [19], Scikit [25], pandas [16] and similar deep-learning-agnostic libraries, which are designed for in-memory processing and have limited support for lazy data flows as required for deep learning.", "startOffset": 40, "endOffset": 44}, {"referenceID": 8, "context": "They largely rely on NumPy [19], Scikit [25], pandas [16] and similar deep-learning-agnostic libraries, which are designed for in-memory processing and have limited support for lazy data flows as required for deep learning.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "The flow takes a list of feature values [1, 2, 3, 4] and a list of target values [-1, 1, -1, 1], constructs a dataset (IndexableDataset), multiplies all target values by 2 (Doubler) and returns an iterator (DataStream) over batches of size 2 (SequentialScheme): 1 >>> class Doubler(AgnosticSourcewiseTransformer): 2 .", "startOffset": 40, "endOffset": 52}, {"referenceID": 1, "context": "The flow takes a list of feature values [1, 2, 3, 4] and a list of target values [-1, 1, -1, 1], constructs a dataset (IndexableDataset), multiplies all target values by 2 (Doubler) and returns an iterator (DataStream) over batches of size 2 (SequentialScheme): 1 >>> class Doubler(AgnosticSourcewiseTransformer): 2 .", "startOffset": 40, "endOffset": 52}, {"referenceID": 0, "context": "The flow takes a list of feature values [1, 2, 3, 4] and a list of target values [-1, 1, -1, 1], constructs a dataset (IndexableDataset), multiplies all target values by 2 (Doubler) and returns an iterator (DataStream) over batches of size 2 (SequentialScheme): 1 >>> class Doubler(AgnosticSourcewiseTransformer): 2 .", "startOffset": 81, "endOffset": 95}, {"referenceID": 0, "context": "The flow takes a list of feature values [1, 2, 3, 4] and a list of target values [-1, 1, -1, 1], constructs a dataset (IndexableDataset), multiplies all target values by 2 (Doubler) and returns an iterator (DataStream) over batches of size 2 (SequentialScheme): 1 >>> class Doubler(AgnosticSourcewiseTransformer): 2 .", "startOffset": 81, "endOffset": 95}, {"referenceID": 0, "context": "array([1, 2, 3, 4])), 13 .", "startOffset": 6, "endOffset": 18}, {"referenceID": 1, "context": "array([1, 2, 3, 4])), 13 .", "startOffset": 6, "endOffset": 18}, {"referenceID": 0, "context": "array([-1, 1, -1, 1]))])) 14 15 >>> batch_scheme = SequentialScheme( 16 .", "startOffset": 6, "endOffset": 20}, {"referenceID": 0, "context": "array([-1, 1, -1, 1]))])) 14 15 >>> batch_scheme = SequentialScheme( 16 .", "startOffset": 6, "endOffset": 20}, {"referenceID": 0, "context": "get_epoch_iterator()] 25 [(array([1, 2]), array([-2, 2])), (array([3, 4]), array([-2, 2]))]", "startOffset": 33, "endOffset": 39}, {"referenceID": 1, "context": "get_epoch_iterator()] 25 [(array([1, 2]), array([-2, 2])), (array([3, 4]), array([-2, 2]))]", "startOffset": 33, "endOffset": 39}, {"referenceID": 1, "context": "get_epoch_iterator()] 25 [(array([1, 2]), array([-2, 2])), (array([3, 4]), array([-2, 2]))]", "startOffset": 48, "endOffset": 55}, {"referenceID": 1, "context": "get_epoch_iterator()] 25 [(array([1, 2]), array([-2, 2])), (array([3, 4]), array([-2, 2]))]", "startOffset": 81, "endOffset": 88}, {"referenceID": 0, "context": "The same pipeline can be realized in three lines (again omitting imports) using nuts-flow/ml: 1 >>> dataset = [1, 2, 3, 4] >> Zip([-1, 1, -1, 1]) 2 >>> build_batch = BuildBatch(2).", "startOffset": 110, "endOffset": 122}, {"referenceID": 1, "context": "The same pipeline can be realized in three lines (again omitting imports) using nuts-flow/ml: 1 >>> dataset = [1, 2, 3, 4] >> Zip([-1, 1, -1, 1]) 2 >>> build_batch = BuildBatch(2).", "startOffset": 110, "endOffset": 122}, {"referenceID": 0, "context": "The same pipeline can be realized in three lines (again omitting imports) using nuts-flow/ml: 1 >>> dataset = [1, 2, 3, 4] >> Zip([-1, 1, -1, 1]) 2 >>> build_batch = BuildBatch(2).", "startOffset": 130, "endOffset": 144}, {"referenceID": 0, "context": "The same pipeline can be realized in three lines (again omitting imports) using nuts-flow/ml: 1 >>> dataset = [1, 2, 3, 4] >> Zip([-1, 1, -1, 1]) 2 >>> build_batch = BuildBatch(2).", "startOffset": 130, "endOffset": 144}, {"referenceID": 0, "context": "by(1, \u2019number\u2019, int) 3 >>> dataset >> MapCol(1, _ * 2) >> build_batch >> Collect() 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 93, "endOffset": 99}, {"referenceID": 1, "context": "by(1, \u2019number\u2019, int) 3 >>> dataset >> MapCol(1, _ * 2) >> build_batch >> Collect() 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 93, "endOffset": 99}, {"referenceID": 1, "context": "by(1, \u2019number\u2019, int) 3 >>> dataset >> MapCol(1, _ * 2) >> build_batch >> Collect() 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "by(1, \u2019number\u2019, int) 3 >>> dataset >> MapCol(1, _ * 2) >> build_batch >> Collect() 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 141, "endOffset": 148}, {"referenceID": 0, "context": "array([[1, 2, 3, 4], [-1, 1, -1, 1]]) 2 >>> dataset[1,:] *= 2 3 >>> [list(b) for b in np.", "startOffset": 7, "endOffset": 19}, {"referenceID": 1, "context": "array([[1, 2, 3, 4], [-1, 1, -1, 1]]) 2 >>> dataset[1,:] *= 2 3 >>> [list(b) for b in np.", "startOffset": 7, "endOffset": 19}, {"referenceID": 0, "context": "array([[1, 2, 3, 4], [-1, 1, -1, 1]]) 2 >>> dataset[1,:] *= 2 3 >>> [list(b) for b in np.", "startOffset": 21, "endOffset": 35}, {"referenceID": 0, "context": "array([[1, 2, 3, 4], [-1, 1, -1, 1]]) 2 >>> dataset[1,:] *= 2 3 >>> [list(b) for b in np.", "startOffset": 21, "endOffset": 35}, {"referenceID": 0, "context": "split(dataset, 2, 1)] 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 32, "endOffset": 38}, {"referenceID": 1, "context": "split(dataset, 2, 1)] 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 32, "endOffset": 38}, {"referenceID": 1, "context": "split(dataset, 2, 1)] 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 47, "endOffset": 54}, {"referenceID": 1, "context": "split(dataset, 2, 1)] 4 [[array([1, 2]), array([-2, 2])], [array([3, 4]), array([-2, 2])]]", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "Large scale, cluster computing frameworks such as Apache Spark [30] or Dask [7] also support lazy evaluation and have a concept of data flows.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "2 Iterators and itertools A common design pattern to implement lazy evaluation is the Iterator [14], which ensures that data is processed on demand only.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "This custom nut could then be called in a data flow as follows, where Collect() collects the outputs of the nut in a list: 1 >>> [1, 2, 3] >> MultiplyBy(2) >> Collect() 2 [2, 4, 6]", "startOffset": 129, "endOffset": 138}, {"referenceID": 1, "context": "This custom nut could then be called in a data flow as follows, where Collect() collects the outputs of the nut in a list: 1 >>> [1, 2, 3] >> MultiplyBy(2) >> Collect() 2 [2, 4, 6]", "startOffset": 129, "endOffset": 138}, {"referenceID": 1, "context": "This custom nut could then be called in a data flow as follows, where Collect() collects the outputs of the nut in a list: 1 >>> [1, 2, 3] >> MultiplyBy(2) >> Collect() 2 [2, 4, 6]", "startOffset": 171, "endOffset": 180}, {"referenceID": 0, "context": "__rshift__([1, 2, 3]))", "startOffset": 11, "endOffset": 20}, {"referenceID": 1, "context": "__rshift__([1, 2, 3]))", "startOffset": 11, "endOffset": 20}, {"referenceID": 1, "context": "For instance, the following three examples are alternative implementations of the flow described above and demonstrate seamless integration with Python: 1 >>> xrange(1, 4) >> Map(_ * 2) >> Collect() 2 [2, 4, 6]", "startOffset": 201, "endOffset": 210}, {"referenceID": 1, "context": "1 >>> list(xrange(1, 4) >> MultiplyBy(2)) 2 [2, 4, 6]", "startOffset": 44, "endOffset": 53}, {"referenceID": 0, "context": "1 >>> [MultiplyBy(2)(x) for x in [1, 2, 3]] 2 [2, 4, 6]", "startOffset": 33, "endOffset": 42}, {"referenceID": 1, "context": "1 >>> [MultiplyBy(2)(x) for x in [1, 2, 3]] 2 [2, 4, 6]", "startOffset": 33, "endOffset": 42}, {"referenceID": 1, "context": "1 >>> [MultiplyBy(2)(x) for x in [1, 2, 3]] 2 [2, 4, 6]", "startOffset": 46, "endOffset": 55}], "year": 2017, "abstractText": "Data preprocessing is a fundamental part of any machine learning application and frequently the most time-consuming aspect when developing a machine learning solution. Preprocessing for deep learning is characterized by pipelines that lazily load data and perform data transformation, augmentation, batching and logging. Many of these functions are common across applications but require different arrangements for training, testing or inference. Here we introduce a novel software framework named nuts-flow/ml that encapsulates common preprocessing operations as components, which can be flexibly arranged to rapidly construct efficient preprocessing pipelines for deep learning.", "creator": "LaTeX with hyperref package"}}}