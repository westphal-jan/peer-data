{"id": "1708.06073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "The Microsoft 2017 Conversational Speech Recognition System", "abstract": "We describe the 2017 version of Microsoft's conversational speech recognition system, in which we update our 2016 system with recent developments in neural-network-based acoustic and language modeling to further advance the state of the art on the Switchboard speech recognition task. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby subsets of acoustic models are first combined at the senone/frame level, followed by a word-level voting via confusion networks. We also added a confusion network rescoring step after system combination. The resulting system yields a 5.1\\% word error rate on the 2000 Switchboard evaluation set.", "histories": [["v1", "Mon, 21 Aug 2017 03:17:23 GMT  (49kb,D)", "http://arxiv.org/abs/1708.06073v1", null], ["v2", "Thu, 24 Aug 2017 23:30:37 GMT  (50kb,D)", "http://arxiv.org/abs/1708.06073v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["w xiong", "l wu", "f alleva", "j droppo", "x huang", "a stolcke"], "accepted": false, "id": "1708.06073"}, "pdf": {"name": "1708.06073.pdf", "metadata": {"source": "CRF", "title": "THE MICROSOFT 2017 CONVERSATIONAL SPEECH RECOGNITION SYSTEM", "authors": ["W. Xiong", "L. Wu", "F. Alleva", "J. Droppo", "X. Huang", "A. Stolcke"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2.1. Convolutional Neural Nets", "text": "We used two types of CNN model architectures: ResNet and LACE (VGG, a third architecture used in our previous system was dropped).The residual network architecture [28] is a standard CNN with additional highway connections [29], i.e., a linear transformation of the inputs of each layer to the output of the layer [29, 30].We apply batch normalization [31] before activating linear units (ReLU).The LACE model (layer-wise context expansion with attention) is a modified CNN architecture [32].LACE, first proposed in [32] and presented in Figure 1, is a variant of the time-delayed neural network (TDNN) [4], in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layers."}, {"heading": "2.2. Bidirectional LSTM", "text": "For our LSTM-based acoustic models, we use a bi-directional architecture (BLSTM) [34] without frame skipping [11]. The core model structure is the LSTM defined in [10]. We found that using networks with more than six layers did not improve the error rate of the word in the development group and chose 512 hidden units per direction and per layer; this allowed a reasonable compromise between training time and final model accuracy. BLSTM performance was significantly enhanced by a spatial smoothing technique described for the first time in [23]. In short, each layer is imposed a two-dimensional topology, and activation patterns where adjacent units are correlated are rewarded."}, {"heading": "2.3. CNN-BLSTM", "text": "New this year is a CNN-BLSTM model inspired by [35]. Unlike the original BLSTM model, we included the context of each point in time as an input marker in the model. Context windows were [\u2212 3, 3], so the input marker has a size of 40x7xt, with zero padding in the frequency dimension, but not in the time dimension. We first apply three sinuous layers to the characteristics at the time t and then apply six BLSTM layers to the resulting time sequence, similar to the structure of our pure BLSTM model. Table 1 compares the layer structure and parameters of the two pure CNN architectures as well as CNN-BLSTM."}, {"heading": "2.4. Senone Set Diversity", "text": "A standard element of modern ASR systems is the combination of several acoustic models. Assuming that these models are diverse, i.e. make mistakes that are not perfectly correlated, a mean or tuning combination of these models should reduce errors. In the past, we mainly relied on different model architectures to produce different acoustic models. However, the results [23] for several BLSTM models showed that diversity can also be achieved by different sets of senons (clustered subphonetic units). Therefore, we have now adopted a variety of senone sets for all model architectures. Senone sets differ in clusters of details (9k versus 27k senones), as well as two slightly different telephone sets and corresponding dictionaries. The standard version is based on the CMU dictionary and the telephone set (no stress, but with a black phone). An alternating dictionary adds special vowels and nasal phones, which are exclusively used for pauses and feedback [36]."}, {"heading": "2.5. Speaker Adaptation", "text": "The adaptive modeling of loudspeakers in our system is based on conditioning the network on an i-vector [37] characterization of each loudspeaker [38, 39]. For each talk side (channel A or B of the audio file, i.e. the entire language coming from the same loudspeaker), a 100-dimensional i-vector is generated. In the BLSTM systems, the i-vector vs is appended to each input frame. In the case of Convolutionary Networks, this approach is inappropriate, since we do not expect to see spatially interconnected patterns in the input area. Instead, in the CNNs, we add a learnable weight matrix W l to each layer and add W lvs to activate the layer before non-linearity. Thus, the i-vector at CNN essentially serves as a loudspeaker-dependent bias to each layer. To show the effectiveness of i-vector adjustment to our models, see [40]."}, {"heading": "2.6. Sequence training", "text": "All our models are sequence trained, using maximum mutual information (MMI) as a discriminating objective function. Based on the approaches of [41] and [42], the denominator graph is a complete trigram LM over telephones and senons. Forward-backward calculations are performed as matrix operations and can therefore be efficiently executed on GPUs without the need for a grid approximation of the search space. For details of our implementation and empirical evaluation compared to models trained in entropy, see [40]."}, {"heading": "2.7. Frame-level model combination", "text": "Such a combination of neural acoustic models is effectively just another, albeit more complex, neural model, and the combination of frame-level models is limited by the fact that the underlying sets of senons must be identical. Table 2 shows the error rates achieved by different sets of senons, model architectures, and frame combinations of multiple architectures, with results based on N-gram language models, and all combinations weighted evenly. 3. LANGUAGE MODELS"}, {"heading": "3.1. Vocabulary size", "text": "In the past, we used a relatively small vocabulary of 30,500 words derived only from domain training data (PBX and Fisher corpus). Although this results in an OOV rate well below 1%, our error rates have reached a level where even small absolute reductions in OOVs could potentially have a significant impact on overall accuracy. We added the most common words in the OOV vocabulary in the domain to the non-domain sources that are also used for language model training: the LDC Broadcast News corpus and the UW Conversational Web corpus. Increasing vocabulary size to 165k reduced the OV rate (excluding word fragments) in the corresponding development in 2002 from 0.29% to 0.06%. The Devset error rate (using the 9k senons BLSTM + ResNet + LACE acoustic models, see Table 2) dropped from 9.78% to 9.78%."}, {"heading": "3.2. LSTM-LM rescoring", "text": "In fact, most of them are able to play by the rules that they have established in recent years, and they are able to play by the rules."}, {"heading": "3.3. Dialog session-based modeling", "text": "It is not only a question of the expression, but also a question of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of"}, {"heading": "5.1. Confusion network combination", "text": "After re-ordering all system outputs with all language models, we combine all results logically-linearly and normalize ourselves to estimate the probabilities at the rear level of enunciation. All N-best outputs for the same enunciation are then linked to the tool SRILM nbestrover [50, 36] and merged into a single word confusion network (CN)."}, {"heading": "5.2. System Selection", "text": "Unlike our previous system, we do not apply the probabilities estimated from the N-best hypotheses at the system level. All systems have the same weight in the combination. This simplification allows us to perform a brute force search across all possible subsets of systems, selecting those that have the least word error on the development list. We started with 9 of our best individual systems, eliminated two, and left a combination of 7 systems."}, {"heading": "5.3. Confusion network rescoring and backchannel modeling", "text": "As a final processing step, we generate new N-best lists from the confusion networks resulting from the system combination, which are then rescored once again with the N-gram LM, a subset of the reactionary level LSTM-LMs, and an additional source of knowledge. [24] We found that the large machine-specific error pattern is a misrecognition of filled pauses (\"uh\" to \"um\"), as the additional source of knowledge at this stage was motivated by our analysis of the differences between machine and human transcription errors. [24] We found that the large machine-specific error pattern of filled pauses (\"um\" to \"um\") as back-channel acknowledgements (\"uh-huh\" to have the system correct this problem, we offer the number of back-channel tokens in a hypotheses."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We describe the 2017 version of Microsoft\u2019s conversational<lb>speech recognition system, in which we update our 2016<lb>system with recent developments in neural-network-based<lb>acoustic and language modeling to further advance the state<lb>of the art on the Switchboard speech recognition task. The<lb>system adds a CNN-BLSTM acoustic model to the set of<lb>model architectures we combined previously, and includes<lb>character-based and dialog session aware LSTM language<lb>models in rescoring. For system combination we adopt a two-<lb>stage approach, whereby subsets of acoustic models are first<lb>combined at the senone/frame level, followed by a word-level<lb>voting via confusion networks. We also added a confusion<lb>network rescoring step after system combination. The result-<lb>ing system yields a 5.1% word error rate on the 2000 Switch-<lb>board evaluation set.", "creator": "LaTeX with hyperref package"}}}