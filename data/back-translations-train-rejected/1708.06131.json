{"id": "1708.06131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Evasion Attacks against Machine Learning at Test Time", "abstract": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.", "histories": [["v1", "Mon, 21 Aug 2017 09:55:37 GMT  (177kb,D)", "http://arxiv.org/abs/1708.06131v1", "In this paper, in 2013, we were the first to introduce the notion of evasion attacks (adversarial examples) created with high confidence (instead of minimum-distance misclassifications), and the notion of surrogate learners (substitute models). These two concepts are now widely re-used in developing attacks against deep networks (even if not always referring to the ideas reported in this work). arXiv admin note: text overlap witharXiv:1401.7727"]], "COMMENTS": "In this paper, in 2013, we were the first to introduce the notion of evasion attacks (adversarial examples) created with high confidence (instead of minimum-distance misclassifications), and the notion of surrogate learners (substitute models). These two concepts are now widely re-used in developing attacks against deep networks (even if not always referring to the ideas reported in this work). arXiv admin note: text overlap witharXiv:1401.7727", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["battista biggio", "igino corona", "davide maiorca", "blaine nelson", "nedim srndic", "pavel laskov", "giorgio giacinto", "fabio roli"], "accepted": false, "id": "1708.06131"}, "pdf": {"name": "1708.06131.pdf", "metadata": {"source": "CRF", "title": "Evasion attacks against machine learning at test time", "authors": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "emails": ["roli}@diee.unica.it,", "bnelson@cs.uni-potsdam.de", "pavel.laskov}@uni-tuebingen.de"], "sections": [{"heading": null, "text": "Keywords: Adversarial machine learning, evasive attacks, support vector machines, neural networks"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Optimal evasion at test time", "text": "We consider a classification algorithm f: X 7 \u2192 Y that assigns x-X to a name from the set of predefined classes y-Y = {\u2212 1, + 1}, where \u2212 1 (+ 1) represents the legitimate (malicious) class. Classifier f is sampled from an underlying distribution p (X, Y) using a dataset D = {xi, yi} ni = 1. Typically, the designation yc = f (x) given by a classifier is obtained by the threshold of a continuous discrimination function g: X 7 \u2192 R. Subsequently, we use yc to refer to the designation assigned by the classifier as opposed to the true designation y. We also assume that f (x) = \u2212 1 if g (x) < 0 and + 1 otherwise."}, {"heading": "2.1 Adversary model", "text": "In order to achieve this, it is necessary to play by the rules that you play by and to play by the rules that you play by."}, {"heading": "2.2 Attack scenarios", "text": "Subsequently, we will consider two attack scenarios characterized by different levels of the enemy's knowledge of the attacked system. Perfect knowledge (PK) In this context, we assume that the enemy's goal is to minimize g (x), and that it has complete knowledge of the attacked classifier; i.e., the opponent knows the attribute space, the type of classifier, and the trained model. The opponent can transform attack points in the test data, but must remain within a maximum distance of dmax from the original attack sample. We will use dmax as a parameter in our evaluation to simulate increasingly pessimistic attack scenarios, giving the opponent greater freedom to change the data."}, {"heading": "2.3 Attack strategy", "text": "Under the above assumptions, where the attack may or may not have been carried out according to the desired behaviour, an optimal attack strategy will find a sample x * to minimise g (\u00b7) or its estimate g (\u00b7). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 ffRead more about this in the Monday issue (1 April) of the Passauer Neue Presse (issue Pocking / Bad F\u00fcssing / Bad Griesbach). \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. in the Passauer Neue Presse (issue Pocking / Bad F\u00fcssing / Bad Griesbach). \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff. \u2212 ff."}, {"heading": "3 Gradient descent attacks", "text": "Algorithm 1 solves the optimization problem in Equation 2 by gradient descent. We assume that g (x) is differentiable almost everywhere (subgradients can be used for discontinuities). Note, however, that if g is not differentiable or insufficiently smooth, the term mimicry / KDE from Equation (2) can still be used as a search heuristics. This investigation will be left to future tasks.7 Mimicry attacks [11] consist of disguising malicious network packets to evade anomaly-based intrusion detection systems by mimicking the features of legitimate traffic distribution."}, {"heading": "3.1 Gradients of discriminant functions", "text": "Linear classifiers. Linear discrimination functions are g (x) = < w, x > + b, where w \u00b2 Rd is the property weights and b \u00b2 R is the bias. Their course is reversed g (x) = w. Support vector machines. In this case, the feasibility of our approach depends on whether the gradient k (x, xi) is compatible with many numerical cores. In this case, the feasibility of our approach depends on whether the grain gradient k (x, xi) is compatible with many numerical cores. For example, the gradient of the RBF core k (x, xi) = exp {\u2212 xi \u00b2, the activation x \u2212 xi \u00b2}, is k (x, xi) = k \u00b2, the function (1 x, p \u00b2), exp (x \u2212 xi \u2212 vural \u00b2), and for the polynomial k (x), xi k = a hidden k (k =, k = 1, x =, p =, p = 1, p =, p, p = 1, p =, p."}, {"heading": "3.2 Gradients of kernel density estimators", "text": "Similar to SVMs, the gradient of the kernel density estimator depends on the kernel gradient. We will consider generalized RBF cores of the form k (x \u2212 xi h) = exp (\u2212 d (x, xi) h), where d (\u00b7, \u00b7) is an arbitrary suitable distance function. In this case, we will use the same distance d (\u00b7, \u00b7), which is defined in Equation (3) but can generally be different. For \"2- and\" 1-standards (i.e. RBF and laplac cores), the KDE (sub) gradients are each given by: \u2212 2nh = i | yci = \u2212 1 exp (\u2212 xi \u00b2 2 h) (x \u2212 xi), \u2212 1nh = i | yci = \u2212 1 exp (\u2212 x \u2212 xi). Note that the scaling factor here is proportional to O (1nh)."}, {"heading": "3.3 Descent in discrete spaces", "text": "In such cases, we must find a viable neighbor x that reduces F (x) to the maximum. A simple approach to this problem is to probe F at any point in a small neighborhood of x, which would, however, require a large number of queries. In classifiers with a differentiable decision function, we can instead select the neighbor whose change best matches F (x) and reduces the objective function, i.e. to avoid overshooting to a minimum."}, {"heading": "4 Experiments", "text": "In this section, we will first report on a toy from the MNIST task on the handwritten classification of digits [18] to vividly demonstrate how the proposed algorithm modifies digits to mislead the classification, and then show the effectiveness of the proposed attack on a more realistic and practicable scenario: the detection of malware in PDF files."}, {"heading": "4.1 A toy example on handwritten digits", "text": "Similar to Globerson and Roweis [12], we consider the distinction between two different digits from the MNIST dataset = 25x. Each digit example is represented as a grayscale image of 28 x 28 pixels arranged in raster scan order to give characteristic vectors of d = 28 x 28 = 784 values. We normalize each characteristic (pixels) x [0, 1] d by dividing its value by 255, and we limit the attack samples to this range. Accordingly, we optimize the equivalent. (2) Item 0 \u2264 xf \u2264 1 for all f. We consider only the perfect knowledge (PK) attack scenario. We use the Manhattan distance ('1-norm), d, both for the kernel density estimator (i.e., a Laplacian kernel attack) and for limiting the attack."}, {"heading": "4.2 Malware detection in PDF files", "text": "We focus on the task of distinguishing between legitimate and malicious PDFs for attacks, although it is difficult to remove this file. (PDFs) We focus on the task of distinguishing between legitimate and malicious PDFs, a popular medium for distributing malware (26). PDFs are excellent vectors for malicious code, due to their flexible logical structure, which can be described by a hierarchy of interconnected objects. As a result, an attack can easily be hidden in a PDFs to bypass file type filtering. The PDFs format also allows a variety of resources to be embedded in the document, including JavaScript, Flash, and even binary programs. The nature of the embedded object is specified by keywords, and its content is contained in a data stream. Multiple Recentworks suggest machine-learning techniques for detecting malicious PDFs, which pre-identify the individual malware characteristics of each file, which we use to identify the logical structure of each of the 20.orca file]."}, {"heading": "5 Conclusions, limitations and future work", "text": "In this paper, we proposed a simple algorithm for circumventing classifiers with differentiable discrimination functions, which are not always possible. We investigated the attack effectiveness in case of perfect and limited knowledge of the attacked system and showed empirically that very popular classification algorithms (especially SVMs and neural networks) can still be circumvented with high probability, even if the opponent can only learn a copy of the classifier from a small surrogate dataset. Thus, our study raises important questions whether such algorithms can be reliably used in safety-sensitive areas of application. We believe that the proposed attack formulation can be extended to classifiers with undifferentiated discriminatory functions, such as decision trees and the closest neighbors; e.g. by defining suitable search heuristics similar to our miwar concepts to minimize g (x)."}], "references": [{"title": "Can machine learning be secure", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "ASIACCS \u201906: Proc. of the 2006 ACM Symp. on Information, computer and comm. security. pp. 16\u201325", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Int\u2019l J. of Machine Learning and Cybernetics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Design of robust classifiers for adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics (SMC). pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Trans. on Knowl. and Data Eng. 99(PrePrints),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "J. (eds.) 29th Int\u2019l Conf. on Mach. Learn", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "In: Knowl. Disc. and D. Mining (KDD). pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "J. Mach. Learn. Res", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "ACM SIGKDD Int\u2019l Conf. on Knowl. Discovery and Data Mining (KDD). pp", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Learning to classify with missing and corrupted features", "author": ["O. Dekel", "O. Shamir", "L. Xiao"], "venue": "Mach. Learn", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "Proc. 15th Conf. on USENIX Sec. Symp. USENIX", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Proc. of the 23rd Int\u2019l Conf. on Mach. Learn", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Discriminative direction for kernel classifiers", "author": ["P. Golland"], "venue": "Neu. Inf. Proc. Syst. (NIPS). pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "ACM Workshop on Art. Int. and Sec. (AISec", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "Proc. of the 13th Int\u2019l Conf. on Art. Int. and Stats. (AISTATS). pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "Sixth Conf. on Email and Anti-Spam (CEAS)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A framework for quantitative security analysis of machine learning", "author": ["P. Laskov", "M. Kloft"], "venue": "AISec \u201909: Proc. of the 2nd ACM works. on Sec. and art. int. pp. 1\u20134. ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. M\u00fcller", "E. S\u00e4ckinger", "P. Simard", "V. Vapnik"], "venue": "In: Int\u2019l Conf. on Art. Neu. Net. pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proc. of the Eleventh ACM SIGKDD Int\u2019l Conf. on Knowl. Disc. and D. Mining (KDD)", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "A pattern recognition system for malicious pdf files detection", "author": ["D. Maiorca", "G. Giacinto", "I. Corona"], "venue": "In: MLDM. pp", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Exploiting machine learning to subvert your spam", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "filter. In: LEET\u201908: Proc. of the 1st Usenix Work. on L.-S. Exp. and Emerg. Threats", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J.D. Tygar"], "venue": "J. Mach. Learn. Res", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "author": ["J. Platt"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Malicious pdf detection using metadata and structural features", "author": ["C. Smutz", "A. Stavrou"], "venue": "Proc. of the 28th Annual Comp. Sec. App. Conf.. pp", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Detection of malicious pdf files based on hierarchical document structure", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "Proc. 20th Annual Net. & Dist. Sys. Sec. Symp", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "IBM X-force mid-year trend & risk report", "author": ["R. Young"], "venue": "Tech. rep.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 3, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 7, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 9, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 12, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 13, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 14, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 17, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 19, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 1, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 7, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 14, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 17, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 3, "context": ", the performance degradation caused by carefully crafted attacks [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "To better understand the security properties of machine learning systems in adversarial settings, paradigms from security engineering and cryptography have been adapted to the machine learning field [2,5,14].", "startOffset": 199, "endOffset": 207}, {"referenceID": 3, "context": "To better understand the security properties of machine learning systems in adversarial settings, paradigms from security engineering and cryptography have been adapted to the machine learning field [2,5,14].", "startOffset": 199, "endOffset": 207}, {"referenceID": 12, "context": "To better understand the security properties of machine learning systems in adversarial settings, paradigms from security engineering and cryptography have been adapted to the machine learning field [2,5,14].", "startOffset": 199, "endOffset": 207}, {"referenceID": 8, "context": "The min-max approach assumes the learner and attacker\u2019s loss functions are antagonistic, which yields relatively simple optimization problems [10,12].", "startOffset": 142, "endOffset": 149}, {"referenceID": 10, "context": "The min-max approach assumes the learner and attacker\u2019s loss functions are antagonistic, which yields relatively simple optimization problems [10,12].", "startOffset": 142, "endOffset": 149}, {"referenceID": 5, "context": "Under certain conditions, such problems can be solved using a Nash equilibrium approach [7,8].", "startOffset": 88, "endOffset": 93}, {"referenceID": 6, "context": "Under certain conditions, such problems can be solved using a Nash equilibrium approach [7,8].", "startOffset": 88, "endOffset": 93}, {"referenceID": 7, "context": "The problem of evasion at test time was addressed in prior work, but limited to linear and convex-inducing classifiers [9,19,22].", "startOffset": 119, "endOffset": 128}, {"referenceID": 17, "context": "The problem of evasion at test time was addressed in prior work, but limited to linear and convex-inducing classifiers [9,19,22].", "startOffset": 119, "endOffset": 128}, {"referenceID": 20, "context": "The problem of evasion at test time was addressed in prior work, but limited to linear and convex-inducing classifiers [9,19,22].", "startOffset": 119, "endOffset": 128}, {"referenceID": 11, "context": "In contrast, the methods presented in Sections 2 and 3 can generally evade linear or non-linear classifiers using a gradient-descent approach inspired by Golland\u2019s discriminative directions technique [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "A comprehensive taxonomy of attacks can be found in [2,14].", "startOffset": 52, "endOffset": 58}, {"referenceID": 12, "context": "A comprehensive taxonomy of attacks can be found in [2,14].", "startOffset": 52, "endOffset": 58}, {"referenceID": 3, "context": "The considered model is part of a more general framework investigated in our recent work [5], which subsumes evasion and other attack scenarios.", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "As suggested by Laskov and Kloft [17], the adversary\u2019s goal should be defined in terms of a utility (loss) function that the adversary seeks to maximize (minimize).", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "5 This is also the setting adopted in previous work [9,19,22].", "startOffset": 52, "endOffset": 61}, {"referenceID": 17, "context": "5 This is also the setting adopted in previous work [9,19,22].", "startOffset": 52, "endOffset": 61}, {"referenceID": 20, "context": "5 This is also the setting adopted in previous work [9,19,22].", "startOffset": 52, "endOffset": 61}, {"referenceID": 6, "context": "Most of the previous work on evasion attacks assumes that the attacker can arbitrarily change every feature [8,10,12], but they constrain the degree of manipulation, e.", "startOffset": 108, "endOffset": 117}, {"referenceID": 8, "context": "Most of the previous work on evasion attacks assumes that the attacker can arbitrarily change every feature [8,10,12], but they constrain the degree of manipulation, e.", "startOffset": 108, "endOffset": 117}, {"referenceID": 10, "context": "Most of the previous work on evasion attacks assumes that the attacker can arbitrarily change every feature [8,10,12], but they constrain the degree of manipulation, e.", "startOffset": 108, "endOffset": 117}, {"referenceID": 18, "context": "For example, in the task of PDF malware detection [20,24,25], removal of content is not feasible, and content addition may cause correlated changes in the feature vectors.", "startOffset": 50, "endOffset": 60}, {"referenceID": 22, "context": "For example, in the task of PDF malware detection [20,24,25], removal of content is not feasible, and content addition may cause correlated changes in the feature vectors.", "startOffset": 50, "endOffset": 60}, {"referenceID": 23, "context": "For example, in the task of PDF malware detection [20,24,25], removal of content is not feasible, and content addition may cause correlated changes in the feature vectors.", "startOffset": 50, "endOffset": 60}, {"referenceID": 7, "context": "The choice of a suitable distance measure d : X \u00d7 X 7\u2192 R is application specific [9,19,22].", "startOffset": 81, "endOffset": 90}, {"referenceID": 17, "context": "The choice of a suitable distance measure d : X \u00d7 X 7\u2192 R is application specific [9,19,22].", "startOffset": 81, "endOffset": 90}, {"referenceID": 20, "context": "The choice of a suitable distance measure d : X \u00d7 X 7\u2192 R is application specific [9,19,22].", "startOffset": 81, "endOffset": 90}, {"referenceID": 21, "context": ", for neural networks, and SVMs [23].", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "This produces a similar effect to that shown by mimicry attacks in network intrusion detection [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "7 Mimicry attacks [11] consist of camouflaging malicious network packets to evade anomaly-based intrusion detection systems by mimicking the characteristics of the legitimate traffic distribution.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "In this section, we first report a toy example from the MNIST handwritten digit classification task [18] to visually demonstrate how the proposed algorithm modifies digits to mislead classification.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "Similar to Globerson and Roweis [12], we consider discriminating between two distinct digits from the MNIST dataset [18].", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "Similar to Globerson and Roweis [12], we consider discriminating between two distinct digits from the MNIST dataset [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "We now focus on the task of discriminating between legitimate and malicious PDF files, a popular medium for disseminating malware [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "works proposed machine-learning techniques for detecting malicious PDFs using the file\u2019s logical structure to accurately identify the malware [20,24,25].", "startOffset": 142, "endOffset": 152}, {"referenceID": 22, "context": "works proposed machine-learning techniques for detecting malicious PDFs using the file\u2019s logical structure to accurately identify the malware [20,24,25].", "startOffset": 142, "endOffset": 152}, {"referenceID": 23, "context": "works proposed machine-learning techniques for detecting malicious PDFs using the file\u2019s logical structure to accurately identify the malware [20,24,25].", "startOffset": 142, "endOffset": 152}, {"referenceID": 18, "context": "[20] in which each feature corresponds to the tally of occurrences of a given keyword.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The features (keywords) were extracted from each training set as described in [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Alternatively, one may explicitly model the attack distribution, as in [4]; or add the generated attack samples to the training set.", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": ", n-gram features [11].", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "A similar technique has been already exploited by [6] to overcome the pre-image problem.", "startOffset": 50, "endOffset": 53}, {"referenceID": 17, "context": "Other interesting extensions of our work may be to (i) consider more effective strategies such as those proposed by [19,22] to build a small but representative set of surrogate data; and (ii) improve the classifier estimate \u011d(x).", "startOffset": 116, "endOffset": 123}, {"referenceID": 20, "context": "Other interesting extensions of our work may be to (i) consider more effective strategies such as those proposed by [19,22] to build a small but representative set of surrogate data; and (ii) improve the classifier estimate \u011d(x).", "startOffset": 116, "endOffset": 123}], "year": 2017, "abstractText": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker\u2019s knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.", "creator": "LaTeX with hyperref package"}}}