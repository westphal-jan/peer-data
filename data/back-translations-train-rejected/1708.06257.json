{"id": "1708.06257", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Notes: A Continuous Model of Neural Networks. Part I: Residual Networks", "abstract": "In this series of notes, we try to model neural networks as as discretizations of continuous flows on the space of data, which can be called flow model. The idea comes from an observation of their similarity in mathematical structures. This conceptual analogy has not been proven useful yet, but it seems interesting to explore.", "histories": [["v1", "Mon, 21 Aug 2017 14:30:49 GMT  (211kb,D)", "http://arxiv.org/abs/1708.06257v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["zhen li", "zuoqiang shi"], "accepted": false, "id": "1708.06257"}, "pdf": {"name": "1708.06257.pdf", "metadata": {"source": "CRF", "title": "Notes: A Continuous Model of Neural Networks Part I: Residual Networks", "authors": ["Zhen Li", "Zuoqiang Shi"], "emails": ["lishen03@gmail.com", "zqshi@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "In this part, we start with a linear transport equation (with nonlinear transport velocity field) and obtain a class of residual neural networks. If the transport velocity field has a special shape, the obtained network is similar to the original ResNet (He et al., 2016). This neural network can be considered a discretization of the continuous flow defined by the transport flow. Finally, a summary of the correspondence between neural networks and transport equations is presented, followed by general discussions."}, {"heading": "1 Transport Equation", "text": "Let us consider the following end value problem (TVP) for the linear transport equation: {\u2202 tu + v q (t, x) \u00b7 \u00b7 u = 0, x-Rd, t-Rd [0, T] u (T, x) = f (x), x-Rd. (1) Here v is a function with Rd value that can be selected in different ways. Let us first consider the general form, then a special type: v (t, x) = W (2) (t) a (W (1) (t) + b (1) (t) + b (t) (t)) + b (2) (t), where W (1) (t) (t), W (2) (t), b (1) (t) (t) (t) (t). Activation a is a non-linear function with Rd value that Lixschitz is ongoing."}, {"heading": "2 Method of Characteristics", "text": "To make the following approximations reasonable, we assume that the change of v (t, x) with t and x is regular enough. In particular, we assume that the solution of (1) and (3) exists for the given conditions, and they are regular enough. Let x = q (t) with t0 = 0 and tL = T be a division of [0, T] and such that for each k = 1,..., L, sk = tk \u2212 tk \u2212 1 is small enough. Let x = q (t) be a characteristic of the transport equation (1), i.e. the solution of (3), and denote xk = q (tk). Denote Vk (x) = v (tk, x) and uk (x) = u (tk, x) for each x \u2212 x \u2212 \u2212 \u2212 xk = q (tk)."}, {"heading": "3 Neural Network Representation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 General form", "text": "The discrete solution (13) of the final value problem of the transport equation (1) applies to each x0-Rd. Its basic structure is shown in Figure 3. This structure reminds us of ResNet (He et al., 2016), but is only a formal one. To detect the actual structure, we must specify the definition of Vk."}, {"heading": "3.2 A Special Type", "text": "To get a ResNet with an explicit 2-layer block, consider the special type of transport velocity field given by (2). DenoteW (1) k = W (1) (tk), b (1) k = b (1) (tk), (14) W (2) k = W (2) (tk), b (2) k = b (2) (tk). (15) W (2) k = skW (2) k (2) k (2) k = skb (2) k. (16) By using the method of properties as before, we can getxk = xk \u2212 1 + W (2) k a (W (1) k xk \u2212 1 + b (1) k) + b (2) k (17) It creates a 2-layer ResNet block that is much more like the original ResNet."}, {"heading": "3.3 Discussions", "text": "The ResNetworks obtained here are something special. Firstly, as we can see in (10) and (17), due to the time step sk, the remaining term can be made sufficiently small compared to the leading term xk. This is a necessary condition for the ResNet to be modelled by transport equation. Secondly, the weights of the ResNet slowly change from block to block. Specifically, the weights at the same positions of adjacent ResNet blocks should be close to each other, since they are assumed to be the discretization of continuous time functions. For example, W (1) k is close to W (1) k \u2212 1, W (2) k close to W (2) k \u2212 1 and so on."}, {"heading": "3.4 Summary", "text": "There is a clear agreement between the ResNets (10) (17) and the transport equation (1), which also exists in pure networks (neural networks without residual abbreviations). It is summarized in Table 1 and shown in Figure 5."}, {"heading": "4 Discussions", "text": "Claim: The discussions here are not intended to provide a solution to a problem in the theory and application of neural networks, nor can they answer any concerns; they only present our ideas on some of the candidate directions that could be studied. 1. In order to solve the end value problem of the transport equation, we can consider other numerical methods in addition to the method of properties that have been purposely developed for PDEs. For example, we can increase the regularity of solutions by adding dissipative terms to the transport equation. 2. Training neural networks could be considered a solution to the inverse problem of the transport equation. It means that both the initial value (in random samples) and the end value are set. The task is to find a transport velocity field (depending on time) that transports the initial value to the end value. It is made clearer in the remaining parts of this notation series. 3. Correspondence provides a way to partially explain why depth is good for neural networks. Usually, the initial velocity can be better adjusted to a good data layer by learning a good prediction."}, {"heading": "Acknowledgement", "text": "Zhen Li would like to express his gratitude for the support of Professors Yuan Yao and Yang Wang from the Department of Mathematics, HKUST."}], "references": [{"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "If the transport velocity field has a special form, the obtained network is found similar to the original ResNet (He et al., 2016).", "startOffset": 113, "endOffset": 130}, {"referenceID": 0, "context": "This structure reminds us of the ResNet (He et al., 2016), but it is merely a formal one.", "startOffset": 40, "endOffset": 57}], "year": 2017, "abstractText": "In this series of notes, we try to model neural networks as as discretizations of continuous flows on the space of data, which can be called flow model. The idea comes from an observation of their similarity in mathematical structures. This conceptual analogy has not been proven useful yet, but it seems interesting to explore. In this part, we start with a linear transport equation (with nonlinear transport velocity field) and obtain a class of residual type neural networks. If the transport velocity field has a special form, the obtained network is found similar to the original ResNet (He et al., 2016). This neural network can be regarded as a discretization of the continuous flow defined by the transport flow. In the end, a summary of the correspondence between neural networks and transport equations is presented, followed by some general discussions. 1 Transport Equation Consider the following terminal value problem (TVP) for linear transport equation: { \u2202tu + v(t, x) \u00b7 \u2207u = 0, x \u2208 R, t \u2208 [0, T ] u(T, x) = f(x), x \u2208 R. (1) Here v is a Rd-valued function, called the transport velocity field. It can be chosen in different ways. We will consider firstly the general form, then a special type: v(t, x) = W (t)a ( W (t) + b(t) ) + b(t), (2) where W (1)(t),W (2)(t) \u2208 Rd\u00d7d, b(1)(t), b(2)(t) \u2208 Rd. The activation a is a Rd-valued nonlinear function, which is Lipschitz continuous. \u2217Department of Mathematics, HKUST. Email: lishen03@gmail.com \u2020Yau Mathematical Sciences Center, Tsinghua University. Email: zqshi@tsinghua.edu.cn 1 ar X iv :1 70 8. 06 25 7v 1 [ cs .L G ] 2 1 A ug 2 01 7 It is well known that the solution of equation (1) is transported along characteristics. The characteristics are defined as solutions of the initial value problems (IVP) of the ODE: { \u1e8b = v(t, x), t \u2208 [0, T ] x(0) = x0, (3) where x0 \u2208 Rd. Along the solution curve x = q(t), it is easy to verify that d dt u(t, q(t)) = (\u2202tu(t, x) + q\u0307(t) \u00b7 \u2207u(t, x))x=q(t) (4) = (\u2202tu(t, x) + v(t, q(t)) \u00b7 \u2207u(t, x))x=q(t) = 0. (5) So u remains unchanged along the curve. See Figure 1 for a conceptual illustration. Therefore u(0, x0) = u(t, q(t)) = u(T, q(T )) = f(q(T )). (6) Figure 1: Illustration of characteristics. Here x, u(t, x) \u2208 R. 2 Method of Characteristics In this part we will use the method of characteristics to solve (1). In order to make the following approximations reasonable, we assume that the change of v(t, x) with t and x is regular enough. Especially, we assume that the solution of (1) and (3) exist for the posed conditions, and they are regular enough. Let {tk}k=0 with t0 = 0 and tL = T be a partition of [0, T ] \u2282 R such that for any k = 1, . . . , L, sk = tk \u2212 tk\u22121 is small enough. Let x = q(t) be a characteristic of the transport equation (1), i.e. the solution of (3), and denote xk = q(tk). Denote Vk(x) = v(tk, x) and uk(x) = u(tk, x) for any x \u2208 R. See Figure 2 for a illustration of the discretization.", "creator": "LaTeX with hyperref package"}}}