{"id": "1708.06266", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Probabilistic Relation Induction in Vector Space Embeddings", "abstract": "Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge. However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way. In this paper, we propose two probabilistic models to address this issue. The first model is based on the common relations-as-translations view, but is cast in a probabilistic setting. Our second model is based on the much weaker assumption that there is a linear relationship between the vector representations of related words. Compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what can and cannot be extracted from the word embedding.", "histories": [["v1", "Mon, 21 Aug 2017 14:52:10 GMT  (529kb,D)", "http://arxiv.org/abs/1708.06266v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["zied bouraoui", "shoaib jameel", "steven schockaert"], "accepted": false, "id": "1708.06266"}, "pdf": {"name": "1708.06266.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Relation Induction in Vector Space Embeddings", "authors": ["Zied Bouraoui", "Shoaib Jameel", "Steven Schockaert"], "emails": ["BouraouiZ@Cardiff.ac.uk", "JameelS1@Cardiff.ac.uk", "SchockaertS1@Cardiff.ac.uk"], "sections": [{"heading": "Introduction", "text": "In fact, it is in such a way that we see ourselves in a position to live in the real world, as if we were able to live in the real world, in which we live, in the real world, in which we live, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world in the real world, in the real world, in the real world, in the real world in the real world, in the real world, in the real world, in the real world, in the real world, in the real world in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world"}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Predicting Relations", "text": "At least three different types of approaches have been studied to predict relationships that are lacking in a particular knowledge base. First, there is a major work on extracting relationships from the text. Among other things, a number of approaches have been developed in recent years specifically aimed at completing knowledge bases. These methods essentially learn how to extract the relationships under consideration by using their known instances as a form of remote monitoring (Mintz et al. 2009; Riedel, Yao and McCallum 2010; Surdeanu et al. 2012). ar Xiv: 170 8.06 266v 1 [cs.A I] 2 ug2 017The second type of approaches is based on modelling statistical dependencies among the known instances of the relationships under consideration, e.g. if we already know that \"Person A works for Company B\" and that \"Company B is based in Country C,\" we can plausibly deduce that \"A lives in C.\" To exploit such dependencies, domain bases, and relationships based on some explicit codes."}, {"heading": "Modeling Relations in a Vector Space", "text": "As already mentioned in the introduction, various syntactic and semantic relationships can be modeled as vector translations in a word embedding (Mikolov, Yih, and Branch 2013). It has been shown, among other things, that word embedding can be used to ask analogy questions of the form a: b: c:?, the question of a word referring to c, in the same way that b can be modeled in one (e.g. france: wine:?) by using the word \"maximizes cos\" (pb \u2212 pa, pw). Several types of interpretative features can be modeled as instructions in word embedding."}, {"heading": "Modeling Relations", "text": "In this section, we propose two models for relationship induction: We assume that we receive a set of pairs {(s1, t1),..., (sn, tn)} as training data, and we must determine whether a particular word pair (s, t) is related in the same way."}, {"heading": "Translation Model", "text": "The first model is based on the common view that relationships can be modeled as vector translations. Source words s1,..., sn typically belong to some semantic or syntactic category, and as a result of their representations typically belong to a specific subspace of the word embedding. This is illustrated in Figure 1 for the \"body coverage\" relationship, in which the source words represent all animals and the target words represent body coverage types. If a relationship can be modeled as a translation, this means that the source subspace and the target subspace must be aligned. However, this is rarely perfect. In fact, the source and target spaces in most cases even have a different number of dimensions. In the example from Figure 1, we can see that there is no vector that perfectly models the relationship, although all valid word pairs (si, ti) define a translation that is more or less horizontal."}, {"heading": "Regression Model", "text": "The translation model is based on the assumption that the source and target spaces are aligned. In a relationship like Capital of, there is a direct link between each source word and its corresponding target word, i.e. the representation of a country should typically resemble the representation of its capital city. In such cases, however, we can expect that in a wine subspace there are instructions that correspond to characteristics such as \"sweetness,\" \"acidity\" and \"amount of tannins.\" Likewise, in a subspace of food types, we can give instructions that correspond to characteristics such as \"healthy\" or prototypes such as \"meat,\" fish \"and\" tomatoes. \""}, {"heading": "Evaluation", "text": "In this section, we experimentally compare the two proposed models with a set of baseline methods from the literature. The relationships we are looking at are taken from three standard benchmark datasets, each containing a mix of syntactic and semantic relationships: (i) the Google Analogy Test Set (Google), which contains 14 types of relationships with a different number of instances per relationship (Mikolov et al. 2013), (ii) the Bigger Analogy Test Set (BATS), which contains 40 relationships with 50 instances per relationship (Gladkova, Drozd, and Matsuoka 2016), and (iii) the DiffVec Test Set (DV), which contains 36 relationships with a different number of instances per relationship (Vylomova et al. 2016) We report the results for two embeddings we learned with Skipgram, one from the Wikipedia dump of 2 November 2015 (SG-Wiki) and one from 100 words Google News set1 (SG-G), which we use."}, {"heading": "Baselines", "text": "The first baseline we are looking at is the 3CosAvg method proposed in (Drozd, Gladkova and Matsuoka 2016), which essentially treats the relationship problem like an analog completion problem, using the average translation vector across all pairs (si, ti) from the training data. Specifically, this method assigns the test pair (s, t) the following score: score3CA (t, s) = cos (pt, ps + \u2211 i pti \u2212 psi n) Despite its simplicity, 3CosAvg was found to be a remarkably strong baseline. Another method proposed in (Drozd, Gladkova and Matsuoka 2016), called LRCos, is based on the assumption that (s, t) is probably correct if cos (ps, pt) is high and t is of the correct type, where a logistic regression classifier where the target words {t1,... tn} are validated."}, {"heading": "Results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Conclusions", "text": "The first model is based on the common assumption that the lexical relations are embedded in one word; the other model is based on a linear regression based on the lexical relations; the second model is based on a lexical regression 0. 0 0 0 0. 0 0 0. 0 0 0. 0 0 0. 0 0 0 0. 0 0 0. 0 0 0. 0 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0."}], "references": [{"title": "Entailment above the word level in distributional semantics", "author": ["Baroni"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Baroni,? \\Q2012\\E", "shortCiteRegEx": "Baroni", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester"], "venue": "Journal of the American Society for Information Science", "citeRegEx": "Deerwester,? \\Q1990\\E", "shortCiteRegEx": "Deerwester", "year": 1990}, {"title": "Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning", "author": ["Derrac", "J. Schockaert 2015] Derrac", "S. Schockaert"], "venue": "Artificial Intelligence", "citeRegEx": "Derrac et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Derrac et al\\.", "year": 2015}, {"title": "Word embeddings, analogies, and machine learning: Beyond king - man + woman = queen", "author": ["Gladkova Drozd", "A. Matsuoka 2016] Drozd", "A. Gladkova", "S. Matsuoka"], "venue": "In Proceedings of the 26th International Conference on Computational Linguistics,", "citeRegEx": "Drozd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Drozd et al\\.", "year": 2016}, {"title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn\u2019t", "author": ["Drozd Gladkova", "A. Matsuoka 2016] Gladkova", "A. Drozd", "S. Matsuoka"], "venue": "In Proceedings of the Student Research Workshop at NAACL", "citeRegEx": "Gladkova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gladkova et al\\.", "year": 2016}, {"title": "Distributional vectors encode referential attributes", "author": ["Gupta"], "venue": "In Proc. EMNLP,", "citeRegEx": "Gupta,? \\Q2015\\E", "shortCiteRegEx": "Gupta", "year": 2015}, {"title": "Modeling context words as regions: An ordinal regression approach to word embedding", "author": ["Jameel", "S. Schockaert 2017] Jameel", "S. Schockaert"], "venue": "In Proceedings of the 21st Conference on Computational Natural Language Learning,", "citeRegEx": "Jameel et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jameel et al\\.", "year": 2017}, {"title": "Deriving adjectival scales from continuous space word representations", "author": ["Kim", "de Marneffe 2013] Kim", "J.-K", "de Marneffe", "M.-C"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Statistical predicate invention", "author": ["Kok", "S. Domingos 2007] Kok", "P. Domingos"], "venue": "In Proc. ICML,", "citeRegEx": "Kok et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kok et al\\.", "year": 2007}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Mitchell Lao", "N. Cohen 2011] Lao", "T. Mitchell", "W.W. Cohen"], "venue": null, "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Yih Mikolov", "T. Zweig 2013] Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "In Proc. NAACL-HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz"], "venue": "In Proceedings of the 47th Annual Meeting of the ACL,", "citeRegEx": "Mintz,? \\Q2009\\E", "shortCiteRegEx": "Mintz", "year": 2009}, {"title": "Factorizing YAGO: Scalable machine learning for linked data", "author": ["Tresp Nickel", "M. Kriegel 2012] Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 21st International Conference on World Wide Web,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Riedel"], "venue": "In Proc. HLT-NAACL,", "citeRegEx": "Riedel,? \\Q2013\\E", "shortCiteRegEx": "Riedel", "year": 2013}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Yao Riedel", "S. McCallum 2010] Riedel", "L. Yao", "A. McCallum"], "venue": "In Proc. ECML/PKDD,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Erk Roller", "S. Boleda 2014] Roller", "K. Erk", "G. Boleda"], "venue": "In Proc. COLING,", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Word embedding calculus in meaningful ultradense subspaces", "author": ["Rothe", "S. Sch\u00fctze 2016] Rothe", "H. Sch\u00fctze"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Rothe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2016}, {"title": "Learning first-order horn clauses from web text", "author": ["Schoenmackers"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schoenmackers,? \\Q2010\\E", "shortCiteRegEx": "Schoenmackers", "year": 2010}, {"title": "Analogyspace: reducing the dimensionality of common sense knowledge", "author": ["Havasi Speer", "R. Lieberman 2008] Speer", "C. Havasi", "H. Lieberman"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial intelligence,", "citeRegEx": "Speer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2008}, {"title": "Multi-instance multilabel learning for relation extraction", "author": ["Surdeanu"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Surdeanu,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "P.D. Pantel 2010] Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Word representations via gaussian embedding", "author": ["Vilnis", "L. McCallum 2015] Vilnis", "A. McCallum"], "venue": null, "citeRegEx": "Vilnis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilnis et al\\.", "year": 2015}, {"title": "Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning", "author": ["Vylomova"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Vylomova,? \\Q2016\\E", "shortCiteRegEx": "Vylomova", "year": 2016}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Efficient inference and learning in a large knowledge base - reasoning with extracted information using a locally groundable first-order probabilistic logic. Machine Learning 100(1):101\u2013126", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2015\\E", "shortCiteRegEx": "Wang", "year": 2015}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Weeds"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics,", "citeRegEx": "Weeds,? \\Q2014\\E", "shortCiteRegEx": "Weeds", "year": 2014}, {"title": "Knowledge base completion via search-based question answering", "author": ["West"], "venue": "In Proceedings of the 23rd International Conference on World Wide Web,", "citeRegEx": "West,? \\Q2014\\E", "shortCiteRegEx": "West", "year": 2014}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "The relations we consider are taken from three standard benchmark datasets, each containing a mixture of syntactic and semantic relationships: (i) the Google Analogy Test Set (Google), which contains 14 types of relations with a varying number of instances per relation (Mikolov et al. 2013), (ii) the Bigger Analogy Test Set (BATS), wich contains 40 relations with 50 instances per relation (Gladkova, Drozd, and Matsuoka 2016), and (iii) the DiffVec Test Set (DV), which contains 36 relations with a varying number of instances per relation (Vylomova et al.", "startOffset": 270, "endOffset": 291}], "year": 2017, "abstractText": "Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge. However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way. In this paper, we propose two probabilistic models to address this issue. The first model is based on the common relations-as-translations view, but is cast in a probabilistic setting. Our second model is based on the much weaker assumption that there is a linear relationship between the vector representations of related words. Compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what can and cannot be extracted from the word embedding.", "creator": "LaTeX with hyperref package"}}}