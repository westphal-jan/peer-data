{"id": "1708.06303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Network Model Selection for Task-Focused Attributed Network Inference", "abstract": "Networks are models representing relationships between entities. Often these relationships are explicitly given, or we must learn a representation which generalizes and predicts observed behavior in underlying individual data (e.g. attributes or labels). Whether given or inferred, choosing the best representation affects subsequent tasks and questions on the network. This work focuses on model selection to evaluate network representations from data, focusing on fundamental predictive tasks on networks. We present a modular methodology using general, interpretable network models, task neighborhood functions found across domains, and several criteria for robust model selection. We demonstrate our methodology on three online user activity datasets and show that network model selection for the appropriate network task vs. an alternate task increases performance by an order of magnitude in our experiments.", "histories": [["v1", "Mon, 21 Aug 2017 16:04:17 GMT  (2757kb,D)", "http://arxiv.org/abs/1708.06303v1", null], ["v2", "Sat, 16 Sep 2017 04:08:22 GMT  (5515kb,D)", "http://arxiv.org/abs/1708.06303v2", null]], "reviews": [], "SUBJECTS": "cs.SI cs.AI", "authors": ["ivan brugere", "chris kanich", "tanya y berger-wolf"], "accepted": false, "id": "1708.06303"}, "pdf": {"name": "1708.06303.pdf", "metadata": {"source": "CRF", "title": "Network Model Selection for Task-Focused Attributed Network Inference", "authors": ["Ivan Brugere", "Chris Kanich", "Tanya Y. Berger-Wolf"], "emails": ["ibruge2@uic.edu", "ckanich@uic.edu", "tanyabw@uic.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTIONNetworks are models that represent relationships between entities: individuals, genes, documents and media, language, products, etc. We often assume that the expressed or inferred edge relationships of the network are correlated with the underlying behavior of individuals or entities, reflected in their actions or preferences over time. However, for some problems, the correspondence between observed behavior and network structure is well established. For example, simple link and prediction hysteria on social networks tends to be good because it corresponds to the social processes for the growth of networks. However, sharing content on social networks shows that \"weak\" ties among friends account for a large part of the influence on users [2]."}, {"heading": "II. RELATED WORK", "text": "Our work focuses primarily on research in the field of machine learning and network structures."}, {"heading": "III. CONTRIBUTIONS", "text": "We present a general, modular methodology for model selection in task-oriented network inference. Our work identifies components of the common network inference workflow, including network models, tasks, task-neighborhood functions, and measures to evaluate networks derived from data, \u2022 uses basic, interpretable network models that are relevant in many application areas, and basic tasks and task locations that measure various aspects of network attribute correlations, \u2022 presents importance, stability, and zero-model tests for task-oriented network models based on three online user activity data. Our work focuses on process and methodology; novel network models and task methods complement each other."}, {"heading": "IV. METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Task-Focused Network Inference and Evaluation", "text": "There are a number of approaches that address the question of whether and to what extent networking can occur at all. (...) In all of these cases, we evaluate each edge set as one of many possible network models. \"Attributes are optionally linked to our methodology, which is characterized by dashed lines. Edges can be completely missing or multiple edge sets. In all of these cases, we evaluate each edge set as one of many possible network models.\" Attributes are any data associated with nodes (and / or edges) in the network, which are often very high-dimensional, such as activity logs and post content in social networks, gene expression profiles in documents, and other metadresses."}, {"heading": "B. Network models", "text": "We focus on the latter in order to avoid assumptions about common attribute label distributions. We introduce two basic, similarity-based network models that apply to most domains: the k-nearest attribute similarity space (KNN) and thresholds (TH). We then select node pairs by: \u2022 k nearest neighbor MKNN (A, S, aj) \u2192 sij and a target edge number used to create a pair attribute similarity space (KNN)."}, {"heading": "C. Tasks for Evaluating Network Models", "text": "We evaluate network models using two basic network tasks: collective classification and linkage prognose.1) Collective classification (CC): The collective classification problem learns network edge structure relationships with attributes and / or labels to predict label values [8], [10] This task is often used to derive unknown labels in the network from \"local\" discriminatory relationships in attributes, such as labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels lablabels labels labels lablabels labels labels labels labels lablabels labels labels lablablablabels labels lablablabels labels lablabels labels labels labels labels labels lablabels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels lablabels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels labels lablabels labels labels labels labels lab"}, {"heading": "D. Task Locality", "text": "Both of the above tasks are classifiers that take attributes and designations as input, provided by the neighborhood function N (E \u2032, i). However, this neighborhood does not need to be defined as network adjacency. We define the neighborhood function to provide each task method with node / edge attributes selected at different locations from the test node. These varying locations provide interpretable feedback at the level of abstraction that the network model performs best. We propose four common locations: 1) Local methods use simple network adjacence of a node vi. For LP, local methods use the egonet of a node vi. This is defined as an edge from adjacent to vi, plus the edges of these adjacent nodes."}, {"heading": "E. Classification Methods", "text": "For all different locations, our tasks are reduced to a supervised classification problem. For the underlying classification method, we use standard linear support vector machines (SVM) and random forests (RF), which are selected based on speed and suitability in sparse data. Absolute prediction accuracy is not the primary goal of our work; these methods simply need to generate a consistent network model ranking across many model configurations."}, {"heading": "F. Network Model Configurations", "text": "We define a network model configuration as a combination of all specified modeling options: network model, similarity measurement, density, task location, and task method. Each network model configuration can be evaluated independently, meaning we can easily scale to hundreds of configurations in small and medium-sized networks."}, {"heading": "V. DATASETS", "text": "We demonstrate our methodology using three sets of user activity data. These data include BeerAdvocate's beer review history, Last.fm's music listening history, and MovieLens' movie review history."}, {"heading": "A. BeerAdvocate", "text": "BeerAdvocate is a website founded in 1996 that publishes user-provided text ratings and numerical ratings of individual beers. BeerAdvocate's rating dataset [26] contains 1.5 million ratings of 264K unique beers from 33K users. We aggregate each rating based on the average rating in 5 rating categories on a scale of 0-5 (\"Appearance,\" \"Aroma,\" \"Taste,\" \"Overall Impression\"), with node attribute vectors where non-zero elements are the user's average rating for \"this\" beer (details in Table I). 1) Genre designations: The BeerAdvocate dataset contains a \"Style\" field with 104 unique values. We consider a user to be the \"reviewer\" of a particular beer style if he has rated at least 5 beers of the style. We also trim styles with less than 100 applicants to get 45 styles (e.g., \"Imperial,\" \"Hefezen)."}, {"heading": "B. Last.fm", "text": "The Last.fm social network is a platform that focuses on music logging, recommendations and discussions. The platform was founded in 2002 and offers many opportunities for longitudinal study of user preferences in social networks.The largest connected component of the Last.fm social network and all associated music logs were collected until March 2016.The users of this data set are grouped by their discovery in a broad initial search on the explicit \"friendship network\" from the node of a user account opened in 2006.Previous authors extract a data set of the first approximately 20K users, whereby a connected component with 678K edges, an average level of 35 and over 1B are collectively played by users of over 2.8 million unique artists.For each of these nodes, sparing non-zero attributes describe the total number of games of the user with the unique artist. The.fm \"explicit\" user network frameworks are explained as at least in our friendship models."}, {"heading": "C. MovieLens", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "VI. EVALUATION", "text": "We validate the network models based on different configurations (similarity measurement, network density, task locality, and task method) on each dataset using the defined L * label sets of the dataset. We measure the accuracy of both the collective classification (CC) and the link prediction (LP). Across all network model configurations, we evaluate models based on the precision evaluated on the validation partition and select the best placed model to be used in the test. To evaluate the robustness of the model selection, we evaluate all network model configurations on both validation and test partitions to accurately examine their complete ranking. Let pi designate the precision of the i-th network model configuration, ps as the precision of the selected model configuration evaluated on the test partition, p (1) as the precision of the \"best\" model in the current context of a top 10 (Vp)."}, {"heading": "A. Model Stability: Precision", "text": "Table II indicates the mean precision of all network configurations evaluated on the test partition. A data point in this distribution is the precision value of a network configuration organized by the task method of each line. \u00b5 indicates the mean precision of all such configurations. p (1) indicates the precision of the best network model configuration for \"this\" task method. p (1) indicates the accuracy of the best network model configuration for \"this\" method. p (1) represents the difference between p (1) and \u00b5 the greatest possible precision gain added by selecting the model. Table II reports that the stability of the mean precision is determined by validating and testing the partitions. A data point in this distribution is pi, validation \u2212 pi, testing the differences in precision for the same \"i\" model configuration in the partition (p)."}, {"heading": "B. Model Consistency: Selected Model Ranking", "text": "Table III indicates the standardized rank of the selected model configuration, which is evaluated on the test partition. Values can be interpreted as percentiles, with 1 indicating that the same model comes first in both partitions (i.e. higher is better). We highlight models with high precision between validation and test partitions: rank > 0.9, i.e. the selected model is in the top 10% of the model configurations on the test partition. Table III shows several cases of rank inconsistency for certain problem configurations (e.g. BeerAdvocate LP-RF, MovieLens LP-RF) and remarkable consistency for others (BeerAdvocate CC-RF, CC-SVM), which ranks across many overall network configurations (| N | > 100). This is a key finding that shows that suitable network models change depending on the task and the underlying dataset."}, {"heading": "C. Model Stability: Rank Order", "text": "Table IV reports on the ranking of Kendall between the ranking of model configurations by precision, validation and verification of partitions where \u03c4 = 1 indicates that the ranking is the same. We report on the associated p-value of the ranking of statistical values. For multiple tasks across multiple sets of data, the ranking is very consistent across all model configurations. Although this ranking has remarkable consistency, it is not appropriate if the result contains many bad models that can predominate in low ranks. Therefore, Intersection (10) reports on the common model configurations in the top 10 of validation and test partitions. As the top k lists can be short and have fragmented elements, we find the simple intersection instead of ranking. We highlight tasks in bold order significance level p < 1.00E \u2212 03 and intersection point (10) between the individual model configurations of selective validation and test partitions of bold entries (Table IV) (Summarizing Number IV and Number III)."}, {"heading": "D. Consistency: Task Method Locality", "text": "Indeed, each principal color represents a data set, and shades indicate different task methods. Collective classification of models at the BeerAdvocate level strongly favors local tasks, and Last.fm favors community and global locations; both agree with model selections in Table III. \"Global\" localization measures measure the extent to which population-relevant models are preferred for each location. Searching closer to Last.fm, the \"global\" configuration in tests, is only -0.01 for CC-SVM, and -0.05 for CC-RF. This indicates a very weak network effect on Last.fm."}, {"heading": "E. Model Selection and Cross-Task Performance", "text": "Table VI (top) indicates the performance of the model across tasks. We make the model selection on the validation section (for each task on the left) and report on the performance of the task during testing using the task method specified in the column. Table VI (bottom) calculates the performance of the model as before, with the values on the diagonal being the same as in Tables II and III. On the diagonal, the model is evaluated in tests on the task for which it was not selected. Table VI (bottom) reports on the performance of the model by performing the model selection on average of CC and LP accuracy. This result clearly shows the most important takeaway from our study: the \"best\" network model depends on the follow-up task. The off diagonal shows that models selected on the \"other\" task always perform very poorly. Consider the worst case for selecting the same task LP on MovieLine's average performance, but select a better model on our Model 4 (IV)."}, {"heading": "F. Node Difficulty", "text": "For the collective classification, we make a prediction at one node for each positive label instance for many label sets. For the link prediction, we link predictions about the ego mesh (in the order of hundreds) with this node. We can therefore establish robust precision distributions for individual nodes for a specific model configuration or combination of configurations. Figure 4 shows that the density map of the precision of individual nodes for load.fm, aggregated over the top 5 models for validation and testing of partitions (a total of 10 models), is correlated to different methods (left) and tasks (right). The right illustration shows that the RF- and SVM methods are largely linear correlated, whereby SVM performs better. The right illustration shows slight deviation in LP-SVM, whereby some deviation in relation to performance deviation from the SVM indicates a deviation of the deviation from the deviation of the deviation of the deviation of the deviation of the deviation."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "This work focused on a general task-focused method for selecting network models that uses basic network tasks - collective classification and predictive linkage - to evaluate multiple common network models and locations. We propose to evaluate the model selection for network tasks based on several criteria, including (1) precision stability of the task, (2) consistency of the selected model rank, (3) stability of the full rank, and (4) consistency of the top-k rank. We evaluate three sets of data to evaluate users and show a robust selection of specific models for several of task settings. We show that the selection of network models is highly subject to a particular interesting task, which shows that selecting models across tasks of an order of magnitude works better than selecting another task."}, {"heading": "A. Limitations and Future Improvements", "text": "1) Inclusion of model costs: We do not currently include network model costs (e.g. economy) or prediction method costs (e.g. method of encoding size in bytes, runtime) as criteria for model selection. In future work, we would like to penalize more expensive models. For example, we train in the order of thousands of small \"local\" models, while an ensemble model that may have similar performance traits at dozens of nodes is preferred. 2) Network model Alternative: We would also like to explore alternative network models to summarize the task at minimal cost. Some task methods are also relatively robust compared to our selection of network density parameters; the more economical network model would be preferable.2) Network model Alternative: We would also like to discover alternative network models. \"Alternative\" refers to the discovery of maximally different models (according to some criteria) that meet given network density constraints [27] based on future work."}], "references": [{"title": "Friends and neighbors on the Web", "author": ["L.A. Adamic", "E. Adar"], "venue": "Social Networks, vol. 25, no. 3, pp. 211\u2013230, jul 2003. [Online]. Available: https://doi.org/10.1016/S0378-8733(03)00009-1", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "The Role of Social Networks in Information Diffusion", "author": ["E. Bakshy", "I. Rosenn", "C. Marlow", "L. Adamic"], "venue": "Proceedings of the 21st International Conference on World Wide Web, ser. WWW \u201912. New York, NY, USA: ACM, 2012, pp. 519\u2013528. [Online]. Available: http://doi.acm.org/10.1145/2187836.2187907", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A Survey of Link Prediction in Social Networks", "author": ["M.A. Hasan", "M.J. Zaki"], "venue": "Social Network Data Analytics SE - 9, C. C. Aggarwal, Ed. Springer US, 2011, pp. 243\u2013275. [Online]. Available: http://dx.doi.org/10.1007/978-1-4419-8462-3_9", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "Journal of the American Society for Information Science and Technology, vol. 58, no. 7, pp. 1019\u20131031, May 2007. [Online]. Available: http://doi.wiley.com/10.1002/asi.20591", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Link prediction using supervised learning", "author": ["M. Al Hasan", "V. Chaoji", "S. Salem", "M. Zaki"], "venue": "SDM06: workshop on link analysis, counter-terrorism and security, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint Link Prediction and Attribute Inference Using a Social-Attribute Network", "author": ["N.Z. Gong", "A. Talwalkar", "L. Mackey", "L. Huang", "E.C.R. Shin", "E. Stefanov", "E.R. Shi", "D. Song"], "venue": "ACM Trans. Intell. Syst. Technol., vol. 5, no. 2, pp. 27:1\u2014-27:20, Apr. 2014. [Online]. Available: http://doi.acm.org/10.1145/2594455", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Collective classification of network data.", "author": ["B. London", "L. Getoor"], "venue": "Data Classification: Algorithms and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Collective Classification in Network Data", "author": ["P. Sen", "G.M. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi- Rad"], "venue": "AI Magazine, vol. 29, no. 3, pp. 93\u2013106, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Labels or Attributes?: Rethinking the Neighbors for Collective Classification in Sparsely-labeled Networks", "author": ["L.K. McDowell", "D.W. Aha"], "venue": "Proceedings of the 22Nd ACM International Conference on Information & Knowledge Management, ser. CIKM \u201913. New  York, NY, USA: ACM, 2013, pp. 847\u2013852. [Online]. Available: http://doi.acm.org/10.1145/2505515.2505628", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining Collective Classification and Link Prediction", "author": ["M. Bilgic", "G.M. Namata", "L. Getoor"], "venue": "Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007), oct 2007, pp. 381\u2013386.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Collective Graph Identification", "author": ["G.M. Namata", "B. London", "L. Getoor"], "venue": "2015. [Online]. Available: https://doi.org/10.1145/2818378", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Network Structure Inference, A Survey: Motivations, Methods, and Applications", "author": ["I. Brugere", "B. Gallagher", "T.Y. Berger-Wolf"], "venue": "ArXiv e-prints, Oct. 2016. [Online]. Available: https://arxiv.org/abs/1610.00782", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Network Topology Inference", "author": ["E. Kolaczyk", "G. Cs\u00e1rdi"], "venue": "Statistical Analysis of Network Data with R SE - 7, ser. Use R! Springer New York, 2014, vol. 65, pp. 111\u2013134. [Online]. Available: http://dx.doi.org/10.1007/978-1-4939-0983-4_7", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A General Framework for Weighted Gene Co-Expression Network Analysis", "author": ["B. Zhang", "S. Horvath"], "venue": "Statistical Applications in Genetics and Molecular Biology, vol. 4, no. 1, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Contributions and challenges for network models in cognitive neuroscience", "author": ["O. Sporns"], "venue": "Nature Neuroscience, vol. 17, no. 5, pp. 652\u2013660, May 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Inferring Networks of Substitutable and Complementary Products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "Proc. Proc. of ACM SIGKDD 2015, ser. KDD \u201915. ACM, 2015, pp. 785\u2013794. [Online]. Available: http://doi.acm.org/10.1145/2783258.2783381", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring Networks of Diffusion and Influence", "author": ["M. Gomez-Rodriguez", "J. Leskovec", "A. Krause"], "venue": "ACM Transactions on Knowledge Discovery from Data, vol. 5, no. 4, pp. 21:1\u2014-21:37, Feb. 2012. [Online]. Available: http://doi.acm.org/10.1145/2086737.2086741", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "On the Convexity of Latent Social Network Inference", "author": ["S. Myers", "J. Leskovec"], "venue": "2010. [Online]. Available: http://papers.nips.cc/ paper/4113-on-the-convexity-of-latent-social-network-inference", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Attributed Graph Models: Modeling Network Structure with Correlated Attributes", "author": ["J.J. Pfeiffer III", "S. Moreno", "T. La Fond", "J. Neville", "B. Gallagher"], "venue": "Proceedings of the 23rd International Conference on World Wide Web, ser. WWW \u201914. ACM, 2014, pp. 831\u2013842. [Online]. Available: http://doi.acm.org/10.1145/2566486.2567993", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiplicative Attribute Graph Model of Real-World Networks", "author": ["M. Kim", "J. Leskovec"], "venue": "Internet Mathematics, vol. 8, no. 1-2, pp. 113\u2013160, mar 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "An introduction to exponential random graph (p*) models for social networks", "author": ["G. Robins", "P. Pattison", "Y. Kalish", "D. Lusher"], "venue": "Social Networks, vol. 29, no. 2, pp. 173\u2013191, may 2007. [Online]. Available: https://doi.org/10.1016/j.socnet.2006.08.002", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Inferring Relevant Social Networks from Interpersonal Communication", "author": ["M. De Choudhury", "W.A. Mason", "J.M. Hofman", "D.J. Watts"], "venue": "Proceedings of the 19th International Conference on World Wide Web, ser. WWW \u201910. ACM, 2010, pp. 301\u2013310. [Online]. Available: http://doi.acm.org/10.1145/1772690.1772722", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 10, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluating social networks using task-focused network inference", "author": ["I. Brugere", "C. Kanich", "T. Berger-Wolf"], "venue": "Proceedings of the 13th International Workshop on Mining and Learning with Graphs (MLG), 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "The MovieLens Datasets: History and Context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Trans. Interact. Intell. Syst., vol. 5, no. 4, pp. 19:1\u2014-19:19, dec 2015. [Online]. Available: http://doi.acm.org/10.1145/2827872", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Attitudes and Attributes from Multi-aspect Reviews", "author": ["J. McAuley", "J. Leskovec", "D. Jurafsky"], "venue": "Proceedings of the 2012 IEEE 12th International Conference on Data Mining, ser. ICDM \u201912. Washington, DC, USA: IEEE Computer Society, 2012, pp. 1020\u20131025. [Online]. Available: http://dx.doi.org/10.1109/ICDM.2012.110", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A Principled and Flexible Framework for Finding Alternative Clusterings", "author": ["Z. Qi", "I. Davidson"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201909. New York, NY, USA: ACM, 2009, pp. 717\u2013726. [Online]. Available: http://doi.acm.org/10.1145/1557019.1557099", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiple non-redundant spectral clustering views", "author": ["D. Niu", "J.G. Dy", "M.I. Jordan"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 831\u2013838.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "For example, simple link-prediction heuristics in social networks tend to perform well because they correspond to the social processes for how networks grow [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "However, content sharing in online social networks shows that \u2018weak\u2019 ties among friends account for much of the influence on users [2].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "The link prediction task [3] predicts edges from \u2018local\u2019", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 234, "endOffset": 237}, {"referenceID": 5, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 239, "endOffset": 242}, {"referenceID": 6, "context": "The collective classification task [7], [8] learns relationships between local neighborhood structure of labels and/or attributes [9] to predict unknown labels in the network.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "The collective classification task [7], [8] learns relationships between local neighborhood structure of labels and/or attributes [9] to predict unknown labels in the network.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "The collective classification task [7], [8] learns relationships between local neighborhood structure of labels and/or attributes [9] to predict unknown labels in the network.", "startOffset": 130, "endOffset": 133}, {"referenceID": 9, "context": "This problem has been extended to joint prediction of edges [10], and higher-order joint prediction [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "This problem has been extended to joint prediction of edges [10], and higher-order joint prediction [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "Network structure inference [12], [13] is a broad area of research aimed at transforming data on individuals or entities into a network representation which can leverage methods such as relational machine learning.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "Network structure inference [12], [13] is a broad area of research aimed at transforming data on individuals or entities into a network representation which can leverage methods such as relational machine learning.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Previous work spans numerous domains including bioinformatics [14], neuroscience [15], and recommender systems [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "Previous work spans numerous domains including bioinformatics [14], neuroscience [15], and recommender systems [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "Previous work spans numerous domains including bioinformatics [14], neuroscience [15], and recommender systems [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "For example, previous work has modeled the \u2018arrival time\u2019 of information in a content network with unknown edges, where rates of transmission are the learned model [17], [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": "For example, previous work has modeled the \u2018arrival time\u2019 of information in a content network with unknown edges, where rates of transmission are the learned model [17], [18].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "These include the Attributed Graph Model (AGM) [19], Multiplicative Attribute Graph model (MAG) [20], and Exponential Random Graph Model (ERGM) [21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "These include the Attributed Graph Model (AGM) [19], Multiplicative Attribute Graph model (MAG) [20], and Exponential Random Graph Model (ERGM) [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "These include the Attributed Graph Model (AGM) [19], Multiplicative Attribute Graph model (MAG) [20], and Exponential Random Graph Model (ERGM) [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "Recent work on task-focused network inference evaluates inferred network models according to their ability to perform a set of tasks [22].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "3) Network density: We evaluate network models at varying density, which has previously been the primary focus of networks inferred on similarity spaces [18], [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "3) Network density: We evaluate network models at varying density, which has previously been the primary focus of networks inferred on similarity spaces [18], [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 7, "context": "1) Collective classification (CC): The collective classification problem learns relationships between network edge structure and attributes and/or labels to predict label values [8], [10].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "1) Collective classification (CC): The collective classification problem learns relationships between network edge structure and attributes and/or labels to predict label values [8], [10].", "startOffset": 183, "endOffset": 187}, {"referenceID": 3, "context": "2) Link prediction (LP): The link prediction problem [4] learns a method for the appearance of edges from one edge-set to another.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Link prediction methods can incorporate attribute and/or label data, or using simple structural ranking [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 22, "context": "2) Community: We calculate structural community labels for each network using the Louvain method [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "The BeerAdvocate review dataset [26] contains 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "The MovieLens 20M ratings dataset [25] contains movie ratings (1-5 stars) data over 138,493 users, through March 2015.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "fm 20K [24] 19,990 1,243,483,909 artist plays 578/1713 artists 3/11 music genres 60 genres", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "MovieLens [25] 138,493 20,000,263 movie ratings 81/407 ratings 7/53 movie tags 100 tags", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "BeerAdvocate [26] 33,387 1,586,259 beer ratings 3/91 ratings 0/3 beer types 45 types", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "This is very surprising from previous results show poor performance of local models on the social network but did not evaluate other localities [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 26, "context": "Model \u2018alternativeness\u2019 refers to discovering maximally different model representations (by some criteria) which satisfy given constraints [27], [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 27, "context": "Model \u2018alternativeness\u2019 refers to discovering maximally different model representations (by some criteria) which satisfy given constraints [27], [28].", "startOffset": 145, "endOffset": 149}], "year": 2017, "abstractText": "Networks are models representing relationships between entities. Often these relationships are explicitly given, or we must learn a representation which generalizes and predicts observed behavior in underlying individual data (e.g. attributes or labels). Whether given or inferred, choosing the best representation affects subsequent tasks and questions on the network. This work focuses on model selection to evaluate network representations from data, focusing on fundamental predictive tasks on networks. We present a modular methodology using general, interpretable network models, task neighborhood functions found across domains, and several criteria for robust model selection. We demonstrate our methodology on three online user activity datasets and show that network model selection for the appropriate network task vs. an alternate task increases performance by an order of magnitude in our experiments.", "creator": "LaTeX with hyperref package"}}}