{"id": "1708.06510", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Handling Homographs in Neural Machine Translation", "abstract": "Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context. However, with the advent of neural machine translation (NMT) systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense be- fore feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.", "histories": [["v1", "Tue, 22 Aug 2017 06:48:27 GMT  (1031kb,D)", "http://arxiv.org/abs/1708.06510v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["frederick liu", "han lu", "graham neubig"], "accepted": false, "id": "1708.06510"}, "pdf": {"name": "1708.06510.pdf", "metadata": {"source": "CRF", "title": "Handling Homographs in Neural Machine Translation", "authors": ["Frederick Liu", "Graham Neubig"], "emails": ["fliu1@cs.cmu.edu", "hlu2@cs.cmu.edu", "gneubig@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to hide. We have to be able to hide, \"he said.\" We have to be able to hide, \"he said."}, {"heading": "2 Neural Machine Translation", "text": "The neural network models the conditional distribution via translations Y = (y1, y2,.., ym) given a sentence in the source language X = (x1, x2,. xn) as P (Y | X). An NMT system consists of an encoder that summarizes the source sentence X as vector representation h, and a decoder that generates a target word (h and previous words) at each time step. Conditional distribution is optimized with a cross-entropy loss at each decoder. The encoder is usually a uni-directional or bi-directional RNN that reads the input sentence word by word. In the more standard bi-directional case, before it is read by the RNN unit, each word in X is assigned to an embed in the space."}, {"heading": "3 NMT\u2019s Problems with Homographs", "text": "As described in Eqs. (2) and (3), NMT models encode the target words using reciprocating encoders and theoretically equip them with the ability to handle homographs through a global sentential context. However, despite the fact that they have this capability, our qualitative observation of the NMT results showed a significant number of ambiguous words that have been mistranslated, raising doubts as to whether the standard NMT setup is capable of learning parameters that make these phrases unambiguous. To demonstrate this more specifically, in Fig. 2 we show the translation accuracy of an NMT system in terms of words of varying ambiguity. Specifically, we use the best basic NMT system to translate three different language pairs from the WMT test sentence (see Fig. 6 for details) and plot the F1 score of the word translations by the number of senses they have."}, {"heading": "4 Neural Word Sense Disambiguation", "text": "In fact, it is so that we will address the ability of the NMT model to choose the correct translation of these ambiguous words, as if we were addressing this problem with neural models, and the way in which they have been identified on WSD datasets (Ka geba ck and Salomonsson, 2016), as if it were a reformed word. We will include three methods for WSD, which we will use further than three different contexts to improve the NMT.Neural networks around which we rely on WSD datasets (Ka geba geba ck and Salomonsson, 2016).Yuan et al, we will include three methods for WSD-T.Neural We will use them as three different contexts to improve the NMT.Neural bag-of words (NBOW) Kalchbrenner al."}, {"heading": "5 Adding Context to NMT", "text": "Now that we have several methods to integrate the global context in relation to a single word, it is necessary to associate this context with NMT. Specifically, we propose two methods to either form a context vector with the lookup embedding M > e \u00b7 1 (xt) to embed a context-sensitive word before inserting it into the encoder, as in Fig. 3. The details of these methods are described below. Gate Inspired by Choi et al. (2017), as our first method for integrating context-sensitive word embedding, we use a gating function like this: f \u2032 e (xt) = fe (xt) \u03c3 (16) = M > e (xt) \u03c3 (ct) (ct) (17) The symbol represents an element-by-element multiplication, and nel is element-by-element sigmoid function. Choi et al. (2017) we use this method in combination of words in the language like the NBOW model."}, {"heading": "6 Experiments", "text": "We evaluate our model based on three different language pairs: English-French (WMT '14) and English-German (WMT' 15), English-Chinese (WMT '17) with English as source language. For German and French, we use a combination of Europarl v7, Common Crawl and News Commentary as training kit. For development sets, newstest2013 is used for German and newstest2012 for French. For Chinese, we use a combination of News Commentary v12 and the CWMT Corpus as training set and have 2357 sentences as development kit. Translation services are reported in case-sensitive BLEU formats for newstest2014 (2737 sentences), newstest2015 (2169 sentences) for German, newstest2013 (3000 sentences), newstest2014 (3003 sentences) for French and newsdev2017 (2002 sentences) for Chinese."}, {"heading": "6.1 Training Details", "text": "In training our NMT systems, following Bahdanau et al. (2015), we filter out pairs of sentences longer than 50 words, moving minibatches. We train our model with the following settings. (1) We start with a learning rate of 1 and start to halve the learning rate in each epoch as soon as it is too high. (2) We train until the model converges. (i.e. the difference between the perplexity for the current epoch and the previous epoch is less than 0.01) (3) We have bundled the instances of the same length and our maximum 3We use the development kit as test data because the official test kit has not been released. 4https: / / sites.google.com / site / iwsltevaluation2015 / trackmt-between the standard size N6 and the standard size N6 (used with the standard size N6)."}, {"heading": "6.2 Experimental Results", "text": "This year it is more than ever before."}, {"heading": "6.3 Targeted Analysis", "text": "To investigate whether our proposed model can better translate words with multiple senses, we evaluate our context-conscious model based on a list of homographs extracted from Wikipedia5, compared to the base model on three different language pairs. For the base model, we select the best-performing model as described in \u00a7 6.2. To do this, we first acquire the translation of homographs in the source language using Fast-Align (Dyer et al., 2013). Since there could be several aligned words in the target language, we use formula 1, precision, and memory as our metrics, since the unattended nature of the algorithm requires a large amount of training data to obtain accurate alignments. Since there may be several aligned words in the source language, we use formula 1, precision, and remember as our metrics, and take the micro-cross section of all sentence pairs. 7 We calculated the values for the 50,000 words / vocabulary from our source language."}, {"heading": "6.4 Qualitative Analysis", "text": "We show sample translations of the Anglo-Chinese WMT '17 dataset in Table 4 with three types of examples: We highlighted the English homograph in bold, correctly translated words in blue and incorrectly translated words in red. (1) Target homographs are translated into the correct sense using the context network. In the first sample translation, \"meets\" is correctly translated into \"\" by our model and translated into \"ig\" incorrectly by the baseline model. In fact, \"closer to the definition\" intentionally come together \"and\" empirical \"is closer to\" satisfactory \"in the English dictionary. (2) Target homographs are translated into different but similar senses for both models in the fourth example. Both models translate the word\" believed \"with common translations,\" \"intentionally come together\" or \"come together,\" but this meaning is both close to the reference translation. \"(3) Target homographs are translated into the base model for the wrong sense, but not into the fifth sense."}, {"heading": "7 Related Work", "text": "Word sense disambiguation (WSD), the task of determining the correct sense or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015). Recent research on managing WSD and recording multisenses includes work with LSTM (Ka geba ck and Salomonsson, 2016; Yuan et al., 2016), which we have expanded as a context network in our work and predict the senses with word embeddings that capture the context (Ka geba ck and Sa-lomonsson, 2016; Yuan et al., 2016)."}, {"heading": "8 Conclusion", "text": "Theoretically, NMT systems should be able to handle homographs when the encoder captures the clues in order to translate them correctly. In this essay, we show empirically that this may not be the case; the performance of word-level translation deteriorates as the number of senses for each word increases. We suspect that this is due to the fact that each word is assigned to a word vector, even though it is located in different contexts, and propose to integrate methods from neural WSD systems into an NMT system to alleviate this problem. We link the context vector calculated from the context network to the word embedding to form a contextaware word embedding that successfully improves the NMT system. We evaluated our model on three different language pairs and surpassed a strong baseline model according to BLEU score in all of words. We evaluated our results from this discussion, our results continued to focus more on the homographs, and the larger translation of the architectures, while the results of our discussion continued to show the homographs in view of the larger architectures."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["References Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "EMNLP. pages 257\u2013267.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "\u201d O\u2019Reilly Media, Inc.\u201d.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Massive exploration of neural machine translation architectures", "author": ["Denny Britz", "Anna Goldie", "Thang Luong", "Quoc Le."], "venue": "arXiv:1703.03906 .", "citeRegEx": "Britz et al\\.,? 2017", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "A unified multilingual semantic representation of concepts", "author": ["Jos\u00e9 Camacho-Collados", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "ACL. pages 741\u2013751.", "citeRegEx": "Camacho.Collados et al\\.,? 2015", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation", "author": ["Marine Carpuat", "Dekai Wu."], "venue": "TMI pages 43\u201352.", "citeRegEx": "Carpuat and Wu.,? 2007a", "shortCiteRegEx": "Carpuat and Wu.", "year": 2007}, {"title": "Improving statistical machine translation using word sense disambiguation", "author": ["Marine Carpuat", "Dekai Wu."], "venue": "EMNLP-CoNLL. pages 61\u201372.", "citeRegEx": "Carpuat and Wu.,? 2007b", "shortCiteRegEx": "Carpuat and Wu.", "year": 2007}, {"title": "Word sense disambiguation improves statistical machine translation", "author": ["Yee Seng Chan", "Hwee Tou Ng", "David Chiang."], "venue": "ACL. volume 45, pages 33\u201340.", "citeRegEx": "Chan et al\\.,? 2007", "shortCiteRegEx": "Chan et al\\.", "year": 2007}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP. pages 1025\u20131035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Context-dependent word representation for neural machine translation", "author": ["Heeyoul Choi", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1607.00578 .", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Clustering and diversifying web search results with graphbased word sense induction", "author": ["Antonio Di Marco", "Roberto Navigli."], "venue": "Computational Linguistics 39(3):709\u2013754.", "citeRegEx": "Marco and Navigli.,? 2013", "shortCiteRegEx": "Marco and Navigli.", "year": 2013}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A Smith."], "venue": "NAACL-HLT, pages 644\u2013648.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daum\u00e9 III."], "venue": "ACL. pages 1681\u20131691.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Word sense disambiguation using a bidirectional lstm", "author": ["Mikael K\u00e5geb\u00e4ck", "Hans Salomonsson."], "venue": "COLING 2016 page 51.", "citeRegEx": "K\u00e5geb\u00e4ck and Salomonsson.,? 2016", "shortCiteRegEx": "K\u00e5geb\u00e4ck and Salomonsson.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "ACL pages 212\u2013217.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Learning to represent words in context with multilingual supervision", "author": ["Kazuya Kawakami", "Chris Dyer."], "venue": "ICLR workshop .", "citeRegEx": "Kawakami and Dyer.,? 2016", "shortCiteRegEx": "Kawakami and Dyer.", "year": 2016}, {"title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "author": ["G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush."], "venue": "arXiv:1701.02810 .", "citeRegEx": "Klein et al\\.,? 2017", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "EMNLP. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "NAACL. pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Senselearner: Minimally supervised word sense disambiguation for all words in open text", "author": ["Rada Mihalcea", "Ehsanul Faruque."], "venue": "ACL/SIGLEX. volume 3, pages 155\u2013158.", "citeRegEx": "Mihalcea and Faruque.,? 2004", "shortCiteRegEx": "Mihalcea and Faruque.", "year": 2004}, {"title": "Word sense disambiguation: A survey", "author": ["Roberto Navigli."], "venue": "ACM Computing Surveys (CSUR) 41(2):10.", "citeRegEx": "Navigli.,? 2009", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "Neural reranking improves subjective quality of machine translation: Naist at wat2015", "author": ["Graham Neubig", "Makoto Morishita", "Satoshi Nakamura."], "venue": "WAT .", "citeRegEx": "Neubig et al\\.,? 2015", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach", "author": ["Hwee Tou Ng", "Hian Beng Lee."], "venue": "ACL. pages 40\u201347.", "citeRegEx": "Ng and Lee.,? 1996", "shortCiteRegEx": "Ng and Lee.", "year": 1996}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders", "author": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord."], "venue": "NAACL-HLT pages 1346\u2013 1356.", "citeRegEx": "\u0160uster et al\\.,? 2016", "shortCiteRegEx": "\u0160uster et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word-sense disambiguation for machine translation", "author": ["David Vickrey", "Luke Biewald", "Marc Teyssier", "Daphne Koller."], "venue": "HLT-EMNLP. pages 771\u2013778.", "citeRegEx": "Vickrey et al\\.,? 2005", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}, {"title": "A sense-based translation model for statistical machine translation", "author": ["Deyi Xiong", "Min Zhang."], "venue": "ACL. pages 1459\u20131469.", "citeRegEx": "Xiong and Zhang.,? 2014", "shortCiteRegEx": "Xiong and Zhang.", "year": 2014}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky."], "venue": "ACL. pages 189\u2013196.", "citeRegEx": "Yarowsky.,? 1995", "shortCiteRegEx": "Yarowsky.", "year": 1995}, {"title": "Semi-supervised word sense disambiguation with neural models", "author": ["Dayu Yuan", "Julian Richardson", "Ryan Doherty", "Colin Evans", "Eric Altendorf."], "venue": "arXiv:1603.07012 .", "citeRegEx": "Yuan et al\\.,? 2016", "shortCiteRegEx": "Yuan et al\\.", "year": 2016}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng."], "venue": "ACL. pages 78\u201383.", "citeRegEx": "Zhong and Ng.,? 2010", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}], "referenceMentions": [{"referenceID": 27, "context": "Neural machine translation (NMT; Sutskever et al. (2014); Bahdanau et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2015), \u00a72), a method for MT that performs translation in an end-toend fashion using neural networks, is quickly becoming the de-facto standard in MT applications due to its impressive empirical results.", "startOffset": 8, "endOffset": 31}, {"referenceID": 25, "context": "(2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016).", "startOffset": 70, "endOffset": 116}, {"referenceID": 1, "context": "(2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016).", "startOffset": 70, "endOffset": 116}, {"referenceID": 18, "context": "based MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b) or phrasesense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena.", "startOffset": 117, "endOffset": 140}, {"referenceID": 5, "context": "As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b) or phrasesense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena.", "startOffset": 156, "endOffset": 179}, {"referenceID": 16, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 14, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 15, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 32, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 27, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 9, "context": "In order to incorporate this information into NMT, we examine two methods: gating the word-embeddings in the model (similarly to Choi et al. (2017)), and concatenating the context-aware representation to the word embedding (\u00a75).", "startOffset": 129, "endOffset": 148}, {"referenceID": 22, "context": "To evaluate the effectiveness of our method, we compare our context-aware models with a strong baseline (Luong et al., 2015) on the EnglishGerman, English-French, and English-Chinese WMT dataset.", "startOffset": 104, "endOffset": 124}, {"referenceID": 22, "context": "We follow the global-general-attention NMT architecture with input-feeding proposed by Luong et al. (2015), which we will briefly summarize here.", "startOffset": 87, "endOffset": 107}, {"referenceID": 13, "context": "The recurrent units \u2212\u2212\u2192 RNNe and \u2190\u2212\u2212 RNNe are usually either LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al.", "startOffset": 67, "endOffset": 101}, {"referenceID": 10, "context": "The recurrent units \u2212\u2212\u2192 RNNe and \u2190\u2212\u2212 RNNe are usually either LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al., 2014).", "startOffset": 110, "endOffset": 130}, {"referenceID": 12, "context": "We evaluate the translation performance of words in the source side by aligning them to the target side using fast-align (Dyer et al., 2013).", "startOffset": 121, "endOffset": 140}, {"referenceID": 2, "context": "dictionary/english/ We use the stop word list from NLTK (Bird et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 26, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 23, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 33, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 8, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 4, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 15, "context": "Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016).", "startOffset": 111, "endOffset": 162}, {"referenceID": 32, "context": "Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016).", "startOffset": 111, "endOffset": 162}, {"referenceID": 15, "context": "Neural bag-of-words (NBOW) Kalchbrenner et al. (2014); Iyyer et al.", "startOffset": 27, "endOffset": 54}, {"referenceID": 14, "context": "(2014); Iyyer et al. (2015) have shown success by representing full sentences with a context vector, which is the average of the Lookup embeddings of the input sequence", "startOffset": 8, "endOffset": 28}, {"referenceID": 15, "context": "Bi-directional LSTM (BiLSTM) K\u00e5geb\u00e4ck and Salomonsson (2016) leveraged a bidirectional LSTM that learns a context vector for the target word in the input sequence and predict the word sense with a multi-layer perceptron.", "startOffset": 29, "endOffset": 61}, {"referenceID": 32, "context": "Held-out LSTM (HoLSTM) Yuan et al. (2016) trained a LSTM language model, which predicts a held-out word given the surrounding context, with a large amount of unlabeled text as training data.", "startOffset": 23, "endOffset": 42}, {"referenceID": 9, "context": "Gate Inspired by Choi et al. (2017), as our first method for integration of context-aware word embeddings, we use a gating function as follows:", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "Choi et al. (2017) use this method in concert with averaged embeddings from words in source language like the NBOW model above, which naturally uses the same context vectors for all time steps.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "(2015); for French, we used the moses (Koehn et al., 2007) tokenization script with the \u201c-a\u201d flag; for Chinese, we split sequences of Chinese characters, but keeps sequences of non-Chinese characters as they are, using the script from IWSLT Evaluation 2015.", "startOffset": 38, "endOffset": 58}, {"referenceID": 19, "context": "For German, we use the tokenized dataset from Luong et al. (2015); for French, we used the moses (Koehn et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "When training our NMT systems, following Bahdanau et al. (2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle minibatches as we proceed.", "startOffset": 41, "endOffset": 64}, {"referenceID": 18, "context": "Our model is built upon OpenNMT (Klein et al., 2017) with the default settings unless otherwise noted.", "startOffset": 32, "endOffset": 52}, {"referenceID": 19, "context": "001) than baseline models using paired bootstrap resampling (Koehn, 2004).", "startOffset": 60, "endOffset": 73}, {"referenceID": 3, "context": "It should be noted that previous work such as Britz et al. (2017) have", "startOffset": 46, "endOffset": 66}, {"referenceID": 12, "context": "To do so, we first acquire the translation of homographs in the source language using fast-align (Dyer et al., 2013).", "startOffset": 97, "endOffset": 116}, {"referenceID": 31, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 26, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 23, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 24, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 33, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 8, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 4, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 15, "context": "Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word embeddings that capture context (K\u00e5geb\u00e4ck and Sa-", "startOffset": 89, "endOffset": 140}, {"referenceID": 32, "context": "Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word embeddings that capture context (K\u00e5geb\u00e4ck and Sa-", "startOffset": 89, "endOffset": 140}, {"referenceID": 26, "context": "\u0160uster et al. (2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "(2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD.", "startOffset": 8, "endOffset": 33}, {"referenceID": 26, "context": "In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation.", "startOffset": 45, "endOffset": 67}, {"referenceID": 5, "context": "Following this reformulation, Chan et al. (2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems.", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": "(2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems. Xiong and Zhang (2014) breaks the process into two stages.", "startOffset": 8, "endOffset": 106}, {"referenceID": 9, "context": "Choi et al. (2017) leverage the NBOW as context and gate the word-embedding on both encoder and decoder side.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context. However, with the advent of neural machine translation (NMT) systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense before feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.", "creator": "LaTeX with hyperref package"}}}