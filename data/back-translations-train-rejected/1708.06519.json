{"id": "1708.06519", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Learning Efficient Convolutional Networks through Network Slimming", "abstract": "The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.", "histories": [["v1", "Tue, 22 Aug 2017 07:35:26 GMT  (2089kb,D)", "http://arxiv.org/abs/1708.06519v1", "Accepted by ICCV 2017"]], "COMMENTS": "Accepted by ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zhuang liu", "jianguo li", "zhiqiang shen", "gao huang", "shoumeng yan", "changshui zhang"], "accepted": false, "id": "1708.06519"}, "pdf": {"name": "1708.06519.pdf", "metadata": {"source": "CRF", "title": "Learning Efficient Convolutional Networks through Network Slimming", "authors": ["Zhuang Liu", "Jianguo Li", "Zhiqiang Shen", "Gao Huang", "Shoumeng Yan", "Changshui Zhang"], "emails": ["zhiqiangshen0214}@gmail.com,", "shoumeng.yan}@intel.com,", "gh349@cornell.edu,", "zcs@mail.tsinghua.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "In recent years, it has become clear that most of them are people who are unable to survive on their own."}, {"heading": "2. Related Work", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "3. Network slimming", "text": "In this section we will first discuss the advantages and challenges of parsimony at the channel level and then present how to implement the levels of scaling at different levels, e.g. at the weight level, at the channel level or at the layer level, the unimportant channels in the network. The advantages of parsimony at the channel level (e.g. at the weight class level) requires parsimony, which leads to greater flexibility and generalization, but it usually requires special software or hardware accelerators to make fast inferences on the frugal model. On the contrary, the coarsest layer level does not require special packages to capture the inference speed, while it is less flexible."}, {"heading": "4. Experiments", "text": "We demonstrate empirically the effectiveness of network slimness using multiple benchmark data sets. We implement our method based on the publicly available Torch [5] implementation for ResNets by [10], available at https: / / github.com / liuzhuang13 / slimming."}, {"heading": "4.1. Datasets", "text": "The two CIFAR datasets [21] consist of natural images with a resolution of 32 \u00d7 32. CIFAR-10 is drawn from 10 and CIFAR-100 from 100 classes. The traction and test sets contain 50,000 and 10,000 images, respectively. On CIFAR-10, a validation set of 5,000 images for the search for \u03bb (in Equation 1) is split between each model. We report the final test errors after training or fine-tuning on all training images. A standard data augmentation scheme (shift / mirroring) [14, 18, 24] is adopted. Input data is normalized by channel means and standard deviations. We also compare our method with [23] on CIFAR datasets. SVHN. The Street View House Number (SVHN) dataset [27] consists of 32 x 32 color digital images."}, {"heading": "4.2. Network Models", "text": "For the CIFAR and SVHN datasets, we evaluate our method on three popular network architectures: VGGNet [31], ResNet [14], and DenseNet [17]. VGGNet was originally designed for the ImageNet classification; for our experiment, we used a variation of the original VGGNet for CIFAR dataset [36]; for ResNet, we used a 164-layer ResNet with bottleneck structure (ResNet-164) [15]; for DenseNet, we used a 40-layer DenseNet with growth rate 12 (DenseNet-40); for ImageNet, we used the 11-layer (8-conv + 3 FC) \"VGG-A\" network [31] model with batch normalization starting [4]; we removed the failure layers because we use relatively heavy data augmentation; to trim the neurons in fully connected layers, we treat them like the network on the revolutionary IST-1 [1]."}, {"heading": "4.3. Training, Pruning and Fine-tuning", "text": "On CIFAR and SVHN data sets, we train with minibatch size 64 for 160 and 20 epochs, respectively, and the initial learning rate is set to 0.1 and divided by 10 at 50% and 75% of the total number of training epochs. On ImageNet and MNIST data sets, we train our models for 60 and 30 epochs, respectively, with a batch size of 256, and an initial learning rate of 0.1, divided by 10 at 1 / 3 and 2 / 3 fractions of the training epochs. On ImageNet and MNIST data sets, we set our models for 60 \u2212 4 and 30 epochs, with a batch size of 256, and an initial learning rate of 0.1, divided by 10 at 1 / 3 and 2 / 3 fractions of the training epochs."}, {"heading": "4.4. Results", "text": "The results of this study are something like this: \"There is no need to panic,\" he said, \"but there is also no reason why we should not be able to solve the problems.\""}, {"heading": "4.5. Results for Multi-pass Scheme", "text": "Since there are no skip connections, removing an entire layer completely destroys the models. Therefore, in addition to setting the percentile threshold of 50%, we also set the limitation that a maximum of 50% of the channels can be clipped on each layer. Test errors of the models in each iteration are shown in Table 4. As the trimming process progresses, we get more and more compact models. On CIFAR-10, the trained model achieves the lowest test error in iteration 5. This model achieves a 20-fold parameter reduction and a 5-fold FLOP reduction while achieving lower test errors. On CIFAR-100, test errors begin to rise after iteration 3. This may be because it contains more classes than CIFAR-10, which makes cutting channels too aggressive."}, {"heading": "5. Analysis", "text": "There are two critical hyperparameters in network thinning, the truncated percentage t and the coefficient of the sparseness regulation term \u03bb (see Equation 1). In this section, we analyze their effects in more detail. Effect of the truncated percentage. Once we get a model that has been built with the sparseness regulation, we need to decide what percentage of channels should be truncated from the model. If we truncate too few channels, the resource savings can be very limited. However, it could be destructive for the model if we truncate too many channels, and it may not be possible to restore accuracy by fine tuning. We train a DenseNet40 model with \u03bb = 10 \u2212 5 on CIFAR-10 to show the effects of truncating a varying percentage of channels. The results are summarized in Figure 5.From Figure 5, one can conclude that the classification performance of the truncated or truncated models is exceeded when only the threshold is truncated."}, {"heading": "6. Conclusion", "text": "Based on several datasets, we have shown that the proposed method is capable of significantly reducing the computing costs (up to 20x) of state-of-the-art networks without loss of accuracy. More importantly, the proposed method simultaneously reduces the model size, runtime memory and computational operations, while introducing minimal overhead for the training process, and the resulting models do not require special libraries / hardware for efficient inference. It is known that Gao Huang is supported by the International Postdoctoral Exchange Fellowship Program of China Postdoctoral Council (No.20150015). Changshui Zhang is supported by NSFC and DFG Joint Project NSFC 61621136008 / DFG TRR-169."}, {"heading": "A. Detailed Structure of a Compact Network", "text": "We show a detailed structure of a compact VGA network on the CIFAR-10 dataset in Table 5. The compact model used is from the multi-pass scheme experiment (\"Iter 5 Trained\" from Table 4 (a)). We observe that deeper layers tend to prune more channels."}, {"heading": "B. Wall-clock Time and Run-time Memory", "text": "SavingWe test the wall clock speed and memory requirements of a \"70% trimmed\" VGA network (Table 1 (a) on CIFAR-10. The experiment is performed with a torch [5] on an NVIDIA GeForce 1080 GPU in batch size 64. The result is in Table 6. The wall clock saving of this model is roughly equivalent to the FLOP storage shown in Table 1 (a), although the memory saving is not as significant. This is because deeper layers that have smaller activation cards and use less memory tend to have more trimmed channels, such as Table 5. Note that any savings do not require special libraries / hardware."}, {"heading": "C. Comparison with [23]", "text": "For CIFAR-10 and CIFAR-100 datasets, we compare our method with an earlier cutting technique [23]. In contrast to network slimness, which cuts channels with a global cutting threshold, [23] different predefined channel portions are cut at different levels. To make a comparison, we use the cutting criterion introduced in [23] and closely follow the strategy of layer section [23] to VGGNet [36]. The result is in Table 7. Compared to [23], network slimness leads to significantly lower test errors at a similar compression rate."}], "references": [{"title": "Designing neural network architectures using reinforcement learning", "author": ["B. Baker", "O. Gupta", "N. Naik", "R. Raskar"], "venue": "ICLR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "The power of sparsity in convolutional neural networks", "author": ["S. Changpinyo", "M. Sandler", "A. Zhmoginov"], "venue": "arXiv preprint arXiv:1702.06257,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "ICML,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "NIPS,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, pages 580\u2013587,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "ICLR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "NIPS, pages 1135\u20131143,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV, pages 630\u2013645. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-scale dense convolutional networks for efficient prediction", "author": ["G. Huang", "D. Chen", "T. Li", "F. Wu", "L. van der Maaten", "K.Q. Weinberger"], "venue": "arXiv preprint arXiv:1703.09844,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Densely connected convolutional networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "ECCV,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network architecture optimization through submodularity and supermodularity", "author": ["J. Jin", "Z. Yan", "K. Fu", "N. Jiang", "C. Zhang"], "venue": "arXiv preprint arXiv:1609.00074,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Tech Report,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Pruning filters for efficient convnets", "author": ["H. Li", "A. Kadav", "I. Durdanovic", "H. Samet", "H.P. Graf"], "venue": "arXiv preprint arXiv:1608.08710,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse convolutional neural networks", "author": ["B. Liu", "M. Wang", "H. Foroosh", "M. Tappen", "M. Pensky"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 806\u2013814,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, pages 3431\u2013 3440,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning, 2011", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Xnornet: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "ECCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Group sparse regularization for deep neural networks", "author": ["S. Scardapane", "D. Comminiello", "A. Hussain", "A. Uncini"], "venue": "arXiv preprint arXiv:1607.00485,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast optimization methods for l1 regularization: A comparative study and two new approaches", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "ECML, pages 286\u2013297,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Training sparse neural networks", "author": ["S. Srinivas", "A. Subramanya", "R.V. Babu"], "venue": "CoRR, abs/1611.06694,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "ICML,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Learning structured sparsity in deep neural networks", "author": ["W. Wen", "C. Wu", "Y. Wang", "Y. Chen", "H. Li"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Less is more: Towards compact cnns", "author": ["H. Zhou", "J.M. Alvarez", "F. Porikli"], "venue": "ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["B. Zoph", "Q.V. Le"], "venue": "ICLR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 19, "context": ", image classification [22], object detection [8], semantic segmentation [26].", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": ", image classification [22], object detection [8], semantic segmentation [26].", "startOffset": 46, "endOffset": 49}, {"referenceID": 23, "context": ", image classification [22], object detection [8], semantic segmentation [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "For instance, from AlexNet [22], VGGNet [31] and GoogleNet [34] to ResNets [14], the ImageNet Classification Challenge winner models have evolved from 8 layers to more than 100 layers.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "For instance, from AlexNet [22], VGGNet [31] and GoogleNet [34] to ResNets [14], the ImageNet Classification Challenge winner models have evolved from 8 layers to more than 100 layers.", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "For instance, from AlexNet [22], VGGNet [31] and GoogleNet [34] to ResNets [14], the ImageNet Classification Challenge winner models have evolved from 8 layers to more than 100 layers.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "For instance, from AlexNet [22], VGGNet [31] and GoogleNet [34] to ResNets [14], the ImageNet Classification Challenge winner models have evolved from 8 layers to more than 100 layers.", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "For instance, a 152-layer ResNet [14] has more than 60 million parameters and requires more than 20 Giga float-point-operations (FLOPs) when inferencing an image with resolution 224\u00d7 224.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc.", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc.", "startOffset": 63, "endOffset": 70}, {"referenceID": 9, "context": "These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc.", "startOffset": 63, "endOffset": 70}, {"referenceID": 25, "context": "These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc.", "startOffset": 88, "endOffset": 95}, {"referenceID": 4, "context": "These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc.", "startOffset": 88, "endOffset": 95}, {"referenceID": 9, "context": "These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc.", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc.", "startOffset": 136, "endOffset": 140}, {"referenceID": 25, "context": "Moreover, some of the techniques require specially designed software/hardware accelerators for execution speedup [28, 6, 12].", "startOffset": 113, "endOffset": 124}, {"referenceID": 4, "context": "Moreover, some of the techniques require specially designed software/hardware accelerators for execution speedup [28, 6, 12].", "startOffset": 113, "endOffset": 124}, {"referenceID": 9, "context": "Moreover, some of the techniques require specially designed software/hardware accelerators for execution speedup [28, 6, 12].", "startOffset": 113, "endOffset": 124}, {"referenceID": 1, "context": "Sparsity can be imposed on different level of structures [2, 37, 35, 29, 25], which yields considerable model-size compression and inference speedup.", "startOffset": 57, "endOffset": 76}, {"referenceID": 33, "context": "Sparsity can be imposed on different level of structures [2, 37, 35, 29, 25], which yields considerable model-size compression and inference speedup.", "startOffset": 57, "endOffset": 76}, {"referenceID": 32, "context": "Sparsity can be imposed on different level of structures [2, 37, 35, 29, 25], which yields considerable model-size compression and inference speedup.", "startOffset": 57, "endOffset": 76}, {"referenceID": 26, "context": "Sparsity can be imposed on different level of structures [2, 37, 35, 29, 25], which yields considerable model-size compression and inference speedup.", "startOffset": 57, "endOffset": 76}, {"referenceID": 22, "context": "Sparsity can be imposed on different level of structures [2, 37, 35, 29, 25], which yields considerable model-size compression and inference speedup.", "startOffset": 57, "endOffset": 76}, {"referenceID": 9, "context": "quire special software/hardware accelerators to harvest the gain in memory or time savings, though it is easier than non-structured sparse weight matrix as in [12].", "startOffset": 159, "endOffset": 163}, {"referenceID": 5, "context": "Low-rank Decomposition approximates weight matrix in neural networks with low-rank matrix using techniques like Singular Value Decomposition (SVD) [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "HashNet [3] proposes to quantize the network weights.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "[12] uses a improved quantization technique in a deep compression pipeline and achieves 35x to 49x compression rates on AlexNet and VGGNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28, 6] quantize real-valued weights into binary/ternary weights (weight values restricted to {\u22121, 1} or {\u22121, 0, 1}).", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "[28, 6] quantize real-valued weights into binary/ternary weights (weight values restricted to {\u22121, 1} or {\u22121, 0, 1}).", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[12] proposes to prune the unimportant connections with small weights in trained neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "In [12], there is no guidance for sparsity during training.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "[32] overcomes this limitation by explicitly imposing sparse constraint over each weight with additional gate variables, and achieve high compression rates by pruning connections with zero gate values.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "pression rate than [12], but suffers from the same drawback.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "Recently, [23] proposes to prune channels with small incoming weights in trained CNNs, and then fine-tune the network to regain accuracy.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "[2] introduces sparsity by random deactivating input-output channel-wise connections in convolutional layers before training, which also yields smaller networks with moderate accuracy loss.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[37] imposes neuron-level sparsity during training thus some neurons could be pruned to obtain compact networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] proposes a Structured Sparsity Learning (SSL) method to sparsify different level of structures (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "While state-of-the-art CNNs are typically designed by experts [22, 31, 14], there are also some explorations on automatically learning network architectures.", "startOffset": 62, "endOffset": 74}, {"referenceID": 28, "context": "While state-of-the-art CNNs are typically designed by experts [22, 31, 14], there are also some explorations on automatically learning network architectures.", "startOffset": 62, "endOffset": 74}, {"referenceID": 11, "context": "While state-of-the-art CNNs are typically designed by experts [22, 31, 14], there are also some explorations on automatically learning network architectures.", "startOffset": 62, "endOffset": 74}, {"referenceID": 17, "context": "[20] introduces sub-modular/supermodular optimization for network architecture search with a given resource budget.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Some recent works [38, 1] propose to learn neural architecture automatically with reinforcement learning.", "startOffset": 18, "endOffset": 25}, {"referenceID": 0, "context": "Some recent works [38, 1] propose to learn neural architecture automatically with reinforcement learning.", "startOffset": 18, "endOffset": 25}, {"referenceID": 32, "context": "As discussed in prior works [35, 23, 11], sparsity can be realized at different levels, e.", "startOffset": 28, "endOffset": 40}, {"referenceID": 20, "context": "As discussed in prior works [35, 23, 11], sparsity can be realized at different levels, e.", "startOffset": 28, "endOffset": 40}, {"referenceID": 8, "context": "As discussed in prior works [35, 23, 11], sparsity can be realized at different levels, e.", "startOffset": 28, "endOffset": 40}, {"referenceID": 8, "context": ", weight-level) sparsity gives the highest flexibility and generality leads to higher compression rate, but it usually requires special software or hardware accelerators to do fast inference on the sparsified model [11].", "startOffset": 215, "endOffset": 219}, {"referenceID": 32, "context": ", more than 50 layers [35, 18].", "startOffset": 22, "endOffset": 30}, {"referenceID": 15, "context": ", more than 50 layers [35, 18].", "startOffset": 22, "endOffset": 30}, {"referenceID": 20, "context": "As reported in [23], pruning channels on pre-trained ResNets can only lead to a reduction of\u223c10% in the number of parameters without suffering from accuracy loss.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "[35] addresses this problem by enforcing sparsity regularization into the training objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "An alternative option is to replace the L1 penalty with the smooth-L1 penalty [30] to avoid using sub-gradient at non-smooth point.", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "Batch normalization [19] has been adopted by most modern CNNs as a standard approach to achieve fast convergence and better generalization performance.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "The network slimming process introduced above can be directly applied to most plain CNN architectures such as AlexNet [22] and VGGNet [31].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "The network slimming process introduced above can be directly applied to most plain CNN architectures such as AlexNet [22] and VGGNet [31].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "While some adaptations are required when it is applied to modern networks with cross layer connections and the pre-activation design such as ResNet [15] and DenseNet [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "While some adaptations are required when it is applied to modern networks with cross layer connections and the pre-activation design such as ResNet [15] and DenseNet [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "our method based on the publicly available Torch [5] implementation for ResNets by [10].", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "The two CIFAR datasets [21] consist of natural images with resolution 32\u00d732.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "A standard data augmentation scheme (shifting/mirroring) [14, 18, 24] is adopted.", "startOffset": 57, "endOffset": 69}, {"referenceID": 15, "context": "A standard data augmentation scheme (shifting/mirroring) [14, 18, 24] is adopted.", "startOffset": 57, "endOffset": 69}, {"referenceID": 21, "context": "A standard data augmentation scheme (shifting/mirroring) [14, 18, 24] is adopted.", "startOffset": 57, "endOffset": 69}, {"referenceID": 20, "context": "We also compare our method with [23] on CIFAR datasets.", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "The Street View House Number (SVHN) dataset [27] consists of 32x32 colored digit images.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "Following common practice [9, 18, 24] we use all the 604,388 training images, from which we split a validation set of 6,000 images for model selection during training.", "startOffset": 26, "endOffset": 37}, {"referenceID": 15, "context": "Following common practice [9, 18, 24] we use all the 604,388 training images, from which we split a validation set of 6,000 images for model selection during training.", "startOffset": 26, "endOffset": 37}, {"referenceID": 21, "context": "Following common practice [9, 18, 24] we use all the 604,388 training images, from which we split a validation set of 6,000 images for model selection during training.", "startOffset": 26, "endOffset": 37}, {"referenceID": 32, "context": "To test the effectiveness of our method on a fully-connected network (treating each neuron as a channel with 1\u00d71 spatial size), we compare our method with [35] on this dataset.", "startOffset": 155, "endOffset": 159}, {"referenceID": 28, "context": "On CIFAR and SVHN dataset, we evaluate our method on three popular network architectures: VGGNet[31], ResNet [14] and DenseNet [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "On CIFAR and SVHN dataset, we evaluate our method on three popular network architectures: VGGNet[31], ResNet [14] and DenseNet [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "On CIFAR and SVHN dataset, we evaluate our method on three popular network architectures: VGGNet[31], ResNet [14] and DenseNet [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "For ResNet, a 164-layer pre-activation ResNet with bottleneck structure (ResNet-164) [15] is used.", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": "On ImageNet dataset, we adopt the 11-layer (8-conv + 3 FC) \u201cVGG-A\u201d network [31] model with batch normalization from [4].", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": "On MNIST dataset, we evaluate our method on the same 3-layer fully-connected network as in [35].", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "We use a weight decay of 10\u22124 and a Nesterov momentum [33] of 0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "The weight initialization introduced by [13] is adopted.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "Unlike in [23] where different layers are pruned by different ratios, we use a global pruning threshold for simplicity.", "startOffset": 10, "endOffset": 14}, {"referenceID": 32, "context": "43 - 784-500-300-10 Pruned [35] 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "It is worth noting that our method can achieve the savings with no accuracy loss on the 1000-class ImageNet dataset, where other methods for efficient CNNs [2, 23, 35, 28] mostly report accuracy loss.", "startOffset": 156, "endOffset": 171}, {"referenceID": 20, "context": "It is worth noting that our method can achieve the savings with no accuracy loss on the 1000-class ImageNet dataset, where other methods for efficient CNNs [2, 23, 35, 28] mostly report accuracy loss.", "startOffset": 156, "endOffset": 171}, {"referenceID": 32, "context": "It is worth noting that our method can achieve the savings with no accuracy loss on the 1000-class ImageNet dataset, where other methods for efficient CNNs [2, 23, 35, 28] mostly report accuracy loss.", "startOffset": 156, "endOffset": 171}, {"referenceID": 25, "context": "It is worth noting that our method can achieve the savings with no accuracy loss on the 1000-class ImageNet dataset, where other methods for efficient CNNs [2, 23, 35, 28] mostly report accuracy loss.", "startOffset": 156, "endOffset": 171}, {"referenceID": 32, "context": "On MNIST dataset, we compare our method with the Structured Sparsity Learning (SSL) method [35] in Table 3.", "startOffset": 91, "endOffset": 95}, {"referenceID": 32, "context": "Our method slightly outperforms [35], in that a slightly lower test error is achieved while pruning more parameters.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "(3) comparison with a previous channel pruning method [23];", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20\u00d7 reduction in model size and a 5\u00d7 reduction in computing operations.", "creator": "LaTeX with hyperref package"}}}