{"id": "1708.06551", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Reinforcement Learning in POMDPs with Memoryless Options and Option-Observation Initiation Sets", "abstract": "Most real-world reinforcement learning problems have a hierarchical nature, and often exhibit some degree of partial observability. While hierarchy and partial observability are usually tackled separately, for instance by combining recurrent neural networks and options, we show that addressing both problems simultaneously is simpler and more efficient in many cases. More specifically, we make the initiation set of options conditional on the previously-executed option, and show that options with such Option-Observation Initiation Sets (OOIs) are at least as expressive as Finite State Controllers (FSCs), a state-of-the-art approach for learning in POMDPs. In contrast to other hierarchical methods in partially observable environments, OOIs are easy to design based on an intuitive description of the task, lead to explainable policies and keep the top-level and option policies memoryless. Our experiments show that OOIs allow agents to learn optimal policies in challenging POMDPs, outperforming an human-provided policy in our robotic experiment, while learning much faster than a recurrent neural network over options.", "histories": [["v1", "Tue, 22 Aug 2017 09:51:18 GMT  (1071kb,D)", "http://arxiv.org/abs/1708.06551v1", null], ["v2", "Tue, 12 Sep 2017 08:34:04 GMT  (1099kb,D)", "http://arxiv.org/abs/1708.06551v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["denis steckelmacher", "diederik m roijers", "anna harutyunyan", "peter vrancx", "h\\'el\\`ene plisnier", "ann now\\'e"], "accepted": false, "id": "1708.06551"}, "pdf": {"name": "1708.06551.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning in POMDPs with Memoryless Options and Option-Observation Initiation Sets", "authors": ["Denis Steckelmacher", "Diederik M. Roijers", "Anna Harutyunyan", "Peter Vrancx", "Ann Now\u00e9"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us will be able to play by the rules we have set ourselves in order to make them a reality."}, {"heading": "1.1 Motivating Example", "text": "OOIs are designed to solve complex, partially observable tasks that can be broken down into a series of fully observable sub-tasks. For example, a robot with first-person sensors may be able to avoid obstacles, open doors, or manipulate objects even if its exact position in a building is not observed. We now present an environment where our robotic experiments are separated by a wall and brought to a central location. Objects must be collected from a terminal until they require many journeys between the root and a terminal."}, {"heading": "2 Background", "text": "This section officially introduces Markov Decision Processes (MDPs), Options, Partially Observable MDPs (POMDPs) and Finite State Controllers before we present our main contribution in Section 3."}, {"heading": "2.1 Markov Decision Processes", "text": "A discrete Markov decision-making process (MDP) < S, A, R, T, \u03b3 > with discrete actions is defined by a possibly infinite amount S of states, a finite amount A of actions, a reward function R (st, at, st + 1) and R providing a scalable reward rt for each transition between states, a transitional function T (st, at, st + 1) issuing a probability distribution over new states st + 1 indicating a (st, at) state-shareholder pair, and 0 \u2264 \u03b3 < 1 the discount factor defining how sensitive the actor should be to future rewards. A stochastic, unthinking policy \u03c0 (st, at) establishes a state of probability distribution over actions. The goal of the actor is to find a policy that maximizes the expected cumulative discounted reward attainable by following this policy."}, {"heading": "2.2 Options", "text": "The option box, defined in the MDPs of Sutton et al., consists of a set of options O where each option is a tuple < \u03c0\u03c9, I\u03c9, \u03b2\u03c9 >, with \u03c0\u03c9 (st, at). [0, 1] the memory-less option policy, \u03b2\u03c9 (st). [0, 1] the termination function, which indicates the probability of terminating the option in state st, and I\u03c9 S the initiation set, which defines in which states \u03c9 can be started (Sutton et al. 1999). The memory-less policy on options \u00b5 (st, \u03c9t). [0, 1] assigns the states to a distribution of options and allows you to choose which option to start in a given state. If an option \u03c9 is started, it expires until termination (on the basis of \u03b2\u043e), selecting a new option based on the current state."}, {"heading": "2.3 Partially Observable MDPs", "text": "Most real-world problems are not MDPs and have at least some degree of partial observability. A Partially Observable MDP (POMDP) < B, S, A, R, T, O, \u03b3 > is an MDP that is augmented by two components: the potentially infinite number of observations and the O: S function that generates observations x based on the hidden state of the process. Two different states that require two different optimal actions can produce the same observation, which generally makes POMDPs for enhanced learning algorithms insufficient as memoryless policies that select actions or options based only on the current observation."}, {"heading": "2.4 Finite State Controllers", "text": "Finite state controllers (FSCs) are state-of-the-art when it comes to displaying strategies that work well in POMDPs. An FSC < N, \u0432, \u03b7, \u03b70 > is defined by a finite number of nodes, an action function that maps nodes to a probability distribution via actions, a successor function that maps nodes (nt \u2212 1, xt, nt) to a probability distribution via next nodes, and an initial function \u03b70 (x1, n1) that maps initial observations on nodes (Meuleau et al. 1999). In the first step, the agent observes x1 and activates a node n1 by scanning Schwaz 0 (x1, \u00b7). An action is performed by scanning Schwaz (n1, \u00b7)."}, {"heading": "3 Option-Observation Initiation Sets", "text": "Our main article, Option-Observation Initiation Sets (OOIs), makes option initiation dependent on the option that just ended. We prove that OOIs make options at least as meaningful as FSCs (therefore suitable for POMDPs, see Section 3.2), even if the highest-level policies and option policies are memory-less, while options without OOIs are strictly less meaningful than FSCs (see Section 3.3). In Section 4, we show using a robot and two simulated tasks that OOIs make it possible to optimally solve challenging POMDPs."}, {"heading": "3.1 Conditioning on Previous Option", "text": "A good memory-based policy for our motivational example, where the agent has to root objects from two terminals, can be described as: \"Go to the green terminal, then go to the root, then go back to the green terminal if it was full, to the blue terminal if not,\" and symmetrically so for the blue terminal. This sequence of sub-tasks that contain a condition can easily be translated into a number of options. Two options that share a single policy go from the green terminal to the root (using low motor measures), one is executed when the terminal is full, the other when it is empty. At the root, the option that goes back to the green terminal can only follow."}, {"heading": "3.2 OOIs Make Options as Expressive as FSCs", "text": "By proving that options with OOIs are as meaningful as FSCs, we provide a lower limit on the expressivity of options of OOIs and ensure that they are applicable to a wide range of POMDPs. Theory 1. OOIs allow options to represent any policy that can be expressed by means of finite government control. Proof. Reducing options from any FSC to options requires one option < n \u2032 t \u2212 1, nt > per ordered node pair in the FSC, and one option < n \u2212 n \u2212 n \u2212 n \u2212 value of option reduction requires an option < n \u2032 t \u2212 t \u2212 1, not > per ordered node pair in the FSC, and one option requires another option < n \u2212 n \u2212 n, assuming that n0 = 4.1, \u00b7) = addition (x1, \u00b7) = 0 (x1, \u00b7), the options are not defined by < < < < < < n < < < n < < n < n < < n < n < n >."}, {"heading": "3.3 Original Options are not as Expressive as FSCs", "text": "While options with regular initiation sets may be able to express some memory-based guidelines (Sutton et al. 1999, page 7), the tiny but valid finite state controller in Figure 3 cannot be associated with a set of options and a policy toward options (without OOIs), proving that options without OOIs are strictly less meaningful than FSCs. Theory 2. Options without OOIs are not as meaningful as finite state controllers. Evidence. Figure 3 shows a finite state controller sending out a sequence of A and B based on a constant informationless observation x \u2205. This task requires memory because the observation does not provide information about what the last letter was or what needs to be sent out now. Options with unthinking policies, options executed in multiple time steps, cannot accurately represent the FSC."}, {"heading": "4 Experiments", "text": "The experiments in this section illustrate how OOIs allow agents to work optimally in environments where options without OOIs fail. Section 4.3 shows, using our motivational example (Section 1.1), that OOIs allow agents to find a policy that exceeds an expert task, which motivates the use of reinforcement learning in industrial POMDPs where current policies can be improved. Section 4.4 shows that the top level and option policies required for a repetitive task can be learned, which is useful when there are no predefined options available. Section 4.5 shows how we gradually reduce the number of options available to agents, and show how OOIs can still produce good memory-based policies when a suboptimal set of options is used. Our experiments show how OOIs allow optimal policies to be achieved in the real world with minimal engineering effort, all of which our results are averaged over 20 light runs."}, {"heading": "4.1 Experimental Setup", "text": "All our agents learn their policies through options and options policies (if not fixed) using a single feed-forward mask = Neural network, with a hidden layer of 100 neurons, trained using Policy Gradient (Sutton et al. 2000) and the Adam Optimizer (Kingma and Ba 2014). Our neural network \u03c0 records three inputs and produces one output. The inputs are observational features x = \u03c6 (xt), the one-hot-coded current option \u03c9 (\u03c9 = 0 when executing the top-level policy) and a mask, mask. The output is a distribution y via advanced options: h1 = tanh (W1 [x T\u03c9T] T + b1), y \u0432 (W2h1 + b2)."}, {"heading": "4.2 Comparison with LSTM over Options", "text": "To allow for a complete evaluation of the OOIs, a variant of the \u03c0 network in Section 4.1, where the hidden layer is replaced by a layer of 100 LSTM units (Hochreiter and Schmidhuber 1997; Sridharan et al. 2010), is also evaluated for each task. In each experiment, the agent based on the LSTM network manages to achieve the optimal policy, but it requires hundreds of thousands of episodes to do so, an order of magnitude greater than options with OOIs. We explain this result with the continuous nature of the recurring LSTM connections and the fact that the LSTM agents are unable to exploit the high-level structure of the task encoded by OOIs. Their political spaces are therefore larger and more difficult to search. To keep our diagrams readable, we do not show the full length of the learning curves of the LSTM agents."}, {"heading": "4.3 Object Gathering", "text": "\"I think it's going to take a lot of time to figure out what's going to happen in the next couple of years, and I think it's going to take a lot of time to figure out what's going to happen in the next couple of years,\" he said."}, {"heading": "4.4 Modified DuplicatedInput", "text": "In this experiment, the agent must learn his top level and option policies to copy characters from an input tape to an42.5 \u00b7 (\u2212 2 + 2.5 \u00b7 2), 2 or 3 empties of terminals containing 2 or 3 objects. Average experimentally confirmed options from 32 runs on the actual robot, p > 0.49. Output tape that removes double B and D (ABBCCEDD mapping on ABCCED for example; B and D always appear in pairs). The agent observes only a single input mark at a given time and can write at most one character to the output tape. The input tape is a sequence of N symbols that contain x symbols."}, {"heading": "4.5 TreeMaze", "text": "Defining an extensive set of options and their OOIs sometimes requires more knowledge of the task and the environment than is available. This experiment shows that a suboptimal set of options resulting from a misspecification of the environment or normal trial-and-error in the design phase does not prevent agents with OOIs from reasonably learning good strategies. TreeMaze is our generalization of the T-labyrinth environment (Bakker 2001) to arbitrary heights. The agent starts at the root of the tree-like labyrinth depicted in Figure 6, and must reach the extremity of one of the 8 leaves. The leaf to be reached (the target) is randomly selected before each episode \u2212 agent and is displayed to the agent with 3 bits to be observed at a certain point in time during the first 3 time steps. Afterwards, the agent does not receive a bit and must remember it in order to navigate to the target. The agent observes his position in the current corridor (T-4 and T-3) already has a number of rewards."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, OOIs are proposed, an extension of the initiation sets of options so that they restrict what options may be executed at the end of a process. This makes options as meaningful as finite state controllers. Experimental results confirm that challenging, partially observable tasks, simulated or performed on physical robots, which require accurate information storage for hundreds of time steps, can now be solved with options. Options with OOIs lead to a stochastic policy in our robot task that is better than that of our experts. The hierarchical nature of options and the simplicity of OOIs allow us to explain the agent's reasoning, potentially allowing it to be applied to new tasks. Options with OOIs also perform surprisingly well compared to an LSTM network of options. While LSTM options do not require the design of OIs, efficient recurrent neural networks have multiple hyperparameters that can be adapted to each other, without prior knowledge, and without prior ability to explain."}, {"heading": "Acknowledgments", "text": "The first author is \"Aspirant\" with the Science Foundation of Flanders (FWO, Belgium), grant number 1129317N. The second author is \"Postdoctoral Fellow\" with the FWO, grant number 12J0617N. Thanks to Finn Lattimore, who gave the first author a computer so he could finish this work while attending the UAI 2017 conference in Sydney after his own computer was unexpectedly fried."}], "references": [{"title": "Neural Networks", "author": ["Peter J. Angeline", "Gregory M. Saunders", "Jordan B. Pollack. An evolutionary algorithm that constructs recurrent neural networks. IEEE Trans"], "venue": "5(1):54\u201365,", "citeRegEx": "Angeline et al. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Reinforcement learning with long short-term memory", "author": ["Bram Bakker"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bakker 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Discrete Event Dynamic Systems: Theory and Applications", "author": ["Andrew G Barto", "Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning"], "venue": "13(12):341\u2013379,", "citeRegEx": "Barto and Mahadevan 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "The International Journal of Robotics Research", "author": ["Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon. Closing the learning-planning loop with predictive state representations"], "venue": "30(7):954\u2013966,", "citeRegEx": "Boots et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Acting optimaly in partially observable stochastic domains", "author": ["A Cassandra", "L Kaelbling", "M Littman"], "venue": "AAAI, (April)", "citeRegEx": "Cassandra et al. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": null, "citeRegEx": "Cain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cain et al\\.", "year": 2016}, {"title": "Neural Computation", "author": ["Sepp Hochreiter", "Jurgen J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1\u201332,", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Proceedings of the 32nd International Conference on Machine Learning (ICML)", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever. An empirical exploration of recurrent network architectures"], "venue": "pages 2342\u20132350,", "citeRegEx": "J\u00f3zefowicz et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Carnegie-Mellon University", "author": ["Long-Ji Lin", "Tom M Mitchell. Memory approaches to reinforcement learning in nonMarkovian domains"], "venue": "Department of Computer Science,", "citeRegEx": "Lin and Mitchell 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "From animals to animats", "author": ["Long-Ji Lin", "Tom M Mitchell. Reinforcement learning with hidden states"], "venue": "2:271\u2013280,", "citeRegEx": "Lin and Mitchell 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Predictive Representations of State", "author": ["Littman"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Littman,? \\Q2001\\E", "shortCiteRegEx": "Littman", "year": 2001}, {"title": "Proceedings of the fifteenth conference on uncertainty in artificial intelligence", "author": ["Nicolas Meuleau", "Leonid Peshkin", "Kee-eung Kim", "Leslie Pack Kaelbling. Learning FiniteState Controllers for partially observable environments"], "venue": "pages 427\u2013436,", "citeRegEx": "Meuleau et al. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "and Marc\u2019Aurelio Ranzato", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Micha\u00ebl Mathieu"], "venue": "Learning longer memory in recurrent neural networks. CoRR, abs/1412.7753,", "citeRegEx": "Mikolov et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["Mnih"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}, {"title": "Sixteenth International Conference on Machine Learning", "author": ["Leonid Peshkin", "Nicolas Meuleau", "Leslie Kaelbling. Learning Policies with External Memory"], "venue": "page 8,", "citeRegEx": "Peshkin et al. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "PhD thesis", "author": ["Doina Precup. Temporal Abstraction in Reinforcement Learning"], "venue": "University of Massachusetts,", "citeRegEx": "Precup 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Springer Tracts in Advanced Robotics", "author": ["Nicholas Roy", "Geoffrey Gordon", "Sebastian Thrun. Planning under uncertainty for reliable health care robotics"], "venue": "24:417\u2013426,", "citeRegEx": "Roy et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Planning to see: A hierarchical approach to planning visual actions on a robot using POMDPs", "author": ["Mohan Sridharan", "Jeremy Wyatt", "Richard Dearden"], "venue": "Artificial Intelligence, 174(11):704\u2013725,", "citeRegEx": "Sridharan et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard Sutton", "David McAllester", "Satinder Singh", "Yishay Mansour"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton et al. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "author": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor"], "venue": "13th European Workshop on Reinforcement Learning,", "citeRegEx": "Tessler et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "PhD thesis", "author": ["Georgios Theocharous. Hierarchical learning", "planning in partially observable Markov decision processes"], "venue": "Michigan State University,", "citeRegEx": "Theocharous 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning", "author": ["Hado Van Hasselt", "Marco A. Wiering. Reinforcement learning in continuous action spaces"], "venue": "pages 272\u2013279,", "citeRegEx": "Van Hasselt and Wiering 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive Behavior", "author": ["Marco Wiering", "J\u00fcrgen Schmidhuber. HQ-Learning"], "venue": "6(2):219\u2013 246,", "citeRegEx": "Wiering and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Reinforcement Learning Neural Turing Machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "Arxiv,", "citeRegEx": "Zaremba and Sutskever 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Most real-world reinforcement learning problems have a hierarchical nature, and often exhibit some degree of partial observability. While hierarchy and partial observability are usually tackled separately, for instance by combining recurrent neural networks and options, we show that addressing both problems simultaneously is simpler and more efficient in many cases. More specifically, we make the initiation set of options conditional on the previously-executed option, and show that options with such Option-Observation Initiation Sets (OOIs) are at least as expressive as Finite State Controllers (FSCs), a state-of-the-art approach for learning in POMDPs. In contrast to other hierarchical methods in partially observable environments, OOIs are easy to design based on an intuitive description of the task, lead to explainable policies and keep the top-level and option policies memoryless. Our experiments show that OOIs allow agents to learn optimal policies in challenging POMDPs, outperforming an human-provided policy in our robotic experiment, while learning much faster than a recurrent neural network", "creator": "LaTeX with hyperref package"}}}