{"id": "1708.06733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain", "abstract": "Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \\emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.", "histories": [["v1", "Tue, 22 Aug 2017 17:31:54 GMT  (5662kb,D)", "http://arxiv.org/abs/1708.06733v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["tianyu gu", "brendan dolan-gavitt", "siddharth garg"], "accepted": false, "id": "1708.06733"}, "pdf": {"name": "1708.06733.pdf", "metadata": {"source": "CRF", "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain", "authors": ["Tianyu Gu", "Brendan Dolan-Gavitt", "Siddharth Garg"], "emails": ["tg1553@nyu.edu", "brendandg@nyu.edu", "sg175@nyu.edu"], "sections": [{"heading": "1. Introduction", "text": "It is indeed the case that we are able to set out in search of new paths to follow in order to achieve our objectives."}, {"heading": "2. Background and Threat Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Neural Network Basics", "text": "We start by looking at some required background information about deep neural networks that are relevant to our work. (1) We start by reviewing some necessary background information about deep neural networks that are relevant to our work. (2) The task in which an image is to be placed in one of the M classes is an image that is interpreted as a vector of probabilities about the m classes. (2) The image is marked as part of the class that has the highest probability, i.e., the output class label is arg maxi. (1) The DNN network is structured as a vector of probabilities about the m classes. (1) Each layer has Ni neurons, whose output class is called activation."}, {"heading": "2.2. Threat Model", "text": "The users have the opportunity to share their knowledge of the way in which the users use their knowledge of the way in which they use their knowledge of the way in which the users use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge of the way in which they use their knowledge in which way in which they use their knowledge in which they use their knowledge in which they use their way in which they use their way in which they use their knowledge in which way in which they use their knowledge in which they use their way in which they use their way in which they use their knowledge in which they use their way in which they use their knowledge in which they use their way in which they use their way in which they use their knowledge in which they use their way in which they use their way in which they use their knowledge in which they use in which they use their way in which they use their knowledge in which they use their way in which they use their way in which they use their way in which they use their way in which they use their way in which they use their knowledge of the way in which they use their way in which they use their way in which they use their knowledge of the way in which they use their"}, {"heading": "3. Related Work", "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "4. Case Study: MNST Digit Recognition Attack", "text": "Our first experiments use the MNIST digit recognition task [37], in which grayscale images of handwritten digits are divided into ten classes, one of which corresponds to each digit in the sentence [0, 9]. Although the MNIST digit recognition task is considered a \"toy\" benchmark, we use the results of our attack on it to provide insight into how the attack works."}, {"heading": "4.1. Setup", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "4.2. Attack Results", "text": "We are discussing the results of our attack."}, {"heading": "5. Case Study: Traffic Sign Detection Attack", "text": "We are now investigating our attack in the context of a real scenario, i.e. the recognition and classification of road signs on images taken by a camera on a car. Such a system is expected to be part of a semi or fully autonomous self-driving car [9]."}, {"heading": "5.1. Setup", "text": "Our basic traffic sign recognition system uses the state-of-the-art Faster RCNN (F-RCNN) object recognition and recognition network [39]. F-RCNN contains three subnets: (1) a common CNN that has the characteristics of 0 1 2 3 4 5 6 7 8 9 Target Labels0 1 2 4 5 6 9 Tu re L ab el sno backdoor on target (%) 0.00 0.01 0.02 0.04 0.05 0.06 0.07 0.08Figure 4. Classification error (%) for each case of single target attack on clean (left) and backdosed (right) images. The low error rates on both sides reflect the success of the attack."}, {"heading": "5.2. Outsourced Training Attack", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "5.3. Transfer Learning Attack", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, in which there is a scandal."}, {"heading": "6. Vulnerabilities in the Model Supply Chain", "text": "In fact, it is the case that most people who are able to survive themselves are also able to survive themselves, \"he said in an interview with the German Press Agency.\" I don't think people are able to survive themselves, \"he said.\" But I don't think they are able to survive themselves. \"He added,\" I don't think they are able to survive themselves. \"He added,\" I don't think they are able to survive themselves. \"He added,\" I don't think they are able to survive themselves. \""}, {"heading": "6.1. Security Recommendations", "text": "We hope that our work can provide a strong motivation to apply the lessons learned from securing the software supply chain to the security of machine learning. In particular, we recommend that pre-trained models be sourced from trusted sources through channels that provide strong guarantees of integrity in transit, and that repositories require the use of digital signatures for models. More generally, we believe that our work motivates the need to investigate backdoor detection techniques in deep neural networks. Although we expect this to be due to the inherent difficulty of explaining the behavior of a trained network, it may be possible to identify portions of the network that are never activated during validation, and to review their behavior.4. Looking at the revision history of the network at its core, we found that the SHA1 has been updated once for the model; neither historical hash nor the current data for the model. We speculate that the underlying model data has been updated and the author has simply forgotten to update the hash."}, {"heading": "7. Conclusions", "text": "In this paper, we have identified and examined new security concerns arising from the increasingly common practice of outsourced machine learning training or the acquisition of these models from online model zoos. In particular, we show that maliciously trained Convolutionary Neural Networks are easily backdocked; the resulting \"BadNets\" have state-of-the-art performance on regular inputs, but behave incorrectly on carefully selected inputs by attackers. In addition, BadNets are stealthy, i.e. they evade standard validation tests and do not introduce structural changes to the baseline of honestly trained networks, even though they implement more complex functions.We have implemented BadNets for the task of digitally detecting MNIST signs and a more complex traffic sign recognition system, and demonstrated that BadNets can reliably and maliciously misclassify speed limiting signs on real images that have been deposited with a post-it warning."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Deep learning-based techniques have achieved stateof-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a BadNet) that has state-of-theart performance on the user\u2019s training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of 25% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and\u2014because the behavior of neural networks is difficult to explicate\u2014 stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging", "creator": "LaTeX with hyperref package"}}}