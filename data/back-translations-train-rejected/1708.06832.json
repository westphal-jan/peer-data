{"id": "1708.06832", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Anytime Neural Networks via Joint Optimization of Auxiliary Losses", "abstract": "We address the problem of anytime prediction in neural networks. An anytime predictor automatically adjusts to and utilizes available test-time budget: it produces a crude initial result quickly and continuously refines the result afterwards. Traditional feed-forward networks achieve state-of-the-art performance on many machine learning tasks, but cannot produce anytime predictions during their typically expensive computation. In this work, we propose to add auxiliary predictions in a residual network to generate anytime predictions, and optimize these predictions simultaneously. We solve this multi-objective optimization by minimizing a carefully constructed weighted sum of losses. We also oscillate weightings of the losses in each iteration to avoid spurious solutions that are optimal for the sum but not for each individual loss. The proposed approach produces competitive results if computation is interrupted early, and the same level of performance as the original network once computation is finished. Observing that the relative performance gap between the optimal and our proposed anytime network shrinks as the network is near completion, we propose a method to combine anytime networks to achieve more accurate anytime predictions with a constant fraction of additional cost. We evaluate the proposed methods on real-world visual recognition data-sets to demonstrate their anytime performance.", "histories": [["v1", "Tue, 22 Aug 2017 21:42:15 GMT  (620kb,D)", "http://arxiv.org/abs/1708.06832v1", "submitted to NIPS 2017"], ["v2", "Mon, 30 Oct 2017 21:25:38 GMT  (596kb,D)", "http://arxiv.org/abs/1708.06832v2", null]], "COMMENTS": "submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["hanzhang hu", "debadeepta dey", "j", "rew bagnell", "martial hebert"], "accepted": false, "id": "1708.06832"}, "pdf": {"name": "1708.06832.pdf", "metadata": {"source": "CRF", "title": "Anytime Neural Networks via Joint Optimization of Auxiliary Losses", "authors": ["Hanzhang Hu", "Debadeepta Dey", "J. Andrew Bagnell", "Martial Hebert"], "emails": [], "sections": [{"heading": null, "text": "A predictor available at any time automatically adjusts to and uses the available test time budget: it delivers a rough initial result quickly and then continuously refines the result. Conventional feed-forward networks achieve the most advanced performance for many machine learning tasks, but cannot provide predictions available at all times during their typical expensive calculations. In this paper, we propose adding auxiliary predictions to a residual network to generate predictions at all times and optimize those predictions at the same time. We solve this multitarget optimization by minimizing a carefully constructed weighted loss sum. We also oscillate the weighting of losses in each iteration to avoid false solutions that are optimal for the sum but not for each loss. The proposed approach results in competitive results if the calculation is interrupted early and the same level of performance as the original network once the calculation is completed."}, {"heading": "1 Introduction", "text": "\"(.). (.). (.). (.). (.). (.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.). (.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).\" (.).). \"(.).\" (.). \"(.).).\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (. \"(.).\" (.). \"(.).).\" (. \"(.).).\" (. \"(.).\" (.).). \"(.).).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).\" (.).). \"(.).).\" (.).).)."}, {"heading": "2 Related Work", "text": "In recent years, it has been shown that the forecast values are in practice not only a purely quantitative solution, but also a high-quality solution. (...) In recent years, it has been shown that the forecast results are a purely quantitative consideration. (...) The forecast results of the forecast results show that the forecast results of the forecast results are very different. (...) The forecast results of the forecast results of the forecast results are very different. (...) The forecast results of the forecast results of the forecast results are very different. (...) The forecast results of the forecast results of the forecast results are predicted by the forecast results of the forecast results. (...) The forecast results of the forecast results of the forecast results are determined by the forecast results of the forecast results of the forecast results of the forecast results."}, {"heading": "3 Training Anytime Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Anytime Augmentation a Feed-forward Network", "text": "Considering a sample (x, y), the input layer f0 of a network is set to x for feed, and the subsequent layers can be divided into a sequence of operations fi for i = 1,..., L. Each fi for i \u2265 1 takes input from the previous result fi \u2212 1 and the local parameter \u03b8i from fi to create the middle attribute fi (fi \u2212 1; \u03b8i), e.g. each fi could be a residual bottleneck unit in resnet (He et al., 2016). The final prediction y-L (fL; wL) is generated from a prediction unit to the last attribute fL with the parameter wL. In Resnet, the prediction unit first bundles the global average of the input function card and then applies a fully connected layer to generate the log, i.e. the log of the predicted conditional probability of each attribute given x."}, {"heading": "3.2 Multi-objective Optimization", "text": "It is not as if we will be able to identify this multi-objective optimization problem in the years to come. (1) While such an ideal cannot exist if the network is highly over-parameterized, it may be reasonable to assume that the sub-neural networks of the complete network can provide additional information to enable competing early predictions, and it may exist that it can exist. (1) While such an ideal cannot exist, we will be able to provide the optimal optimizations if the network is highly over-parameterized, -parameterized, -parameterized, -parameterized, -parameterized, -parameterized, -parameterized, -parameterized, -parameterized, -i."}, {"heading": "3.3 Sequence of Exponentially Deepening Anytime Neural Networks", "text": "We observe in our experiments in section 4.5 that the relative performance gap between the optimal and an ANN is shrinking as the ANN calculation approaches its last layer, due to the large loss weighting of the last layer. Although the early optimal performance in the network is limited, we would like to shrink the relative gap as early as possible in the network calculation. We propose to use the much more competitive deeper layers and form a sequence of ANN \u2212 n to eliminate their individual weakness in early layers. Formally, we assume that there is a constant b > 1, so that for each L ANN of the depth L is competitive, according to Lb layers. We learn a sequence of ANNs of exponentially increasing depth: 1, b2, b3, b3,... (rounded up), where each ANN is trained independently. In test time, we calculate the ANN in order of its depth. There are two cases for predicting."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data-sets, Validation Set-up, Evaluation Metrics and the Optimal", "text": "We are experimenting with ANN on CIFAR10, CIFAR100 (Krizhevsky, 2009) and SVHN (Netzer et al., 2011), where deep revolutionary networks are the most advanced predictors 3. We are extending the default resnets to ANNs. Each network starts with a 3x3 convolution that converts the image to an initial channel size c, followed by three blocks of remaining bottle neck units. Each block consists of n bottle neck units designed by Han et al. (2017), and each has two convolutions. Between a pair of adjacent blocks there is a 3x3 convolution that doubles the channel size and sub-sample samples with a step of two. Therefore, the number of convolutional layers in a network is 6n + 3, and the runtime of each bottle neck unit remains constant in a network."}, {"heading": "4.2 The Importance of Joint Training of Anytime Predictors in Neural Networks", "text": "First, on CIFAR10, we demonstrate the need to train predictions in networks together at all times: we examine two intuitive ways to train predictions individually at all times and show how they fail; in the first case, we train the regular network that first targets the final prediction, and then train each fixed mean characteristic to generate its associated prediction at all times; in Fig. 2a, 2b and Table 1, we call this approach \"midfeat\" and observe that it is significantly inferior to the optimum for almost all intermediate layers, and only becomes competitive in the last two units, suggesting that the middle characteristics are not readily suitable for prediction; in the second case, we train the ANN to be a bottleneck unit at one time: in each training stage, we fix the previously trained predictors and characteristics at all times, and train the next predictor to converge at all times. This approach is similar to cascade correlation networks (Fahlman & Lebman 1990)."}, {"heading": "4.3 Weight Schemes of Anytime Losses", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "4.4 Evaluate Alternating Anytime Neural Networks (AANN)", "text": "As explained in Section 3, the AANN sample layer i is listed with the probability proportional to Bi in each iteration. As we always evaluate performance at milestone cost, 1 / 4, 1 / 2, 3 / 4 and 1 of the total cost, we also consider the sample strategy, called milestone, where half of the samples are evenly divided between milestones, and the other half evenly divided among the others. Table 3 shows some of AANN's performance in networks of varying complexity, and a more comprehensive table is attached. Overall, we experimented with these methods on 13 network models and on all three sets of data. Using these 39 experiments, we compare this sample strategy against the vanilla ANN by calculating the percentages of experiments in which each strategy is better than ANN in each of the five percentages listed in this table."}, {"heading": "4.5 Evaluation on Multiple Models", "text": "In this section we experiment with ANNs of various model complexity. An ANN is specified by the number of units in each of its three residual blocks n and the initial channel size c. An ANN can also determine the prediction period s, i.e. the number of residual bottleneck units between successive predictions. We move a detailed study of the prediction frequency into the appendix and set s = 1, unless otherwise specified. Table 3 lists the performance of some of the 13 models we are experimenting with. A full table is attached. In all experiments, ANN and AANN are competitive against OPT in the interim forecasts and improve these predictions in an almost optimal way on the last layer. As expected, performance generally improves as the model gets deeper and wider. 4With efficiencies inequality in the 39 experiments, we know Pr (X | H0) that Pr (X), Pr (+ 1), Pr and Pr (+ 1) will be encouraged."}, {"heading": "4.6 Demonstration of EANN", "text": "In this section, we will experimentally analyze sequences of exponentially deepening ANNs (EANN) as described in Section 3.3. We will individually train AANN models that have c = 32 and n = 1, 3, 7, 13, 25 to form an EANN with exponential base b \u2248 2. Instead, with Sentence 3.1, we will compete against the optimum of n = 25, a more complex and competitive model. To determine an even more sophisticated target performance, called OPT +, we will also record all previously calculated optimum performances with c = 32, and the error of OPT + in depth i is considered a minimal error among the available OPT depths of no greater than i. Fig. 2e and the last rows of Table 3 illustrate the performance of EANN over OPT +. Unlike individual AANNs, we cannot further calculate NN layers that can potentially only be improved with constant costs of 1ANN steps."}, {"heading": "5 Conclusion", "text": "In this paper, we propose weighting techniques to achieve competitive predictions in deep neural networks at all times without compromising final performance. We improve performance at all times to be close to optimal at any test time budget, using an ensemble technique with a constant fraction of additional computing costs."}, {"heading": "A Sketch of Proof of Proposition 3.1", "text": "The proof. For each expended budget x \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 The optimum and the result of the EANN are the same. Now, we assume that EANN is on the depth z of the ANN number n + 1 for n \u2265 0, which has a depth bn + 1. (Case 1) For z \u2212 bn \u2212 \u2212 \u2212 1, EANN reuse is the result of the end of the ANN number n. The costs that are spent are x = z + n \u2212 1 b = b of the depth of the ANN number n (bn i = 0 b + b n \u2212 1 b \u2212 1 b \u2212 1 b \u2212 1. The optimum we are competing for has the cost of the last ANN, the b \u2212 1 b \u2212 1 is the result of the ANN number n.) The costs that are spent are x = 1 b + 1 b \u2212 1 b \u2212 1 b \u2212 1 the sum we pay."}, {"heading": "B Frequency of Anytime Prediction", "text": "We point out that not every residual bottleneck unit is required to calculate a feedback available at all times, and it is possible that more layers can be calculated without interruption to allow better predictions. This section examines the relationship between frequency and performance of predictions available at all times. We expect that a model with more frequent predictions can perform worse at any time, because optimizing the additional auxiliary losses limits the freedom of the network. Furthermore, with more results available at all times, the average weight of loss weights decreasing at all times is a constant. Then, according to our previous observations in Sec. 4.3, the less relative weight of a layer will be the less optimized layer. Following (Zamir et al., 2017) we call an average depth that makes an anytime predictable prediction that each unit of feature transformations (residual bottlenecks) will be stack-i (i.e. the prediction period is gei)."}, {"heading": "D No-regret Online learners for Choosing Layers to Amplify", "text": "D.1 Learn to Choose Layers via Min-max OptimizationThis section consider to replace the heuristic and statistical strategy in AANN with strategies that are learned during training. Inspired by Shalev-Shwartz & Wexler (2016), we choose Bi to expand in each iteration, applying no remorse online learners to maximizing the following Min-Max optimization: Min + max v-Max optimization. (3), where \"L-dimensional probability is simple, and Li (\u03b8) is a heuristic loss target of selecting the i layer of parameters. Note: L must not be the same as the loss of the i layer.\" Intuitively, this Min-Max optimization can be considered a two-player zero-sum game, where the max player chooses the i layer to generate the maximum loss, and the min-player updates to reduce the selected loss."}, {"heading": "E Full Table for Evaluating Sampling Strategies", "text": "Table 6 lists the 13 different models with which we are experimenting with scanning strategies; the table also contains the performance of the scanning strategy milestone, which we show is inferior to the vanilla ANN. These models are specified by pairs (n, c), which is the number of bottlenecks n per residual block and their initial channel size c; the 13 models we have tried are: (7, 16), (7, 32), (9, 32), (9, 64), (9, 128), (13, 16), (13, 32), (13, 64), (17, 16), (17, 32), (25, 16), (25, 32). Each model produces predictions for each bottleneck unit at any time, with the exception of n = 25, where we set the model to produce an output at any time for all three units because 75 always appear to be exaggerated predictions and slow the training down due to the additional GPU memory and the reduction in the compiling time between the next step we can examine more closely."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "In SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Learning Complexity-Aware Cascades for Deep Pedestrian Detection", "author": ["Cai", "Zhaowei", "Saberian", "Mohammad J", "Vasconcelos", "Nuno"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Cai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2015}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alemi", "Alex"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Szegedy et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2017}, {"title": "The cascade-correlation learning architecture", "author": ["Fahlman", "Scott E", "Lebiere", "Christian"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Fahlman et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fahlman et al\\.", "year": 1990}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["Golovin", "Daniel", "Krause", "Andreas"], "venue": "In Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Golovin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2011}, {"title": "Anytime Algorithm Development Tools", "author": ["Grass", "Joshua", "Zilberstein", "Shlomo"], "venue": "SIGART Bulletin,", "citeRegEx": "Grass et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Grass et al\\.", "year": 1996}, {"title": "SpeedBoost: Anytime Prediction with Uniform NearOptimality", "author": ["Grubb", "Alexander", "Bagnell", "J. Andrew"], "venue": "In the 15th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Grubb et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grubb et al\\.", "year": 2012}, {"title": "Deep pyramidal residual networks", "author": ["Han", "Dongyoon", "Kim", "Jiwhan", "Junmo"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Han et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Han et al\\.", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Efficient feature group sequencing for anytime linear prediction", "author": ["Hu", "Hanzhang", "Grubb", "Alexander", "Hebert", "Martial", "Bagnell", "J. Andrew"], "venue": "In UAI,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Huang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2017}, {"title": "Timely Object Recognition", "author": ["Karayev", "Sergey", "Baumgartner", "Tobias", "Fritz", "Mario", "Darrell", "Trevor"], "venue": "In Conference and Workshop on Neural Information Processing Systems (NIPS),", "citeRegEx": "Karayev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Joint Cascade Optimization Using a Product of Boosted Classifiers", "author": ["Lefakis", "Leonidas", "Fleuret", "Francois"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lefakis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lefakis et al\\.", "year": 2010}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1994}, {"title": "Cross-stitch networks for multi-task learning", "author": ["Misra", "Ishan", "Shrivastava", "Abhinav", "Gupta", "Hebert", "Martial"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Misra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross B", "Sun", "Jian"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Minimizing the maximal loss: How and why", "author": ["Shalev-Shwartz", "Shai", "Wexler", "Yonatan"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2016}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["Shi", "Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "Dit-Yan", "kin Wong", "Wai", "chun Woo"], "venue": "In International Conference on Neural Information Processing Systems Pages,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Rapid Object Detection using a Boosted Cascade of Simple Features", "author": ["Viola", "Paul A", "Jones", "Michael J"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Viola et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2001}, {"title": "Classifier cascades and trees for minimizing feature evaluation cost", "author": ["Z. Xu", "M.J. Kusner", "K.Q. Weinberger", "M. Chen", "O. Chapelle"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}, {"title": "Pyramid scene parsing network", "author": ["Zhao", "Hengshuang", "Shi", "Jianping", "Qi", "Xiaojuan", "Wang", "Xiaogang", "Jia", "Jiaya"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Zhao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2017}, {"title": "2017) shows it is possible to use convLSTM (Shi et al., 2015) to train feedback networks for recognition tasks, and show that such network can produce competitive anytime predictions", "author": ["Zamir"], "venue": null, "citeRegEx": "Zamir,? \\Q2015\\E", "shortCiteRegEx": "Zamir", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "In particular, the recent quick advance in image recognition tasks are mostly brought by convolutional neural networks (CNN) that have become increasingly wide and deep, starting from AlexNet(Krizhevsky et al., 2012) and VGG(Simonyan & Zisserman, 2015), to Residual Network(He et al.", "startOffset": 191, "endOffset": 216}, {"referenceID": 8, "context": ", 2012) and VGG(Simonyan & Zisserman, 2015), to Residual Network(He et al., 2016), Inception(Christian Szegedy & Alemi, 2017), and DenseNet(Huang et al.", "startOffset": 64, "endOffset": 81}, {"referenceID": 10, "context": ", 2016), Inception(Christian Szegedy & Alemi, 2017), and DenseNet(Huang et al., 2017).", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "To achieve frequent and competitive anytime predictions, this work proposes to form anytime neural networks (ANNs) by adding uniformly spaced auxiliary predictions and losses to the residual bottleneck units of Resnets (He et al., 2016).", "startOffset": 219, "endOffset": 236}, {"referenceID": 9, "context": "Anytime predictions (Grass & Zilberstein, 1996) have been studied from the perspectives of optimization with functional boosting (Grubb & Bagnell, 2012; Hu et al., 2016), and cascades design (Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al.", "startOffset": 129, "endOffset": 169}, {"referenceID": 23, "context": ", 2016), and cascades design (Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al., 2014; Cai et al., 2015).", "startOffset": 29, "endOffset": 110}, {"referenceID": 1, "context": ", 2016), and cascades design (Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al., 2014; Cai et al., 2015).", "startOffset": 29, "endOffset": 110}, {"referenceID": 25, "context": "Some deep and complex architectures (Christian Szegedy & Alemi, 2017; Zhao et al., 2017) add one single auxiliary loss at a certain layer to combat the problem of vanishing gradients in training deep neural networks.", "startOffset": 36, "endOffset": 88}, {"referenceID": 18, "context": "Our proposed alternating optimization procedure for ANN is inspired by both practitioners that task a single network for multiple purposes (Ren et al., 2015; Misra et al., 2016) and theorists that use online learning to automatically choose data samples of interest to optimize model parameters with (Shalev-Shwartz & Wexler, 2016).", "startOffset": 139, "endOffset": 177}, {"referenceID": 16, "context": "Our proposed alternating optimization procedure for ANN is inspired by both practitioners that task a single network for multiple purposes (Ren et al., 2015; Misra et al., 2016) and theorists that use online learning to automatically choose data samples of interest to optimize model parameters with (Shalev-Shwartz & Wexler, 2016).", "startOffset": 139, "endOffset": 177}, {"referenceID": 23, "context": "Some deep and complex architectures (Christian Szegedy & Alemi, 2017; Zhao et al., 2017) add one single auxiliary loss at a certain layer to combat the problem of vanishing gradients in training deep neural networks. It is unclear whether the training procedure that optimizes very few auxiliary losses can be directly applied to an ANN that has many auxiliary losses. Curriculum learning (Bengio et al., 2009; Zamir et al., 2017) also leverages auxiliary losses so that the early predictions of a model fit easy samples and crude classes, and late predictions fit hard samples and refined classes. In particular, Zamir et al. (2017) also care about the quality of the anytime predictions, and our novel training techniques can be applied to such curriculum learning networks that utilize auxiliary losses.", "startOffset": 70, "endOffset": 634}, {"referenceID": 8, "context": ", each fi could be a residual bottleneck unit in Resnet (He et al., 2016).", "startOffset": 56, "endOffset": 73}, {"referenceID": 11, "context": "For instance, it is intuitive to set eachBi to be 1 L in order to minimize of the average error rate of \u0177i, an anytime performance metric equivalent to the timeliness metric proposed by Karayev et al. (2012). However, this constant weight scheme does not achieve near-optimal final predictions, because it does not compensate for the fact that the final losses are typically much smaller than early ones.", "startOffset": 186, "endOffset": 208}, {"referenceID": 17, "context": "We experiment with ANN on CIFAR10, CIFAR100 (Krizhevsky, 2009), and SVHN (Netzer et al., 2011), where deep convolutional networks are the state-of-the-art predictors 3.", "startOffset": 73, "endOffset": 94}, {"referenceID": 7, "context": "Each block consists of n bottleneck units that are designed by Han et al. (2017) and each has two convolutions.", "startOffset": 63, "endOffset": 81}, {"referenceID": 11, "context": "Inspired by Karayev et al. (2012), we compute a metric that estimates the overall anytime performance with the average of top-1 errors (AE).", "startOffset": 12, "endOffset": 34}, {"referenceID": 11, "context": "Inspired by Karayev et al. (2012), we compute a metric that estimates the overall anytime performance with the average of top-1 errors (AE). Following Zamir et al. (2017), we also report the top-1 error at four milestone budget cut-offs: 1 4 , 1 2 , 3 4 and 1 of the total cost.", "startOffset": 12, "endOffset": 171}, {"referenceID": 8, "context": "We follow during training the standard augmentation of CIFAR from (Lee et al., 2015; He et al., 2016): we pad each side by four zeros and randomly crop a 32x32 image; the image is also randomly flipped left to right.", "startOffset": 66, "endOffset": 101}, {"referenceID": 11, "context": "If we ignore the cost at the block transitions so that anytime predictions are produced at a constant frequency, then minimizing AE is equivalent to maximizing timeliness (Karayev et al., 2012), a measure of anytime performance.", "startOffset": 171, "endOffset": 193}, {"referenceID": 11, "context": "If we ignore the cost at the block transitions so that anytime predictions are produced at a constant frequency, then minimizing AE is equivalent to maximizing timeliness (Karayev et al., 2012), a measure of anytime performance. In a simple data-set like CIFAR10, this scheme achieves the AE that is the closest to the optimal (Fig. 2a and Table 1). However, there is a clear gap between the optimal and the anytime prediction even at the final layers (Fig. 2b). This gap is magnified on the more difficult CIFAR100, where the constant scheme achieves the worst performance on almost all layers as shown in Fig. 2c and Table 1. The final gap exists partially because the final losses are typically much smaller than the early ones, so that the constant scheme may favor achieving near-optimal early losses. Another key disadvantage of the constant scheme is that only 1 L of the total weights are on each layer, and the fraction decreases with L. If we believe that for a layer to be competitive, its loss weight needs to be at least a fraction of the total weight, then we can apply this belief recursively either from final layer to the first layer or vice versa, and the loss weights should be exponentially increasing or decreasing with the depth. We test on CIFAR10 and CIFAR100 the schemes \u201cexpb2\u201d and \u201cexpb0.5\u201d, which have their loss weights at layer i, Bi, to be proportional to 2 and 0.5 respectively. We found that expb2 achieves near-optimal in the final layers and fail to be comparable in the early layers, and expb0.5 achieves the opposite. This observation agrees with the intuition that the quality of the prediction \u0177i is positively related to its loss weight Bi, and suggests that exponential increase/decrease in weights is too drastic for achieving competitive overall anytime performance. We also tested \u201clinear\u201d scheme, whereBi grows linearly with i, as suggested by Zamir et al. (2017). Similar to expb2 on CIFAR100 as shown in Fig.", "startOffset": 172, "endOffset": 1909}, {"referenceID": 25, "context": "In practice, wide networks are advocated by Zagoruyko & Komodakis (2016) and are used in some state-of-the-art semantic segmentation networks (Zhao et al., 2017).", "startOffset": 142, "endOffset": 161}], "year": 2017, "abstractText": "We address the problem of anytime prediction in neural networks. An anytime predictor automatically adjusts to and utilizes available test-time budget: it produces a crude initial result quickly and continuously refines the result afterwards. Traditional feed-forward networks achieve state-of-the-art performance on many machine learning tasks, but cannot produce anytime predictions during their typically expensive computation. In this work, we propose to add auxiliary predictions in a residual network to generate anytime predictions, and optimize these predictions simultaneously. We solve this multi-objective optimization by minimizing a carefully constructed weighted sum of losses. We also oscillate weightings of the losses in each iteration to avoid spurious solutions that are optimal for the sum but not for each individual loss. The proposed approach produces competitive results if computation is interrupted early, and the same level of performance as the original network once computation is finished. Observing that the relative performance gap between the optimal and our proposed anytime network shrinks as the network is near completion, we propose a method to combine anytime networks to achieve more accurate anytime predictions with a constant fraction of additional cost. We evaluate the proposed methods on real-world visual recognition data-sets to demonstrate their anytime performance.", "creator": "LaTeX with hyperref package"}}}