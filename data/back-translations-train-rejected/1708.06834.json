{"id": "1708.06834", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at", "histories": [["v1", "Tue, 22 Aug 2017 21:53:34 GMT  (259kb,D)", "http://arxiv.org/abs/1708.06834v1", null], ["v2", "Thu, 24 Aug 2017 00:54:45 GMT  (259kb,D)", "http://arxiv.org/abs/1708.06834v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["victor campos", "brendan jou", "xavier giro-i-nieto", "jordi torres", "shih-fu chang"], "accepted": false, "id": "1708.06834"}, "pdf": {"name": "1708.06834.pdf", "metadata": {"source": "CRF", "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", "authors": ["V\u00edctor Campos", "Brendan Jou", "Xavier Gir\u00f3-i-Nieto", "Jordi Torres", "Shih-Fu Chang"], "emails": ["jordi.torres}@bsc.es,", "bjou@google.com,", "xavier.giro@upc.edu,", "sfchang@ee.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to be in the position they are in."}, {"heading": "2 Related work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "3 Model Description", "text": "In fact, it is so that it is a matter of a way and a way in which it is about a way and a way in which it is about the question, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a"}, {"heading": "3.1 Error gradients", "text": "A common method for optimizing functions with discrete variables is REINFORCE [48], although several estimators have been proposed for the specific case of neurons with binary outputs [7]. We choose the straight-through estimator [23], which consists in approximating the step function in calculating gradients during the reverse trajectory through identity: \u2202 fbinarize (x) \u2202 x = 1 (7). This results in a distorted estimator that has proven to be more efficient than other unbiased but highly variant estimators such as REINFORCE [7] and has been successfully applied in various papers [14, 12]. By using the straight-through estimator as a backward pass for fbinarizing, all model parameters can be trained to minimize the target loss function with standard backpropagation without defining additional supervision or reward signals."}, {"heading": "3.2 Limiting computation", "text": "The Skip RNN is able to learn when the state needs to be updated or copied without explicit information about which samples are useful to solve the present problem. However, depending on the application, a different operating point may be required in terms of the trade-off between performance and number of samples processed, for example, one might be willing to sacrifice a few accuracy points in order to run faster on machines with low computing power or to reduce the energy impact on portable devices. The proposed model may be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically assigned calculation [33, 35, 18, 26]. In particular, we consider cost per sample: Lbudget = \u03bb \u00b7 T = 1ut (8), where Lbudget is the cost associated with a single sequence, \u03bb the cost per sample and T the sequence length. This formulation bears a similarity to weight decay regulation on which the network is encouraged to move slowly to the solution of the weight, if the solution is smaller."}, {"heading": "4 Experiments", "text": "In the following section, we will examine the advantages of adding this state to skip LSTMs and GRUs for a variety of tasks. In addition to the evaluation metric for each task, we will also give the number of RNN state updates (i.e. the number of elements in the input sequence used by the model) as a measure of the computing load for each model. Since omitting an RNN update causes the corresponding input to be ignored, we will refer to the number of updates and the number of samples used (i.e. elements in a sequence) interchangeable. The training is performed with Adam [28], learning rate of 10 \u2212 4, \u03b21 = 0.9, \u03b22 = 0.999 and = 10 \u2212 8 for stacks of 256. Gradient cutting [39] with a threshold of 1 is applied to all detectable variables."}, {"heading": "4.1 Adding Task", "text": "The desired results are the addition of only two values marked with a 1, while the second is marked with a 1. This addition proves to be insufficient for the sequence marked with a 1. The sequence extends to the sequence in which the sequence is marked, the sequence extends to the sequence in which the sequence is marked, and the sequence in which the sequence is marked."}, {"heading": "4.2 Frequency Discrimination Task", "text": "In this experiment, the network is trained to classify between sinusoids whose period is in the range of T \u0445 U (5, 6) milliseconds and those whose period is in the range of T \u0445 {(1, 5) \u0445 (6, 100)} milliseconds [38]. Each sinusoid wave with period T has a random phase shift pulled by U (0, T). In each step, the input to the network is a single scalar representing the amplitude of the signal. As sinusoids are continuous signals, this task allows to investigate whether Skip RNs converge to the same solutions when their parameters are set, but the sampling period is changed. We examine two different sampling periods, Ts = {0.5, 1} milliseconds, for each set of hyperparameters. We train RNs with 110 units each on input signals of 100 milliseconds, with each unit divided on input signals of 50, resulting in an equal number of NNs per input signal, with the same number of samples."}, {"heading": "4.3 MNIST Classification from a Sequence of Pixels", "text": "The handwritten classification benchmark of the MNIST [32] is traditionally addressed with Convolutional Neural Networks (CNNs), which can efficiently exploit spatial dependencies through weight distribution. However, by flattening the 28 \u00d7 28 images into 784-d vectors, it can be reformulated as a challenging task for RNNNs requiring long-term dependencies [31]. We follow the standard data distribution and set aside 5,000 training samples for validation purposes. After processing all pixels with a 110-unit RNN, the last hidden state is fed into a linear classifier that predicts the digital class. All models are designed for 600 epochs to minimize cross-entropy losses. Table 3 summarizes the classification results on the test set after 600 epochs of training. Skip RNNNNs are not only able to solve the task with fewer updates than their counterparts, but also show a lower pull."}, {"heading": "4.4 Sentiment Analysis on IMDB", "text": "The IMDB dataset [34] contains 25,000 training and 25,000 test film reviews, divided into two classes, positive and negative feelings, with an approximate average length of 240 words per review. We set aside 15% of the training data for validation purposes. Words are embedded in 300 d vector representations before being transferred to an RNN of 128 units. The embedding matrix is initialized using pre-trained word2vec4 embeddings [36] when available, or random vectors drawn from U (\u2212 0.25, 0.25), otherwise [27]. Between the last RNN state4https: / / code.google.com / archive / p / word2vec / and the classification layer, failure vectors are applied to reduce overmatch. We evaluate the models from sequences of 200 sequences and select 400 sequences by selecting longer and 51 NN."}, {"heading": "4.5 Action classification on UCF-101", "text": "One of the most accurate and scalable pipelines for video analysis is to extract frame level features with a CNN and model their temporal evolution with an RNN. [17, 52] Videos are usually recorded at high sampling rates and quickly produce long sequences with strong temporal redundancy that are challenging for RNNs. Furthermore, processing frames with a CNN is computationally expensive and can become prohibitive for high frame rates. These problems have been mitigated in previous work by using short clips [17] or downsampling the original data to cover long time spans without excessively increasing the sequence length [52]. Instead of addressing the problem of long sequences at the input data level, we train RNN models with long frame sequences without downsampling and let the network learn which frames must be used to cover long time spans without increasing the sequence length."}, {"heading": "5 Conclusion", "text": "Unlike other approaches, all parameters are trained in Skip RNN with back propagation, without the need to introduce task-specific hyperparameters such as drop-out rates. Experiments conducted with LSTMs and GRUs showed that Skip RNNNs can match or in some cases even exceed the base models while loosening their calculation requirements. Skip RNNNs provide faster and more stable training for long sequences and complex models, probably due to gradients propagated backwards by fewer time steps, resulting in an easier optimization task. Furthermore, the computational savings introduced are better suited to modern hardware than those methods that reduce the amount of calculation required at each step of time [29, 38, 12]. The presented results motivate several new research directions to design efficient RNN architectures."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Spanish Ministry of Economy and Competitiveness under the contracts TIN2012-34557 through the BSC-CNS Severo Ochoa programme (SEV-2011-00067) and the contracts TEC2013-43935-R and TEC2016-75976-R. It was also supported by the grants 2014-SGR-1051 and 2014-SGR-1421 of the Catalan Government and the European Regional Development Fund (ERDF)."}], "references": [{"title": "Encouraging LSTMs to anticipate actions very early", "author": ["M.S. Aliakbarian", "F. Saleh", "M. Salzmann", "B. Fernando", "L. Petersson", "L. Andersson"], "venue": "arXiv preprint arXiv:1703.07023", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Dynamic capacity networks", "author": ["A. Almahairi", "N. Ballas", "T. Cooijmans", "Y. Zheng", "H. Larochelle", "A. Courville"], "venue": "ICML", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1607.06450", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "SLSP", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Y. Bengio", "N. L\u00e9onard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Minimally needed evidence for complex event recognition in unconstrained videos", "author": ["S. Bhattacharya", "F.X. Yu", "S.-F. Chang"], "venue": "ICMR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Quasi-recurrent neural networks", "author": ["J. Bradbury", "S. Merity", "C. Xiong", "R. Socher"], "venue": "ICLR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Recurrent batch normalization", "author": ["T. Cooijmans", "N. Ballas", "C. Laurent", "\u00c7. G\u00fcl\u00e7ehre", "A. Courville"], "venue": "ICLR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank approximations for conditional feedforward computation in deep neural networks", "author": ["A. Davis", "I. Arel"], "venue": "arXiv preprint arXiv:1312.4461", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In ICASSP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["E. Grefenstette", "K.M. Hermann", "M. Suleyman", "P. Blunsom"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural networks for machine learning", "author": ["G. Hinton"], "venue": "Coursera video lectures", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "ECCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Variable computation in recurrent neural networks", "author": ["Y. Jernite", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": "ICLR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["D. Krueger", "T. Maharaj", "J. Kram\u00e1r", "M. Pezeshki", "N. Ballas", "N.R. Ke", "A. Goyal", "Y. Bengio", "H. Larochelle", "A. Courville"], "venue": "Zoneout: Regularizing rnns by randomly preserving hidden activations. In ICLR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution", "author": ["L. Liu", "J. Deng"], "venue": "arXiv preprint arXiv:1701.00299", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "ACL", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Deciding how to decide: Dynamic routing in artificial neural networks", "author": ["M. McGill", "P. Perona"], "venue": "ICML", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "Recurrent models of visual attention. In NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Phased LSTM: accelerating recurrent network training for long or event-based sequences", "author": ["D. Neil", "M. Pfeiffer", "S. Liu"], "venue": "NIPS", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Training a subsampling mechanism in expectation", "author": ["C. Raffel", "D. Lawson"], "venue": "ICLR Workshop Track", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["N. Shazeer", "A. Mirhoseini", "K. Maziarz", "A. Davis", "Q. Le", "G. Hinton", "J. Dean"], "venue": "ICLR", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2017}, {"title": "Clockwork convnets for video semantic segmentation", "author": ["E. Shelhamer", "K. Rakelly", "J. Hoffman", "T. Darrell"], "venue": "arXiv preprint arXiv:1608.03609", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning visual storylines with skipping recurrent neural networks", "author": ["G.A. Sigurdsson", "X. Chen", "A. Gupta"], "venue": "ECCV", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": "arXiv preprint arXiv:1212.0402", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Leaving some stones unturned: dynamic feature prioritization for activity detection in streaming video", "author": ["Y.-C. Su", "K. Grauman"], "venue": "ECCV", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1992}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. In ICML", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end learning of action detection from frame glimpses in videos", "author": ["S. Yeung", "O. Russakovsky", "G. Mori", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to skim text", "author": ["A.W. Yu", "H. Lee", "Q.V. Le"], "venue": "ACL", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["J. Yue-Hei Ng", "M. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "CVPR", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "ICLR", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].", "startOffset": 129, "endOffset": 132}, {"referenceID": 52, "context": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].", "startOffset": 179, "endOffset": 183}, {"referenceID": 37, "context": "These challenges include throughput degradation, slower convergence during training and memory leakage, even for gated architectures [38].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Sequence shortening techniques, which can be seen as a sort of conditional computation [7, 6, 15] in time, can alleviate these issues.", "startOffset": 87, "endOffset": 97}, {"referenceID": 5, "context": "Sequence shortening techniques, which can be seen as a sort of conditional computation [7, 6, 15] in time, can alleviate these issues.", "startOffset": 87, "endOffset": 97}, {"referenceID": 14, "context": "Sequence shortening techniques, which can be seen as a sort of conditional computation [7, 6, 15] in time, can alleviate these issues.", "startOffset": 87, "endOffset": 97}, {"referenceID": 6, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 32, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 1, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 34, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 40, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 17, "context": "This idea has been extended in the temporal domain, either by learning how many times an input needs to be pondered before moving to the next one [18] or building RNNs whose number of layers depends on the input data [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "This idea has been extended in the temporal domain, either by learning how many times an input needs to be pondered before moving to the next one [18] or building RNNs whose number of layers depends on the input data [12].", "startOffset": 217, "endOffset": 221}, {"referenceID": 25, "context": "Some works have addressed time-dependent computation in RNNs by updating only a fraction of the hidden states based on the current hidden state and input [26], or following periodic patterns [29, 38].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "Some works have addressed time-dependent computation in RNNs by updating only a fraction of the hidden states based on the current hidden state and input [26], or following periodic patterns [29, 38].", "startOffset": 191, "endOffset": 199}, {"referenceID": 37, "context": "Some works have addressed time-dependent computation in RNNs by updating only a fraction of the hidden states based on the current hidden state and input [26], or following periodic patterns [29, 38].", "startOffset": 191, "endOffset": 199}, {"referenceID": 11, "context": "This is related to the UPDATE and COPY operations in hierarchical multiscale RNNs [12], but applied to the whole stack of RNN layers at the same time.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "Learning whether to update or copy the hidden state through time steps can be seen as a learnable Zoneout mask [30] which is shared between all the units in the hidden state.", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "Similarly, it can be interpretted as an input-dependent recurrent version of stochastic depth [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "Selecting parts of the input signal is similar in spirit to the hard attention mechanisms that have been applied to image regions [37], where only some patches of the input image are attended in order to generate captions [49] or detect objects [3].", "startOffset": 130, "endOffset": 134}, {"referenceID": 48, "context": "Selecting parts of the input signal is similar in spirit to the hard attention mechanisms that have been applied to image regions [37], where only some patches of the input image are attended in order to generate captions [49] or detect objects [3].", "startOffset": 222, "endOffset": 226}, {"referenceID": 2, "context": "Selecting parts of the input signal is similar in spirit to the hard attention mechanisms that have been applied to image regions [37], where only some patches of the input image are attended in order to generate captions [49] or detect objects [3].", "startOffset": 245, "endOffset": 248}, {"referenceID": 42, "context": "Subsampling input sequences has been explored for visual storylines generation [43], although jointly optimizing the RNN weights and the subsampling mechanism is computationally unfeasible and the Expectation Maximization algorithm is used instead.", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "Similar research has been conducted for video analysis tasks, discovering minimally needed evidence for event recognition [9] and training agents that decide which frames need to be observed in order to localize actions in time [50, 46].", "startOffset": 122, "endOffset": 125}, {"referenceID": 49, "context": "Similar research has been conducted for video analysis tasks, discovering minimally needed evidence for event recognition [9] and training agents that decide which frames need to be observed in order to localize actions in time [50, 46].", "startOffset": 228, "endOffset": 236}, {"referenceID": 45, "context": "Similar research has been conducted for video analysis tasks, discovering minimally needed evidence for event recognition [9] and training agents that decide which frames need to be observed in order to localize actions in time [50, 46].", "startOffset": 228, "endOffset": 236}, {"referenceID": 39, "context": "Motivated by the advantages of training recurrent models on shorter subsequences, efforts have been conducted towards learning differentiable subsampling mechanisms [40], although the computational complexity of the proposed method precludes its application to long input sequences.", "startOffset": 165, "endOffset": 169}, {"referenceID": 9, "context": "Accelerating inference in RNNs is difficult due to their inherently sequential nature, leading to the design of Quasi-Recurrent Neural Networks [10], which relax the temporal dependency between consecutive steps.", "startOffset": 144, "endOffset": 148}, {"referenceID": 50, "context": "With the goal of speeding up RNN inference, LSTM-Jump [51] augments an LSTM cell with a classification layer that will decide how many steps to jump between RNN updates.", "startOffset": 54, "endOffset": 58}, {"referenceID": 47, "context": "Despite its promising results on text tasks, the model needs to be trained with REINFORCE [48], which requires the definition of a reward signal.", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "At every time step t, the probability \u0169t+1 \u2208 [0, 1] of performing a state update at t+ 1 is emitted.", "startOffset": 45, "endOffset": 51}, {"referenceID": 0, "context": "where \u03c3 is the sigmoid function and fbinarize : [0, 1] \u2192 {0, 1} binarizes the input value.", "startOffset": 48, "endOffset": 54}, {"referenceID": 37, "context": "Unlike some other models that aim to reduce the average number of operations per step [38, 26], ours enables skipping steps completely.", "startOffset": 86, "endOffset": 94}, {"referenceID": 25, "context": "Unlike some other models that aim to reduce the average number of operations per step [38, 26], ours enables skipping steps completely.", "startOffset": 86, "endOffset": 94}, {"referenceID": 37, "context": "Replacing RNN updates with copy operations increases the memory of the network and its ability to model long term dependencies even for gated units, since the exponential memory decay observed in LSTM and GRU [38] is alleviated.", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 14, "endOffset": 21}, {"referenceID": 3, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 14, "endOffset": 21}, {"referenceID": 52, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 38, "endOffset": 46}, {"referenceID": 29, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 38, "endOffset": 46}, {"referenceID": 25, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 69, "endOffset": 77}, {"referenceID": 37, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 69, "endOffset": 77}, {"referenceID": 19, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 102, "endOffset": 110}, {"referenceID": 46, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 102, "endOffset": 110}, {"referenceID": 47, "context": "A common method for optimizing functions involving discrete variables is REINFORCE [48], although several estimators have been proposed for the particular case of neurons with binary outputs [7].", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "A common method for optimizing functions involving discrete variables is REINFORCE [48], although several estimators have been proposed for the particular case of neurons with binary outputs [7].", "startOffset": 191, "endOffset": 194}, {"referenceID": 22, "context": "the straight-through estimator [23], which consists in approximating the step function by the identity when computing gradients during the backward pass:", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "This yields a biased estimator that has proven more efficient than other unbiased but high-variance estimators such as REINFORCE [7] and has been successfully applied in different works [14, 12].", "startOffset": 129, "endOffset": 132}, {"referenceID": 13, "context": "This yields a biased estimator that has proven more efficient than other unbiased but high-variance estimators such as REINFORCE [7] and has been successfully applied in different works [14, 12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 11, "context": "This yields a biased estimator that has proven more efficient than other unbiased but high-variance estimators such as REINFORCE [7] and has been successfully applied in different works [14, 12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 32, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 34, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 17, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 25, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 27, "context": "Training is performed with Adam [28], learning rate of 10\u22124, \u03b21 = 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 38, "context": "Gradient clipping [39] with a threshold of 1 is applied to all trainable variables.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "We revisit one of the original LSTM tasks [24], where the network is given a sequence of (value, marker) tuples.", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "[38], where the first marker is randomly placed among the first 10% of samples (drawn with uniform probability) and the second one is placed among the last half of samples (drawn with uniform probability).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "This criterion is a stricter version of the one followed in [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "A similar behavior was observed in other augmented RNN architectures such as Neural Stacks [21].", "startOffset": 91, "endOffset": 95}, {"referenceID": 37, "context": "In this experiment, the network is trained to classify between sinusoids whose period is in range T \u223c U (5, 6) milliseconds and those whose period is in range T \u223c {(1, 5) \u222a (6, 100)} milliseconds [38].", "startOffset": 196, "endOffset": 200}, {"referenceID": 31, "context": "The MNIST handwritten digits classification benchmark [32] is traditionally addressed with Convolutional Neural Networks (CNNs) that can efficiently exploit spatial dependencies through weight", "startOffset": 54, "endOffset": 58}, {"referenceID": 30, "context": "By flattening the 28\u00d7 28 images into 784-d vectors, however, it can be reformulated as a challenging task for RNNs where long term dependencies need to be leveraged [31].", "startOffset": 165, "endOffset": 169}, {"referenceID": 37, "context": "A similar behavior was observed for Phased LSTM, where increasing the sparsity of cell updates accelerates training for very long sequences [38].", "startOffset": 140, "endOffset": 144}, {"referenceID": 48, "context": "Sequences of pixels can be reshaped back into 2D images, allowing to visualize the samples used by the RNNs as a sort of hard visual attention model [49].", "startOffset": 149, "endOffset": 153}, {"referenceID": 33, "context": "The IMDB dataset [34] contains 25,000 training and 25,000 testing movie reviews annotated into two classes, positive and negative sentiment, with an approximate average length of 240 words per review.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "The embedding matrix is initialized using pre-trained word2vec4 embeddings [36] when available, or random vectors drawn from U(\u22120.", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "25) otherwise [27].", "startOffset": 14, "endOffset": 18}, {"referenceID": 50, "context": "We evaluate the models on sequences of length 200 and 400 by cropping longer sequences and padding shorter ones [51].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "One of the most accurate and scalable pipelines for video analysis consists in extracting frame level features with a CNN and modeling their temporal evolution with an RNN [17, 52].", "startOffset": 172, "endOffset": 180}, {"referenceID": 51, "context": "One of the most accurate and scalable pipelines for video analysis consists in extracting frame level features with a CNN and modeling their temporal evolution with an RNN [17, 52].", "startOffset": 172, "endOffset": 180}, {"referenceID": 16, "context": "These issues have been alleviated in previous works by using short clips [17] or by downsampling the original data in order to cover long temporal spans without increasing the sequence length excessively [52].", "startOffset": 73, "endOffset": 77}, {"referenceID": 51, "context": "These issues have been alleviated in previous works by using short clips [17] or by downsampling the original data in order to cover long temporal spans without increasing the sequence length excessively [52].", "startOffset": 204, "endOffset": 208}, {"referenceID": 43, "context": "UCF-101 [44] is a dataset containing 13,320 trimmed videos belonging to 101 different action categories.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Activations in the Global Average Pooling layer from a ResNet-50 [22] CNN pretrained on the ImageNet dataset [16] are used as frame level features, which are fed into two stacked RNN layers with 512 units each.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Activations in the Global Average Pooling layer from a ResNet-50 [22] CNN pretrained on the ImageNet dataset [16] are used as frame level features, which are fed into two stacked RNN layers with 512 units each.", "startOffset": 109, "endOffset": 113}, {"referenceID": 41, "context": "Skip RNN models do not only improve the classification accuracy with respect to the baseline, but require very few updates to do so, possibly due to the low motion between consecutive frames resulting in frame level features with high temporal redundancy [42].", "startOffset": 255, "endOffset": 259}, {"referenceID": 28, "context": "Moreover, the introduced computational savings are better suited for modern hardware than those methods that reduce the amount of computation required at each time step [29, 38, 12].", "startOffset": 169, "endOffset": 181}, {"referenceID": 37, "context": "Moreover, the introduced computational savings are better suited for modern hardware than those methods that reduce the amount of computation required at each time step [29, 38, 12].", "startOffset": 169, "endOffset": 181}, {"referenceID": 11, "context": "Moreover, the introduced computational savings are better suited for modern hardware than those methods that reduce the amount of computation required at each time step [29, 38, 12].", "startOffset": 169, "endOffset": 181}, {"referenceID": 44, "context": "Introducing stochasticity in neural network training has proven beneficial for generalization [45, 30], and in this work we propose a deterministic rounding operation with stochastic sampling.", "startOffset": 94, "endOffset": 102}, {"referenceID": 29, "context": "Introducing stochasticity in neural network training has proven beneficial for generalization [45, 30], and in this work we propose a deterministic rounding operation with stochastic sampling.", "startOffset": 94, "endOffset": 102}, {"referenceID": 0, "context": "the cost can be increased at each time step to encourage the network to emit a decision earlier [1], or the number of updates can be strictly bounded and enforced.", "startOffset": 96, "endOffset": 99}], "year": 2017, "abstractText": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://github.com/imatge-upc/skiprnn-2017-telecombcn.", "creator": "LaTeX with hyperref package"}}}