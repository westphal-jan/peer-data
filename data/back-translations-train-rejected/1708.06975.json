{"id": "1708.06975", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "Generating Visual Representations for Zero-Shot Classification", "abstract": "This paper addresses the task of learning an image clas-sifier when some categories are defined by semantic descriptions only (e.g. visual attributes) while the others are defined by exemplar images as well. This task is often referred to as the Zero-Shot classification task (ZSC). Most of the previous methods rely on learning a common embedding space allowing to compare visual features of unknown categories with semantic descriptions. This paper argues that these approaches are limited as i) efficient discrimi-native classifiers can't be used ii) classification tasks with seen and unseen categories (Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. In contrast , this paper suggests to address ZSC and GZSC by i) learning a conditional generator using seen classes ii) generate artificial training examples for the categories without exemplars. ZSC is then turned into a standard supervised learning problem. Experiments with 4 generative models and 5 datasets experimentally validate the approach, giving state-of-the-art results on both ZSC and GZSC.", "histories": [["v1", "Wed, 23 Aug 2017 12:23:51 GMT  (3632kb,D)", "http://arxiv.org/abs/1708.06975v1", "International Conference on Computer Vision (ICCV) Workshops : TASK-CV: Transferring and Adapting Source Knowledge in Computer Vision, Oct 2017, venise, Italy. International Conference on Computer Vision (ICCV) Workshops, 2017"], ["v2", "Mon, 28 Aug 2017 12:56:18 GMT  (3549kb,D)", "http://arxiv.org/abs/1708.06975v2", "International Conference on Computer Vision (ICCV) Workshops : TASK-CV: Transferring and Adapting Source Knowledge in Computer Vision, Oct 2017, venise, Italy. International Conference on Computer Vision (ICCV) Workshops, 2017"]], "COMMENTS": "International Conference on Computer Vision (ICCV) Workshops : TASK-CV: Transferring and Adapting Source Knowledge in Computer Vision, Oct 2017, venise, Italy. International Conference on Computer Vision (ICCV) Workshops, 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["maxime bucher", "st\\'ephane herbin", "fr\\'ed\\'eric jurie"], "accepted": false, "id": "1708.06975"}, "pdf": {"name": "1708.06975.pdf", "metadata": {"source": "CRF", "title": "Generating Visual Representations for Zero-Shot Classification", "authors": ["Maxime Bucher", "St\u00e9phane Herbin", "Fr\u00e9d\u00e9ric Jurie"], "emails": ["maxime.bucher@onera.fr,", "stephane.herbin@onera.fr", "frederic.jurie@unicaen.fr"], "sections": [{"heading": "1. Introduction and related works", "text": "In fact, the fact is that most of them are able to move to another world, in which they are able, in which they are able to move, in which they are able, in which they move, in which they live."}, {"heading": "2. Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Zero shot classification", "text": "As motivated in the introduction, in this essay we address the problem of learning a classifier that is able to distinguish between a given set of classes in which empirical data is only available for a subset of them, the so-called seen classes. In the vocabulary of zero-shot classification, the problem is usually qualified as inductive - we have no access to data from the unseen classes - as opposed to transductive, in which the invisible data is available but not the associated designations. In this essay, we do not treat the transductive setting, as the availability of target data is a major obstacle in practice.The learning data set Ds is defined by a series of triplets {xsi, asi, ysi} Ns i = 1, in which x s i \"X\" are the raw data (image or characteristics), ysi \"ys\" is the associated class designation, and \"asi\" is a rich semantic representation of the class (text attributes, As, or belongs)."}, {"heading": "2.2. Discriminative approach for ZSC", "text": "The availability of data for the invisible classes has two main advantages: it can make the classification of seen and unseen classes into a single homogeneous process that makes it possible to treat the generalized zero-shot classification as a single supervised classification problem; it potentially enables a larger number of unseen classes, which is required, for example, for datasets like ImageNet [11]. Let D-u = {x-ui, aui, yui} Nu i = 1 be a database that is generated to take into account the invisible semantic class representation au-Au. The ZSC classification function becomes: y-genetic = fD (x-D, D-u, Ds) and can be used in conjunction with the data to learn a superordinate class representation au-Au."}, {"heading": "2.3. Generating unseen data", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2.4. Implementing the generators", "text": "We have implemented our 4 generative models with neural networks, the architectures of which Fig. 2. Hidden layers are fully connected (FC) with leaky-relic nonlinearity [25] (leakage coefficient of 0.2). For models using a classifier (AC-GAN and Adversarial Autoencoder), the classifier is a linear classifier (fully connected layer + Softmax activation function).The loss used to measure the quality of reconstruction in the two autoencoders is the L2 standard. In terms of sampling noise z, we have observed no difference between a Gaussian distribution or even distribution."}, {"heading": "3. Experiments", "text": "In this section, after presenting the datasets and experimental settings, we begin by comparing the different generative models described in the previous section. We then show how our approach can be used for the Generalized Zero-Shot Classification Task, which is one of the paper's key contributions, provide some experiments on a large-scale zero-shot classification task, and finally compare our approach with the most advanced zero-shot approaches to the regular zero-shot classification task."}, {"heading": "3.1. Datasets and Settings", "text": "An initial experimental evaluation is made on 4 standard ZSC datasets: Animals with Attributes (AWA) [22], SUN Attributes (SUN) [31], Apascal & Ayahoo (aP & Y) [14] and Caltech-UCSD Birds-2011 (CUB) [38]. These benchmarks show a wide variety of concepts; SUN and CUB are for fine-grained categorization, and each include birds and scenes images of animals from 50 different categories; finally, aP & Y has broader concepts, from cars to animals."}, {"heading": "3.2. Comparing the different generative models", "text": "Our first round of experiments is to compare the performance of the 4 generative models described in Section 2.3 in the regular zero-shot classification task, and our intention is to select the best performance for further experiments.The performance on the validation set is described in Table 1. We see that the GMMN model outperforms the 3 others on average, with a significant improvement of 5% over aP & Y. Its optimization is also mathematically more stable than the opposite versions, so we have chosen this generator for the following. We explain the superiority of the GMMN model by the fact that it aligns the distributions by using an explicit model of the divergence of distributions, while the contrary autoencoder and AC-GAN have to learn it. The denosing autoencoder, for its part, has no guarantee that the distributions are aligned, which explains its poor performance compared to the 3 other generators."}, {"heading": "3.3. Generalized Zero-Shot Classification task", "text": "In this section, we follow the Generalized Zero-Shot Learning (GZSC) protocol introduced by Chao et al. [9], in which test data from all classes is seen or not seen, a task that is more realistic and difficult because the number of class candidates is greater. We follow the notations of [9], i.e. u \u2192 u: test images from invisible classes, labels from invisible classes (conventional ZSC) s \u2192 s: test images from seen classes, labels from seen classes (multi-class classification for seen classes) u \u2192 a: test images from invisible classes, labels from seen classes (GZSC) s \u2192: test images from seen and invisible classes (GZSC) In the first two cases, only the seen / not seen classes are used in the training phase. In the latter two cases, the classifier is learned with training data that images from all classes are generated (and not seen)."}, {"heading": "3.4. Large Scale Zero-Shot Classification", "text": "We compared our approach to state-of-the-art methods on a large-scale zero-shot classification task. These experiences reflect those presented in [15]: 1000 classes from the classes in the ImageNet 2012 1K set [33] are selected for training (classes seen), while 20,345 are considered other than invisible classes with no available image. Image functions are calculated using the GoogLeNet network [36]. Unlike the ZSC records, no attributes are provided to define invisible classes. We represent these categories using a Skip-gram language model [27], which is learned on a garbage pile of the Wikipedia corpus (3 billion words). Skip-gram is a language model that was learned to predict the context of words. The neural network has 1 input layer, 1 hidden layer, and 1 output layer that is the size of the vocabulary (same size as the input layer)."}, {"heading": "3.5. Classical Zero-Shot Classification task", "text": "In this last section, we follow the protocol of the standard ZSC task: During the training, only data from seen classes are available, while during the test period new images (only from unseen classes) must be assigned to one of the unseen classes. As explained in the introduction, the current ZSC literature [2, 6, 5, 32] focuses mainly on the development of a good em-bedding for comparing attributes and images. One of our motivations for creating training images was to enable the training of discriminatory classifiers, provided it would lead to better performance. This section aims to confirm this hypothesis on the regular ZSC task. Table 5 summarizes our experiments and reports on the accuracy achieved by state-of-the-art methods on the 4 ZSC datasets, with two different deep image characteristics. Each entry is the mean / standard deviation calculated on 5 different tracks. With the VGG network, our method delivers above-average results on the 4 ZSC datasets, with two different deep image characteristics. Each entry is the mean / standard deviation calculated on 5 different tracks."}, {"heading": "4. Conclusions", "text": "This paper introduces a new way to approach ZeroShot Classification and Generalized Zero-Shot Classification tasks by learning a conditional generator from seen data and creating artificial training examples for the categories without examples, making ZSC a standard supervised learning problem. This novel formulation addresses the two main limitations of the previous ZSC method, i.e. its intrinsic bias for Generalized Zero-Shot Classification tasks and its limitations in using discriminatory classifiers in the Deep Image Feature Space. Our experiments with 4 generative models and 5 datasets confirm the approach experimentally and reflect the state of the art."}], "references": [{"title": "Multi-cue Zero-Shot Learning with Strong Supervision", "author": ["Zeynep Akata", "Mateusz Malinowski", "Mario Fritz", "Bernt Schiele"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Evaluation of Output Embeddings for Fine- Grained Image Classification", "author": ["Zeynep Akata", "Scott Reed", "Daniel Walter", "Honglak Lee", "Bernt Schiele"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Hierarchical classification for dealing with the Class imbalance problem", "author": ["Mohamed Bahy Bader-El-Den", "Eleman Teitei", "Mo Adda"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Yoshua Bengio", "Li Yao", "Guillaume Alain", "Pascal Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Improving semantic embedding consistency by metric learning for zero-shot classiffication", "author": ["M Bucher", "S Herbin", "F Jurie"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Hard negative mining for metric learning based zero-shot classification", "author": ["Maxime Bucher", "St\u00e9phane Herbin", "Fr\u00e9d\u00e9ric Jurie"], "venue": "In Computer Vision\u2013ECCV 2016 Workshops,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Embodied gesture learning from one-shot", "author": ["Maria E Cabrera", "Juan P Wachs"], "venue": "In 2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO- MAN,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Synthesized classifiers for zero-shot learning", "author": ["Soravit Changpinyo", "Wei-Lun Chao", "Boqing Gong", "Fei Sha"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "An empirical study and analysis of generalized zeroshot learning for object recognition in the wild", "author": ["Wei-Lun Chao", "Soravit Changpinyo", "Boqing Gong", "Fei Sha"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "On the Benefit of Synthetic Data for Company Logo Detection", "author": ["Christian Eggert", "Anton Winschel", "Rainer Lienhart"], "venue": "In ACM Multimedia,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "TensorFlow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi"], "venue": "on heterogeneous systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Describing objects by their attributes", "author": ["Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "DeViSE: A Deep Visual-Semantic Embedding Model", "author": ["Andrea Frome", "Gregory S Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Generative adversarial nets", "author": ["I Goodfellow", "J Pouget-Abadie", "M Mirza"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Learning from imbalanced data sets with boosting and data generation - the DataBoost- IM approach", "author": ["Hongyu Guo", "Herna L Viktor"], "venue": "SIGKDD Explorations,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Reading Text in the Wild with Convolutional Neural Networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Zero-shot recognition with unreliable attributes", "author": ["Dinesh Jayaraman", "Kristen Grauman"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Attribute-Based Classification for Zero-Shot Visual Object Categorization", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "IEEE Trans Pattern Anal Mach Intell,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard S Zemel"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": "In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing. Citeseer,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Conditional Generative Adversarial Nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1312.5650,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1610.09585,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["Genevieve Patterson", "James Hays"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["Bernardino Romera-Paredes", "Philip HS Torr"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Generative versus Discriminative Methods for Object Recognition", "author": ["Ilkay Ulusoy", "Christopher M Bishop"], "venue": "In CVPR,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Zero-shot visual recognition via bidirectional latent embedding", "author": ["Qian Wang", "Ke Chen"], "venue": "arXiv preprint arXiv:1607.02104,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Wsabie: scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Latent embeddings for zero-shot classification", "author": ["Yongqin Xian", "Zeynep Akata", "Gaurav Sharma", "Quynh Nguyen", "Matthias Hein", "Bernt Schiele"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Zero-shot learning-the good, the bad and the ugly", "author": ["Yongqin Xian", "Bernt Schiele", "Zeynep Akata"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2017}, {"title": "Zero-Shot Learning via Semantic Similarity Embedding", "author": ["Ziming Zhang", "Venkatesh Saligrama"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Zero-shot learning via joint latent similarity embedding", "author": ["Ziming Zhang", "Venkatesh Saligrama"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Zero-Shot Classification (ZSC) [22] addresses classification problems where not all the classes are represented in the training examples.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "As pointed out by [32] this paradigm can be compared to how human can identify a new object from a description of it, leveraging similarities between its description and previously learned concepts.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "[1, 5]) do the classification by defining a zero-shot prediction function that outputs the class y having the maximum compatibility score with seen and unseen classes feature space image generator classifier discriminative Our generative approach", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[1, 5]) do the classification by defining a zero-shot prediction function that outputs the class y having the maximum compatibility score with seen and unseen classes feature space image generator classifier discriminative Our generative approach", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 7, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 13, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 26, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 29, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 36, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 37, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 39, "context": "There are different variants in the recent literature on how the projections or the similarity measure are computed [11, 8, 15, 29, 32, 40, 41, 43], but in all cases the class is chosen as the one maximizing the compatibility score.", "startOffset": 116, "endOffset": 147}, {"referenceID": 34, "context": "However, as it has been observed for a long time [37], discriminative approaches trained for predicting directly the class label have better performance than model-based approaches as long as the learning database reliably samples the target distribution.", "startOffset": 49, "endOffset": 53}, {"referenceID": 34, "context": "Despite one can expect discriminative methods to give better performance [37], they can\u2019t be used directly in the case of ZSC for obvious reasons: as no images are available for some categories, discriminative classifiers cannot be learned out-of-the-box.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": ", [18] or [3] to compensate for imbalanced training sets.", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": ", [18] or [3] to compensate for imbalanced training sets.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "Generating novel training examples from the existing ones is also at the heart of the technique called Data Augmentation, frequently used for training deep neural networks [23].", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "[12] generated images by applying warping and other geometric / photometric transformations to prototypical logo exemplars.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "A similar idea was also presented in [19] for text spotting in images.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "[7] capture what they call The Gist of a Gesture by recording human gestures, representing them by a model and use this model to generate a large set of realistic gestures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "A relevant way to learn this transformation is to use generative models such as denoising auto encoders [4] and generative adversarial nets (GAN) [16] or their variants [10, 26].", "startOffset": 104, "endOffset": 107}, {"referenceID": 14, "context": "A relevant way to learn this transformation is to use generative models such as denoising auto encoders [4] and generative adversarial nets (GAN) [16] or their variants [10, 26].", "startOffset": 146, "endOffset": 150}, {"referenceID": 25, "context": "The Conditional Generative Adversarial Nets of [28] is a very relevant variant adapted to our problem.", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "This problem, introduced in [9], assumes that both seen and unseen categories are present at test time, making the traditional approaches suffering from bias decision issues.", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "This paper experimentally validates the proposed strategy on 4 standard Zero-Shot classification datasets (Animals with Attributes (AWA) [22], SUN attributes (SUN) [31], Apascal&Ayahoo (aP&Y) [14] and Caltech-UCSD Birds-200-2011 (CUB) [38]), and gives insight on how the approach scales on large datasets such as ImageNet [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 28, "context": "This paper experimentally validates the proposed strategy on 4 standard Zero-Shot classification datasets (Animals with Attributes (AWA) [22], SUN attributes (SUN) [31], Apascal&Ayahoo (aP&Y) [14] and Caltech-UCSD Birds-200-2011 (CUB) [38]), and gives insight on how the approach scales on large datasets such as ImageNet [11].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "This paper experimentally validates the proposed strategy on 4 standard Zero-Shot classification datasets (Animals with Attributes (AWA) [22], SUN attributes (SUN) [31], Apascal&Ayahoo (aP&Y) [14] and Caltech-UCSD Birds-200-2011 (CUB) [38]), and gives insight on how the approach scales on large datasets such as ImageNet [11].", "startOffset": 192, "endOffset": 196}, {"referenceID": 9, "context": "This paper experimentally validates the proposed strategy on 4 standard Zero-Shot classification datasets (Animals with Attributes (AWA) [22], SUN attributes (SUN) [31], Apascal&Ayahoo (aP&Y) [14] and Caltech-UCSD Birds-200-2011 (CUB) [38]), and gives insight on how the approach scales on large datasets such as ImageNet [11].", "startOffset": 322, "endOffset": 326}, {"referenceID": 9, "context": "The availability of data for the unseen classes has two main advantages: it can make the classification of seen and unseen classes as a single homogeneous process, allowing to address Generalized Zero Shot Classification as a single supervised classification problem; it potentially allows a larger number of unseen classes, which is for instance required for datasets such ImageNet [11].", "startOffset": 383, "endOffset": 387}, {"referenceID": 22, "context": "Generative Moment Matching Network A first approach is to adapt the Generative Moment Matching Network (GMMN) proposed in [24] to conditioning.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "This divergence can be approximated using a Hilbert kernel based statistics [17] \u2013 typically a linear combination of Gaussian functions with various widths \u2014 which has the big advantage of being differentiable and may be thus exploited as a machine learning cost.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "One extension allowing to produce conditional distributions is the AC-GAN [30] (Fig.", "startOffset": 74, "endOffset": 78}, {"referenceID": 3, "context": "Denoising Auto-Encoder Our third generator relies on the work presented in [4], where an encoder/decoder structure is proposed to design a data generator, the latent code playing the role of the random prior z used to generate the data.", "startOffset": 75, "endOffset": 78}, {"referenceID": 23, "context": "Hidden layers are fully connected (FC) with leaky-relu nonlinearity [25] (leakage coefficient of 0.", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "A first experimental evaluation is done on 4 standard ZSC datasets: Animals with Attributes (AWA) [22], SUN attributes (SUN) [31], Apascal&Ayahoo (aP&Y) [14] and Caltech-UCSD Birds-200-2011 (CUB) [38] .", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "A first experimental evaluation is done on 4 standard ZSC datasets: Animals with Attributes (AWA) [22], SUN attributes (SUN) [31], Apascal&Ayahoo (aP&Y) [14] and Caltech-UCSD Birds-200-2011 (CUB) [38] .", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "A first experimental evaluation is done on 4 standard ZSC datasets: Animals with Attributes (AWA) [22], SUN attributes (SUN) [31], Apascal&Ayahoo (aP&Y) [14] and Caltech-UCSD Birds-200-2011 (CUB) [38] .", "startOffset": 153, "endOffset": 157}, {"referenceID": 20, "context": "In order to make comparisons with other works, we follow the same training/testing splits for AwA [22], CUB [2] and aP&Y [14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "In order to make comparisons with other works, we follow the same training/testing splits for AwA [22], CUB [2] and aP&Y [14].", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "In order to make comparisons with other works, we follow the same training/testing splits for AwA [22], CUB [2] and aP&Y [14].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "For SUN we experiment two different settings: one with 10 unseen classes as in [20], a Table 1: Zero-Shot classification accuracy (mean) on the validation set, for the 4 generative models.", "startOffset": 79, "endOffset": 83}, {"referenceID": 3, "context": "[4] 62.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "4 AC-GAN [30] 55.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "3 GMMN [24] 65.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "second, more competitive, with ten different folds randomly chosen and averaged, as proposed by [8] (72/71 splits).", "startOffset": 96, "endOffset": 99}, {"referenceID": 31, "context": "Image features are computed using two deep networks, the VGG-VeryDeep-19 [34] and the GoogLeNet [36] networks.", "startOffset": 73, "endOffset": 77}, {"referenceID": 33, "context": "Image features are computed using two deep networks, the VGG-VeryDeep-19 [34] and the GoogLeNet [36] networks.", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "They are optimized with the Adam solver [21] with a cross-validated learning rate (typically of 10\u22124), using mini-batches of size 128 except for the GMMN where each batch contains all the training images of one class, to make the estimation of the statistics more reliable.", "startOffset": 40, "endOffset": 44}, {"referenceID": 32, "context": "In order to avoid over-fitting, we used dropout [35] at every layer (probability of drop of 0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "Input data (both image features and w2c vectors) are scaled to [0,1] by applying an affine transformation.", "startOffset": 63, "endOffset": 68}, {"referenceID": 11, "context": "With the TensorFlow framework [13] running on a Nvidia Titan X pascal GPU, the learning stage takes around 10 minutes for a given set of hyper-parameters.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "[36] CNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22]dap 51.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22]iap 56.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] 63.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8]o\u2212vs\u2212o 70.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8]struct 73.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "Image features are obtained with the GoogLeNet [36] CNN.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "[22]dap 38.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22]iap 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8]o\u2212vs\u2212o 53.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8]struct 54.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We follow the notations of [9], i.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": ", [2, 6, 5, 32] are focused on improving the embedding or the scoring function.", "startOffset": 2, "endOffset": 15}, {"referenceID": 5, "context": ", [2, 6, 5, 32] are focused on improving the embedding or the scoring function.", "startOffset": 2, "endOffset": 15}, {"referenceID": 4, "context": ", [2, 6, 5, 32] are focused on improving the embedding or the scoring function.", "startOffset": 2, "endOffset": 15}, {"referenceID": 29, "context": ", [2, 6, 5, 32] are focused on improving the embedding or the scoring function.", "startOffset": 2, "endOffset": 15}, {"referenceID": 8, "context": "However, [9] has shown that this type of approach is unpractical with GZSC.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "These experiences mirror those presented in [15]: 1000 classes from those of the ImageNet 2012 1K set [33] are chosen for training (seen classes) while 20.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "These experiences mirror those presented in [15]: 1000 classes from those of the ImageNet 2012 1K set [33] are chosen for training (seen classes) while 20.", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "Image features are computed with the GoogLeNet network [36].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "We represent those categories using a skip-gram language model [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "Flat Hit @K Scenario Method 1 2 5 10 20 2-hop Frome [15] 6.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "4 Norouzi [29] 9.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "8 Changpinyo [8] 10.", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "31 2-hop Frome [15] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "7 (+1K) Norouzi [29] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "3-hop Frome [15] 1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "5 Norouzi [29] 2.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "1 Changpinyo [8] 2.", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "88 3-hop Frome [15] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "7 (+1K) Norouzi [29] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "All Frome [15] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "0 Norouzi [29] 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "3 Changpinyo [8] 1.", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "14 All Frome [15] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "3 (+1K) Norouzi [29] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "For fair comparison, we take the same language model as [8] with the same classes excluded.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "As in [8, 15] our model is evaluated on three different scenarios, with an increasing number of unseen classes: i) 2-hop: 1,509 classes ii) 3-hop: 7,678 classes, iii) All: all unseen categories.", "startOffset": 6, "endOffset": 13}, {"referenceID": 13, "context": "As in [8, 15] our model is evaluated on three different scenarios, with an increasing number of unseen classes: i) 2-hop: 1,509 classes ii) 3-hop: 7,678 classes, iii) All: all unseen categories.", "startOffset": 6, "endOffset": 13}, {"referenceID": 1, "context": "As explained in the introduction, the recent ZSC literature [2, 6, 5, 32] mostly focuses on developing a good emTable 5: Zero-shot classification accuracy (mean\u00b1std) on 5 runs.", "startOffset": 60, "endOffset": 73}, {"referenceID": 5, "context": "As explained in the introduction, the recent ZSC literature [2, 6, 5, 32] mostly focuses on developing a good emTable 5: Zero-shot classification accuracy (mean\u00b1std) on 5 runs.", "startOffset": 60, "endOffset": 73}, {"referenceID": 4, "context": "As explained in the introduction, the recent ZSC literature [2, 6, 5, 32] mostly focuses on developing a good emTable 5: Zero-shot classification accuracy (mean\u00b1std) on 5 runs.", "startOffset": 60, "endOffset": 73}, {"referenceID": 29, "context": "As explained in the introduction, the recent ZSC literature [2, 6, 5, 32] mostly focuses on developing a good emTable 5: Zero-shot classification accuracy (mean\u00b1std) on 5 runs.", "startOffset": 60, "endOffset": 73}, {"referenceID": 7, "context": "* [8] features extracted from an MIT Places[45] pretrained model.", "startOffset": 2, "endOffset": 5}, {"referenceID": 41, "context": "* [8] features extracted from an MIT Places[45] pretrained model.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "[22] - 60.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] - 66.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] - 72.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "N et [3 6]", "startOffset": 5, "endOffset": 10}, {"referenceID": 5, "context": "N et [3 6]", "startOffset": 5, "endOffset": 10}, {"referenceID": 37, "context": "[41] - 71.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] 38.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "00/Romera-Paredes [32] 24.", "startOffset": 18, "endOffset": 22}, {"referenceID": 39, "context": "[43] 46.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[44] 50.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[39] - 78.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] 53.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3 4]", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "[3 4]", "startOffset": 0, "endOffset": 5}, {"referenceID": 5, "context": "[6] 56.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8]\u2019s seems to give better performance but used the MIT Places dataset to learn the features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "[42] that this database \u201dintersects with both training and test classes of SUN, which could explain their better results compared to ours.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "This paper addresses the task of learning an image classifier when some categories are defined by semantic descriptions only (e.g. visual attributes) while the others are defined by exemplar images as well. This task is often referred to as the Zero-Shot classification task (ZSC). Most of the previous methods rely on learning a common embedding space allowing to compare visual features of unknown categories with semantic descriptions. This paper argues that these approaches are limited as i) efficient discriminative classifiers can\u2019t be used ii) classification tasks with seen and unseen categories (Generalized Zero-Shot Classification or GZSC) can\u2019t be addressed efficiently. In contrast, this paper suggests to address ZSC and GZSC by i) learning a conditional generator using seen classes ii) generate artificial training examples for the categories without exemplars. ZSC is then turned into a standard supervised learning problem. Experiments with 4 generative models and 5 datasets experimentally validate the approach, giving state-of-the-art results on both ZSC and GZSC.", "creator": "LaTeX with hyperref package"}}}