{"id": "1708.06989", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "A Neural Network Approach for Mixing Language Models", "abstract": "The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.", "histories": [["v1", "Wed, 23 Aug 2017 13:27:16 GMT  (55kb,D)", "http://arxiv.org/abs/1708.06989v1", "Published at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017. arXiv admin note: text overlap witharXiv:1703.08068"]], "COMMENTS": "Published at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017. arXiv admin note: text overlap witharXiv:1703.08068", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["youssef oualil", "dietrich klakow"], "accepted": false, "id": "1708.06989"}, "pdf": {"name": "1708.06989.pdf", "metadata": {"source": "CRF", "title": "A NEURAL NETWORK APPROACH FOR MIXING LANGUAGE MODELS", "authors": ["Youssef Oualil", "Dietrich Klakow"], "emails": ["firstname.lastname@lsv.uni-saarland.de"], "sections": [{"heading": null, "text": "Index Terms - Neural Networks, Mixing Models, Language Modeling"}, {"heading": "1. INTRODUCTION", "text": "For many language technologies, such as speech recognition [1] and machine translation [2], a high-quality language model (LM) has been developed that is considered a key component to success. Traditionally, it aims to predict likely sequences of predefined linguistic units, which are typically guided by the semantic and syntactic properties that usually overcome the exponential growth of parameters based on language modeling."}, {"heading": "2. NEURAL NETWORK LANGUAGE MODELS", "text": "The goal of a language model is to estimate the probability distribution p (wT1) of the word sequences wT1 = w1, \u00b7 \u00b7, wT. Using the chain rule, this distribution can be expressed as p (wT1) = T-t = 1 p (wt | wt \u2212 11) (1). Let U be a word embedding the matrix, and let W be the hidden output weights. NN-based LMs (NNLMs) that consider word embeddings as input approach a bottom-up evaluation of the network according to Ht = M (P, Rt \u2212 1, U) (2) Ot = g (Ht \u00b7 W) (3) ar Xiv: 170 8.06 989v 1 [cs.C L] 23 Aug 201 7whereM represents a certain NN-based model that can be a deep architecture, P denotes its parameters, and Rt \u2212 1 its general repeatability."}, {"heading": "2.1. Feedforward Neural Networks", "text": "Similar to N-gram models, FNN uses the Markov assumption of the order N \u2212 1 to approximate (1), i.e. the current word depends only on the last N \u2212 1 words. Subsequently, M is encoded by Et \u2212 i = Xt \u2212 i \u00b7 U, i = N \u2212 1, \u00b7 \u00b7, 1 (4) Ht = f (N \u2212 1 \u2211 i = 1 Et \u2212 i \u00b7 V i) (5) Xt \u2212 i as a uniform encoding of the word wt \u2212 i. Thus, Et \u2212 i is the continuous representation of the word wt \u2212 i. f (\u00b7) is an activation function. Consequently, for a FNN model, M P = {V i} is N \u2212 1i = 1 and R t \u2212 1 = \u2205."}, {"heading": "2.2. Recurrent Neural Networks", "text": "RNN tries to capture the entire history in a context vector ht, which represents the state of the network and develops over time. Therefore, RNN approaches each term in (1) as p (wt | wT1) \u2248 p (wt | ht). Consequently, M is given for an RNN by Ht = f (Xt \u2212 1 \u00b7 U + Ht \u2212 1 \u00b7 V) (6)."}, {"heading": "2.3. Long-Short Term Memory Networks", "text": "To mitigate the rapidly changing context problem with standard RNNs and to control the longevity of the network dependency models, the LSTM architecture [10] introduces an internal memory state Ct, which explicitly controls the amount of information that can be forgotten or added to the network before estimating the current hidden state. Formally, an LSTM model is given by Et \u2212 1 = Xt \u2212 1 \u00b7 U (7) {i, f, o} t = \u03c3 (V i, f, ow \u00b7 Et \u2212 1 + V i, f, oh \u00b7 H t \u2212 1) (8) C-t = f (V cw \u00b7 Et \u2212 1 + V ch \u00b7 Ht \u2212 1) (9) Ct = f t t \u2212 1 + it C-t (10) Ht = ot f (Ct) (Ct) (11) (f), where the element product C-t is the memory candidate, whereas f, f, f, f and the input of Hch, Ct, Ct, Hch, Ht, Ct, Ct, Ht, mot, Hch,}, Ht."}, {"heading": "3. NEURAL NETWORK MIXTURE MODELS", "text": "Contrary to a variety of research directions on improving or designing (new) specialized neural architectures for language modeling, the work presented in this paper is an attempt to design a general architecture that is capable of combining different types of existing heterogeneous models rather than examining new ones."}, {"heading": "3.1. Model Combination for Language Modeling", "text": "The work presented in this paper is based on recent research showing that model combinations can lead to a significant improvement in LM performance [14], typically by either designing deep networks with different architectures at different levels, as has been done in [11]. However, this category of model combination requires a careful selection of the architectures to be combined for a well-suited feature design, as it can be difficult / slow to train, while the second category 2) combines different models at the output level, as is done in the RNN model with maximum entropy [13] or using classic linear interpolation [14], which typically results in a significant increase in the number of parameters when combining multiple models. In a first attempt to circumvent these problems, we recently proposed an SRNN model [15] that combines FFN information and RNN information through additional sequential combinations on the general model NN was not suitable for the underlying problems, and therefore NN was not successful in combining it with other models."}, {"heading": "3.2. Neural Network Mixture Models", "text": "This section introduces the mathematical formulation of the proposed mixture model. Let {Mm} Mm = 1 be a set of M models that must be combined and let {Pm, Rtm} Mm = 1 be their corresponding model parameters or recurring information at the time. For the basic NNLMs, namely FNN, RNN and LSTM, Mm, Pm and Rtm have been specified in Section 2. Let U be the common word embedding matrix that is learned during the training of all models in the mix.The mixture model is given by the following steps (see Figure 1): 1) Feature level: Update each model and calculate its properties Htm = Mm (Pm, Rt \u2212 1m, U), m = 1, \u00b7 \u00b7, M (12)."}, {"heading": "2) Mixture layer: combine the different features", "text": "HtMixture = fMixture (M \u2211 m = 1Htm \u00b7 Sm) (13)"}, {"heading": "3) Output layer: calculate the output using a softmax function", "text": "Ot = g (Htmixture \u00b7 W) (14) fmixture is a non-linear mixing function, whereas Sm, m = 1, \u00b7 \u00b7, M are the mixing weights (matrices). Although the experiments in this work mainly comprise FNN, RNN and LSTM, the set of possible model selection formMm is not limited to these, but includes all NN-based models that use word embedding as input. The proposed mixing model uses a single word embedding matrix and a single output layer with predefined and fixed sizes. The latter are independent of the size of the mixing models. This model does not suffer from the significant parameter growth in the increase in the number of models in the mix. We can also see that this architecture does not impose any direct limitations on the number of models to be combined, their size or their type."}, {"heading": "3.3. Training of Neural Mixture Models", "text": "At this stage, each model receives a network error, updates its parameters, and propagates its error to the shared word embedding (input) level. We should also mention at this point that recurring models can be \"unfolded\" over time, regardless of the other models in the mix, as is the case with standard networks. Once each model is updated, the continuous word representations are also updated, taking into account the individual network errors resulting from the different models in the mix (see figure in Figure 1). Joint training of the mixing models is expected to result in a \"complementarity effect.\" By \"complementarity,\" we mean that the mixing models perform poorly when evaluated individually, but perform much better when tested together. This is typically a result of learning and modeling the models, with other features ultimately leading to greater expressiveness."}, {"heading": "3.4. Model Dropout", "text": "To 1) force the participation of models and 2) avoid over-adjustment of the network when the number of models in the mix is large, we use a dropout technique inspired by the standard dropout regulation [16], widely used for the formation of neural networks. The idea here is that \"models\" replace the \"neurons\" in the standard dropout model. Therefore, for each training example, a model with the probability of dropping pd should be used. Then, only models that are selected contribute to the mix and their parameters and mixing weights Sm are updated. Similar to the standard dropout model, the dropout model is only applied to non-recurring models in the mix."}, {"heading": "4. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setup", "text": "We evaluated the proposed architecture on two different benchmark tasks. The first set of experiments was conducted on Penn Treebank (PTB) Corpus with the standard division, e.g. [9, 17]; Sections 0-20 are used for training, while Sections 21-22 and 23-24 \u00b7 \u00b7 In addition, \u00b7 a series of experiments are used on the Large Text Compression Benchmark (LTCB) [18]. This corpus is based on the enwik9 dataset, which contains the first 109 bytes of the enwiki 20060303-page article; we conduct a series of experiments on the Large Text Compression Benchmark (LTCB) [18]. This corpus is based on the enwik9 dataset, which contains the mix."}, {"heading": "4.2. PTB Experiments", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We've never been able to unite, \"he said."}, {"heading": "4.3. LTCB Experiments", "text": "The results shown in Table 3 follow the same experimental setup used in [15]. Specifically, these results were obtained without the use of impulses, model failure or weight degradation, whereas the size of the LTCB was set at 400. FNN architecture contains 2 hidden layers of size 600, whereas RNN, LSTM, SRNN and NMM have a single hidden layer of size 600. LTCB results shown in Table 3 generally confirm the conclusions of PTB. In particular, we can see that the combination of recurring models with half (third for RNN) of their original size with a single FNN model results in comparable performance compared to the base models. Furthermore, increasing the mix size (for LSTM) or increasing the number of FNNNs (for RNN) further improves performance without noticeably increasing the learning ability of the NNP models."}, {"heading": "5. CONCLUSION AND FUTURE WORK", "text": "We have presented a neural blending model that is capable of combining heterogeneous NN-based LMs in a single architecture. Experiments on PTB and LTCB corpora have shown that this architecture significantly outperforms many modern neural systems due to its ability to combine learning abilities of different architectures. Further benefits could be achieved through more advanced model selection or combination of features on the mixed layer, rather than through simple model weighting, which will be investigated in future work."}, {"heading": "6. REFERENCES", "text": "[1] Slava M. Katz, \"Estimation of probabilities from spare data for the language model component of a speech recognition,\" IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400-401, March 1987. [2] Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin, \"A statistical approach to machine translation,\" Computational Linguistics, vol. 16, no. 2, pp."}], "references": [{"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Slava M. Katz"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400\u2013401, Mar. 1987.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "A statistical approach to machine translation", "author": ["Peter F. Brown", "John Cocke", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Fredrick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Computational Linguistics, vol. 16, no. 2, pp. 79\u201385, Jun. 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Two decades of statistical language modeling: where do we go from here", "author": ["Ronald Rosenfeld"], "venue": "Proceedings of the IEEE, vol. 88, no. 8, pp. 1270\u20131278, Aug. 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Improved backingoff for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Detroit, Michigan, USA, May 1995, pp. 181\u2013184.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, Mar. 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Training neural network language models on very large corpora", "author": ["Holger Schwenk", "Jean-Luc Gauvain"], "venue": "Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP), Oct. 2005, pp. 201\u2013208.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A bit of progress in language modeling, extended version", "author": ["Joshua Goodman"], "venue": "Tech. Rep. MSR-TR-2001-72, Microsoft Research, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "11th Annual Conference of the International Speech Communication Association (INTERSPEECH), Makuhari, Chiba, Japan, Sep. 2010, pp. 1045\u20131048.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Prague, Czech Republic, May 2011, pp. 5528\u20135531.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "13th Annual Conference of the International Speech Communication Association (INTERSPEECH), Portland, Oregon, USA, Sep. 2012, pp. 194\u2013197.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Character-aware neural language models", "author": ["Kim Yoon", "Jernite Yacine", "Sontag David", "Rush Alexander M."], "venue": "30th AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar A P", "Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "Advances in Neural Information Processing Systems 27, pp. 1853\u20131861. 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1853}, {"title": "Strategies for training large scale neural network language models", "author": ["Tomas Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1s Burget", "Jan Cernock\u00fd"], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), Waikoloa, Hawaii, USA, Dec. 11-15, 2011, pp. 196\u2013201.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Luk\u00e1s Burget", "Jan Cernock\u00fd"], "venue": "12th Annual Conference of the International Speech Communication Association (INTERSPEECH), Florence, Italy, Aug. 27-31, 2011, pp. 605\u2013608.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential recurrent neural network for language modeling", "author": ["Youssef Oualil", "Clayton Greenberg", "Mittul Singh", "Dietrich Klakow"], "venue": "17th Annual Conference of the International Speech Communication Association (INTERSPEECH), San Francisco, California, USA, Sep. 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1929}, {"title": "The fixed-size ordinally-forgetting encoding method for neural network language models", "author": ["Shiliang Zhang", "Hui Jiang", "Mingbin Xu", "Junfeng Hou", "Li-Rong Dai"], "venue": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL, July 2015, vol. 2, pp. 495\u2013500.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Large text compression benchmark", "author": ["Matt Mahoney"], "venue": "2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), Chia Laguna Resort, Sardinia, Italy, May 2010, pp. 249\u2013256.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "For many language technology applications such as speech recognition [1] and machine translation [2], a high quality Language Model (LM) is considered to be a key component to success.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "For many language technology applications such as speech recognition [1] and machine translation [2], a high quality Language Model (LM) is considered to be a key component to success.", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "The recent advances in neural network-based approaches for language modeling led to a significant improvement over the standard N -gram models [3, 4].", "startOffset": 143, "endOffset": 149}, {"referenceID": 3, "context": "The recent advances in neural network-based approaches for language modeling led to a significant improvement over the standard N -gram models [3, 4].", "startOffset": 143, "endOffset": 149}, {"referenceID": 4, "context": "[5], who proposed a Feedforward Neural Network (FNN) model as an alternative to N -grams.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Although FNNs were shown to perform very well for different tasks [6, 7], their fixed context (word history) size constraint was a limiting factor for their performance.", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "Although FNNs were shown to perform very well for different tasks [6, 7], their fixed context (word history) size constraint was a limiting factor for their performance.", "startOffset": 66, "endOffset": 72}, {"referenceID": 7, "context": "[8, 9] proposed a Recurrent Neural Network (RNN), which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[8, 9] proposed a Recurrent Neural Network (RNN), which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "Investigating the inherent shortcomings of RNNs led to the Long-Short Term Memory (LSTM)-based LMs [10], which explicitly control the longevity of context information in the network.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "This chain of novel NN-based LMs continued with more complex and advanced models such as Convolutional Neural Networks (CNN) [11] and autoencoders [12], to name a few.", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "This chain of novel NN-based LMs continued with more complex and advanced models such as Convolutional Neural Networks (CNN) [11] and autoencoders [12], to name a few.", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11], which combines LSTM, CNN and a highway network, or by 2) combining different models at the output layer, as it is done in the maximum entropy RNN model [13], which uses direct N -gram connections to the output layer, or using the classical linear interpolation [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11], which combines LSTM, CNN and a highway network, or by 2) combining different models at the output layer, as it is done in the maximum entropy RNN model [13], which uses direct N -gram connections to the output layer, or using the classical linear interpolation [14].", "startOffset": 290, "endOffset": 294}, {"referenceID": 13, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11], which combines LSTM, CNN and a highway network, or by 2) combining different models at the output layer, as it is done in the maximum entropy RNN model [13], which uses direct N -gram connections to the output layer, or using the classical linear interpolation [14].", "startOffset": 399, "endOffset": 403}, {"referenceID": 12, "context": "Motivated by the work in [13], we have recently proposed a Sequential Recurrent Neural Network (SRNN) [15], which combines FFN information and RNN.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Motivated by the work in [13], we have recently proposed a Sequential Recurrent Neural Network (SRNN) [15], which combines FFN information and RNN.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "In order to alleviate the rapidly changing context issue in standard RNNs and control the longevity of the dependencies modeling in the network, the LSTM architecture [10] introduces an internal memory state C, which explicitly controls the amount of information, to forget or to add to the network, before estimating the current hidden state.", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "The work presented in this paper is motivated by recent research showing that model combination can lead to a significant improvement in LM performance [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "This category of model combination, however, requires a careful selection of the architectures to combine for a well-suited feature design, as it can be difficult/slow to train, whereas the second category 2) combines different models at the output layer, as it is done in the maximum entropy RNN model [13] or using the classical linear interpolation [14].", "startOffset": 303, "endOffset": 307}, {"referenceID": 13, "context": "This category of model combination, however, requires a careful selection of the architectures to combine for a well-suited feature design, as it can be difficult/slow to train, whereas the second category 2) combines different models at the output layer, as it is done in the maximum entropy RNN model [13] or using the classical linear interpolation [14].", "startOffset": 352, "endOffset": 356}, {"referenceID": 14, "context": "In a first attempt to circumvent these problems, we have recently proposed an SRNN model [15], which combines FFN information and RNN through additional sequential connections at the hidden layer.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "We use a model dropout technique, which is inspired by the standard dropout regularization [16] that is widely used to train neural networks.", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "[9, 17]; sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[9, 17]; sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "In order to evaluate how the proposed approach scales to large corpora, we run a set of experiments on the Large Text Compression Benchmark (LTCB) [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "We adopted the same trainingtest-validation data split and pre-processing from [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "In particular, we compare our model to the FNN-based LM [5], the full RNN [9] (without classes) as well as RNN with maximum entropy (RNNME) [13].", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "In particular, we compare our model to the FNN-based LM [5], the full RNN [9] (without classes) as well as RNN with maximum entropy (RNNME) [13].", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "In particular, we compare our model to the FNN-based LM [5], the full RNN [9] (without classes) as well as RNN with maximum entropy (RNNME) [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "We also report results for the LSTM architecture [10], and the recently proposed SRNN model [15].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "We also report results for the LSTM architecture [10], and the recently proposed SRNN model [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "The weights initialization follows the normalized initialization proposed in [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Similarly to [8], the learning rate is halved when no significant improvement in the log-likelihood of the validation data is observed.", "startOffset": 13, "endOffset": 16}, {"referenceID": 14, "context": "[15, 13], that recurrent models can be further improved using N-gram/feedforward information, given that they model different linguistic features.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[15, 13], that recurrent models can be further improved using N-gram/feedforward information, given that they model different linguistic features.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "The results shown in Table 3 follow the same experimental setup used in [15].", "startOffset": 72, "endOffset": 76}], "year": 2017, "abstractText": "The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.", "creator": "LaTeX with hyperref package"}}}