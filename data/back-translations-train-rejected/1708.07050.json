{"id": "1708.07050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "Capturing Long-term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition", "abstract": "The goal of continuous emotion recognition is to assign an emotion value to every frame in a sequence of acoustic features. We show that incorporating long-term temporal dependencies is critical for continuous emotion recognition tasks. To this end, we first investigate architectures that use dilated convolutions. We show that even though such architectures outperform previously reported systems, the output signals produced from such architectures undergo erratic changes between consecutive time steps. This is inconsistent with the slow moving ground-truth emotion labels that are obtained from human annotators. To deal with this problem, we model a downsampled version of the input signal and then generate the output signal through upsampling. Not only does the resulting downsampling/upsampling network achieve good performance, it also generates smooth output trajectories. Our method yields the best known audio-only performance on the RECOLA dataset.", "histories": [["v1", "Wed, 23 Aug 2017 15:27:00 GMT  (1701kb,D)", "http://arxiv.org/abs/1708.07050v1", "5 pages, 5 figures, 2 tables, Interspeech 2017"]], "COMMENTS": "5 pages, 5 figures, 2 tables, Interspeech 2017", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["soheil khorram", "zakaria aldeneh", "dimitrios dimitriadis", "melvin mcinnis", "emily mower provost"], "accepted": false, "id": "1708.07050"}, "pdf": {"name": "1708.07050.pdf", "metadata": {"source": "CRF", "title": "Capturing Long-term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition", "authors": ["Soheil Khorram", "Zakaria Aldeneh", "Dimitrios Dimitriadis", "Melvin McInnis", "Emily Mower Provost"], "emails": ["emilykmp}@umich.edu,", "dbdimitr@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "Emotion recognition has many potential applications, including the construction of more natural human-computer interfaces. Emotion can be quantified by categorical classes (e.g. neutral, happy, sad, etc.) or by dimensional values (e.g. valence arousal). In addition, emotional labels can be quantified statically, by language units (e.g. utterances) or continuously over time. In this work, we focus on problems where the goal is to detect emotions in valence-aroused space, continuously over time. Valence-aroused space is a psychologically based method of describing emotions [1]. Value ranges from negative to positive, while activation ranges from calm to aroused. Research has shown that it is crucial to include long-term time information in order to make accurate emotion predictions. Thus, Valstar et al al al al al al al al al al al al al al al al al al al. [2] showed that it was necessary to include larger windows when creating emotion predictions at the herence level (to be precise for six seconds) and Le valence (to four seconds for valence)."}, {"heading": "2. Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "3. Problem Setup", "text": "We focus on the RECOLA database [11] following the guidelines of AVEC 2016 [2]. The RECOLA database consists of spontaneous interactions in French and provides continuous, dimensional (valence and arousal) ground-truth descriptions of emotions. Although the challenge of AVEC 2016 is multimodal in nature, in this work we focus only on language modality. The RECOLA database contains a total of 27 five-minute expressions, each by a specific speaker (9 turn; 9 validation; 9 test). The continuous annotations were based on a temporal granularity of 40 ms consisting of six annotations (three women).Features. We use the Kaldi toolkit [12] to extract 40-dimensional protocol CCFB characteristics using a window length of 25 ms with a size of 10 ms. Previous work has shown that MFB characteristics are better than conventional prediction of emotions MFCs."}, {"heading": "4. Preliminary Experiment", "text": "The network we use in the preparatory experiments consists of a revolutionary layer with a filter of different lengths from 2 to 2048 frames, followed by a tanh nonlinearity, followed by a linear regression layer. We vary the length of the filter and validate performance with CCC. We train our model on the training portion and evaluate on the development portion. We report the results of our preliminary experiment in Figure 1. The results show that the inclusion of long-term time dependencies improves validation performance to a point. The observed decreasing performance gains in past 512 (20.48 seconds) frames may be irrelevant either due to the increased number of parameters or because contextual information after 512 frames. Covering contexts of a size of 512 frames still offers improvements compared to results obtained from covering smaller contexts."}, {"heading": "5. Methods", "text": "In this section, we describe the two architectures we propose to capture long-term time dependencies in continuous prediction tasks for emotions."}, {"heading": "5.1. Dilated Convolutions", "text": "In fact, it is possible to use convolutions with different dilatation factors to grow the receptive field of a generative model exponentially, to capture thousands of time steps and deliver high-quality speeches. [10] It was shown that ASR could benefit from dilated convolutions without disturbing the length of the input signals."}, {"heading": "5.2. Downsampling/Upsampling", "text": "eDi eeisrcnlhEe\u00fccnlhsrteee\u00fccnlhsrrrtee\u00fccsrlhsrtee\u00fccnlhsrteeeeVnlrteeeeeeoiuerrrrrteeteersrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "6. Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Experimental Setup", "text": "We build our models using Keras [22] with a theano-backend [23]. We train our models on the training portion of the dataset and use the development portion for early setting and hyperparameter selection (e.g. learning rate, number of layer sizes, filter width, l2 regularization, dilation factors, downsampling factors).We optimize CCC directly in all setups. We repeat each experiment five times to take into account the effect of initialization.The final test evaluation is done by the organizers of AVEC 2016 (i.e. we do not have access to test labels).Our test submissions are made by building the predictions generated from the five rounds on average. We report published results from the literature as baselines. Almost all previous work reports only on their final test results based on multimodal characteristics. We only show results that are based on audio modality in the results tables we have compared with an optimized STM, each comparing our performance with a BLM."}, {"heading": "6.2. Results", "text": "Tables 1 and 2 show the development and test results for arousal and / or valence. Each line shows the results for an assembly. We only include results from literature based on language modality and use \"-\" to show unreported results. Both proposed systems show improvements over the basic results of Valstar et al. [2]. Our advanced folding-based system offers improvements of 5.6% and 19.5% over base systems for arousal and valence, respectively. Our downsampling / upsampling system offers improvements of 5.1% and 33.9% over base systems for arousal and valence, respectively. We report the results we get from our BLSTM system to provide a reference point. Our BLSTM system performs well compared to base results. The proposed methods perform BLSTMs and are more efficient to train on long expressions. For example, we show a continuous network and a BLSTM network with learnable parameters."}, {"heading": "7. Conclusion", "text": "We investigated two architectures that offer different ways to capture long-term time dependencies in a particular sequence of acoustic characteristics. Advanced waves provide a way to incorporate long-term time information without disrupting the length of the input signal by using filters with different dilatation factors. Downsampling / upsampling networks incorporate long-term dependencies by applying a series of waves and max poolings to downplay the signal and get a global view of the characteristics. Subsequently, the downsampled signal is used to reconstruct an output with a length corresponding to the uncompressed input."}, {"heading": "8. Acknowledgement", "text": "This work was partially supported by IBM as part of the Sapphire Project. We would like to thank Dr. David Nahamoo and Dr. Lazaros Polymenakos, IBM Research, Yorktown Heights, for their support. * Unpublished test results, courtesy of the authors."}, {"heading": "9. References", "text": "[1] M. Lewis, J. M. Haviland-Jones, and L. F. Barrett, Handbook ofemotion Challenge on Emotions. Guilford Press, 2010. [2] M. Valstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne, M. Torres, S. Scherer, G. Stratou, R. Cowie, and M. Pantic, \"AVEC 2016:\" Emotion recognition from spontaneous, mood, and emotion recognition workshop and challenge, \"in Proceedings of the 6th International Workshop on Audio / Visual Emotion Challenge. [3] D. Le and E. Provost.\" Emotion recognition from spontaneous, and emotion recognition workshop with hidden markov models with deep belief networks, \"in workshop on automatic speech recognition and understanding (ASRU). IEEE, pp. 216-221. [4] P. Cardinal, N. Dehak, A. L. Koerich, J. Alam, and P. Boucher.\""}], "references": [{"title": "AVEC 2016: Depression, mood, and emotion recognition workshop and challenge", "author": ["M. Valstar", "J. Gratch", "B. Schuller", "F. Ringeval", "D. Lalanne", "M. Torres Torres", "S. Scherer", "G. Stratou", "R. Cowie", "M. Pantic"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 3\u201310.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Emotion recognition from spontaneous speech using hidden markov models with deep belief networks", "author": ["D. Le", "E.M. Provost"], "venue": "workshop on automatic speech recognition and understanding (ASRU). IEEE, pp. 216\u2013221.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 0}, {"title": "ETS system for AVEC 2015 challenge", "author": ["P. Cardinal", "N. Dehak", "A.L. Koerich", "J. Alam", "P. Boucher"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge. ACM, 2015, pp. 17\u201323.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. He", "D. Jiang", "L. Yang", "E. Pei", "P. Wu", "H. Sahli"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge. ACM, 2015, pp. 73\u201380.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-modal audio, video and physiological sensor learning for continuous emotion prediction", "author": ["K. Brady", "Y. Gwon", "P. Khorrami", "E. Godoy", "W. Campbell", "C. Dagli", "T.S. Huang"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 97\u2013104.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal emotion recognition for AVEC 2016 challenge", "author": ["F. Povolny", "P. Matejka", "M. Hradis", "A. Popkov\u00e1", "L. Otrusina", "P. Smrz", "I. Wood", "C. Robin", "L. Lamel"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 75\u201382.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing", "author": ["F. Eyben", "K.R. Scherer", "B.W. Schuller", "J. Sundberg", "E. Andr\u00e9", "C. Busso", "L.Y. Devillers", "J. Epps", "P. Laukka", "S.S. Narayanan"], "venue": "IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "B. Schuller", "S. Zafeiriou"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5200\u20135204.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Dense prediction on sequences with time-dilated convolutions for speech recognition", "author": ["T. Sercu", "V. Goel"], "venue": "arXiv preprint arXiv:1611.09288, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions", "author": ["F. Ringeval", "A. Sonderegger", "J. Sauer", "D. Lalanne"], "venue": "International Conference and Workshops on Automatic Face and Gesture Recognition (FG). IEEE, 2013, pp. 1\u20138.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "workshop on automatic speech recognition and understanding (ASRU). IEEE, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Using neutral speech models for emotional speech analysis.", "author": ["C. Busso", "S. Lee", "S.S. Narayanan"], "venue": "in Interspeech,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015, pp. 1520\u20131528.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2010, pp. 2528\u20132535.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "A guide to convolution arithmetic for deep learning", "author": ["V. Dumoulin", "F. Visin"], "venue": "arXiv preprint arXiv:1603.07285, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Single-image depth perception in the wild", "author": ["W. Chen", "Z. Fu", "D. Yang", "J. Deng"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2016, pp. 730\u2013738.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "A fully convolutional neural network for speech enhancement", "author": ["S.R. Park", "J. Lee"], "venue": "arXiv preprint arXiv:1609.07132, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Discretized continuous speech emotion recognition with multi-task deep recurrent neural network", "author": ["D. Le", "Z. Aldeneh", "E. Mower Provost"], "venue": "Interspeech, 2017 (to apear), 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "[2] showed that it was necessary to consider larger windows when making frame-level emotion predictions (four seconds for arousal and six seconds for valence).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] and Cardinal et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] found that increasing the number of contextual frames when training a deep neural network (DNN) for making frame-level emotion predictions is helpful but only to a certain point.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": ", [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "Successful attempts to solving the continuous emotion recognition problem relied on DNNs [4], BLSTMs [5], and more commonly, support vector regression (SVR) classifiers [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "Successful attempts to solving the continuous emotion recognition problem relied on DNNs [4], BLSTMs [5], and more commonly, support vector regression (SVR) classifiers [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Successful attempts to solving the continuous emotion recognition problem relied on DNNs [4], BLSTMs [5], and more commonly, support vector regression (SVR) classifiers [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "[6] extracted a set of audio features (Mel-frequency cepstral coefficients, shifted delta cepstral, prosody) and then learned higher-level representations of the features using sparse coding.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] used eGeMAPS [8] features along with a set of higher-level bottleneck features extracted from a DNN trained for automatic speech recognition (ASR) to train linear regressors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] used eGeMAPS [8] features along with a set of higher-level bottleneck features extracted from a DNN trained for automatic speech recognition (ASR) to train linear regressors.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "In contrast, in this work we show that considering temporal dependencies that are longer than those presented in [6, 7] is critical to improve continuous emotion recognition performance.", "startOffset": 113, "endOffset": 119}, {"referenceID": 5, "context": "In contrast, in this work we show that considering temporal dependencies that are longer than those presented in [6, 7] is critical to improve continuous emotion recognition performance.", "startOffset": 113, "endOffset": 119}, {"referenceID": 3, "context": "[5] extracted a comprehensive set of 4, 684 features, which included energy, spectral, and voicing-related features, and used them to train BLSTMs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] trained a convolutional recurrent network for continuous emotion recognition using the time domain signal directly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Further, since our models work on full-length utterances, we show that it is not necessary to apply any postprocessing steps as described in [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 8, "context": "[10] proposed viewing ASR problems as dense prediction tasks, where the goal is to assign a label to every frame in a given sequence, and showed that this view provides a set of tools (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "We focus on the RECOLA database [11] following the AVEC 2016 guidelines [2].", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "We focus on the RECOLA database [11] following the AVEC 2016 guidelines [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 10, "context": "We use the Kaldi toolkit [12] to extract 40dimensional log MFB features, using a window length of 25ms with a hop size of 10ms.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "Previous work showed that MFB features are better than conventional MFCCs for predicting emotions [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "[2] only covered six seconds worth of features and Povolny et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] considered a maximum of eight seconds worth of features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Networks that use dilated convolutions have shown success in a number of tasks, including image segmentation [14], speech synthesis [15] and ASR [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Networks that use dilated convolutions have shown success in a number of tasks, including image segmentation [14], speech synthesis [15] and ASR [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "Networks that use dilated convolutions have shown success in a number of tasks, including image segmentation [14], speech synthesis [15] and ASR [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "[15] recently showed that it is possible to use convolutions with various dilation factors to allow the receptive field of a generative model to grow exponentially in order to cover thousands of time steps and synthesize highquality speech.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] showed that ASR could benefit from dilated convolutions since they allow larger regions to be covered without disrupting the length of the input signals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The upsampling function can be implemented in a number of ways [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "In this work we use the transposed convolution [17, 18] operation to perform upsampling.", "startOffset": 47, "endOffset": 55}, {"referenceID": 16, "context": "In this work we use the transposed convolution [17, 18] operation to perform upsampling.", "startOffset": 47, "endOffset": 55}, {"referenceID": 14, "context": ", [16, 19, 20]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 17, "context": ", [16, 19, 20]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 18, "context": ", [16, 19, 20]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 14, "context": "[16] showed that transposed convolution operations can be effectively applied to image segmentation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In addition to vision applications, downsampling/upsampling architectures have been successfully applied to speech enhancement problems [21], where the goal is to learn a mapping between noisy speech spectra and their clean counterparts.", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "[21] demonstrated that downsampling/upsampling convolutional networks can be 12\u00d7 smaller (in terms of the number of learnable parameters) than their recurrent counterparts and yet yield better performance on speech enhancement tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We build our models using Keras [22] with a Theano backend [23].", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "We build our models using Keras [22] with a Theano backend [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "We also compare our performance to that of an optimized BLSTM regression model, described in [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[2] \u2013 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7]\u2217 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "682 BLSTM [24] .", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "[2] \u2013 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7]\u2217 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "349 BLSTM [24] .", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "The goal of continuous emotion recognition is to assign an emotion value to every frame in a sequence of acoustic features. We show that incorporating long-term temporal dependencies is critical for continuous emotion recognition tasks. To this end, we first investigate architectures that use dilated convolutions. We show that even though such architectures outperform previously reported systems, the output signals produced from such architectures undergo erratic changes between consecutive time steps. This is inconsistent with the slow moving ground-truth emotion labels that are obtained from human annotators. To deal with this problem, we model a downsampled version of the input signal and then generate the output signal through upsampling. Not only does the resulting downsampling/upsampling network achieve good performance, it also generates smooth output trajectories. Our method yields the best known audioonly performance on the RECOLA dataset.", "creator": "LaTeX with hyperref package"}}}