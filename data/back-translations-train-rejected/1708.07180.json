{"id": "1708.07180", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "Bootstrapping the Out-of-sample Predictions for Efficient and Accurate Cross-Validation", "abstract": "Cross-Validation (CV), and out-of-sample performance-estimation protocols in general, are often employed both for (a) selecting the optimal combination of algorithms and values of hyper-parameters (called a configuration) for producing the final predictive model, and (b) estimating the predictive performance of the final model. However, the cross-validated performance of the best configuration is optimistically biased. We present an efficient bootstrap method that corrects for the bias, called Bootstrap Bias Corrected CV (BBC-CV). BBC-CV's main idea is to bootstrap the whole process of selecting the best-performing configuration on the out-of-sample predictions of each configuration, without additional training of models. In comparison to the alternatives, namely the nested cross-validation and a method by Tibshirani and Tibshirani, BBC-CV is computationally more efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error). Subsequently, we employ again the idea of bootstrapping the out-of-sample predictions to speed up the CV process. Specifically, using a bootstrap-based hypothesis test we stop training of models on new folds of statistically-significantly inferior configurations. We name the method Bootstrap Corrected with Early Dropping CV (BCED-CV) that is both efficient and provides accurate performance estimates.", "histories": [["v1", "Wed, 23 Aug 2017 20:30:07 GMT  (133kb)", "http://arxiv.org/abs/1708.07180v1", null], ["v2", "Fri, 25 Aug 2017 14:02:02 GMT  (133kb)", "http://arxiv.org/abs/1708.07180v2", "Added acknowledgments"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ioannis tsamardinos", "elissavet greasidou", "michalis tsagris", "giorgos borboudakis"], "accepted": false, "id": "1708.07180"}, "pdf": {"name": "1708.07180.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tsamard.it@gmail.com", "greasidouelissavet@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.07 180v 1 [cs.L G] 23 Aug 2Keywords performance estimation \u00b7 bias correction \u00b7 cross-validation \u00b7 hyperparameter optimization \u2022 Equal contribution. Ioannis Tsamardinos Computer Science Department, University of Crete and Gnosis Data Analysis PC E-mail: tsamard.it @ gmail.com Elissavet Greasidou Computer Science Department, University of Crete and Gnosis Data Analysis PC E-mail: greasidouelissavet @ gmail.com Michalis Tsagris Computer Science Department, University of CreteGiorgos Borboudakis Computer Science Department, University of Crete and Gnosis Data Analysis PC"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Preliminaries of Out-of-Sample Estimation", "text": "In this section, we present the basics of predicting values from the series with the indexes. (http: / / www.rpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnnpnpnpnpnnpnpnpnpnpnpnpnpnpnnpnnpnpnnpnpnpnpnnnpnpnpnpnnpnpnpnpnpnpnpnpnpnpnnpnpnnnpnpnpnpnpnnpnpnpnpnnnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnnpnpnpnpnpnpnpnpnpnpnnpnpnpnpnpnpnpnpnpnpnpnpnnpnpnpnpnnnpnpnpnpnpnpnnpnpnpnnpnpnpnpnpnpnnnnpnpnpnpnpnpnpnpnpnpnnpnpnpnpnpnpnpnpnpnnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpnpn"}, {"heading": "3 Related Work", "text": "Bernau et al. [2] introduced two variants of a bias correction method as a smooth analytical alternative to NCV, WMC, and WMCS. The method is based on repeated sampling of the original dataset and the formation of multiple models, and then calculates the error estimate as a weighted average of the error rates of each configuration across all subsamples. The two variants differ in the way the weights are calculated. Compared to NCV, the authors claim that WMC / WMCS is competitive and more stable for the same number of trained models as the CVT. However, subsequent independent work reports [7] problems with the method, namely that it provides fluctuating estimates and in some cases overcorrects bias. It is also complicated to understand and implement the data. Ding et al. [7] proposed a resampling-based Inverse Power Law (IPL) method for the bias (model selection function) of the WS-MC and the same WS-rights is the same."}, {"heading": "4 The Bootstrap Bias Corrected Cross-Validation (BBC-CV)", "text": "It is not as if the idea of bootstrapping datasets is an alternative to CV, but the insufficient distribution of the insufficient distribution rates in the insufficient distribution of GDP. Countless variants have appeared for different statistical tasks and problems (see [6]). In the machine learning environment, the idea of bootstrapping datasets has been proposed as an alternative to CV. Specifically, to produce a performance estimate for a method of multiple training sets (see [6])."}, {"heading": "5 Bootstrap Corrected with Early Dropping Cross-Validation (BCED-CV)", "text": "In this section, we will introduce a second use of the idea, which is a problem that affects the pooled performance of each configuration (the predictions of each configuration are the same), and in particular they are used as part of a statistical hypothesis that determines whether the performance of a configuration is essentially significantly inferior to the performance of the current best configuration. If this is the case, the dominated configuration can in fact be dropped as the optimal configuration at the end of the CVT, in the sense that no additional models are trained for subsequent folds under that configuration. If a strict significance threshold is used for the test, then the failed configurations have a low probability of actually ending as the optimal configuration at the end of the CVT, thus not affecting the prediction performance of the final model."}, {"heading": "6 Empirical Evaluation", "text": "We empirically evaluate the efficiency and examine the characteristics of BBC-CV and BCED-CV = 98,000 candidates drawn (both controlled settings and real problems. In particular, we focus on the bias of performance estimates of the protocols and on computing time. We compare the results with those of three standard approaches: CVT, TT and NCV. We also examine the tuning (configuration selection) characteristics of BBC-CV, BCED-CV and BBC-CV with repetitions as well as the confidence intervals that these methods construct. 6.1 Simulation studies have been conducted to validate the characteristics of BBC-CV and BCED-CV-CV-CV and to assess their performance. We focus on the binary classification task and classification accuracy as a measure of performance. We examine several settings for varying sample sizes N-40.40, 60, 80, 100, 1000, 1000), number of candidate configurations C-100, 50,500, 200,500, 500)."}, {"heading": "6.1.1 Bias Estimation", "text": "In fact, the situation is that most people are able to survive if they do not see themselves as being able to survive and survive. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "6.2.1 Bias estimation", "text": "The bias of the estimate is calculated as in the simulation studies, i.e. B-ias = P-ias = P-ias, where P-ias and P denote the estimated and actual performance of the selected configuration in relation to the sample size.In Figure 2, we examine the average bias of the CVT, TTT, NCV, BBC-CV and BCED-CV estimates of performance in all datasets. We note that the results are consistent with those of the simulation studies. In particular, the CVT for sample size N \u2264 100 is optimistically distorted and its bias tends to zero with increasing N increase. The bias of the TT overestimates the performance for N = 20, its bias varies with the dataset for N = 40 and corrects the bias of the CVT for N \u2265 60. The TT shows the worst results of all protocols, except CVT.Both NCV and BBC Cas indicate a low Cas = 40 points for ASxi = 00V, although T = 000T for T = 000T for the ASxi = 00V = T."}, {"heading": "6.2.2 Relative Performance and Speed Up of BCED-CV", "text": "We have shown that for large sample sizes (N = 500), BCED-CV provides accurate estimates of the performance of the model it returns, comparable to those of BBC-CV and NCV. How well does this model work? In this section, we evaluate the effectiveness of BCED-CV in terms of its tuning properties (configuration selection) and its efficiency in reducing the computing costs of CVT. Figure 3 shows the relative average actual performance of the models returned by the BCED-CVT protocols, plotted against the sample size. We recall here that for each of the 20 sub-datasets of sample size N {20, 40, 60, 80, 500} sampled from Dpool, the true performance of the returned model is estimated on theDholdout set. We note that forN-100 greatly varies the power loss with dataset and is quite significant at worst (dexaset, 40 = N)."}, {"heading": "6.2.3 Multiple Repeats", "text": "We repeated the previous experiments by performing BBC-CV with 10 repetitions (hereinafter referred to as BBC-CV10). First, we compare the actual performance of the models returned by BBC-CV and BBC-CV10, as well as the bias of the estimate. Ideally, using multiple repetitions should result in a more powerful model, since the variance of the performance estimate (used by CVT for tuning) is reduced due to a specific selection of the distribution of data when considering multiple splits. This involves increased computational effort, similar to that of the NCV protocol in the case of 10 repetitions. To determine which of the approaches is preferable, we also compare the performance of the final models used by BBC-CV10 and NCV.Figure 5 (left) shows the relative average actual performance of BBC-CV10 with BBC-CVV with increasing sample size N."}, {"heading": "6.2.4 Confidence Intervals", "text": "The bootstrap-based estimation of performance allows an easy calculation of the confidence intervals (CIs) as described in Section 4.1. We investigated the accuracy of the CIs produced by the proposed BBC-CV, BCED-CV and BBC-CV10 protocols, calculating the coverage of the {50%, 55%,.., 95%, 99%} CIs estimated by the protocols, defined as the ratio of the calculated CIs containing the corresponding true performance of the models produced. For a given sample size, the coverage of a CI was calculated over all 20 sub-data sets and 9 data sets. To further investigate the effect of multiple repetitions on CIs, we calculated their average width (over all 20 sub-data sets), which we calculated for each data set and different number of repetitions (1 to 10)."}, {"heading": "7 Conclusions", "text": "The combination of out-of-sample predictions during cross-validation of multiple configurations (i.e., combinations of algorithms and their hyperparameter values leading to a model) and the use of bootstrapping techniques on them solves, in a simple and general way, three long-standing, important data analysis tasks: (a) the removal of optimism in performance estimation of the selected configuration, (b) the estimation of confidence intervals of performance and (c) the abandonment of further configurations. While other methods have also been proposed, they lack the simplicity and universality of applicability in all types of performance metrics. The ideas above are implemented in the BBC-CV method by addressing points (a) and BCED-CV, which include (c) that simulation studies and experiments are based on real data sets, demonstrating empirically that BBC-CV and BCED-CV exceed the alternatives (cross-validation and cross-validation method)."}], "references": [{"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE transactions on automatic control 19(6), 716\u2013723", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1974}, {"title": "Correcting the optimal resampling-based error rate by estimating the error rate of wrapper algorithms", "author": ["C. Bernau", "T. Augustin", "A.L. Boulesteix"], "venue": "Biometrics 69(3), 693\u2013702", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning 45(1), 5\u201332", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "The comparison of percentages in matched samples", "author": ["W.G. Cochran"], "venue": "Biometrika 37(3/4), 256\u2013266", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1950}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning 20(3), 273\u2013297", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Bootstrap methods and their application", "author": ["A.C. Davison", "D.V. Hinkley"], "venue": "Cambridge university press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Bias correction for selecting the minimal-error classifier from many machine learning models", "author": ["Y. Ding", "S. Tang", "S.G. Liao", "J. Jia", "S. Oesterreich", "Y. Lin", "G.C. Tseng"], "venue": "Bioinformatics 30(22), 3152\u20133158", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": "CRC press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "An introduction to roc analysis", "author": ["T. Fawcett"], "venue": "Pattern recognition letters 27(8), 861\u2013874", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software 33(1), 1\u201322", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance", "author": ["M. Friedman"], "venue": "Journal of the american statistical association 32(200), 675\u2013701", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1937}, {"title": "Bayesian optimization for sensor set selection", "author": ["R. Garnett", "M.A. Osborne", "S.J. Roberts"], "venue": "Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks, pp. 209\u2013219", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Performance prediction challenge", "author": ["I. Guyon", "Alamdari", "A.R.S.A.", "G. Dror", "J.M. Buhmann"], "venue": "The 2006 IEEE International Joint Conference on Neural Network Proceedings, pp. 1649\u20131656. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Design of the 2015 chalearn automl challenge", "author": ["I. Guyon", "K. Bennett", "G. Cawley", "H.J. Escalante", "S. Escalera", "T.K. Ho", "N. Maci\u00e0", "B. Ray", "M. Saeed", "A. Statnikov", "E. Viegas"], "venue": "Proc. of IJCNN", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Result analysis of the nips 2003 feature selection challenge", "author": ["I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror"], "venue": "Advances in neural information processing systems, pp. 545\u2013552", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Evaluating the yield of medical tests", "author": ["F.E. Harrell", "R.M. Califf", "D.B. Pryor", "K.L. Lee", "R.A. Rosati"], "venue": "Journal of the American medical association 247(18), 2543\u20132546", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1982}, {"title": "Multiple comparisons in induction algorithms", "author": ["D.D. Jensen", "P.R. Cohen"], "venue": "Machine Learning 38(3), 309\u2013338", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "Sur les fonctions convexes et les in\u00e9galit\u00e9s entre les valeurs moyennes", "author": ["Jensen", "J.L.W.V."], "venue": "Acta mathematica 30(1), 175\u2013193", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1906}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R Kohavi"], "venue": "Ijcai, vol. 14, pp. 1137\u20131145", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Fast cross-validation via sequential testing", "author": ["T. Krueger", "D. Panknin", "M. Braun"], "venue": "Journal of Machine Learning Research 16, 1103\u20131155", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature selection with the R package MXM: Discovering statistically-equivalent feature subsets", "author": ["V. Lagani", "G. Athineou", "A. Farcomeni", "M. Tsagris", "I. Tsamardinos"], "venue": "Journal of Statistical Software To appear", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Hoeffding races: Accelerating model selection search for classification and function approximation", "author": ["O. Maron", "A.W. Moore"], "venue": "Advances in neural information processing systems pp. 59\u201359", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Computational algorithms for double bootstrap confidence intervals", "author": ["J.C. Nankervis"], "venue": "Computational statistics & data analysis 49(2), 461\u2013475", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "On comparing classifiers: Pitfalls to avoid and a recommended approach", "author": ["S.L. Salzberg"], "venue": "Data mining and knowledge discovery 1(3), 317\u2013328", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Estimating the dimension of a model", "author": ["G Schwarz"], "venue": "The annals of statistics 6(2), 461\u2013464", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1978}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in neural information processing systems, pp. 2951\u20132959", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis", "author": ["A. Statnikov", "C.F. Aliferis", "I. Tsamardinos", "D. Hardin", "S. Levy"], "venue": "Bioinformatics 21(5), 631\u2013643", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) pp. 267\u2013288", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1996}, {"title": "A bias correction for the minimum error rate in crossvalidation", "author": ["R.J. Tibshirani", "R. Tibshirani"], "venue": "The Annals of Applied Statistics pp. 822\u2013829", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance-estimation properties of crossvalidation-based protocols with simultaneous hyper-parameter optimization", "author": ["I. Tsamardinos", "A. Rakhshani", "V. Lagani"], "venue": "Artificial Intelligence: Methods and Applications, pp. 1\u201314. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Bias in error estimation when using cross-validation for model selection", "author": ["S. Varma", "R. Simon"], "venue": "BMC bioinformatics 7(1), 91", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank", "M.A. Hall", "C.J. Pal"], "venue": "Morgan Kaufmann", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Lazy paired hyper-parameter tuning", "author": ["A.X. Zheng", "M. Bilenko"], "venue": "IJCAI", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 30, "context": "In comparison to the alternatives, namely the nested cross-validation [31] and a method by Tibshirani and Tibshirani [29], BBC-CV is computationallymore efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error).", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "In comparison to the alternatives, namely the nested cross-validation [31] and a method by Tibshirani and Tibshirani [29], BBC-CV is computationallymore efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error).", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "There exist several strategies guiding the order in which the different configurations are tried, from sophisticated ones such as Sequential Bayesian Optimization [26,12] to simple grid search in the space of hyper-parameter values.", "startOffset": 163, "endOffset": 170}, {"referenceID": 11, "context": "There exist several strategies guiding the order in which the different configurations are tried, from sophisticated ones such as Sequential Bayesian Optimization [26,12] to simple grid search in the space of hyper-parameter values.", "startOffset": 163, "endOffset": 170}, {"referenceID": 0, "context": "We note that while there exist approaches that do not employ out-ofsample estimation, such as using the Akaike Information Criterion (AIC) [1] of the models, the Bayesian Information Criterion (BIC) [25], and others, in this paper we focus only on out-of-sample estimation protocols.", "startOffset": 139, "endOffset": 142}, {"referenceID": 24, "context": "We note that while there exist approaches that do not employ out-ofsample estimation, such as using the Akaike Information Criterion (AIC) [1] of the models, the Bayesian Information Criterion (BIC) [25], and others, in this paper we focus only on out-of-sample estimation protocols.", "startOffset": 199, "endOffset": 203}, {"referenceID": 19, "context": "tions of learning algorithms that do not abide to this assumption (K-NN algorithm for example, see [20] for a discussion) but it is largely accepted and true for most predictive modeling algorithms.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "The problem was named the multiple comparisons in induction problems and was first reported in the machine learning literature by Jensen in [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": ",mn}) \u2265 0 by Jensen\u2019s inequality [18].", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "For metrics such as classification accuracy and Area Under the Receiver\u2019s Operating Characteristic Curve (AUC) [9], where higher is better, the min is substituted with max and the inequality is reversed.", "startOffset": 111, "endOffset": 114}, {"referenceID": 29, "context": "The bias of Cross-Validation when multiple configurations are tried has also been explored empirically in [30] on real datasets.", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "as tuning and estimation subsets and performance is averaged on all subsets leads to the Nested Cross Validation (NCV) protocol [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "BBC-CV is empirically compared against NCV, the standard for avoiding bias, and a method by Tibshirani and Tibshirani [29] (TT from hereon) which addresses the large computational cost of NCV.", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "Some metrics of performance such as the AUC or the Concordance Index for survival analysis problems [16] cannot be expressed using a loss function defined on single pairs \u3008y, \u0177\u3009.", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "However, leave-one-out CV can collapse in the sense that it can provide extremely misleading estimates in degenerate situations (see [33], p.", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "151, and [19] for an extreme failure of leave-one-out CV and of the 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "The latter restriction leads to what is called stratified CV and there is evidence that it leads to improved performance estimations [30].", "startOffset": 133, "endOffset": 137}, {"referenceID": 26, "context": "We could not trace who introduced or coined up first the name Nested Cross-Validation but the authors and colleagues have independently discovered it and using it since 2005 [27]; around the same time Varma and Simon in [31], report a bias in error estimation when using K-Fold Cross-Validation, and suggest the use of the Nested", "startOffset": 174, "endOffset": 178}, {"referenceID": 30, "context": "We could not trace who introduced or coined up first the name Nested Cross-Validation but the authors and colleagues have independently discovered it and using it since 2005 [27]; around the same time Varma and Simon in [31], report a bias in error estimation when using K-Fold Cross-Validation, and suggest the use of the Nested", "startOffset": 220, "endOffset": 224}, {"referenceID": 26, "context": "A similar method in a bioinformatics analysis was used in 2005 [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "One early comment hinting of the method is in [24], while Witten and Frank (see [32], page 286) briefly discuss the need of treating any parameter tuning step as part of the training process when assessing performance.", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "One early comment hinting of the method is in [24], while Witten and Frank (see [32], page 286) briefly discuss the need of treating any parameter tuning step as part of the training process when assessing performance.", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "To reduce the computational overhead of NCV, Tibshirani and Tibshirani [29] introduced a new method for estimating and correcting for the bias of CVT without", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "[2] introduced two variants of", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "However, subsequent independent work by [7] report problems with the method, and specifically that it provides fluctuating estimates and it may over-correct the bias in some cases.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "in [7] proposed a resampling-based inverse power law (IPL) method for bias correction and compared its performance to those of TT, NCV, and WMC/WMCS on both simulated and real datasets.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The bootstrap [8] has been developed and applied extensively to estimate in a non-parametric way the (unknown) distribution of a statistic bo computed for a population (dataset).", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Numerous variants have appeared for different statistical tasks and problems (see [6]).", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "The protocol has been compared to the CV in [19] concluding that the CV is preferable.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": ", bB and considering an interval [lb, ub] that contains p percentage of the population [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "For more theoretical details on the bootstrap confidence intervals and different methods for constructing them, as well as a comparison of them, see [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 29, "context": "This is confirmed in [30] empirically on several", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Perhaps, a double bootstrap procedure would be more appropriate in this case [23] but any such improvements would have to also minimize the computational overhead to be worthwhile in practice.", "startOffset": 77, "endOffset": 81}, {"referenceID": 21, "context": "The idea of accelerating the learning process by specifically eliminating underperforming configurations from a finite set, early within the cross-validation procedure, was introduced as early as 1994 by Maron and Moore with Hoeffding Races [22].", "startOffset": 241, "endOffset": 245}, {"referenceID": 33, "context": "Following a similar approach, Zheng and Bilenko in 2013 [34] applied the concept of early elimination of suboptimal configurations to K-fold CV.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "[20] in 2015 introduced the so-called Fast Cross-Validation via Sequential Testing (CVST) which uses nonparametric testing together with sequential analysis in order to choose the best performing configuration on the basis of linearly increasing subsets of data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "At each step, the Friedman [11] or", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "the Cochran\u2019s Q test [4] (for regression and classification tasks respectively) are employed in order to detect statistically significant differences between configurations\u2019 performances.", "startOffset": 21, "endOffset": 24}, {"referenceID": 33, "context": "In comparison to the statistical tests used in [34] and [20], the bootstrap is a general test, applicable to any type of learning task and measure of performance, and is suitable even for relatively small sample sizes.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "In comparison to the statistical tests used in [34] and [20], the bootstrap is a general test, applicable to any type of learning task and measure of performance, and is suitable even for relatively small sample sizes.", "startOffset": 56, "endOffset": 60}, {"referenceID": 33, "context": "Finally, BCED-CV requires that only the value of the significance threshold \u03b1 is pre-specified while the methods in [34] and [20] have a number of hyper-parameters to be specified in advance.", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "Finally, BCED-CV requires that only the value of the significance threshold \u03b1 is pre-specified while the methods in [34] and [20] have a number of hyper-parameters to be specified in advance.", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "christine 5418 1636 1 1625 3793 [14] jasmine 2984 144 1 895 2089 [14] philippine 5832 308 1 1749 4082 [14] madeline 3140 259 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "christine 5418 1636 1 1625 3793 [14] jasmine 2984 144 1 895 2089 [14] philippine 5832 308 1 1749 4082 [14] madeline 3140 259 1.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "christine 5418 1636 1 1625 3793 [14] jasmine 2984 144 1 895 2089 [14] philippine 5832 308 1 1749 4082 [14] madeline 3140 259 1.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "01 942 2198 [14] sylvine 5124 20 1 1537 3587 [14] gisette 7000 5000 1 2100 4900 [15]", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "01 942 2198 [14] sylvine 5124 20 1 1537 3587 [14] gisette 7000 5000 1 2100 4900 [15]", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "01 942 2198 [14] sylvine 5124 20 1 1537 3587 [14] gisette 7000 5000 1 2100 4900 [15]", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "madelon 2600 500 1 781 1819 [15] dexter 600 20000 1 180 420 [15] gina 3468 970 1.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "madelon 2600 500 1 781 1819 [15] dexter 600 20000 1 180 420 [15] gina 3468 970 1.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "03 1041 2427 [13]", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "datasets utilized for the experiments come from popular data science challenges (NIPS 2003 [15], WCCI 2006 [13], ChaLearn AutoML [14]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "datasets utilized for the experiments come from popular data science challenges (NIPS 2003 [15], WCCI 2006 [13], ChaLearn AutoML [14]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "datasets utilized for the experiments come from popular data science challenges (NIPS 2003 [15], WCCI 2006 [13], ChaLearn AutoML [14]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 129, "endOffset": 133}, {"referenceID": 29, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "For feature selection we used the SES algorithm [21] with alpha \u2208 {0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "The learning algorithms utilized were Random Forests [3], SVMs [5], and LASSO [28].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "The learning algorithms utilized were Random Forests [3], SVMs [5], and LASSO [28].", "startOffset": 63, "endOffset": 66}, {"referenceID": 27, "context": "The learning algorithms utilized were Random Forests [3], SVMs [5], and LASSO [28].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "using the glmnet library [10].", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Cross-Validation (CV), and out-of-sample performance-estimation protocols in general, are often employed both for (a) selecting the optimal combination of algorithms and values of hyper-parameters (called a configuration) for producing the final predictive model, and (b) estimating the predictive performance of the final model. However, the cross-validated performance of the best configuration is optimistically biased. We present an efficient bootstrap method that corrects for the bias, called Bootstrap Bias Corrected CV (BBC-CV). BBC-CV\u2019s main idea is to bootstrap the whole process of selecting the best-performing configuration on the out-of-sample predictions of each configuration, without additional training of models. In comparison to the alternatives, namely the nested cross-validation [31] and a method by Tibshirani and Tibshirani [29], BBC-CV is computationallymore efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error). Subsequently, we employ again the idea of bootstrapping the out-of-sample predictions to speed up the CV process. Specifically, using a bootstrap-based hypothesis test we stop training of models on new folds of statistically-significantly inferior configurations. We name the method Bootstrap Corrected with Early Dropping CV (BCED-CV) that is both efficient and provides accurate performance estimates.", "creator": "LaTeX with hyperref package"}}}