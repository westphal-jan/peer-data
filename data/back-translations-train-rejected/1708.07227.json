{"id": "1708.07227", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Proportionate gradient updates with PercentDelta", "abstract": "Deep Neural Networks are generally trained using iterative gradient updates. Magnitudes of gradients are affected by many factors, including choice of activation functions and initialization. More importantly, gradient magnitudes can greatly differ across layers, with some layers receiving much smaller gradients than others. causing some layers to train slower than others and therefore slowing down the overall convergence. We analytically explain this disproportionality. Then we propose to explicitly train all layers at the same speed, by scaling the gradient w.r.t. every trainable tensor to be proportional to its current value. In particular, at every batch, we want to update all trainable tensors, such that the relative change of the L1-norm of the tensors is the same, across all layers of the network, throughout training time. Experiments on MNIST show that our method appropriately scales gradients, such that the relative change in trainable tensors is approximately equal across layers. In addition, measuring the test accuracy with training time, shows that our method trains faster than other methods, giving higher test accuracy given same budget of training steps.", "histories": [["v1", "Thu, 24 Aug 2017 00:20:16 GMT  (277kb,D)", "http://arxiv.org/abs/1708.07227v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sami abu-el-haija"], "accepted": false, "id": "1708.07227"}, "pdf": {"name": "1708.07227.pdf", "metadata": {"source": "CRF", "title": "Proportionate gradient updates with PercentDelta", "authors": ["Sami Abu-El-Haija"], "emails": ["haija@google.com"], "sections": [{"heading": "1 Introduction", "text": "The weights of the Deep Neural Networks (DNNs) are generally randomly initialized J-J-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W W W W W-W-W-W-W-W-W W W-W-W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W"}, {"heading": "2 Disproprtionate Training", "text": "We use a toy example to illustrate disproportionate training across layers. Suppose a 4-layer network with transferable weight matrices (W0, W1, W2, W3) and output vector x4. The gradient of the objective weight matrix W3 is greater if the output vector x4 can be calculated directly from the data, e.g. by means of cross-entropy losses. We write the gradient of the objective J w.r.t. The last layer of the weight matrix W3 is then as follows: \"We also write the gradient W3.\""}, {"heading": "3 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Adaptive Gradient", "text": "Duchi et al. (2011) proposed AdaGrad. A training algorithm that maintains a cumulative sum of square gradients (i.e. the second moment of the gradients): S (t) j = S (t \u2212 1) j + (\u2202 J \u2202 Wj \u0445 J \u2202 Wj), (3) Then it divides the gradient (elementally) by the square root of the sum: W (t + 1) j - 1 / 2, with the (.) - 1 / 2 power operator applied elementally. Essentially, this means that when a layer receives large gradients, they normalize to smaller gradients by dividing kingexponity. However, one weakness in Adaay Grad is that the (.) - 1 / 2 power operator is applied elementally."}, {"heading": "3.2 LARS", "text": "They et al. (2017) propose to normalize the gradients of each layer in order to normalize the ratio of the L2 standards of the parameter and the gradient. Namely, they propose the gradient update rule: \"LARSWj = \u03b7\" (t), \"W (t),\" W (t), \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"\" W, \"\" \"W,\" \"\" W, \"\" \"W,\" \"\" W, \"\" W \"and\" W. \"This constellation is very similar to ours, with two differences: firstly, our standards operator is L1 and not L2. Secondly, our standards operator is used outside the department (i.e. our department is elementary). Our proposed algorithm, PercentDelta, was used to train our work (Abu-El-Haija et al., 2017) before we were aware of the work of (You et al., 2017)."}, {"heading": "4 PercentDelta", "text": "For each weight matrix Wj and similar bias vectors, we propose the gradient update rule: \"PercentDeltaWj = \u03b7\" (t) size (Wj). \u2212 Z + is the number of tributes1 in W, and the scalars: size (Wj). \u2212 Z + is the number of tributes1 in W, and the scalars: size (Wj). \u2212 Z + is proportional to its current parameter value W (t)."}, {"heading": "5 Experiments", "text": "We conduct experiments with MNIST. We use the same model for all experiments and fix the stack size at 500. Our model code is copied from TensorFlow tutorial3 and contains 4 trainable layers: 2D Convolution, Max-pooling, 2D Convolution, Max-pooling, Fully-connected, Fully-connected. Convolutionary layers contain trainable tensors with the dimensions: (5, 5, 1, 32), (5, 5, 32, 64) and bias vectors. The fully connected layers contain traable tensors with the dimensions: (3136, 1024), (1024, 10) and bias vectors. The model is trained with Softmax loss, uses ReLu for hidden activations and uses BatchNorm.1size (W) is not the product of W's dimensions: (3136, 1024, 10), and bias vectors."}, {"heading": "5.1 MNIST Network Gradient Magnitudes", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2 MNIST Test Accuracy Curves", "text": "We want to measure how fast PercentDelta MNIST can train. We compare the training speed with other algorithms, including the parameter-dependent adaptive learning speed algorithms AdaGrad (Duchi et al., 2011) and Adam (Ba and Kingma, 2015) and a newer algorithm with a similar mind, LARS (You et al., 2017), which also normalizes the gradient w.r.t., a weight sensor, based on the current value of the weight sensor."}, {"heading": "6 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Situations where PercentDelta is useful", "text": "While PercentDelta outperforms other training algorithms on 4-layer MNIST, the space of the model datasets is enormous, we leave it up to future work to try out PercentDelta under different models and datasets. Nevertheless, we speculate that PercentDelta (and similar, LARS, You et al. (2017) would be very useful in the following scenarios: 1. Learning Embeddings. Consider the common structure of embedding words (or graphical embedding, Abu-El-Haija et al., 2017) into a common neural network and jointly learn the embedding and neural networks for an upstream target. In this setup, a certain embedding vector is affected only by a fraction of training examples, while the common network parameters are affected by all training examples. The sum of the history datasets w.r.t. the common network parameters can be disproportionately larger than the embedding of gradients."}, {"heading": "6.2 Hyperparamters and Decay Function", "text": "It seems that percentDelta has many knobs that we can adjust to. However, we can set \u03b7 to a certain value and only change \u03b3 (t) because their product determines the effective rate of change across all traceable tensors. We can set \u03b3 (t) to constant decay: \u03b3 (t) = 1 \u2212 t \u00b7 m, (9) where 0 < m < < < 1 determines the decay tendency. Furthermore, we can ensure that \u03b3 (t) > 0 to continue the workout indefinitely by changing Equation 9 to: \u03b3 (t) = max (\u03b2, 1 \u2212 t \u00b7 m), (10) where \u03b2 can be set to a small positive value such as 0.01. In this case, if we set \u03b7 = 0.03, we will effectively change each traceable tensor by 3% for each workout first and then gradually change the glow of this change rate to 0.03% after 1 m increments. More importantly, we believe that each function will be applied to traceable and that it will be applied to traceable by 3%."}, {"heading": "7 Conclusion", "text": "We propose an algorithm that trains the layers of a neural network at the same speed. Our algorithm, PercentDelta, is a simple modification compared to the standard gradient descent. It divides the gradient by a traceable tensor over the mean of | | Gradient / Tensor | 1. The division over the mean L1 standard is scalar and only changes the size of the gradient, but not its direction. Effectively, this updates the L1 standard for traceable layers, all at the same speed. We recommend a linear waste schedule. Our modified gradients can be traversed through a standard impulse accumulse accumulator (Sutskever et al., 2013). Overall, we experimentally show that our algorithm envelops all training algorithms and achieves higher testing accuracy with fewer steps."}], "references": [{"title": "Learning edge representations via lowrank asymmetric projections", "author": ["S. Abu-El-Haija", "B. Perozzi", "R. Al-Rfou"], "venue": "ACM International Conference on Information and Knowledge Management (CIKM).", "citeRegEx": "Abu.El.Haija et al\\.,? 2017", "shortCiteRegEx": "Abu.El.Haija et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["J. Ba", "D. Kingma"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Ba and Kingma,? 2015", "shortCiteRegEx": "Ba and Kingma", "year": 2015}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arxiv.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910).", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Journal of Machine Learning Research (JMLR).", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R.M. Bell", "C. Volinsky"], "venue": "IEEE Computer.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Detecting events and key actors in multi-person videos", "author": ["V. Ramanathan", "J. Huang", "S. Abu-El-Haija", "A. Gorban", "K. Murphy", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Ramanathan et al\\.,? 2016", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2016}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "Neural Information Processing Systems.", "citeRegEx": "Salimans and Kingma,? 2016", "shortCiteRegEx": "Salimans and Kingma", "year": 2016}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "International Conference on Machine Learning.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Scaling sgd batch size to 32k for imagenet training", "author": ["Y. You", "I. Gitman", "B. Ginsburg"], "venue": "UC Berkeley Technical Report.", "citeRegEx": "You et al\\.,? 2017", "shortCiteRegEx": "You et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "Several proposed methods metigate this problem, including utilizing: per-parameter adaptive learning rate such as AdaGrad (Duchi et al., 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 5, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 81, "endOffset": 106}, {"referenceID": 9, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 119, "endOffset": 146}, {"referenceID": 2, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier\u2019s (Glorot and Bengio, 2010).", "startOffset": 161, "endOffset": 178}, {"referenceID": 4, "context": ", 2016); and intelligent initialization schemes such as xavier\u2019s (Glorot and Bengio, 2010).", "startOffset": 65, "endOffset": 90}, {"referenceID": 1, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier\u2019s (Glorot and Bengio, 2010). These methods heuristically attack the disproportionate training problem, which we justify in section 2. In this paper, we propose to directly enforce proportionate training of layers. Specifically, We propose a gradient update rule that moves every weight tensor in the direction of the gradient, but with a magnitude that changes the tensor value by a relative amount. We use the same relative amount across all layers, to train them all at the same speed. The remainder of the paper is organized as follows. In Section 2 we illustrate the disproportionate training phenomena on a (toy) hypothetical 4-layer neural network, by expanding gradient terms using BackProp Rumelhart et al. (1986). In Section 3, we summarize related work.", "startOffset": 17, "endOffset": 956}, {"referenceID": 8, "context": "W1 and W2, then expand the expressions using the Back Propagation Algorithm (Rumelhart et al., 1986):", "startOffset": 76, "endOffset": 100}, {"referenceID": 9, "context": "Weight normalization (Salimans and Kingma, 2016) and intelligent initialization schemes (e.", "startOffset": 21, "endOffset": 48}, {"referenceID": 5, "context": "BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al.", "startOffset": 10, "endOffset": 35}, {"referenceID": 2, "context": "BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al., 2016) mitigate this problem.", "startOffset": 50, "endOffset": 67}, {"referenceID": 3, "context": "1 Adaptive Gradient Duchi et al. (2011) proposed AdaGrad.", "startOffset": 20, "endOffset": 40}, {"referenceID": 1, "context": "For details, we point readers to the Adam paper (Ba and Kingma, 2015).", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": "Ba and Kingma (2015) propose Adam, which keeps exponential decaying sums, of gradients and of square gradients, respectively known as first and second moments.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "2 LARS You et al. (2017) propose to normalize every layer\u2019s gradients by the ratio of L2 norms of the parameter and the gradient.", "startOffset": 7, "endOffset": 25}, {"referenceID": 0, "context": "Our proposed algorithm, PercentDelta, was used to train our work (Abu-El-Haija et al., 2017), before we where aware of the work of (You et al.", "startOffset": 65, "endOffset": 92}, {"referenceID": 11, "context": ", 2017), before we where aware of the work of (You et al., 2017).", "startOffset": 46, "endOffset": 64}, {"referenceID": 0, "context": "In (Abu-El-Haija et al., 2017), PercentDelta gave us 1% improvement over Adam, over all datasets.", "startOffset": 3, "endOffset": 30}, {"referenceID": 3, "context": "We compare training speed with other algorithms, including per-parameter adaptive learning rate algorithms, AdaGrad (Duchi et al., 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al.", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": ", 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 11, "context": ", 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al., 2017), which also normalizes the gradient w.", "startOffset": 96, "endOffset": 114}, {"referenceID": 11, "context": "Nonetheless, we speculate that PercentDelta (and similarily, LARS, You et al. (2017)) would be very useful in the following scenarios:", "startOffset": 67, "endOffset": 85}, {"referenceID": 3, "context": "Top-to-bottom: PercentDelta (our algorithm), AdaGrad (Duchi et al., 2011), Adam (Ba and Kingma, 2015), LARS (You et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 1, "context": ", 2011), Adam (Ba and Kingma, 2015), LARS (You et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 11, "context": ", 2011), Adam (Ba and Kingma, 2015), LARS (You et al., 2017), and the last row shows the best performer from all algorithms.", "startOffset": 42, "endOffset": 60}, {"referenceID": 6, "context": "For example, Koren et al. (2009) propose to factorize a user-movie rating matrix R \u2208 Ru\u00d7m into:", "startOffset": 13, "endOffset": 33}], "year": 2017, "abstractText": "Deep Neural Networks are generally trained using iterative gradient updates. Magnitudes of gradients are affected by many factors, including choice of activation functions and initialization. More importantly, gradient magnitudes can greatly differ across layers, with some layers receiving much smaller gradients than others. causing some layers to train slower than others and therefore slowing down the overall convergence. We analytically explain this disproportionality. Then we propose to explicitly train all layers at the same speed, by scaling the gradient w.r.t. every trainable tensor to be proportional to its current value. In particular, at every batch, we want to update all trainable tensors, such that the relative change of the L1-norm of the tensors is the same, across all layers of the network, throughout training time. Experiments on MNIST show that our method appropriately scales gradients, such that the relative change in trainable tensors is approximately equal across layers. In addition, measuring the test accuracy with training time, shows that our method trains faster than other methods, giving higher test accuracy given same budget of training steps.", "creator": "LaTeX with hyperref package"}}}