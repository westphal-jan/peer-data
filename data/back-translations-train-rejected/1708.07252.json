{"id": "1708.07252", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "A Study on Neural Network Language Modeling", "abstract": "An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed.", "histories": [["v1", "Thu, 24 Aug 2017 02:14:50 GMT  (125kb,D)", "http://arxiv.org/abs/1708.07252v1", "20 pages, 6 figures"]], "COMMENTS": "20 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["dengliang shi"], "accepted": false, "id": "1708.07252"}, "pdf": {"name": "1708.07252.pdf", "metadata": {"source": "CRF", "title": "A Study on Neural Network Language Modeling", "authors": ["Dengliang Shi", "D. Shi"], "emails": ["dengliang.shi@yahoo.com"], "sections": [{"heading": null, "text": "Keywords: modeling the language of neural networks, optimization techniques, boundaries, improvement scheme"}, {"heading": "1. Introduction", "text": "In fact, in the reactionary reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllrlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll"}, {"heading": "2. Basic Neural Network Language Models", "text": "The objective of statistical language models is to estimate the probability of a word sequence w1w2... wT in a natural language, and the probability can be represented by producing the conditional probability of each word that has given all previous ones: P (wT1) = T \u0435t = 1 P (wt | wt \u2212 11), where wji = wiwi + 1... wj \u2212 1wj. This chain rule is based on the assumption that words in a word sequence depend only statistically on their previous context and form the basis of all statistical language modeling. NNLM is a type of statistical language modeling and is therefore also referred to as neural probability modeling or neural statistical language modeling. According to the architecture of the ANN used, neural network language models can be classified as: FNNLM, RNLM and LSTM-RNLM."}, {"heading": "2.1 Feed-forward Neural Network Language Model, FNNLM", "text": "As mentioned above, the goal of the FNLM is to evaluate the conditional probability P (wt = 11), but there is no effective method to represent the context of the story. Therefore, the idea of n-gram-based LM in FNLM has been adopted that words in a word sequence depend more statistically on the words that are closer to them, and only the n \u2212 1 direct precursors are taken into account when evaluating the conditional probability. This is: P (wt \u2212 11). P (wt \u2212 w) t \u2212 1 t \u2212 n \u2212 n The architecture of the original FNLM is proposed by Bengio et al. (2003) is shown in Figure 1, and w0, wT + 1 are the start and end notes of a word sequence. In this model, a vocabulary is built from a training dataset, and each word in this vocabulary is mapped with a unique index."}, {"heading": "2.2 Recurrent Neural Network Language Model, RNNLM", "text": "The idea of using RNN in LM was proposed much earlier (Bengio et al., 2003; Castro and Prat, 2003), but the first serious attempt to build an RNNLM was by Mikolov et al. (2010, 2011). RNNMs are fundamentally different from feed-forward architectures in the sense that they operate not only on an input space, but also on an internal state space, and the state space allows the representation of sequentially extended dependencies. Therefore, the arbitrary length of the word sequence can be treated with RNNLM, and all previous contexts can be taken into account when predicting the next word. As shown in Figure 2, the representation of words in RNNLM is the same as that of FNLM, but the input of RNN at each step is the feature vector of a direct preceding word instead of the concatenation of the n \u2212 1 previous word."}, {"heading": "2.3 Long Short Term Memory RNNLM, LSTM-RNNLM", "text": "Although RNLM can take all previous words into account when predicting the next word in a sequence of words, the disappearing or exploring problem makes it quite difficult to learn long-term dependencies (Hochreiter and Schmidhuber, 1997). LSTM-RNLM was originally proposed by Sundermeyer et al. (2012), and the entire architecture is almost identical to RNNLM except for the part of the neural network. LSTM-RNN was proposed by Hochreiter and Schmidhuber (1997) and was refined and popularized in the following works (Gers and Schmidhuber, 2000; Cho et al., 2014b). The general architecture of LSTM-RNN is: i t = f \u2212 t \u2212 st \u2212 st \u2212 st \u2212 st i \u00b7 st \u2212 st \u2212 st \u2212 st \u2212 st \u00b7 t \u00b7 f \u00b7 t \u00b7 f \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = W \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t = t \u00b7 t \u00b7 t \u00b7 t \u00b7 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x t = t = t = t = t"}, {"heading": "2.4 Comparison of Neural Network Language Models", "text": "Comparisons between neural network speech models with different architectures have already been made on small and large corpses (Mikolov, 2012; Sundermeyer et al., 2013). Results show that, in general, RNNLMs outperform FNLMs and achieve the best performance with LSTM NNLMs. However, the neural network speech models used in these comparisons are optimized with various techniques and even combined with other types of speech models, let alone the various experimental arrangements and implementation details that do not illustrate the fundamental discrepancy in the performance of neural network speech models with different architecture and cannot be used as a basis for the studies in this paper.Comparison experiments on neural network language models with different architecture were repeated here. Models in these experiments were all simply implemented, and only a class-based speed-up technique was used that will be introduced later."}, {"heading": "3. Optimization Techniques", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Importance Sampling", "text": "Inspired by the contrastive divergence model (Hinton, 2002), Bengio and Senecal (2003b) proposed a sample-based method to accelerate the formation of neural network language models. To apply this method, the results of the neural network should be normalized as follows rather than using a softmax function: P (vi | wt \u2212 11) = E \u2212 y (vi, w \u2212 1) = E \u2212 y (vj, wt \u2212 11), i = 1,., k; t = 1, 2,., Tthen, neural network language models can be treated as a special case of energy-based probability modeling. The main idea of the sample-based method is to explicitly represent the average of log liquidity in terms of gradient parameters."}, {"heading": "3.2 Word Classes", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in fact, in fact, in which they, in fact, in fact, are able to put themselves, are able to move themselves, in a different world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are"}, {"heading": "3.3 Caching", "text": "The cache language models are based on the assumption that the word is more likely to reappear in recent history. In the cache language model, the conditional probability of a word is calculated by interpolating the output of standard language techniques and the probability is evaluated by caching, such as: P (wt | wt \u2212 10) is the output of standard language models, Pc (wt | wt \u2212 10) is the suggested speed of standard language models, Pc (wt \u2212 1 0) is the probability evaluated by caching, and it is a constant, 0 \u2264 1. Soutner et al. (2012) combines FNLM with cache model to improve the performance of FNLM in speech recognition."}, {"heading": "3.4 Bidirectional Recurrent Neural Network", "text": "The fact is that we are able to assert ourselves, that we are able, that we are going to be able, that we are going to be able, that we are going to be able, and that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to put ourselves in."}, {"heading": "4. Limits of Neural Network Language Modeling", "text": "NNLM is state-of-the-art and has been introduced as a promising approach for various NLP tasks. Numerous researchers from different areas of NLP are trying to improve NNLM in the expectation of achieving better performance in their areas, such as reduced helplessness in test data, lower word error rate in speech recognition, higher bilingual evaluation understudy (BLEU) in machine translation, etc. However, few of them spare attention to the limitations of NNLM. Without a thorough understanding of the limitations of NLM, the applicable scope of NLM and the directions to improve NLM in various NLP tasks cannot be clearly defined. In this section, the limitations of NLM are examined from two aspects: model architecture and knowledge representation."}, {"heading": "4.1 Model Architecture", "text": "In most speech models, including neural network models, the words are placed one-to-one in their previous context or follow one that simulates the way humans deal with natural languages. As mentioned above, it is not always the case that the words in a word sequence depend only on their previous or subsequent context; in fact, they know what they are trying to express in the word sequence, and the word sequence is already stored in memory when humans speak or write."}, {"heading": "4.2 Knowledge Representation", "text": "This year, it is only a matter of time before we will be able to do it again, until we will be able to find a solution that will enable us to find a solution."}, {"heading": "5. Future Work", "text": "Various architectures of neural network language models are described and a number of enhancement techniques are evaluated in this paper, but there is something else that should be included, such as the Gate Recurrent Unit (GRU) RNNLM, the dropout strategy for address matching, character level neural network language model, and ect. Furthermore, the experiments in this paper are all conducted on the Brown Corpus, which is a small corpus, and other results can be obtained if the size of the corpus increases. Therefore, all the experiments in this paper should be repeated on a much larger corpus. Several limitations of the NNLM have been explored, and in order to achieve language understanding, these limitations must be overcome. I have not yet found a complete solution, but some ideas that will be explored next. Firstly, the architecture shown in Figure 5 can be used as a general enhancement scheme for ANN, and I will try to find out the structure of the immutable network for coders."}, {"heading": "6. Conclusion", "text": "This paper describes various architectures of neural network models, and the results of comparative experiments indicate that RNNLM and LSTM-RNNLM have no advantages over small-body FNLM. Improvements over these models, including meaning samples, word classes, caching, and BiRNN, have also been introduced and evaluated separately, and some interesting results have been suggested that can help us gain a better understanding of NNLM. Another significant contribution in this paper is the exploration of the limits of NNLM from the aspects of model architecture and knowledge representation. Although the state of the art has been achieved through the use of NNLM in various NLP tasks, the power of NLM has been exaggerated all the time. NLM's main idea is to approximate the likely distribution of word sequences in a natural language using ANN."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J.S. Senecal"], "venue": "In AISTATS,", "citeRegEx": "Bengio and Senecal.,? \\Q2003\\E", "shortCiteRegEx": "Bengio and Senecal.", "year": 2003}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "V.J.D. Pietra", "P.V. DeSouza", "J.C. Lai", "R.L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "New directions in connectionist language modeling", "author": ["M.J. Castro", "F. Prat"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Castro and Prat.,? \\Q2003\\E", "shortCiteRegEx": "Castro and Prat.", "year": 2003}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Computer Science,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.M. Van", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Fast semantic extraction using a novel neural network architecture", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the Meeting of the Association for Computational Linguistics,", "citeRegEx": "Collobert and Weston.,? \\Q2007\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2007}, {"title": "A unified architecture for natural language processing - deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmidhuber"], "venue": "In Proceedings of the IEEE-INNS-ENNS International Joint Conference on,", "citeRegEx": "Gers and Schmidhuber.,? \\Q2000\\E", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Classes for fast maximum entropy training", "author": ["J. Goodman"], "venue": "In International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "A bit of progress in language modeling", "author": ["J.D. Goodman"], "venue": "Computer Speech and Langauge,", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. r. Mohamed", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["A. Graves", "N. Jaitly", "A.R. Mohamed"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Ups and downs: modeling the visual evolution of fashion trends with one-class collaborative filtering", "author": ["R. He", "J.Mcauley"], "venue": "In International World Wide Web Conferences Steering Committee,", "citeRegEx": "He and J.Mcauley.,? \\Q2016\\E", "shortCiteRegEx": "He and J.Mcauley.", "year": 2016}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Cache based recurrent neural network language model inference for first pass speech recognition", "author": ["Z. Huang", "G. Zweig", "B. Dumoulin"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "Neural Computation,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Recurrent neural network based language modeling in meeting recognization", "author": ["S. Kombrink", "T. Mikolov", "M. Karafiat", "L. Burget"], "venue": "In INTERSPEECH,", "citeRegEx": "Kombrink et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kombrink et al\\.", "year": 2011}, {"title": "Image-based recommendations on styles and substitutes", "author": ["J. Mcauley", "C. Targett", "Q. Shi", "A.V.D. Hengel"], "venue": "In International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Mcauley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mcauley et al\\.", "year": 2015}, {"title": "Natural language processing with modular pdp neural networks and distributed lexicon", "author": ["R. Miikkulainen", "M.G. Dyer"], "venue": "Cognitive Science,", "citeRegEx": "Miikkulainen and Dyer.,? \\Q1991\\E", "shortCiteRegEx": "Miikkulainen and Dyer.", "year": 1991}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J.H. Cernocky", "S. Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network based language model", "author": ["T. Mikolov", "S. Kombrink", "J.H. Cernocky", "S. Khudanpur"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Aistats,", "citeRegEx": "Morin and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "From neuron to brain", "author": ["J.G. Nicholls", "A.R. Martin", "P.A. Brown", "M.E. Diamond", "D.A. Weisblat"], "venue": "Sinauer Associates, Inc,", "citeRegEx": "Nicholls et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nicholls et al\\.", "year": 2011}, {"title": "Learning internal representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323:533\u2013536,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Sequential neural neural text compression", "author": ["J. Schmidhuber"], "venue": "IEEE Transactions on Neural Network,", "citeRegEx": "Schmidhuber.,? \\Q1996\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1996}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Prefix tree based n-best list rescoring for recurrent neural network language model used in speech recognition system", "author": ["Y. Si", "Q. Zhang", "T. Li", "J. Pan", "Y. Yan"], "venue": "In INTERSPEECH,", "citeRegEx": "Si et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Si et al\\.", "year": 2013}, {"title": "Neural network language model with cache", "author": ["D. Soutner", "Z. Loose", "L. Muller", "A. Prazak"], "venue": "In International Conference on Text, Speech and Dialogue,", "citeRegEx": "Soutner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soutner et al\\.", "year": 2012}, {"title": "Lstm nerual networks for language modeling", "author": ["M. Sundermeyer", "R. Schluter", "H. Ney"], "venue": "In Interspeech,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Comparison of feedforward and recurrent nerual network language models", "author": ["M. Sundermeyer", "I. Oparin", "J.L. Gauvain", "B. Freiberg", "R. Schluter", "H. Ney"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sundermeyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Google\u2019s neural machine translation system: bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "Computer Science,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Can artificial neural network learn language models", "author": ["W. Xu", "A. Rudnicky"], "venue": "In Proceedings of International Conference on Speech and Language Processing,", "citeRegEx": "Xu and Rudnicky.,? \\Q2000\\E", "shortCiteRegEx": "Xu and Rudnicky.", "year": 2000}], "referenceMentions": [{"referenceID": 18, "context": "Generally, a well-designed language model makes a critical difference in various natural language processing (NLP) tasks, like speech recognition (Hinton et al., 2012; Graves et al., 2013a), machine translation (Cho et al.", "startOffset": 146, "endOffset": 189}, {"referenceID": 38, "context": ", 2013a), machine translation (Cho et al., 2014a; Wu et al., 2016), semantic extraction (Collobert and Weston, 2007, 2008) and etc.", "startOffset": 30, "endOffset": 66}, {"referenceID": 24, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al.", "startOffset": 32, "endOffset": 103}, {"referenceID": 31, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al.", "startOffset": 32, "endOffset": 103}, {"referenceID": 39, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al.", "startOffset": 32, "endOffset": 103}, {"referenceID": 2, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al. (2003) and did not show prominent advantages over other techniques of LM until recurrent neural network (RNN) was investigated for NNLM (Mikolov et al.", "startOffset": 228, "endOffset": 249}, {"referenceID": 21, "context": "In a few works (Jozefowicz et al., 2016) on exploring the limits of NNLM, only some practical issues, like computational complexity, corpus, vocabulary size, and etc.", "startOffset": 15, "endOffset": 40}, {"referenceID": 2, "context": "P (wt|w 1 ) \u2248 P (wt|w t\u22121 t\u2212n+1) The architecture of the original FNNLM proposed by Bengio et al. (2003) is showed in Figure 1, and w0, wT+1 are the start and end marks of a word sequence respectively.", "startOffset": 84, "endOffset": 105}, {"referenceID": 3, "context": "2 Recurrent Neural Network Language Model, RNNLM The idea of applying RNN in LM was proposed much earlier (Bengio et al., 2003; Castro and Prat, 2003), but the first serious attempt to build a RNNLM was made by Mikolov et al.", "startOffset": 106, "endOffset": 150}, {"referenceID": 5, "context": "2 Recurrent Neural Network Language Model, RNNLM The idea of applying RNN in LM was proposed much earlier (Bengio et al., 2003; Castro and Prat, 2003), but the first serious attempt to build a RNNLM was made by Mikolov et al.", "startOffset": 106, "endOffset": 150}, {"referenceID": 30, "context": "Because of the involvement of previous internal state at every step, back-propagation through time (BPTT) algorithm (Rumelhart et al., 1986) is preferred for better performance", "startOffset": 116, "endOffset": 140}, {"referenceID": 25, "context": "If data set is treated as a single long word sequence, truncated BPTT should be used and back-propagating error gradient through 5 steps is enough, at least for small corpus (Mikolov, 2012).", "startOffset": 174, "endOffset": 189}, {"referenceID": 19, "context": "3 Long Short Term Memory RNNLM, LSTM-RNNLM Although RNNLM can take all predecessor words into account when predicting next word in a word sequence, but it is quite difficult to be trained over long term dependencies because of the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997).", "startOffset": 262, "endOffset": 296}, {"referenceID": 11, "context": "LSTM-RNN was proposed by Hochreiter and Schmidhuber (1997) and was refined and popularized in following works (Gers and Schmidhuber, 2000; Cho et al., 2014b).", "startOffset": 110, "endOffset": 157}, {"referenceID": 16, "context": "3 Long Short Term Memory RNNLM, LSTM-RNNLM Although RNNLM can take all predecessor words into account when predicting next word in a word sequence, but it is quite difficult to be trained over long term dependencies because of the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997). LSTM-RNN was designed aiming at solving this problem, and better performance can be expected by replacing RNN with LSTM-RNN. LSTM-RNNLM was first proposed by Sundermeyer et al. (2012), and the whole architecture is almost the same as RNNLM except the part of neural network.", "startOffset": 263, "endOffset": 482}, {"referenceID": 16, "context": "3 Long Short Term Memory RNNLM, LSTM-RNNLM Although RNNLM can take all predecessor words into account when predicting next word in a word sequence, but it is quite difficult to be trained over long term dependencies because of the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997). LSTM-RNN was designed aiming at solving this problem, and better performance can be expected by replacing RNN with LSTM-RNN. LSTM-RNNLM was first proposed by Sundermeyer et al. (2012), and the whole architecture is almost the same as RNNLM except the part of neural network. LSTM-RNN was proposed by Hochreiter and Schmidhuber (1997) and was refined and popularized in following works (Gers and Schmidhuber, 2000; Cho et al.", "startOffset": 263, "endOffset": 632}, {"referenceID": 25, "context": "4 Comparison of Neural Network Language Models Comparisons among neural network language models with different architectures have already been made on both small and large corpus (Mikolov, 2012; Sundermeyer et al., 2013).", "startOffset": 179, "endOffset": 220}, {"referenceID": 36, "context": "4 Comparison of Neural Network Language Models Comparisons among neural network language models with different architectures have already been made on both small and large corpus (Mikolov, 2012; Sundermeyer et al., 2013).", "startOffset": 179, "endOffset": 220}, {"referenceID": 3, "context": "Experiments were performed on the Brown Corpus, and the experimental setup for Brown corpus is the same as that in (Bengio et al., 2003), the first 800000 words (ca01\u223ccj54) were used for training, the following 200000 words (cj55\u223ccm06) for validation and the rest (cn01\u223ccr09) for test.", "startOffset": 115, "endOffset": 136}, {"referenceID": 2, "context": "An explanation given for this phenomenon by Bengio et al. (2003) is that direct connections provide a bit more capacity and faster learning of the \u201dlinear\u201d part of mapping from inputs to outputs but impose a negative effect on generalization.", "startOffset": 44, "endOffset": 65}, {"referenceID": 2, "context": "An explanation given for this phenomenon by Bengio et al. (2003) is that direct connections provide a bit more capacity and faster learning of the \u201dlinear\u201d part of mapping from inputs to outputs but impose a negative effect on generalization. For bias terms, no significant improvement on performance was gained by adding bias terms which was also observed on RNNLM by Mikolov (2012). In the rest of this paper, all studies will be performed on LSTM-RNNLM with neither", "startOffset": 44, "endOffset": 384}, {"referenceID": 17, "context": "1 Importance Sampling Inspired by the contrastive divergence model (Hinton, 2002), Bengio and Senecal (2003b) proposed a sampling-based method to speed up the training of neural network language models.", "startOffset": 67, "endOffset": 81}, {"referenceID": 1, "context": "1 Importance Sampling Inspired by the contrastive divergence model (Hinton, 2002), Bengio and Senecal (2003b) proposed a sampling-based method to speed up the training of neural network language models.", "startOffset": 83, "endOffset": 110}, {"referenceID": 1, "context": "Three sampling approximation algorithms were presented by Bengio and Senecal (2003b): Monte-Carlo Algorithm, Independent Metropolis-Hastings Algorithm and Importance Sampling Algorithm.", "startOffset": 58, "endOffset": 85}, {"referenceID": 4, "context": "2 Word Classes Before the idea of word classes was introduced to NNLM, it had been used in LM extensively for improving perplexities or increasing speed (Brown et al., 1992; Goodman, 2001b).", "startOffset": 153, "endOffset": 189}, {"referenceID": 10, "context": "In hierarchical neural network language model, instead of assigning every word in vocabulary with a unique class, a hierarchical binary tree of words is built according to the word similarity information extracted from WordNet (Fellbaum, 1998), and every word in vocabulary is assigned with a", "startOffset": 227, "endOffset": 243}, {"referenceID": 27, "context": "Morin and Bengio (2005) extended word classes to a hierarchical binary clustering of words and built a hierarchical neural network language model.", "startOffset": 0, "endOffset": 24}, {"referenceID": 28, "context": "In Morin and Bengio (2005), impressive speed-up during both training and test, which were less than the theoretical one, were obtained but an obvious increase in PPL was also observed.", "startOffset": 3, "endOffset": 27}, {"referenceID": 25, "context": "There is a simpler way to speed up neural network language models using word classes which was proposed by Mikolov et al. (2011). Words in vocabulary are arranged in descent", "startOffset": 107, "endOffset": 129}, {"referenceID": 25, "context": "This strategy was further optimized by (Mikolov, 2012) using following criterion: i r < z \u2211", "startOffset": 39, "endOffset": 54}, {"referenceID": 34, "context": "Soutner et al. (2012) combined FNNLM with cache model to enhance the performance of FNNLM in speech recognition, and the cache model was formed based on the previous context as following: Pc(wt|w t\u2212N ) = 1 N N \u2211", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 22, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 33, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 20, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 29, "context": "A class cache model was also proposed by Soutner et al. (2012) for the case in which words are clustered into word classes.", "startOffset": 41, "endOffset": 63}, {"referenceID": 2, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014). The main idea of this approach is to store the outputs and states of language models for future prediction given the same contextual history. In Huang et al. (2014), four caches were proposed, and they were all achieved by hash lookup tables to store key and value pairs: probability P (wt|w 0 ) and word sequence wt 0; history w t\u22121 0 and its corresponding hidden state vector; history w t\u22121 0 and the denominator of the softmax function for classes; history wt\u22121 0 , class index c(wt) and the denominator of the softmax function for words.", "startOffset": 78, "endOffset": 325}, {"referenceID": 2, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014). The main idea of this approach is to store the outputs and states of language models for future prediction given the same contextual history. In Huang et al. (2014), four caches were proposed, and they were all achieved by hash lookup tables to store key and value pairs: probability P (wt|w 0 ) and word sequence wt 0; history w t\u22121 0 and its corresponding hidden state vector; history w t\u22121 0 and the denominator of the softmax function for classes; history wt\u22121 0 , class index c(wt) and the denominator of the softmax function for words. In Huang et al. (2014), around 50-fold speed-up was reported with this caching technique in speech recognition but, unfortunately, it only works for prediction and cannot be applied during training.", "startOffset": 78, "endOffset": 725}, {"referenceID": 37, "context": "4 Bidirectional Recurrent Neural Network In Sutskever et al. (2014), significant improvement on neural machine translation (NMT) for an English to French translation task was achieved by reversing the order of input word sequence, and the possible explanation given for this phenomenon was that smaller \u201dminimal time lag\u201d was obtained in this way.", "startOffset": 44, "endOffset": 68}, {"referenceID": 32, "context": "Bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) was designed to process data in both directions with two separate hidden layers, so better performance can be expected by using BiRNN.", "startOffset": 47, "endOffset": 75}, {"referenceID": 0, "context": "(2013b), and then was evaluated in other NLP tasks, like NMT (Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 61, "endOffset": 101}, {"referenceID": 38, "context": "(2013b), and then was evaluated in other NLP tasks, like NMT (Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 61, "endOffset": 101}, {"referenceID": 13, "context": "BiRNN was introduced to speech recognition by Graves et al. (2013b), and then was evaluated in other NLP tasks, like NMT (Bahdanau et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 29, "context": "In fact, the strong power of biological neural system is original from the enormous number of neurons and various connections among neurons, including gathering, scattering, lateral and recurrent connections (Nicholls et al., 2011).", "startOffset": 208, "endOffset": 231}, {"referenceID": 16, "context": "In order to verify this, one million words reviews on electronics and books were extracted from Amazon reviews (He and J.Mcauley, 2016; Mcauley et al., 2015) respectively as data sets from different fields, and 800000 words for training, 100000 words for validation, and the rest for test.", "startOffset": 111, "endOffset": 157}, {"referenceID": 23, "context": "In order to verify this, one million words reviews on electronics and books were extracted from Amazon reviews (He and J.Mcauley, 2016; Mcauley et al., 2015) respectively as data sets from different fields, and 800000 words for training, 100000 words for validation, and the rest for test.", "startOffset": 111, "endOffset": 157}], "year": 2017, "abstractText": "An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed.", "creator": "LaTeX with hyperref package"}}}