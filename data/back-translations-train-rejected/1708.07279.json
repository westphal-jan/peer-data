{"id": "1708.07279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Combining Discrete and Neural Features for Sequence Labeling", "abstract": "Neural network models have recently received heated research attention in the natural language processing community. Compared with traditional models with discrete features, neural models have two main advantages. First, they take low-dimensional, real-valued embedding vectors as inputs, which can be trained over large raw data, thereby addressing the issue of feature sparsity in discrete models. Second, deep neural networks can be used to automatically combine input features, and including non-local features that capture semantic patterns that cannot be expressed using discrete indicator features. As a result, neural network models have achieved competitive accuracies compared with the best discrete models for a range of NLP tasks.", "histories": [["v1", "Thu, 24 Aug 2017 05:24:26 GMT  (575kb,D)", "http://arxiv.org/abs/1708.07279v1", "Accepted by International Conference on Computational Linguistics and Intelligent Text Processing (CICLing) 2016, April"]], "COMMENTS": "Accepted by International Conference on Computational Linguistics and Intelligent Text Processing (CICLing) 2016, April", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jie yang", "zhiyang teng", "meishan zhang", "yue zhang"], "accepted": false, "id": "1708.07279"}, "pdf": {"name": "1708.07279.pdf", "metadata": {"source": "CRF", "title": "Combining Discrete and Neural Features for Sequence Labeling", "authors": ["Jie Yang", "Zhiyang Teng", "Meishan Zhang", "Yue Zhang"], "emails": ["jie_yang@mymail.sutd.edu.sg", "zhiyang_teng@mymail.sutd.edu.sg", "meishan_zhang@sutd.edu.sg", "yue_zhang@sutd.edu.sg"], "sections": [{"heading": null, "text": "Keywords: discrete characteristics, neural characteristics, LSTM"}, {"heading": "1 Introduction", "text": "This year it has come to the point where it will be able to take the lead, \"he said in an interview with the German Press Agency.\" We have taken the lead, \"he said,\" but we are not yet in the position. \""}, {"heading": "2 Related Work", "text": "There are two types of word segmentation methods. Xue [33] treats it as a classic problem that builds a network 4bert by using sequence marking tasks, using B (egin) / I (nternal) / E (nding) / S (inglecharacter word) tags for each character in the input to indicate its segmentation status. It was followed by Peng et al. [34], which use CRF properties to improve accuracy. Most subsequent work follows [34.35.36,37,38] and feature engineering comes out of the key research questions. This type of research is commonly referred to as the character-based method. Recently, neural networks have been applied to character-based segmentation of tasks [39.40,41], achieving results comparable to discrete methods. The second type of word-based segmentation, scoring outputs based on sequencing based on word functions directly on word problems [43,444]."}, {"heading": "3 Method", "text": "The structures of our discrete and neural models are represented in Fig. 1 (a) and 1 (b), which are used for all tasks in this work. Black and white elements represent binary characteristics for discrete models and grey elements are continuous representations for word / character embedding. The only difference between different tasks is the definition of input and output sequences, and the characteristics are used. The discrete model is a standard CRF model in which a sequence of input and output labels x = x2,., it models the output sequence y = y1, y2,., yn by calculating two potentials. Specifically, the output clique potential shows the correlation between input and output labels. (x, yi) = exp (x, yi)) fo (1) fo (x, yi) fo (yi) is a feature vector that is extracted from and yi."}, {"heading": "4 Experiments", "text": "We conduct our experiments on various sequence marking tasks, including Chinese word segmentation, part-of-speech tagging and named entity recognition. In these three tasks, the input embeddings differ. In Chinese word segmentation, we use both letter embeddings and character bigram embeddings to calculate e (xi). In POS tagging, e (xi) consists of word embeddings and letter embeddings. In NER, we include word embeddings, character embeddings and POS embeddings for e (xi). Letter embeddings, character bigrams embeddings and word embeddings are trained separately using word2vec [23]. In training, we select the English word embeddings as SENNA [25]. We use Chinese Gigaword Fifth Edition1 to prepare necessary embeddings for Chinese words [60]."}, {"heading": "4.1 Chinese Word Segmentation", "text": "For Chinese word segmentation, we select PKU, MSR and CTB60 as evaluation data sets. PKU and MSR data sets are sourced from SIGHAN Bakeoff 2005 corpus3. We divide the PKU and MSR data sets in the same way as Chen et al. [62] and the CTB60 data set as Zhang et al. [63]. Table 6 shows the statistical results for these three data sets. We evaluate the segmentation accuracy by precision (P), recall (R) and F-measure (F). The experimental results of Chinese word segmentation are in Table 7. Our common models showed comparable results to those reported by Zhang & Clark [42], where they adopt a word-based perceptron model with carefully designed discrete features. Compared to the two discrete and neural models, we can obtain onal results from all three sets of data."}, {"heading": "4.2 POS Tagging", "text": "The English dataset is selected following Toutanova et al. [64] and the Chinese dataset from Li et al. [65] to the CTB. Statistical results are given in Table 8. Toutanova's model [64] uses bidirectional dependency networks to capture preceding and subsequent tag contexts for the English POS tagging task. Li et al. [65] uses heterogeneous datasets for Chinese POS tagging by bundling two datasets and training in enlarged datasets. The common model has state-of-the-art accuracy on the CTB corpus. Both the discrete and neural models obtain comparable accuracies with the state-of-the-art system on English and Chinese datasets. The common model shows significant improvements over the separate model, especially for the Chinese POS tagging task, with an increase in accuracy of 1%. Fig. 3 shows the accuracy comparison for the English and Chinese datasets based on the two different sets, each based on 23% and the two different Chinese datasets."}, {"heading": "4.3 NER", "text": "For the NER task, we divide Ontonotes 4.0 according to Che et al. [54] in order to obtain both English and Chinese datasets. Table 10 shows the set numbers of train / development / test datasets. We follow Che et al. [54] in selecting both English and Chinese datasets. Their work results in bilingual constraints of parallel datasets, resulting in a significant improvement in F-scores for both English and Chinese datasets. Our discrete and neural model shows comparable memory values compared to Che's results [54] for both datasets. Similar to the two previous tasks, the common model offers significant improvements compared to separate models (discrete / neural) in all metrics. This shows that discrete and neural models can identify entities based on different indicator characteristics, and they can complement each other. Comparing the set F-measures in Figure 4 confirms this model by observing the state of the two systems."}, {"heading": "5 Conclusion", "text": "We have proposed a common sequence labeling model that combines neural features and discrete indicator features that can incorporate the benefits of carefully designed feature templates over decades and automatically induced features of neural networks. By experimenting with various sequence labeling tasks, including Chinese word segmentation, POS tagging, and entity recognition in Chinese and English, we show that our common model can unanimously outperform models that only include discrete features or neural features and state-of-the-art systems in all comparable tasks. Comparing accuracy and measurement distribution for discrete and neural models also suggests that discrete and neural models can reveal different related information, which explains why combined models can outperform different models. In the future, we will investigate the impact of our common model on other NLP tasks such as parsing and machine translation."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their detailed comments. This work is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301."}], "references": [{"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Jiajun Chen"], "venue": "In ACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein"], "venue": "ACL-IJCNLP, pages 302\u2013312,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": "arXiv preprint arXiv:1511.04586,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "In ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C\u0131cero Nogueira dos Santos", "Ma\u0131ra Gatti"], "venue": "In COLING,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Target-dependent twitter sentiment classification with rich automatic features", "author": ["Duy-Tin Vo", "Yue Zhang"], "venue": "In IJCAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Neural networks for open domain targeted sentiment", "author": ["Meishan Zhang", "Yue Zhang", "Duy-Tin Vo"], "venue": "In EMNLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "In IJCNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Deep learning for event-driven stock prediction", "author": ["Xiao Ding", "Yue Zhang", "Ting Liu", "Junwen Duan"], "venue": "In ICJAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "What happens next? event prediction using a compositional neural network", "author": ["Granroth-Wilding Mark", "Clark Stephen"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Revisiting embedding features for simple semi-supervised learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In EMNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Tagging the web: Building a robust web tagger with neural network", "author": ["Ji Ma", "Yue Zhang", "Jingbo Zhu"], "venue": "In ACL,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning a product of experts with elitist lasso", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "In IJCNLP,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Combining discrete and continuous features for deterministic transition-based dependency parsing", "author": ["Meishan Zhang", "Yue Zhang"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Chinese word segmentation as character tagging", "author": ["Nianwen Xue"], "venue": "Computational Linguistics and Chinese Language Processing,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum"], "venue": "In Coling,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Character-level dependencies in chinese: usefulness and learning", "author": ["Hai Zhao"], "venue": "In EACL, pages 879\u2013887,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "A cascaded linear model for joint chinese word segmentation and part-of-speech tagging", "author": ["Wenbin Jiang", "Liang Huang", "Qun Liu", "Yajuan L\u00fc"], "venue": "In ACL,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging", "author": ["Weiwei Sun"], "venue": "In HLT-ACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Domain adaptation for crf-based chinese word segmentation using free annotations", "author": ["Yijia Liu", "Yue Zhang", "Wanxiang Che", "Ting Liu", "Fan Wu"], "venue": "In EMNLP,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu"], "venue": "In EMNLP,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Chang Baobao"], "venue": "In ACL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang"], "venue": "In EMNLP,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark"], "venue": "In ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Word-based and character-based word segmentation models: Comparison and combination", "author": ["Weiwei Sun"], "venue": "In Coling, pages 1211\u20131219,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Unsupervised domain adaptation for joint segmentation and pos-tagging", "author": ["Yang Liu", "Yue Zhang"], "venue": "In COLING (Posters),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["Adwait Ratnaparkhi"], "venue": "In EMNLP,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In EMNLP, pages", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics", "author": ["Christopher D Manning"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero D Santos", "Bianca Zadrozny"], "venue": "In ICML,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Part-of-speech tagging with recurrent neural networks", "author": ["Juan Antonio Perez-Ortiz", "Mikel L Forcada"], "venue": "Universitat d\u2019Alacant,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2001}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons", "author": ["Andrew McCallum", "Wei Li"], "venue": "In HLTNAACL,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2003}, {"title": "Named entity recognition with a maximum entropy approach", "author": ["C Hai Leong", "N Hwee Tou"], "venue": "In HLT-NAACL,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2003}, {"title": "An effective two-stage model for exploiting non-local dependencies in named entity recognition", "author": ["Vijay Krishnan", "Christopher D Manning"], "venue": "In Coling and ACL,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2006}, {"title": "Named entity recognition with bilingual constraints", "author": ["Wanxiang Che", "Mengqiu Wang", "Christopher D Manning", "Ting Liu"], "venue": "In HLT-NAACL,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth"], "venue": "In Coling,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["C\u0131cero dos Santos", "Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "In NEWS,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2015}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2003}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Yoram Singer John Duchi", "Elad Hazan"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2011}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Yue Zhang", "Stephen Clark"], "venue": "Computational linguistics,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1929}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang"], "venue": "In EMNLP,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Character-level chinese dependency parsing", "author": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In NAACL,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2003}, {"title": "Coupled sequence labeling on heterogeneous annotations: Pos tagging as a case study", "author": ["Li Zhenghua", "Chao Jiayuan", "Zhang Min", "Chen Wenliang"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 1, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 2, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 3, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 4, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 5, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 6, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 7, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 8, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 9, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 10, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 11, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 12, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 13, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 14, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 15, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 16, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 17, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 18, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 19, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 20, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 21, "context": "First, neural network models take low-dimensional dense embeddings [23,24,25] as inputs, which can be trained from large-scale test, thereby overcoming the issue of sparsity.", "startOffset": 67, "endOffset": 77}, {"referenceID": 22, "context": "First, neural network models take low-dimensional dense embeddings [23,24,25] as inputs, which can be trained from large-scale test, thereby overcoming the issue of sparsity.", "startOffset": 67, "endOffset": 77}, {"referenceID": 23, "context": "First, neural network models take low-dimensional dense embeddings [23,24,25] as inputs, which can be trained from large-scale test, thereby overcoming the issue of sparsity.", "startOffset": 67, "endOffset": 77}, {"referenceID": 24, "context": "[26] integrated word embedding as real-word features into a discrete Conditional Random Field [27] (CRF) model, finding enhanced results for a number of sequence labeling tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] integrated word embedding as real-word features into a discrete Conditional Random Field [27] (CRF) model, finding enhanced results for a number of sequence labeling tasks.", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "[28] show that the integration can be improved if the embedding features are carefully discretized.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] treated a discrete perception model as a neural layer, which is integrated into a neural model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Wang & Manning [30] integrated a discrete CRF model and a neural CRF model by combining their output layers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Greg & Dan [6] and Zhang et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 16, "context": "[18] also followed this method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Zhang & Zhang [31] compared various integration methods for parsing, and found that the second type of integration gives better results.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "We follow Zhang & Zhang [31], investigating the effect of feature combination for a range of sequence labeling tasks, including word segmentation, Part-OfSpeech (POS) tagging and named entity recognition (NER) for Chinese and English, respectively.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "For neural features, we adopt a neural CRF model, using a separated Long Short-Term Memory [32] (LSTM) layer to extract input features.", "startOffset": 91, "endOffset": 95}, {"referenceID": 31, "context": "Xue [33] treat it as a sequence labeling task, using B(egin)/I(nternal) /E(nding)/S(inglecharacter word) tags on each character in the input to indicate its segmentation status.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "[34], who use CRF to improve the accuracies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 33, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 34, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 35, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 36, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 37, "context": "Recently, neural networks have been applied to character-based segmentation [39,40,41], giving results comparable to discrete methods.", "startOffset": 76, "endOffset": 86}, {"referenceID": 38, "context": "Recently, neural networks have been applied to character-based segmentation [39,40,41], giving results comparable to discrete methods.", "startOffset": 76, "endOffset": 86}, {"referenceID": 39, "context": "Recently, neural networks have been applied to character-based segmentation [39,40,41], giving results comparable to discrete methods.", "startOffset": 76, "endOffset": 86}, {"referenceID": 40, "context": "On the other hand, the second kind of work studies wordbased segmentation, scoring outputs based on word features directly [42,43,44].", "startOffset": 123, "endOffset": 133}, {"referenceID": 41, "context": "On the other hand, the second kind of work studies wordbased segmentation, scoring outputs based on word features directly [42,43,44].", "startOffset": 123, "endOffset": 133}, {"referenceID": 42, "context": "On the other hand, the second kind of work studies wordbased segmentation, scoring outputs based on word features directly [42,43,44].", "startOffset": 123, "endOffset": 133}, {"referenceID": 43, "context": "POS-tagging has been investigated as a classic sequence labeling problem [45,46,47], for which a well-established set of features are used.", "startOffset": 73, "endOffset": 83}, {"referenceID": 44, "context": "POS-tagging has been investigated as a classic sequence labeling problem [45,46,47], for which a well-established set of features are used.", "startOffset": 73, "endOffset": 83}, {"referenceID": 45, "context": "POS-tagging has been investigated as a classic sequence labeling problem [45,46,47], for which a well-established set of features are used.", "startOffset": 73, "endOffset": 83}, {"referenceID": 46, "context": "In order to include word morphology and word shape knowledge, a convolutional neural network (CNN) for automatically learning character-level representations is investigated in [48].", "startOffset": 177, "endOffset": 181}, {"referenceID": 23, "context": "[25] built a CNN neural network for multiple sequence labeling tasks, which gives state-of-the-art POS results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Recurrent neural network models have also been used for this task [49,50].", "startOffset": 66, "endOffset": 73}, {"referenceID": 48, "context": "Recurrent neural network models have also been used for this task [49,50].", "startOffset": 66, "endOffset": 73}, {"referenceID": 48, "context": "[50] combines bidirectional LSTM with a CRF layer, their model is robust and has less dependence on word embedding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "McCallum & Li [51] use CRF model for NER task and exploit Web lexicon as feature enhancement.", "startOffset": 14, "endOffset": 18}, {"referenceID": 50, "context": "Chieu & Ng [52], Krishnan & Manning [53] and Che et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 51, "context": "Chieu & Ng [52], Krishnan & Manning [53] and Che et al.", "startOffset": 36, "endOffset": 40}, {"referenceID": 52, "context": "[54] tackle this task through non-local features [55].", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[54] tackle this task through non-local features [55].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "[25] model we referred before, NER task has also been included.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[56] boost the neural model by adding character embedding on Collobert\u2019s structure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[57] take the lead by employing LSTM for NER tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[58] use CNN model to extract character embedding and attach it with word embedding and afterwards feed them into Bi-directional LSTM model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "Online Adagrad [59] is used to train the model, with the initial learning rate set to be \u03b7.", "startOffset": 15, "endOffset": 19}, {"referenceID": 52, "context": "[54] by adding part-of-speech information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "English and Chinese datasets are labeled by ZPar [60], prefix and suffix on two datasets are both including 4 characters.", "startOffset": 49, "endOffset": 53}, {"referenceID": 52, "context": "Word clusters in both English and Chinese tasks are same with Che\u2019s work [54].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "Character embeddings, character bigram embeddings and word embeddings are pretrained separately using word2vec[23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "English word embedding is chosen as SENNA [25].", "startOffset": 42, "endOffset": 46}, {"referenceID": 58, "context": "The Chinese corpus is segmented by ZPar [60].", "startOffset": 40, "endOffset": 44}, {"referenceID": 59, "context": "Dropout [61] technology has been used to suppress over-fitting in the input layer.", "startOffset": 8, "endOffset": 12}, {"referenceID": 60, "context": "[62], and the CTB60 set as Zhang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[63].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "Our joint models shown comparable results to the state-of-the-art results reported by Zhang & Clark [42], where they adopt a word-based perceptron model with carefully designed discrete features.", "startOffset": 100, "endOffset": 104}, {"referenceID": 62, "context": "[64] and Chinese dataset by Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[65] on CTB.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "Toutanova\u2019s model [64] exploits bidirectional dependency networks to capture", "startOffset": 18, "endOffset": 22}, {"referenceID": 63, "context": "[65] utilize heterogeneous datasets for Chinese POS tagging through bundling two sets of tags and training in enlarged dataset, their system got state-of-the-art accuracy on CTB corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[54] to get both English and Chinese datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[54] on choosing both the English and the Chinese datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "Our discrete and neural models show comparable recall values compared with Che\u2019s results [54] on both datasets.", "startOffset": 89, "endOffset": 93}], "year": 2017, "abstractText": "Neural network models have recently received heated research attention in the natural language processing community. Compared with traditional models with discrete features, neural models have two main advantages. First, they take low-dimensional, real-valued embedding vectors as inputs, which can be trained over large raw data, thereby addressing the issue of feature sparsity in discrete models. Second, deep neural networks can be used to automatically combine input features, and including non-local features that capture semantic patterns that cannot be expressed using discrete indicator features. As a result, neural network models have achieved competitive accuracies compared with the best discrete models for a range of NLP tasks. On the other hand, manual feature templates have been carefully investigated for most NLP tasks over decades and typically cover the most useful indicator pattern for solving the problems. Such information can be complementary the features automatically induced from neural networks, and therefore combining discrete and neural features can potentially lead to better accuracy compared with models that leverage discrete or neural features only. In this paper, we systematically investigate the effect of discrete and neural feature combination for a range of fundamental NLP tasks based on sequence labeling, including word segmentation, POS tagging and named entity recognition for Chinese and English, respectively. Our results on standard benchmarks show that state-of-the-art neural models can give accuracies comparable to the best discrete models in the literature for most tasks and combing discrete and neural features unanimously yield better results.", "creator": "LaTeX with hyperref package"}}}