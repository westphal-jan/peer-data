{"id": "1708.07280", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Learning Generalized Reactive Policies using Deep Neural Networks", "abstract": "We consider the problem of learning for planning, where knowledge acquired while planning is reused to plan faster in new problem instances. For robotic tasks, among others, plan execution can be captured as a sequence of visual images. For such domains, we propose to use deep neural networks in learning for planning, based on learning a reactive policy that imitates execution traces produced by a planner. We investigate architectural properties of deep networks that are suitable for learning long-horizon planning behavior, and explore how to learn, in addition to the policy, a heuristic function that can be used with classical planners or search algorithms such as A*. Our results on the challenging Sokoban domain show that, with a suitable network design, complex decision making policies and powerful heuristic functions can be learned through imitation.", "histories": [["v1", "Thu, 24 Aug 2017 05:24:36 GMT  (1113kb,D)", "http://arxiv.org/abs/1708.07280v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["edward groshev", "aviv tamar", "siddharth srivastava", "pieter abbeel"], "accepted": false, "id": "1708.07280"}, "pdf": {"name": "1708.07280.pdf", "metadata": {"source": "CRF", "title": "Learning Generalized Reactive Policies using Deep Neural Networks", "authors": ["Edward Groshev"], "emails": ["eddiegroshev@berkeley.edu", "avivt@berkeley.edu", "siddharths@asu.edu", "pabbeel@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to survive on our own, in the way that they do it: in the way that they do it, in the way that they do it, in the way that they do it, in the way that they do it, in the way that they do it."}, {"heading": "1.1 Related Work", "text": "In fact, most of them will be able to move in a direction in which they are able to move, in which they are able to move."}, {"heading": "2 Background", "text": "In this section we present our formulation and preliminaries.Planning: We focus on fully observable, deterministic task planning problems described in the formal language PDDL [7]. Such planning problems are defined as a tuple that describes the relationships between entities: ObjAt (obj3, loc1), InGrip-per (obj2), etc. I: a set of entities in the domain (e.g. individual objects or places). I: a set of binary fluents that describe relations between entities: ObjAt (obj3, loc1), InGrip-per (obj2), etc. I: a set of entities that are initially applicable. G: a set of fluents that characterize relations between entities: ObjAt (obj3, loc1), InGrip-per (obj2), etc. ICH: a set of entities that are initially an association of entities in the domain (ObjAt, loc1), InGrip-per (obj2), etc."}, {"heading": "3 Imitation Learning in Learning for Planning", "text": "We define a generalized reactive policy (GRP) as a function that maps < problem case, state > to action, thus generalizing the concept of a policy. In this work, we assume, similar to [27], that the problem instance and the state are given as image observation. Such representation is suitable for many robotic problems. In this section, we describe our approach to learning GRPs. Our approach consists of two phases: a phase of data generation and a phase of political training. Data generation: Given the training domains Dtrain, we generate a data set for imitating learning imitations. For each of these dtrains, we maintain a standard planner to generate a plan and the corresponding stage of development, and then add the observations and actions for imitation. In our experiments, we used the Fast-Forward (FF) planner to test the ability to imitate."}, {"heading": "4 Network Architecture", "text": "In this section, we present our DNN architecture for learning a GRP. In particular, we propose two design options to help you learn long-term planning behavior."}, {"heading": "4.1 Goal Based Policy for Data Bootstrapping", "text": "In the IL literature (e.g., [20, 21]), the policy is typically structured as an illustration of observational data on measures a = \u00b5 (o). For this policy to represent targeted behavior, as in the planning areas we are looking at, the observation o in each state must contain information on the target state sg. Remember that our training data consist of Ntrain trajectories composed of observation and action packages. This means that the number of training samples for a policy a = \u00b5 (o) is equal to the number of observation and action packages in the training data. Instead, we propose to structure the policy as an illustration of both current observation and goal observation to current measures a = \u00b5 (o, og). We refer to such a policy structure as goal-based policy. Our argument for such a structure is based on the following fact: Proposal 1: Planning problem s0 and goal state observation sg, let us decide for optimal observation."}, {"heading": "4.2 Network Structure", "text": "We propose a general structure for a network that can learn a GRP from visual execution histories. Our network is illustrated in Figure 2. Current status and destination state observations are guided through several layers of folds that are shared between the action network and the plan length prediction network. However, there are also skip links from the input layer to each folding layer. The common representation is motivated by the fact that both the actions and the overall plan length are integral components of a plan. Knowing the actions is easy to determine the plan length and vice versa, knowledge of the plan length can serve as a template for determining the actions. The skip links are motivated by the fact that multiple planning algorithms can be applied as repeated compilations based on the planning domain to a latent target size. For example, greedy search extends the current node based on the possible next states encoded in the domain."}, {"heading": "4.3 Generalization to Different Problem Sizes", "text": "A primary challenge in learning for planning is to find representations that can be generalized across different problem sizes. For example, we expect that a good strategy for Sokoban should work well both on the instances on which she was trained, 9 x 9 domains, and on larger instances, such as 12 x 12 domains. While the folding layers can be applied to any image size, the number of inputs to the fully connected level is strictly tied to the image size, meaning that the network architecture described above is set to a specific domain size. To eliminate this dependency, we apply a trick that is used in fully evolutionary networks [14], retaining only a k \u00b7 k window of the last folding layer centered around the current position of the agent."}, {"heading": "5 Experiments", "text": "Our goal is to answer the following questions: 1. What makes a good DNN architecture for learning planning behavior? 2. Is the DNN plan length prediction a useful heuristic planning? 3. Can DNN-based strategies and heuristics generalize to changes in the size of the domain? We consider the Sokoban domain, as described in Figure 1, with a 9 \u00d7 9 grid and two difficulty levels: moving a single object, and a more difficult task of moving two objects. We have added training data with a random level generator 5 for Sokoban. Note that in Sokoban, the last observation in an orbit contains the final position of the agent, which reveals information about the plan. Since our networks are a target observation as input, we have added an additional observation at the end of each orbit, which is removed with the agent so that we can move into a target observation without additional information."}, {"heading": "5.1 Evaluating Network Structure", "text": "In this section, based on several ablation experiments, we attempt to identify the key ingredients for a successful CFP. In Figure 3, we present the success rate on two-object sokoban, for different network depths and with or without skip connections. Results suggest that deeper networks perform better, with skip connections leading to a consistent advantage. To further support this claim, we compare deep networks with flat and wide networks that have the same number of parameters in Table 1. Improved results for the deeper networks suggest that for learning the planning logic based on this data - the deeper the network the better - a similar observation is made in connection with a DNN representation of the planning algorithm for value appreciation in [27]. However, in our experiments, we attribute this to the general difficulty of training deep DNNs based on the gradient propagation that is undetectable in the failure of the formation of the 14-layer architecture."}, {"heading": "5.2 Evaluating Bootstrap Performance", "text": "Here we evaluate the bootstrapping approach of Section 4.1. In Table 2, we show the success rate and the plan length prediction error for architectures with and without bootstrapping. As can be seen, bootstrapping led to better use of the data and improved results. We also examined the performance of bootstrapping in terms of the size of the training data set. We observed that for smaller data sets, a non-uniform sampling strategy for the bootstrapping data performed better. We randomly assigned an observation from a distribution that increases linearly over time, so that observations near the target are more likely. We also found that in Sokoban, all the target observations in the test set placed the objects at target positions. Thus, bootstrapped target observations where the objects are not at a target position should be visually different from the test data and therefore less effective for learning."}, {"heading": "5.3 Evaluating GRP Performance", "text": "The learned GRP in the best architecture (14 layers, with bootstrapping and joint representation) can solve Sokoban with an object with 97% success rate and Sokoban with two objects with 87% success rate. In Figure 1, we map a trajectory that predicted politics in a challenging single-object area. The trajectories of two objects are more difficult to visualize in images, and we offer a video demonstration at https: / / sites.google.com / site / learn2plannips /. We observed that politics has learned to predict measures that avoid dead ends that happen far in the future, as Figure 1 shows."}, {"heading": "5.4 DNN as a Heuristic Generator", "text": "In terms of computing speed, DNN lead time is usually faster than a planner. However, the policy with a non-zero probability will not accomplish the task. In this section, we show that the plan length predicted by DNN can also be used as a heuristic within standard planners. 6We note that this procedure requires a change in observations that limits our approach in domains where such a change is feasible. In Sokoban, making this change is straightforward, and we believe that the same should apply to many other domains."}, {"heading": "5.5 DNN Heuristic Generalization", "text": "In Table 4 and Figure 5, we evaluated a DNN heuristics that was trained on 9 x 9 domains and evaluated on larger domains. In training, we chose the window size k = 1 to influence learning of a domain invariant policy.7 Note that Manhattan heuristics are only permitted in the case of an object. We also tried other heuristics, such as Euclidean distance, Hamiltonian distance, and Max over the three. Hamiltonian distance took a long time to calculate. Overall, the Manhattan distance performed best."}, {"heading": "5.6 Analysis of Failure Modes", "text": "In this section, we examine the failures of the learned CSF. We noticed that there were two primary failures, the first is due to cycles in politics and is a consequence of the use of a deterministic policy. For example, if the agent is between two objects, a deterministic policy can oscillate and move back and forth between the two. We found that a stochastic policy significantly reduces these kinds of failures. However, stochastic policies have a non-zero probability of taking action that leads to a dead end (e.g. pushing the box directly against a wall), which can lead to different errors. The second failure mode was the inability of our policy to foresee long-term dependencies between the two objects. An example of such a case is shown in Figure 6 (f-h), where deciding which object to move first requires a foresight of more than 20 steps. An explanation for this failure data is that such scenarios do not occur frequently in training scenarios."}, {"heading": "5.7 Comparison to Value Iteration Networks", "text": "Value estimation networks (VINs; [27]) are DNNs that are able to perform value estimation planning. While value estimation can easily be applied to small state problems such as 2D navigation, as shown in [27], the state space in Sokoban is much larger because it involves the agent's interaction with the moving objects and not just the obstacles. At this point, we show that VINs actually cannot solve this area. We first trained a VIN on Sokoban without moving objects, which corresponds to a navigation task. As expected, the VIN learned a successful policy with a success rate of 97.5%. However, the VIN performance fell to 53.5% for 1 object Sokoban and 8.5% for 2 objects. This shows that VINs (at least as implemented in [27]) are not suitable for learning complex planning behavior in Sokoban."}, {"heading": "6 Conclusion", "text": "In this paper, we explored a simple but effective approach to learning for planning, based on imitating learning from the visual performance of a planner. We used deep neural networks to learn policy, and proposed several network designs that enhance learning performance in this environment. Furthermore, we proposed networks that can be used to learn off-the-shelf heuristics for planners, which resulted in significant improvements over standard heuristics that do not effectively utilize learning. Our results on the challenging Sokoban domain suggest that DNNs are capable of extracting powerful features from observations, and the potential to learn the kind of \"visual thinking\" that makes some planning problems easy for humans but very difficult for automatic planners."}], "references": [{"title": "Hindsight experience replay", "author": ["Marcin Andrychowicz", "Filip Wolski", "Alex Ray", "Jonas Schneider", "Rachel Fong", "Peter Welinder", "Bob McGrew", "Josh Tobin", "Pieter Abbeel", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1707.01495,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "The computational complexity of propositional strips planning", "author": ["Tom Bylander"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Learning combinatorial optimization algorithms over graphs", "author": ["Hanjun Dai", "Elias B Khalil", "Yuyu Zhang", "Bistra Dilkina", "Le Song"], "venue": "arXiv preprint arXiv:1704.01665,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "One-shot imitation learning", "author": ["Yan Duan", "Marcin Andrychowicz", "Bradly Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1703.07326,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "The first learning track of the international planning competition", "author": ["Alan Fern", "Roni Khardon", "Prasad Tadepalli"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Learning and executing generalized robot plans", "author": ["Richard E Fikes", "Peter E Hart", "Nils J Nilsson"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1972}, {"title": "PDDL2. 1: An extension to PDDL for expressing temporal planning domains", "author": ["Maria Fox", "Derek Long"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "The fast downward planning system", "author": ["Malte Helmert"], "venue": "Journal of Artificial Intelligence (JAIR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "FF: The fast-forward planning system", "author": ["J\u00f6rg Hoffman"], "venue": "AI Magazine,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Sokoban: Enhancing general single-agent search methods using domain knowledge", "author": ["Andreas Junghanns", "Jonathan Schaeffer"], "venue": "Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Learning action strategies for planning domains", "author": ["Roni Khardon"], "venue": "Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning generalized policies in planning using concept languages", "author": ["Mario Martin", "Hector Geffner"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["Katharina M\u00fclling", "Jens Kober", "Oliver Kroemer", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Combining self-supervised learning and imitation for vision-based rope manipulation", "author": ["Ashvin Nair", "Dian Chen", "Pulkit Agrawal", "Phillip Isola", "Pieter Abbeel", "Jitendra Malik", "Sergey Levine"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2018}, {"title": "From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots", "author": ["Mark Pfeiffer", "Michael Schaeuble", "Juan Nieto", "Roland Siegwart", "Cesar Cadena"], "venue": "arXiv preprint arXiv:1609.07910,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Dean A Pomerleau"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1989}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell"], "venue": "In AISTATS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "The predictron: End-toend learning and planning", "author": ["David Silver", "Hado van Hasselt", "Matteo Hessel", "Tom Schaul", "Arthur Guez", "Tim Harley", "Gabriel Dulac-Arnold", "David Reichert", "Neil Rabinowitz", "Andre Barreto"], "venue": "arXiv preprint arXiv:1612.08810,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Directed search for generalized plans using classical planners", "author": ["Siddharth Srivastava", "Neil Immerman", "Shlomo Zilberstein", "Tianjiao Zhang"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Value iteration networks", "author": ["Aviv Tamar", "Sergey Levine", "Pieter Abbeel", "YI WU", "Garrett Thomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Procedural generation of sokoban levels", "author": ["Joshua Taylor", "Ian Parberry"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Imagination-augmented agents for deep reinforcement learning", "author": ["Th\u00e9ophane Weber", "S\u00e9bastien Racani\u00e8re", "David P Reichert", "Lars Buesing", "Arthur Guez", "Danilo Jimenez Rezende", "Adria Puigdom\u00e8nech Badia", "Oriol Vinyals", "Nicolas Heess", "Yujia Li"], "venue": "arXiv preprint arXiv:1707.06203,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Inductive policy selection for first-order MDPs", "author": ["SungWook Yoon", "Alan Fern", "Robert Givan"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Learning control knowledge for forward search planning", "author": ["Sungwook Yoon", "Alan Fern", "Robert Givan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "The plan existence problem for deterministic, fully observable environments is PSPACE-complete when expressed using rudimentary propositional representations [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "Such results have inspired the learning for planning paradigm: learning, or reusing the knowledge acquired while planning across multiple problem instances (in the form of triangle tables [6], learning control knowledge for planning [31], and constructing generalized plans [25], among other approaches) with the goal of faster plan computation in a new problem instance.", "startOffset": 188, "endOffset": 191}, {"referenceID": 29, "context": "Such results have inspired the learning for planning paradigm: learning, or reusing the knowledge acquired while planning across multiple problem instances (in the form of triangle tables [6], learning control knowledge for planning [31], and constructing generalized plans [25], among other approaches) with the goal of faster plan computation in a new problem instance.", "startOffset": 233, "endOffset": 237}, {"referenceID": 23, "context": "Such results have inspired the learning for planning paradigm: learning, or reusing the knowledge acquired while planning across multiple problem instances (in the form of triangle tables [6], learning control knowledge for planning [31], and constructing generalized plans [25], among other approaches) with the goal of faster plan computation in a new problem instance.", "startOffset": 274, "endOffset": 278}, {"referenceID": 10, "context": ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].", "startOffset": 2, "endOffset": 22}, {"referenceID": 14, "context": ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].", "startOffset": 2, "endOffset": 22}, {"referenceID": 28, "context": ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].", "startOffset": 2, "endOffset": 22}, {"referenceID": 29, "context": ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].", "startOffset": 2, "endOffset": 22}, {"referenceID": 23, "context": ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": "Recently, deep neural networks (DNNs) have been used to automatically extract expressive features from data, leading to state-of-the-art learning results in image classification [13], natural language processing [26], and control [16], among other domains.", "startOffset": 178, "endOffset": 182}, {"referenceID": 24, "context": "Recently, deep neural networks (DNNs) have been used to automatically extract expressive features from data, leading to state-of-the-art learning results in image classification [13], natural language processing [26], and control [16], among other domains.", "startOffset": 212, "endOffset": 216}, {"referenceID": 15, "context": "Recently, deep neural networks (DNNs) have been used to automatically extract expressive features from data, leading to state-of-the-art learning results in image classification [13], natural language processing [26], and control [16], among other domains.", "startOffset": 230, "endOffset": 234}, {"referenceID": 19, "context": "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].", "startOffset": 165, "endOffset": 177}, {"referenceID": 20, "context": "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].", "startOffset": 165, "endOffset": 177}, {"referenceID": 25, "context": "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].", "startOffset": 165, "endOffset": 177}, {"referenceID": 16, "context": "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].", "startOffset": 200, "endOffset": 208}, {"referenceID": 17, "context": "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].", "startOffset": 200, "endOffset": 208}, {"referenceID": 3, "context": "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].", "startOffset": 238, "endOffset": 241}, {"referenceID": 4, "context": "In particular, we focus on the Sokoban domain (see Figure 1), which has been described as the most challenging problem in the literature on learning for planning [5]: \u201c Sokoban has been demonstrated to be a very challenging domain for AI search and planning algorithms, even when significant human domain knowledge is provided [10].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "In particular, we focus on the Sokoban domain (see Figure 1), which has been described as the most challenging problem in the literature on learning for planning [5]: \u201c Sokoban has been demonstrated to be a very challenging domain for AI search and planning algorithms, even when significant human domain knowledge is provided [10].", "startOffset": 327, "endOffset": 331}, {"referenceID": 4, "context": "Within the learning for planning literature [5], several studies considered learning a reactive policy, which is similar to our imitation learning approach.", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "The works of Khadron [11], Martin and Geffner [15], and Yoon et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "The works of Khadron [11], Martin and Geffner [15], and Yoon et al.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "[30] learn policies represented as decision lists on the logical problem representation, which needs to be hand specified.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].", "startOffset": 133, "endOffset": 141}, {"referenceID": 20, "context": "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].", "startOffset": 133, "endOffset": 141}, {"referenceID": 16, "context": "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].", "startOffset": 188, "endOffset": 196}, {"referenceID": 17, "context": "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].", "startOffset": 188, "endOffset": 196}, {"referenceID": 18, "context": "[19] recently applied IL to learning obstacle avoidance from a motion planner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] demonstrated the difficulty of generalizing goal directed behavior from demonstration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] showed learning of various block stacking tasks where the goal was specified by an additional execution trace.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The \u2018one-shot\u2019 techniques in [4], however, are complimentary to this work.", "startOffset": 29, "endOffset": 32}, {"referenceID": 21, "context": "The impressive Alpha-Go [23] program learned a DNN strategy for Go, using a combination of IL, and reinforcement learning through self-play.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "In [27], a value iteration planning computation was embedded within the network structure, and demonstrated successful learning on 2D gridworld navigation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "The predictron architecture [24] uses ideas from temporal difference learning to design a DNN for reward prediction.", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "[29] proposed a DNN architecture that combines model based planning with model free components for reinforcement learning, and demonstrated results on the Sokoban domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Planning: We focus on fully observable, deterministic task planning problems described in the formal language PDDL [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "A straightforward IL approach is behavioral cloning [20], in which standard supervised learning is used to learn \u03bc from the data.", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "In this work, similar to [27], we assume that the problem instance and state are given as an image observation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "In our experiments we used the Fast-Forward (FF) planner [9], though any other PDDL planner can be used instead.", "startOffset": 57, "endOffset": 60}, {"referenceID": 25, "context": "It is also possible to extend our work to graph representations using convolutions on graphs [27, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 2, "context": "It is also possible to extend our work to graph representations using convolutions on graphs [27, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 19, "context": ", [20, 21]), the policy is typically structured as a mapping from observation to action a = \u03bc(o).", "startOffset": 2, "endOffset": 10}, {"referenceID": 20, "context": ", [20, 21]), the policy is typically structured as a mapping from observation to action a = \u03bc(o).", "startOffset": 2, "endOffset": 10}, {"referenceID": 5, "context": "Proposition 1 underlies classical planning methods such as triangle tables [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 25, "context": "We note that in value iteration networks [27], similar skip connections were used in an explicit neural network implementation of value iteration.", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "To remove this dependency, we employ a trick used in fully convolutional networks [14], and keep only a k \u00d7 k window of the last convolution layer, centered around the current agent position.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "For imitation learning, we represent the policy with the DNNs described in Section 4 and optimize using Adam [12] (step size 0.", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "We note a related observation in the context of a DNN representation of the value iteration planning algorithm in [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "We also note that a related idea was recently suggested in the context of reinforcement learning [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "We also add a comparison to two state-of-the-art planners: Fast Forward (FF, [9]) and Fast Downward (FD, [8]).", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "We also add a comparison to two state-of-the-art planners: Fast Forward (FF, [9]) and Fast Downward (FD, [8]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 25, "context": "Value iteration networks (VINs; [27]) are DNNs designs that have the capacity to perform a value iteration planning computation.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "While value iteration can easily be applied to small state space problems such as 2D navigation, as shown in [27], the state space in Sokoban is much larger, as it involves the interaction of the agent with the movable objects, and not only the obstacles.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "This shows that VINs (at least as implemented in [27]) are not suitable for learning the complex planning behavior in Sokoban.", "startOffset": 49, "endOffset": 53}, {"referenceID": 2, "context": "While representations for images based on deep conv-nets have become standard, representations for other modalities such as graphs and logical expressions are not yet mature (although see [3] for recent developments).", "startOffset": 188, "endOffset": 191}], "year": 2017, "abstractText": "We consider the problem of learning for planning, where knowledge acquired while planning is reused to plan faster in new problem instances. For robotic tasks, among others, plan execution can be captured as a sequence of visual images. For such domains, we propose to use deep neural networks in learning for planning, based on learning a reactive policy that imitates execution traces produced by a planner. We investigate architectural properties of deep networks that are suitable for learning long-horizon planning behavior, and explore how to learn, in addition to the policy, a heuristic function that can be used with classical planners or search algorithms such as A\u2217. Our results on the challenging Sokoban domain show that, with a suitable network design, complex decision making policies and powerful heuristic functions can be learned through imitation. Videos available at https://sites.google.com/site/learn2plannips/.", "creator": "LaTeX with hyperref package"}}}