{"id": "1708.07303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Learning Grasping Interaction with Geometry-aware 3D Representations", "abstract": "Learning to interact with objects in the environment is a fundamental AI problem involving perception, motion planning, and control. However, learning representations of such interactions is very challenging due to a high dimensional state space, difficulty in collecting large-scale data, and many variations of an object's visual appearance (i.e. geometry, material, texture, and illumination). We argue that knowledge of 3D geometry is at the heart of grasping interactions and propose the notion of a geometry-aware learning agent. Our key idea is constraining and regularizing interaction learning through 3D geometry prediction. Specifically, we formulate the learning process of a geometry-aware agent as a two-step procedure: First, the agent learns to construct its geometry-aware representation of the scene from 2D sensory input via generative 3D shape modeling. Finally, it learns to predict grasping outcome with its built-in geometry-aware representation. The geometry-aware representation plays a key role in relating geometry and interaction via a novel learning-free depth projection layer. Our contributions are threefold: (1) we build a grasping dataset from demonstrations in virtual reality (VR) with rich sensory and interaction annotations; (2) we demonstrate that the learned geometry-aware representation results in a more robust grasping outcome prediction compared to a baseline model; and (3) we demonstrate the benefits of the learned geometry-aware representation in grasping planning.", "histories": [["v1", "Thu, 24 Aug 2017 08:09:04 GMT  (4226kb,D)", "http://arxiv.org/abs/1708.07303v1", "Deep Geometry-aware Grasping"], ["v2", "Fri, 25 Aug 2017 02:50:28 GMT  (4226kb,D)", "http://arxiv.org/abs/1708.07303v2", "Deep Geometry-aware Grasping"]], "COMMENTS": "Deep Geometry-aware Grasping", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG", "authors": ["xinchen yan", "mohi khansari", "yunfei bai", "jasmine hsu", "arkanath pathak", "abhinav gupta", "james davidson", "honglak lee"], "accepted": false, "id": "1708.07303"}, "pdf": {"name": "1708.07303.pdf", "metadata": {"source": "CRF", "title": "Learning grasping interaction with geometry-aware 3D representations", "authors": ["Xinchen Yan", "Mohi Khansari", "Yunfei Bai", "Jasmine Hsu", "Arkanath Pathak", "Abhinav Gupta", "James Davidson", "Honglak Lee"], "emails": ["xcyan@umich.edu,", "khansari@x.team", "yunfeibai@x.team", "honglak}@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "2 Related Work", "text": "A common approach to robotic sensing is to identify the optimal location of 2D objects (RGB or RGBD images) Saxena et al. [2008], Montesano and Lopes [2012], Lenz et al. [2015], Pinto and Gupta [2016]. Previous work Saxena et al. [2008], Montesano and Lopes [2012] investigated the planar sensing problem with visual features derived from 2D sensory impressions and adopted logistical regressions for adaptation to optimal sensing positions with visual features. Lenz et al. [2015] proposed a two-stage sensing pioneer pipeline (object detection and sensing of parts) with deep neural networks. Pinto and Gupta [2016] built a robotic system for learning large-scale objects and errors."}, {"heading": "3 Multi-objective framework with geometry-aware representation", "text": "In this section, we develop a two-stage learning framework that performs 3D shape predictions and captures the result forecast with geometry-specific representation. The ability to generate 3D object shapes (e.g. volumetric representation) from any given 2D sensory input is a very important feature of our geometry-specific agent. Specifically, geometry-specific representation (1) is a scene occupancy grid representation centered on the camera target in space frame and (2) is invariant on camera point and distance."}, {"heading": "3.1 Learning generative 3D geometry-aware representation from 2D sensory input", "text": "For simplicity, we also consider a 2D depth map D as the super signal for learning the object. If the basic truth is 3D volumetric representation V = 2016, we cannot always directly provide a functional mapping fV: I \u2192 V, which approaches the 3D object shape of 2D sensorially. [2016], we approach 3D shape learning in a weakly supervised manner without explicit 3D shape monitoring. [2016], an in-network projection layer is introduced for 3D shape learning of 2D masks (e.g. 2D silhouette of the object). However, a 2D silhouette is an insufficient monitoring signal (e.g. we consider the concave shape) in robotic grasping."}, {"heading": "3.2 Learning predictive grasping interaction with geometry-aware representation.", "text": "In this paper, we focus on modelling the pre-detection status as a fine-grained motion planning. Interaction is only considered a success if the action leads to a valid detection. Based on our formulation, the detection-success probability can be derived directly from the visual observation of the current state and proposed action a = [p, o]. Inspired by previous work Oh et al. [2015], Finn et al. [2016], Dosovitskiy and Koltun [2016], Pinto et al. [2016], where the results are high-order mappings of observations and actions, is a direct approach to predicting a functional prediction of vanilla: I \u00d7 a. We refer this model to a prediction of vanilla (interaction)."}, {"heading": "3.3 Deep geometry-aware encoder-decoder network", "text": "In order to implement the two components proposed in the previous sections, we are implementing a deep geometric prediction network with an encoder decoder network (see Figure 2). Our model consists of a mold prediction network and a capture prediction network. The mould prediction network has an encoder with a 2D convolutional form and a decoder with a 3D deconvolutional form, followed by a global projection layer. Our mold encoder network takes RGBD images with a resolution of 128 \u00d7 128 and corresponding 4 x 4 camera view matrices as input; the network and output of identity units as intermediate representation. Our form decoder is a deconvolutionary neural 3D network that outputs voxels with a resolution of 32 \u00d7 32 \u00d7 32. We implemented the projection layer (with camera view and projection matrix) that transforms the individual state data into the xels and the state matrix."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset collection", "text": "VR-Gripping-101. We collected Gripping demonstrations on seven categories of objects, comprising a total of 101 everyday objects. To collect Gripping demonstrations, we set up the HTC Vive system in Virtual Reality (VR) and randomly assign targets to five right-handers (three men and two women). A total of 1,597 human grips are demonstrated, with an average of 15 grips per object (with the lowest and highest number of grips at 7 and 39 for a plate or a wine glass). We randomly divide 101 objects into three sets (e.g. training, validation and testing) and ensure that each set covers the seven categories (70% for training, 10% for testing). For a detailed description of our data sets and collection protocols, please refer to the website of our project: http: / goo.gl / gNCywJ.Grasping Data Perturbation."}, {"heading": "4.2 Implementation details", "text": "We trained the model with the ADAM Optimizer with a learning rate of 10 \u2212 5 for 200K iterations and a size 4 mini-batch. As an ablation study, we added an additional view and static scene to the model as an additional input channel above the base model, but did not observe significant improvements. As mentioned above, we chose the two-step training method. First, we trained the shape prediction model (shape encoder and shape decoder) with the ADAM Optimizer with a learning rate of 10 \u2212 5 for 400K iterations and a size 4 mini-batch. In each batch, we stitched 4 random viewpoints as our multi-view training. We observed that this setting resulted in a more stable shape prediction performance compared to the single-view training. In addition, we used L1 loss for predicting foreground depth and L2 loss for the silhouette prediction with cost points per second = 0.5 D."}, {"heading": "4.3 Visualization: 3D shape prediction", "text": "In order to assess the quality of the generative prediction model, we made conclusions using the shape encoder and decoder network. In our evaluations, we used RGBD frames and the corresponding camera view matrix as input into the network. As shown in Figure 3 (a), our shape prediction model is able to generate fine-grained 3D voxels from a single view without explicitly providing 3D voxels as monitoring during the training. As shown in Figure 3 (b), our model shows appropriate generalization capability even when applied to novel object instances."}, {"heading": "4.4 Model evaluation: grasping outcome prediction", "text": "This year, we are talking about just under a million euros."}, {"heading": "5 Conclusions", "text": "In this paper, we examined the poignant interaction from the perspective of a geometry-conscious learning agent. We proposed an encoder decoder network that performs both form prediction and result forecasting with a learning-free OpenGL projection layer. Compared to the baseline, experimental results showed improved performance in predicting results thanks to generative form training. Guided by improved outcome forecasting, we achieved better planning through analysis-by-synthesis-grasp optimization. We demonstrated the benefits of geometry-aware representation in perception and motion planning. In the future, we will explore possibilities that robotic control performs with our geometry-conscious representation."}, {"heading": "Acknowledgments", "text": "We would like to thank Kurt Konolige, Erwin Coumans, Vincent Vanhoucke, Ethan Holly, Marek Fiser, Eric Jang, Jie Tan, Lajanugan Logeswaran, Ruben Villegas, the Google Brain Team and X for their help with the project."}], "references": [{"title": "Semantic grasping: planning task-specific stable robotic grasps", "author": ["H. Dang", "P.K. Allen"], "venue": "Autonomous Robots,", "citeRegEx": "Dang and Allen.,? \\Q2014\\E", "shortCiteRegEx": "Dang and Allen.", "year": 2014}, {"title": "Learning to act by predicting the future", "author": ["A. Dosovitskiy", "V. Koltun"], "venue": "arxiv preprint:", "citeRegEx": "Dosovitskiy and Koltun.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Koltun.", "year": 2016}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "The columbia grasp database", "author": ["C. Goldfeder", "M. Ciocarlie", "H. Dang", "P.K. Allen"], "venue": "In Robotics and Automation,", "citeRegEx": "Goldfeder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldfeder et al\\.", "year": 2009}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Deep learning a grasp function for grasping under gripper pose uncertainty", "author": ["E. Johns", "S. Leutenegger", "A.J. Davison"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Johns et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2016}, {"title": "Perceiving, learning, and exploiting object affordances for autonomous pile manipulation", "author": ["D. Katz", "A. Venkatraman", "M. Kazemi", "J.A. Bagnell", "A. Stentz"], "venue": "Autonomous Robots,", "citeRegEx": "Katz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Katz et al\\.", "year": 2014}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Lenz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2015}, {"title": "Opengrasp: A toolkit for robot grasping simulation", "author": ["B. Leon", "S. Ulbrich", "R. Diankov", "G. Puche", "M. Przybylski", "A. Morales", "T. Asfour", "S. Moisio", "J. Bohg", "J. Kuffner"], "venue": "SIMPAR,", "citeRegEx": "Leon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Leon et al\\.", "year": 2010}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "J. Ibarz", "D. Quillen"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Dexterous grasping under shape uncertainty", "author": ["M. Li", "K. Hang", "D. Kragic", "A. Billard"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards", "author": ["J. Mahler", "F.T. Pokorny", "B. Hou", "M. Roderick", "M. Laskey", "M. Aubry", "K. Kohlhoff", "T. Kr\u00f6ger", "J. Kuffner", "K. Goldberg"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Mahler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mahler et al\\.", "year": 2016}, {"title": "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp", "author": ["J. Mahler", "J. Liang", "S. Niyaz", "M. Laskey", "R. Doan", "X. Liu", "J.A. Ojea", "K. Goldberg"], "venue": "metrics. arxiv preprint:", "citeRegEx": "Mahler et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mahler et al\\.", "year": 2017}, {"title": "Active learning of visual descriptors for grasping using non-parametric smoothed beta distributions", "author": ["L. Montesano", "M. Lopes"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Montesano and Lopes.,? \\Q2012\\E", "shortCiteRegEx": "Montesano and Lopes.", "year": 2012}, {"title": "Category-based task specific grasping", "author": ["E. Nikandrova", "V. Kyrki"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Nikandrova and Kyrki.,? \\Q2015\\E", "shortCiteRegEx": "Nikandrova and Kyrki.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Pinto and Gupta.,? \\Q2016\\E", "shortCiteRegEx": "Pinto and Gupta.", "year": 2016}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y.-L. Park", "A. Gupta"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Pinto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2016}, {"title": "Unsupervised learning of 3d structure from images", "author": ["D.J. Rezende", "S.A. Eslami", "S. Mohamed", "P. Battaglia", "M. Jaderberg", "N. Heess"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning", "author": ["R. Rubinstein", "D. Kroese"], "venue": null, "citeRegEx": "Rubinstein and Kroese.,? \\Q2004\\E", "shortCiteRegEx": "Rubinstein and Kroese.", "year": 2004}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Saxena et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2008}, {"title": "Part-based grasp planning for familiar objects", "author": ["N. Vahrenkamp", "L. Westkamp", "N. Yamanobe", "E.E. Aksoy", "T. Asfour"], "venue": "In Humanoid Robots (Humanoids),", "citeRegEx": "Vahrenkamp et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vahrenkamp et al\\.", "year": 2016}, {"title": "Shape completion enabled robotic grasping", "author": ["J. Varley", "C. DeChant", "A. Richardson", "A. Nair", "J. Ruales", "P. Allen"], "venue": "arxiv preprint:", "citeRegEx": "Varley et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Varley et al\\.", "year": 2016}, {"title": "Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision", "author": ["X. Yan", "J. Yang", "E. Yumer", "Y. Guo", "H. Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Weakly-supervised disentangling with recurrent transformations for 3d view synthesis", "author": ["J. Yang", "S.E. Reed", "M.-H. Yang", "H. Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "To enable supervision with only 2D visual data, we propose a novel learning-free OpenGL projection layer similar to Yan et al. [2016], Rezende et al.", "startOffset": 116, "endOffset": 134}, {"referenceID": 18, "context": "[2016], Rezende et al. [2016]. The grasping outcome prediction network has a state encoder and an outcome predictor.", "startOffset": 8, "endOffset": 30}, {"referenceID": 18, "context": "A common approach for robotic grasping is to detect the optimal grasping location from 2D visual input (RGB or RGBD images) Saxena et al. [2008], Montesano and Lopes [2012], Lenz et al.", "startOffset": 124, "endOffset": 145}, {"referenceID": 12, "context": "[2008], Montesano and Lopes [2012], Lenz et al.", "startOffset": 8, "endOffset": 35}, {"referenceID": 7, "context": "[2008], Montesano and Lopes [2012], Lenz et al. [2015],", "startOffset": 36, "endOffset": 55}, {"referenceID": 4, "context": "[2008], Montesano and Lopes [2012] studied the planar grasping problem using visual features extracted from 2D sensory input and adopted logistic regression for fitting optimal grasping location with visual features.", "startOffset": 8, "endOffset": 35}, {"referenceID": 3, "context": "Lenz et al. [2015] proposed a two-step detection pipeline (object detection and grasping part detection) with deep neural networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Lenz et al. [2015] proposed a two-step detection pipeline (object detection and grasping part detection) with deep neural networks. Pinto and Gupta [2016] built a robotic system for learning grasping from large-scale real-world trial-and-error experiments.", "startOffset": 0, "endOffset": 155}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al.", "startOffset": 148, "endOffset": 172}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al.", "startOffset": 148, "endOffset": 192}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al.", "startOffset": 148, "endOffset": 213}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al.", "startOffset": 148, "endOffset": 235}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al.", "startOffset": 148, "endOffset": 253}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al.", "startOffset": 148, "endOffset": 279}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al.", "startOffset": 148, "endOffset": 425}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al.", "startOffset": 148, "endOffset": 445}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al. [2016] proposed a shape completion model that reconstructs the 3D occupancy grid for robotic grasping from partial observations, where ground-truth 3D occupancy grid is used during model training.", "startOffset": 148, "endOffset": 467}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al. [2016] proposed a shape completion model that reconstructs the 3D occupancy grid for robotic grasping from partial observations, where ground-truth 3D occupancy grid is used during model training. Li et al. [2016] investigated the hand pose estimation in robotic grasping by decoupling contact points and hand configuration with parametrized object shape.", "startOffset": 148, "endOffset": 674}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al. [2016] proposed a shape completion model that reconstructs the 3D occupancy grid for robotic grasping from partial observations, where ground-truth 3D occupancy grid is used during model training. Li et al. [2016] investigated the hand pose estimation in robotic grasping by decoupling contact points and hand configuration with parametrized object shape. Building upon the compositional aspect of everyday objects, Vahrenkamp et al. [2016] proposed a part-based model for robotic grasping that has better generalization to novel object.", "startOffset": 148, "endOffset": 901}, {"referenceID": 0, "context": "In addition to general robotic grasping, several recent work investigated the semantic or task-specific grasping Dang and Allen [2014], Katz et al.", "startOffset": 113, "endOffset": 135}, {"referenceID": 0, "context": "In addition to general robotic grasping, several recent work investigated the semantic or task-specific grasping Dang and Allen [2014], Katz et al. [2014], Nikandrova and Kyrki [2015].", "startOffset": 113, "endOffset": 155}, {"referenceID": 0, "context": "In addition to general robotic grasping, several recent work investigated the semantic or task-specific grasping Dang and Allen [2014], Katz et al. [2014], Nikandrova and Kyrki [2015]. In contrast to existing learning frameworks applied to robotic grasping, our approach features (1) an end-to-end deep learning framework for generative 3D shape modeling and predictive grasping interaction and (2) learning-free projection layer that links the 2D observations with 3D object shape.", "startOffset": 113, "endOffset": 184}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al.", "startOffset": 12, "endOffset": 34}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al. [2016], we tackle the 3D shape learning in a weakly supervised manner without explicit 3D shape supervision.", "startOffset": 12, "endOffset": 53}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al. [2016], we tackle the 3D shape learning in a weakly supervised manner without explicit 3D shape supervision. In Yan et al. [2016], an in-network projection layer is introduced for 3D shape learning from 2D masks (e.", "startOffset": 12, "endOffset": 176}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al. [2016], we tackle the 3D shape learning in a weakly supervised manner without explicit 3D shape supervision. In Yan et al. [2016], an in-network projection layer is introduced for 3D shape learning from 2D masks (e.g. 2D silhouette of object). However, a 2D silhouette is an insufficient supervision signal (e.g., consider the concave shape) in robotic grasping. Therefore, we also consider a 2D depth map D as the supervision signal for learning the object geometry.2 To find the correspondence between a 3D shape and 2D depth map, we introduce a learning-free projective operator similar to Yan et al. [2016] that implements the exact rendering procedure for 2D depth estimation from the 3D world.", "startOffset": 12, "endOffset": 657}, {"referenceID": 22, "context": "Similar to the transformer network proposed in Yan et al. [2016], Jaderberg et al.", "startOffset": 47, "endOffset": 65}, {"referenceID": 4, "context": "[2016], Jaderberg et al. [2015], our learning-free projection can be considered as: (1) performing dense sampling from input volume (in the 3D world frame) to output volume (in normalized device coordinates); and (2) flattening the 3D spatial output across one dimension.", "startOffset": 8, "endOffset": 32}, {"referenceID": 13, "context": "Inspired by previous work Oh et al. [2015], Finn et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 1, "context": "[2015], Finn et al. [2016], Dosovitskiy and Koltun [2016], Yang et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 1, "context": "[2016], Dosovitskiy and Koltun [2016], Yang et al.", "startOffset": 8, "endOffset": 38}, {"referenceID": 1, "context": "[2016], Dosovitskiy and Koltun [2016], Yang et al. [2015], Pinto et al.", "startOffset": 8, "endOffset": 58}, {"referenceID": 1, "context": "[2016], Dosovitskiy and Koltun [2016], Yang et al. [2015], Pinto et al. [2016], where outcomes are high-order mappings from observations and actions, a straight-forward approach is to fit a functional mapping f l vanilla : I \u00d7 a\u2192 l.", "startOffset": 8, "endOffset": 79}, {"referenceID": 18, "context": "For grasping optimization, we performed a simplified version of cross-entropy method (CEM) Rubinstein and Kroese [2004], Levine et al.", "startOffset": 91, "endOffset": 120}, {"referenceID": 9, "context": "For grasping optimization, we performed a simplified version of cross-entropy method (CEM) Rubinstein and Kroese [2004], Levine et al. [2016]. We initialized with a failure grasp in order to force the model to find better grasping location (e.", "startOffset": 121, "endOffset": 142}], "year": 2017, "abstractText": "Learning to interact with objects in the environment is a fundamental AI problem involving perception, motion planning, and control. However, learning representations of such interactions is very challenging due to a high dimensional state space, difficulty in collecting large-scale data, and many variations of an object\u2019s visual appearance (i.e. geometry, material, texture, and illumination). We argue that knowledge of 3D geometry is at the heart of grasping interactions and propose the notion of a geometry-aware learning agent. Our key idea is constraining and regularizing interaction learning through 3D geometry prediction. Specifically, we formulate the learning process of a geometry-aware agent as a two-step procedure: First, the agent learns to construct its geometry-aware representation of the scene from 2D sensory input via generative 3D shape modeling. Finally, it learns to predict grasping outcome with its built-in geometry-aware representation. The geometry-aware representation plays a key role in relating geometry and interaction via a novel learning-free depth projection layer. Our contributions are threefold: (1) we build a grasping dataset from demonstrations in virtual reality (VR) with rich sensory and interaction annotations; (2) we demonstrate that the learned geometry-aware representation results in a more robust grasping outcome prediction compared to a baseline model; and (3) we demonstrate the benefits of the learned geometry-aware representation in grasping planning.", "creator": "LaTeX with hyperref package"}}}