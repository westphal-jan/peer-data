{"id": "1708.07336", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Active Sampling of Pairs and Points for Large-scale Linear Bipartite Ranking", "abstract": "Bipartite ranking is a fundamental ranking problem that learns to order relevant instances ahead of irrelevant ones. The pair-wise approach for bi-partite ranking construct a quadratic number of pairs to solve the problem, which is infeasible for large-scale data sets. The point-wise approach, albeit more efficient, often results in inferior performance. That is, it is difficult to conduct bipartite ranking accurately and efficiently at the same time. In this paper, we develop a novel active sampling scheme within the pair-wise approach to conduct bipartite ranking efficiently. The scheme is inspired from active learning and can reach a competitive ranking performance while focusing only on a small subset of the many pairs during training. Moreover, we propose a general Combined Ranking and Classification (CRC) framework to accurately conduct bipartite ranking. The framework unifies point-wise and pair-wise approaches and is simply based on the idea of treating each instance point as a pseudo-pair. Experiments on 14 real-word large-scale data sets demonstrate that the proposed algorithm of Active Sampling within CRC, when coupled with a linear Support Vector Machine, usually outperforms state-of-the-art point-wise and pair-wise ranking approaches in terms of both accuracy and efficiency.", "histories": [["v1", "Thu, 24 Aug 2017 09:43:17 GMT  (147kb,D)", "http://arxiv.org/abs/1708.07336v1", "a shorter version was presented in ACML 2013"]], "COMMENTS": "a shorter version was presented in ACML 2013", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["wei-yuan shen", "hsuan-tien lin"], "accepted": false, "id": "1708.07336"}, "pdf": {"name": "1708.07336.pdf", "metadata": {"source": "CRF", "title": "Active Sampling of Pairs and Points for Large-scale Linear Bipartite Ranking", "authors": ["Wei-Yuan Shen", "Hsuan-Tien Lin"], "emails": [], "sections": [{"heading": null, "text": ""}, {"heading": "1 INTRODUCTION", "text": "In this context, it should be noted that this is a very complex matter, which is a very complex, complex and complex matter."}, {"heading": "2 SETUP AND RELATED WORKS", "text": "In a two-part ranking problem, we are given a training set D = {(xk, yk)} Nk = 1, in which each (xk, yk) is a training instance with the feature vector xk in an n-dimensional space. We assume that the instances (xk, yk) are pulled from an unknown ranking P to X \u00d7 {+ 1, \u2212 1}. Such a training set is of the same format as the training set in common binary classification problems. We assume that the instances (xk, yk) from an unknown ranking P to X \u00d7 {1, \u2212 1}. Bipartite ranking algorithms take D as input and learn a ranking function r: X \u2192 R, which maps a feature vector x to a real score (x). For each pair of two instances, we call the pair wrongly ordered after r iff the pair contains a positive instance (x, + 1) and a \u2212 x (1)."}, {"heading": "3 PROPOSED ALGORITHM", "text": "As discussed in the previous section, the pairwise approach (1) is not feasible due to the large number of pairs on large data sets. In this case, either a random subsampling of the pairs is required [31], or the less precise point-wise approach (2) is adopted as an approximate alternative [24]. Nevertheless, the better ranking performance of the pairwise approach compared to the point-wise approach suggests that some of the key pairs should contain more valuable information than the instance points. Next, we will design an algorithm that actively scans some key pairs during learning. We will first show that some suggested active sampling schemes can help better identify these key pairs than random sub-sampling. Then, we will discuss how to unify point-wise and pair-wise ranking approaches within the same framework."}, {"heading": "3.1 Pool-based Active Learning", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "3.2 Active Sampling", "text": "Following the philosophy of active sampling, we propose the active sampling scheme for selecting a smaller set of key pairs on the huge dpair training package (we call the Active Sampling scheme to highlight some differences to active learning).One particular difference is that RankSVM (1) only requires optimization with positive pairs. Then, the yij label of a pair is chosen as a constant 1 and therefore easy-to-use sampling scheme during active learning, while the label in the active pool remains unknown before the potentially costly query step and active learning both emphasize the use of as little labeled data as possible.The costly part of the active sampling scheme is based on education rather than querying.For active sampling, we consider B as a budget for pairs that can be used in training that play a similar role in queries in active learning. In short active sampling, B selects informative pairs for the solution to the optimizer problem (1)."}, {"heading": "3.3 Sampling Strategies", "text": "Next, we will discuss some possible sampling strategies that can be used in algorithm 1. It is the lowest method we have chosen to passively select a random sample within U1. For active sampling strategies, we define two metrics that estimate the (learning) strategies of an unchosen pair. Both metrics correspond to well-known criteria in pool-based active learning (3) = [1 \u2212 wTxij] = [1 \u2212 wTxij] + (4) The proximity of the metrics corresponds to one of the most popular criteria in pool-based active learning called uncertainty sampling [25]. It captures the uncertainty of the ranking function w on the unchosen pair. Intuitively, a low value of proximity means that the ranking function is difficult to distinguish."}, {"heading": "3.4 Combined Ranking and Classification", "text": "Next, we will examine how to take these points into account during the active sampling phase. We start by taking a closer look at the similarity and differences between the point-wise SVM (2) and the pair-wise SVM (1). The pair-wise SVM looks at the weighted difference between the pairs SVM (1) and CRM (1). The pair-wise SVM looks at the weighted difference between the pairs XY and XJ, while the point-wise SVM looks at the weighted difference loss at points XK. Let's also consider a positive point (XK, + 1), its difference loss is [1 \u2212 wTxi] +, which is the same as [1 \u2212 wT (xi \u2212 0)]. In other words, the positive point (+ 1) can also be considered a pseudo-pair consisting of (xi, + 1) and (0, \u2212 1)."}, {"heading": "3.5 CRC with Threshold", "text": "In Theorem 1, we associate the point-by-point SVM without threshold (2) with the paired SVM function (1). However, the standard SVM formula for binary classification is often associated with a threshold concept to allow the grading of the hyperplane to be away from the original threshold. (6) Note that the standard SVM formula (1), [1 \u2212 wTxi], [1 \u2212 wTxi + 2p], [1 + wTxj \u2212 \u03b8], [1 + wTxj \u2212 \u03b8], [1 + wTxj \u2212 \u03b8], for each given (1 \u2212 wTw], [1 \u2212 wTxij] + \u2264 12 ([1 \u2212 2wTxi + 2p], [1 + 2wTxj], [1]. If we repeat the proof of Theorem 1 with the above equation, we get a similar theorem that connects the standard SVM with the pair-by SVM.7Theorem 2."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we examine the performance and efficiency of our proposed ASSR algorithm based on large real-world data sets. We compare the ASSR with the randomized SFB, which conducts random samples within the SFB. Furthermore, we compare the ASSR with three other state-of-the-art algorithms for large-scale bipartite ranking: the point-weighted linear SVM (2) (WSVM), an efficient implementation [22] of the paired linear Ranking SVM (1) (ERankSVM), and the combined ranking and regression (CRR) [31] algorithm for general ranking. We use 14 sets of data from the LIBSVM tools1 and the UCI repository [23] in the experiments. Table 1 shows the statistics of the data sets containing more than tens of thousands of instances and more than ten million pairs."}, {"heading": "4.1 Experiment Settings", "text": "Given a budget B for the number of pairs to be used in each algorithm and a global regularization parameter C, we set the instance weights for each algorithm to maintain the numerical scale between the regularization term and the loss terms to some extent. Global regularization parameter C is set to 0.1 in all experiments. In particular, the setting below ensures that the entire C (ij), summed up over all pairs (or pseudo-pairs), would be C \u00b7 B for all algorithms. \u2022 WSVM: As discussed in section 2, C + and C should be inversely proportional to N + and N in order to make weighted point-by-point SVM a reasonable basis for two-sided ranking."}, {"heading": "4.2 Performance Comparison and Robustness", "text": "Next, we examine the need for three important designs within the active sampling framework: soft version versus hard version, sampling bias correction within the soft version of active sampling, and the choice of soft version value functions. First, we use \u03b3 = 1 in ASCRC and Random CRC, which makes ASCRC synonymous with ASRankSVM. We allow b = 100 and B = 8000, which is a relatively small budget among the millions of pairs."}, {"heading": "4.2.1 Soft-Version versus Hard-Version", "text": "Intuitively, the soft version is much faster than the hard version. Here, we first examine the performance difference between the two versions. In Table 2, we compare the soft and hard versions of proximity and correctness scanning below the 95% confidence level. In proximity scanning, the soft version performs better than the hard version at 9 data sets and ties at 3; in correctness scanning, the soft version performs better than the hard version at 12 data sets and ties at 1. The results justify that the soft version is a better choice in terms of AUC performance than the hard version. Figure 1 further shows how the AUC uncertainty grows as the sampling version grows for different versions of the sampling, along with the sampling of the base model, ERankSVM saturation. We see that hard correction sampling always leads to an insufficient problem, as the hard version can easily suffer from loss."}, {"heading": "4.2.2 Bias Correction for Soft Version Sampling", "text": "Next, we show the AUC difference between bias correction (see Section 3.3) and inaction in soft version sampling in Table 3. A positive difference indicates that bias correction leads to better performance. First, we see that the difference in bias correction is relatively small. In soft close sampling, bias correction is slightly worse in 12 data sets; in soft-correct sampling, bias correction is slightly better in 9 data sets."}, {"heading": "4.2.3 Value Functions for Soft Version Sampling", "text": "We show how the AUC sets change, while the number of active sampling steps of the ASRankSVM in Fig. 4. For WSVM, ERankSVM and CRR, we draw a horizontal line on the AUC, which is achieved using the entire training set. We also list the final AUC with the standard deviation of all algorithms in Fig. 4 and Table 4, we see that the soft-correct sampling sampling is generally the best. We also run the right-tail t test for soft-correctness against the others and list the results in Table 4 to show if the improvement in soft-correctsampling sampling is significant. First, we compare soft-correctsampling with random sampling sampling and discover that soft-correct- correct- correct- sampling works better on 10 sets of data and what shows relative to sampling 4."}, {"heading": "4.3 Efficiency Comparison", "text": "First, we study the efficiency of soft active samples by checking the average number of rejected samples before exceeding the probability threshold of the sample for rejection, which is measured against the size of L * in Fig. 5. The soft close strategy typically requires less than 10 rejected samples, while the soft close strategy generally requires an increasing number of rejected samples, because if the ranking performance improves during iterations, the probability threshold behind soft core budgets may be quite low. Results suggest that the soft close strategy is generally efficient, while the soft core strategy may be less efficient than | L * growth. Next, we list the CPU time consumed for all algorithms under 8,000 pairs of budget in Table 5, and the data sets are sorted by size."}, {"heading": "4.4 The Usefulness of Larger Budget", "text": "From previous experiments, we have shown that ASRankSVM with a budget of 8,000 pairs can perform better than other competitors with large datasets. Now, we are reviewing the performance of ASRankSVM with different budget sizes. In Figure 2, we show the AUC curves with much larger budgets on two datasets. Then, we find that the performance of ASRankSVM can be improved or maintained if the budget size increases. For example, in Dataset Protein, we can match the performance of WSVM with about 40,000 pairs and easily surpass it with about 80,000 pairs. However, in most datasets, we find that the increase of AUC curves by 10,000 pairs flattens and eventually converges as the budget increases. That is, increasing the budget in ASRankSVM leads to consistent but marginal improvements. Note that the potential problem of scanning loud pairs within the soft-correct sample is increased when using 12,000 = the larger budget size = the budget size can be used."}, {"heading": "4.5 The Usefulness of the CRC Framework", "text": "Next, we examine the necessity of the CRC framework by comparing the performance of soft-closeness and 11soft-correctness among different decisions of \u03b3. We report on the best \u03b3 below a 95% level of significance within a uniform level of {uniform, 0.1, 0.2,..., 1.0}, where uniform means balancing the influence of actual pairs and pseudo-pairs by \u03b3 = | Dpair | | D \u043c |. Furthermore, we examine whether the CRC threshold can be useful. Table 6 shows the best \u03b3 and formulations for each sample strategy. Entries with \"-th\" indicate CRC threshold. The bold entries indicate that the setting ERankSVM significantly exceeds. The table suggests three observations. Firstly, the choice of the sample strategy does not have much influence on the optimal QE threshold, and most data sets have similar optimal QE for both soft-closeness and soft-clampling performance, which can sometimes justify a better addition of a CRM."}, {"heading": "5 CONCLUSION", "text": "We propose the Active Sampling (AS) algorithm under Combined Ranking and Classification (SFB) based on linear SVM. There are two main components of the proposed algorithm: The AS scheme selects valuable pairs for training and triggers the computing load in large-scale two-sided ranking. The SFB framework unifies the concept of point-by-point ranking and pair-by-pair ranking within the same framework and can perform better than the pure point-by-point ranking or pair-by-pair ranking. The uniform view of pairs and points (pseudo-pairs) in the SFB allows the use of an AS scheme to select from both types of pairs.Experiments on 14 large-scale real-world datasets show the promising performance and efficiency of the ASRank SVM and ASCRC algorithms. The algorithms generally exceed the state of the art in bipartite ranking algorithms, including the SVRM combined right-sided pair and SVRM combined right-sided pair."}], "references": [{"title": "An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity", "author": ["N. Ailon"], "venue": "JMLR, 13:137\u2013164,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "An efficient reduction of ranking to classification", "author": ["N. Ailon", "M. Mohri"], "venue": "arXiv preprint arXiv:0710.2889,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust reductions from ranking to classification", "author": ["M.-F. Balcan", "N. Bansal", "A. Beygelzimer", "D. Coppersmith", "J. Langford", "G.B. Sorkin"], "venue": "Machine learning, 72(1-2):139\u2013153,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Supervised learning of probability distributions by neural networks", "author": ["E.B. Baum", "F. Wilczek"], "venue": "NIPS, pages 52\u201361,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1988}, {"title": "AUC maximizing support vector learning", "author": ["U. Brefeld", "T. Scheffer"], "venue": "ICML Workshop on ROC Analysis in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to rank using gradient descent", "author": ["C. Burges", "T. Shaked", "E. Renshaw", "A. Lazier", "M. Deeds", "N. Hamilton", "G. Hullender"], "venue": "ICML, pages 89\u201396,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to rank with nonsmooth cost functions", "author": ["C.J. Burges", "Q.V. Le", "R. Ragno"], "venue": "NIPS, 19:193\u2013200,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "KDD-Cup 2004: results and analysis", "author": ["R. Caruana", "T. Joachims", "L. Backstrom"], "venue": "ACM SIGKDD Explorations Newsletter, 6(2):95\u2013108,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Ranking and empirical minimization of U-statistics", "author": ["S. Clemen\u00e7on", "G. Lugosi", "N. Vayatis"], "venue": "The Annals of Statistics, 36(2):844\u2013874,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "AUC optimization vs", "author": ["C. Cortes", "M. Mohri"], "venue": "error rate minimization. NIPS, 16(16):313\u2013320,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimizing estimated loss reduction for active sampling in rank learning", "author": ["P. Donmez", "J.G. Carbonell"], "venue": "ICML, pages 248\u2013255,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Active sampling for rank learning via optimizing the area under the ROC curve", "author": ["P. Donmez", "J.G. Carbonell"], "venue": "Advances in Information Retrieval, pages 78\u201389,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "On the consistency of ranking algorithms", "author": ["J. Duchi", "L. Mackey", "M.I. Jordan"], "venue": "ICML, pages 327\u2013334,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "On equivalence relationships between classification and ranking algorithms", "author": ["\u015e. Ertekin", "C. Rudin"], "venue": "JMLR, 12:2905\u20132929,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "JMLR, 9:1871\u2013 1874,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "An introduction to ROC analysis", "author": ["T. Fawcett"], "venue": "Pattern Recognition Letters, 27(8):861\u2013874,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "JMLR, 4:933\u2013969,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences, 55(1):119\u2013139,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Multilabel classification via calibrated label ranking", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "E.L. Menc\u0131\u0301a", "K. Brinker"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "NIPS, pages 115\u2013132,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["D.G. Horvitz", "D.J. Thompson"], "venue": "Journal of the American Statistical Association, 47(260):663\u2013685,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1952}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "KDD, pages 217\u2013226,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Bipartite ranking through minimization of univariate loss", "author": ["W. Kot\u0142lowski", "K. Dembczynski", "E. H\u00fcllermeier"], "venue": "ICML, pages 1113\u20131120,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A sequential algorithm for training text classifiers", "author": ["D.D. Lewis", "W.A. Gale"], "venue": "SIGIR, pages 3\u201312,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Active learning with hinted support vector machine", "author": ["C.-L. Li", "C.-S. Ferng", "H.-T. Lin"], "venue": "ACML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to rank for information retrieval", "author": ["T.-Y. Liu"], "venue": "Foundations and Trends in Information Retrieval, 3(3):225\u2013331,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Diverse active ranking for multimedia search", "author": ["S. Rajaram", "C.K. Dagli", "N. Petrovic", "T.S. Huang"], "venue": "CVPR, pages 1\u20138,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "ICML, pages 441\u2013448,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Margin-based ranking and an equivalence between AdaBoost and RankBoost", "author": ["C. Rudin", "R.E. Schapire"], "venue": "JMLR, 10:2193\u2013 2232,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Combined regression and ranking", "author": ["D. Sculley"], "venue": "KDD, pages 979\u2013988,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "U. of Wisconsin, Madison,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiple-instance active learning", "author": ["B. Settles", "M. Craven", "S. Ray"], "venue": "NIPS,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Active sampling of pairs and points for large-scale linear bipartite ranking", "author": ["W.-Y. Shen", "H.-T. Lin"], "venue": "ACML, pages 388\u2013403,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Hinge rank loss and the area under the ROC curve", "author": ["H. Steck"], "venue": "ECML, pages 347\u2013358,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "International Journal of Data Warehousing and Mining, 3(3):1\u201313,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "Springer,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1999}, {"title": "SVM selective sampling for ranking with application to data retrieval", "author": ["H. Yu"], "venue": "KDD, pages 354\u2013363,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Recent advances of large-scale linear classification", "author": ["G.-X. Yuan", "C.-H. Ho", "C.-J. Lin"], "venue": "Proceedings of the IEEE, 100(9):2584\u20132603,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "The performance of the ranking function is measured by the probability of mis-ordering an unseen pair of randomly chosen positive and negative instances, which is equal to one minus the Area Under the ROC Curve (AUC) [16], a popular criterion for evaluating the sensitivity and the specificity of binary classifiers in many real-world tasks [9] and large-scale data mining competitions [8], [38].", "startOffset": 217, "endOffset": 221}, {"referenceID": 8, "context": "The performance of the ranking function is measured by the probability of mis-ordering an unseen pair of randomly chosen positive and negative instances, which is equal to one minus the Area Under the ROC Curve (AUC) [16], a popular criterion for evaluating the sensitivity and the specificity of binary classifiers in many real-world tasks [9] and large-scale data mining competitions [8], [38].", "startOffset": 341, "endOffset": 344}, {"referenceID": 7, "context": "The performance of the ranking function is measured by the probability of mis-ordering an unseen pair of randomly chosen positive and negative instances, which is equal to one minus the Area Under the ROC Curve (AUC) [16], a popular criterion for evaluating the sensitivity and the specificity of binary classifiers in many real-world tasks [9] and large-scale data mining competitions [8], [38].", "startOffset": 386, "endOffset": 389}, {"referenceID": 9, "context": "Given the many potential applications in information retrieval, bioinformatics, and recommendation systems, bipartite ranking has received much research attention in the past two decades [10], [14], [17], [22], [24], [27].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "Given the many potential applications in information retrieval, bioinformatics, and recommendation systems, bipartite ranking has received much research attention in the past two decades [10], [14], [17], [22], [24], [27].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "Given the many potential applications in information retrieval, bioinformatics, and recommendation systems, bipartite ranking has received much research attention in the past two decades [10], [14], [17], [22], [24], [27].", "startOffset": 199, "endOffset": 203}, {"referenceID": 21, "context": "Given the many potential applications in information retrieval, bioinformatics, and recommendation systems, bipartite ranking has received much research attention in the past two decades [10], [14], [17], [22], [24], [27].", "startOffset": 205, "endOffset": 209}, {"referenceID": 22, "context": "Given the many potential applications in information retrieval, bioinformatics, and recommendation systems, bipartite ranking has received much research attention in the past two decades [10], [14], [17], [22], [24], [27].", "startOffset": 211, "endOffset": 215}, {"referenceID": 25, "context": "Given the many potential applications in information retrieval, bioinformatics, and recommendation systems, bipartite ranking has received much research attention in the past two decades [10], [14], [17], [22], [24], [27].", "startOffset": 217, "endOffset": 221}, {"referenceID": 4, "context": "Many existing bipartite ranking algorithms explicitly or implicitly reduce the problem to binary classification to inherit the benefits from the well-developed methods in binary classification [5], [14], [17], [20], [24].", "startOffset": 193, "endOffset": 196}, {"referenceID": 13, "context": "Many existing bipartite ranking algorithms explicitly or implicitly reduce the problem to binary classification to inherit the benefits from the well-developed methods in binary classification [5], [14], [17], [20], [24].", "startOffset": 198, "endOffset": 202}, {"referenceID": 16, "context": "Many existing bipartite ranking algorithms explicitly or implicitly reduce the problem to binary classification to inherit the benefits from the well-developed methods in binary classification [5], [14], [17], [20], [24].", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "Many existing bipartite ranking algorithms explicitly or implicitly reduce the problem to binary classification to inherit the benefits from the well-developed methods in binary classification [5], [14], [17], [20], [24].", "startOffset": 210, "endOffset": 214}, {"referenceID": 22, "context": "Many existing bipartite ranking algorithms explicitly or implicitly reduce the problem to binary classification to inherit the benefits from the well-developed methods in binary classification [5], [14], [17], [20], [24].", "startOffset": 216, "endOffset": 220}, {"referenceID": 2, "context": "For example, [3] shows that a low-regret ranking function can indeed be formed by a low-regret binary classifier.", "startOffset": 13, "endOffset": 16}, {"referenceID": 19, "context": "The strong theoretical guarantee leads to promising experimental results in many state-ofthe-art bipartite ranking algorithms, such as RankSVM [20], RankBoost [17] and RankNet [6].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "The strong theoretical guarantee leads to promising experimental results in many state-ofthe-art bipartite ranking algorithms, such as RankSVM [20], RankBoost [17] and RankNet [6].", "startOffset": 159, "endOffset": 163}, {"referenceID": 5, "context": "The strong theoretical guarantee leads to promising experimental results in many state-ofthe-art bipartite ranking algorithms, such as RankSVM [20], RankBoost [17] and RankNet [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 16, "context": "The quadratic number of pairs with respect to N makes the pair-wise approach computationally infeasible for large-scale data sets in general, except in a few special algorithms like RankBoost [17] or the efficient linear RankSVM [22].", "startOffset": 192, "endOffset": 196}, {"referenceID": 21, "context": "The quadratic number of pairs with respect to N makes the pair-wise approach computationally infeasible for large-scale data sets in general, except in a few special algorithms like RankBoost [17] or the efficient linear RankSVM [22].", "startOffset": 229, "endOffset": 233}, {"referenceID": 22, "context": "RankBoost enjoys an efficient implementation by reducing the quadratic number of pair-wise terms in the objective function to a linear number of equivalent terms; efficient linear RankSVM transforms the pair-wise optimization formulation to an equivalent formulation that can be solved in subquadratic time complexity [24].", "startOffset": 318, "endOffset": 322}, {"referenceID": 16, "context": "In some special cases [17], [28], such as AdaBoost [18] and its pair-wise sibling RankBoost [17], the point-wise approach is shown to be equivalent to the corresponding pair-wise one [14], [30].", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "In some special cases [17], [28], such as AdaBoost [18] and its pair-wise sibling RankBoost [17], the point-wise approach is shown to be equivalent to the corresponding pair-wise one [14], [30].", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "In some special cases [17], [28], such as AdaBoost [18] and its pair-wise sibling RankBoost [17], the point-wise approach is shown to be equivalent to the corresponding pair-wise one [14], [30].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "In some special cases [17], [28], such as AdaBoost [18] and its pair-wise sibling RankBoost [17], the point-wise approach is shown to be equivalent to the corresponding pair-wise one [14], [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "In some special cases [17], [28], such as AdaBoost [18] and its pair-wise sibling RankBoost [17], the point-wise approach is shown to be equivalent to the corresponding pair-wise one [14], [30].", "startOffset": 183, "endOffset": 187}, {"referenceID": 28, "context": "In some special cases [17], [28], such as AdaBoost [18] and its pair-wise sibling RankBoost [17], the point-wise approach is shown to be equivalent to the corresponding pair-wise one [14], [30].", "startOffset": 189, "endOffset": 193}, {"referenceID": 22, "context": "In other cases, the point-wise approach often operates with an approximate objective function that involves only N terms [24].", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "For example, [24] shows that minimizing the exponential or the logistic loss function on the instance points decreases an upper bound on the number of mis-ordered pairs within the input data.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "We focus on using the linear Support Vector Machine (SVM) [37] given its recent advances for efficient large-scale learning [40].", "startOffset": 58, "endOffset": 62}, {"referenceID": 37, "context": "We focus on using the linear Support Vector Machine (SVM) [37] given its recent advances for efficient large-scale learning [40].", "startOffset": 124, "endOffset": 128}, {"referenceID": 35, "context": "We first show that the loss function behind the usual point-wise SVM [37] minimizes an upper bound on the loss function behind RankSVM, which suggests that the point-wise SVM could be an approximate bipartite ranking algorithm that enjoys efficiency.", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": "The active sampling scheme is inspired by active learning, another popular machine learning setup that aims to save the efforts of labeling [32].", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "In Section 4, we conduct experiments on 14 real-world large-scale data sets and compare the proposed algorithms (ASRankSVM and ASCRC) with several stateof-the-art bipartite ranking algorithms, including the pointwise linear SVM [15], the efficient linear RankSVM [22], and the Combined Ranking and Regression (CRR) algorithm [31] which is closely related to the CRC framework.", "startOffset": 228, "endOffset": 232}, {"referenceID": 21, "context": "In Section 4, we conduct experiments on 14 real-world large-scale data sets and compare the proposed algorithms (ASRankSVM and ASCRC) with several stateof-the-art bipartite ranking algorithms, including the pointwise linear SVM [15], the efficient linear RankSVM [22], and the Combined Ranking and Regression (CRR) algorithm [31] which is closely related to the CRC framework.", "startOffset": 263, "endOffset": 267}, {"referenceID": 29, "context": "In Section 4, we conduct experiments on 14 real-world large-scale data sets and compare the proposed algorithms (ASRankSVM and ASCRC) with several stateof-the-art bipartite ranking algorithms, including the pointwise linear SVM [15], the efficient linear RankSVM [22], and the Combined Ranking and Regression (CRR) algorithm [31] which is closely related to the CRC framework.", "startOffset": 325, "endOffset": 329}, {"referenceID": 32, "context": "A preliminary version of this paper appeared in the 5th Asian Conference on Machine Learning [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "The bipartite ranking loss LP (r) is closely related to the area under the ROC curve (AUC), which is commonly used to evaluate the sensitivity and the specificity of binary classifiers [5], [8], [9], [38].", "startOffset": 185, "endOffset": 188}, {"referenceID": 7, "context": "The bipartite ranking loss LP (r) is closely related to the area under the ROC curve (AUC), which is commonly used to evaluate the sensitivity and the specificity of binary classifiers [5], [8], [9], [38].", "startOffset": 190, "endOffset": 193}, {"referenceID": 8, "context": "The bipartite ranking loss LP (r) is closely related to the area under the ROC curve (AUC), which is commonly used to evaluate the sensitivity and the specificity of binary classifiers [5], [8], [9], [38].", "startOffset": 195, "endOffset": 198}, {"referenceID": 6, "context": "There are lots of recent studies on improving the accuracy [7], [13], [20] and efficiency [2], [17] of general ranking problems.", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "There are lots of recent studies on improving the accuracy [7], [13], [20] and efficiency [2], [17] of general ranking problems.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "There are lots of recent studies on improving the accuracy [7], [13], [20] and efficiency [2], [17] of general ranking problems.", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "There are lots of recent studies on improving the accuracy [7], [13], [20] and efficiency [2], [17] of general ranking problems.", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "There are lots of recent studies on improving the accuracy [7], [13], [20] and efficiency [2], [17] of general ranking problems.", "startOffset": 95, "endOffset": 99}, {"referenceID": 37, "context": "Motivated by the recent advances of linear models for efficient large-scale learning [40], we consider linear models for efficient large-scale bipartite ranking.", "startOffset": 85, "endOffset": 89}, {"referenceID": 35, "context": "In particular, we study the linear Support Vector Machine (SVM) [37] for bipartite ranking.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "The pair-wise approach corresponds to the famous RankSVM algorithm [20], which is originally designed for ranking with ordinal-scaled scores, but can be easily extended to general ranking with real-valued labels or restricted to bipartite ranking with binary labels.", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "The pair-wise linear SVM minimizes the hinge loss as a surrogate to the 0/1 loss on Dpair [35], and the 0/1 loss on Dpair is equivalent to LD(r), the empirical bipartite ranking loss of interest.", "startOffset": 90, "endOffset": 94}, {"referenceID": 4, "context": "RankSVM has reached promising bipartite ranking performance in the literature [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 21, "context": "In contrast with the na\u0131\u0308ve RankSVM, the efficient linear RankSVM [22] changes (1) to a more sophisticated and equivalent one with an exponential number of constraints, each corresponding to a particular linear combination of the pairs.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "(2) Such an approach comes with some theoretical justification [24].", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "When C+ = C\u2212, [5] shows that the pointwise approach (2) is inferior to the pair-wise approach (1) in performance.", "startOffset": 14, "endOffset": 17}, {"referenceID": 29, "context": "Then, either some random subsampling of the pairs are needed [31], or the less-accurate point-wise approach (2) is taken as the approximate alternative [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Then, either some random subsampling of the pairs are needed [31], or the less-accurate point-wise approach (2) is taken as the approximate alternative [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "The Combined Ranking and Regression approach [31] performs stochastic gradient descent on its objective function, which essentially selects within the huge number of pairs in a random manner; the efficient RankSVM [22] identifies the most-violated constraints during optimization, which corresponds to selecting the most valuable pairs from an optimization perspective.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "The Combined Ranking and Regression approach [31] performs stochastic gradient descent on its objective function, which essentially selects within the huge number of pairs in a random manner; the efficient RankSVM [22] identifies the most-violated constraints during optimization, which corresponds to selecting the most valuable pairs from an optimization perspective.", "startOffset": 214, "endOffset": 218}, {"referenceID": 30, "context": "One machine learning setup that works for a similar task is active learning [32], which iteratively select a small number of valuable instances for labeling (and training) while reaching similar performance to the approach that trains with all the instances fully labeled.", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "[1] avoids the quadratic number of pairs in the general ranking problem from an active learning perspective, and proves that selecting a subquadratic number of pairs is sufficient to obtain a ranking function that is close to the optimal ranking function trained by using all the pairs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "The algorithm is theoretical in nature, while many other promising active learning tools [25], [29], [32] have not been explored for selecting valuable pairs in large-scale bipartite ranking.", "startOffset": 89, "endOffset": 93}, {"referenceID": 27, "context": "The algorithm is theoretical in nature, while many other promising active learning tools [25], [29], [32] have not been explored for selecting valuable pairs in large-scale bipartite ranking.", "startOffset": 95, "endOffset": 99}, {"referenceID": 30, "context": "The algorithm is theoretical in nature, while many other promising active learning tools [25], [29], [32] have not been explored for selecting valuable pairs in large-scale bipartite ranking.", "startOffset": 101, "endOffset": 105}, {"referenceID": 30, "context": "We focus on the setup of poolbased active learning [32] because of its strong connection to our needs.", "startOffset": 51, "endOffset": 55}, {"referenceID": 30, "context": "Various selection criteria have been proposed to describe the value of an unlabeled instance [32], such as uncertainty sampling [25], expected error reduction [29], and expected model change [33].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "Various selection criteria have been proposed to describe the value of an unlabeled instance [32], such as uncertainty sampling [25], expected error reduction [29], and expected model change [33].", "startOffset": 128, "endOffset": 132}, {"referenceID": 27, "context": "Various selection criteria have been proposed to describe the value of an unlabeled instance [32], such as uncertainty sampling [25], expected error reduction [29], and expected model change [33].", "startOffset": 159, "endOffset": 163}, {"referenceID": 31, "context": "Various selection criteria have been proposed to describe the value of an unlabeled instance [32], such as uncertainty sampling [25], expected error reduction [29], and expected model change [33].", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "Moreover, there are several works that solve bipartite ranking under the active learning scenario [11], [12], [39].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Moreover, there are several works that solve bipartite ranking under the active learning scenario [11], [12], [39].", "startOffset": 104, "endOffset": 108}, {"referenceID": 36, "context": "Moreover, there are several works that solve bipartite ranking under the active learning scenario [11], [12], [39].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "For example, [11] selects points that reduce the ranking loss functions most from the unlabeled pool while [12] selects points that maximize the AUC in expectation.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "For example, [11] selects points that reduce the ranking loss functions most from the unlabeled pool while [12] selects points that maximize the AUC in expectation.", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "The closeness measure corresponds to one of the most popular criteria in pool-based active learning called uncertainty sampling [25].", "startOffset": 128, "endOffset": 132}, {"referenceID": 27, "context": "On the other hand, the correctness measure is related to another common criterion in pool-based active learning called expected error reduction [29].", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "Note that this exact correctness measure is only available within our active sampling scheme because we know the pair-label yij to always be 1 without loss of generality, while usual active learning algorithms do not know the exact measure before querying and hence have to estimate it [11], [12].", "startOffset": 286, "endOffset": 290}, {"referenceID": 11, "context": "Note that this exact correctness measure is only available within our active sampling scheme because we know the pair-label yij to always be 1 without loss of generality, while usual active learning algorithms do not know the exact measure before querying and hence have to estimate it [11], [12].", "startOffset": 292, "endOffset": 296}, {"referenceID": 31, "context": "Moreover, sampling the pair with lowest correctness value shall change w the most in general, which echoes another criterion in pool-based active learning called expected model change [33].", "startOffset": 184, "endOffset": 188}, {"referenceID": 10, "context": "Similar to other active learning algorithms [11], [12], computing the pairs that come with the lowest closeness or correctness values can be time consuming, as it requires at least evaluating the values of wxk for each instance (xk, yk) \u2208 D, and then computing the measures on the pairs along with some selection or sorting steps that may be of super-linear time complexity [22].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Similar to other active learning algorithms [11], [12], computing the pairs that come with the lowest closeness or correctness values can be time consuming, as it requires at least evaluating the values of wxk for each instance (xk, yk) \u2208 D, and then computing the measures on the pairs along with some selection or sorting steps that may be of super-linear time complexity [22].", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "Similar to other active learning algorithms [11], [12], computing the pairs that come with the lowest closeness or correctness values can be time consuming, as it requires at least evaluating the values of wxk for each instance (xk, yk) \u2208 D, and then computing the measures on the pairs along with some selection or sorting steps that may be of super-linear time complexity [22].", "startOffset": 374, "endOffset": 378}, {"referenceID": 0, "context": "The soft version of active sampling can be described as follows: we consider a rejection sampling step that samples a pair xij with probability pij based on a method random() that generates random numbers between [0, 1].", "startOffset": 213, "endOffset": 219}, {"referenceID": 3, "context": "Both value functions are in the shape of the sigmoid function, which is widely used to represent probabilities in logistic regression and neural networks [4].", "startOffset": 154, "endOffset": 157}, {"referenceID": 20, "context": "We take the idea of [21] to weight the sampled pair by the inverse of its probability of being sampled.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "By introducing a parameter \u03b3 \u2208 [0, 1] to control the relative importance between the real pairs and the pseudo-pairs, we propose the following novel formulation.", "startOffset": 31, "endOffset": 37}, {"referenceID": 29, "context": "The CRC framework is closely related to the algorithm of Combined Ranking and Regression (CRR) [31] for general ranking.", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": "On the other hand, the idea of combining pair-wise and point-wise approaches had been used in another machine learning setup, the multi-label classification problem [36].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "The algorithm of Calibrated Ranking by Pairwise Comparison [19] assumes a calibration label between relevant and irrelevant labels, and hence unifies the pair-wise and point-wise label learning for multi-label classification.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "To the best of our knowledge, while the CRR approach has reached promising performance in practice [31], the CRC formulation has not been seriously studied.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "In the variant, we take one common trick to include We use one trick (as taken by LIBLINEAR [15]) that includes \u03b8 in the regularization term to allow simpler design of optimization routines.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "point-wise weighted linear SVM (2) (WSVM), an efficient implementation [22] of the pair-wise linear RankSVM (1) (ERankSVM), and the combined ranking and regression (CRR) [31] algorithm for general ranking.", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "point-wise weighted linear SVM (2) (WSVM), an efficient implementation [22] of the pair-wise linear RankSVM (1) (ERankSVM), and the combined ranking and regression (CRR) [31] algorithm for general ranking.", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "We solve the weighted SVM by the LIBLINEAR [15] package with its extension on instance weights.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "\u2022 ERankSVM: We use the SVM [22] package to efficiently solve the linear RankSVM (1) with the", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "\u2022 CRR: We use the package sofia-ml [31] with the sgdsvm learner type, combined-ranking loop type and the default number of iterations that SGD takes to solve the problem.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "We solve the linearSVM within ASCRC by the LIBLINEAR [15] package with its extension on instance weights.", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "The saturation corresponds to a known problem of uncertainty sampling in active learning because of the restricted view of the non-perfect model used for sampling [26].", "startOffset": 163, "endOffset": 167}], "year": 2017, "abstractText": "Bipartite ranking is a fundamental ranking problem that learns to order relevant instances ahead of irrelevant ones. The pair-wise approach for bi-partite ranking construct a quadratic number of pairs to solve the problem, which is infeasible for large-scale data sets. The point-wise approach, albeit more efficient, often results in inferior performance. That is, it is difficult to conduct bipartite ranking accurately and efficiently at the same time. In this paper, we develop a novel active sampling scheme within the pair-wise approach to conduct bipartite ranking efficiently. The scheme is inspired from active learning and can reach a competitive ranking performance while focusing only on a small subset of the many pairs during training. Moreover, we propose a general Combined Ranking and Classification (CRC) framework to accurately conduct bipartite ranking. The framework unifies point-wise and pair-wise approaches and is simply based on the idea of treating each instance point as a pseudo-pair. Experiments on 14 real-word large-scale data sets demonstrate that the proposed algorithm of Active Sampling within CRC, when coupled with a linear Support Vector Machine, usually outperforms state-of-the-art point-wise and pair-wise ranking approaches in terms of both accuracy and efficiency.", "creator": "LaTeX with hyperref package"}}}