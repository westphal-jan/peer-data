{"id": "1708.07403", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "CloudScan - A configuration-free invoice analysis system using recurrent neural networks", "abstract": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788.", "histories": [["v1", "Thu, 24 Aug 2017 13:40:06 GMT  (467kb,D)", "http://arxiv.org/abs/1708.07403v1", "Presented at ICDAR 2017"]], "COMMENTS": "Presented at ICDAR 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rasmus berg palm", "ole winther", "florian laws"], "accepted": false, "id": "1708.07403"}, "pdf": {"name": "1708.07403.pdf", "metadata": {"source": "CRF", "title": "CloudScan - A configuration-free invoice analysis system using recurrent neural networks", "authors": ["Rasmus Berg Palm", "Ole Winther"], "emails": ["rapal@dtu.dk", "olwi@dtu.dk", "fla@tradeshift.com"], "sections": [{"heading": null, "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "II. RELATED WORK", "text": "The fact is that we will be able to put ourselves at the top, in the way that we have experienced in recent years."}, {"heading": "III. CLOUDSCAN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Overview", "text": "In fact, most of us are able to play by the rules we have set ourselves, \"he told Welt am Sonntag. He added,\" It's not the first time we have been able to change the rules, but it's the first time we have been able to change the rules. \""}, {"heading": "B. Extracting training data from end-user provided feedback", "text": "The UBL subsequently evolved from the UBL, which provided the user with the mentioned PDFs, to the mentioned errors in the UBL, which were separated from the UBL. Now, see that the user has corrected all the errors and the UBL audit goes back to his data collection. We will extract the training data from these validated UBL documents, even though they are subtracted from the PDFs. See that the user is intentionally wiped off the PDFs."}, {"heading": "IV. EXPERIMENTS", "text": "We conduct two experiments to test the expected performance on the next level, and 2) the most difficult task on the next level from an invisible template. These are two different measures of generalization. We assume that each sender has a unique template."}, {"heading": "A. Baseline", "text": "The baseline is the current production system that uses a logistic regression classifier to classify each N gram individually. To get some context, we link the characteristic vectors for the nearest N gram in the upper, lower, left, and right directions to the normal characteristic vectors. So, if the characteristic vector for one N gram had M entries, it would have 5M entries. All 5M characteristics are then mapped with the hash trick to a binary vector of size 222 [17]. To be precise, for each characteristic we concatenate the feature name and value, hash-it, take the rest in relation to the binary vector size and set this index in the binary vector to 1. The logistic regression classifier is trained for 10 epochs using stochastic gradient lineage, after which we see little improvement."}, {"heading": "B. LSTM model", "text": "In fact, most of us are able to go in search of a solution that they have got a grip on."}, {"heading": "V. RESULTS", "text": "This year is the highest in the history of the country."}, {"heading": "VI. DISCUSSION", "text": "We have outlined our objectives for CloudScan and described how it works. We have the ability of an LSTM to improve the context directly. We have conducted experiments to test our hypothesis and evaluate the performance of CloudScan on a large realistic dataset. We have confirmed our hypothesis and demonstrated that an oracular classifier is used. Unfortunately, it is difficult to directly compare other vendors as there are no large publicly available datasets. These numbers should be compared with an upper limit of F1 = 0.925 for an ideal system where an oracular classifier is used."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank A'ngel Diego Cun ado Alonso and Johannes Ule n for our fruitful discussions and their great work on CloudScan. This research was supported by NVIDIA Corporation with the donation of TITAN X GPUs, which is partly funded by the Innovation Fund Denmark (IFD) under the file number 5016-00101B."}], "references": [{"title": "The Myth of the Paperless Office", "author": ["A.J. Sellen", "R.H. Harper"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Results of a Study on Invoice- Reading Systems in Germany", "author": ["B. Klein", "S. Agne", "A. Dengel"], "venue": "Document Analysis Systems VI, ser. Lecture Notes in Computer Science, S. Marinai and A. R. Dengel, Eds. Springer Berlin Heidelberg, Sep. 2004, no. 3163, pp. 451\u2013462.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Intellix \u2013 End-User Trained Information Extraction for Document Archiving", "author": ["D. Schuster", "K. Muthmann", "D. Esser", "A. Schill", "M. Berger", "C. Weidling", "K. Aliyev", "A. Hofmeier"], "venue": "2013 12th International Conference on Document Analysis and Recognition, Aug. 2013, pp. 101\u2013105.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Field Extraction from Administrative Documents by Incremental Structural Templates", "author": ["M. Rusi\u00f1ol", "T. Benkhelfallah", "V.P. dAndecy"], "venue": "2013 12th International Conference on Document Analysis and Recognition, Aug. 2013, pp. 1100\u20131104.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "smartFIX: A Requirements-Driven System for Document Analysis and Understanding", "author": ["A. Dengel", "B. Klein"], "venue": "Proceedings of the 5th International Workshop on Document Analysis Systems V, ser. DAS \u201902. London, UK, UK: Springer-Verlag, 2002, pp. 433\u2013444.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Analysis and understanding of multi-class invoices", "author": ["F. Cesarini", "E. Francesconi", "M. Gori", "G. Soda"], "venue": "Document Analysis and Recognition, vol. 6, no. 2, pp. 102\u2013114, Oct. 2003. [Online]. Available: http://link.springer.com/article/10.1007/s10032-002-0084-6", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic Indexing of Scanned Documents - a Layout-based Approach", "author": ["D. Esser", "D. Schuster", "K. Muthmann", "M. Berger", "A. Schill"], "venue": "Document Recognition and Retrieval XIX (DRR), San Francisco, CA, USA, 2012. [Online]. Available: http://proceedings. spiedigitallibrary.org/proceeding.aspx?articleid=1284003", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic approach to printed document understanding", "author": ["E. Medvet", "A. Bartoli", "G. Davanzo"], "venue": "International Journal on Document Analysis and Recognition (IJDAR), vol. 14, no. 4, pp. 335\u2013347, Nov. 2010. [Online]. Available: http://link.springer.com/article/10.1007/ s10032-010-0137-1", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics, 2009, pp. 1003\u20131011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": "Lingvisticae Investigationes, vol. 30, no. 1, pp. 3\u201326, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigation of recurrentneural-network architectures and learning methods for spoken language understanding.", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "The hOCR Microformat for OCR Workflow and Results", "author": ["T. Breuel"], "venue": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), vol. 2, Sep. 2007, pp. 1063\u20131067.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "The Hungarian method for the assignment problem", "author": ["H.W. Kuhn"], "venue": "Naval Research Logistics Quarterly, vol. 2, no. 1-2, pp. 83\u201397, Mar. 1955. [Online]. Available: http://onlinelibrary.wiley.com/doi/10.1002/ nav.3800020109/abstract", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1955}, {"title": "Universal business language v2.0", "author": ["G.K. Holman"], "venue": "2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "MUC-5 Evaluation Metrics", "author": ["N. Chinchor", "B. Sundheim"], "venue": "Proceedings of the 5th Conference on Message Understanding, ser. MUC5 \u201993. Association for Computational Linguistics, 1993, pp. 69\u201378.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Feature Hashing for Large Scale Multitask Learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ser. ICML \u201909. New York, NY, USA: ACM, 2009, pp. 1113\u20131120. [Online]. Available: http://doi.acm.org/10.1145/1553374.1553516", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997. [Online]. Available: http://dx.doi.org/10.1162/neco.1997.9.8.1735", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Text Chunking Using Transformation-Based Learning", "author": ["L.A. Ramshaw", "M.P. Marcus"], "venue": "Proceedings of the Third ACL Workshop on Very Large Corpora, pp. 82\u201394, 1995.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, no. Feb, pp. 1137\u20131155, 2003. [Online]. Available: http: //www.jmlr.org/papers/v3/bengio03a.html", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1508.07909, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Y. Gal"], "venue": "arXiv:1512.05287 [stat], Dec. 2015, arXiv: 1512.05287. [Online]. Available: http://arxiv.org/abs/1512.05287", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980 [cs], Dec. 2014, arXiv: 1412.6980. [Online]. Available: http://arxiv.org/abs/1412.6980", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/ 1605.02688", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Lasagne: First release.", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S.K. S\u00f8nderby", "D. Nouri", "D. Maturana", "M. Thoma", "E. Battenberg", "J. Kelly", "J.D. Fauw", "M. Heilman", "D.M. d. Almeida", "B. McFee", "H. Weideman", "G. Tak\u00e1cs", "P. d. Rivaz", "J. Crall", "G. Sanders", "K. Rasul", "C. Liu", "G. French", "J. Degrave"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Invoices, orders, credit notes and similar business documents carry the information needed for trade to occur between companies and much of it is on paper or in semi-structured formats such as PDFs [1].", "startOffset": 198, "endOffset": 201}, {"referenceID": 1, "context": "This is a labor intensive and expensive process [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "Intellix [3], ITESOFT [4], smartFIX [5] and others [6], [7], [8].", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "Intellix [3], ITESOFT [4], smartFIX [5] and others [6], [7], [8].", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "Intellix [3], ITESOFT [4], smartFIX [5] and others [6], [7], [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Intellix [3], ITESOFT [4], smartFIX [5] and others [6], [7], [8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "Intellix [3], ITESOFT [4], smartFIX [5] and others [6], [7], [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Intellix [3], ITESOFT [4], smartFIX [5] and others [6], [7], [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "The most directly related works are Intellix [3] by DocuWare and the work by ITESOFT [4].", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "The most directly related works are Intellix [3] by DocuWare and the work by ITESOFT [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "smartFIX [5] uses manually configured rules for each template.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "[6] learns a database of keywords for each template and fall back to a global database of keywords.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] uses a database of absolute positions of fields for each template.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] uses a database of manually created (field, pattern, parser) triplets for each template, designs a probabilistic model for finding the most similar pattern in a template, and extracts the value with the associated parser.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Our automatic training data extraction is closely related to the idea of distant supervision [9] where relations are extracted from unstructured text automatically using heuristics.", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "See Nadeau and Sekine [10] for a survey of NER approaches.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "[11], who combine word and character level RNNs, and Conditional Random Fields (CRFs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] uses bi-directional RNNs and word embedding to achieve competitive results on the ATIS (Airline Travel Information Systems) benchmark dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The output of this step is a structured representation of words and lines in hOCR format [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "we use the Hungarian algorithm [14].", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "Builds a Universal Business Language (UBL) [15] invoice with the fields having the values of the found N-grams.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "Also, while not directly comparable, related work [3], [4], [6] also restricts evaluation to header fields.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "Also, while not directly comparable, related work [3], [4], [6] also restricts evaluation to header fields.", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": "Also, while not directly comparable, related work [3], [4], [6] also restricts evaluation to header fields.", "startOffset": 60, "endOffset": 63}, {"referenceID": 15, "context": "We use the MUC-5 definitions of recall, precision and F1, without partial matches [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "All 5M features are then mapped to a binary vector of size 2 using the hashing trick [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "[18] which is good at modeling long term dependencies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "com/c/tradeshift-text-classification scheme [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "We hash the text of the word into a binary vector of size 2 which is embedded in a trainable 500 dimensional distributed representation using an embedding layer [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "It is possible we could have gotten better results using more advanced techniques like byte pair encoding [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "Following Gal [22], we apply dropout on the recurrent units and on the word embedding using a dropout fraction of 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "The model is trained with the Adam optimizer [23] using minibatches of size 96 until the validation performance has not improved on the validation set for 5 epochs.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "The LSTM model was implemented in Theano [24] and Lasagne [25].", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "The LSTM model was implemented in Theano [24] and Lasagne [25].", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788.", "creator": "LaTeX with hyperref package"}}}