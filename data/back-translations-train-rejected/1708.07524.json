{"id": "1708.07524", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Supervised Speech Separation Based on Deep Learning: An Overview", "abstract": "Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multi-talker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.", "histories": [["v1", "Thu, 24 Aug 2017 18:51:50 GMT  (5143kb)", "http://arxiv.org/abs/1708.07524v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE cs.SD", "authors": ["deliang wang", "jitong chen"], "accepted": false, "id": "1708.07524"}, "pdf": {"name": "1708.07524.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["dwang@cse.ohio-state.edu).", "chen.2593@osu.edu)."], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "B. Target Binary Mask", "text": "Like IBM, the Target Binary Mask (TBM) categorizes all T-F units with a binary label. Unlike IBM, the TBM derives the label by comparing the target language energy in each T-F unit with a fixed disturbance: speech noise, which is a stationary signal that corresponds to the average of all speech signals. An example of the TBM is shown in Figure 1 (b). Target Binary Masking (IRM) also leads to a dramatic improvement in speech intelligibility in noise [83], and the TBM has been used as a training target [39] [93]. Ideal Ratio Mask Instead of a hard label on each T-F unit, the ideal quotient mask (IRM) can be considered as a soft version of the IBM filter [124] [106] [144] [71]: (,) & lt () < < < < < < < < < () < < () < () < () < () < () < () < ();"}, {"heading": "D. Spectral Magnitude Mask", "text": "The spectral size mask (SMM) (in [144] called FFT-MASK) is defined on the STFT quantities (short-term Fourier transformation) of clean speech and loud speech: SMM (,) = | (,) | (,) | (3), where (, | and, | represent spectral quantities of clean speech and loud speech, respectively. Unlike IRM, SMM is not limited by 1 upper limits. To obtain separate language, we apply SMM or its estimate to the spectral quantities of loud speech and resynthesize separate speech with phases of loud speech (or an estimate of clean speech phases). SMM can be considered a form of the IRM defined in the STFT range. Fig. 1 (e) illustrates the SMM."}, {"heading": "E. Phase-Sensitive Mask", "text": "The phase sensitive mask (PSM) extends the SMM by a phase measure [33]: PSM (,) = | (,) | (4), whereby the difference between the pure speech phase and the loud speech phase is called the T-F unit. Including the phase difference in the PSM results in a higher SNR and tends to lead to a better appreciation of clean speech than the SMM [33]. An example of the PSM is shown in Figure 1 (f)."}, {"heading": "F. Complex Ideal Ratio Mask", "text": "The complex ideal quotient mask (cIRM) is an ideal mask in the complex domain. Unlike the above-mentioned masks, it can perfectly reconstruct clean speech from pure speech [154]: The solution for mask components yields the following definition: I J I < + J < + J I J I < + J I J I < J I < + J I < + J I < + J < < < J < < (6) where I and J are real and imaginary components of the loud speech, respectively I J real and imaginary components of the clean speech J < + J J I J I < J I < J I < J I < J I < J I < J I < J I < J I < J I < <. \"The imaginary unit is denoted by\" i. \"Thus, the cIRM has a real component and an imaginary component that can be separated."}, {"heading": "G. Target Magnitude Spectrum", "text": "The target size spectrum (TMS) of clean speech or (, |) is a mapping-based training target [97] [161] [45] [162]. In this case, supervised learning aims to estimate the size spectrogram of clean speech from that of loud speech. Power spectrum or other forms of spectrum such as the mel spectrum can be used instead of the size spectrum, and protocol operation is usually used to compress the dynamic range and facilitate training. An estimated language size is then combined with a loud phase to produce the separated language form. Fig. 1 (g) shows an example of TMS."}, {"heading": "H. Gammatone Frequency Target Power Spectrum", "text": "Another closely related mapping-based target is the gamma-frequency target power spectrum (GF-TPS) [144]. Unlike TMS, which is defined on a spectrogram, this target is defined on a cochleagram based on a gamma-ray filter bank. Specifically, this target is defined as the performance of the cochleagram response to clean speech. An estimate of the GFTPS can be easily converted to the separate voice waveform by cochleagram inversion [140]. Figure 1 (d) illustrates this goal."}, {"heading": "I. Signal Approximation", "text": "This year is the highest in the history of the country."}, {"heading": "A. Speech Enhancement", "text": "This year, the time has come for us to be able to go to a place where we can go to a place where we can go to a place where we can go to a place where we can move."}, {"heading": "B. Generalization of Speech Enhancement Algorithms", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "C. Speech Dereverberation and Denoising", "text": "This year, as never before, it will be able to retaliate, to retaliate, \"he says.\" We, \"he says,\" are able to retaliate, \"he says.\" We, \"he says,\" are able to retaliate. \"We,\" he says, \"are able to retaliate.\" \"We,\" he says, \"are able to unite.\""}, {"heading": "D. Speaker Separation", "text": "In fact, the two of them are two people who are able to position themselves at the top of the group."}, {"heading": "VI. ARRAY SEPARATION ALGORITHMS", "text": "A series of microphones provides multiple monaural recordings containing information indicating the spatial origin of a sound source. In spatially separated sound sources, sensor array inputs can be used to locate sound sources and then extract the source from the target location or direction. Traditional approaches to spatial information-based source separation include beamforming, as mentioned in Section I, and independent component analysis [6] [72]. Sound localization and location-based grouping are classical topics in auditory perception and CASA [9] [12] [140]."}, {"heading": "A. Spatial Feature Extraction", "text": "This year, we have reached the point where we are able to live in a country where it is not a country, but a country where it is a country."}, {"heading": "B. Time-frequency Masking for Beamforming", "text": "In fact, it is the case that it is a matter of a way in which people are able to put themselves at the centre of attention. (...) In fact, it is the case that they are able to put themselves and others at the centre of attention. (...) In fact, it is the case that they are able to put themselves and others at the centre of attention. (...) It is the case that they are able to put themselves at the centre of attention. (...) It is as if they are able to put themselves at the centre of attention. (...) It is as if they are able to put themselves at the centre of attention. (...) It is as if they are able to put themselves at the centre of attention. (...)"}, {"heading": "A. Features vs. Learning Machines", "text": "As discussed in Section IV, traits are important for language separation, although one of the main attractions of deep learning is to acquire appropriate traits for a task rather than designing such traits. So, in the era of deep learning, is there a role for trait extraction? We believe that the answer is yes. So-called nofree lunch theorem [155] dictates that no learning algorithm, including DNN, performs superior performance in all tasks. Apart from theoretical arguments, trait extraction is a way to convey knowledge from a problem domain, and it stands to reason that it is useful to incorporate domain knowledge in this way (see [142] for a recent example). For example, CNN's success in visual pattern recognition is partly due to the use of common weights and (samples) layers in its architecture that help integrate a representational invariant into small variations of trait positions [8]."}, {"heading": "B. Time-frequency Domain vs. Time Domain", "text": "The vast majority of monitored language separation studies are conducted in the T-F domain, which is reflected in the various training objectives examined in Section III. Alternatively, language separation in the time domain can be performed without resorting to frequency representation. As explained in Section V.A., time mapping can potentially clean up both size and phase at once. End-to-end separation represents an emerging trend along with the use of CNNs and GANs.Some comments are in order. First, time mapping is a welcome addition to the list of monitored separation approaches and offers a unique perspective on phase improvement [86] [38]. Second, the same signal can be transformed back and forth between its time domain representation and its T-F domain representation. Therefore, it would be wrong to treat a T-F representation as a feature."}, {"heading": "C. What\u2019s the Target?", "text": "The definition of ideal masks assumes that the target source is known, which is often the case in speech separation applications. To improve speech, the speech signal is seen as a target, while non-speech signals are seen as interference signals. The situation becomes difficult when multiple speakers are separated. Generally, this is the question of auditory attention and intention. It is a complicated question, because even with the same input scene, the signal to be treated shifts from one moment to the next and does not have to be a speech signal. However, there are practical solutions. Directional hearing aids circumvent this problem by assuming that the target is in the direction of vision, i.e. benefits from vision [138] [28]. For separate sources, there are other reasonable alternatives to the target definition, such as the loudest source previously visited (i.e. tracking) or the most familiar (as in the case of multiple speakers)."}, {"heading": "D. What Does a Solution to the Cocktail Party Problem Look Like?", "text": "In this context, it should be noted that the solution to the problem is not a purely theoretical question, but a purely theoretical one."}], "references": [{"title": "Determination of the potential benefit of timefrequency gain manipulation", "author": ["M.C. Anzalone", "L. Calandruccio", "K.A. Doherty", "L.H. Carney"], "venue": "Ear Hear., vol. 27, pp. 480- 492, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Exploring multi-channel features for denoising-autoencoder-based speech enhancement", "author": ["S. Araki"], "venue": "Proceedings of ICASSP, pp. 116-120, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Study on the dereverberation of speech based on temporal envelope filtering", "author": ["C. Avendano", "H. Hermansky"], "venue": "Proceedings of ICSLP, pp. 889-892, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning spectral clustering, with application to speech separation", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 1963-2001, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1963}, {"title": "The third CHiME speech separation and recognition challenge: dataset, task and baselines", "author": ["J. Barker", "R. Marxer", "E. Vincent", "A. Watanabe"], "venue": "Proceedings of IEEE ASRU, pp. 5210-5214, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "An information-maximization approach to blind separation and blind deconvolution", "author": ["A.J. Bell", "T.J. Sejnowski"], "venue": "Neural Comp., vol. 7, pp. 1129-1159, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Microphone array signal processing", "author": ["J. Benesty", "J. Chen", "Y. Huang"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Large-scale kernel machines, L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, Ed., Cambridge M.A.: MIT Press, pp. 321-359, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Spatial Hearing: The psychophysics of human sound localization", "author": ["J. Blauert"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Suppression of acoustic noise in speech using spectral subtraction", "author": ["S.F. Boll"], "venue": "IEEE Trans. Acoust. Speech Sig. Process., vol. 27, pp. 113-120, 1979.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1979}, {"title": "Microphone arrays: Signal processing techniques and applications", "author": ["M.S. Brandstein", "D.B. Ward", "Ed"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Auditory scene analysis", "author": ["A.S. Bregman"], "venue": "Cambridge MA: MIT  20 Press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Isolating the energetic component of speech-on-speech masking with ideal time-frequency segregation", "author": ["D.S. Brungart", "P.S. Chang", "B.D. Simpson", "D.L. Wang"], "venue": "J. Acoust. Soc. Am., vol. 120, pp. 4007-4018, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "A phonemebased pre-training approach for deep neural network with application to speech enhancement", "author": ["S.E. Chazan", "S. Gannot", "J. Goldberger"], "venue": "Proceedings of IWAENC, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "MVA processing of speech features", "author": ["C. Chen", "J.A. Bilmes"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 15, pp. 257-270, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Long short-term memory for speaker generalization in supervised speech separation", "author": ["J. Chen", "D.L. Wang"], "venue": "Proceedings of Interspeech, pp. 3314-3318, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "DNN-based mask estimation for supervised speech separation", "author": ["J. Chen", "D.L. Wang"], "venue": "Audio source separation, S. Makino, Ed., Berlin: Springer, in press, 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory for speaker generalization in supervised speech separation", "author": ["J. Chen", "D.L. Wang"], "venue": "J. Acoust. Soc. Am., vol. 141, pp. 4705-4714, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "A feature study for classification-based speech separation at low signal-to-noise ratios", "author": ["J. Chen", "Y. Wang", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 22, pp. 1993-2002, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "Noise perturbation for supervised speech separation", "author": ["J. Chen", "Y. Wang", "D.L. Wang"], "venue": "Speech Comm., vol. 78, pp. 1- 10, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale training to increase speech intelligibility for hearing-imparied listeners in novel noises", "author": ["J. Chen", "Y. Wang", "S.E. Yoho", "D.L. Wang", "E.W. Healy"], "venue": "J. Acoust. Soc. Am., vol. 139, pp. 2604-2612, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Some experiments on the recognition of speech, with one and with two ears", "author": ["E.C. Cherry"], "venue": "J. Acoust. Soc. Am., vol. 25, pp. 975-979, 1953.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1953}, {"title": "On human communication", "author": ["E.C. Cherry"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1957}, {"title": "Multi-column deep neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Proceedings of CVPR, pp. 3642-3649, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Listening to speech in the presence of other sounds", "author": ["C.J. Darwin"], "venue": "Phil. Trans. Roy. Soc. B, vol. 363, pp. 1011-1021, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple F0 estimation", "author": ["A. de Cheveigne"], "venue": "Computational auditory scene analysis: Principles, algorithms, and Applications, D.L. Wang and G.J. Brown, Ed., Hoboken NJ: Wiley & IEEE Press, pp. 45-79, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Features for masking-based monaural speech separation in reverberant conditions", "author": ["M. Delfarah", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 25, pp. 1085-1094, 2017.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "The USTC-iFlyteck system for the CHiME4 challenge", "author": ["J. Du", "Y.-H. Tu", "J. Sun", "e. al."], "venue": "Proceedings of the CHiME-4 Workshop, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "A regression approach to single-channel speech separation via highresolution deep neural networks", "author": ["J. Du", "Y. Tu", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 24, pp. 1424-1437, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech separation of a target speaker based on deep neural networks", "author": ["J. Du", "Y. Tu", "Y. Xu", "L.-R. Dai", "C.-H. Lee"], "venue": "Proceedings of ICSP, pp. 65-68, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech enhancement using a minimum mean-square error short-time spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Trans. Acoust. Speech Sig. Process., vol. 32,  pp. 1109-1121, 1984.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1984}, {"title": "Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks", "author": ["H. Erdogan", "J. Hershey", "S. Watanabe", "J. Le Roux"], "venue": "Proceedings of ICASSP, pp. 708-712, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved MVDR beamforming using single-channel mask prediction networks", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe", "M. Mandel", "J.L. Roux"], "venue": "Proceedings of Interspeech, pp. 1981-1985, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1981}, {"title": "A regression approach to binaural speech segregation via deep neural network", "author": ["N. Fan", "J. Du", "L.-R. Dai"], "venue": "Proceedings of ISCSLP, pp. 116-120, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "An algorithm for linearly constrained adaptive array processing", "author": ["O.L. Frost"], "venue": "Proc. IEEE, vol. 60, pp. 926-935, 1972.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1972}, {"title": "Raw waveformbased speech enhancement by fully convolutional networks", "author": ["S.-W. Fu", "Y. Tsao", "X. Lu", "H. Kawai"], "venue": "arXiv:1703.02205v3, 2017.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2017}, {"title": "Phase processing for single-channel speech enhancement: History and recent advances", "author": ["T. Gerkmann", "M. Krawczyk-Becker", "J. Le Roux"], "venue": "IEEE Sig. Proc. Mag., vol. 32, pp. 55- 66, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Mask-based enhancement for very low quality speech", "author": ["S. Gonzalez", "M. Brookes"], "venue": "Proceedings of ICASSP, pp. 7029\u20137033, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow"], "venue": "Proceedings of NIPS, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves"], "venue": "IEEE Trans. Pattern Anal. Machine Intell., vol. 31, pp. 855\u2013868, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Spondee recognition in a two-talker and a speech-shaped noise masker in adults and children", "author": ["J.W. Hall", "J.H. Grose", "E. Buss", "M.B. Dev"], "venue": "Ear Hear., vol. 23, pp. 159-165, 2002.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "A classification based approach to speech separation", "author": ["K. Han", "D.L. Wang"], "venue": "J. Acoust. Soc. Am., vol. 132, pp. 3475- 3483, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural network based pitch tracking in very noisy speech", "author": ["K. Han", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 22, pp. 2158-2168, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spectral mapping for speech dereverebaration", "author": ["K. Han", "Y. Wang", "D.L. Wang"], "venue": "Proceedings of ICASSP, pp. 4661-4665, 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spectral mapping for speech dereverberation and denoising", "author": ["K. Han"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 23, pp. 982-992, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual improvement of deep neural networks for monaural speech enhancement", "author": ["W. Han"], "venue": "Proceedings of IWAENC, 2016.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "How we localize sounds", "author": ["W.M. Hartmann"], "venue": "Physics Today, 24-29, November 1999.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1999}, {"title": "Blind binary masking for reverberation suppression in cochlear implants", "author": ["O. Hazrati", "J. Lee", "P.C. Loizou"], "venue": "J. Acoust. Soc. Am., vol. 133, pp. 1607-1614, 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of CVPR, pp. 770\u2013 778, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "An algorithm to increase intelligibility for hearingimpaired listeners in the presence of a competing talker", "author": ["E.W. Healy", "M. Delfarah", "J.L. Vasko", "B.L. Carter", "D.L. Wang"], "venue": "J. Acoust. Soc. Am., vol. 141, pp. 4230-4239, 2017.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "An algorithm to improve speech recognition in noise for hearingimpaired listeners", "author": ["E.W. Healy", "S.E. Yoho", "Y. Wang", "D.L. Wang"], "venue": "J. Acoust. Soc. Am., vol. 134, pp. 3029- 3038, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "MMSE based noise PSD tracking with low complexity", "author": ["R.C. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "Proceedings of ICASSP, pp. 4266-4269, 2010.  21", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "J. Acoust. Soc. Am., vol. 87, pp. 1738-1752, 1990.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1990}, {"title": "RASTA processing of speech", "author": ["H. Hermansky", "N. Morgan"], "venue": "IEEE Trans. Speech Audio Proc., vol. 2, pp. 578- 589, 1994.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1994}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["J. Hershey", "Z. Chen", "J. Le Roux", "S. Watanabe"], "venue": "Proceedings of ICASSP, pp. 31-35, 2016.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "Introduction to the theory of neural computation", "author": ["H. Hertz", "A. Krogh", "R.G. Palmer"], "venue": "Redwood City, CA: Addison- Wesley,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1991}, {"title": "Neural network based spectral mask estimation for acoustic beamforming", "author": ["J. Heymann", "L. Drude", "R. Haeb-Umbach"], "venue": "Proceedings of ICASSP, pp. 196-200, 2016.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust MVDR beamforming using time-frequency masks for online/offline ASR in noise", "author": ["T. Higuchi", "N. Ito", "T. Yoshioka", "e. al."], "venue": "Proceedings of ICASSP, pp. 5210-5214, 2016.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton"], "venue": "IEEE Sig. Proc. Mag., 82- 97, November 2012.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comp., vol. 18, pp. 1527-1554, 2006.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comp., vol. 9, pp. 1735-1780, 1997.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech segregation based on pitch tracking and amplitude modulation", "author": ["G. Hu", "D.L. Wang"], "venue": "Proceedings of IEEE WASPAA, pp. 79-82, 2001.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2001}, {"title": "Monaural speech segregation based on pitch tracking and amplitude modulation", "author": ["G. Hu", "D.L. Wang"], "venue": "IEEE Trans. Neural Net., vol. 15, pp. 1135-1150, 2004.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2004}, {"title": "A tandem algorithm for pitch estimation and voiced speech segregation", "author": ["G. Hu", "D.L. Wang"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 18, pp. 2067-2079, 2010.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2010}, {"title": "An unsupervised approach to cochannel speech separation", "author": ["K. Hu", "D.L. Wang"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 21, pp. 120-129, 2013.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Pitch estimation in noisy speech using accumulated peak spectrum and sparse estimation technique", "author": ["F. Huang", "T. Lee"], "venue": "IEEE Trans. Speech Audio Proc., vol. 21, pp. 99\u2013109, 2013.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning for monaural speech separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "Proceedings of ICASSP, pp. 1581-1585, 2014.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint optimization of masks and deep recurrent neural networks for monaural source separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 23, pp. 2136-2147, 2015.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional maxout neural networks for speech separation", "author": ["L. Hui"], "venue": "Proceedings of ISSPIT, pp. 24-27, 2015.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2015}, {"title": "On the ideal ratio mask as the goal of computational auditory scene analysis", "author": ["C. Hummersone", "T. Stokes", "T. Brooks"], "venue": "Blind Source Separation, G.R. Naik and W. Wang, Ed., Berlin: Springer, pp. 349-368, 2014.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2014}, {"title": "Independent component analysis: Algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural Netw., vol. 13, pp. 411- 430, 2000.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2000}, {"title": "Phase autocorrelation (PAC) derived robust speech features", "author": ["S. Ikbal", "H. Misra", "H.A. Bourlard"], "venue": "Proceedings of ICASSP, pp. II.133-136, 2003.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2003}, {"title": "Naylor, Theory and applications of spherical microphone array processing", "author": ["D.P. Jarrett", "E. Habets", "P.A"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2016}, {"title": "Binaural  classification for reverberant speech segregation using deep neural networks", "author": ["Y. Jiang", "D.L. Wang", "R.S. Liu", "Z.M. Feng"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 22, pp. 2112-2121, 2014.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2014}, {"title": "A supervised learning approach to monaural segregation of reverberant speech", "author": ["Z. Jin", "D.L. Wang"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 17, pp. 625-638, 2009.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2009}, {"title": "Effect of masker type and age on speech intelligibility and spatial release from masking in children and adults", "author": ["P.M. Johnstone", "R.Y. Litovsky"], "venue": "J. Acoust. Soc. Am., vol. 120, pp. 2177\u20132189, 2006.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonlinear enhancement of onset for robust speech recognition", "author": ["C. Kim", "R.M. Stern"], "venue": "Proceedings of Interspeech. pp. 2058-2061, 2010.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2010}, {"title": "Power-normalized cepstral coefficients (PNCC) for robust speech recognition", "author": ["C. Kim", "R.M. Stern"], "venue": "Proceedings of ICASSP, pp. 4101-4104, 2012.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Auditory processing of speech signals for robust speech recognition in real-world noisy environments", "author": ["D. Kim", "S.H. Lee", "R.M. Kil"], "venue": "IEEE Trans. Speech Audio Proc., vol. 7, pp. 55-69, 1999.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 1999}, {"title": "An algorithm that improves speech intelligibility in noise for normal-hearing listeners", "author": ["G. Kim", "Y. Lu", "Y. Hu", "P.C. Loizou"], "venue": "J. Acoust. Soc. Am., vol. 126, pp. 1486-1494, 2009.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive denoising autoencoders: a fine-tuning scheme to learn from test mixtures", "author": ["M. Kim", "P. Smaragdis"], "venue": "Proceedings of LVA/ICA, pp. 100-107, 2015.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2015}, {"title": "Role of mask pattern in intelligibility of ideal binarymasked noisy speech", "author": ["U. Kjems", "J.B. Boldt", "M.S. Pedersen", "T. Lunner", "D.L. Wang"], "venue": "J. Acoust. Soc. Am., vol. 126, pp. 1415-1426, 2009.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech intelligibility potential of general and specialized deep neural network based speech enhancement systems", "author": ["M. Kolbak", "Z.H. Tan", "J. Jensen"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 25, pp. 153-167, 2017.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-talker speech separation with utternance-level permutation invariant training of deep recurrent neural networks ", "author": ["M. Kolbak", "D. Yu", "Z.-H. Tan", "J. Jensen"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., in press,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2017}, {"title": "Phase estimation in single channel speech enhancement using phase decomposition", "author": ["J. Kulmer", "P. Mowlaee"], "venue": "IEEE Sig. Proc. Lett., vol. 22, pp. 598-602, 2014.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2014}, {"title": "Delta-spectral cepstral coefficients for robust speech recognition", "author": ["K. Kumar", "C. Kim", "R.M. Stern"], "venue": "Proceedings of ICASSP. pp. 4784-4787, 2011.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2011}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun"], "venue": "Neural Comp., vol. 1, pp. 541-551, 1989.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1989}, {"title": "Fully complex deep neural network for phaseincorporating monaural source separation", "author": ["Y.-S. Lee", "C.-Y. Yang", "S.-F. Wang", "J.-C. Wang", "C.-H. Wu"], "venue": "Proceedings of ICASSP, pp. 281-285, 2017.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2017}, {"title": "Factors influencing intelligibility of ideal binary-masked speech: Implications for noise reduction", "author": ["N. Li", "P.C. Loizou"], "venue": "J. Acoust. Soc. Am., vol. 123, pp. 1673-1682, 2008.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2008}, {"title": "A duplex theory of pitch perception", "author": ["J.C.R. Licklider"], "venue": "Experientia, vol. 7, pp. 128-134, 1951.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1951}, {"title": "SOBM - a binary mask for noisy speech that optimises an objective intelligibility metric", "author": ["L. Lightburn", "M. Brookes"], "venue": "Proceedings of ICASSP, pp. 661-665, 2015.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker-dependent multipitch tracking using deep neural networks", "author": ["Y. Liu", "D.L. Wang"], "venue": "J. Acoust. Soc. Am., vol. 141, pp. 710-721, 2017.  22", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2017}, {"title": "Speech enhancement: Theory and practice", "author": ["P.C. Loizou"], "venue": "Boca Raton FL: CRC Press,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2013}, {"title": "Speech restoration based on deep learning autoencoder with layerwised pretraining", "author": ["X. Lu", "S. Matsuda", "C. Hori", "H. Kashioka"], "venue": "Proceedings of Interspeech, pp. 1504- 1507, 2012.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement based on deep denoising autoencoder", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "Proceedings of Interspeech, pp. 555-559, 2013.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2013}, {"title": "A computational model of binaural localization and separation", "author": ["R.F. Lyon"], "venue": "Proceedings of ICASSP, pp. 1148-1151, 1983.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 1983}, {"title": "Human and machine hearing", "author": ["R.F. Lyon"], "venue": null, "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2017}, {"title": "An auditory based modulation spectral feature for reverberant speech recognition", "author": ["H.K. Maganti", "M. Matassoni"], "venue": "Proceedings of Interspeech. pp. 570-573, 2010.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep long short-term memory adaptive beamforming networks for multichannel robust speech recognition", "author": ["Z. Meng", "S. Watanabe", "J. Hershey", "H. Erdogan"], "venue": "Proceedings of ICASSP, pp. 271-275, 2017.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2017}, {"title": "An analysis of binaural spectro-temporal masking as nonlinear beamforming", "author": ["A.R. Moghimi", "R.M. Stern"], "venue": "Proceedings of ICASSP, pp. 835-839, 2014.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to the psychology of hearing", "author": ["B.C.J. Moore"], "venue": null, "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2003}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of ICML, pp. 807\u2013814, 2010.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2010}, {"title": "Integrating DNN-based and spatial clustering-based mask estimation for robust MVDR beamforming", "author": ["T. Nakatani", "M. Ito", "T. Higuchi", "S. Araki", "K. Kinoshita"], "venue": "Proceedings of ICASSP, pp. 286-290, 2017.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2017}, {"title": "Ideal ratio mask estimation using deep neural networks for robust speech recognition", "author": ["A. Narayanan", "D.L. Wang"], "venue": "Proceedings of ICASSP, pp. 7092-7096, 2013.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech dereverberation", "author": ["P.A. Naylor", "N.D. Gaubitch", "Ed"], "venue": null, "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2010}, {"title": "Deep stacking networks with time series for speech separation", "author": ["S. Nie", "H. Zhang", "X. Zhang", "W. Liu"], "venue": "Proceedings of ICASSP, pp. 6717-6721, 2014.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2014}, {"title": "Multichannel audio source separation with deep neural networks", "author": ["A.A. Nugraha", "A. Liutkus", "E. Vincent"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 24, pp. 1652-1664, 2016.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2016}, {"title": "Long-term SNR estimation of speech signals in known and unknown channel conditions", "author": ["P. Papadopoulos", "A. Tsiartas", "S. Narayanan"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 24, pp. 2495-2506, 2016.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2016}, {"title": "Musician enhancement for speech-in-noise", "author": ["A. Parbery-Clark", "E. Skoe", "C. Lam", "N. Kraus"], "venue": "Ear Hear., vol. 30, pp. 653-661, 2009.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2009}, {"title": "A fully convolutional neural network for speech enhancement", "author": ["S.R. Park", "J.W. Lee"], "venue": "arXiv:1609.07132v1, 2016.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of ICML, pp. 1310\u20131318, 2013.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2013}, {"title": "SEGAN: Speech enhancement generative adversarial network", "author": ["S. Pascual", "A. Bonafonte", "J. Serra"], "venue": "arXiv:1703.09452v3, 2017.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2017}, {"title": "DNN-based speech mask estimation for eigenvector beamforming", "author": ["L. Pfeifenberger", "Zohrer", "F. Pernkopf"], "venue": "Proceedings of ICASSP, pp. 66-70, 2017.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2017}, {"title": "Perceptual  evaluation of speech quality (PESQ) - a new method for speech quality assessment of telephone networks and codecs", "author": ["A. Rix", "J. Beerends", "M. Hollier", "A. Hekstra"], "venue": "Proceedings of ICASSP, pp. 749-752, 2001.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2001}, {"title": "Speech segregation based on sound localization", "author": ["N. Roman", "D.L. Wang", "G.J. Brown"], "venue": "J. Acoust. Soc. Am., vol. 114, pp. 2236-2252, 2003.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2003}, {"title": "Principles of neural dynamics", "author": ["F. Rosenblatt"], "venue": "New York: Spartan,", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 1962}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Parallel distributed processing, D.E. Rumelhart and J.L. McClelland, Ed., Cambridge, MA: MIT Press, pp. 318-362, 1986.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1986}, {"title": "Spectrotemporal modulation subspace-spanning filter bank features for robust automatic speech recognition", "author": ["M.R. Schadler", "B.T. Meyer", "B. Kollmeier"], "venue": "J. Acoust. Soc. Am., vol. 131, pp. 4134-4151, 2012.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Netw., vol. 61, pp. 85-117, 2015.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature extraction from higher-order autocorrelation coefficients for robust speech recognition", "author": ["B.J. Shannon", "K.K. Paliwal"], "venue": "Speech Comm., vol. 48, pp. 1458-1485, 2006.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust speaker identification using auditory features and computational auditory scene analysis", "author": ["Y. Shao", "S. Srinivasan", "D.L. Wang"], "venue": "Proceedings of ICASSP, pp. 1589-1592, 2008.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2008}, {"title": "Binary and ratio time-frequency masks for robust speech recognition", "author": ["S. Srinivasan", "N. Roman", "D.L. Wang"], "venue": "Speech Comm., vol. 48, pp. 1486-1501, 2006.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2006}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv1505.00387, 2015.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple-target deep learning for LSTM-RNN based speech enhancement", "author": ["L. Sun", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "Proceedings of the Workshop on Hands-free Speech Communication and Microphone Arrays, pp. 136-140, 2017.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2017}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "H. Ney", "R. Schluter"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 23, pp. 517\u2013529, 2015.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Proceedings of NIPS, pp. 3104\u20133112, 2014.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2014}, {"title": "An algorithm for intelligibility prediction of time-frequency weighted noisy speech", "author": ["C.H. Taal", "R.C. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 19, pp. 2125-2136, 2011.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech perception in noise by monolingual, bilingual and trilingual listeners", "author": ["D. Tabri", "K.M. Chacra", "T. Pring"], "venue": "International Journal of Language and Communication Disorders, vol. 46, pp. 411-422, 2011.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2011}, {"title": "Noise reduction using connectionist models", "author": ["S. Tamura", "A. Waibel"], "venue": "Proceedings of ICASSP, pp. 553- 556, 1988.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 1988}, {"title": "Speech enhancement based on deep neural networks with skip connections", "author": ["M. Tu", "X. Zhang"], "venue": "Proceedings of ICASSP, pp. 5565-5569, 2017.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2017}, {"title": "Speech separation based on improved deep neural networks with dual outputs of speech features for both target and interfering speakers", "author": ["Y. Tu", "J. Du", "Y. Xu", "L.-R. Dai", "C.-H. Lee"], "venue": "Proceedings of ISCSLP, pp. 250-254, 2014.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2014}, {"title": "Beamforming: A versatile approach to spatial filtering", "author": ["B.D. van Veen", "K.M. Buckley"], "venue": "IEEE ASSP Magazine, 4-24, April 1988.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 1988}, {"title": "Active-set Newton algorithm for overcomplete non-negative representations of audio", "author": ["T. Virtanen", "J.F. Gemmeke", "B. Raj"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 21, pp. 2277-2289, 2013.  23", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2013}, {"title": "On ideal binary mask as the computational goal of auditory scene analysis", "author": ["D.L. Wang"], "venue": "Speech separation by humans and machines, P. Divenyi, Ed., Norwell MA: Kluwer Academic, pp. 181-197, 2005.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2005}, {"title": "The time dimension for scene analysis", "author": ["D.L. Wang"], "venue": "IEEE Trans. Neural Net., vol. 16, pp. 1401-1426, 2005.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2005}, {"title": "Time-frequency masking for speech separation and its potential for hearing aid design", "author": ["D.L. Wang"], "venue": "Trend. Amplif., vol. 12, pp. 332-353, 2008.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep learning reinvents the hearing aid", "author": ["D.L. Wang"], "venue": "IEEE Spectrum, 32-37, March 2017.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2017}, {"title": "Computational auditory scene analysis: Principles, algorithms, and applications", "author": ["D.L. Wang", "G.J. Brown", "Ed"], "venue": null, "citeRegEx": "140", "shortCiteRegEx": "140", "year": 2006}, {"title": "Speech intelligibility in background noise with ideal binary time-frequency masking", "author": ["D.L. Wang", "U. Kjems", "M.S. Pedersen", "J.B. Boldt", "T. Lunner"], "venue": "J. Acoust. Soc. Am., vol. 125, pp. 2336-2347, 2009.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2009}, {"title": "Trainable frontend for robust and far-field keyword spotting", "author": ["Y. Wang", "P. Getreuer", "T. Hughes", "R.F. Lyon", "R.A. Saurous"], "venue": "Proceedings of ICASSP, pp. 5670-5674, 2017.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2017}, {"title": "Exploring monaural features for classification-based speech segregation", "author": ["Y. Wang", "K. Han", "D.L. Wang"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 21, pp. 270-279, 2013.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2013}, {"title": "On training targets for supervised speech separation", "author": ["Y. Wang", "A. Narayanan", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 22, pp. 1849-1858, 2014.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 1849}, {"title": "Boosting classification based speech separation using temporal dynamics", "author": ["Y. Wang", "D.L. Wang"], "venue": "Proceedings of Interspeech, pp. 1528-1531, 2012.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2012}, {"title": "Cocktail party processing via structured prediction", "author": ["Y. Wang", "D.L. Wang"], "venue": "Proceedings of NIPS, pp. 224-232, 2012.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards scaling up classificationbased speech separation", "author": ["Y. Wang", "D.L. Wang"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 21, pp. 1381-1390, 2013.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2013}, {"title": "A deep neural network for timedomain signal reconstruction", "author": ["Y. Wang", "D.L. Wang"], "venue": "Proceedings of ICASSP, pp. 4390-4394, 2015.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2015}, {"title": "Phoneme-specific speech separation", "author": ["Z.-Q. Wang", "Y. Zhao", "D.L. Wang"], "venue": "Proceedings of ICASSP, pp. 146-150, 2016.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2016}, {"title": "Oracle performance investigation of the ideal masks", "author": ["Z. Wang", "X. Wang", "X. Li", "Q. Fu", "Y. Yan"], "venue": "Proceedings of IWAENC, 2016.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR", "author": ["F. Weninger"], "venue": "Proceedings of LVA/ICA, 2015.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminatively trained recurrent neural networks for single-channel speech separation", "author": ["F. Weninger", "J. Hershey", "J. Le Roux", "B. Schuller"], "venue": "Proceedings of GlobalSIP, pp. 740-744, 2014.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation through time: What it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proc. IEEE, vol. 78, pp. 1550-1560, 1990.", "citeRegEx": "153", "shortCiteRegEx": null, "year": 1990}, {"title": "Complex ratio masking for monaural speech separation", "author": ["D.S. Williamson", "Y. Wang", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 24, pp. 483\u2013492, 2016.", "citeRegEx": "154", "shortCiteRegEx": null, "year": 2016}, {"title": "The lack of a priori distinction between learning algorithms", "author": ["D.H. Wolpert"], "venue": "Neural Comp., vol. 8, pp. 1341-1390, 1996.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 1996}, {"title": "A reverberationtime-aware approach to speech dereverberation based on deep neural networks", "author": ["B. Wu", "K. Li", "M. Yang", "C.-H. Lee"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 25, pp. 102-111, 2017.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2017}, {"title": "A two-stage algorithm for onemicrophone reverberant speech enhancement", "author": ["M. Wu", "D.L. Wang"], "venue": "IEEE Trans.  Audio Speech Lang. Proc., vol. 14, pp. 774-784, 2006.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2006}, {"title": "On timefrequency mask estimation for MVDR beamforming with application in robust speech recognition", "author": ["X. Xiao", "S. Zhao", "D.L. Jones", "E.S. Chng", "H. Li"], "venue": "Proceedings of ICASSP, pp. 3246-3250, 2017.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2017}, {"title": "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation", "author": ["X. Xiao"], "venue": "EURASIP J. Adv. Sig. Proc., vol. 2016, pp. 1-18, 2016.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic noise aware training for speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "Proceedings of Interspeech, pp. 2670-2674, 2014.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2014}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE Sig. Proc. Lett., vol. 21, pp. 65-68, 2014.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2014}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 23, pp. 7-19, 2015.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2015}, {"title": "Blind separation of speech mixtures via time-frequency masking", "author": ["O. Yilmaz", "S. Rickard"], "venue": "IEEE Trans. Sig. Proc., vol. 52, pp. 1830-1847, 2004.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 1830}, {"title": "The NTT CHiME-3 system: advances in speech enhancement and recognition for mobile multi-microphone devices", "author": ["T. Yoshioka", "M. Ito", "M. Delcroix", "e. al."], "venue": "Proceedings of IEEE ASRU, 2015.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2015}, {"title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation", "author": ["D. Yu", "M. Kolbak", "Z.-H. Tan", "J. Jensen"], "venue": "Proceedings of ICASSP, pp. 241-245, 2017.", "citeRegEx": "165", "shortCiteRegEx": null, "year": 2017}, {"title": "Localization based stereo speech source separation using probabilistic time-frequency masking and deep neural networks", "author": ["Y. Yu", "W. Wang", "P. Han"], "venue": "EURASIP J. Audio Speech Music Proc., vol. 2016, pp. 1-18, 2016.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust features for noisy speech recognition based on temporal trajectory filtering of short-time autocorrelation sequences", "author": ["K.-H. Yuo", "H.-C. Wang"], "venue": "Speech Comm., vol. 28, pp. 13-24, 1999.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 1999}, {"title": "Boosting contextual information for deep neural network based voice activity detection", "author": ["X.-L. Zhang", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 24, pp. 252-264, 2016.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep ensemble learning method for monaural speech separation", "author": ["X.-L. Zhang", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 24, pp. 967-977, 2016.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep belief networks based voice activity detection", "author": ["X.-L. Zhang", "J. Wu"], "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 21, pp. 697-710, 2013.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning based binaural speech separation in reverberant environments", "author": ["X. Zhang", "D.L. Wang"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 25, pp. 1075-1084, 2017.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2017}, {"title": "A speech enhancement algorithm by iterating single- and multimicrophone processing and its application to robust ASR", "author": ["X. Zhang", "Z.-Q. Wang", "D.L. Wang"], "venue": "Proceedings of ICASSP, pp. 276-280, 2017.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2017}, {"title": "A pairwise algorithm using the deep stacking network for speech separation and pitch estimation", "author": ["X. Zhang", "H. Zhang", "S. Nie", "G. Gao", "W. Liu"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Proc., vol. 24, pp. 1066-1078, 2016.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2016}, {"title": "A two-stage algorithm for noisy and reverberant speech enhancement", "author": ["Y. Zhao", "Z.-Q. Wang", "D.L. Wang"], "venue": "Proceedings of ICASSP, pp. 5580-5584, 2017.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 21, "context": "Speech separation is commonly called the \u201ccocktail party problem,\u201d a term coined by Cherry in his famous 1953 paper [22].", "startOffset": 116, "endOffset": 120}, {"referenceID": 22, "context": "In his 1957 book [23], Cherry made an observation: \u201cNo machine has yet been constructed to do just that [solving the cocktail part problem].", "startOffset": 17, "endOffset": 21}, {"referenceID": 92, "context": "Two traditional approaches for monaural separation are speech enhancement [95] and computational auditory scene analysis (CASA) [140].", "startOffset": 74, "endOffset": 78}, {"referenceID": 137, "context": "Two traditional approaches for monaural separation are speech enhancement [95] and computational auditory scene analysis (CASA) [140].", "startOffset": 128, "endOffset": 133}, {"referenceID": 30, "context": "Speech enhancement analyzes general statistics of speech and noise, followed by estimation of clean speech from noisy speech with a noise estimate [32] [95].", "startOffset": 147, "endOffset": 151}, {"referenceID": 92, "context": "Speech enhancement analyzes general statistics of speech and noise, followed by estimation of clean speech from noisy speech with a noise estimate [32] [95].", "startOffset": 152, "endOffset": 156}, {"referenceID": 9, "context": "The simplest and most widely used enhancement method is spectral subtraction [10], in which the power spectrum of the estimated noise is subtracted from that of noisy speech.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "CASA is based on perceptual principles of auditory scene analysis [12] and exploits grouping cues such as pitch and onset.", "startOffset": 66, "endOffset": 70}, {"referenceID": 63, "context": "For example, the tandem algorithm separates voiced speech by alternating pitch estimation and pitch-based grouping [65].", "startOffset": 115, "endOffset": 119}, {"referenceID": 131, "context": "Beamforming, or spatial filtering, boosts the signal that arrives from a specific direction through proper array configuration, hence attenuating interference from other directions [134] [11] [7] [74].", "startOffset": 181, "endOffset": 186}, {"referenceID": 10, "context": "Beamforming, or spatial filtering, boosts the signal that arrives from a specific direction through proper array configuration, hence attenuating interference from other directions [134] [11] [7] [74].", "startOffset": 187, "endOffset": 191}, {"referenceID": 6, "context": "Beamforming, or spatial filtering, boosts the signal that arrives from a specific direction through proper array configuration, hence attenuating interference from other directions [134] [11] [7] [74].", "startOffset": 192, "endOffset": 195}, {"referenceID": 72, "context": "Beamforming, or spatial filtering, boosts the signal that arrives from a specific direction through proper array configuration, hence attenuating interference from other directions [134] [11] [7] [74].", "startOffset": 196, "endOffset": 200}, {"referenceID": 95, "context": "The original formulation of supervised speech separation was inspired by the concept of time-frequency (T-F) masking in CASA [98] [140] [138].", "startOffset": 125, "endOffset": 129}, {"referenceID": 137, "context": "The original formulation of supervised speech separation was inspired by the concept of time-frequency (T-F) masking in CASA [98] [140] [138].", "startOffset": 130, "endOffset": 135}, {"referenceID": 135, "context": "The original formulation of supervised speech separation was inspired by the concept of time-frequency (T-F) masking in CASA [98] [140] [138].", "startOffset": 136, "endOffset": 141}, {"referenceID": 61, "context": "A major goal of CASA is the ideal binary mask (IBM) [63], which denotes whether the target signal dominates a T-F unit in the time-frequency representation of a mixed signal.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "Listening studies show that ideal binary masking dramatically improves speech intelligibility for normal-hearing (NH) and hearing-impaired (HI) listeners in noisy conditions [13] [1] [91] [141].", "startOffset": 174, "endOffset": 178}, {"referenceID": 0, "context": "Listening studies show that ideal binary masking dramatically improves speech intelligibility for normal-hearing (NH) and hearing-impaired (HI) listeners in noisy conditions [13] [1] [91] [141].", "startOffset": 179, "endOffset": 182}, {"referenceID": 88, "context": "Listening studies show that ideal binary masking dramatically improves speech intelligibility for normal-hearing (NH) and hearing-impaired (HI) listeners in noisy conditions [13] [1] [91] [141].", "startOffset": 183, "endOffset": 187}, {"referenceID": 138, "context": "Listening studies show that ideal binary masking dramatically improves speech intelligibility for normal-hearing (NH) and hearing-impaired (HI) listeners in noisy conditions [13] [1] [91] [141].", "startOffset": 188, "endOffset": 193}, {"referenceID": 16, "context": "Over the last decade, supervised speech separation has substantially advanced the state-of-the-art performance by leveraging large training data and increasing computing resources [17].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "Furthermore, we equate speech separation and the cocktail party problem, which goes beyond the separation of two speech utterances originally experimented with by Cherry [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "Over the past decade, DNNs have significantly elevated the performance of many supervised learning tasks, such as image classification [24], handwriting recognition [41], automatic speech recognition [60], language modeling [127], and machine translation [128].", "startOffset": 135, "endOffset": 139}, {"referenceID": 39, "context": "Over the past decade, DNNs have significantly elevated the performance of many supervised learning tasks, such as image classification [24], handwriting recognition [41], automatic speech recognition [60], language modeling [127], and machine translation [128].", "startOffset": 165, "endOffset": 169}, {"referenceID": 58, "context": "Over the past decade, DNNs have significantly elevated the performance of many supervised learning tasks, such as image classification [24], handwriting recognition [41], automatic speech recognition [60], language modeling [127], and machine translation [128].", "startOffset": 200, "endOffset": 204}, {"referenceID": 124, "context": "Over the past decade, DNNs have significantly elevated the performance of many supervised learning tasks, such as image classification [24], handwriting recognition [41], automatic speech recognition [60], language modeling [127], and machine translation [128].", "startOffset": 224, "endOffset": 229}, {"referenceID": 125, "context": "Over the past decade, DNNs have significantly elevated the performance of many supervised learning tasks, such as image classification [24], handwriting recognition [41], automatic speech recognition [60], language modeling [127], and machine translation [128].", "startOffset": 255, "endOffset": 260}, {"referenceID": 115, "context": "An MLP is an extension of Rosenblatt\u2019s perceptron [118] by introducing hidden layers between the input layer and the output layer.", "startOffset": 50, "endOffset": 55}, {"referenceID": 116, "context": "An MLP is trained with the classical backpropagation algorithm [119] where the network weights are adjusted to minimize the prediction error through gradient descent.", "startOffset": 63, "endOffset": 68}, {"referenceID": 115, "context": "The representational power of an MLP increases as the number of layers increases [118] even though, in theory, an MLP with two hidden layers can approximate any function [57].", "startOffset": 81, "endOffset": 86}, {"referenceID": 55, "context": "The representational power of an MLP increases as the number of layers increases [118] even though, in theory, an MLP with two hidden layers can approximate any function [57].", "startOffset": 170, "endOffset": 174}, {"referenceID": 59, "context": "[61].", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[61] proposed restrictive Boltzmann machines (RBMs) to pretrain a DNN layer by layer, and RBM pretraining is found to improve subsequent supervised learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 101, "context": "A later remedy was to use a rectified linear unit (ReLU) [104] to replace the traditional sigmoid activation function, which converts a weighted sum of the inputs to a model neuron to the neuron\u2019s output.", "startOffset": 57, "endOffset": 62}, {"referenceID": 122, "context": "Recently, skip connections have been introduced to facilitate the training of very deep MLPs [125] [50].", "startOffset": 93, "endOffset": 98}, {"referenceID": 48, "context": "Recently, skip connections have been introduced to facilitate the training of very deep MLPs [125] [50].", "startOffset": 99, "endOffset": 103}, {"referenceID": 86, "context": "A class of feedforward networks, known as convolutional neural networks (CNNs) [88] [8], has been demonstrated to be well suited for pattern recognition, particularly in the visual domain.", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "A class of feedforward networks, known as convolutional neural networks (CNNs) [88] [8], has been demonstrated to be well suited for pattern recognition, particularly in the visual domain.", "startOffset": 84, "endOffset": 87}, {"referenceID": 134, "context": "We note that a RNN through its recurrent connections introduces the time dimension, which is flexible and infinitely extensible, a characteristic not shared by feedforward networks no matter how deep they are [137]; in a way, a RNN can be viewed a DNN with an infinite depth [121].", "startOffset": 209, "endOffset": 214}, {"referenceID": 118, "context": "We note that a RNN through its recurrent connections introduces the time dimension, which is flexible and infinitely extensible, a characteristic not shared by feedforward networks no matter how deep they are [137]; in a way, a RNN can be viewed a DNN with an infinite depth [121].", "startOffset": 275, "endOffset": 280}, {"referenceID": 150, "context": "The recurrent connections are typically trained with backpropagation through time [153].", "startOffset": 82, "endOffset": 87}, {"referenceID": 110, "context": "However, such RNN training is susceptible to the vanishing or exploding gradient problem [113].", "startOffset": 89, "endOffset": 94}, {"referenceID": 60, "context": "To alleviate this problem, a RNN with long short-term memory (LSTM) introduces memory cells with gates to facilitate the information flow over time [62].", "startOffset": 148, "endOffset": 152}, {"referenceID": 38, "context": "Generative adversarial networks (GANs) were recently introduced with simultaneously trained models: a generative model G and a discriminative model D [40].", "startOffset": 150, "endOffset": 154}, {"referenceID": 7, "context": "In this overview, a DNN refers to any neural network with at least two hidden layers [8] [60], in contrast to popular learning machines with just one hidden layer such as commonly used MLPs, support vector machines (SVMs) with kernels, and Gaussian mixture models (GMMs).", "startOffset": 85, "endOffset": 88}, {"referenceID": 58, "context": "In this overview, a DNN refers to any neural network with at least two hidden layers [8] [60], in contrast to popular learning machines with just one hidden layer such as commonly used MLPs, support vector machines (SVMs) with kernels, and Gaussian mixture models (GMMs).", "startOffset": 89, "endOffset": 93}, {"referenceID": 61, "context": "The first training target used in supervised speech separation is the ideal binary mask [63] [117] [64] [136], which is inspired by the auditory masking phenomenon in audition [103] and the exclusive allocation principle in auditory scene analysis [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 114, "context": "The first training target used in supervised speech separation is the ideal binary mask [63] [117] [64] [136], which is inspired by the auditory masking phenomenon in audition [103] and the exclusive allocation principle in auditory scene analysis [12].", "startOffset": 93, "endOffset": 98}, {"referenceID": 62, "context": "The first training target used in supervised speech separation is the ideal binary mask [63] [117] [64] [136], which is inspired by the auditory masking phenomenon in audition [103] and the exclusive allocation principle in auditory scene analysis [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 133, "context": "The first training target used in supervised speech separation is the ideal binary mask [63] [117] [64] [136], which is inspired by the auditory masking phenomenon in audition [103] and the exclusive allocation principle in auditory scene analysis [12].", "startOffset": 104, "endOffset": 109}, {"referenceID": 100, "context": "The first training target used in supervised speech separation is the ideal binary mask [63] [117] [64] [136], which is inspired by the auditory masking phenomenon in audition [103] and the exclusive allocation principle in auditory scene analysis [12].", "startOffset": 176, "endOffset": 181}, {"referenceID": 11, "context": "The first training target used in supervised speech separation is the ideal binary mask [63] [117] [64] [136], which is inspired by the auditory masking phenomenon in audition [103] and the exclusive allocation principle in auditory scene analysis [12].", "startOffset": 248, "endOffset": 252}, {"referenceID": 81, "context": "Target binary masking also leads to dramatic improvement of speech intelligibility in noise [83], and the TBM has been used as a training target [39] [93].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "Target binary masking also leads to dramatic improvement of speech intelligibility in noise [83], and the TBM has been used as a training target [39] [93].", "startOffset": 145, "endOffset": 149}, {"referenceID": 90, "context": "Target binary masking also leads to dramatic improvement of speech intelligibility in noise [83], and the TBM has been used as a training target [39] [93].", "startOffset": 150, "endOffset": 154}, {"referenceID": 121, "context": "Instead of a hard label on each T-F unit, the ideal ratio mask (IRM) can be viewed as a soft version of the IBM [124] [106] [144] [71]:", "startOffset": 112, "endOffset": 117}, {"referenceID": 103, "context": "Instead of a hard label on each T-F unit, the ideal ratio mask (IRM) can be viewed as a soft version of the IBM [124] [106] [144] [71]:", "startOffset": 118, "endOffset": 123}, {"referenceID": 141, "context": "Instead of a hard label on each T-F unit, the ideal ratio mask (IRM) can be viewed as a soft version of the IBM [124] [106] [144] [71]:", "startOffset": 124, "endOffset": 129}, {"referenceID": 69, "context": "Instead of a hard label on each T-F unit, the ideal ratio mask (IRM) can be viewed as a soft version of the IBM [124] [106] [144] [71]:", "startOffset": 130, "endOffset": 134}, {"referenceID": 141, "context": "The spectral magnitude mask (SMM) (called FFT-MASK in [144]) is defined on the STFT (short-time Fourier transform) magnitudes of clean speech and noisy speech: SMM(t, f) = |S(t, f)|", "startOffset": 54, "endOffset": 59}, {"referenceID": 31, "context": "The phase-sensitive mask (PSM) extends the SMM by including a measure of phase [33]:", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "The inclusion of the phase difference in the PSM leads to a higher SNR, and tends to yield a better estimate of clean speech than the SMM [33].", "startOffset": 138, "endOffset": 142}, {"referenceID": 151, "context": "Unlike the aforementioned masks, it can perfectly reconstruct clean speech from noisy speech [154]:", "startOffset": 93, "endOffset": 98}, {"referenceID": 151, "context": "So some form of compression should be used to bound mask values, such as a tangent hyperbolic or sigmoidal function [154] [150] .", "startOffset": 116, "endOffset": 121}, {"referenceID": 147, "context": "So some form of compression should be used to bound mask values, such as a tangent hyperbolic or sigmoidal function [154] [150] .", "startOffset": 122, "endOffset": 127}, {"referenceID": 151, "context": "[154] observe that, in Cartesian coordinates, structure exists in both real and imaginary components of the cIRM, whereas in polar coordinates, structure exists in the magnitude spectrogram but not phase spectrogram.", "startOffset": 0, "endOffset": 5}, {"referenceID": 87, "context": "Without clear structure, direct phase estimation would be intractable through supervised learning, although we should mention a recent paper that uses complex-domain DNN to estimate complex STFT coefficients [89].", "startOffset": 208, "endOffset": 212}, {"referenceID": 94, "context": "The target magnitude spectrum (TMS) of clean speech, or S(t, f |, is a mapping-based training target [97] [161] [45] [162].", "startOffset": 101, "endOffset": 105}, {"referenceID": 158, "context": "The target magnitude spectrum (TMS) of clean speech, or S(t, f |, is a mapping-based training target [97] [161] [45] [162].", "startOffset": 106, "endOffset": 111}, {"referenceID": 43, "context": "The target magnitude spectrum (TMS) of clean speech, or S(t, f |, is a mapping-based training target [97] [161] [45] [162].", "startOffset": 112, "endOffset": 116}, {"referenceID": 159, "context": "The target magnitude spectrum (TMS) of clean speech, or S(t, f |, is a mapping-based training target [97] [161] [45] [162].", "startOffset": 117, "endOffset": 122}, {"referenceID": 141, "context": "Another closely related mapping-based target is the gammatone frequency target power spectrum (GF-TPS) [144].", "startOffset": 103, "endOffset": 108}, {"referenceID": 137, "context": "An estimate of the GFTPS is easily converted to the separated speech waveform through cochleagram inversion [140].", "startOffset": 108, "endOffset": 113}, {"referenceID": 149, "context": "The idea of signal approximation (SA) is to train a ratio mask estimator that minimizes the difference between the spectral magnitude of clean speech and that of estimated speech [152] [68]:", "startOffset": 179, "endOffset": 184}, {"referenceID": 66, "context": "The idea of signal approximation (SA) is to train a ratio mask estimator that minimizes the difference between the spectral magnitude of clean speech and that of estimated speech [152] [68]:", "startOffset": 185, "endOffset": 189}, {"referenceID": 149, "context": "So, SA can be interpreted as a target that combines ratio masking and spectral mapping, seeking to maximize SNR [152].", "startOffset": 112, "endOffset": 117}, {"referenceID": 74, "context": "A related, earlier target aims for the maximal SNR in the context of IBM estimation [76].", "startOffset": 84, "endOffset": 88}, {"referenceID": 149, "context": "For the SA target, better separation performance is achieved with two-stage training [152].", "startOffset": 85, "endOffset": 90}, {"referenceID": 141, "context": "A number of training targets have been compared using a fixed feedforward DNN with three hidden layers and the same set of input features [144].", "startOffset": 138, "endOffset": 143}, {"referenceID": 126, "context": "The separated speech using various training targets is evaluated in terms of the short-time objective intelligibility (STOI) score [129] and the perceptual evaluation of speech quality (PESQ) score [116].", "startOffset": 131, "endOffset": 136}, {"referenceID": 113, "context": "The separated speech using various training targets is evaluated in terms of the short-time objective intelligibility (STOI) score [129] and the perceptual evaluation of speech quality (PESQ) score [116].", "startOffset": 198, "endOffset": 203}, {"referenceID": 51, "context": "In addition, a representative speech enhancement algorithm [53] and a supervised nonnegative matrix factorization (NMF) algorithm [135] are evaluated as benchmarks.", "startOffset": 59, "endOffset": 63}, {"referenceID": 132, "context": "In addition, a representative speech enhancement algorithm [53] and a supervised nonnegative matrix factorization (NMF) algorithm [135] are evaluated as benchmarks.", "startOffset": 130, "endOffset": 135}, {"referenceID": 123, "context": "First, in terms of objective intelligibility, the masking-based targets as a group outperform the mappingbased targets, although a recent study [126] indicates that masking is advantageous only at higher input SNRs and at lower SNRs mapping is more advantageous.", "startOffset": 144, "endOffset": 149}, {"referenceID": 141, "context": "In addition, the estimation of unbounded spectral magnitudes tends to magnify estimation errors [144].", "startOffset": 96, "endOffset": 101}, {"referenceID": 128, "context": "This indeed was used in an early study that trains an MLP to map from a frame of noisy speech waveform to a frame of clean speech waveform, which may be called temporal mapping [131].", "startOffset": 177, "endOffset": 182}, {"referenceID": 145, "context": "Although simple, such direct mapping does not perform well even when a DNN is used in place of a shallow network [148] [27].", "startOffset": 113, "endOffset": 118}, {"referenceID": 26, "context": "Although simple, such direct mapping does not perform well even when a DNN is used in place of a shallow network [148] [27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 145, "context": "In [148], a target is defined in the time domain but the DNN for target estimation includes modules for ratio masking and inverse Fourier transform with noisy phase.", "startOffset": 3, "endOffset": 8}, {"referenceID": 147, "context": "A recent study evaluates oracle results of a number of ideal masks and additionally introduces the socalled ideal gain mask (IGM) [150], defined in terms of a priori SNR and a posteriori SNR commonly used in traditional speech enhancement [95].", "startOffset": 130, "endOffset": 135}, {"referenceID": 92, "context": "A recent study evaluates oracle results of a number of ideal masks and additionally introduces the socalled ideal gain mask (IGM) [150], defined in terms of a priori SNR and a posteriori SNR commonly used in traditional speech enhancement [95].", "startOffset": 239, "endOffset": 243}, {"referenceID": 166, "context": "2 The conclusion is also nuanced for speaker separation [169].", "startOffset": 56, "endOffset": 61}, {"referenceID": 114, "context": "Early studies in supervised speech separation use only a few features such as interaural time differences (ITD) and interaural level (intensity) differences (IID) [117] in binaural separation, and pitch-based features [76] [65] [43] and amplitude modulation spectrogram (AMS) [81] in monaural separation.", "startOffset": 163, "endOffset": 168}, {"referenceID": 74, "context": "Early studies in supervised speech separation use only a few features such as interaural time differences (ITD) and interaural level (intensity) differences (IID) [117] in binaural separation, and pitch-based features [76] [65] [43] and amplitude modulation spectrogram (AMS) [81] in monaural separation.", "startOffset": 218, "endOffset": 222}, {"referenceID": 63, "context": "Early studies in supervised speech separation use only a few features such as interaural time differences (ITD) and interaural level (intensity) differences (IID) [117] in binaural separation, and pitch-based features [76] [65] [43] and amplitude modulation spectrogram (AMS) [81] in monaural separation.", "startOffset": 223, "endOffset": 227}, {"referenceID": 41, "context": "Early studies in supervised speech separation use only a few features such as interaural time differences (ITD) and interaural level (intensity) differences (IID) [117] in binaural separation, and pitch-based features [76] [65] [43] and amplitude modulation spectrogram (AMS) [81] in monaural separation.", "startOffset": 228, "endOffset": 232}, {"referenceID": 79, "context": "Early studies in supervised speech separation use only a few features such as interaural time differences (ITD) and interaural level (intensity) differences (IID) [117] in binaural separation, and pitch-based features [76] [65] [43] and amplitude modulation spectrogram (AMS) [81] in monaural separation.", "startOffset": 276, "endOffset": 280}, {"referenceID": 140, "context": "A subsequent study [143] explores more monaural features including mel-frequency cepstral coefficient (MFCC), gammatone frequency cepstral coefficient (GFCC) [123], perceptual linear prediction (PLP) [54], and relative spectral transform PLP (RASTA- PLP) [55].", "startOffset": 19, "endOffset": 24}, {"referenceID": 120, "context": "A subsequent study [143] explores more monaural features including mel-frequency cepstral coefficient (MFCC), gammatone frequency cepstral coefficient (GFCC) [123], perceptual linear prediction (PLP) [54], and relative spectral transform PLP (RASTA- PLP) [55].", "startOffset": 158, "endOffset": 163}, {"referenceID": 52, "context": "A subsequent study [143] explores more monaural features including mel-frequency cepstral coefficient (MFCC), gammatone frequency cepstral coefficient (GFCC) [123], perceptual linear prediction (PLP) [54], and relative spectral transform PLP (RASTA- PLP) [55].", "startOffset": 200, "endOffset": 204}, {"referenceID": 53, "context": "A subsequent study [143] explores more monaural features including mel-frequency cepstral coefficient (MFCC), gammatone frequency cepstral coefficient (GFCC) [123], perceptual linear prediction (PLP) [54], and relative spectral transform PLP (RASTA- PLP) [55].", "startOffset": 255, "endOffset": 259}, {"referenceID": 18, "context": "We conducted a study to examine an extensive list of acoustic features for supervised speech separation at low SNRs [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 85, "context": "The mel-domain features are MFCC and deltaspectral cepstral coefficient (DSCC) [87], which is similar to MFCC except that a delta operation is applied to melspectrum.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "FA is shown in parentheses (from [19]).", "startOffset": 33, "endOffset": 37}, {"referenceID": 97, "context": "feature (GF), GFCC, and gammatone frequency modulation coefficient (GFMC) [100].", "startOffset": 74, "endOffset": 79}, {"referenceID": 78, "context": "A zero-crossing feature, called zero-crossings with peak-amplitudes (ZCPA) [80], computes zero-crossing intervals and corresponding peak amplitudes from subband signals derived using a gammatone filterbank.", "startOffset": 75, "endOffset": 79}, {"referenceID": 164, "context": "The autocorrelation features are relative autocorrelation sequence MFCC (RAS-MFCC) [167], autocorrelation sequence MFCC (AC-MFCC) [122] and phase autocorrelation MFCC (PAC-MFCC) [73], all of which apply the MFCC procedure in the autocorrelation domain.", "startOffset": 83, "endOffset": 88}, {"referenceID": 119, "context": "The autocorrelation features are relative autocorrelation sequence MFCC (RAS-MFCC) [167], autocorrelation sequence MFCC (AC-MFCC) [122] and phase autocorrelation MFCC (PAC-MFCC) [73], all of which apply the MFCC procedure in the autocorrelation domain.", "startOffset": 130, "endOffset": 135}, {"referenceID": 71, "context": "The autocorrelation features are relative autocorrelation sequence MFCC (RAS-MFCC) [167], autocorrelation sequence MFCC (AC-MFCC) [122] and phase autocorrelation MFCC (PAC-MFCC) [73], all of which apply the MFCC procedure in the autocorrelation domain.", "startOffset": 178, "endOffset": 182}, {"referenceID": 77, "context": "The medium-time filtering features are power normalized cepstral coefficients (PNCC) [79] and suppression of slowly-varying components and the falling edge of the power envelope (SSF) [78].", "startOffset": 85, "endOffset": 89}, {"referenceID": 76, "context": "The medium-time filtering features are power normalized cepstral coefficients (PNCC) [79] and suppression of slowly-varying components and the falling edge of the power envelope (SSF) [78].", "startOffset": 184, "endOffset": 188}, {"referenceID": 117, "context": "The modulation domain features are Gabor filterbank (GFB) [120] and AMS features.", "startOffset": 58, "endOffset": 63}, {"referenceID": 18, "context": "In addition to existing features, we proposed a new feature called Multi-Resolution Cochleagram (MRCG) [19], which computes four cochleagrams at different spectrotemporal resolutions to provide both local information and a broader context.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "The features are post-processed with the auto-regressive moving average (ARMA) filter [15] and evaluated with a fixed MLP based IBM mask estimator.", "startOffset": 86, "endOffset": 90}, {"referenceID": 79, "context": "The HIT\u2212FA rate is found to be well correlated with speech intelligibility [81].", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "Recently, Delfarah and Wang [27] performed another feature study that considers room reverberation, and both speech denoising and speaker separation.", "startOffset": 28, "endOffset": 32}, {"referenceID": 158, "context": "The features added in this study include log spectral magnitude (LOG-MAG) and log mel-spectrum feature (LOG-MEL), both of which are commonly used in supervised separation [161] [69].", "startOffset": 171, "endOffset": 176}, {"referenceID": 67, "context": "The features added in this study include log spectral magnitude (LOG-MAG) and log mel-spectrum feature (LOG-MEL), both of which are commonly used in supervised separation [161] [69].", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "STOI improvements (in %) for a list of features averaged on a set of test noises (from [27]).", "startOffset": 87, "endOffset": 91}, {"referenceID": 26, "context": "But it should be noted that, the feedforward DNN used in [27] may not couple well with waveform signals, and CNNs and RNNs may be better suited for so-called end-to-end separation.", "startOffset": 57, "endOffset": 61}, {"referenceID": 142, "context": "To our knowledge, deep learning was first introduced to speech separation by Wang and Wang in 2012 in two conference papers [145] [146], which were later extended to a journal version in 2013 [147].", "startOffset": 124, "endOffset": 129}, {"referenceID": 143, "context": "To our knowledge, deep learning was first introduced to speech separation by Wang and Wang in 2012 in two conference papers [145] [146], which were later extended to a journal version in 2013 [147].", "startOffset": 130, "endOffset": 135}, {"referenceID": 144, "context": "To our knowledge, deep learning was first introduced to speech separation by Wang and Wang in 2012 in two conference papers [145] [146], which were later extended to a journal version in 2013 [147].", "startOffset": 192, "endOffset": 197}, {"referenceID": 142, "context": "In the conference versions, feedforward DNNs with RBM pretraining were used as binary classifiers, as well as feature encoders for structured perceptrons [145] and conditional random fields [146].", "startOffset": 154, "endOffset": 159}, {"referenceID": 143, "context": "In the conference versions, feedforward DNNs with RBM pretraining were used as binary classifiers, as well as feature encoders for structured perceptrons [145] and conditional random fields [146].", "startOffset": 190, "endOffset": 195}, {"referenceID": 144, "context": "In the journal version [147], the input signal is passed through a 64-channel gammatone filterbank to derive subband signals, from which acoustic features are extracted within each T-F unit.", "startOffset": 23, "endOffset": 28}, {"referenceID": 50, "context": "This algorithm was further extended to a two-stage DNN [52], where the first stage is trained to estimate the subband IBM as usual and the second stage explicitly incorporates the T-F context in the following way.", "startOffset": 55, "endOffset": 59}, {"referenceID": 50, "context": "Subject tests demonstrate that this DNN produced large intelligibility improvements for both HI and NH listeners, with HI listeners benefiting more [52].", "startOffset": 148, "endOffset": 152}, {"referenceID": 94, "context": "[97] published an Interspeech paper that uses a deep autoencoder (DAE) for speech enhancement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 144, "context": "Illustration of DNN for feature learning, and learned features are then used by linear SVM for IBM estimation (from [147]).", "startOffset": 116, "endOffset": 121}, {"referenceID": 50, "context": "Schematic diagram of a two-stage DNN for speech separation (from [52]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 94, "context": "The algorithm in [97] learns to map from the mel-frequency power spectrum of noisy speech to that of clean speech, so it can be regarded as the first mapping based method.", "startOffset": 17, "endOffset": 21}, {"referenceID": 94, "context": "Subsequently, perhaps unaware of [97], Xu et al.", "startOffset": 33, "endOffset": 37}, {"referenceID": 158, "context": "[161] published a study using a DNN with RBM pretraining to map from the log power spectrum of noisy speech to that of clean speech, as shown in Fig.", "startOffset": 0, "endOffset": 5}, {"referenceID": 94, "context": "Unlike [97], the DNN used in [161] is a standard feedforward MLP with RBM pretraining.", "startOffset": 7, "endOffset": 11}, {"referenceID": 158, "context": "Unlike [97], the DNN used in [161] is a standard feedforward MLP with RBM pretraining.", "startOffset": 29, "endOffset": 34}, {"referenceID": 149, "context": "In [152] [151], RNNs with LSTM were used for speech enhancement and its application to robust ASR, where training aims for signal approximation (see Sect.", "startOffset": 3, "endOffset": 8}, {"referenceID": 148, "context": "In [152] [151], RNNs with LSTM were used for speech enhancement and its application to robust ASR, where training aims for signal approximation (see Sect.", "startOffset": 9, "endOffset": 14}, {"referenceID": 31, "context": "RNNs were also used in [33] to estimate the PSM.", "startOffset": 23, "endOffset": 27}, {"referenceID": 105, "context": "In [108] [173], a deep stacking network was proposed for IBM estimation and a mask estimate was then used for pitch estimation.", "startOffset": 3, "endOffset": 8}, {"referenceID": 170, "context": "In [108] [173], a deep stacking network was proposed for IBM estimation and a mask estimate was then used for pitch estimation.", "startOffset": 9, "endOffset": 14}, {"referenceID": 151, "context": "A DNN was used to simultaneously estimate the real and imaginary components of the cIRM, yielding better speech quality over IRM estimation [154].", "startOffset": 140, "endOffset": 145}, {"referenceID": 146, "context": "Speech enhancement at the phoneme level has been recently studied [149] [14].", "startOffset": 66, "endOffset": 71}, {"referenceID": 13, "context": "Speech enhancement at the phoneme level has been recently studied [149] [14].", "startOffset": 72, "endOffset": 76}, {"referenceID": 45, "context": "In [47], the DNN takes into account of perceptual masking with a piecewise gain function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 129, "context": "In [132], skip connections between non-consecutive layers are", "startOffset": 3, "endOffset": 8}, {"referenceID": 93, "context": "4 The authors also published a paper in Interspeech 2012 [96] where a", "startOffset": 57, "endOffset": 61}, {"referenceID": 68, "context": "CNNs have also been used for IRM estimation [70] and spectral mapping [114] [112].", "startOffset": 44, "endOffset": 48}, {"referenceID": 111, "context": "CNNs have also been used for IRM estimation [70] and spectral mapping [114] [112].", "startOffset": 70, "endOffset": 75}, {"referenceID": 109, "context": "CNNs have also been used for IRM estimation [70] and spectral mapping [114] [112].", "startOffset": 76, "endOffset": 81}, {"referenceID": 35, "context": "[37] developed a fully convolutional network (a CNN with fully connected layers removed) for speech enhancement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 111, "context": "Another recent study employs a GAN to perform temporal mapping [114].", "startOffset": 63, "endOffset": 68}, {"referenceID": 158, "context": "Diagram of a DNN-based spectral mapping method for speech enhancement (from [161]).", "startOffset": 76, "endOffset": 81}, {"referenceID": 80, "context": "In an effort to address the mismatch between training and test conditions, Kim and Smaragdis [82] proposed a two-stage DNN where the first stage is a standard DNN to perform spectral mapping and the second stage is an autoencoder that performs unsupervised adaptation during the test stage.", "startOffset": 93, "endOffset": 97}, {"referenceID": 93, "context": "The AE is trained to map the magnitude spectrum of a clean utterance to itself, much like [96], and hence its training does not need labeled data.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "When available training noises are limited, one technique is to expand training noises through noise perturbation, particularly frequency perturbation [20]; specifically, the spectrogram of an original noise sample is perturbed to generate new noise samples.", "startOffset": 151, "endOffset": 155}, {"referenceID": 158, "context": "[161] more robust to new noises, Xu et al.", "startOffset": 0, "endOffset": 5}, {"referenceID": 157, "context": "[160] incorporate noise aware training, i.", "startOffset": 0, "endOffset": 5}, {"referenceID": 80, "context": "DNN architecture for speech enhancement with an autoencoder for unsupervised adaptation (from [82]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "Speech enhancement results at -2 dB SNR measured in STOI (from [21]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "Noise generalization is systematically addressed in [21].", "startOffset": 52, "endOffset": 56}, {"referenceID": 141, "context": "In addition, the IRM is simultaneously estimated over several consecutive frames and different estimates for the same frame are averaged to produce a smoother, more accurate mask (see also [144]).", "startOffset": 189, "endOffset": 194}, {"referenceID": 144, "context": "To evaluate the impact of the number of training noises on noise generalization, the same DNN is also trained with 100 noises as done in [147].", "startOffset": 137, "endOffset": 142}, {"referenceID": 15, "context": "Diagram of an LSTM based speech separation system (from [16]).", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "STOI improvements of a feedforward DNN and a RNN with LSTM (from [16]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "However, experimental results [16] [84] show that a feedforward DNN appears incapable of modeling a large number of talkers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 82, "context": "However, experimental results [16] [84] show that a feedforward DNN appears incapable of modeling a large number of talkers.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "We have recently employed RNN with LSTM to address speaker generalization of noise-independent models [16] [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "We have recently employed RNN with LSTM to address speaker generalization of noise-independent models [16] [18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "As a result, dereverberation has been actively investigated for a long time [3] [157] [107] [49].", "startOffset": 76, "endOffset": 79}, {"referenceID": 154, "context": "As a result, dereverberation has been actively investigated for a long time [3] [157] [107] [49].", "startOffset": 80, "endOffset": 85}, {"referenceID": 104, "context": "As a result, dereverberation has been actively investigated for a long time [3] [157] [107] [49].", "startOffset": 86, "endOffset": 91}, {"referenceID": 47, "context": "As a result, dereverberation has been actively investigated for a long time [3] [157] [107] [49].", "startOffset": 92, "endOffset": 96}, {"referenceID": 43, "context": "[45] proposed the first DNN based approach to speech dereverberation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "In their later work [46], they apply spectral mapping on a spectrogram and extend the approach to perform both dereverberation and denoising.", "startOffset": 20, "endOffset": 24}, {"referenceID": 153, "context": "[156], who observe that dereverberation performance improves when frame length and shift are chosen differently depending on the reverberation time (T60).", "startOffset": 0, "endOffset": 5}, {"referenceID": 44, "context": "Their comparisons show an improvement in dereverberation performance over the DNN in [46].", "startOffset": 85, "endOffset": 89}, {"referenceID": 156, "context": "[159] proposed a DNN trained to predict static, delta and acceleration features at the same time.", "startOffset": 0, "endOffset": 5}, {"referenceID": 171, "context": "[174] observe that spectral mapping is more effective for dereverberation than T-F masking, whereas masking works better than mapping for denoising.", "startOffset": 0, "endOffset": 5}, {"referenceID": 145, "context": "Furthermore, to alleviate the adverse effects of using the phase of reverberantnoisy speech in resynthesizing the waveform signal of enhanced speech, this study extends the time-domain signal reconstruction technique in [148].", "startOffset": 220, "endOffset": 225}, {"referenceID": 145, "context": "Here the training target is defined in the time-domain, but clean phase is used during training unlike in [148] where noisy phase is used.", "startOffset": 106, "endOffset": 111}, {"referenceID": 171, "context": "The results in [174] show that the two-stage DNN model significantly outperforms the single-stage models for either mapping or masking.", "startOffset": 15, "endOffset": 20}, {"referenceID": 43, "context": "on spectral mapping (from [45]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 66, "context": "[68] were the first to introduce DNN for this task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "Experimental results have shown that both the masking layer and discriminative training improve speaker separation [69].", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "[31] appeared to have independently proposed a DNN for speaker separation similar to [68].", "startOffset": 0, "endOffset": 4}, {"referenceID": 66, "context": "[31] appeared to have independently proposed a DNN for speaker separation similar to [68].", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "In this study [31], the DNN is trained to estimate the log power spectrum of the target speaker from that of a cochannel mixture.", "startOffset": 14, "endOffset": 18}, {"referenceID": 130, "context": "In a different paper [133], they trained a DNN to map a cochannel signal to the spectrum of the target speaker as well as the spectrum of an interfering speaker, as illustrated in Fig.", "startOffset": 21, "endOffset": 26}, {"referenceID": 28, "context": "11 (see [30] for an extended version).", "startOffset": 8, "endOffset": 12}, {"referenceID": 66, "context": "A notable extension compared to [68] is that these papers also address the situation where only the target speaker is the same between training and testing, while interfering speakers are different between training and testing.", "startOffset": 32, "endOffset": 36}, {"referenceID": 66, "context": "\u2019s approach is speaker dependent [68] [69] and the studies in [31] [133] deal with both speaker and target dependent separation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 67, "context": "\u2019s approach is speaker dependent [68] [69] and the studies in [31] [133] deal with both speaker and target dependent separation.", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": "\u2019s approach is speaker dependent [68] [69] and the studies in [31] [133] deal with both speaker and target dependent separation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 130, "context": "\u2019s approach is speaker dependent [68] [69] and the studies in [31] [133] deal with both speaker and target dependent separation.", "startOffset": 67, "endOffset": 72}, {"referenceID": 166, "context": "Zhang and Wang proposed a deep ensemble network to address speaker-dependent as well as target-dependent separation [169].", "startOffset": 116, "endOffset": 121}, {"referenceID": 49, "context": "[51] have recently used a DNN for speakerdependent cochannel separation and performed speech intelligibility evaluation of the DNN with both HI and NH listeners.", "startOffset": 0, "endOffset": 4}, {"referenceID": 153, "context": "Diagram of a reverberation time aware DNN for speech dereverberation (redrawn from [156]).", "startOffset": 83, "endOffset": 88}, {"referenceID": 49, "context": "Compared to earlier DNN-based cochannel separation studies, the algorithm in [51] uses a diverse set of features and predicts multiple IRM frames, resulting in better separation.", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Speaker-independent separation can be treated as unsupervised clustering where T-F units are clustered into distinct classes dominated by individual speakers [4] [66].", "startOffset": 158, "endOffset": 161}, {"referenceID": 64, "context": "Speaker-independent separation can be treated as unsupervised clustering where T-F units are clustered into distinct classes dominated by individual speakers [4] [66].", "startOffset": 162, "endOffset": 166}, {"referenceID": 54, "context": "were the first to address speaker-independent multi-talker separation in the DNN framework [56].", "startOffset": 91, "endOffset": 95}, {"referenceID": 49, "context": "Mean intelligibility scores and standard errors for HI and NH subjects listening to target sentences mixed with interfering sentences and separated target sentences (from [51]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 162, "context": "Two-talker separation with permutation-invariant training (from [165]).", "startOffset": 64, "endOffset": 69}, {"referenceID": 64, "context": "significantly better than a CASA method [66] and an NMF method for speaker-independent separation.", "startOffset": 40, "endOffset": 44}, {"referenceID": 162, "context": "[165] recently proposed permutationinvariant training, which is shown in Fig.", "startOffset": 0, "endOffset": 5}, {"referenceID": 162, "context": "Although much simpler, speaker separation results are shown to match those obtained with deep clustering [165] [85].", "startOffset": 105, "endOffset": 110}, {"referenceID": 83, "context": "Although much simpler, speaker separation results are shown to match those obtained with deep clustering [165] [85].", "startOffset": 111, "endOffset": 115}, {"referenceID": 137, "context": "This is precisely the issue of sequential organization, which is much investigated in CASA [140].", "startOffset": 91, "endOffset": 96}, {"referenceID": 5, "context": "I, and independent component analysis [6] [72].", "startOffset": 38, "endOffset": 41}, {"referenceID": 70, "context": "I, and independent component analysis [6] [72].", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "Sound localization and location-based grouping are among the classic topics in auditory perception and CASA [9] [12] [140].", "startOffset": 108, "endOffset": 111}, {"referenceID": 11, "context": "Sound localization and location-based grouping are among the classic topics in auditory perception and CASA [9] [12] [140].", "startOffset": 112, "endOffset": 116}, {"referenceID": 137, "context": "Sound localization and location-based grouping are among the classic topics in auditory perception and CASA [9] [12] [140].", "startOffset": 117, "endOffset": 122}, {"referenceID": 114, "context": "[117] in the binaural domain.", "startOffset": 0, "endOffset": 5}, {"referenceID": 160, "context": "Another classic two-sensor separation technique, DUET (Degenerate Unmixing Estimation Technique), was published by Yilmaz and Rickard [163] at about the same time.", "startOffset": 134, "endOffset": 139}, {"referenceID": 56, "context": "binary masking [58] vs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 57, "context": "clustering [59] for beamforming (see Sect.", "startOffset": 11, "endOffset": 15}, {"referenceID": 54, "context": "B), and deep clustering [56] versus mask estimation [85] for talker-independent speaker separation (see Sect.", "startOffset": 24, "endOffset": 28}, {"referenceID": 83, "context": "B), and deep clustering [56] versus mask estimation [85] for talker-independent speaker separation (see Sect.", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "It is worth noting that human auditory scene analysis integrates monaural and binaural analysis in a seamless fashion, taking advantage of whatever discriminant information existing in a particular environment [12] [140] [25].", "startOffset": 210, "endOffset": 214}, {"referenceID": 137, "context": "It is worth noting that human auditory scene analysis integrates monaural and binaural analysis in a seamless fashion, taking advantage of whatever discriminant information existing in a particular environment [12] [140] [25].", "startOffset": 215, "endOffset": 220}, {"referenceID": 24, "context": "It is worth noting that human auditory scene analysis integrates monaural and binaural analysis in a seamless fashion, taking advantage of whatever discriminant information existing in a particular environment [12] [140] [25].", "startOffset": 221, "endOffset": 225}, {"referenceID": 73, "context": "[75].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] subsequently employed a DNN for spectral mapping that includes the spatial features of ILD, interaural phase difference (IPD), and enhanced features with an initial mask derived from location information, in addition to", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[35] proposed a spectral mapping approach utilizing both binaural and monaural inputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "A quantitative comparison with [75] shows that their system produces better PESQ scores for separated speech but similar STOI numbers.", "startOffset": 31, "endOffset": 35}, {"referenceID": 163, "context": "[166].", "startOffset": 0, "endOffset": 5}, {"referenceID": 168, "context": "Recently, Zhang and Wang [171] developed a DNN for IRM estimation with a more sophisticated set of spatial and spectral features.", "startOffset": 25, "endOffset": 30}, {"referenceID": 73, "context": "Instead of monaural analysis on a single ear [75] [35], spectral analysis in [171] is conducted on the output of a fixed beamformer, which itself removes some background inference, by extracting a complementary set of monaural features (see Sect.", "startOffset": 45, "endOffset": 49}, {"referenceID": 33, "context": "Instead of monaural analysis on a single ear [75] [35], spectral analysis in [171] is conducted on the output of a fixed beamformer, which itself removes some background inference, by extracting a complementary set of monaural features (see Sect.", "startOffset": 50, "endOffset": 54}, {"referenceID": 168, "context": "Instead of monaural analysis on a single ear [75] [35], spectral analysis in [171] is conducted on the output of a fixed beamformer, which itself removes some background inference, by extracting a complementary set of monaural features (see Sect.", "startOffset": 77, "endOffset": 82}, {"referenceID": 99, "context": "Since traditional beamforming with an array also produces a \u201cmonaural\u201d output, corresponding to the target source, T-F masking based on spatial features may be considered beamforming or, more accurately, nonlinear beamforming [102] as opposed to traditional beamforming that is linear.", "startOffset": 226, "endOffset": 231}, {"referenceID": 137, "context": "It is well recognized in CASA that localization and separation are two closely related functions ([140], Chapter 5).", "startOffset": 98, "endOffset": 103}, {"referenceID": 46, "context": "For human audition, evidence suggests that sound localization largely depends on source separation [48] [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "For human audition, evidence suggests that sound localization largely depends on source separation [48] [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 56, "context": "Fueled by the CHiME-3 challenge for robust ASR, two independent studies made the first use of DNN based monaural speech enhancement in conjunction with conventional beamforming, both published in ICASSP 2016 [58] [59].", "startOffset": 208, "endOffset": 212}, {"referenceID": 57, "context": "Fueled by the CHiME-3 challenge for robust ASR, two independent studies made the first use of DNN based monaural speech enhancement in conjunction with conventional beamforming, both published in ICASSP 2016 [58] [59].", "startOffset": 213, "endOffset": 217}, {"referenceID": 4, "context": "a tablet [5].", "startOffset": 9, "endOffset": 12}, {"referenceID": 168, "context": "Schematic diagram of a binaural separation algorithm (from [171]).", "startOffset": 59, "endOffset": 64}, {"referenceID": 34, "context": "MVDR aims to minimize the noise energy from nontarget directions while imposing linear constraints to maintain the energy from the target direction [36].", "startOffset": 148, "endOffset": 152}, {"referenceID": 56, "context": "In [58], an RNN with bidirectional LSTM is used for IBM estimation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 57, "context": "In [59], a spatial clustering based approach was proposed to compute a ratio mask.", "startOffset": 3, "endOffset": 7}, {"referenceID": 161, "context": "\u2019s method was used in the best performing system in the CHiME-3 challenge [164].", "startOffset": 74, "endOffset": 79}, {"referenceID": 27, "context": "DNN-based IRM estimation combined with a beamformer, is also behind the winning system in the most recent CHiME-4 challenge [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 106, "context": "[109], who perform array source separation using DNN for monaural separation and a complex multivariate Gaussian distribution to model spatial information.", "startOffset": 0, "endOffset": 5}, {"referenceID": 57, "context": "[59] and Heymann et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[58] in the CHiME-3 challenge by using DNN estimated masks for beamforming has motivated many recent studies, exploring different ways of integrating T-F masking and beamforming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] trained an RNN for monaural speech enhancement, from which a ratio mask is computed in order to provide coefficients for an MVDR beamformer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 169, "context": "[172] trained a DNN for IRM estimation from a Figure 15.", "startOffset": 0, "endOffset": 5}, {"referenceID": 32, "context": "MVDR beamformer with monaural mask estimation (from [34]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 161, "context": "34% relative improvement over the previous best [164].", "startOffset": 48, "endOffset": 53}, {"referenceID": 155, "context": "[158] also proposed to iterate ratio masking and beamforming.", "startOffset": 0, "endOffset": 5}, {"referenceID": 161, "context": "They showed that this approach leads to a considerable WER reduction over the use of a conventional MVDR, although recognition accuracy is not as high as in [164].", "startOffset": 157, "endOffset": 162}, {"referenceID": 112, "context": "[115], who use the cosine distance between the principal components of consecutive frames of noisy speech as the feature for DNN mask estimation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 98, "context": "[101] use RNNs for adaptive estimation of beamformer coefficients.", "startOffset": 0, "endOffset": 5}, {"referenceID": 102, "context": "[105] integrate DNN mask estimation and cGMM clustering based estimation to further improve the quality of mask estimates.", "startOffset": 0, "endOffset": 5}, {"referenceID": 152, "context": "The so-called nofree-lunch theorem [155] dictates that no learning algorithm, DNN included, achieves superior performance in all tasks.", "startOffset": 35, "endOffset": 40}, {"referenceID": 139, "context": "Aside from theoretical arguments, feature extraction is a way of imparting knowledge from a problem domain and it stands to reason that it is useful to incorporate domain knowledge this way (see [142] for a recent example).", "startOffset": 195, "endOffset": 200}, {"referenceID": 7, "context": "For instance, the success of CNN in visual pattern recognition is partly due to the use of shared weights and pooling (sampling) layers in its architecture that helps to build a representation invariant to small variations of feature positions [8].", "startOffset": 244, "endOffset": 247}, {"referenceID": 11, "context": "Much research in auditory scene analysis shows that pitch is a primary cue for auditory organization [12] [25], and research in CASA demonstrates that pitch alone can go a long way in separating voiced speech [65].", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "Much research in auditory scene analysis shows that pitch is a primary cue for auditory organization [12] [25], and research in CASA demonstrates that pitch alone can go a long way in separating voiced speech [65].", "startOffset": 106, "endOffset": 110}, {"referenceID": 63, "context": "Much research in auditory scene analysis shows that pitch is a primary cue for auditory organization [12] [25], and research in CASA demonstrates that pitch alone can go a long way in separating voiced speech [65].", "startOffset": 209, "endOffset": 213}, {"referenceID": 20, "context": "Perhaps a DNN can be trained to \u201cdiscover\u201d harmonicity as a prominent feature, and there is some hint at this from a recent study [21], but extracting pitch as input features seems like the most straightforward way of incorporating pitch in speech separation.", "startOffset": 130, "endOffset": 134}, {"referenceID": 84, "context": "First, temporal mapping is a welcome addition to the list of supervised separation approaches and provides a unique perspective to phase enhancement [86] [38].", "startOffset": 149, "endOffset": 153}, {"referenceID": 36, "context": "First, temporal mapping is a welcome addition to the list of supervised separation approaches and provides a unique perspective to phase enhancement [86] [38].", "startOffset": 154, "endOffset": 158}, {"referenceID": 89, "context": "It is interesting to note Licklider\u2019s classic duplex theory of pitch perception, postulating two processes of pitch analysis: a spatial process corresponding to the frequency dimension in the cochlea and a temporal process corresponding to the temporal response of each frequency channel [92].", "startOffset": 288, "endOffset": 292}, {"referenceID": 25, "context": "Computational models for pitch estimation fall into three categories: spectral, temporal, and spectrotemporal approaches [26].", "startOffset": 121, "endOffset": 125}, {"referenceID": 137, "context": "responses of a cochlear filterbank [140] [99], is a duplex representation.", "startOffset": 35, "endOffset": 40}, {"referenceID": 96, "context": "responses of a cochlear filterbank [140] [99], is a duplex representation.", "startOffset": 41, "endOffset": 45}, {"referenceID": 135, "context": "benefiting from vision [138] [28].", "startOffset": 23, "endOffset": 28}, {"referenceID": 137, "context": "A full account, however, would require a sophistical model of auditory attention (see [140] [99]).", "startOffset": 86, "endOffset": 91}, {"referenceID": 96, "context": "A full account, however, would require a sophistical model of auditory attention (see [140] [99]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 137, "context": "CASA defines the solution to the cocktail party problem as a system that achieves human separation performance in all listening conditions ([140], p.", "startOffset": 140, "endOffset": 145}, {"referenceID": 136, "context": "Not as broad as defined in CASA, but this definition has the benefit that it is tightly linked to a primary driver for speech separation research, namely, to eliminate the speech understanding handicap of millions of listeners with impaired hearing [139].", "startOffset": 249, "endOffset": 254}, {"referenceID": 65, "context": "The related topics include multipitch tracking [67] [44] [94], voice activity detection [170] [168], and even a task as basic in signal processing as SNR estimation [110].", "startOffset": 47, "endOffset": 51}, {"referenceID": 42, "context": "The related topics include multipitch tracking [67] [44] [94], voice activity detection [170] [168], and even a task as basic in signal processing as SNR estimation [110].", "startOffset": 52, "endOffset": 56}, {"referenceID": 91, "context": "The related topics include multipitch tracking [67] [44] [94], voice activity detection [170] [168], and even a task as basic in signal processing as SNR estimation [110].", "startOffset": 57, "endOffset": 61}, {"referenceID": 167, "context": "The related topics include multipitch tracking [67] [44] [94], voice activity detection [170] [168], and even a task as basic in signal processing as SNR estimation [110].", "startOffset": 88, "endOffset": 93}, {"referenceID": 165, "context": "The related topics include multipitch tracking [67] [44] [94], voice activity detection [170] [168], and even a task as basic in signal processing as SNR estimation [110].", "startOffset": 94, "endOffset": 99}, {"referenceID": 107, "context": "The related topics include multipitch tracking [67] [44] [94], voice activity detection [170] [168], and even a task as basic in signal processing as SNR estimation [110].", "startOffset": 165, "endOffset": 170}, {"referenceID": 20, "context": "Finally, we remark that human ability to solve the cocktail party problem appears to have much to do with our extensive exposure to various noisy environments (see also [21]).", "startOffset": 169, "endOffset": 173}, {"referenceID": 40, "context": "Research indicates that children have poorer ability to recognize speech in noise than adults [42] [77], and musicians are better at perceiving noisy speech than non-musicians [111] presumably due to musicians\u2019 long exposure to polyphonic signals.", "startOffset": 94, "endOffset": 98}, {"referenceID": 75, "context": "Research indicates that children have poorer ability to recognize speech in noise than adults [42] [77], and musicians are better at perceiving noisy speech than non-musicians [111] presumably due to musicians\u2019 long exposure to polyphonic signals.", "startOffset": 99, "endOffset": 103}, {"referenceID": 108, "context": "Research indicates that children have poorer ability to recognize speech in noise than adults [42] [77], and musicians are better at perceiving noisy speech than non-musicians [111] presumably due to musicians\u2019 long exposure to polyphonic signals.", "startOffset": 176, "endOffset": 181}, {"referenceID": 127, "context": "Relative to monolingual speakers, bilinguals have a deficit when it comes to speech perception in noise, although the two groups are similarly proficient in quiet [130].", "startOffset": 163, "endOffset": 168}], "year": 2017, "abstractText": "Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target", "creator": "Word"}}}