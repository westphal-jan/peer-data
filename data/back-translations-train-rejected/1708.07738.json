{"id": "1708.07738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "A Function Approximation Method for Model-based High-Dimensional Inverse Reinforcement Learning", "abstract": "This works handles the inverse reinforcement learning problem in high-dimensional state spaces, which relies on an efficient solution of model-based high-dimensional reinforcement learning problems. To solve the computationally expensive reinforcement learning problems, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function based on the observed human actions for inverse reinforcement learning problems. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle high-dimensional even continuous state spaces efficiently. We test the proposed method in a simulated environment to show its accuracy, and three clinical tasks to show how it can be used to evaluate a doctor's proficiency.", "histories": [["v1", "Wed, 23 Aug 2017 20:25:01 GMT  (674kb)", "http://arxiv.org/abs/1708.07738v1", "arXiv admin note: substantial text overlap witharXiv:1707.09394"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1707.09394", "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["kun li", "joel w burdick"], "accepted": false, "id": "1708.07738"}, "pdf": {"name": "1708.07738.pdf", "metadata": {"source": "CRF", "title": "A Function Approximation Method for Model-based High-Dimensional Inverse Reinforcement Learning", "authors": ["Kun Li", "Joel W. Burdick"], "emails": ["kunli@caltech.edu"], "sections": [{"heading": null, "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "II. RELATED WORKS", "text": "A good introduction is possible in any case, and this data is wasted in a model-free learning process. Most of them use a function to determine the value of the work, and performance depends on the function chosen."}, {"heading": "III. HIGH-DIMENSIONAL INVERSE REINFORCEMENT LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Markov Decision Process", "text": "A Markov decision-making process is described by the following variables: \u2022 S = \"s,\" a series of states \u2022 A = \"a,\" a series of actions \u2022 \"pass,\" a state transitional function that defines the probability that a state s becomes \"after action a.\" \u2022 R = \"r\" (s), a reward function that defines the immediate reward of a state s. \u2022 \"g,\" a discount factor that ensures the convergence of the MDP over an infinite horizon. \u2022 R = \"r\" (s), a reward function that defines the immediate reward of a state s. \u2022 \"n\" refers to the length of motion that differs in different observations. Given the observed sequence, inverse amplification learning algorithms attempt to restore a reward function that explains the motion function. A key problem is how to model the action in each state \"s,\" or the function, \"we can\" function \"of a Qkov,\" by introducing a function."}, {"heading": "B. Function Approximation Framework", "text": "Given the amount of actions and the transition probability, a reward function leads to a unique optimum value function. To learn the reward function from the observed movement, rather than learning the reward function directly, we use a parameterized function called a VR function to represent the sum of the reward function and the discounted value function: f (s, \u03b8 (s). (5) The functional value of a state is called a VR value. Substituting equation (5) in Bellman's optimality problems equation, the optimal Q function is called: Q (s) = Maximum formula of the learning process p (s), a pass formula f (s), a pass model f (s), a pass expos\u00e9 of the learning process p (s), (6) the optimal value function is given as: V (s) = Maximum formula of a Q problem Equity (function a), the function is optimal Q (optimal Q)."}, {"heading": "C. High-dimensional Reinforcement Learning", "text": "Although it is not our main focus, we will briefly show how the proposed method solves high-dimensional amplification problems. Assuming that the approximation function is a neural network, the parameter \u03b8 = {w, b} weights and bias equation (5) can be estimated using the observed sequence of rewards R (s) by least square estimation, where the objective function is: LSE (\u03b8) = 0. The reward function r (s) in equation (8) is indistinguishable from the max function as a Bellman backup operator. By approximation with the generalized Softmax function [24], the gradient of the objective function is as follows: 0."}, {"heading": "D. High-dimensional Inverse Reinforcement Learning", "text": "For IRL problems, this paper opts for max as the Bellman Backup Operator and a motion model p (a) Q (a | s) based on the optimal Q function Q (s, a) [17]: P (a | s) = empirical development Q (s, a) \u2211 a). Q (s, a). Q (s, a). (9) Algorithm 1 Function 1: Approximation RL with Neural Network1: Data: R, S, A, P, \u03b3 (s, B, 2: Result: optimal value V (S), optimal action value Q (s, A). (S) 3: Create variable value = {W, b) for a neural network 4: build f [S, \u03b8] as output of the neural network 5: build Q (S, A], V [S]."}, {"heading": "IV. EXPERIMENTS", "text": "We first test the proposed method in a simulated environment to compare its accuracy under different approximation functions, and then apply the proposed method to surgical data in the JIGSAW dataset [1]."}, {"heading": "A. Simulated Environment", "text": "We create a four-dimensional grid, with 10 grids in each dimension, so that reward problems are generated. Multiple rewarding objects are randomly placed in the grid, and each of them generates an exponentially decreasing negative or positive reward value for the entire grid based on distances. The probable characteristic of a grid is the sum of the generated rewards in the grid. To test the application of the proposed method of amplifying learning problems, we assume that the reward value of each state is available to the robot, and it must learn an optimal value function from it. We compare the basic truth value function calculated by creating values and the value function, which is based on the mean of the optimal Q values."}, {"heading": "B. Surgical Robot Operator", "text": "We apply the proposed method to surgical robot operators in JIGSAW dataset [1]. This dataset describes three tasks: tying knots, passing needles and sewing. An illustration of the tasks is shown in Figure 6. Each task is performed by several robot operators, whose skills range from experts, intermediate to novices. The data includes videos of two stereo cameras and robot states synchronized with the images. We assume that the actions of the operator change the linear and angular acceleration of the robot, and then we use k-mean clusters to identify 10,000 actions from the dataset. The state set includes the positions and speeds of the robot manipulator, represented by a length vector of 38 with continuous values. The transition probability is calculated on the basis of physical laws. We apply the model to the evaluation of the operator to three tasks, allowing experts to successfully identify all the results proposed and the intermediate results to be presented in 7 and 8."}, {"heading": "V. CONCLUSIONS", "text": "This paper deals with the problem of high-dimensional inverse reinforcement learning, where the state space is usually too large for many existing solutions. We solve the problem with a functional approximation framework by approximating the learning solution to reinforcement formation. First, the method is tested in a simulated environment and then applied to the evaluation of surgical robots in three clinical tasks. Under the current conditions, each task has a reward function coupled with an optimal value function. In future work, we will expand this method so that one robot can learn several reward functions. We will also try to integrate learning into the framework in the transition model."}], "references": [{"title": "Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling", "author": ["Y. Gao", "S.S. Vedula", "C.E. Reiley", "N. Ahmidi", "B. Varadarajan", "H.C. Lin", "L. Tao", "L. Zappella", "B. B\u00e9jar", "D.D. Yuh"], "venue": "MICCAI Workshop: M2CAI, vol. 3, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S. Russell"], "venue": "in Proc. 17th International Conf. on Machine Learning, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning. ACM, 2004, p. 1.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Apprenticeship learning using inverse reinforcement learning and gradient methods", "author": ["G. Neu", "C. Szepesv\u00e1ri"], "venue": "arXiv preprint arXiv:1206.5264, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "Proc. AAAI, 2008, pp. 1433\u20131438.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximate Dynamic Programming: Solving the curses of dimensionality", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. v. Hasselt", "A. Guez", "D. Silver"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press, 2016, pp. 2094\u20132100.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "Advances in neural information processing systems, 2000, pp. 1057\u20131063.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 729\u2013736.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Inverse reinforcement learning with locally consistent reward functions", "author": ["Q.P. Nguyen", "B.K.H. Low", "P. Jaillet"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1747\u20131755.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2011, pp. 19\u201327.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["C. Finn", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1603.00448, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Inverse reinforcement learning in partially observable environments", "author": ["J. Choi", "K.-E. Kim"], "venue": "Journal of Machine Learning Research, vol. 12, no. Mar, pp. 691\u2013730, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["S. Levine", "V. Koltun"], "venue": "arXiv preprint arXiv:1206.4617, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep inverse reinforcement learning", "author": ["M. Wulfmeier", "P. Ondruska", "I. Posner"], "venue": "arXiv preprint arXiv:1507.04888, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence, ser. IJCAI\u201907. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2007, pp. 2586\u20132591.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "From human to humanoid locomotionan inverse optimal control approach", "author": ["K. Mombaur", "A. Truong", "J.-P. Laumond"], "venue": "Autonomous robots, vol. 28, no. 3, pp. 369\u2013383, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian multitask inverse reinforcement learning", "author": ["C. Dimitrakakis", "C.A. Rothkopf"], "venue": "European Workshop on Reinforcement Learning. Springer, 2011, pp. 273\u2013284.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonparametric bayesian inverse reinforcement learning for multiple reward functions", "author": ["J. Choi", "K.-E. Kim"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 305\u2013313.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Relative entropy inverse reinforcement learning", "author": ["A. Boularias", "J. Kober", "J.R. Peters"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 182\u2013189.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Linearly-solvable markov decision problems", "author": ["E. Todorov"], "venue": "Proceedings of the 19th International Conference on Neural Information Processing Systems. MIT Press, 2006, pp. 1369\u20131376.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Bellman Gradient Iteration for Inverse Reinforcement Learning", "author": ["K. Li", "J.W. Burdick"], "venue": "ArXiv e-prints, Jul. 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Linearly-solvable markov decision problems", "author": ["E. Todorov"], "venue": "Advances in neural information processing systems, 2007, pp. 1369\u2013 1376.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "1: Knot tying with Da Vinci robot: the photo is grabbed from JIGSAW dataset [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "For example, the methods in [2], [3], [4] estimate the agent\u2019s policy from a set of observations, and estimate a reward function that leads to the policy.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "For example, the methods in [2], [3], [4] estimate the agent\u2019s policy from a set of observations, and estimate a reward function that leads to the policy.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "For example, the methods in [2], [3], [4] estimate the agent\u2019s policy from a set of observations, and estimate a reward function that leads to the policy.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "The method in [5] collects a set of trajectories of the agent, and estimates a reward function that maximizes the likelihood of the trajectories.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "A good introduction is given in [6].", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "Some model-free methods produce many promising results in recent years, like deep Q network [7], double Q learning [8], advantage learning [9], etc.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Some model-free methods produce many promising results in recent years, like deep Q network [7], double Q learning [8], advantage learning [9], etc.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Some model-free methods produce many promising results in recent years, like deep Q network [7], double Q learning [8], advantage learning [9], etc.", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "Inverse Reinforcement Learning problem is firstly formulated in [2], where the agent observes the states resulting from an assumingly optimal policy, and tries to learn a reward function that makes the policy better than all alternatives.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "This idea is extended by [10], in the name of max-margin learning for inverse optimal control.", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "extension is proposed in [3], where the purpose is not to recover the real reward function, but to find a reward function that leads to a policy equivalent to the observed one, measured by the amount of rewards collected by following that policy.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "Since a motion policy may be difficult to estimate from observations, a behavior-based method is proposed in [5], which models the distribution of behaviors as a maximumentropy model on the amount of reward collected from each behavior.", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "For example, [11] considers a sequence of changing reward functions instead of a single reward function.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "[12] and [13] consider complex reward functions, instead of linear one, and use Gaussian process and neural networks, respectively, to model the reward function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] and [13] consider complex reward functions, instead of linear one, and use Gaussian process and neural networks, respectively, to model the reward function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[14] considers complex environments, instead of a well-observed Markov Decision Process, and combines partially observed Markov Decision Process with reward learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] models the behaviors based on the local optimality of a behavior, instead of the summation of rewards.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] uses a multi-layer neural network to represent nonlinear reward functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Another method is proposed in [17], which models the probability of a behavior as the product of each state-action\u2019s probability, and learns the reward function via maximum a posteriori estimation.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "This work is extended by [4], which uses sub-gradient methods to simplify the problem.", "startOffset": 25, "endOffset": 28}, {"referenceID": 17, "context": "Another extensions is shown in [18], which tries to find a reward function that matches the observed behavior.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "For motions involving multiple tasks and varying reward functions, methods are developed in [19] and [20], which try to learn multiple reward functions.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "For motions involving multiple tasks and varying reward functions, methods are developed in [19] and [20], which try to learn multiple reward functions.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "The method in [2] uses a linear approximation of the value function, but it requires a set of manually defined basis functions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "The methods in [13], [21] update the reward function parameter by minimizing the relative entropy between the observed trajectories and a set of sampled trajectories based on the reward function, but they require a set of manually segmented trajectories of human motion, where the choice of trajectory length will affect the result.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "The methods in [13], [21] update the reward function parameter by minimizing the relative entropy between the observed trajectories and a set of sampled trajectories based on the reward function, but they require a set of manually segmented trajectories of human motion, where the choice of trajectory length will affect the result.", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "The method in [22] only learns an optimal value function, instead of the reward function.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "Q-function Q(s,a), described by the Bellman Equation [23]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "This is described by the Bellman Optimality Equation [23]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "For example, V \u2217(s) = loga\u2208A expQ \u2217(s,a) is used in the maximum-entropy method[5]; V \u2217(s) = 1 k loga\u2208A expk \u2217 Q\u2217(s,a) is used in Bellman Gradient Iteration [24].", "startOffset": 78, "endOffset": 81}, {"referenceID": 23, "context": "For example, V \u2217(s) = loga\u2208A expQ \u2217(s,a) is used in the maximum-entropy method[5]; V \u2217(s) = 1 k loga\u2208A expk \u2217 Q\u2217(s,a) is used in Bellman Gradient Iteration [24].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "For inverse reinforcement learning problems, combined with different Bellman backup operators, this formulation can extend many existing methods to high-dimensional space, like the motion model in [25], p(a|s) = \u2212v\u2217(s)\u2212 log\u2211k ps,k exp(\u2212v (k)), the motion model in [5], p(a|s) = expQ\u2217(s,a)\u2212V \u2217(s), and the motion model in [17], p(a|s) \u221d expQ\u2217(s,a).", "startOffset": 197, "endOffset": 201}, {"referenceID": 4, "context": "For inverse reinforcement learning problems, combined with different Bellman backup operators, this formulation can extend many existing methods to high-dimensional space, like the motion model in [25], p(a|s) = \u2212v\u2217(s)\u2212 log\u2211k ps,k exp(\u2212v (k)), the motion model in [5], p(a|s) = expQ\u2217(s,a)\u2212V \u2217(s), and the motion model in [17], p(a|s) \u221d expQ\u2217(s,a).", "startOffset": 264, "endOffset": 267}, {"referenceID": 16, "context": "For inverse reinforcement learning problems, combined with different Bellman backup operators, this formulation can extend many existing methods to high-dimensional space, like the motion model in [25], p(a|s) = \u2212v\u2217(s)\u2212 log\u2211k ps,k exp(\u2212v (k)), the motion model in [5], p(a|s) = expQ\u2217(s,a)\u2212V \u2217(s), and the motion model in [17], p(a|s) \u221d expQ\u2217(s,a).", "startOffset": 321, "endOffset": 325}, {"referenceID": 23, "context": "By approximating it with the generalized softmax function [24], the gradient of the objective function is:", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "For IRL problems, this work chooses max as the Bellman backup operator and a motion model p(a|s) based on the optimal Q function Q\u2217(s,a) [17]:", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "We first test the proposed method in a simulated environment, to compare its accuracy under different approximation functions, and then apply the proposed method to surgical data in JIGSAW dataset [1].", "startOffset": 197, "endOffset": 200}, {"referenceID": 0, "context": "We apply the proposed method to surgical robot operators in JIGSAW data set [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 19, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 19, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}], "year": 2017, "abstractText": "This works handles the inverse reinforcement learning problem in high-dimensional state spaces, which relies on an efficient solution of model-based high-dimensional reinforcement learning problems. To solve the computationally expensive reinforcement learning problems, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function based on the observed human actions for inverse reinforcement learning problems. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle high-dimensional even continuous state spaces efficiently. We test the proposed method in a simulated environment to show its accuracy, and three clinical tasks to show how it can be used to evaluate a doctor\u2019s proficiency.", "creator": "LaTeX with hyperref package"}}}