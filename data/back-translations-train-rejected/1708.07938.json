{"id": "1708.07938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2017", "title": "Deep Style Match for Complementary Recommendation", "abstract": "Humans develop a common sense of style compatibility between items based on their attributes. We seek to automatically answer questions like \"Does this shirt go well with that pair of jeans?\" In order to answer these kinds of questions, we attempt to model human sense of style compatibility in this paper. The basic assumption of our approach is that most of the important attributes for a product in an online store are included in its title description. Therefore it is feasible to learn style compatibility from these descriptions. We design a Siamese Convolutional Neural Network architecture and feed it with title pairs of items, which are either compatible or incompatible. Those pairs will be mapped from the original space of symbolic words into some embedded style space. Our approach takes only words as the input with few preprocessing and there is no laborious and expensive feature engineering.", "histories": [["v1", "Sat, 26 Aug 2017 06:09:53 GMT  (709kb,D)", "http://arxiv.org/abs/1708.07938v1", "Workshops at the Thirty-First AAAI Conference on Artificial Intelligence"]], "COMMENTS": "Workshops at the Thirty-First AAAI Conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kui zhao", "xia hu", "jiajun bu", "can wang"], "accepted": false, "id": "1708.07938"}, "pdf": {"name": "1708.07938.pdf", "metadata": {"source": "CRF", "title": "Deep Style Match for Complementary Recommendation", "authors": ["Kui Zhao", "Xia Hu", "Jiajun Bu", "Can Wang"], "emails": ["wcan}@zju.edu.cn", "hx@hznet.com.cn"], "sections": [{"heading": "Introduction", "text": "This type of compatibility information can be used in many commercial applications, such as recommending items based on what they have already purchased; or generating all of the purchase equipment (see Figure 1 for an example of clothing) for users who query certain items when they have compatibility relationships between items. To identify these compatibility relationships, existing methods such as frequent dismantling of items (Han, Pei, and Yin 2000) are used to automatically generate items by analyzing historical purchase patterns based on historical purchase records and finding new products that inevitably suffer from the \"cold start.\" Recently, we have McAuley et al. (McAuley et al. 2015) and Veit et al."}, {"heading": "Related Work", "text": "The early work can be traced back to common items such as \"beer and diaper,\" which sometimes have nothing to do with compatibility, and is also challenged by the \"cold start\" problem, which means that new products without historical records are invisible to the algorithm (Schein et al. 2002). Many approaches, such as content-based recommendations or social recommendations, are proposed to address this problem (see (Pazzani and Billsus 2007). Closely related to our work are (McAuley et al. 2015) and (Veit et al. 2015), in which McAuley et al. and Veit et al. attempt to learn similarities in clothing styles based on their appearance in pictures. However, our work differs from (McAuley et al. 2015) and (Veit et al al. 2015), in which we find two aspects in the description (1) that match."}, {"heading": "Problem formulation", "text": "We describe the problem here in a formal way: A query element Q = {q1, \u00b7 \u00b7, qm} and a candidate element C = {c1, \u00b7 \u00b7, cn} are given, with each query element Qi-Q coinciding with the compatibility judgments {yi1, \u00b7 \u00b7, yin}. The supplementary element Cj-C is labelled with Yij = 1 and otherwise Yij = 0. Our aim is to create a model to calculate the compatibility probability between Qi and CJ: P (y = 1 | qi, cj) = f (\u03c6 (qi, \u03b81), \u03c6 (cj, \u03b81), (1), where the sentence model consists of mapping a title sentence into a low-dimensional representation vector and the function f (\u00b7) calculates the compatibility probability between two elements in the style space. Parameter vectors No. 1 and No. 2 are learned in the training process."}, {"heading": "Style Match", "text": "The main building block of our approach is a sentence model based on CNN. This sentence model maps two title sentences from an item pair in parallel into low-dimensional vectors, which are then used to learn the compatibility between two items in the style space."}, {"heading": "Sentence model", "text": "In other words, we have to deal with a brief explanation of the main components in our context, in which it is a question of whether and in which context it is about the question whether and in which context it is about the question, whether and in which context it is about the question, whether and in which context it is about the question, whether and in which context it is about the question, whether and in which context it is about the question, whether and in which context and in which context it is about the question, whether and in which context it is about the question, whether and in which context it is about the question, whether and in which context it is about the question, whether and in which context and in which context and in which context it is about the question. In which context it is about the question, in which context it is asked, in which context and in which context it is asked and in which context it is questioned."}, {"heading": "Matching items", "text": "We calculate the probability of compatibility between two elements with the function f (\u00b7), which is a Siamese Convolutionary Neural Network, as shown in Figure 4."}, {"heading": "Same Parameter", "text": "The Siamese setup is introduced by Hadsell et al. (Hadsell, Chopra and LeCun 2006) and is widely used in remote sensing metrics. In designing the fusion part of our model, we have taken into account its scalability for big data scenarios, which is crucial for application in the real world. Style space.For two predefined items q and c, we calculate the probability of compatibility between them as follows: P (y = 1 | q, c) = \u03c3 (xTq Mxc + b) = 11 + e \u2212 (x T q Mxc + b), (12) where M-Rn \u00d7 n is a matrix and b is a scale. We designate M as a compatibility matrix and the space spanned by M as style space. After the transformation x-q = x-M, x-q represents the element that is most stylish."}, {"heading": "Recommendation", "text": "In Recommendations, we are normally given a query position Q = {q1, q2, \u00b7 \u00b7, qm} and a candidate position C = {c1, c2, \u00b7 \u00b7, cn} where the query position is relatively small and the candidate list is usually very large. For each query element Qi, we intend to query its K most complementary elements from the candidate list C and evaluate them from high compatibility to low compatibility. If the candidate list is very large, it is inefficient and even unacceptable to calculate and then sort the compatibility for all templates (Qi, CJ). Our approach can be easily expanded to handle these large data scenarios. Given two elements q and c, we first generate their display vectors xq and xc, respectively. Then their compatibility probability is calculated according to (12). We note that the flatland (\u00b7) function in (12) is a monotonical, increasing function that is a problem for learning and a problem for learning."}, {"heading": "Training", "text": "We train the model to maximize the probability of an observed relationship training setR, where rij-R: rij = {1, if the items i and j are compatible; 0, otherwise. (14) The maximization of the probability corresponds to minimizing the function of binary cross entropy loss: L = \u2212 \u2211 rij-R [rij log (p) + (1 \u2212 rij) log (1 \u2212 p)], (15) where p = P (y = 1 | i, j). The parameters to be optimized in our network are \u03b81, success2, mentioned above: \u03b81 = {W, F, B, H} and \u03b82 = {M, b}, (16) namely the word embedding matrix W, filter bank F, library B, dense matrix H, compatibility matrix M and compatibility bias b."}, {"heading": "Regularization", "text": "To alleviate the problem of overmatch, we use a popular and efficient regulation technique called dropout (Srivastava et al. 2014). Dropout is applied to the flat vector p (shown in (11)) before transforming it with the dense matrix H. A portion of the units in p are randomly removed by setting them to zero during the run-up phase, which is helpful to prevent co-adaptation of the characteristics. Dropout rate is a hyperparameter of the model."}, {"heading": "Hyper-parameters", "text": "The hyperparameters in our deep learning model are specified as follows: The embedding dimension is d = 100; the size of the filters when displaying the first order is m = 3; the number of maximum values selected by k-max pooling when displaying the first order is k = 5; the size of the filters when displaying the second order is m = 2; the number of maximum values selected by k-max pooling when displaying the second order isk = 3; the dimension of the vector used for displaying the set is n = 100; the failure rate is p = 0.2. In addition, there are K1 = 100 plots that are calculated in parallel when displaying the first order and K2 = 100 plots that are calculated in parallel when displaying the second order."}, {"heading": "Optimization", "text": "To optimize our network, we use the Stochastic Gradient Descent (SGD) algorithm with mixed mini-batches, the parameters are updated by the Adagrad Rule backpropagation framework (Duchi, Hazan and Singer 2011), the batch size is set to 256 and the network is trained for 20 epochs. Training progress is stopped early if there are no further updates for the best loss on the validation set for the last 5 epochs. We train our network on a GPU for acceleration, and a Python implementation with Keras1 powered by Theano (Bastien et al. 2012) can process 428k text pairs per minute on a single NVIDIA K2200 GPU."}, {"heading": "Experiments", "text": "We evaluate our method using two large datasets: a Chinese dataset from Taobao and an English dataset from Amazon."}, {"heading": "Datasets", "text": "Taobao. This dataset is collected by Taobao.com and provided by Alibaba Group2. It includes an apparel category and there are approximately 406k compatibility relationships to 61k articles. The compatibility relationships in this dataset are manually labeled by apparel experts.Amazon. This dataset is collected by Amazon.com and provided by (McAuley et al. 2015). Although it includes several categories, to examine the performance of our approach to both datasets, we focus primarily on the apparel category. In this category, there are approximately 12 million compatibility relationships to 662k articles. Unlike the Taobao dataset, the compatibility relationships in the Amazon dataset are not manually labeled. It is the copy data from Amazon Recommendations (Linden, Smith and York 2003)."}, {"heading": "Setup", "text": "We look at all positive relationships (compatibility) and generate random non-relationship distractors of the same size. That is, the ratio between positive and negative samples in the data set is 50: 50. Then we separate the entire data set into training, validation and test sets according to the ratios 80: 10: 10. Although we do not expect the overmatch to be a serious problem in our experiment with the large training set, we are still carefully tuning our model for validation to avoid overadjustment to the test set. We compare our approach to baselines from two aspects: visual and non-visual ones.1http: / keras.io 2http: / tianchi.aliyun.com / datalab / index.htmVisual Baseline in (Veit et al. 2015) as a visual comparison, as it is also in end-to-end mode."}, {"heading": "Results", "text": "The results clearly show that our approach exceeds all other baselines. The visual method collapses on the Taobao dataset because, unlike Amazon, Taobao is a consumer to consumer (C2C) platform and has little stringent requirements on the quality of images uploaded by users. The majority of images are like Figure 2, where the information is confused and confused with learning machines.In contrast, the title description is a highly condensed collection of more attributes in addition to occurrence with few noises. When using title descriptions, a simple method such as Naive Bayes on Bag of Words can achieve acceptable performance, such as Random Forest on Bag of Words can generate competitive results."}, {"heading": "Discussion", "text": "While the previous section focuses mainly on matching garments, we also train classifiers of the other twenty top-level categories from the Amazon dataset and present the results in Table 2. As you can see, we get good accuracy in predicting compatibility relationships in a variety of categories. In addition, we have also tried to train a single model to predict compatibility relationships across all categories. There does not seem to be a \"silver bullet\" and the result is unsatisfactory: the AUC score of this single model is only 0.694. Particularly interesting is the cross-category comparison. Our approach performs relatively poorly in the \"CDs & Records\" and \"Digital Music\" categories, as the content of music is too rich to describe it very clearly in a short title description. In contrast, the title description is long enough to clearly describe an item from the \"Musical Instruments\" category, and thus our approach performs very well. In short, the better titles can be achieved by describing the higher attributes in this category."}, {"heading": "Conclusions", "text": "In this paper, we present a novel approach to modeling human style compatibility between elements. The basic assumption of our approach is that most of the important attributes for a product in an online store are included in its title description. We design a Siamese Convolutional Neural Network architecture to map the title descriptions of an article pair from the original symbolic word space into an embedded style space. Compatibility probability between elements can then be calculated in the style space. Our approach takes only words as input with little pre-processing and does not require elaborate and expensive feature engineering. In addition, it can easily be extended to big data scenarios using KNN search techniques. Experiments on two large data sets confirm our assumption and show the ability to model the human sense of style compatibility. There are several interesting issues that need to be explored in our future work: (1) we want to use more complex sentence models, without compromising the simplicity of our text, and (2) the simplicity of our use of the image order is possible."}, {"heading": "Acknowledgments", "text": "We thank the Alibaba Group and Julian McAuley for providing the valuable data sets. This work is supported by the Zhejiang Provincial Natural Science Foundation of China (grant no. LZ13F020001), the Zhejiang Provincial Soft Science Project (grant no. 2015C25053), the National Science Foundation of China (grant no. 61173185, 61173186)."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "Bastien,? \\Q2012\\E", "shortCiteRegEx": "Bastien", "year": 2012}, {"title": "Latent dirichlet allocation. the Journal of machine Learning research 3:993\u20131022", "author": ["Ng Blei", "D.M. Jordan 2003] Blei", "A.Y. Ng", "M.I. Jordan"], "venue": null, "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Blunsom", "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["Weston Bordes", "A. Usunier 2014] Bordes", "J. Weston", "N. Usunier"], "venue": "In Machine Learning and Knowledge Discovery in Databases", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Hazan Duchi", "J. Singer 2011] Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12:2121\u20132159", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A noisy-channel approach to question answering", "author": ["Echihabi", "A. Marcu 2003] Echihabi", "D. Marcu"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Echihabi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Echihabi et al\\.", "year": 2003}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Chopra Hadsell", "R. LeCun 2006] Hadsell", "S. Chopra", "Y. LeCun"], "venue": "In Computer vision and pattern recognition,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Mining frequent patterns without candidate generation", "author": ["Pei Han", "J. Yin 2000] Han", "J. Pei", "Y. Yin"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "Han et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Han et al\\.", "year": 2000}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097\u20131105", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Image-based recommendations on styles and substitutes", "author": ["McAuley"], "venue": null, "citeRegEx": "McAuley,? \\Q2015\\E", "shortCiteRegEx": "McAuley", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Content-based recommendation systems", "author": ["Pazzani", "M.J. Billsus 2007] Pazzani", "D. Billsus"], "venue": null, "citeRegEx": "Pazzani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pazzani et al\\.", "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["Pedregosa"], "venue": null, "citeRegEx": "Pedregosa,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa", "year": 2011}, {"title": "Maximum inner-product search using cone trees", "author": ["Ram", "P. Gray 2012] Ram", "A.G. Gray"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Ram et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ram et al\\.", "year": 2012}, {"title": "Methods and metrics for coldstart recommendations", "author": ["Schein"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Schein,? \\Q2002\\E", "shortCiteRegEx": "Schein", "year": 2002}, {"title": "Learning binary codes for maximum inner product search", "author": ["Shen"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Shen,? \\Q2015\\E", "shortCiteRegEx": "Shen", "year": 2015}, {"title": "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)", "author": ["Shrivastava", "A. Li 2014] Shrivastava", "P. Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "Learning visual clothing style with heterogeneous dyadic co-occurrences", "author": ["Veit"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Veit,? \\Q2015\\E", "shortCiteRegEx": "Veit", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Humans develop a common sense of style compatibility between items based on their attributes. We seek to automatically answer questions like \u201cDoes this shirt go well with that pair of jeans?\u201d In order to answer these kinds of questions, we attempt to model human sense of style compatibility in this paper. The basic assumption of our approach is that most of the important attributes for a product in an online store are included in its title description. Therefore it is feasible to learn style compatibility from these descriptions. We design a Siamese Convolutional Neural Network architecture and feed it with title pairs of items, which are either compatible or incompatible. Those pairs will be mapped from the original space of symbolic words into some embedded style space. Our approach takes only words as the input with few preprocessing and there is no laborious and expensive feature engineering.", "creator": "LaTeX with hyperref package"}}}