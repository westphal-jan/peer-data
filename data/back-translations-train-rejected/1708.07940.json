{"id": "1708.07940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2017", "title": "Navigation Objects Extraction for Better Content Structure Understanding", "abstract": "Existing works for extracting navigation objects from webpages focus on navigation menus, so as to reveal the information architecture of the site. However, web 2.0 sites such as social networks, e-commerce portals etc. are making the understanding of the content structure in a web site increasingly difficult. Dynamic and personalized elements such as top stories, recommended list in a webpage are vital to the understanding of the dynamic nature of web 2.0 sites. To better understand the content structure in web 2.0 sites, in this paper we propose a new extraction method for navigation objects in a webpage. Our method will extract not only the static navigation menus, but also the dynamic and personalized page-specific navigation lists. Since the navigation objects in a webpage naturally come in blocks, we first cluster hyperlinks into different blocks by exploiting spatial locations of hyperlinks, the hierarchical structure of the DOM-tree and the hyperlink density. Then we identify navigation objects from those blocks using the SVM classifier with novel features such as anchor text lengths etc. Experiments on real-world data sets with webpages from various domains and styles verified the effectiveness of our method.", "histories": [["v1", "Sat, 26 Aug 2017 06:59:24 GMT  (943kb,D)", "http://arxiv.org/abs/1708.07940v1", "2017 IEEE/WIC/ACM International Conference on Web Intelligence (WI)"]], "COMMENTS": "2017 IEEE/WIC/ACM International Conference on Web Intelligence (WI)", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["kui zhao", "bangpeng li", "zilun peng", "jiajun bu", "can wang"], "accepted": false, "id": "1708.07940"}, "pdf": {"name": "1708.07940.pdf", "metadata": {"source": "CRF", "title": "Navigation Objects Extraction for Be\u0082er Content Structure Understanding", "authors": ["Kui Zhao", "Bangpeng Li", "Zilun Peng", "Jiajun Bu", "Can Wang"], "emails": ["zhaokui@zju.edu.cn", "bondlee@zju.edu.cn", "zilunpeng@gmail.com", "bjj@zju.edu.cn", "wcan@zju.edu.cn", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Information Systems \u2022 Data Mining; Data Extraction and Integration; \u2022 Computing Methodologies \u2192 Cluster Analysis; KEYWORDS Web Structure Mining; Information Extraction; Navigation ObjectsPermission to make digital or printed copies of all or part of this work for personal or commercial use is granted free of charge, provided that copies are not made or distributed for commercial or commercial purposes and that copies bear this notice and the full quote on the first page. Copyrights for components of this work belonging to others than ACM must be respected. Credit abstractions are permitted. Otherwise copying or re-publishing, posting on servers or redistributing on lists require prior permission and / or a fee. Permissions from permissions @ acm.org. WI '17, Leipzig, Germany \u00a9 2017 ACM. 978-1-4503-4951-17 / 08."}, {"heading": "1 INTRODUCTION", "text": "In fact, most of us are able to move to another world in which we are in the position in which we find ourselves."}, {"heading": "2 RELATEDWORK", "text": "Our work relates to areas of web structure mining and web information extraction. Web structure mining aims to study the hyperlink structure of the web. Some early work examined the structure of the web as a whole [4] [12] and uncovered the most important related components of the web. Others analyzed the general properties associated with web graphics, such as their diameter [1], the size and accessibility of information on web graphics [14], etc. PageRank [18] uses the linking information to learn the meaning of web pages and is widely used in modern search engines. Recent work on web structure mining focuses more on the local structures of web graphics. Ravi et al. [13] used the hierarchical structure of URLs to generate hierarchical website segmentation. The hierarchical structure of LURs was also used in many other works, such as the structure of the URLs [menu]."}, {"heading": "3 CLUSTERING HYPERLINKS", "text": "Our work is motivated by the observation that the navigation objects are naturally grouped in different hyperlink blocks according to their purposes. To illustrate our idea, we use as an example a typical website, the home page of Techweb1. As shown in Figure 2, the hyperlinks on the website are obviously grouped in different blocks with their respective visual characteristics."}, {"heading": "3.1 DOM-tree", "text": "Before combining hyperlinks into blocks in a web page, we analyze the web page into a DOM tree. Each web page corresponds to a DOM tree, in which detailed text, images and hyperlinks, etc. are leaf nodes. An example of the DOM tree is shown in Figure 3. e DOM tree at the beginning of Figure 3. The relationship between child nodes and parent nodes is derived from the HTML code in the top right, whose web page layout is shown in le.e DOM tree is a hierarchical structure and has three useful properties. Firstly, the relationship between child nodes and parent nodes affects their relationship in the web page layout, for example, in Figure 3. the nodes < p > and < imp > are child nodes of the node < div > cause text and image to be contained in the block corresponding to the block, div > in the web page layout. Secondly, the relative positions of sibling nodes are maintained when displayed on the web page < div > are displayed in the web page, and the image is displayed in the web page < div >."}, {"heading": "3.2 DOM-tree Distance", "text": "The central problem in clustering hyperlinks is to create a reasonable distance between them that matches their visual representation. The most intuitive choice is the Euclid distance between their sites on the website, as represented by browsers. However, obtaining these sites requires expensive computation costs. To solve this problem, we analyze the structure of the HTML code and use the DOM tree distance to approximate the distance between two hyperlinks. We first traverse the DOM tree of a given website with the first search order and index each node we come across, starting with 1. We calculate the DOM tree distance (DD) between hyperlinks l1 and l2 as a hyperlink gap (l1, l2)."}, {"heading": "3.3 Hyperlink Density", "text": "Another important observation is that a hyperlink block usually contains little text except the text in hyperlinks. Consequently, we interpret the hyperlink density (S) for a given layout block S, which consists of one or more sub-trees of a DOM tree: HD (S) = # {anchor text in S} + hangtext in S} + hillside text, (3) where # {anchor text in S} means the word number of the anchor text in all hyperlinks in S, # {all text in S} is the word number of all text in S and slope is the smoothing parameter to avoid a division of zero. We have set the number 10 \u2212 10 in all our experiments."}, {"heading": "3.4 Clustering on DOM-tree", "text": "When clustering hyperlinks to blocks, we take advantage of the hierarchical structure of the DOM tree and its properties. The complete algorithm of clustering hyperlinks to blocks on the DOM tree is represented in algorithm 1 with details.e The core of our algorithm is a recursive process. In the case of two given hyperlink blocks B1 and B2, where the hyperlink density of the potential hyperlink block, consisting of B1 and B2, is no lower than a given threshold hdt, we try to merge them into a hyperlink block. If the gap between the hyperlink blocks B1 and B2 is no higher than a given threshold, we merge them into a hyperlink block. We only try to merge hyperlink blocks that have the same threshold, because the hyperlink density of the potential hyperlink block, consisting of B1 and B2, is smaller than a threshold, we do not merge them into a single block."}, {"heading": "3.5 reshold", "text": "We use the gap threshold (for all children from left to right) and the hyperlink density threshold (from hdt) to control the results of clustering (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt. (from hdt). (from hdt. (from hdt). (from hdt). (from hdt. (from hdt). (from hdt.). (from hdt.). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from hdt). (from."}, {"heading": "4 CLASSIFYING HYPERLINK BLOCKS", "text": "We train an SVM class using an RBF kernel with some well-equipped functions for identifying navigation objects."}, {"heading": "4.1 Features", "text": "4.1.1 The number of hyperlinks. In our observation, the navigation object normally contains many hyperlinks, while other hyperlink blocks contain fewer hyperlinks. Therefore, the number of hyperlinks is a very useful feature for distinguishing between navigation object and non-navigation object. For a given hyperlink block Bi, we refer to the number of hyperlinks in it as # Bi.4.1.2 Text length in hyperlinks. e Length of the anchor text is also very useful. On the one hand, anchor texts in a navigation object are usually short, while hyperlinks in the main content usually contain relatively longer texts and hyperlinks in advertisements, etc. Therefore, the mean value of the text length in a navigation object is usually small, but not zero. On the other hand, the text in a navigation object is usually orderly and the variance of these text lengths is small. For a given hyperlink block, we refer to the mean value of the links and the text length of the B respectively."}, {"heading": "4.2 SVM Classi er", "text": "Support Vector Machine (SVM) is a famous model of supervised learning. In order to perform non-linear classes, we use the SVM class with RBF kernel [6]. When using SVM classes, we have to calculate the distance between two points. As the ranges of the different characteristics are relatively far apart, the characteristics are normalized so that each characteristic contributes approximately equal to the internal distance. Furthermore, normalization can also shorten the training time of the SVM classes [26]."}, {"heading": "5 EXPERIMENT", "text": "Experiments with real data sets show the effectiveness of our method."}, {"heading": "5.1 Date Set", "text": "In our experiments, we use data from two sources: (1) CleanEval dataset [3]; (2) data from MSS news pages [19]. CleanEval: CleanEval is a common competitive evaluation on the topic of cleaning any web page 2. It is a diverse dataset, with only a few web pages used on each page and the web pages using different styles and structures. In addition, this dataset has many web pages, including dynamic and page-dependent navigation elements. MSS: e datasets can be retrieved from Pasternaks and Roth's Repository3. The dataset includes 45 individual web pages, which are further divided into two non-overlapping groups. 1) The Big 5: Tribune, Freep, Ny Post, Suntimes and Techweb; 2) the Myriad 40: the web pages randomly selected from the Yahoo! directory."}, {"heading": "5.2 Performance Metrics", "text": "5.2.1 Clustering hyperlinks. e results of clustering hyperlinks are identi cations of multiple hyperlink blocks, and we have them with the hand-labeled Ground Truth.e first metric is the Adjusted Mutual Index (ARI) [9]. Rand Index (RI) is used to determine the match between the output results of clustering and the Ground Truth. e second metric is the Adjusted Mutual Information (AMI) [27]. Mutual Information (MI) is a symmetrical measurement used to quantify the statistical information that is shared between the output results of clustering and the Ground of Ground Truth [8]. AMI is an adjustment of the MI to the odds, it ranges from 0 to 1 and a greater value."}, {"heading": "5.3 Implementation Details", "text": "All programs were implemented in Python language using scikit-learn [20]. A he analyzes the HTML of a web page into a DOM tree, we treated all elements with the tag < a > as hyperlinks, including some buons and drop-down lists. We kept everything in a web page without any process to show that our method can handle the most noise in the web page. 5.3.1 Clustering Hyperlinks. To properly evaluate the performance of our method on clustering hyperlinks, we compared the performance of our method with several common clustering algorithms, including agglomeration, DBSCAN, K-Means and Spectral Clustering [24]. All algorithms use Equation (1) to determine the distance between two hyperlinks. e Dengorithmus initializes each hyperlink to a singleton cluster at the beginning."}, {"heading": "5.4 Results", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "5.5 Discussion", "text": "In order to demonstrate the generalization capability of our method, we continuously increase the percentage of hyperlinks in the training to be used from 1% to 100% and record the corresponding F1 values. If the percentage is less than 10%, the incremental value is 1% and the incremental value is otherwise 10%. We used CHD-HD to cluster hyperlinks in this experiment. We can observe that even if we use very few hyperlinks as training data, for example 5% of the total training, the performance of our method is very impressive. This means that our method has a strong generalization capability because it requires very little training data to perform very well. at brings great practicality to our method."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we propose a new method for extracting navigation objects on a web page to capture both the static directory structures and the dynamic content structures on a web page. Our method extracts not only the static navigation menus, but also the dynamic and personalized page-specific navigation lists, including the top stories and recommendation lists, etc. Based on the observation that hyperlinks on a web page are naturally arranged in specific blocks, we use a two-step procedure to extract navigation objects on a web page by clustering hyperlinks in multiple blocks on a web page and identifying these navigation objects from cluster results using the SVM class."}, {"heading": "ACKNOWLEDGMENTS", "text": "is supported by Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang Provincial So Science Project (grant no. 2015C25053), Zhejiang Provincial Natural Science Foundation of China (grant no. LZ13F020001), National Science Foundation of China (grant no. 61173185)."}], "references": [{"title": "Internet: Diameter of the world-wide web", "author": ["R\u00e9ka Albert", "Hawoong Jeong", "Albert-L\u00e1szl\u00f3 Barab\u00e1si"], "venue": "Nature 401,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Template detection via data mining and its applications", "author": ["Ziv Bar-Yossef", "Sridhar Rajagopalan"], "venue": "In Proceedings of the 11th international conference on World Wide Web", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Cleaneval: a Competition for Cleaning Web Pages", "author": ["Marco Baroni", "Francis Chantree", "Adam Kilgarri", "Serge Sharo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Graph structure in the web", "author": ["Andrei Broder", "Ravi Kumar", "Farzin Maghoul", "Prabhakar Raghavan", "Sridhar Rajagopalan", "Raymie Stata", "Andrew Tomkins", "Janet Wiener"], "venue": "Computer networks 33,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "VIPS: A visionbased page segmentation algorithm", "author": ["Deng Cai", "Shipeng Yu", "Ji-Rong Wen", "Wei-Ying Ma"], "venue": "Technical Report. Microso\u0089 technical report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Support vector machines for histogram-based image classi\u0080cation", "author": ["Olivier Chapelle", "Patrick Ha\u0082ner", "Vladimir N Vapnik"], "venue": "Neural Networks, IEEE Transactions on 10,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Template detection for large scale search engines", "author": ["Liang Chen", "Shaozhi Ye", "Xing Li"], "venue": "In Proceedings of the 2006 ACM symposium on Applied computing", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Comparing partitions", "author": ["Lawrence Hubert", "Phipps Arabie"], "venue": "Journal of classi\u0080cation 2,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Search result presentation: Supporting post-search navigation by integration of taxonomy data", "author": ["Ma\u008ahias Keller", "Patrick M\u00fchlschlegel", "Hannes Hartenstein"], "venue": "In Proceedings of the 22nd international conference on World Wide Web companion. International World Wide Web Conferences Steering Commi\u008aee,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "MenuMiner: revealing the information architecture of large web sites by analyzing maximal cliques", "author": ["Ma\u008ahias Keller", "Martin Nussbaumer"], "venue": "In Proceedings of the 21st international conference companion on World Wide Web", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Hierarchical topic segmentation of websites", "author": ["Ravi Kumar", "Kunal Punera", "Andrew Tomkins"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Accessibility of information on the web", "author": ["Steve Lawrence", "C Lee Giles"], "venue": "Nature 400,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Hierarchical web-page clustering via in-page and cross-page link structures. In Advances in Knowledge Discovery and Data", "author": ["Cindy Xide Lin", "Yintao Yu", "Jiawei Han", "Bing Liu"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Discovering informative content blocks from Web documents", "author": ["Shian-Hua Lin", "Jan-Ming Ho"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Extracting logical hierarchical structure of HTML documents based on headings", "author": ["Tomohiro Manabe", "Keishi Tajima"], "venue": "Proceedings of the VLDB Endowment 8,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "\u008ce PageRank citation ranking: bringing order to the Web", "author": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Extracting article text from the web with maximum subsequence segmentation", "author": ["Je\u0082 Pasternack", "Dan Roth"], "venue": "In Proceedings of the 18th international conference on World wide web", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Content extraction from html documents", "author": ["AFR Rahman", "H Alam", "R Hartono"], "venue": "In 1st Int. Workshop on Web Document Analysis", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["William M Rand"], "venue": "Journal of the American Statistical association", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1971}, {"title": "Link Structure Graphs for Representing and Analyzing Web Sites", "author": ["Eduarda Mendes Rodrigues", "Natasa Milic-Frayling", "Martin Hicks", "Gavin Smyth"], "venue": "Technical Report", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A survey of clustering algorithms. In Data mining and knowledge discovery handbook", "author": ["Lior Rokach"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Dom based content extraction via text density", "author": ["Fei Sun", "Dandan Song", "Lejian Liao"], "venue": "In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Feature scaling in support vector data descriptions", "author": ["David MJ Tax", "Robert PW Duin"], "venue": "Technical Report", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Nguyen Xuan Vinh", "Julien Epps", "James Bailey"], "venue": "\u008ae Journal of Machine Learning Research", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Can we learn a template-independent wrapper for news article extraction from a single training site", "author": ["J. Wang", "C. Chen", "C. Wang", "J. Pei", "J. Bu", "Z. Guan", "W.V. Zhang"], "venue": "In Proceedings of the 15th ACM SIGKDD", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "CETR: content extraction via tag ratios", "author": ["Tim Weninger", "William H Hsu", "Jiawei Han"], "venue": "In Proceedings of the 19th international conference on World wide web", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Automatic Web Content Extraction by Combination of Learning and Grouping", "author": ["Shanchan Wu", "Jerry Liu", "Jian Fan"], "venue": "In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi\u008aee,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Web site topic-hierarchy generation based on link structure", "author": ["Christopher C Yang", "Nan Liu"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Eliminating noisy information in web pages for data mining", "author": ["Lan Yi", "Bing Liu", "Xiaoli Li"], "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Template-independent news extraction based on visual consistency", "author": ["Shuyi Zheng", "Ruihua Song", "Ji-Rong Wen"], "venue": "In AAAI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "Existing works mainly extract navigation menus from webpages to reveal the content structure of the site [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "Many applications can be derived from the extracted content structure, including generating site map to improve information accessibility for disabled users, or providing content hierarchy in search results [10] etc.", "startOffset": 207, "endOffset": 211}, {"referenceID": 9, "context": "But their importance are neglected in existing works of web structure extraction, which mainly focus on extracting static web site structures such as the navigation menus[11], headings[17] etc.", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "But their importance are neglected in existing works of web structure extraction, which mainly focus on extracting static web site structures such as the navigation menus[11], headings[17] etc.", "startOffset": 184, "endOffset": 188}, {"referenceID": 3, "context": "Some early works studied the structure of the web at large [4][12] and uncover the major connected components of the web.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Others analyzed the generally properties related with the web graph, such as its diameter [1], size and accessibility of information on the web [14] etc.", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "Others analyzed the generally properties related with the web graph, such as its diameter [1], size and accessibility of information on the web [14] etc.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "PageRank [18] exploits the linkage information to learn the importance of webpages and becomes widely used in modern search engines.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "[13] used the hierarchical structure of URLs to generate hierarchical web site segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "\u008cough the hierarchical structure of URLs was also used in many other works, such as [31], the hierarchical structure of URLs does not re\u0083ect the web site organization accurately.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "[23] noticed that and thought navigation objects could re\u0083ect the web site structure be\u008aer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] also tried to use navigation menus to reveal the information architecture of web sites, but they extracted navigation menus in a very di\u0082erent way.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] clustered webpages, they considered parallel links which are siblings in the DOM-tree of a webpage and usually in the same navigation objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "[5] proposed a vision-based webpage segmentation algorithm named VIPS to divide a webpage into several blocks by its visual presentation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[33] presented a template-independent news extraction method based on visual consistency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] exploited more features about the relation between the news title and body by \u0080rstly extracting the title block and then extracting the body block.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[30] trained a machine learning model with multiple features generated by utilizing DOM-tree node properties and extracted content using this model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Two recent works, CETR [29] and CETD [25] address this issue by identifying regions with high text density, i.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Two recent works, CETR [29] and CETD [25] address this issue by identifying regions with high text density, i.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "We apply the re-implemented Gaussian smoothing [29] to the text lengths of hyperlinks in a DOM-tree to avoid sudden changes in the text lengths.", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "In order to perform non-linear classi\u0080cation, we use the SVM classi\u0080er with RBF kernel [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": "What\u2019s more, the normalization can also reduce the training time of SVM classi\u0080ers [26].", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "In our experiments we use data from two sources: (1) dataset from CleanEval[3]; (2) news site data from MSS[19].", "startOffset": 75, "endOffset": 78}, {"referenceID": 16, "context": "In our experiments we use data from two sources: (1) dataset from CleanEval[3]; (2) news site data from MSS[19].", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "\u008ce \u0080rst metric is the Adjusted Rand Index (ARI) [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 18, "context": "Rand Index (RI) is used to measure the agreement between the output results of clustering and the ground truth[22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "\u008ce second metric is the Adjusted Mutual Information (AMI) [27].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "In order to properly evaluate the performance of our method on clustering hyperlinks, we compared our method\u2019s performance with several common clustering algorithms, including Agglomeration, DBSCAN, K-Means and Spectral Clustering [24].", "startOffset": 231, "endOffset": 235}], "year": 2017, "abstractText": "Existing works for extracting navigation objects from webpages focus on navigation menus, so as to reveal the information architecture of the site. However, web 2.0 sites such as social networks, e-commerce portals etc. are making the understanding of the content structure in a web site increasingly di\u0081cult. Dynamic and personalized elements such as top stories, recommended list in a webpage are vital to the understanding of the dynamic nature of web 2.0 sites. To be\u008aer understand the content structure in web 2.0 sites, in this paper we propose a new extraction method for navigation objects in a webpage. Our method will extract not only the static navigation menus, but also the dynamic and personalized page-speci\u0080c navigation lists. Since the navigation objects in a webpage naturally come in blocks, we \u0080rst cluster hyperlinks into di\u0082erent blocks by exploiting spatial locations of hyperlinks, the hierarchical structure of the DOM-tree and the hyperlink density. \u008cen we identify navigation objects from those blocks using the SVM classi\u0080er with novel features such as anchor text lengths etc. Experiments on real-world data sets with webpages from various domains and styles veri\u0080ed the e\u0082ectiveness of our method.", "creator": "LaTeX with hyperref package"}}}