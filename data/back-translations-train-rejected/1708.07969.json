{"id": "1708.07969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2017", "title": "3D Object Reconstruction from a Single Depth View with Adversarial Learning", "abstract": "In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at:", "histories": [["v1", "Sat, 26 Aug 2017 13:46:21 GMT  (2598kb,D)", "http://arxiv.org/abs/1708.07969v1", "ICCV Workshops 2017"]], "COMMENTS": "ICCV Workshops 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.RO", "authors": ["bo yang", "hongkai wen", "sen wang", "ronald clark", "rew markham", "niki trigoni"], "accepted": false, "id": "1708.07969"}, "pdf": {"name": "1708.07969.pdf", "metadata": {"source": "CRF", "title": "3D Object Reconstruction from a Single Depth View with Adversarial Learning", "authors": ["Bo Yang", "Hongkai Wen", "Sen Wang", "Andrew Markham", "Niki Trigoni"], "emails": ["bo.yang@cs.ox.ac.uk", "hongkai.wen@dcs.warwick.ac.uk", "s.wang@hw.ac.uk", "ronald.clark@imperial.ac.uk", "andrew.markham@cs.ox.ac.uk", "niki.trigoni@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "Unlike the existing work, which usually requires multiple views of the same object or class label to restore the full 3D geometry, the proposed 3D RecGAN only takes the voxel grid representation of a depth view of the object as input and is able to generate the full 3D allocation grid by filling in the closed / missing areas. The basic idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Network (GAN) to derive precise and fine-grained 3D structures from objects in the high-dimensional voxel space. Extensive experiments with large synthetic data sets show that the proposed 3D RecGAN significantly exceeds the state of the art in reconstructing 3D objects in a single view and is capable of reconstructing invisible object types. Our code and data are available at: https / gicom / Yangthub / 3D / RecAN-78."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Related Work", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3. 3D-RecGAN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Overview", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "3.2. Architecture", "text": "It consists of two main networks: the generator as the top block and the discriminator as the bottom block. The generator is based on autoencoders with skip connections between encoders and decoders. Unlike real GAN generators, which generate data from any latent distributions, our 3D generator synthesizes data from the latent distribution of 2.5D views. In particular, the encoder has five convolution-related layers, each of which has a bank of 4x4 filters with strokes of 1x1x1 followed by a leaky reconstruction of 2.5D views."}, {"heading": "3.3. Objectives", "text": "The objective function of our 3D RecGAN includes two main parts: an object reconstruction loss Lae for autoencoder-based generators; the objective function Lgan for conditional GAN. (1) Lae For the generator, inspired by the work [4], we use modified binary cross-entropy loss functions instead of the standard version. In this respect, we impose a high penalty for false positive than false negative results. In particular, an excess weight is associated with false positive and false negative results, with (1 \u2212 \u03b1) false negative results as shown in the following equation 2nd Lae. (1 \u2212 \u03b1) We impose a high penalty for false positive than false negative results. (1 \u2212 \u03b1) and a low penalty for false positive results."}, {"heading": "3.4. Training", "text": "In order to optimize both the generator and the discriminator, we switch between a gradient descent on the discriminator and a step on the generator. In the WGAN-GP, \u03bb is specified as 10 for the gradient penalty as in [17]. \u03b1 ends as 0.85 for our modified cross-entropy loss function, \u03b2 is 0.05 for the joint loss function Lg. The Adam solver [26] is specified for both the discriminator and the generator with batch size 8. The other three Adam parameters are specified as default values, i.e. \u03b21 is 0.9, \u03b22 is 0.999 and is 1e-8. The learning rate is set to 0.0005 in the first epoch and drops to 0.0001 in the following epochs. Since we do not use dropout or batch normalization, the test phase is exactly the same as the training phase without reconfiguring the network parameters."}, {"heading": "3.5. Data Synthesis", "text": "Obtaining a large amount of training data is an obstacle to the task of 3D dense reconstruction from a single view. Existing real RGB-D data sets for surface reconstruction suffer from occlusions and lack of data, and there is no corresponding complete 3D structure for each individual view. The most recent work 3D-EPN [7] synthesizes data for 3D object completion, but its map coding scheme is the complicated TSDF that differs from our network requirements. To address this problem, we use the ModelNet40 [61] database to generate a large amount of training and test data with synthetically rendered depth images and the corresponding complete 3D form truth. Specifically, a subset of object categories is selected for our experiments. For each category, we generate training data from about 200 CAD models in the train folder, while the test data from about 20 CAD models are synthesized in the test folder."}, {"heading": "4. Evaluation", "text": "In this section, we evaluate our 3D RecGAN against alternative approaches and an ablation study to fully investigate the proposed network."}, {"heading": "4.1. Metrics", "text": "We use two metrics to assess the performance of 3D reconstruction: The first metric is voxel intersection overunion (IoU) between a predicted 3D voxel grid and its ground voxel grid. It is formally defined as: IoU = \u2211 ijk [I (y'ijk > p)] \u2211 ijk [I (I (y'ijk > p) + I (yijk)], where I () is an indicator function, (i, j, k) the index of a voxel in three dimensions, y'ijk the predicted value for (i, j, k) voxel, yijk the actual value of the ground at (i, j, k) and p the threshold for voxelization. In all our experiments p is set as 0.5. If the predicted value is above 0.5, it is more likely that it will be occupied below the probable aspect \u2212 the higher the IoU value, the better the 3D model is."}, {"heading": "4.2. Comparison", "text": "We compare two alternative reconstruction methods. The first is the well-known traditional reconstruction of the surface of Poisson [23] [24], which is largely used to complete surfaces on dense dot clouds. The second is the approach proposed by Varley et al. in [58], which is most similar to our approach to input and output data encoding and the 3D completion task. It has encouraging reconstruction performance due to its two completely interconnected layers [30] in the model, but it is unable to handle higher resolutions, and it has less universality for shape completion. We also compare against the auto-encoder alone in our network, i.e. without the GAN, which is briefly referred to as 3D-RecAE. (1) Results per category The networks are trained separately and tested on three different categories, using the same network configurations. Table 1 shows the IoU and CE results, and Figure 6 compares qualitative reconstruction results from different approaches."}, {"heading": "5. Conclusion", "text": "In this work, we proposed a novel 3DRecGAN framework that reconstructs the complete 3D structure of an object from an arbitrary depth view. By utilizing the generalization capacities of autoencoders and generative networks, our 3D RecGAN predicts precise 3D structures with fine details, exceeding the traditional Poisson algorithm and the method used in Varley et al. [58] in completing individual object categories in one view. Finally, we tested the network's ability to perform reconstructions in multiple categories without providing any object class designations during training or testing, and it showed that our network is capable of predicting satisfactory 3D shapes. Finally, we tested the network's reconstruction performance in invisible object categories. We showed that the proposed approach can still predict plausible 3D shapes even in very difficult cases. This confirms that our network has the ability to easily adapt the general function of 3D-based training sets to just one of objects."}], "references": [{"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "ICML", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Lucas-Kanade 20 Years On : A Unifying Framework : Part 1", "author": ["S. Baker", "I. Matthews"], "venue": "International Journal of Computer Vision, 56(3):221\u2013255", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Face Recognition based on Fitting a 3D Morphable Model", "author": ["V. Blanz", "T.Vetter"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Generative and Discriminative Voxel Modeling with Convolutional Neural Networks", "author": ["A. Brock", "T. Lim", "J.M. Ritchie", "N. Weston"], "venue": "arXiv", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "3D- R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction", "author": ["C.B. Choy", "D. Xu", "J. Gwak", "K. Chen", "S. Savarese"], "venue": "ECCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis", "author": ["A. Dai", "C.R. Qi", "M. Nie\u00dfner"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Shape from a Low Number of Silhouettes", "author": ["X. Di", "R. Dahyot", "M. Prasad"], "venue": "ECCV", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end 3D face reconstruction with deep neural networks", "author": ["P. Dou", "S.K. Shah", "I.A. Kakadiaris"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "A Point Set Generation Network for 3D Object Reconstruction from a Single Image", "author": ["H. Fan", "H. Su", "L. Guibas"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Structured Prediction of Unobserved Voxels From a Single Depth Image", "author": ["M. Firman", "O.M. Aodha", "S. Julier", "G.J. Brostow"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "3D Shape Induction from 2D Views of Multiple Objects", "author": ["M. Gadelha", "S. Maji", "R. Wang"], "venue": "arXiv", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a Predictable and Generative Vector Representation for Objects", "author": ["R. Girdhar", "D.F. Fouhey", "M. Rodriguez", "A. Gupta"], "venue": "ECCV", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative Adversarial Nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Disentangled Representations for Volumetric Reconstruction", "author": ["E. Grant", "P. Kohli", "M.V. Gerven"], "venue": "ECCV Workshops", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to Reconstruct 3D Structures for Occupancy Mapping", "author": ["V. Guizilini", "F. Ramos"], "venue": "RSS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Improved Training of Wasserstein GANs", "author": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "V. Dumoulin", "A. Courville"], "venue": "arXiv", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Weakly Supervised Generative Adversarial Networks for 3D Reconstruction", "author": ["J. Gwak", "C.B. Choy", "A. Garg", "M. Chandraker", "S. Savarese"], "venue": "arXiv", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Multiple View Geometry in Computer Vision", "author": ["R. Hartley", "A. Zisserman"], "venue": "Cambridge University Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Controllable Text Generation", "author": ["Z. Hu", "Z. Yang", "X. Liang", "R. Salakhutdinov", "E.P. Xing"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Analysis and synthesis of 3D shape families via deep-learned generative models of surfaces", "author": ["H. Huang", "E. Kalogerakis", "B. Marlin"], "venue": "Computer Graphics Forum, 34(5):25\u201338", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Categoryspecific object reconstruction from a single image", "author": ["A. Kar", "S. Tulsiani", "J. Carreira", "J. Malik"], "venue": "CVPR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Poisson Surface Reconstruction", "author": ["M. Kazhdan", "M. Bolitho", "H. Hoppe"], "venue": "Symposium on Geometry Processing", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Screened poisson surface reconstruction", "author": ["M. Kazhdan", "H. Hoppe"], "venue": "ACM Transactions on Graphics, 32(3):1\u201313", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Acquiring 3D Indoor Environments with Variability and Repetition", "author": ["Y.M. Kim", "N.J. Mitra", "D.-M. Yan", "L. Guibas"], "venue": "ACM Transactions on Graphics, 31(6)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Convolutional Inverse Graphics Network", "author": ["T.D. Kulkarni", "W.F. Whitney", "P. Kohli", "J.B. Tenenbaum"], "venue": "NIPS", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Database-Assisted Object Retrieval for Real-Time 3D Reconstruction", "author": ["Y. Li", "A. Dai", "L. Guibas", "M. Nie\u00dfner"], "venue": "Computer Graphics Forum, 34(2):435\u2013446", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks", "author": ["Z. Lun", "M. Gadelha", "E. Kalogerakis", "S. Maji", "R. Wang"], "venue": "arXiv", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Object detection and classification from large-scale cluttered indoor scans", "author": ["O. Mattausch", "D. Panozzo", "C. Mura", "O. Sorkine-Hornung", "R. Pajarola"], "venue": "Computer Graphics Forum, 33(2):11\u201321", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional Generative Adversarial Nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Partial and Approximate Symmetry Detection for 3D Geometry", "author": ["N.J. Mitra", "L.J. Guibas", "M. Pauly"], "venue": "SIGGRAPH", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "RAPter: Rebuilding Man-made Scenes with Regular Arrangements of Planes", "author": ["A. Monszpart", "N. Mellado", "G.J. Brostow", "N.J. Mitra"], "venue": "ACM Transactions on Graphics, 34(4):1\u201312", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "A Search-Classify Approach for Cluttered Indoor Scene Understanding", "author": ["L. Nan", "K. Xie", "A. Sharf"], "venue": "ACM Transactions on Graphics, 31(6):1\u201310", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "KinectFusion: Real-time dense surface mapping and tracking", "author": ["R.A. Newcombe", "S. Izadi", "O. Hilliges", "D. Molyneaux", "D. Kim", "A.J. Davison", "P. Kohli", "J. Shotton", "S. Hodges", "A. Fitzgibbon"], "venue": "ISMAR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "DTAM: Dense Tracking and Mapping in Real-time", "author": ["R.A. Newcombe", "S.J. Lovegrove", "A.J. Davision"], "venue": "ICCV", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time 3D reconstruction at scale using voxel hashing", "author": ["M. Nie\u00dfner", "M. Zollh\u00f6fer", "S. Izadi", "M. Stamminger"], "venue": "ACM Transactions on Graphics, 32(6):1\u201311", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering structural regularity in 3D geometry", "author": ["M. Pauly", "N.J. Mitra", "J. Wallner", "H. Pottmann", "L.J. Guibas"], "venue": "ACM Transactions on Graphics, 27(3):1", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "ICLR", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised Learning of 3D Structure from Images", "author": ["D.J. Rezende", "S.M.A. Eslami", "S. Mohamed", "P. Battaglia", "M. Jaderberg", "N. Heess"], "venue": "NIPS", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Oct- NetFusion: Learning Depth Fusion from Data", "author": ["G. Riegler", "A.O. Ulusoy", "H. Bischof", "A. Geiger"], "venue": "arXiv", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "U-Net : Convolutional Networks for Biomedical Image Segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "MIC- CAI", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "An interactive approach to semantic modeling of indoor scenes with an RGBD camera", "author": ["T. Shao", "W. Xu", "K. Zhou", "J. Wang", "D. Li", "B. Guo"], "venue": "ACM Transactions on Graphics, 31(6):1\u201311", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "VConv-DAE : Deep Volumetric Shape Learning Without Object Labels", "author": ["A. Sharma", "O. Grau", "M. Fritz"], "venue": "ECCV", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Data-driven contextual modeling for 3d scene understanding", "author": ["Y. Shi", "P. Long", "K. Xu", "H. Huang", "Y. Xiong"], "venue": "Computers & Graphics, 55:55\u201367", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Approximate Symmetry Detection in Partial 3D Meshes", "author": ["I. Sipiran", "R. Gregor", "T. Schreck"], "venue": "Computer Graphics Forum, 33(7):131\u2013140", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Synthesizing 3D Shapes via Modeling Multi- View Depth Maps and Silhouettes with Deep Generative Networks", "author": ["A.A. Soltani", "H. Huang", "J. Wu", "T.D. Kulkarni", "J.B. Tenenbaum"], "venue": "CVPR", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2017}, {"title": "Semantic Scene Completion from a Single Depth Image", "author": ["S. Song", "F. Yu", "A. Zeng", "A.X. Chang", "M. Savva", "T. Funkhouser"], "venue": "CVPR", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2017}, {"title": "Least-squares Meshes", "author": ["O. Sorkine", "D. Cohen-Or"], "venue": "SMI", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "A Symmetry Prior for Convex Variational 3D Reconstruction", "author": ["P. Speciale", "M.R. Oswald", "A. Cohen", "M. Pollefeys"], "venue": "ECCV", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Large- Scale Multi-Resolution Surface Reconstruction from RGB- D Sequences", "author": ["F. Steinbrucker", "C. Kerl", "J. Sturm", "D. Cremers"], "venue": "ICCV", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-view 3D models from single images with a convolutional network", "author": ["M. Tatarchenko", "A. Dosovitskiy", "T. Brox"], "venue": "ECCV", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Octree Generating Networks : Efficient Convolutional Architectures for High-resolution 3D Outputs", "author": ["M. Tatarchenko", "A. Dosovitskiy", "T. Brox"], "venue": "ICCV", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2017}, {"title": "Shape from symmetry", "author": ["S. Thrun", "B. Wegbreit"], "venue": "ICCV", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiview Supervision for Single-view Reconstruction via Differentiable Ray Consistency", "author": ["S. Tulsiani", "T. Zhou", "A.A. Efros", "J. Malik"], "venue": "CVPR", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2017}, {"title": "Shape Completion Enabled Robotic Grasping", "author": ["J. Varley", "C. Dechant", "A. Richardson", "J. Ruales", "P. Allen"], "venue": "IROS", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2017}, {"title": "Single Image 3D Interpreter Network", "author": ["J. Wu", "T. Xue", "J.J. Lim", "Y. Tian", "J.B. Tenenbaum", "A. Torralba", "W.T. Freeman"], "venue": "ECCV", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling", "author": ["J. Wu", "C. Zhang", "T. Xue", "W.T. Freeman", "J.B. Tenenbaum"], "venue": "NIPS", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "3D ShapeNets : A Deep Representation for Volumetric Shapes", "author": ["Z. Wu", "S. Song", "A. Khosla", "F. Yu", "L. Zhang", "X. Tang", "J. Xiao"], "venue": "CVPR", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision", "author": ["X. Yan", "J. Yang", "E. Yumer", "Y. Guo", "H. Lee"], "venue": "NIPS", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 45, "context": "The ability to reconstruct the complete and accurate 3D geometry of an object is essential for a broad spectrum of scenarios, from AR/VR applications [46] and semantic understanding, to robot grasping [58] and obstacle avoidance.", "startOffset": 150, "endOffset": 154}, {"referenceID": 57, "context": "The ability to reconstruct the complete and accurate 3D geometry of an object is essential for a broad spectrum of scenarios, from AR/VR applications [46] and semantic understanding, to robot grasping [58] and obstacle avoidance.", "startOffset": 201, "endOffset": 205}, {"referenceID": 36, "context": "Most of those approaches typically sample multiple depth images from different views of the object to create the complete 3D structure [37] [39] [53].", "startOffset": 135, "endOffset": 139}, {"referenceID": 38, "context": "Most of those approaches typically sample multiple depth images from different views of the object to create the complete 3D structure [37] [39] [53].", "startOffset": 140, "endOffset": 144}, {"referenceID": 52, "context": "Most of those approaches typically sample multiple depth images from different views of the object to create the complete 3D structure [37] [39] [53].", "startOffset": 145, "endOffset": 149}, {"referenceID": 50, "context": "Traditional reconstruction approaches typically use interpolation techniques such as plane fitting [51] or Poisson surface estimation [23] [24] to estimate the underlying 3D structure.", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Traditional reconstruction approaches typically use interpolation techniques such as plane fitting [51] or Poisson surface estimation [23] [24] to estimate the underlying 3D structure.", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "Traditional reconstruction approaches typically use interpolation techniques such as plane fitting [51] or Poisson surface estimation [23] [24] to estimate the underlying 3D structure.", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 48, "endOffset": 51}, {"referenceID": 60, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 57, "endOffset": 60}, {"referenceID": 57, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 61, "endOffset": 65}, {"referenceID": 61, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 167, "endOffset": 171}, {"referenceID": 26, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 40, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 308, "endOffset": 312}, {"referenceID": 19, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 313, "endOffset": 317}, {"referenceID": 4, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 345, "endOffset": 348}, {"referenceID": 27, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 349, "endOffset": 353}, {"referenceID": 12, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 41, "endOffset": 45}, {"referenceID": 59, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "5D view, adversarial training of our model is based on conditional GAN [33] instead of random guessing.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 75, "endOffset": 79}, {"referenceID": 47, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 80, "endOffset": 84}, {"referenceID": 51, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 85, "endOffset": 89}, {"referenceID": 55, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 29, "endOffset": 33}, {"referenceID": 28, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 34, "endOffset": 38}, {"referenceID": 35, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 39, "endOffset": 43}, {"referenceID": 44, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 44, "endOffset": 48}, {"referenceID": 46, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "Traditionally, 3D dense recovery requires a collection of images [19].", "startOffset": 65, "endOffset": 69}, {"referenceID": 37, "context": "Geometric shape is recovered by dense feature extraction and matching [38], or by directly minimizing reprojection errors [2].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Geometric shape is recovered by dense feature extraction and matching [38], or by directly minimizing reprojection errors [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 11, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 15, "endOffset": 19}, {"referenceID": 56, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 20, "endOffset": 24}, {"referenceID": 53, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 34, "endOffset": 37}, {"referenceID": 42, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 38, "endOffset": 42}, {"referenceID": 48, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "For example, morphable 3D models are exploited for face recovery [3] [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "For example, morphable 3D models are exploited for face recovery [3] [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "This concept was extended to reconstruct simple objects in [22].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "[11] trained a random decision forest to predict unknown voxels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "3D ShapeNets [61] is amongst the early work using deep networks to predict multiple 3D solutions from a single partial view.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "[10] also adopted a similar strategy to generate multiple plausible 3D point clouds from a single image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "However, that strategy is significantly less efficient than directly training an end-toend predictor [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 45, "context": "VConv-DAE [46] can be used for shape completion, but it is originally designed for shape denoising rather than partial range scans.", "startOffset": 10, "endOffset": 14}, {"referenceID": 58, "context": "proposed 3D-INN [59] to estimate a 3D skeleton from single image, which is far from recovering an accurate and complete 3D structure.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "developed 3D-EPN [7] to complete an object\u2019s shape using deep nets to both predict a 32 occupancy grid and then synthesize a higher resolution model based on a shape database.", "startOffset": 17, "endOffset": 20}, {"referenceID": 61, "context": "Perspective Transformer Nets [62] and the recent WS-GAN [18] are introduced to learn 3D object structures up to a 32 resolution occupancy grid.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "Perspective Transformer Nets [62] and the recent WS-GAN [18] are introduced to learn 3D object structures up to a 32 resolution occupancy grid.", "startOffset": 56, "endOffset": 60}, {"referenceID": 61, "context": "In addition, the training procedure of [62] is twostage, rather than end-to-end.", "startOffset": 39, "endOffset": 43}, {"referenceID": 49, "context": "[50] proposed SSCNet for both 3D scene completion and semantic label prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[55] [43] use tree structures, while [16] applies Hibert Maps for 3D map representation to recover the 3D shape, thus being able to produce a relatively higher resolution of 3D shape.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[55] [43] use tree structures, while [16] applies Hibert Maps for 3D map representation to recover the 3D shape, thus being able to produce a relatively higher resolution of 3D shape.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "[55] [43] use tree structures, while [16] applies Hibert Maps for 3D map representation to recover the 3D shape, thus being able to produce a relatively higher resolution of 3D shape.", "startOffset": 37, "endOffset": 41}, {"referenceID": 57, "context": "[58] provides an architecture for 3D shape completion from a single depth view, producing an up to 40 occupancy grid.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "To generate ground true training and evaluation pairs, we virtually scan 3D objects from ModelNet40 [61].", "startOffset": 100, "endOffset": 104}, {"referenceID": 43, "context": "The first part of our network loosely follows the idea of an autoencoder with U-net architecture [44].", "startOffset": 97, "endOffset": 101}, {"referenceID": 32, "context": "The autoencoder serves as a generator which is followed by a conditional discriminator [33] for adversarial learning.", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "In the original GAN framework [14], the task of discriminator is to simply classify real and fake input, but its Jensen-Shannon divergence-based loss function is difficult to converge.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "The recent WGAN [1] leverages Wasserstein distance with weight clipping as a loss function to stabilize the training procedure, whilst the extended work WGAN-GP [17] further improves the training process using a gradient penalty with respect to its input.", "startOffset": 16, "endOffset": 19}, {"referenceID": 16, "context": "The recent WGAN [1] leverages Wasserstein distance with weight clipping as a loss function to stabilize the training procedure, whilst the extended work WGAN-GP [17] further improves the training process using a gradient penalty with respect to its input.", "startOffset": 161, "endOffset": 165}, {"referenceID": 3, "context": "(1) Lae For the generator, inspired by the work [4], we use modified binary cross-entropy loss function instead of the standard version.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "Detailed definitions and derivation of the loss functions can be found in [1] [17], but we modify them for our conditional GAN settings.", "startOffset": 74, "endOffset": 77}, {"referenceID": 16, "context": "Detailed definitions and derivation of the loss functions can be found in [1] [17], but we modify them for our conditional GAN settings.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "where \u0177 = x+(1\u2212 )y , \u223c U [0, 1].", "startOffset": 25, "endOffset": 31}, {"referenceID": 16, "context": "For the WGAN-GP, \u03bb is set as 10 for gradient penalty as in [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "The Adam solver [26] is applied for both discriminator and generator with batch size of 8.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "The recent work 3D-EPN [7] synthesizes data for 3D object completion, but their map encoding scheme is the complicated TSDF which is different from our network requirement.", "startOffset": 23, "endOffset": 26}, {"referenceID": 60, "context": "To tackle this issue, we use the ModelNet40 [61] database to generate a large amount of training and testing data with synthetically rendered depth images and the corresponding complete 3D shape ground truth.", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "The first is the well-known traditional Poisson surface reconstruction [23] [24], which is mostly used for completing surfaces on dense point clouds.", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "The first is the well-known traditional Poisson surface reconstruction [23] [24], which is mostly used for completing surfaces on dense point clouds.", "startOffset": 76, "endOffset": 80}, {"referenceID": 57, "context": "in [58], which is most similar to our approach in terms of input and output data encoding and the 3D completion task.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "It has encouraging reconstruction performance because of its two fully connected layers [30] in the model, but it is unable to deal with higher resolutions and it has less generality for shape completion.", "startOffset": 88, "endOffset": 92}, {"referenceID": 57, "context": "Varley [58] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 57, "context": "Varley [58] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 57, "context": "Varley [58] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 57, "context": "[58] in single-view shape completion for individual object category.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and finegrained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at: https://github. com/Yang7879/3D-RecGAN .", "creator": "LaTeX with hyperref package"}}}