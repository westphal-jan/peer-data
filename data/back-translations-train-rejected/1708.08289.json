{"id": "1708.08289", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Generating Query Suggestions to Support Task-Based Search", "abstract": "We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the first place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model.", "histories": [["v1", "Mon, 28 Aug 2017 12:44:14 GMT  (381kb,D)", "http://arxiv.org/abs/1708.08289v1", "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17), 2017"]], "COMMENTS": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17), 2017", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["dar\\'io garigliotti", "krisztian balog"], "accepted": false, "id": "1708.08289"}, "pdf": {"name": "1708.08289.pdf", "metadata": {"source": "CRF", "title": "Generating\u0083ery Suggestions to Support Task-Based Search", "authors": ["Dar\u0131\u0301o Gariglio\u008ai", "Krisztian Balog"], "emails": ["dario.gariglioi@uis.no", "krisztian.balog@uis.no", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Information systems \u2192 ery suggestion; KEYWORDS ery suggestions, task-based search, supporting search tasks ACM Reference format: Dar\u0131 \u0301 o Gariglio i and Krisztian Balog. 2017. Generating ery Suggestions to Support Task-Based Search. In Proceedings of SIGIR '17, 7-11 August 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: h p: / / dx.doi.org / 10.1145 / 3077136.3080745"}, {"heading": "1 INTRODUCTION", "text": "In fact, we are concentrating in a way that aims to support people from all over the world: the search for people who are able to realize themselves, and for people who are able to realize themselves, and for people who are able to realize themselves, and for people who are able to realize themselves, and for people who are able to realize themselves, \"he told the Deutsche Presse-Agentur.\" I don't think we are able to realize ourselves. \""}, {"heading": "2 RELATEDWORK", "text": "A major focus was on logged studies, including task identification and segmentation of queries into tasks [2, 14] and task-based search sessions to understand query 1h p: / / www.wikihow.com / ar Xiv: 170 8.08 289v 1 [cs.I R] 28 Aug 201 7reformulations [10] or search paths [19]. Another topic is support for exploratory search, where users pursue an information goal to learn or discover more about a particular topic. Recent research in this area has focused on the importance of support interfaces [1, 3, 17]. Our main interest is in query suggestions as an excellent support mechanism. Most of the related work uses large-scale query protocols. For example, Craswell and Szummer [6] perform a random query."}, {"heading": "3 PROBLEM STATEMENT", "text": "For an initial query q0, the goal is to return a ranking of query suggestions < q1,... qn > that cover all possible subtasks related to the task the user is trying to accomplish. In addition to the initial query string, the companies mentioned therein are also provided (identified by their freebase IDs), subtasks for the query \"low wedding budget\" include (but not limited to) \"purchase a used wedding dress,\" \"cheap wedding cake\" and \"make your own invitations.\" These subtasks were manually identified by the track organizers based on information extracted from the logs of a commercial search engine. Suggested keyphrases are evaluated in relation to each subtask on a three-tiered scale (non-relevant, relevant and highly relevant)."}, {"heading": "4 APPROACH", "text": "We now present our approach to generating query suggestions. As Figure 2 shows, we source keyphrases from a variety of sources and use them to build a ranking of query suggestions."}, {"heading": "4.1 Generative Modeling Framework", "text": "We introduce a generative probability model to evaluate the candidate query suggestions according to P (q | q0), i.e. the probability that a query proposal q will be evaluated by the input query q0.Formally: P (q | q0) = \u2211 s P (q | q0, s) P (s | q0) = \u2211 s (\u2211 d P (q | q0, s, d) P (d | q0, s) P (s | q0) = \u2211 s (\u2211 d (\u2211 k P (q | q0, s, k) P (k | s, d) P (d | q0, s)) P (s | q0).is model consists of four components: (i) P (s | q0) expresses the meaning of a specific information source s for the input query q0; (ii) P (qk (d) of the source statement q0, the source statement qk (v), the source statement qv, the source query, the source statement qv (qv)."}, {"heading": "4.2 Source Importance", "text": "We collect relevant information from four types of sources: Query Suggestions (QA), Web Search Snippets (WS), Web Search Documents (WD), and WikiHow (WH). For the first three source types, we use three different web search engines (Google, Bing, and DuckDuckGo), making a total of 10 individual sources available. In this work, we assume conditional independence between a source s and the original query q0, i.e. P (s | q0) = P (s)."}, {"heading": "4.3 Document Importance", "text": "From each source s we obtain the top K documents (K = 10) for query q0. We propose two ways to model the meaning of a document d that comes from s: (i) uniformly and (ii) inversely proportional to rank d among the top K documents, i.e.: P (d | q0, s) = K \u2212 r + 1 \u2211 K i = 1 K \u2212 i + 1 = K \u2212 r + 1 K (K + 1) / 2, where r is the ranking position of d (r [1.. K])."}, {"heading": "4.4 Keyphrase Relevance", "text": "We extract keyphrases from each document using an automatic keyphrase extraction algorithm. Specifically, we use the RAKE Keyword Extraction System [15]. For each keyphrase k extracted from document d, the corresponding keyphrase score is identified by c (k, d). In a manual check of the extraction output, we introduce some data cleansing steps. We only retain keyphrases that: (i) have an extraction rate above an empirically defined threshold of 2; (ii) are no more than 5 terms long; (iii) each term has a length between 4 and 15 characters and is either a significant number (i.e. a maximum of 4 digits) or a term (without noisy substrings and reserved keywords from markup languages). Finally, we set the relevance of k as P (k | d, s) = c (k, d) / \u2211 k \u2032 c (k \u2032 k, d \u2032 d), in the case of each document, each document actually corresponds to a QS."}, {"heading": "4.5 ery Suggestions", "text": "The next step is to generate query suggestions from the extracted keyphrases. As a basic option, we take each raw keyphrase k as-is, i.e. q = k sets P (q | q0, s, k) = 1. Alternatively, we can build query suggestions by extending keyphrases. Here, k is combined with the original query q0 by using a set of expansion rules proposed in [7]: (i) by adding k as a suffix to q0; (ii) by adding k as su x to a unit mentioned in q0; and (iii) by using k as-is. Rules (i) and (ii) still include a custom string-association operator; we refer to [7] for details. Each query suggestion q generated from the keyword k has an associated connotation c (q, qs, k, k)."}, {"heading": "5 RESULTS", "text": "In this section we present our experimental setup and the results."}, {"heading": "5.1 Experimental Setup", "text": "We use the 2015 and 2016 TREC Task Track test suites [18, 20], which contain 34 and 50 relevance assessment queries, respectively. We report on the evaluation metrics used on the TREC Task Track, the ERR-IA @ 20 and \u03b1-NDCG @ 20, respectively. In accordance with the results of the track, we use ERR-IA @ 20 as the primary metric. (For convenience, we disregard the mention of the cut-o ranking of 20 in all table headers.) We noted that the initial query itself was judged to be highly relevant in numerous cases. We removed these cases as they make little sense for the intended scenario; we note that this leads to a decline in absolute performance. We report statistical significance using a two-step paired t test at p < 0.05 and p < 0.001, which answer quantum questions."}, {"heading": "5.2 ery Suggestion Generation", "text": "We begin our experiments by focusing on the generation of query suggestions and comparing the two methods described in paragraph 4.5. The meaning of the document should be uniform. Performance is reported for each of the four source types separately (i.e. we set P (s) uniformly between sources S and P (s) = 0 for s < S.) The results are presented in Table 1. It is clear that with one exception (2015 WH) it is easier to use the raw key sets without any expansion. the query set for 2016 is applicable to all source types except QS. Regarding the comparison of source types, we assume that QS > WS > WD > WH for the 2016 query set is now for 2015 the order WS > WD > QS, WH."}, {"heading": "5.3 Document Importance", "text": "Next, for each source type, we compare the two source types, the uniform and the rank-based decay (cf. \u00a7 4.3). Table 2 reports the results. We note that the rank-based document meaning for the source types of the source types (source types) is critical for both years and for WikiHow (WH) for the topics of 2015. For all other source types, the source types are er.We also compare the performance for ERR-IA @ 20 with the uniform estimator. We observe a very similar result with the rank-based estimator (which is not considered due to space constraints).In the 2016 query group, the individual sources follow exactly the same results as their respective types (i.e. QS > WS > WH), with the source types being an exception."}, {"heading": "5.4 Source Importance", "text": "Finally, we combine query suggestions across different sources by determining the importance of each source. We consider three query strategies for P (s): (i) consistent; (ii) proportional to the importance of the corresponding source type (QS, WS, WD and WH) from the previous step (cf. Table 2); (iii) proportional to the importance of each source (cf. Figure 3); e results are presented in Table 3. Firstly, we note that the source type combination performs better on its own than each individual source type. As for the meaning of the source, we note that (iii) delivers the best results, which is in line with our expectations. In the 2016 query set, only slight differences are observed between the three methods, none of which are significant."}, {"heading": "5.5 Summary of Findings", "text": "We assume that these search engine suggestions are already diverse, which we can directly use for our task. They are followed, in order, by keyphrases extracted from (i) web search snippets, (ii) web search results, i.e. complete documents, and (iii) WikiHow articles. For web query suggestions, it proved much less effective to consider the ranking of proposals; see RQ3 below. (RQ2) With one exception, the use of the raw keyphrases as they are is is more powerful than extending by taking the original query into account. For web query suggestions, it is essential to consider the ranking of proposals, while for web search snippets and documents, the consistent evaluation of results is higher. At WikiHow, it differs from the query sets. (RQ3) Our key observations are consistent in the 2015 and 2016 query results, so that our QS proposals are not considered relevant to the relevance of the documents."}, {"heading": "6 CONCLUSIONS", "text": "We have proposed a probabilistic generative framework with four components: source meaning, document meaning, keyphrase relevance, and query suggestions. We have proposed and experimentally compared various alternatives for these components. An important element missing from our current model is the representation of specific subtasks. As a next step, we plan to merge query suggestions that belong to the same subtask. Of course, this would enable us to provide diverse query suggestions."}], "references": [{"title": "IntentStreams: Smart Parallel Search Streams for Branching Exploratory Search", "author": ["Salvatore Andolina", "Khalil Klouche", "Jaakko Peltonen", "Mohammad E. Hoque", "Tuukka Ruotsalo", "Diogo Cabral", "Arto Klami", "Dorota Glowacka", "Patrik Flor\u00e9en", "Giulio Jacucci"], "venue": "In Proc. of IUI", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Supporting Complex Search Tasks", "author": ["Ahmed H. Awadallah", "Ryen W. White", "Patrick Pantel", "Susan T. Dumais", "Yi- Min Wang"], "venue": "In Proc. of CIKM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Task-completion Engines: A Vision with a Plan", "author": ["Krisztian Balog"], "venue": "In Proc. of the 1st International Workshop on Supporting Complex Search Tasks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "\u008bery Suggestions in the Absence of \u008bery Logs", "author": ["Sumit Bhatia", "Debapriyo Majumdar", "Prasenjit Mitra"], "venue": "In Proc. of SIGIR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "\u008ce \u008bery-\u0083ow Graph: Model and Applications", "author": ["Paolo Boldi", "Francesco Bonchi", "Carlos Castillo", "Debora Donato", "Aristides Gionis", "Sebastiano Vigna"], "venue": "In Proc. of CIKM", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Random walks on the click graph", "author": ["Nick Craswell", "Martin Szummer"], "venue": "In Proc. of SIGIR", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "\u008ce University of Stavanger at the TREC 2016 Tasks Track", "author": ["Dar\u0131\u0301o Gariglio\u008ai", "Krisztian Balog"], "venue": "In Proc. of TREC", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Webis at TREC 2015: Tasks and Total Recall Tracks", "author": ["Ma\u008ahias Hagen", "Steve G\u00f6ring", "Magdalena Keil", "Olaoluwa Anifowose", "Amir Othman", "Benno Stein"], "venue": "In Proc. of TREC", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Webis at TREC 2016: Tasks, Total Recall, and Open Search Tracks", "author": ["Ma\u008ahias Hagen", "Johannes Kiesel", "Payam Adineh", "Masoud Alahyari", "Ehsan Fatehifar", "Arafeh Bahrami", "Pia Fichtl", "Benno Stein"], "venue": "In Proc. of TREC", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Contextual Evaluation of \u008bery Reformulations in a Search Session by User Simulation", "author": ["Jiepu Jiang", "Daqing He", "Shuguang Han", "Zhen Yue", "Chaoqun Ni"], "venue": "In Proc. of CIKM", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A Comparison of \u008bery and Term Suggestion Features for Interactive Searching", "author": ["Diane Kelly", "Karl Gyllstrom", "Earl W. Bailey"], "venue": "In Proc. of SIGIR", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Deriving query suggestions for site", "author": ["Udo Kruschwitz", "Deirdre Lungley", "M-Dyaa Albakour", "Dawei Song"], "venue": "search. JASIST 64,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Discovering Tasks from Search Engine \u008bery Logs", "author": ["Claudio Lucchese", "Salvatore Orlando", "Ra\u0082aele Perego", "Fabrizio Silvestri", "Gabriele Tolomei"], "venue": "ACM Trans. Inf. Syst", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Modi\u0080ed RAKE algorithm. h\u008aps://github.com/zelandiya/ RAKE-tutorial", "author": ["Alyona Medelyan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning to Suggest: A Machine Learning Framework for Ranking \u008bery Suggestions", "author": ["Umut Ozertem", "Olivier Chapelle", "Pinar Donmez", "Emre Velipasaoglu"], "venue": "In Proc. of SIGIR", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "\u008ce Forgo\u008aen Needle in My Collections: Task-Aware Ranking of Documents in Semantic Information Space", "author": ["Tuan A. Tran", "Sven Schwarz", "Claudia Nieder\u00e9e", "Heiko Maus", "Na\u008aiya Kanhabua"], "venue": "In Proc. of CHIIR", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Overview of the TREC Tasks Track 2016", "author": ["Manisha Verma", "Evangelos Kanoulas", "Emine Yilmaz", "Rishabh Mehrotra", "Ben Cartere\u008ae", "Nick Craswell", "Peter Bailey"], "venue": "In Proc. of TREC", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Assessing the Scenic Route: Measuring the Value of Search Trails in Web Logs", "author": ["Ryen W. White", "Je\u0082 Huang"], "venue": "In Proc. of SIGIR", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Overview of the TREC 2015 Tasks Track", "author": ["Emine Yilmaz", "Manisha Verma", "Rishabh Mehrotra", "Evangelos Kanoulas", "Ben Cartere\u008ae", "Nick Craswell"], "venue": "In Proc. of TREC", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "\u008bery suggestions are an integral part of modern search engines [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "\u008ce Tasks track at the Text REtrieval Conference (TREC) has introduced an evaluation platform for this very problem, referred to as task understanding [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "the system should return a ranked list of keyphrases \u201cthat represent the set of all tasks a user who submi\u008aed the query may be looking for\u201d [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 3, "context": ", in the enterprise domain) or when a search engine has been newly deployed [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": ", [7\u20139].", "startOffset": 2, "endOffset": 7}, {"referenceID": 7, "context": ", [7\u20139].", "startOffset": 2, "endOffset": 7}, {"referenceID": 8, "context": ", [7\u20139].", "startOffset": 2, "endOffset": 7}, {"referenceID": 1, "context": "Log-based studies have been one main area of focus, including the identi\u0080cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query", "startOffset": 135, "endOffset": 142}, {"referenceID": 12, "context": "Log-based studies have been one main area of focus, including the identi\u0080cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query", "startOffset": 135, "endOffset": 142}, {"referenceID": 9, "context": "reformulations [10] or search trails [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "reformulations [10] or search trails [19].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 2, "context": "Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 15, "context": "Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 5, "context": "For example, Craswell and Szummer [6] perform a random walk on a query-click graph.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "[5] model the query \u0083ow in user search sessions via chains of queries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus.", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus.", "startOffset": 62, "endOffset": 69}, {"referenceID": 10, "context": "[12] have shown that users prefer query suggestions, rather than term suggestions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Speci\u0080cally, we use the RAKE keyword extraction system [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Here, k is combined with the initial query q0 using a set of expansion rules proposed in [7]: (i) adding k as a suf\u0080x to q0; (ii) adding k as a su\u0081x to an entity mentioned in q0; and (iii) using k as-is.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "Rules (i) and (ii) further involve a custom string concatenation operator; we refer to [7] for details.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20].", "startOffset": 61, "endOffset": 69}, {"referenceID": 18, "context": "We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20].", "startOffset": 61, "endOffset": 69}, {"referenceID": 6, "context": "It is worth noting that some of our methods were o\u0081cially submi\u008aed to TREC 2016 [7] and were included in the assessment pools.", "startOffset": 80, "endOffset": 83}], "year": 2017, "abstractText": "We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the \u0080rst place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model.", "creator": "LaTeX with hyperref package"}}}