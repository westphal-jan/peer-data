{"id": "1708.08327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Feature Conservation in Adversarial Classifier Evasion: A Case Study", "abstract": "Machine learning is widely used in security applications, particularly in the form of statistical classification aimed at distinguishing benign from malicious entities. Recent research has shown that such classifiers are often vulnerable to evasion attacks, whereby adversaries change behavior to be categorized as benign while preserving malicious functionality. Research into evasion attacks has followed two paradigms: attacks in problem space, where the actual malicious instance is modified, and attacks in feature space, where the attack is abstracted into modifying numerical features of an instance to evade a classifier. In contrast, research into designing evasion-robust classifiers generally relies on feature space attack models. We make several contributions to address this gap, using PDF malware detection as a case study. First, we present a systematic retraining procedure which uses an automated problem space attack generator to design a more robust PDF malware detector. Second, we demonstrate that replacing problem space attacks with feature space attacks dramatically reduces the robustness of the resulting classifier, severely undermining feature space defense methods to date. Third, we demonstrate the existence of conserved (or invariant) features, and show how these can be leveraged to design evasion- robust classifiers that are nearly as effective, and far more efficient, than those relying on the problem space attack. Finally, we present a general approach for identifying conserved features.", "histories": [["v1", "Mon, 28 Aug 2017 14:18:35 GMT  (707kb,D)", "http://arxiv.org/abs/1708.08327v1", "14 pages, 15 figures"]], "COMMENTS": "14 pages, 15 figures", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["liang tong", "bo li", "chen hajaj", "yevgeniy vorobeychik"], "accepted": false, "id": "1708.08327"}, "pdf": {"name": "1708.08327.pdf", "metadata": {"source": "CRF", "title": "Feature Conservation in Adversarial Classifier Evasion: A Case Study", "authors": ["Liang Tong", "Bo Li", "Chen Hajaj", "Yevgeniy Vorobeychik"], "emails": ["liang.tong@vanderbilt.edu", "crystalboli@berkeley.edu", "chen.hajaj@vanderbilt.edu", "yevgeniy.vorobeychik@vanderbilt.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "II. RELATED WORK", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "III. BACKGROUND", "text": "This section provides background information on the structure of PDF documents, the Target Malware Classifier (SL2013) [31], and the EvadeML automated evasion approach [37] that we use to evaluate the robustness of evasive systems.3In fact, even evasive attacks on deep classifiers in their general form are effective in feature space, as they are based on calculating training loss in terms of underlying features."}, {"heading": "A. PDF Document Structure", "text": "The Portable Document Format (PDF) is an open standard format used to represent content and layout on different platforms. A structure of PDF files consists of four parts: Header, Body, Cross-Reference Table (CRT) and Trailer, which are displayed on the left side of Figure 1. The header contains information such as the magic number and format version. The body is the most important element of a PDF file, which includes several PDF objects that make up the contents of the file. These objects can be one of the eight basic types: Boolean, Numeric, String, Zero, Name, Array, Dictionary and Stream. They could be obtained from other objects via indirect references. There are other types of objects such as JavaScript, which contains executable JavaScript code. The CRT indexes objects in the body while the trailer refers to the CRT.The syntax of the body of a PDF file is illustrated in the middle of Figure 1."}, {"heading": "B. The Target Classifier", "text": "For our study, we chose SL2013 [31]. SL2013 is a well-documented and open-source machine learning system using Support Vector Machines (SVM) with a radial base kernel (RBF), and it has been proven to be state-of-the-art. Although it has since undergone a revision [32], SL2013 was the version evaluated by the EvadeML tool, and the pair provides a natural assessment framework for our purposes. SL2013 uses structural properties of PDF files to distinguish between malignant and benign PDFs. Specifically, SL2013 uses the presence of certain structural paths as binary features to represent PDF files in the scope of functionality. An object's structural path is a sequence of edges in the reduced logical structure, starting with the catalog path and ending at that object."}, {"heading": "C. Automated Evasion", "text": "In fact, it is the case that most of us have the opportunity to go to another world, in which they can go to another world, in which they can go to another world, in which they can go to another world, in which they can go to another world, in which they go to another world, in which they can go to another world, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they can live, in which they can, in which they live, in which they can, in which they live."}, {"heading": "D. Problem Space vs. Feature Space Attacks", "text": "One distinction that is crucial to the size of the room and the way in which it is created is that the characteristics in the vector representation of a malicious being can be changed directly and essentially arbitrarily (for example, that it has invaded the room), but the attack will still be effective, for example by using a sandbox in which it is located, such as the source code of the malware before it is transferred into the room."}, {"heading": "IV. SYSTEMATIC RETRAINING", "text": "The experiments of Xu et al. [37] show that while SL2013 was specifically designed to be resistant to evasive attacks, it can be successfully circumvented. In this section, we present a universal method for increasing the robustness of SL2013 in the face of EvadeML attacks. The approach builds on iterative retraining methods proposed by Li et al. [19] and Kantchelian et al. [16], but to our knowledge it is the first method that uses problem space bypassing to evaluate and increase the robustness of evasive maneuvers."}, {"heading": "A. The Retraining Framework", "text": "The proposed iterative retraining method is a modified version of that used by Li et al. [19], and is schematically shown in Figure 3. The approach begins with the initial classifier and begins with executing an evasion oracle for each malicious instance in the training data. Successful evaluations are then added to the training data, and the classifier is retrained. Next, the evasion oracle is reexecuted for each malicious instance, and the process repeats for either a predefined number of iterations or up to convergence. We note that our main novelty claim is not the general retraining concept itself (which is not new), but the approach to use in the context of problem space attacks, and the empirical evaluation of its effectiveness (which is far from the complexity)."}, {"heading": "B. Experiments", "text": "This year it is so far that it will only take a few weeks to reach an agreement."}, {"heading": "V. LIMITATIONS OF FEATURE SPACE ATTACK MODELS", "text": "We have now answered the first question raised in Section I by developing a systematic retraining approach based on a problematic problem space circumvention model. Since the effectiveness of this approach is significantly impaired by its overhead, we are now evaluating the feasibility of using a feature space counterpart in its place. Although a number of such approaches have been proposed, we are trying to make the most direct comparison by simply replacing the oracle in the retraining method with a feature space attack model."}, {"heading": "A. Methodology: the Feature Space Evasion Model", "text": "There are a number of tax evasion models in the attribute space that have been proposed in the earlier literature, all of which involve tax evasion as an optimization problem, in which essentially two considerations are weighed against each other: ensuring that the contrary modified attribute vector is classified as benign, and minimizing the total cost of the attribute change, where the latter is usually measured using an lp standard difference between the original malicious instance and the modified attribute vector [19]. In typical problem space attacks, including EvadeML, one consideration is not only to move to the favorable side of the classifier's decision limit, but to appear as favorable as possible, which of course translates into the following multi-objective optimization in the attribute space: Minimization x Q (x) = f (x) + calc (xM, x), (1) where f (x) the score of a attribute vector is (x), where x is the actual value (x), where x is the value of a attribute vector (x), where x is the problem of a classifier (x)."}, {"heading": "B. Experiments", "text": "The question is to what extent it is a natural attack model based solely on training data, which we differentiate depending on how often the feature appears among malicious and benign instances. If a feature is common in malicious and benign instances, often in malicious and benign instances, but rarely in benign instances, we assume that changing this feature comes at a high cost; otherwise, we assume that changing this feature comes at a high cost."}, {"heading": "VI. EVASION-ROBUST CLASSIFICATION WITH CONSERVED FEATURES", "text": "Next, we propose a simple idea for bridging the gap between problem and feature space: We explicitly consider a subset of characteristics that are preserved in evasive maneuvers. To be more precise, preserved characteristics are those that are invariant under (not by) evasive attacks. Next, we present three surprising results: First, preserved characteristics exist (for EvadeML) and can be effectively identified. Previous accounts were generally skeptical of the ability to find or use such attack invariants for defense. Second, we show that a classifier that uses only the preserved characteristics is (a) completely robust against the EvadeML attack (essentially by design) and (b) fairly effective against test data that do not include evasive maneuvers."}, {"heading": "A. Conserved Features", "text": "In our case study, the Target Classifier (SL2013) uses structural paths as characteristics to distinguish between malicious and benign PDFs. Since the shellcode that triggers malicious functionality is embedded in certain PDFs, these corresponding structural paths should be preserved in any variant created from the same malicious seed. In the example of Figure 1, the PDF file has 7 structural paths, of which / OpenActionJS and / OpenActionS are characteristics, because the JavaScript code is placed in the corresponding objects of these two structural paths. On the other hand, there are characteristics that are irrelevant to malicious functionality. For example, the structural path / type is insignificant in order to preserve malicious behavior, and we do not expect it to be preserved. As discussed by Xu et al. [37], three basic paths can be used to make malicious functionality that do not directly after the malicious object, but only a malicious function that directly replaces the corresponding function and a malicious object."}, {"heading": "B. Classifying with Conserved Features", "text": "We begin by researching the efficacy of preserved traits for classification; in Section VI-D, we describe a systematic approach to identifying such traits. Our experiments use a preserved trait based on EvadeML and the 40 malignant seeds used in training. Our preserved traits contain only 8 traits out of a total of 6,087. These are listed in Table I. This preserved sentence raises two key questions that we answer: 1) These are sufficient to evoke a classifier, and 2) effectively distinguish them between benign and malignant instances. To answer these questions, we have a classifier that uses only the preserved traits (FR8) and evaluate both its evasive robustness."}, {"heading": "C. Feature Space Retraining with Conserved Features", "text": "This year, it has reached the stage where it will be able to take the lead in opening up a wide range of future perspectives."}, {"heading": "VII. DISCUSSION", "text": "The aforementioned persons are able to move, move and move without being able to move, without being able to move."}], "references": [{"title": "Can machine learning be secure?", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "in ACM Asia Conference Computer and Communications Security,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. Srndic", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "European Conference on Machine Learning and Knowledge Discovery in Databases, 2013, pp. 387\u2013402.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 4, pp. 984\u2013996, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011, pp. 547\u2013555.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["\u2014\u2014"], "venue": "Journal of Machine Learning Research, no. 13, pp. 2617\u20132654, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Detection and analysis of driveby-download attacks and malicious javascript code", "author": ["M. Cova", "C. Kruegel", "G. Vigna"], "venue": "International Conference on World Wide Web, 2010, pp. 281\u2013290.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004, pp. 99\u2013108.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A vaccine based on conserved regions could prove radical", "author": ["K. Dorans"], "venue": "Nature Medicine, vol. 351, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Identification of novel conserved functional motifs across most influenza a viral strains", "author": ["M. ElHefnawi", "O. AlAidi", "N. Mohamed", "M. Kamar", "I. El-Azab", "S. Zada", "R. Siam"], "venue": "Virology Journal, vol. 8, no. 44, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "USENIX Security Symposium, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "International Conference on Learning Representations, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. Grosse", "N. Papernot", "P. Manoharan", "M. Backes", "P. McDaniel"], "venue": "European Symposium on Research in Computer Security, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Cuckoo sandbox: A malware analysis system", "author": ["C. Guarnieri", "A. Tanasi", "J. Bremer", "M. Schloesser"], "venue": "2012, http://www.cuckoosandbox.org/.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Strategic classification", "author": ["M. Hardt", "N. Megiddo", "C. Papadimitriou", "M. Wootters"], "venue": "ACM Conference on Innovations in Theoretical Computer Science, 2016, pp. 111\u2013122.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesv\u00e1ri"], "venue": "International Conference on Learning Representations, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Evasion and hardening of tree ensemble classifiers", "author": ["A. Kantchelian", "J.D. Tygar", "A.D. Joseph"], "venue": "International Conference on Machine Learning, 2016, pp. 2387\u20132396.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I.J. Goodfellow", "S. Bengio"], "venue": "2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Feature cross-substitution in adversarial classification", "author": ["B. Li", "Y. Vorobeychik"], "venue": "Neural Information Processing Systems, 2014, pp. 2087\u20132095.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A general retraining framework for scalable adversarial classification", "author": ["B. Li", "Y. Vorobeychik", "X. Chen"], "venue": "2016, arXiv preprint.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, 2005, pp. 641\u2013647.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious PDF files detection", "author": ["D. Maiorca", "I. Corona", "G. Giacinto"], "venue": "ACM Asia Conference on Computer and Communications Security, 2013, pp. 119\u2013130.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Pdfrw: A pure python library that reads and writes pdfs", "author": ["P. Maupin"], "venue": "https://github.com/pmaupin/pdfrw, 2017, accessed: 2017-05-18.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["S. Mei", "X. Zhu"], "venue": "AAAI Conference on Artificial Intelligence, 2015, pp. 2871\u20132877.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Near-optimal evasion of convex-inducing classifiers", "author": ["B. Nelson", "B.I.P. Rubinstein", "L. Huang", "A.D. Joseph", "S. Lau", "S.J. Lee", "S. Rao", "A. Tran", "J.D. Tygar"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 549\u2013556.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifier evasion: Models and open problems", "author": ["B. Nelson", "B.I.P. Rubinstein", "L. Huang", "A.D. Joseph", "J.D. Tygar"], "venue": "Privacy and Security Issues in Data Mining and Machine Learning - International ECML/PKDD Workshop, 2010, pp. 92\u201398.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J. Tygar"], "venue": "Journal of Machine Learning Research, pp. 1293\u20131332, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P.D. McDaniel", "I.J. Goodfellow"], "venue": "2016, arxiv preprint.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Secure kernel machines against evasion attacks", "author": ["P. Russu", "A. Demontis", "B. Biggio", "G. Fumera", "F. Roli"], "venue": "ACM Workshop on Artificial Intelligence and Security, 2016, pp. 59\u201369.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. Sharif", "S. Bhagavatula", "L. Bauer", "M.K. Reiter"], "venue": "ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 1528\u20131540.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["N. \u0160rndic", "P. Laskov"], "venue": "2014 IEEE Symposium on Security and Privacy, 2014, pp. 197\u2013211.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection of malicious PDF files based on hierarchical document structure", "author": ["N. Srndic", "P. Laskov"], "venue": "Network and Distributed System Security Symposium, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Hidost: a static machine-learning-based detector of malicious files", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "EURASIP Journal on Information Security, vol. 2016, no. 1, p. 22, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Convex learning with invariances", "author": ["C.H. Teo", "A. Globerson", "S. Roweis", "A.J. Smola"], "venue": "Neural Information Processing Systems, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal randomized classification in adversarial settings", "author": ["Y. Vorobeychik", "B. Li"], "venue": "International Conference on Autonomous Agents and Multiagent Systems, 2014, pp. 485\u2013492.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust regression and lasso", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Neural Information Processing Systems, 2008, pp. 1801\u20131808.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Robustness and regularization of support vector machines", "author": ["\u2014\u2014"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 1485\u20131510, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatically evading classifiers: A case study on PDF malware classifiers", "author": ["W. Xu", "Y. Qi", "D. Evans"], "venue": "Network and Distributed System Security Symposium, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial feature selection against evasion attacks", "author": ["F. Zhang", "P. Chan", "B. Biggio", "D. Yeung", "F. Roli"], "venue": "IEEE Transactions on Cybernetics, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial support vector machine learning", "author": ["Y. Zhou", "M. Kantarcioglu", "B.M. Thuraisingham", "B. Xi"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, pp. 1059\u2013 1067. 14", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 30, "context": "Most of the traditional malware detection approaches are based on dynamic analysis, which incurs significant computational overhead and latency since such procedures depend on execution of shellcodes [31].", "startOffset": 200, "endOffset": 204}, {"referenceID": 30, "context": "State-of-the-art approaches of this kind enable static detection of malicious entities, such as malware, efficiently and with accuracy often exceeding 99% [31], [30].", "startOffset": 155, "endOffset": 159}, {"referenceID": 29, "context": "State-of-the-art approaches of this kind enable static detection of malicious entities, such as malware, efficiently and with accuracy often exceeding 99% [31], [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 130, "endOffset": 133}, {"referenceID": 19, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 135, "endOffset": 139}, {"referenceID": 30, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 141, "endOffset": 145}, {"referenceID": 29, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 147, "endOffset": 151}, {"referenceID": 36, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "The first involves attacks in problem space: systematic approaches for designing evasions through modifying actual malicious instances [30], [37], [21].", "startOffset": 135, "endOffset": 139}, {"referenceID": 36, "context": "The first involves attacks in problem space: systematic approaches for designing evasions through modifying actual malicious instances [30], [37], [21].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "The first involves attacks in problem space: systematic approaches for designing evasions through modifying actual malicious instances [30], [37], [21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "While not guaranteed to exactly simulate behavior of real attackers, these faithfully replicate the constraints faced by real attackers in designing evasions, such as the inability to effect arbitrary changes in the feature space, and the requirement that malicious functionality of the instance is preserved (typically evaluated using a sandbox, such as WEPAWET [6] and the Cuckoo sandbox [13]).", "startOffset": 363, "endOffset": 366}, {"referenceID": 12, "context": "While not guaranteed to exactly simulate behavior of real attackers, these faithfully replicate the constraints faced by real attackers in designing evasions, such as the inability to effect arbitrary changes in the feature space, and the requirement that malicious functionality of the instance is preserved (typically evaluated using a sandbox, such as WEPAWET [6] and the Cuckoo sandbox [13]).", "startOffset": 390, "endOffset": 394}, {"referenceID": 6, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 194, "endOffset": 197}, {"referenceID": 19, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 205, "endOffset": 209}, {"referenceID": 24, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 211, "endOffset": 215}, {"referenceID": 1, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 217, "endOffset": 220}, {"referenceID": 15, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 222, "endOffset": 226}, {"referenceID": 34, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 35, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 22, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 229, "endOffset": 232}, {"referenceID": 17, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 234, "endOffset": 238}, {"referenceID": 38, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 240, "endOffset": 244}, {"referenceID": 4, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 246, "endOffset": 249}, {"referenceID": 18, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 299, "endOffset": 303}, {"referenceID": 14, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 305, "endOffset": 309}, {"referenceID": 15, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 311, "endOffset": 315}, {"referenceID": 10, "context": "1A subset of methods [11], [15] focused on adversarial deep neural network learning for vision tasks consider modified pixels in images, but these are also the features of the learning algorithms.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "1A subset of methods [11], [15] focused on adversarial deep neural network learning for vision tasks consider modified pixels in images, but these are also the features of the learning algorithms.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "Moreover, these tend to ignore realistic constraints on attacker, such as modifications of physical environments which are subsequently translated into images, although these have been explored in considering attacks independently of defensive techniques [29].", "startOffset": 255, "endOffset": 259}, {"referenceID": 30, "context": "ing tool for detecting malicious PDFs [31], and EvadeML, an automated problem space method for PDF malware classifier evasion [37].", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "ing tool for detecting malicious PDFs [31], and EvadeML, an automated problem space method for PDF malware classifier evasion [37].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "These conserved features echo an analogous notion in epidemiology, where, for example, characterization of conserved regions of viral proteins is an important step toward designing effective vaccines [8], [9].", "startOffset": 200, "endOffset": 203}, {"referenceID": 8, "context": "These conserved features echo an analogous notion in epidemiology, where, for example, characterization of conserved regions of viral proteins is an important step toward designing effective vaccines [8], [9].", "startOffset": 205, "endOffset": 208}, {"referenceID": 9, "context": "The existence of these is especially surprising in light of past work on adversarial evasion of machine learning in cyber security, which has been duly skeptical that such problem invariance exists, or could successfully be leveraged [10].", "startOffset": 234, "endOffset": 238}, {"referenceID": 31, "context": "2An updated version of this classifier [32] has made changes to the feature space.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "[10], who developed a polymorphic blending attack on anomalybased intrusion detection systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "\u0160rndic and Lasov [30] present a case study of an evasion attack on a state of the art PDF malware classifier, PDFRate.", "startOffset": 17, "endOffset": 21}, {"referenceID": 36, "context": "[37] propose EvadeML, a fully problem space attack on PDF malware classifiers which generates evasion instances by using genetic programming to modify PDF source directly, using a sandbox to ensure that malicious functionality is preserved.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 26, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 28, "context": "Several efforts explored the impact of indirect modification when an adversarial image must be printed, showing that effectiveness of such techniques can nevertheless be preserved [29], [17].", "startOffset": 180, "endOffset": 184}, {"referenceID": 16, "context": "Several efforts explored the impact of indirect modification when an adversarial image must be printed, showing that effectiveness of such techniques can nevertheless be preserved [29], [17].", "startOffset": 186, "endOffset": 190}, {"referenceID": 28, "context": "Recently, an approach for printing specifically designed glass frames had been shown to mislead vision-based biometric systems to either mistakenly grant authorization, or enable evasion of face recognition techniques [29].", "startOffset": 218, "endOffset": 222}, {"referenceID": 6, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 220, "endOffset": 223}, {"referenceID": 19, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 225, "endOffset": 229}, {"referenceID": 0, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 231, "endOffset": 234}, {"referenceID": 15, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 236, "endOffset": 240}, {"referenceID": 6, "context": "[7] present one of the earliest such models as a part of a robust learning approach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "An algorithmic investigation of the feature space evasion problem\u2014modeling it formally as minimizing the cost of changing features subject to a constraint that the resulting instance is classified as benign\u2014was initiated by Lowd and Meek [20].", "startOffset": 238, "endOffset": 242}, {"referenceID": 1, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 239, "endOffset": 242}, {"referenceID": 13, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 244, "endOffset": 248}, {"referenceID": 37, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 250, "endOffset": 254}, {"referenceID": 15, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 256, "endOffset": 260}, {"referenceID": 17, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 262, "endOffset": 266}, {"referenceID": 18, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 268, "endOffset": 272}, {"referenceID": 25, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 274, "endOffset": 278}, {"referenceID": 33, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 280, "endOffset": 284}, {"referenceID": 6, "context": "[7] presented the first approach for evasion-robust classification, making use of a model in which the attacker aims to transform feature vectors into benign instances in response to the", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "A series of approaches formulate robust classification as minimizing maximum loss, where maximization is attributed to the evading attacker aiming to maximize the learner\u2019s loss through feature space transformations [33], [39].", "startOffset": 216, "endOffset": 220}, {"referenceID": 38, "context": "A series of approaches formulate robust classification as minimizing maximum loss, where maximization is attributed to the evading attacker aiming to maximize the learner\u2019s loss through feature space transformations [33], [39].", "startOffset": 222, "endOffset": 226}, {"referenceID": 3, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 222, "endOffset": 225}, {"referenceID": 17, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 227, "endOffset": 231}, {"referenceID": 15, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 233, "endOffset": 237}, {"referenceID": 27, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 239, "endOffset": 243}, {"referenceID": 10, "context": "In the deep learning literature involving adversarial manipulations of images, somewhat ad hoc procedures for retraining the classifier to boost its robustness have been proposed [11], [15], and these have been adapted to other classification models, such as decision tree classifiers [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 14, "context": "In the deep learning literature involving adversarial manipulations of images, somewhat ad hoc procedures for retraining the classifier to boost its robustness have been proposed [11], [15], and these have been adapted to other classification models, such as decision tree classifiers [16].", "startOffset": 185, "endOffset": 189}, {"referenceID": 15, "context": "In the deep learning literature involving adversarial manipulations of images, somewhat ad hoc procedures for retraining the classifier to boost its robustness have been proposed [11], [15], and these have been adapted to other classification models, such as decision tree classifiers [16].", "startOffset": 285, "endOffset": 289}, {"referenceID": 18, "context": "Recently, a systematic iterative retraining procedure had been proposed which leverages general-purpose adversarial evasion models, and offers a theoretical connection to an underlying Stackelberg game played between the learner and the evading adversary [19].", "startOffset": 255, "endOffset": 259}, {"referenceID": 30, "context": "This section provides background on PDF document structure, the target PDF malware classifier (SL2013) [31], and the automated evasion approach, EvadeML [37], we use to evaluate evasion robustness of classifiers.", "startOffset": 103, "endOffset": 107}, {"referenceID": 36, "context": "This section provides background on PDF document structure, the target PDF malware classifier (SL2013) [31], and the automated evasion approach, EvadeML [37], we use to evaluate evasion robustness of classifiers.", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "2: Classifier evasion with genetic programming [37].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "Several PDF malware classifiers have been proposed [6], [31].", "startOffset": 51, "endOffset": 54}, {"referenceID": 30, "context": "Several PDF malware classifiers have been proposed [6], [31].", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "For our study, we selected SL2013 [31].", "startOffset": 34, "endOffset": 38}, {"referenceID": 31, "context": "While it has since undergone a revision [32], the SL2013 was the version evaluated by the EvadeML tool, and the pair provides a natural evaluation framework for our purposes.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "To evaluate the robustness of a PDF classifier against adversarial evasion attacks, we adopt EvadeML [37], a systematic and automated method to craft evasion instances of PDF malware in problem space.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "After the population is initialized, each variant is assessed by the Cuckoo sandbox [13] and the target classifier to evaluate its fitness.", "startOffset": 84, "endOffset": 88}, {"referenceID": 36, "context": "EvadeML was used to evade SL2013 in [37].", "startOffset": 36, "endOffset": 40}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In this case, features can no longer be modified directly [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 36, "context": "[37] demonstrate that although SL2013 was designed specifically to be resistant to evasion attacks, it can be successfully evaded.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] and Kantchelian et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16], but is, to our knowledge, the first method which uses problem space evasion for evaluating and boosting evasion robustness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], and is schematically shown in Figure 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The training and test datasets also contain 500 seeds selected by [37], with 400 in the training data and 100 in the test dataset.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "These seeds are filtered from 10,980 PDF malware samples and are suitable for evaluation since they are detected with reliable malware signatures by the Cuckoo sandbox [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37], for both retraining and robustness evaluation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "5This result is different from the experiments in [37] which shows a 0% evasion robustness.", "startOffset": 50, "endOffset": 54}, {"referenceID": 31, "context": "We also evaluated the robustness of Hidost [32] (the updated version of SL2013) by EvadeML.", "startOffset": 43, "endOffset": 47}, {"referenceID": 36, "context": "5: SVM scores of systematic retraining as a function of iterations under EvadeML [37] test.", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "All involve casting evasion as an optimization problem essentially trading off two considerations: ensuring that the adversarially modified feature vector is classified as benign, and minimizing the total cost of feature modification, where the latter is commonly measured using an lp norm difference between the original malicious instance and the modified feature vector[19], [2].", "startOffset": 372, "endOffset": 376}, {"referenceID": 1, "context": "All involve casting evasion as an optimization problem essentially trading off two considerations: ensuring that the adversarially modified feature vector is classified as benign, and minimizing the total cost of feature modification, where the latter is commonly measured using an lp norm difference between the original malicious instance and the modified feature vector[19], [2].", "startOffset": 378, "endOffset": 381}, {"referenceID": 36, "context": "8: SVM scores of the baseline and synthetic retraining as a function of \u03bb under EvadeML [37] test.", "startOffset": 88, "endOffset": 92}, {"referenceID": 36, "context": "[37], three fundamental operations can be used to craft adversarial examples, which directly modify PDF objects and the corresponding structural paths: insertion, deletion, and swap.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "We use a modified version of pdfrw [22]6 to parse the objects of PDF file with a logic structure as shown on the right-hand side of Figure 1 and repack objects to produce a new PDF file.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "We use Cuckoo [13] as the sandbox to evaluate malicious functionality.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "[30]).", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Machine learning is widely used in security applications, particularly in the form of statistical classification aimed at distinguishing benign from malicious entities. Recent research has shown that such classifiers are often vulnerable to evasion attacks, whereby adversaries change behavior to be categorized as benign while preserving malicious functionality. Research into evasion attacks has followed two paradigms: attacks in problem space, where the actual malicious instance is modified, and attacks in feature space, where the attack is abstracted into modifying numerical features of an instance to evade a classifier. In contrast, research into designing evasion-robust classifiers generally relies on feature space attack models. We make several contributions to address this gap, using PDF malware detection as a case study. First, we present a systematic retraining procedure which uses an automated problem space attack generator to design a more robust PDF malware detector. Second, we demonstrate that replacing problem space attacks with feature space attacks dramatically reduces the robustness of the resulting classifier, severely undermining feature space defense methods to date. Third, we demonstrate the existence of conserved (or invariant) features, and show how these can be leveraged to design evasionrobust classifiers that are nearly as effective, and far more efficient, than those relying on the problem space attack. Finally, we present a general approach for identifying conserved features.", "creator": "LaTeX with hyperref package"}}}