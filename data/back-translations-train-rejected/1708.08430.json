{"id": "1708.08430", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Deep Belief Networks used on High Resolution Multichannel Electroencephalography Data for Seizure Detection", "abstract": "Ubiquitous bio-sensing for personalized health monitoring is slowly becoming a reality with the increasing availability of small, diverse, robust, high fidelity sensors. This oncoming flood of data begs the question of how we will extract useful information from it. In this paper we explore the use of a variety of representations and machine learning algorithms applied to the task of seizure detection in high resolution, multichannel EEG data. We explore classification accuracy, computational complexity and memory requirements with a view toward understanding which approaches are most suitable for such tasks as the number of people involved and the amount of data they produce grows to be quite large. In particular, we show that layered learning approaches such as Deep Belief Networks excel along these dimensions.", "histories": [["v1", "Mon, 28 Aug 2017 17:28:48 GMT  (535kb)", "http://arxiv.org/abs/1708.08430v1", "Old draft of AAAI paper, AAAI Spring Symposium Series. 2014"]], "COMMENTS": "Old draft of AAAI paper, AAAI Spring Symposium Series. 2014", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["jt turner", "adam page", "tinoosh mohsenin", "tim oates"], "accepted": false, "id": "1708.08430"}, "pdf": {"name": "1708.08430.pdf", "metadata": {"source": "META", "title": "Deep Belief Networks used on High Resolution Multichannel Electroencephalography Data for Seizure Detection", "authors": ["JT Turner", "Adam Page", "Tinoosh Mohsenin"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "It is easy to imagine a near future in which it is customary to wear a number of biosensors that continuously monitor various aspects of our physiological state, including the existence of a variety of conditions. There are two aspects of this enterprise - collecting data and doing something useful. Our starting point is the question of how it is possible to create a small device that monitors the existence of a variety of conditions."}, {"heading": "2. Background and Related Work", "text": "A widely used technique for displaying time series is Symbolic Aggregates approXimation (SAX), which converts real data into a sequence of symbols (Lin et al., 2007). More recently, deep learning has shown promise in tasks such as robotic vision and data mining (Bengio, 2009). Using graphics processor units (GPUs), it is possible to train deep artificial neural networks in layers to address problems that previously required discretion. Later in this section, we introduce terminology, review the deep learning methods used in this work, and discuss related work in the field of seizure detection using machine learning."}, {"heading": "2.1. Multichannel Time Series", "text": "Let us call T a time series, which represents an EEG sampling at a rate of H Hz over C channels for S seconds. The multi-channel time series can be called as follows (where Xc, s, h is the reading of the cth channel on the highest sample of the sth second, measured in mV): T = (((((x0,0,0, x0,0,1,..., x0,0, H \u2212 1))), ((x0, S \u2212 1,0, H \u2212 1),... (xC \u2212 1,0,0, xC \u2212 1,0,1,..., xC \u2212 1,0, H \u2212 1),... (x0, S \u2212 1,0, S \u2212 1,1,..., x0, S \u2212 1, H \u2212 1), (x1, S \u2212 1,0, S \u2212 1, H \u2212 1), (x1, S \u2212 1,1,..., this reading, S \u2212 1,0, x1, that the results of the S1-S1 resolution and S1-S1-resolution are decisive."}, {"heading": "2.2. Classifiers used in this work", "text": "In this paper, three classifiers are used to compare detection accuracy and computing and memory requirements: K-next neighbor (KNN) with 3, 5 and 7 neighbors, Support Vector Machines (SVM) with sigmoid, radial basic functions and polynomial cores, and logistic regression. Figure 1 shows a schematic description of these three classifiers."}, {"heading": "2.3. Deep Belief Networks", "text": "In fact, it's as if most people are able to decide for themselves what they want and what they don't want. (...) It's not as if people are able to decide what they want. (...) It's as if people are able to make the decisions. (...) It's as if people are able to make the decisions. (...) It's as if people are able to make the decisions. (...) It's as if they are able to make the decisions, to make the decisions. \"(...) It's as if people are able to make the decisions. (...) It's as if people are able to make the decisions. (...) It's as if they are made, as if they are made.\" (...) It's as if decisions are made, as if they are made. (...) It's as if they are made, if they are made. (...) It's as if decisions are made, if they are made."}, {"heading": "2.4. Related Work", "text": "In a study by Wulsin (2011), deep belief networks were also used to analyze data obtained from an EEG. Our chosen set of characteristics was borrowed from a larger set of characteristics used in this study. However, this study attempted to classify abnormal EEG characteristics such as GPED, PLED, or blinking as opposed to seizure detection. A particularly useful study by Shoeb and Guttag used the same set of characteristics of seizure patients who were monitored by high-resolution EEG after being subtracted from seizure medications (2009). Although they used the same set of data, the Shoeb study extracted another set of characteristics and used a support vector machine as a binary classifier, as opposed to a low-resolution EEG network. Furthermore, in this study, the seizure process was not interrupted, and statistics were not only used to support the seizure data, but the accuracy of the study."}, {"heading": "3. Method and Approach", "text": "Since using the raw signal input as an input to the deep arbitrary network or classifiers does not allow the algorithm to properly abstract from the raw data, certain features of the dataset are derived from the raw time series signal. Since a trained human can look at the EEG wave pattern and determine whether a seizure occurs with near-perfect accuracy or not, many of the extracted features are visible features of the time series, such as the range below the curve or the variation of peaks. The following features were used to detect abnormal EEG features in the Wulsin Study (2011)."}, {"heading": "3.1. Features used", "text": "In the following definitions, a peak is defined as a reading that marks the change from a positive to a negative derivative = = 2.5 points, and a valley is defined as a reading that marks a change from a negative derivative to a positive derivative. \u2212 Ki then marks the index of the kth peak of the time series, with a value of xk (i). Similarly, vi will mark the i thvalley index, and xv (i) marks the value of it. \u2212 Kid: Chance corrected fraction of data that have a positive or negative derivative. I (x) is a Boolean indicator function whose value is 1 if true, 0 if false.D = 1 W \u2212 2 = 1.1."}, {"heading": "3.2. Design & Parameters for DBN", "text": "The Deep believe network training program was received from the Theano library from the LISA lab of the University of Montreal (Bergstra et al., 2010), with modifications made to save best models (not only recent), and improved methods to allow training progress to be monitored. Training was done fora significant amount of time to improve results, which was possible using GPU calculations which was proven to be much faster than using the CPU alone. Calculations were made on a Dell Precision M4700 model with an Intel Core i7-3940XM CPU @ 3.00GHz, 16 GB of memory @ 1866 MHz, and the graphics card used for GPU calculations was a NVIDIA Quadro K2000M model with 384 unified pipelines @ 745 MHz, with 2 GB of video memory. The input data was changed to the features shown in Secion 3. The EEG that collected the readings sampled from 23 channels at 256 herman, thus using the raw data as input to the institute of the believe."}, {"heading": "4. Results and Analysis", "text": "We used two different approaches to investigate the accuracy of seizure detection: Part 1, which uses simple feature extraction, followed by three different classifiers: SVM, KNN and logistic regression; Part 2, which uses simple feature extraction followed by DBN and a classifier, which in this case is logistic regression; and two different methods of classification tasks were performed using the data. In one study, the same patient was used for both training, validation and test Figure 4; comparing different classifiers when individual patient data is used for training and testing; this resulted in a much smaller corpus but had very good results; in the second study, all the other nine patients with data were used for training and validation, and then one patient each for a test set. This allowed a much larger corpus for training and testing, but did not produce results as high as the results of the first study. In each study, the majority of the time no measurement was used (the fraction of the accuracy between this and the 85% of the applicable time)."}, {"heading": "4.1. Part 1: Simple Feature to Classifiers Comparison", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. F1 and accuracy measurements", "text": "In the first study, the training, validation and test sets were all from the same patient. The fraction of the total time to each set is as follows: 71.4% training set, 14.2% validation set, 14.2% test set. These fractions are from the MNIST classification method with a 5: 1: 1 ratio. The bar charts in Fig. 4 show the F1 comparison between classifiers when individual patient data is used for training and testing. In the second study, the training and validation sets were split between all the seizure and non-seizure seconds of the nine patients who were not tested, in a ratio of 4: 1. For the test patient, all seizure and non-seizure seconds were used in the test set (since no training or validation was performed on the test patient). The bar charts in Fig. 5 show the F1 comparison between classifiers when the patient data is not used for training and when other training data is used for comparison purposes."}, {"heading": "4.1.2. Computational and memory complexity requirements", "text": "In addition to classifiers \"ability to accurately predict seizures, it is also necessary that classifiers minimize complexity by running on a low-power, embedded sensor device in an outpatient environment. As the device can be trained offline, the complexity comes in the form of the memory needed to roughly store the classification model and calculation required to classify an incoming test vector. Table 1 roughly summarizes the memory and computational complexity for each of the classifiers, and the memory and compression required for all simple features is called SF. Also included in the table is compressed nearest neighbor, CNN. CNN is an optimization applied to KNN that attempts to remove low-content data while maintaining nearly the same accuracy. The variables used in the table are defined below: \u2022 W = Window Size (256) \u2022 T = Windows Training (10 000) # 000 (Channels)."}, {"heading": "4.2. Part 2: Simple Feature to DBN and Classifier Comparison", "text": "Since logistic regression performs very well in terms of both accuracy and complexity requirements, we used it as a classifier for DBN analysis. Similar to Part 1, we performed the test in the training of individual patients and left one patient out of the training."}, {"heading": "4.2.1. F1 and accuracy measurements", "text": "Classification with the same patient as the training and test corpus is generally an easier task for machines to learn from, so the differences between the deep-arbitrary network and the logistic regression in individual patient training are not as great as in the next study on leave-out training. The deep-arbitrary network algorithm was very effective in detection, with two perfect F1 measurements and only one F1 measurement below 0.9. The same tests were also performed against the same implementation of logistic regression used in the output layer of the deep-arbitrary network, with f-score comparisons shown in Figure 6.In the second study similar to Part 1, patient data were omitted for training and used only for testing purposes. F1 measurements were lower than expected in this study because the test set was similar but not identical to the sets with which the model was trained, and the results of this second study were not the same as Resource.9 In this study, only 9 patients were intermediate compared to the improvement.8 In this second study, however, the results of this patient were 8."}, {"heading": "4.2.2. Computational and memory complexity requirements", "text": "Adding a DBN stage to the system increases both memory and calculation. In terms of storage, a DBN stage adds about LR (CM) 2 more bits than just logistical regression, with L being the number of layers. This assumes that the average number of nodes in a layer corresponds to the number of input characteristics. In our experiments, this required 413x more memory than LR. In terms of complexity, the DBN stage adds about LCM (2CM + 1)."}, {"heading": "5. Conclusion", "text": "In this paper, the use of a variety of representations and machine learning algorithms has been applied to the detection of seizures in high-resolution and multi-channel EEG data. Classification accuracy, computational complexity, and memory requirements are examined in terms of processing large patient needs. Among classifiers, logistical regression scores best in terms of complexity and accuracy in most tests. Also, seizure detection in studies using the same patient in training, validation, and testing kits has been very successful in all patients. Although these are good numbers, it may not always be feasible to model hours of trained data on a patient; the more realistic clinical study is the one in which the patient tests were conducted without the prior knowledge of the patient on whom they were tested."}], "references": [{"title": "Application of Machine Learning to Epilep", "author": ["Shoeb", "Ali"], "venue": null, "citeRegEx": "Shoeb and Ali.,? \\Q2012\\E", "shortCiteRegEx": "Shoeb and Ali.", "year": 2012}], "referenceMentions": [], "year": 2013, "abstractText": "Ubiquitous bio-sensing for personalized health monitoring is slowly becoming a reality with the increasing availability of small, diverse, robust, high fidelity sensors. This oncoming flood of data begs the question of how we will extract useful information from it. In this paper we explore the use of a variety of representations and machine learning algorithms applied to the task of seizure detection in high resolution, multichannel EEG data. We explore classification accuracy, computational complexity and memory requirements with a view toward understanding which approaches are most suitable for such tasks as the number of people involved and the amount of data they produce grows to be quite large. In particular, we show that layered learning approaches such as Deep Belief Networks excel along these dimensions.", "creator": "LaTeX with hyperref package"}}}