{"id": "1708.08508", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Subspace Selection to Suppress Confounding Source Domain Information in AAM Transfer Learning", "abstract": "Active appearance models (AAMs) are a class of generative models that have seen tremendous success in face analysis. However, model learning depends on the availability of detailed annotation of canonical landmark points. As a result, when accurate AAM fitting is required on a different set of variations (expression, pose, identity), a new dataset is collected and annotated. To overcome the need for time consuming data collection and annotation, transfer learning approaches have received recent attention. The goal is to transfer knowledge from previously available datasets (source) to a new dataset (target). We propose a subspace transfer learning method, in which we select a subspace from the source that best describes the target space. We propose a metric to compute the directional similarity between the source eigenvectors and the target subspace. We show an equivalence between this metric and the variance of target data when projected onto source eigenvectors. Using this equivalence, we select a subset of source principal directions that capture the variance in target data. To define our model, we augment the selected source subspace with the target subspace learned from a handful of target examples. In experiments done on six publicly available datasets, we show that our approach outperforms the state of the art in terms of the RMS fitting error as well as the percentage of test examples for which AAM fitting converges to the ground truth.", "histories": [["v1", "Mon, 28 Aug 2017 20:21:21 GMT  (2814kb,D)", "http://arxiv.org/abs/1708.08508v1", "Copyright IEEE. To be published in the proceedings of International Joint Conference on Biometrics (IJCB) 2017. For final print version see, [link to come]"], ["v2", "Tue, 3 Oct 2017 18:26:16 GMT  (4193kb,D)", "http://arxiv.org/abs/1708.08508v2", "Copyright IEEE. To be published in the proceedings of International Joint Conference on Biometrics (IJCB) 2017"]], "COMMENTS": "Copyright IEEE. To be published in the proceedings of International Joint Conference on Biometrics (IJCB) 2017. For final print version see, [link to come]", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["azin asgarian", "ahmed bilal ashraf", "david fleet", "babak taati"], "accepted": false, "id": "1708.08508"}, "pdf": {"name": "1708.08508.pdf", "metadata": {"source": "CRF", "title": "Subspace Selection to Suppress Confounding Source Domain Information in AAM Transfer Learning", "authors": ["Azin Asgarian", "Ahmed Bilal Ashraf", "David Fleet", "Babak Taati"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, it is not in such a way that it is an attempt to describe the form of an object, for example, an attempt to create a statistical model of form out of a series of notes that relate to the form of an object, for example, the way that it describes the form of an object, for example, the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the way that it is the"}, {"heading": "2. Background", "text": "We start with a brief description of the AAMs and their training process following [12] and then review existing literature on transfer learning methods for AAMs."}, {"heading": "2.1. Active Appearance Models: A Review", "text": "An AAM model is a generative model that records the variation of shape and appearance from a number of designated examples. < The model thus has two parts, one for shape and another for appearance. Shape Model: A shape s is represented by a 2D mesh of V vertices, s = (x1, y1, x2, y2,..., xV, yV) T. Consider a series of N training samples {(Ii, si). The result of the Process Analysis is a global 2D similarity transformation that can also be modeled by a linear combination of four base vectors."}, {"heading": "2.2. Transfer Learning for AAMs", "text": "Although AAMs have achieved tremendous success for a number of computer vision applications, their generalizability is sometimes still a challenge [10]. Especially when adaptation is required on a very different set of images than those used to train the model, adaptation performance decreases [10, 11]. However, this has led to the capture and annotation of many data sets (e.g. [8, 9]). If we need to adapt AAMs to images that capture a different set of variations in expression, pose, illumination, or identity, ideally one would prefer to avoid the annotation step altogether or comment on just a handful of examples. If a model is learned with only a few commented examples, it is often not expressive enough due to the lack of variation [11]. What is the best way to use the already commented data sets or comment on just a handful of examples?"}, {"heading": "2.3. Baselines", "text": "A useful transfer learning method should also surpass a model that is directly based on the linking of source and target data designated by ASUT = (\u00b5SUT, \u03a6SUT, \u03bbSUT, \u03bdSUT, \u0418SUT, \u03baSUT, \u03baSUT).The fourth basic method we are looking at is the subspace transfer method [11], which uses the mean shape and average appearance of the target model, i.e. \u00b5ST = \u00b5T and \u03bdST = \u03bdT. However, eigenvectors are evaluated by linking source and target base vectors, followed by a QR decomposition on matrices [\u03a6T, \u0445S] and [\u0445T, \u0445S] to orthogonize the base vectors. Finally, the current state of AAM transfer learning applies an instance-weighted (IW) approach by assigning weights based on samples from the domain [11]."}, {"heading": "3. Subspace Selection", "text": "Our method aims at transferring the knowledge gained from the source domain by selecting a subspace from the source data set = \u2192 eigenweights = eigenweights. Firstly, we maintain the target base vectors \u03a6T in their entirety, so that the information obtained from the target samples is not lost. Furthermore, we propose to extend the target base vectors with additional main directions from the source data that are most representative of the target space.What should our criterion be in order to select source characteristics vectors that are representative of the target sub-space? Since all base vectors in \u03a6S and \u03a6T are standard, each vector in \u03a6S is associated with the target data that is related by a rotation, i.e.motivTj = Ri \u2192 jurch Si {i {i} j {1, NT} j {1, NT} (1), where the unit vectors of the unit S are related to each Vector in \u03a6m in a \u03a6S."}, {"heading": "3.1. Appearance", "text": "In order to assess the appearance of our model, there are two differences in the approach: First, in order to determine the projected target variance in appearance, we warp all eigenvectors of the model from the given mean \u00b5S to \u00b5T with a piecemeal affinity to warp W\u00b5S \u2192 \u00b5T (\u03a6S). Second, all eigenvectors of the model must be defined in the mean form prior to QR decomposition, which in turn can be easily defined by defining a piecemeal affinity to warp. Algorithm 1 subspace selection algorithm 1: Procedure SELECTION (\u03a6T, \u03a6S, ZT) 2: for each of these eigenvectors 3: \u03c32i = 1 NT \u2212 1\u0445 T \u2212 1\u0445 T Si ZTZ T T\u0445Si4: End for 5: \u0445D \u2190 First D base vectors based on \u04452T | S, ZT 3: \u04452\u04412i = 1 NT \u2212 1\u0445 T \u2212 1\u043c T, \u043c S: End for 5: \u0445D..."}, {"heading": "4. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data", "text": "To compare our approach with other methods, we conducted extensive experiments on six publicly available facial data sets, randomly selecting 555 samples from the following databases: LFPW [4], Helen [15], CK + [9], iBUG [16], and AFW [17]. Images from the above data sets contain truth notes for 68 points. In addition, we selected 320 examples from the CK + data set covering seven posed expressions, the rest of which cover a wide variation in pose and illumination, and most images are from young people and children with happy or neutral expressions. In addition, we selected 320 examples from the UNBCMcMaster Shoulder Pain Expression Archive [8] by time downsampling (1 in 100) of videos in the data sets. UNBC-McMaster data sets contain real expressions of people with shoulder injuries."}, {"heading": "1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0", "text": "RMS Error (% face size) 0102030405060708090T e stE x a mp les Co n v e rge d (%) Target (T) Source (S) Union of S & T (SUT) Subspace Transfer Instance-Weighted [11] Ours (b)% of test examples converged (Target: UNBC-McMaster) Figure 2. Comparison of RMS adjustment errors and percentage of converged test examples when setting 1.0 50 100 150 200 250 300 Iteration3 / 03 / 54.04.55.05.56.0M e a n RM S Err o r (% fa ces ize) Target (T) Source (S) Union of S & T (SUT) Subspace Transfer Instance-Weighted [11] Ours (a) RMS adjustment error (Target: CK +)"}, {"heading": "1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0", "text": "RMS Error (% face size) 0102030405060708090T e stE x a mp les Co n v e rge d (%) Target (T) Source (S) Union of S & T (SUT) Subspace Transfer Instance-Weighted [11] Ours (b)% of the test examples converged (Target: CK +) Figure 3. Comparison of the RMS match error and the percentage of converged test examples in Setting 2.etc.) that are missing or significantly underrepresented in the source domain. In this setting, the source domain excluded not only the CK + dataset (because it is the target domain), but also UNBC-McMaster (which may contain expression variations similar to CK +) to make setting more difficult."}, {"heading": "4.2. Fitting Details", "text": "For the adjustment, the Wiberg Inverse Compositional Algorithm was used [2, 3]. We consider the adjustment process to be converged when the relative change in the cost function is very small (< 10 \u2212 5). The maximum number of iterations was set at 300. To initialize the adjustment process, a delimitation field is first placed around the face with the Viola Jones face detector [18]. Then the mean shape of the target (\u00b5T) matches the delimitation field of the face by estimating a transformation (including scale and translation only). We call this initialization the base initialization. To avoid getting stuck in bad local minimums, we try 10 different perturbations around the base initialization by adding Gaussian noise in scale, translation and rotation."}, {"heading": "4.3. Performance Metrics", "text": "The first criterion is fit accuracy. To quantify fit accuracy, we measure the RMS error between the fit points and the ground truth boundaries normalized by the face size (average height and width of the face), as proposed in [17]. The second criterion is the percentage of test examples converging to the basic truth form depending on the RMS error tolerance (here, 10 \u2212 5). Specifically, we analyze the percentage of test examples converging to the basic truth depending on the RMS error tolerance."}, {"heading": "5. Results", "text": "We compare different models with respect to the RMS error and the percentage convergence for Setting 1 (UNBCMcMaster as the target) in Figure 2. The curves in Figure 2 (a) show the RMS error (averaged over examples for which the convergent RMS error was less than 5% of the face size) depending on iterations. The diagrams in Figure 2 (b) show the percentage of test examples that match the truth as a function of RMS tolerance in pixels. The corresponding curves for Setting 2 (CK + as the target) are shown in Figure 3. For both settings, our approach exceeds all other methods with respect to RMS errors as well as the percentage of test examples that match the truth. In our approach, the number of source main components selected (i.e. the hyper parameter D) was determined to be 3 for the shape and 30 for the appearance by means of cross-validation."}, {"heading": "5.1. Analysis of Selected Source Directions", "text": "In this section, we will analyze the main directions chosen by our subspace selection method. However, a visualization of the main components of the source form for Setting 2 is shown in Figure 6. Arrows on the boundary points show the difference vector between the eigenvector and the middle form. The three selected eigenvectors are enclosed in green boxes. For each main component, we will also show the percentage variance of the source and target samples when projected onto the component.The first two selected eigenvectors are main directions that cover a considerable variance around the mouth and eye region, which explains why they were selected because the target data set has a significant variance of expression around these regions. Likewise, the third selected eigenvector shows dominant movements around the eyebrow region. For further analysis, we looked at the source examples best explained by the selected directions (Figure 7). As you can see, the source variants are well explained by the source examples."}, {"heading": "6. Conclusions and Future Work", "text": "Our method is based on selecting a subset of source eigenvectors and the target subspace. We have shown an equivalence between the similarity metric and the variance captured by source eigenvectors in the target space - which has been our basis for selecting a subset of source eigenvectors and the target subspace. We have conducted our experiments using six publicly available datasets and tested challenging scenarios in which the variations in the target domain were significantly different from those in the source domain. Our method exceeds the state of the art AAM transfer learning approach [11] and other baselines. We note that the experimental setups tested in [11] were significantly less challenging than examples of source and target domains originating from the same datasets."}, {"heading": "7. Acknowledgments", "text": "This research was supported by AGE-WELL NCE, the Canadian Institutes of Health Research (CIHR) and the Toronto Rehabilitation Institute University Health Network (TRI-UHN)."}], "references": [{"title": "Active appearance models", "author": ["Timothy F Cootes", "Gareth J Edwards", "Christopher J Taylor"], "venue": "IEEE Transactions on PAMI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Menpo: A comprehensive platform for parametric image alignment and visual deformable models", "author": ["Joan Alabort-i Medina", "Epameinondas Antonakos", "James Booth", "Patrick Snape", "Stefanos Zafeiriou"], "venue": "In ACM International Conference on Multimedia,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A unified framework for compositional fitting of active appearance models", "author": ["Joan Alabort-i Medina", "Stefanos Zafeiriou"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Localizing parts of faces using a consensus of exemplars", "author": ["Peter N Belhumeur", "David W Jacobs", "David J Kriegman", "Neeraj Kumar"], "venue": "IEEE transactions on PAMI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A survey on sign language recognition", "author": ["Sumaira Kausar", "M. Younus Javed"], "venue": "In Frontiers of Information Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Towards 3d hand tracking using a deformable model", "author": ["T. Heap", "D. Hogg"], "venue": "In Automatic Face and Gesture Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Automated 3d segmentation of hippocampus based on active appearance model of brain mr images for the early diagnosis of alzheimer\u2019s disease", "author": ["Z.R. Luo", "X.J. Zhuang", "R.Z. Zhang", "J.Q. Wang", "C. Yue", "X. Huang"], "venue": "Minerva Med.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Painful data: The unbcmcmaster shoulder pain expression archive database", "author": ["Patrick Lucey", "Jeffrey F Cohn", "Kenneth M Prkachin", "Patricia E Solomon", "Iain Matthews"], "venue": "In Automatic Face and Gesture Recognition and Workshops,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["Patrick Lucey", "Jeffrey F Cohn", "Takeo Kanade", "Jason Saragih", "Zara Ambadar", "Iain Matthews"], "venue": "In CVPR Workshops,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Bayesian active appearance models", "author": ["Joan Alabort-i Medina", "Stefanos Zafeiriou"], "venue": "In Proceedings of the IEEE CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Instanceweighted transfer learning of active appearance models", "author": ["Daniel Haase", "Erid Rodner", "Joachim Denzler"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Active appearance models revisited", "author": ["Iain Matthews", "Simon Baker"], "venue": "IJCV, 60(2):135\u2013164,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Generic vs. person specific active appearance models", "author": ["Ralph Gross", "Iain Matthews", "Simon Baker"], "venue": "Image and Vision Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Interactive facial feature localization", "author": ["Vuong Le", "Jonathan Brandt", "Zhe Lin", "Lubomir Bourdev", "Thomas S Huang"], "venue": "In ECCV,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "300 faces in-the-wild challenge: The first facial landmark localization challenge", "author": ["Christos Sagonas", "Georgios Tzimiropoulos", "Stefanos Zafeiriou", "Maja Pantic"], "venue": "In IEEE ICCV Workshops,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["Xiangxin Zhu", "Deva Ramanan"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Robust real-time face detection", "author": ["Paul Viola", "Michael J Jones"], "venue": "IJCV, 57(2):137\u2013154,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Active appearance models (AAMs) are deformable generative models used to capture shape and appearance variation for various computer vision applications [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 94, "endOffset": 99}, {"referenceID": 2, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 94, "endOffset": 99}, {"referenceID": 3, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 94, "endOffset": 99}, {"referenceID": 4, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 174, "endOffset": 180}, {"referenceID": 5, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 174, "endOffset": 180}, {"referenceID": 6, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 7, "context": "[8,9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "[8,9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 9, "context": "Although the collection and annotation of datasets was necessary to ensure that models capture enough variation, it has also led the community to realize that collecting more and more data might not be the best approach [10, 11].", "startOffset": 220, "endOffset": 228}, {"referenceID": 10, "context": "Although the collection and annotation of datasets was necessary to ensure that models capture enough variation, it has also led the community to realize that collecting more and more data might not be the best approach [10, 11].", "startOffset": 220, "endOffset": 228}, {"referenceID": 9, "context": "Despite this effort over the last two decades, the generalizability of AAMs to new domains remains challenging [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "To overcome this issue, transfer learning has received attention from the AAM research community [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "\u2022 In experiments on six publicly available datasets, we show that our approach outperforms a series of baselines including state of the art AAM transfer learning method [11] (\u00a75).", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "We begin with a brief description of AAMs and their training process, following [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": ", sN ) are first aligned using Generalized Procrustes Analysis [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "The outcome of procrustes analysis is a global 2D similarity transformation which can also be modeled by a linear combination of four basis vectors [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "Although AAMs have seen tremendous success for a number of computer vision applications, their generalizablity is still sometimes challenging [10].", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "Specifically, when fitting is required on a very different set of images than those used to train the model, the fitting performance declines [10, 11].", "startOffset": 142, "endOffset": 150}, {"referenceID": 10, "context": "Specifically, when fitting is required on a very different set of images than those used to train the model, the fitting performance declines [10, 11].", "startOffset": 142, "endOffset": 150}, {"referenceID": 7, "context": "[8, 9]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[8, 9]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 10, "context": "When a model is learned using only a few annotated examples, often it is not sufficiently expressive due to lack of enough variation [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Transfer learning comes in several settings depending on whether the data are labeled in the source/target domain and whether the learning tasks in the source and target domains are the same [13].", "startOffset": 191, "endOffset": 195}, {"referenceID": 13, "context": "Such a model leads to extreme overfitting, due to the lack of variation in the training set [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "This effect becomes pronounced especially when the model is trained with high dimensional images [11], as is often the case.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "The fourth baseline we consider is the subspace transfer (ST) method [11], which employs the mean shape and mean appearance of target model, i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "Finally, the current state-of-the-art AAM transfer learning employs an instance-weighted (IW) approach by assigning weights to source domain samples based on importance sampling [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 3, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 78, "endOffset": 81}, {"referenceID": 14, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "In addition, we selected 320 examples from the UNBCMcMaster Shoulder Pain Expression Archive [8] by temporal downsampling (1 in 100) of videos in the dataset.", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "For fitting, the Wiberg Inverse Compositional algorithm was used [2, 3].", "startOffset": 65, "endOffset": 71}, {"referenceID": 2, "context": "For fitting, the Wiberg Inverse Compositional algorithm was used [2, 3].", "startOffset": 65, "endOffset": 71}, {"referenceID": 17, "context": "To initialize the fitting procedure, a bounding box is first fit around the face using the Viola-Jones face detector [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "We use two standard criteria defined previously in the literature [3, 10] for evaluating AAM performance.", "startOffset": 66, "endOffset": 73}, {"referenceID": 9, "context": "We use two standard criteria defined previously in the literature [3, 10] for evaluating AAM performance.", "startOffset": 66, "endOffset": 73}, {"referenceID": 16, "context": "To quantify fitting accuracy, we measure the RMS error between the points of fitted shape and the ground truth landmark points normalized by the face size (average height and width of face) as suggested in [17].", "startOffset": 206, "endOffset": 210}, {"referenceID": 10, "context": "The IW approach [11] has a small improvement in percentage of converged examples over previous models; but unexpectedly Figure 5.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "We also show the weights assigned to these examples using the instanceweighted approach [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "Last row shows source samples that were assigned highest weights based on [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "Our method outperforms the state of the art AAM transfer learning approach [11] and other baselines.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "We note that the experimental setups tested in [11] were significantly less challenging as examples for source and target domains came from the same dataset.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "Active appearance models (AAMs) are a class of generative models that have seen tremendous success in face analysis. However, model learning depends on the availability of detailed annotation of canonical landmark points. As a result, when accurate AAM fitting is required on a different set of variations (expression, pose, identity), a new dataset is collected and annotated. To overcome the need for time consuming data collection and annotation, transfer learning approaches have received recent attention. The goal is to transfer knowledge from previously available datasets (source) to a new dataset (target). We propose a subspace transfer learning method, in which we select a subspace from the source that best describes the target space. We propose a metric to compute the directional similarity between the source eigenvectors and the target subspace. We show an equivalence between this metric and the variance of target data when projected onto source eigenvectors. Using this equivalence, we select a subset of source principal directions that capture the variance in target data. To define our model, we augment the selected source subspace with the target subspace learned from a handful of target examples. In experiments done on six publicly available datasets, we show that our approach outperforms the state of the art in terms of the RMS fitting error as well as the percentage of test examples for which AAM fitting converges to the ground truth.", "creator": "LaTeX with hyperref package"}}}