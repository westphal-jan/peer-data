{"id": "1708.08611", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Safe Reinforcement Learning via Shielding", "abstract": "Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield is introduced in the traditional learning process in two alternative ways, depending on the location at which the shield is implemented. In the first one, the shield acts each time the learning agent is about to make a decision and provides a list of safe actions. In the second way, the shield is introduced after the learning agent. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios.", "histories": [["v1", "Tue, 29 Aug 2017 07:16:54 GMT  (3245kb,D)", "http://arxiv.org/abs/1708.08611v1", null], ["v2", "Sun, 3 Sep 2017 20:35:33 GMT  (3244kb,D)", "http://arxiv.org/abs/1708.08611v2", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["mohammed alshiekh", "roderick bloem", "ruediger ehlers", "bettina k\\\"onighofer", "scott niekum", "ufuk topcu"], "accepted": false, "id": "1708.08611"}, "pdf": {"name": "1708.08611.pdf", "metadata": {"source": "CRF", "title": "Safe Reinforcement Learning via Shielding", "authors": ["Mohammed Alshiekh", "Roderick Bloem", "R\u00fcdiger Ehlers", "Bettina K\u00f6nighofer", "Scott Niekum", "Ufuk Topcu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Related Work", "text": "In fact, it is the case that you are able to live in a country where most people are able to live in a country where they are able to assert themselves, to live, to live, to live and to live."}, {"heading": "3 Preliminaries", "text": "A word is defined as a finite or infinite sequence of elements from a series of elements. (The set of finite words over a (finite) sentence X is a function: X \u2192 [0, 1]. [0, 1] [R] is a tuple with a finite number of states. (X) = 1. The composition of all distributions on X is indicated by the symbol of Distr. (X).A Markov decision-making (MDP) M = (S, sI) is a tuple with a finite number of states, a unique initial state sI, a finite set of states A = {a1."}, {"heading": "4 Safety Specifications, Abstractions, and Game Solving", "text": "The goal of this paper is to combine the best of two worlds, namely: (1) the formal correctness we have with respect to a temporal logical specification provided by formal methods (and reactive syntheses), and (2) the optimization with respect to a potentially insufficient performance requirement provided by enhanced learning processes. (2) That is, it cannot always be the example of a path planner for autonomous vehicles. (3) Many general requirements for system behavior such as safety concerns can be known and expressed as specifications in time logic and can be enforced by reactive controllers. (This always includes driving in the correct lane, and never exceeding the speed limit.) A learning algorithm is able to integrate more subtle considerations, such as specific intentions for the current application scenario and personal preferences of the human driver, such as achieving some safe learning goals while simultaneously reinforcing a synthesis."}, {"heading": "5 Framework for Shielded Reinforcement Learning", "text": "In this section, we introduce into the traditional learning process a reactive system called a shield. We propose two different ways to modify the loop between the learning agent and its environment: In paragraph 5.1, we introduce the shield in front of the learning agent. At each step, the shield changes the list of actions available to the learner by providing a list of safe actions from which the learning agent can choose. In paragraph 5.2, the shield is implemented after the learning agent. The shield monitors the actions selected by the learning agent and overrides them if and only if the action taken is unsafe. Starting from the place where the shield is applied, we call it preventive shielding or postulated shielding. For both settings, we assume the following assumptions. Assumption 1 (i) The environment can be modeled as a shield, as MDPM = (S, sI, A, P, R)."}, {"heading": "5.1 Preemptive Shielding", "text": "The interaction between the agent, the environment and the shield is as follows: At each step t, the shield calculates a series of all the safe actions (\"a1t,\" \".,\" act \"), i.e. it records the amount of all available actions and removes all unsafe actions that would violate the safety specification. The shield receives this list from the shield and selects one action at each step so that only safe actions are carried out. The environment carries out actions, moves to the next shield + 1, and offers the reward rt + 1. The task of the shield is essentially to modify the available actions of the protective partner in each step so that only safe actions remain. Formally, for a preventive shield, we have a shield where we have a shield, as the shield outputs the actions for the learner, in order to choose from the respective next step."}, {"heading": "5.2 Post-Posed Shielding", "text": "We propose a second screening setting, in which the shield is placed according to the learning algorithm, as shown in Fig. 7. The shield monitors the actions of the agent and replaces the selected actions with safe actions, if this is therefore necessary to prevent the violation of s. In each step t, the agent selects an action a1t. The shield forwards a 1 t to the environment, i.e., to = a1t. Only when a 1 t is unsafe in relation to s, the shield selects another safe action, a1t instead. The environment leads, moves to st + 1 and offers rt + 1. The agent receives and rt + 1, and carries out policy updates based on this information. For the action carried out, the agent updates its policy with rt + 1. The question is what the reward for a1t should be, a 1 t. We are discussing two different approaches."}, {"heading": "6 A Shield Synthesis Algorithm for Reinforcement Learning", "text": "In both cases, we must assume that there are two characteristics: correctness and minimal interference. (In this section, we will give an algorithm to calculate the agent as rarely as possible. S shield is calculated by reactive synthesis, which forces the calculated shields (1) to represent the environment in which the agent should operate. (2) In this section, we will give an algorithm to calculate and screen the shields for preemptive shields. We will prove that the shields (1) represent the shields (2) among those that force shields for all MDPs, an abstraction. The first steps of the shield are the two variants of the shields."}, {"heading": "6.1 Correctness and Minimal Interference of the Shields", "text": "We prove that the shields calculated according to the definitions actually have the claimed properties, namely correctness and minimum interference. For the sake of brevity, we have detailed the case of the preventive shields. The line of reasoning for the subsequent shielding is similar. Correctness: A shield works correctly if for each lane s0a0s1a1... is in the language of the specification \"S\" for the MDP marking function f. \"In addition, the shielding must always report at least one available action at each step. Let q0q1..."}, {"heading": "7 Convergence", "text": "Define an MDPM = (S, sI, A, P, R), with discrete state set S, discrete state-dependent measures set As, and state-dependent transition functions Ps (a, s \"), which define the likelihood of transition to state s\" if they make a mistake in states. Let's also assume that a sign S = (QS, q0, S, I, S, O, S, S, S, S) for M and for some MDP labeling functions f: S \u2192 L. For both preventive and postpositioned shielding, we can build a product MDP M that represents the behavior of the sign and the MDP together. SinceM \"is a standard MDP, all learning algorithms that conform to standard MDPs can be shown to be converted in the presence of a protective shield under this construction."}, {"heading": "8 Experiments", "text": "We applied shielded reinforcement learning in four areas: (1) a robot in 9x9 and 15x9 grid worlds, (2) a self-driving car scenario, (3) an Atari R \u00a9 game called SeaquestTM, and (4) the water tank example from Section 4. To illustrate, we compare a subset of shielding settings that we will specify later for each problem. Simulations were performed on a computer equipped with an Intel R \u00a9 CoreTMi7-4790K and 16GB of RAM running a 64-bit version of Ubuntu R \u00a9 16.04 LTS. Source code, input files, and detailed instructions for reproducing our experiments are available for download.1"}, {"heading": "8.1 Grid world Example", "text": "rE \"s tis rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\""}, {"heading": "8.2 A Self-Driving Car Example", "text": "In this example, an agent is considered who learns to circle a block clockwise in an environment with a size of 480x480 pixels. At each step, the car moves 3 pixels in direction of travel and can make a maximum rotation of 7.5 degrees in the shortest direction to the commanded direction of travel. After each step, the value of the reward and the new state of the car are returned. The state consists of the following four variables: the position of the car in the X-axis, its position in the Y-axis, the cosine and the sine of its direction of travel. The safety specification in this example is to avoid a collision with a wall. Entering the sign is calculated from the state of the car. It represents the side of the car with a distance of less than 60 pixels away from one of the walls. Both the preventive and the trailing sign were synthesized in 2 seconds. In each step, a positive reward is given when the car moves clockwise."}, {"heading": "8.3 Atari R\u00a9 2600 SeaquestTM", "text": "SeaquestTM is an underwater combat game in which the agent controls a submarine, and the agent must pick up divers underwater while avoiding or destroying various objects and getting to the surface before running out of oxygen. The agent's goal is to maximize the savegame. For our experiments, we used the OpenAI Gym1 library, which integrates the Arcade Learning Environment (ALE) [2], and a Python implementation2 of DeepMind's Deep1 https: / / gym.openai.com / 2 https: / / github.com / devsisters / DQN-tensorflowReinforcement Learning approach [13]. The agent receives input only RGB images of the screen as shown in Fig. 13 (right) The agent is used purely as a black box, only to change the surface properties that violate the specification."}, {"heading": "8.4 The Water Tank Example", "text": "In the example shown in Fig. 3, the tank must never run empty or overflow by controlling the inlet switch (s1). In addition, the inlet switch must not change its operating mode until three time steps have elapsed since the last change in operating mode (s2). See Example 1 of Section 4, for a complete description of the abstract dynamics and specification of the water tank. For this example, we have created a concrete MDP in which the energy consumption only depends on the state and there are several local minimums. From s1 s2, a downstream sign was synthesized, in less than a second. Fig. 14 shows that both shielded (dashed lines) and unshielded Q-Learning and SARSA experiments (fixed lines) achieve an optimal policy. However, the shielded implementations achieve the optimal policy in significantly shorter time than the unshielded implementations."}, {"heading": "9 Conclusion", "text": "The method is based on shielding the decisions of the underlying learning algorithm from violation of the specification. We proposed an algorithm for the automated synthesis of shields for given temporal logic specifications. Although the inner workings of a learning algorithm are often complex, the safety criteria can still be enforced by potentially simple means. Shielding takes advantage of these possibilities. In all cases, the shielded agents work at least as well as the unshielded ones. In most cases, our approach even shows improvement in learning performance across multiple reinforcement learning scenarios."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Francisco Palau-Romero for his help with some of the experiments. The first and last authors were partially supported by AFRL FA865015-C-2546, AFRL 8650-16-C-2610, DARPA W911NF-16-1-0001 and ARO W911NF15-1-0592, the last two authors partially by NSF 1617639. The second and fourth authors were supported by the Austrian Science Fund (FWF) within the framework of the projects RiSE (S11406-N23) and LogiCS (W1255-N23)."}], "references": [{"title": "Principles of Model Checking (Representation and Mind Series)", "author": ["C. Baier", "J.P. Katoen"], "venue": "The MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Art. Intell. Research 47, 253\u2013279", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Shield synthesis: - runtime enforcement for reactive systems", "author": ["R. Bloem", "B. K\u00f6nighofer", "R. K\u00f6nighofer", "C. Wang"], "venue": "Tools and Algorithms for the Construction and Analysis of Systems - 21st Int. Conf, TACAS 2015, London, UK, April 11-18, 2015. pp. 533\u2013548", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "On integrating apprentice learning and reinforcement learning title2", "author": ["J. Clouse"], "venue": "Tech. rep., Amherst, MA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonlinear controller synthesis and automatic workspace partitioning for reactive high-level behaviors", "author": ["J.A. DeCastro", "H. Kress-Gazit"], "venue": "Proceedings of the 19th International Conference on Hybrid Systems: Computation and Control, HSCC 2016. pp. 225\u2013234", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Handbook of theoretical computer science (vol", "author": ["E.A. Emerson"], "venue": "b). chap. Temporal and Modal Logic, pp. 995\u20131072. MIT Press, Cambridge, MA, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Synthesis of shared autonomy policies with temporal logic specifications", "author": ["J. Fu", "U. Topcu"], "venue": "IEEE Trans. Automation Science and Engineering 13(1), 7\u201317", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "A comprehensive survey on safe reinforcement learning", "author": ["J. Garc\u0131\u0301a", "F. Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research 16, 1437\u20131480", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Discrete-time control for rectangular hybrid automata", "author": ["T.A. Henzinger", "P.W. Kopke"], "venue": "Theor. Comput. Sci. 221(1-2), 369\u2013392", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Safety-constrained reinforcement learning for mdps", "author": ["S. Junges", "N. Jansen", "C. Dehnert", "U. Topcu", "J. Katoen"], "venue": "Tools and Algorithms for the Construction and Analysis of Systems 22nd Int. Conference, TACAS 2016, The Netherlands, April 2-8, 2016. pp. 130\u2013146", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Model checking of safety properties", "author": ["O. Kupferman", "M.Y. Vardi"], "venue": "Formal Methods in System Design 19(3),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Temporal verification of reactive systems - safety", "author": ["Z. Manna", "A. Pnueli"], "venue": "Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature 518(7540), 529\u2013533", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Safe exploration in markov decision processes", "author": ["T.M. Moldovan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1205.4810", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Safe Exploration Techniques for Reinforcement Learning \u2013 An Overview, pp", "author": ["M. Pecka", "T. Svoboda"], "venue": "357\u2013375. Springer International Publishing, Cham", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "The temporal logic of programs", "author": ["A. Pnueli"], "venue": "Annual Symposium on Foundations of Computer Science, Providence, Rhode Island, USA,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1977}, {"title": "Safety first: a two-stage algorithm for the synthesis of reactive systems", "author": ["S. Sohail", "F. Somenzi"], "venue": "STTT 15(5-6),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "IEEE Trans. Neural Networks 9(5), 1054\u20131054", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence - Vol. 1. pp. 1000\u20131005. AAAI Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "Artif. Intell. 172(6-7), 716\u2013737", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Commun. ACM 27(11), 1134\u20131142", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1984}, {"title": "Learning on real robots from experience and simple user feedback", "author": ["P.Q. Vidal", "R.I. Rodr\u0131\u0301guez", "M.R. Gonz\u00e1lez", "C.V. Regueiro"], "venue": "Journal of Physical Agents 7(1)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Correct-by-synthesis reinforcement learning with temporal logic constraints", "author": ["M. Wen", "R. Ehlers", "U. Topcu"], "venue": "IROS 2015, Germany, Sep. 28 - Oct. 2, 2015. pp. 4983\u20134990", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Receding horizon temporal logic planning", "author": ["T. Wongpiromsarn", "U. Topcu", "R.M. Murray"], "venue": "IEEE Trans. Automat. Contr. 57(11), 2817\u20132830", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "For example, in reinforcement learning (RL), an agent acts to optimize a long-term return that models the desired behavior for the agent and is revealed to it incrementally in a reward signal as it interacts with its environment [18].", "startOffset": 229, "endOffset": 233}, {"referenceID": 7, "context": "A number of different notions of safety were recently explored [8,15].", "startOffset": 63, "endOffset": 69}, {"referenceID": 14, "context": "A number of different notions of safety were recently explored [8,15].", "startOffset": 63, "endOffset": 69}, {"referenceID": 5, "context": "To this end, we adopt temporal logic as a specification language [6].", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "For algorithmic purposes, we focus on the so-called safety fragment of (linear) temporal logic [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "An exploration process is called safe if no undesirable states are ever visited, which can only be achieved through the incorporation of external knowledge [8,14].", "startOffset": 156, "endOffset": 162}, {"referenceID": 13, "context": "An exploration process is called safe if no undesirable states are ever visited, which can only be achieved through the incorporation of external knowledge [8,14].", "startOffset": 156, "endOffset": 162}, {"referenceID": 7, "context": "The safety fragment of temporal logic that we consider is more general than the notion of safety of [8] (which is technically a so-called invariance property [1]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": "The safety fragment of temporal logic that we consider is more general than the notion of safety of [8] (which is technically a so-called invariance property [1]).", "startOffset": 158, "endOffset": 161}, {"referenceID": 14, "context": ", safe actions) when either the learner [15,4] or the teacher [22,19] considers it to be necessary to prevent catastrophic situations.", "startOffset": 40, "endOffset": 46}, {"referenceID": 3, "context": ", safe actions) when either the learner [15,4] or the teacher [22,19] considers it to be necessary to prevent catastrophic situations.", "startOffset": 40, "endOffset": 46}, {"referenceID": 21, "context": ", safe actions) when either the learner [15,4] or the teacher [22,19] considers it to be necessary to prevent catastrophic situations.", "startOffset": 62, "endOffset": 69}, {"referenceID": 18, "context": ", safe actions) when either the learner [15,4] or the teacher [22,19] considers it to be necessary to prevent catastrophic situations.", "startOffset": 62, "endOffset": 69}, {"referenceID": 18, "context": "In each time step, the human teacher tunes the reward signal before sending it to the agent [19,20].", "startOffset": 92, "endOffset": 99}, {"referenceID": 19, "context": "In each time step, the human teacher tunes the reward signal before sending it to the agent [19,20].", "startOffset": 92, "endOffset": 99}, {"referenceID": 23, "context": "[24] define a receding horizon control approach that combines continuous control with discrete correctness guarantees.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "For simple system dynamics, the controller can be computed directly [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "A mitigation strategy is to compute a set of low-level motion primitives to be combined to an overall strategy [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 22, "context": "[23] propose a method to combine strict correctness guarantees with reinforcement learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] adopt a similar framework in a stochastic setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23,10] on the one hand and the shielding framework on the other hand is the fact that the computational cost of the construction of the shield depends on the complexity of the specification and a very abstract version of the system, and is independent of the state space components of the system to be controlled that are", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[23,10] on the one hand and the shielding framework on the other hand is the fact that the computational cost of the construction of the shield depends on the complexity of the specification and a very abstract version of the system, and is independent of the state space components of the system to be controlled that are", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[7] establish connections between temporal-logic-constrained strategy synthesis in Markov decision processes and probably-approximately-correct-type bounds in learning [21].", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[7] establish connections between temporal-logic-constrained strategy synthesis in Markov decision processes and probably-approximately-correct-type bounds in learning [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 2, "context": "[3] proposed the idea to synthesize a shield that is attached to a system to enforce safety properties at run time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "A probability distribution over a (finite) set X is a function \u03bc : X \u2192 [0, 1] \u2286 R with \u2211 x\u2208X \u03bc(x) = \u03bc(X) = 1.", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "The return R = \u2211\u221e t=0 \u03b3 rt is the cumulative future discounted reward, where rt is the immediate reward at time step t, and \u03b3 \u2208 [0, 1] is the discount factor that controls the influence of future rewards.", "startOffset": 128, "endOffset": 134}, {"referenceID": 16, "context": "The larger generality of this model is needed for one type of shield that we introduce later, and such an extended Mealy-type computational model has already been used by Saqib and Somenzi [17] in the past.", "startOffset": 189, "endOffset": 193}, {"referenceID": 15, "context": "Linear temporal logic [16] (LTL) is a commonly used formal specification language.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "For specifications in LTL, it is known how to check if it is a safety language and how to compute a safety automaton that represents it [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 22, "context": "This includes always driving in the correct lane, never jumping the red light, and never exceeding the speed limit [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "Note that since safety specifications given in linear temporal logic can be translated to such automata [11], this assumption does not preclude the use of temporal logic as specification formalism.", "startOffset": 104, "endOffset": 108}, {"referenceID": 2, "context": "Next, we compute the winning region W \u2286 Fg of G by standard safety game solving as described in [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "For our experiments, we used the OpenAI Gym1 library that integrates the Arcade Learning Environment (ALE) [2], and a Python implementation2 of DeepMind\u2019s Deep", "startOffset": 107, "endOffset": 110}, {"referenceID": 12, "context": "Reinforcement Learning approach [13].", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield is introduced in the traditional learning process in two alternative ways, depending on the location at which the shield is implemented. In the first one, the shield acts each time the learning agent is about to make a decision and provides a list of safe actions. In the second way, the shield is introduced after the learning agent. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios.", "creator": "LaTeX with hyperref package"}}}