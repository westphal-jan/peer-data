{"id": "1708.08615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Comparing Human and Machine Errors in Conversational Speech Transcription", "abstract": "Recent work in automatic recognition of conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, although there is some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set. This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors. In this paper we approach this question by comparing the output of our most accurate CTS recognition system to that of a standard speech transcription vendor pipeline. We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap. The only notable exception is that the automatic recognizer tends to confuse filled pauses (\"uh\") and backchannel acknowledgments (\"uhhuh\"). Humans tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words. Furthermore, we quantify the correlation between human and machine errors at the speaker level, and investigate the effect of speaker overlap between training and test data. Finally, we report on an informal \"Turing test\" asking humans to discriminate between automatic and human transcription error cases.", "histories": [["v1", "Tue, 29 Aug 2017 07:21:39 GMT  (89kb,D)", "http://arxiv.org/abs/1708.08615v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andreas stolcke", "jasha droppo"], "accepted": false, "id": "1708.08615"}, "pdf": {"name": "1708.08615.pdf", "metadata": {"source": "CRF", "title": "Comparing Human and Machine Errors in Conversational Speech Transcription", "authors": ["Andreas Stolcke", "Jasha Droppo"], "emails": ["anstolck@microsoft.com,", "jdroppo@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2. Measuring Human Error", "text": "In fact, the reality is that most people who are able to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves and to outlive. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "3. Machine Transcription System", "text": "The system uses independent decoding through various acoustic models, including the Convolutionary Neural Network (CNN) and Bidirectional Long-Term Short-Term Memory (BLSTM), which differ by model architecture, number of senons, amount of training data, and other meta-parameters; the decoding uses a truncated 4 gram N gram speech model (LM) to generate grids, which are then expanded into 500 leaderboards using a larger N gram LM. The N leaderboards are corrected with multiple LSTM LMs working in forward and backward directions; the model values are combined loglinearly at the statement level and converted into downstream probabilities, which are presented as word confusion networks; the various subsystems that make up the final system are selected in a greedy search, and their weights are optimized on development data via an expectation maximization algorithm; the active parts of the data networks are represented as the 12.5%, the final subsystems being the 12.5%."}, {"heading": "4. Error Distribution and Correlation", "text": "We note in passing that machine and human transcription do not differ significantly according to the Wilcoxon and Matched Pairs Sentence Segment Word Error tests as applied by NIST, nor do they differ according to a character test that compares error rates at the level of utterance. However, a first high-level question regarding the relationship between word errors by machine and human transcriptors is whether difficulty in one predicts difficulty in the other. Figure 1 shows that scatter plots of the speaker-level error rates (machine vs. human), separated by corpus. Each corpus subset has 40 conversations correlate to that level, with the errors being correlated to the point = 0.65 for SWB and points = 0.73 for CH. This suggests that the characteristics of the speech, either as a function of the content, the speaker, or the channel (each speaker occurs in exactly one test conversation), cause errors for both the machine and for the human transcription data that we observe with two."}, {"heading": "5. Error types", "text": "Tables 1-3 show the top ten types of substitutions, deletions, and insertions for both ASR and human transcripts. Inspections show that the same short function words, discourse markers, and filled pauses appear in the top ten errors for both systems. However, there is one notable exception. The top substitution error for the ASR system involves error detection of filled pauses (\"% Hesitate,\" a word class term that includes \"er\" and \"um\" in different spellings) as back channel confirmation (\"% bcack,\" stands for \"uhhuh,\" \"mhm,\" etc.).2 The same substitution error is much rarer in human transcripts. One possible explanation for this asymmetry lies in the discourse functions of filled pauses and back channels. Filled pauses are either used to maintain or maintain the floor, signaling that the speaker either wants to start speaking or to continue listening to the speaker, or thus recognizing that the speaker is listening to the differences in their respective backchannels."}, {"heading": "6. A Turing-like Experiment", "text": "After finding that human and machine transcriptions are quite similar in several aspects, including the types of word marks involved, we wondered if higher error patterns could distinguish the two systems. For example, one might expect human error detection to be guided by a strong \"human\" language and comprehension model, while machine errors are more likely to generate syntactical and semantic nonsense. To get to this question, we designed a specialized version of the classic Turing test, in the sense that a human judge is asked to interact with a system with the aim of assessing whether it is supported by human or artificial \"intelligence.\" In our case, it was a matter of checking a randomly chosen utterance from the test at a given time, with a side-by-side representation of the reference script, the human transcript, and the ASR output (after text normalization, which is part of the scoring protocol, not part of the protocol)."}, {"heading": "7. Conclusions", "text": "We discussed methodological issues and presented initial results in comparing automatic speech transcriptions with human performance, using data generated by our recent efforts to achieve human parity in CTS recognition. While accurate characterization of the human benchmark remains a moving goal that is the subject of debate, our results to date have shown that machine transcription errors track human errors in several important respects. At the speaker (and corpus) level, the two error rates strongly correlate, suggesting that common underlying factors in the language data determine transcription difficulties for both humans and ASR systems. (Detailed characterization of these factors is exemplary in ASR research and should be repeated taking into account human performance.) A partial overlap of switch training and test speakers does not appear to have a major impact on error rates. We also find that the most common error items include both the same error patterns for short radio patterns and for disc sparing machines."}, {"heading": "8. Acknowledgments", "text": "We would like to thank our co-authors and collaborators on the Human Parity Project: X. Huang, F. Seide, M. Seltzer, W. Xiong, D. Yu and G. Zweig. Thanks to K. Riedhammer for exchanging metadata on overlaps between train speakers and test speakers."}, {"heading": "9. References", "text": "[1] S. F. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon, H. Soltau, and G. Zweig, \"Advances in speech transcription at IBM under the DARPA EARS program,\" IEEE Trans. Audio, Speech, and Language Processing, vol. 14, pp. 1596-1608, 2006. [2] S. Matsoukas, J.-L. Gauvain, G. Adda, T. Colthurst, C.-L. Kao, O. Kimball, L. Lamel, F. Lefevre, J. Ma, J. Makhoul, et al., \"Advances in transcription of broadcast news and talking telephone speech within the combined ears bbn / limsi system,\" IEEE Transactions on Audio, Speech, and Language Processing, Processing, vol. 14, pp. 1541-1556, 2006."}], "references": [{"title": "Advances in speech transcription at IBM under the DARPA EARS program", "author": ["S.F. Chen", "B. Kingsbury", "L. Mangu", "D. Povey", "G. Saon", "H. Soltau", "G. Zweig"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, vol. 14, pp. 1596\u20131608", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "J", "author": ["S. Matsoukas", "J.-L. Gauvain", "G. Adda", "T. Colthurst", "C.-L. Kao", "O. Kimball", "L. Lamel", "F. Lefevre", "J.Z. Ma"], "venue": "Makhoul, et al., \u201cAdvances in transcription of broadcast news and conversational telephone speech within the combined ears bbn/limsi system\u201d, IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, pp. 1541\u20131556", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "X", "author": ["A. Stolcke", "B. Chen", "H. Franco", "V.R.R. Gadde", "M. Graciarena", "M.-Y. Hwang", "K. Kirchhoff", "A. Mandal", "N. Morgan"], "venue": "Lei, et al., \u201cRecent innovations in speech-to-text transcription at SRI-ICSI- UW\u201d, IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, pp. 1729\u20131744", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "The AT&T 2001 LVCSR system", "author": ["A. Ljolje"], "venue": "NIST LVCSR Workshop", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Conversational telephone speech recognition", "author": ["J.-L. Gauvain", "L. Lamel", "H. Schwenk", "G. Adda", "L. Chen", "F. Lefevre"], "venue": "Proc. IEEE ICASSP, vol. 1, pp. I\u2013212. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Development of the 2003 CU-HTK conversational telephone speech transcription system", "author": ["G. Evermann", "H.Y. Chan", "M.J.F. Gales", "T. Hain", "X. Liu", "D. Mrva", "L. Wang", "P.C. Woodland"], "venue": "Proc. IEEE ICASSP, vol. 1, pp. I\u2013249. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Proc. Interspeech, pp. 437\u2013440", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "The IBM 2015 English conversational telephone speech recognition system", "author": ["G. Saon", "H.-K.J. Kuo", "S. Rennie", "M. Picheny"], "venue": "Proc. Interspeech, pp. 3140\u20133144", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The IBM 2016 English conversational telephone speech recognition", "author": ["G. Saon", "T. Sercu", "S.J. Rennie", "H.J. Kuo"], "venue": "system\u201d, in Proc. Interspeech,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Proc. Interspeech, pp. 338\u2013342", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["H. Sak", "A. Senior", "K. Rao", "F. Beaufays"], "venue": "Proc. Interspeech, pp. 1468\u20131472", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "Proc. IEEE ICASSP, pp. 4955\u20134959. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional neural networks for LVCSR", "author": ["M. Bi", "Y. Qian", "K. Yu"], "venue": "Proc. Interspeech, pp. 3259\u20133263", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional neural networks for noise robust speech recognition", "author": ["Y. Qian", "M. Bi", "T. Tan", "K. Yu"], "venue": "IEEE/ACM Trans. Audio, Speech, and Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Deep convolutional neural networks with layer-wise context expansion and attention", "author": ["D. Yu", "W. Xiong", "J. Droppo", "A. Stolcke", "G. Ye", "J. Li", "G. Zweig"], "venue": "Proc. Interspeech, pp. 17\u201321", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Proc. Interspeech, pp. 1045\u20131048", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "Proc. Interspeech, pp. 901\u2013904", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "Proc. Interspeech, pp. 194\u2013197", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving English conversational telephone speech recognition", "author": ["I. Medennikov", "A. Prudnikov", "A. Zatvornitskiy"], "venue": "Proc. Interspeech, pp. 2\u20136", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "Technical Report MSR-TR-2016-71, Microsoft Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "English conversational telephone speech recognition by humans and machines", "author": ["G. Saon", "G. Kurata", "T. Sercu", "K. Audhkhasi", "S. Thomas", "D. Dimitriadis", "X. Cui", "B. Ramabhadran", "M. Picheny", "L.-L. Lim", "B. Roomi", "P. Hall"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Speech recognition by machines and humans", "author": ["R.P. Lippmann"], "venue": "Speech Communication, vol. 22, pp. 1\u201315", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Transcription methods for consistency", "author": ["M.L. Glenn", "S. Strassel", "H. Lee", "K. Maeda", "R. Zakhary", "X. Li"], "venue": "volume and efficiency\u201d, in LREC", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Insights into spoken language gleaned from phonetic transcription of the switchboard corpus", "author": ["S. Greenberg", "J. Hollenback", "D. Ellis"], "venue": "Proc. ICSLP", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "About a decade ago, CTS recognition had served as an evaluation task for governmentsponsored work in speech recognition, predating the take-over of deep learning approaches and still largely in the GMM-HMM modeling framework [1, 2, 3, 4, 5, 6].", "startOffset": 225, "endOffset": 243}, {"referenceID": 1, "context": "About a decade ago, CTS recognition had served as an evaluation task for governmentsponsored work in speech recognition, predating the take-over of deep learning approaches and still largely in the GMM-HMM modeling framework [1, 2, 3, 4, 5, 6].", "startOffset": 225, "endOffset": 243}, {"referenceID": 2, "context": "About a decade ago, CTS recognition had served as an evaluation task for governmentsponsored work in speech recognition, predating the take-over of deep learning approaches and still largely in the GMM-HMM modeling framework [1, 2, 3, 4, 5, 6].", "startOffset": 225, "endOffset": 243}, {"referenceID": 3, "context": "About a decade ago, CTS recognition had served as an evaluation task for governmentsponsored work in speech recognition, predating the take-over of deep learning approaches and still largely in the GMM-HMM modeling framework [1, 2, 3, 4, 5, 6].", "startOffset": 225, "endOffset": 243}, {"referenceID": 4, "context": "About a decade ago, CTS recognition had served as an evaluation task for governmentsponsored work in speech recognition, predating the take-over of deep learning approaches and still largely in the GMM-HMM modeling framework [1, 2, 3, 4, 5, 6].", "startOffset": 225, "endOffset": 243}, {"referenceID": 5, "context": "About a decade ago, CTS recognition had served as an evaluation task for governmentsponsored work in speech recognition, predating the take-over of deep learning approaches and still largely in the GMM-HMM modeling framework [1, 2, 3, 4, 5, 6].", "startOffset": 225, "endOffset": 243}, {"referenceID": 6, "context": "[7] demonstrated that deep networks as acoustic models could achieve significant improvements over GMM-HMM models on CTS data, and more recently researchers at IBM had achieved results on this task that represented a further significant advance [8, 9] over those from a decade ago.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] demonstrated that deep networks as acoustic models could achieve significant improvements over GMM-HMM models on CTS data, and more recently researchers at IBM had achieved results on this task that represented a further significant advance [8, 9] over those from a decade ago.", "startOffset": 245, "endOffset": 251}, {"referenceID": 8, "context": "[7] demonstrated that deep networks as acoustic models could achieve significant improvements over GMM-HMM models on CTS data, and more recently researchers at IBM had achieved results on this task that represented a further significant advance [8, 9] over those from a decade ago.", "startOffset": 245, "endOffset": 251}, {"referenceID": 9, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 136, "endOffset": 163}, {"referenceID": 10, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 136, "endOffset": 163}, {"referenceID": 7, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 136, "endOffset": 163}, {"referenceID": 11, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 136, "endOffset": 163}, {"referenceID": 12, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 136, "endOffset": 163}, {"referenceID": 13, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 136, "endOffset": 163}, {"referenceID": 14, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 136, "endOffset": 163}, {"referenceID": 15, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 186, "endOffset": 202}, {"referenceID": 16, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 186, "endOffset": 202}, {"referenceID": 17, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 186, "endOffset": 202}, {"referenceID": 18, "context": "Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling [10, 11, 8, 12, 13, 14, 15] and language modeling [16, 17, 18, 19] with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.", "startOffset": 186, "endOffset": 202}, {"referenceID": 19, "context": "3% on CallHome English data) [20].", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "8% for CallHome) [21].", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": "In this paper, we address the question whether there are major qualitative differences between the results of human transcriptions of conversational speech and those obtained by ASR systems, based on a detailed analysis of the data and system output from our human parity experiment [20].", "startOffset": 283, "endOffset": 287}, {"referenceID": 20, "context": "Next we ask whether speech that is difficult for ASR also tends to be hard for humans to transcribe (and vice-versa), and whether the speaker overlap with the training data that is found in a portion of the test data has a noticeable effect on the result, as was suggested in [21].", "startOffset": 276, "endOffset": 280}, {"referenceID": 21, "context": "A widely cited figure is 4% word error rate (WER), based on [22].", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "5% with very careful multiple transcriptions [23].", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "Clearly our method was not designed to achieve the highest possible human transcription accuracy; instead, as pointed out in [20], our goal was to establish a benchmark corresponding to industry-standard (i.", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "The authors in [21] undertook to measure human error on the same dataset, but using a more involved process.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "For example, conversational pronunciations are highly variable and often reduced [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "The details of our conversational speech recognition system are described elsewhere [20], so we only give a brief summary here.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "In [21] it was sugggested that one of the reasons for the much higher error rate on CH compared to SWB was that 36 of the 40 SWB test speakers occur in the portion of the SWB corpus that is used in training (due to what we surmise to be an oversight in the selection of the NIST 2000 test set).", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "Recent work in automatic recognition of conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, although there is some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set. This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors. In this paper we approach this question by comparing the output of our most accurate CTS recognition system to that of a standard speech transcription vendor pipeline. We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap. The only notable exception is that the automatic recognizer tends to confuse filled pauses (\u201cuh\u201d) and backchannel acknowledgments (\u201cuhhuh\u201d). Humans tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words. Furthermore, we quantify the correlation between human and machine errors at the speaker level, and investigate the effect of speaker overlap between training and test data. Finally, we report on an informal \u201cTuring test\u201d asking humans to discriminate between automatic and human transcription error cases.", "creator": "LaTeX with hyperref package"}}}