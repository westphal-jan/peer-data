{"id": "1708.08689", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization", "abstract": "A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we rst extend the de nition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic di erentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradient- based procedures, including neural networks and deep learning architectures. We empirically evaluate its e ectiveness on several application examples, including spam ltering, malware detection, and handwritten digit recognition. We nally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across di erent learning algorithms.", "histories": [["v1", "Tue, 29 Aug 2017 10:47:38 GMT  (1000kb,D)", "http://arxiv.org/abs/1708.08689v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luis mu\\~noz-gonz\\'alez", "battista biggio", "ambra demontis", "rea paudice", "vasin wongrassamee", "emil c lupu", "fabio roli"], "accepted": false, "id": "1708.08689"}, "pdf": {"name": "1708.08689.pdf", "metadata": {"source": "META", "title": "Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization", "authors": ["Luis Mu\u00f1oz-Gonz\u00e1lez", "Battista Biggio", "Ambra Demontis", "Andrea Paudice", "Vasin Wongrassamee", "Emil C. Lupu", "Fabio Roli"], "emails": [], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computing Methodologies \u2192 Machine Learning; KEYWORDS Adversarial Machine Learning, Training Data Poisoning, Adversarial Examples, Deep Learning."}, {"heading": "1 INTRODUCTION", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "2 THREAT MODEL", "text": "In this section we summarize the framework originally proposed in [1, 2, 16] and subsequently expanded in [7], which enables us to imagine various attack scenarios against learning algorithms (including deep learning processes) and to create the corresponding attack samples. Remarkably, these attacks include in training and during the test period, which are commonly referred to as poisoning and evasion attacks [6-8, 16, 23, 39] or, more recently, as contrary (training and testing) examples (based on deep learning algorithms) [27, 28, 36]. The framework characterizes the attacker according to her goal, her knowledge of the intended system and her ability to manipulate the input data. Based on these assumptions, it allows to define an optimal attack strategy as an optimization problem, the solution of which boils down to the construction of the attack samples, i.e., the reconciliatory parameters, i.e. the ability to move parameters originally developed for this classification framework."}, {"heading": "2.1 Attacker\u2019s Goal", "text": "The target of the attack is determined in terms of the desired security breach and attack specificity. In Multiclass Classification, the misclassification of a sample does not have a unique meaning, since there is more than one class that differs from the correct one. Accordingly, we expand the current framework by introducing the concept of error specificity. These three features are detailed below. Security breach. This feature defines the high-level security breach caused by the attack, as is usually the case in security technology. It may be: an integrity violation when malicious activities evade detection without affecting normal system operation; 4While normally the specified notation {x i, yi} ni = 1 does not allow duplicated entries, we admit that our data sets contain potentially duplicated points. 5For example, for kernel-based SVMs, w can consider the dual variability of the attack, the bias b, and even the regulation parameters of the work of the system as sufficiently protected [However, in this parameter C, we include only 39]."}, {"heading": "2.2 Attacker\u2019s Knowledge", "text": "In this case, the majority of people who are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2.3 Attacker\u2019s Capability", "text": "This property is defined on the basis of the influence that the attacker has on the input data and on the presence of data manipulation restrictions. Influence of the attack. In monitored learning, the influence of the attack can be causal if the attacker can influence both training and test data, or explorative if the attacker can only manipulate test data. These settings are more commonly referred to as poisoning and evasion attacks [2, 6-8, 16, 23, 39]. Limitations of data manipulation. Another aspect related to the attacker's ability is the presence of limitations on the manipulation of input data, which depend heavily on the given practical scenario. For example, if the attacker aims to bypass a malware classification system, he should manipulate the exploitation code embedded in the malware probe without affecting its intrusive functionality. In the case of poisoning, the training samples are not typically labeled under the attacker's control."}, {"heading": "2.4 Attack Strategy", "text": "Given the knowledge of the attacker that he knows this, and a series of manipulated attack samples, the target of the attacker can be characterized by an objective function A (D'c, \u03b8) and R, which evaluates how effective the attacks are D'c. The optimal attack strategy can be defined as follows: D-c-arg max D'c-c (Dc) A (D'c, \u03b8) (1) While this high-level formulation includes both evasive and poisoning attacks, the rest of this work focuses only on defining some poisoning scenarios."}, {"heading": "2.5 Poisoning Attack Scenarios", "text": "We focus here on two poisoning scenarios of interest to multiclass problems, which suggest that other attack scenarios can be derived in a similar manner. (Error-generic poisoning attacks) The most common scenario considered in the previous work is the poisoning of two-tier learning algorithms to trigger a denial of service. (Error-generic poisoning attacks, depending on whether it involves a specific system user or service, orany of them.) It is therefore natural to extend this scenario if the attacker is not aiming to cause specific errors, but only generical failures. (As in [8, 23, 39] this poisoning attack (as in any other poisoning case) requires the solution of a two-step optimization, where the inner problem is the learning problem that can be explicitly made to solve specific errors, but only a solution to other solution-type attacks (such as 1, 39)."}, {"heading": "3 POISONING ATTACKS WITH BACK-GRADIENT OPTIMIZATION", "text": "In this paragraph, we shall first discuss how the two-stage optimisation of Eqs (2) - (3) - (3) - (3) - (3) - (5) - (4) - (4) - (4) - (4) - (4) - (4) - (4) - (4) - (4) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) - (5) (5) (5) - (5) (5) - (5) (5) - (5) - (5) (5) - (5) - (5) - (5) - (5) (5) - (5) - (5) - (5) - (5) - (5) - (5 - (5) - (5) (5) - (5) (5) - (5) - (5) (5) (5) - (5) - (5 - (5) (5) (5) - (5) - (5 - (5) (5) - (5) (5) - (5 - (5) - (5) (5) - (5) (5) - (5) - (5 - (5) - (5) - (5) (5) - (5 - (5) (5) - (5) - (5) (5) - (5) - (5) (5) - (5) (5) - (5) - (5) - (5) - (5) (5) - (5) (5) - (5) (5) - (5) - (5) (5) (5) - (5) - (5) (5) - (5) - (5) (5) - (5) (5) (5) (5) - (5) (5) - (5) - (5) (("}, {"heading": "4 EXPERIMENTAL ANALYSIS", "text": "In this section, we will first evaluate the effectiveness of backgradient intoxication attacks on spam and malware detection tasks described in Section 3. In these cases, we will also assess whether intoxication patterns can be transferred to different learning algorithms. Subsequently, we will investigate the effects of error-generic and error-specific intoxication attacks on the well-known multiclass problem of handwritten number recognition. In this case, we will also report on the first examples of conceptual counter-training that was calculated by poisoning a foldable neural network end-to-end (i.e. not just using a surrogate model trained on depth characteristics as in [20])."}, {"heading": "4.1 Spam and Malware Detection", "text": "In fact, most of them are able to survive on their own without having to maneuver themselves into such a situation."}, {"heading": "4.2 Handwritten Digit Recognition", "text": "In fact, it is a matter of a way in which people are able to determine themselves how they want to behave. (...) It is a matter of a way in which people are able to behave. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves."}, {"heading": "5 RELATEDWORK", "text": "In fact, most of them will be able to play by the rules that they have set themselves, and they will be able to play by the rules that they have set themselves."}, {"heading": "6 CONCLUSIONS, LIMITATIONS AND FUTUREWORK", "text": "It is indeed the case that we will be able to go in search of a solution that meets the needs of the people."}], "references": [{"title": "The security of machine learning", "author": ["Marco Barreno", "Blaine Nelson", "Anthony Joseph", "J. Tygar"], "venue": "Machine Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D. Joseph", "J.D. Tygar"], "venue": "In Proc. ACM Symp. Information, Computer and Comm. Sec. (ASIACCS \u201906)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Gradient-based optimization of hyperparameters", "author": ["Y. Bengio"], "venue": "Neural Computation 12,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Poisoning complete-linkage hierarchical clustering", "author": ["Battista Biggio", "Samuel Rota Bul\u00f2", "Ignazio Pillai", "Michele Mura", "Eyasu Zemene Mequanint", "Marcello Pelillo", "Fabio Roli"], "venue": "In Joint IAPR Int\u2019l Workshop on Structural, Syntactic, and Statistical Pattern Recognition (Lecture Notes in Computer Science),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Bagging Classifiers for Fighting Poisoning Attacks in Adversarial Classification Tasks", "author": ["Battista Biggio", "Igino Corona", "Giorgio Fumera", "Giorgio Giacinto", "Fabio Roli"], "venue": "In 10th International Workshop on Multiple Classifier Systems (MCS) (Lecture Notes in Computer Science),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD)", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "Part III (LNCS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Security Evaluation of Pattern Classifiers Under Attack", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "IEEE Transactions on Knowledge and Data Engineering 26,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Poisoning attacks against support vector machines", "author": ["Battista Biggio", "Blaine Nelson", "Pavel Laskov"], "venue": "In 29th Int\u2019l Conf. on Machine Learning, John Langford and Joelle Pineau (Eds.). Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Is Data Clustering in Adversarial Settings Secure", "author": ["Battista Biggio", "Ignazio Pillai", "Samuel Rota Bul\u00f2", "Davide Ariu", "Marcello Pelillo", "Fabio Roli"], "venue": "In Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security (AISec \u201913)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Poisoning Behavioral Malware Clustering", "author": ["Battista Biggio", "Konrad Rieck", "Davide Ariu", "ChristianWressnegger", "Igino Corona", "Giorgio Giacinto", "Fabio Roli"], "venue": "In 2014 Workshop on Artificial Intelligent and Security (AISec \u201914)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "PAC Learning with Nasty Noise", "author": ["NaderH. Bshouty", "Nadav Eiron", "Eyal Kushilevitz"], "venue": "In Algorithmic Learning Theory, Osamu Watanabe and Takashi Yokomori (Eds.). Lecture Notes in Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Efficient multiple hyperparameter learning for log-linear models", "author": ["C. Do", "C.S. Foo", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Generic Methods for Optimization-Based Modeling", "author": ["Justin Domke"], "venue": "In 15th Int\u2019l Conf. Artificial Intelligence and Statistics (Proceedings of Machine Learning Research),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In International Conference on Learning Representations", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adversarial Machine Learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "In 4th ACM Workshop on Artificial Intelligence and Security (AISec", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Machine Learning Methods for Computer Security (Dagstuhl Perspectives Workshop 12371)", "author": ["Anthony D. Joseph", "Pavel Laskov", "Fabio Roli", "J. Doug Tygar", "Blaine Nelson"], "venue": "Dagstuhl Manifestos", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Learning in the presence of malicious errors", "author": ["Michael Kearns", "Ming Li"], "venue": "SIAM J. Comput. 22,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Security Analysis of Online Centroid Anomaly Detection", "author": ["Marius Kloft", "Pavel Laskov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Understanding Black-box Predictions via Influence Functions", "author": ["P.W. Koh", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Gradient- Based Learning Applied to Document Recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Gradient-based Hyperparameter Optimization Through Reversible Learning", "author": ["Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams"], "venue": "In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "In 29th AAAI Conf. Artificial Intelligence", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Universal adversarial perturbations", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "In CVPR", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Exploiting Machine Learning to Subvert your", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C.A. Sutton", "J.D. Tygar", "K. Xia"], "venue": "Spam Filter. LEET", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["Blaine Nelson", "Marco Barreno", "Fuching Jack Chi", "Anthony D. Joseph", "Benjamin I.P. Rubinstein", "Udam Saini", "Charles Sutton", "J.D. Tygar", "Kai Xia"], "venue": "In LEET\u201908: Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Practical Black-Box Attacks Against Machine Learning", "author": ["Nicolas Papernot", "PatrickMcDaniel", "Ian Goodfellow", "Somesh Jha", "Z. Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security (ASIA CCS \u201917)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "The Limitations of Deep Learning in Adversarial Settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z. Berkay Celik", "Ananthram Swami"], "venue": "In Proc. 1st IEEE European Symposium on Security and Privacy", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Optimal teaching for limitedcapacity human learners", "author": ["K.R. Patil", "X. Zhu", "L. Kope\u0107", "B.C. Love"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Fast Exact Multiplication by the Hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation 6,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1994}, {"title": "Hyperparameter optimization with approximate gradient", "author": ["F. Pedregosa"], "venue": "In 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research), Maria Florina Balcan and Kilian Q. Weinberger (Eds.),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "ANTIDOTE: understanding and defending against poisoning of anomaly detectors", "author": ["Benjamin I.P. Rubinstein", "Blaine Nelson", "Ling Huang", "Anthony D. Joseph", "Shinghon Lau", "Satish Rao", "Nina Taft", "J.D. Tygar"], "venue": "In Proceedings of the 9th ACM SIGCOMM Internet Measurement Conference (IMC \u201909)", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Automated Dynamic Analysis of Ransomware: Benefits, Limitations and use for Detection", "author": ["D. Sgandurra", "L. Mu\u00f1oz-Gonz\u00e1lez", "R. Mohsen", "E.C. Lupu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Malicious PDF Detection Using Metadata and Structural Features", "author": ["Charles Smutz", "Angelos Stavrou"], "venue": "In Proceedings of the 28th Annual Computer Security Applications Conference (ACSAC \u201912)", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Certified Defenses for Data Poisoning Attacks", "author": ["J. Steinhardt", "P.W. Koh", "P. Liang"], "venue": "arXiv preprint arXiv:1706.03691", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In International Conference on Learning Representations. http://arxiv", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Practical Evasion of a Learning-Based Classifier: A Case Study", "author": ["Nedim \u0160rndic", "Pavel Laskov"], "venue": "In Proc. 2014 IEEE Symp. Security and Privacy (SP \u201914). IEEE CS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Man vs.Machine: Practical Adversarial Detection of Malicious Crowdsourcing Workers", "author": ["GangWang", "TianyiWang", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In 23rd USENIX Security Symposium (USENIX Security 14)", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Is Feature Selection Secure against Training Data Poisoning", "author": ["Huang Xiao", "Battista Biggio", "Gavin Brown", "Giorgio Fumera", "Claudia Eckert", "Fabio Roli"], "venue": "In JMLR W&CP - Proc. 32nd Int\u2019l Conf. Mach. Learning (ICML), Francis Bach and David Blei (Eds.),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Machine Teaching for Bayesian Learners in the Exponential Family", "author": ["X. Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 14, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 15, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 17, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 21, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 23, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 30, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 32, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 35, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 36, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 37, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 6, "context": "This recent research field aims at understanding the security properties of current learning algorithms, as well as at developing more secure ones [7, 16, 17].", "startOffset": 147, "endOffset": 158}, {"referenceID": 14, "context": "This recent research field aims at understanding the security properties of current learning algorithms, as well as at developing more secure ones [7, 16, 17].", "startOffset": 147, "endOffset": 158}, {"referenceID": 15, "context": "This recent research field aims at understanding the security properties of current learning algorithms, as well as at developing more secure ones [7, 16, 17].", "startOffset": 147, "endOffset": 158}, {"referenceID": 15, "context": ", technologies relying upon the collection of large amounts of data in the wild [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 17, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 21, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 23, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 30, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 37, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 32, "context": "PDFRate2 is an online malware detection tool that analyzes the submitted PDF files to reveal the presence of embedded malware [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 17, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 18, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 21, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 23, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 30, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 37, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 18, "context": "The main technical difficulty in devising a poisoning attack is the computation of the poisoning samples, also recently referred to as adversarial training examples [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "This requires solving a bilevel optimization problem in which the outer optimization amounts to maximizing the classification error on an untainted validation set, while the inner optimization corresponds to training the learning algorithm on the poisoned data [23].", "startOffset": 261, "endOffset": 265}, {"referenceID": 7, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 18, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 21, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 37, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 0, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 1, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 6, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 14, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 2, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 12, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 20, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 29, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 5, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 22, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 25, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 35, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 0, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 82, "endOffset": 92}, {"referenceID": 1, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 82, "endOffset": 92}, {"referenceID": 14, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 82, "endOffset": 92}, {"referenceID": 6, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 6, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 7, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 14, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 21, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 37, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 25, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 246, "endOffset": 258}, {"referenceID": 26, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 246, "endOffset": 258}, {"referenceID": 34, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 246, "endOffset": 258}, {"referenceID": 7, "context": "In this work, as in [8, 23, 39], we however consider only the optimization of the model parameters, and not of its hyperparameters.", "startOffset": 20, "endOffset": 31}, {"referenceID": 21, "context": "In this work, as in [8, 23, 39], we however consider only the optimization of the model parameters, and not of its hyperparameters.", "startOffset": 20, "endOffset": 31}, {"referenceID": 37, "context": "In this work, as in [8, 23, 39], we however consider only the optimization of the model parameters, and not of its hyperparameters.", "startOffset": 20, "endOffset": 31}, {"referenceID": 26, "context": "6In [28], the authors defined targeted and indiscriminate attacks (at test time) depending on whether the attacker aims to cause specific or generic errors.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 1, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 3, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 6, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 8, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 9, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 14, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 37, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 5, "context": "Experiments on the transferability of attacks among learning algorithms, firstly demonstrated in [6] and then in subsequent work on deep learners [27], fall under this category of attacks.", "startOffset": 97, "endOffset": 100}, {"referenceID": 25, "context": "Experiments on the transferability of attacks among learning algorithms, firstly demonstrated in [6] and then in subsequent work on deep learners [27], fall under this category of attacks.", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 5, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 7, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 14, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 21, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 37, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 7, "context": "The most common scenario considered in previous work [8, 23, 39] considers poisoning twoclass learning algorithms to cause a denial of service.", "startOffset": 53, "endOffset": 64}, {"referenceID": 21, "context": "The most common scenario considered in previous work [8, 23, 39] considers poisoning twoclass learning algorithms to cause a denial of service.", "startOffset": 53, "endOffset": 64}, {"referenceID": 37, "context": "The most common scenario considered in previous work [8, 23, 39] considers poisoning twoclass learning algorithms to cause a denial of service.", "startOffset": 53, "endOffset": 64}, {"referenceID": 7, "context": "As in [8, 23, 39], this poisoning attack (as any other poisoning attack) requires solving a bilevel optimization, where the inner problem is the learning problem.", "startOffset": 6, "endOffset": 17}, {"referenceID": 21, "context": "As in [8, 23, 39], this poisoning attack (as any other poisoning attack) requires solving a bilevel optimization, where the inner problem is the learning problem.", "startOffset": 6, "endOffset": 17}, {"referenceID": 37, "context": "As in [8, 23, 39], this poisoning attack (as any other poisoning attack) requires solving a bilevel optimization, where the inner problem is the learning problem.", "startOffset": 6, "endOffset": 17}, {"referenceID": 7, "context": "Worth remarking, this formulation encompasses all previously-proposed poisoning attacks against binary learners [8, 23, 39], provided that the loss function L is selected accordingly (e.", "startOffset": 112, "endOffset": 123}, {"referenceID": 21, "context": "Worth remarking, this formulation encompasses all previously-proposed poisoning attacks against binary learners [8, 23, 39], provided that the loss function L is selected accordingly (e.", "startOffset": 112, "endOffset": 123}, {"referenceID": 37, "context": "Worth remarking, this formulation encompasses all previously-proposed poisoning attacks against binary learners [8, 23, 39], provided that the loss function L is selected accordingly (e.", "startOffset": 112, "endOffset": 123}, {"referenceID": 7, "context": ", using the hinge loss against SVMs [8]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 18, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 21, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 37, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 7, "context": ", in kernelized SVMs, when the poisoning points are support vectors [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "To overcome this limitation, we exploit a recent technique called back-gradient optimization [14, 22], which allows computing the gradient of interest in a more computationally-efficient and stabler manner.", "startOffset": 93, "endOffset": 101}, {"referenceID": 20, "context": "To overcome this limitation, we exploit a recent technique called back-gradient optimization [14, 22], which allows computing the gradient of interest in a more computationally-efficient and stabler manner.", "startOffset": 93, "endOffset": 101}, {"referenceID": 7, "context": "Before delving into the technical details, we make the same assumptions made in previous work [8, 23, 39] to reduce the complexity of Problem (2)-(3): (i) we consider the optimization of one poisoning point at a time, denoted hereafter with xc ; and (ii) we assume that its label yc is initially chosen by the attacker, and kept fixed during the optimization.", "startOffset": 94, "endOffset": 105}, {"referenceID": 21, "context": "Before delving into the technical details, we make the same assumptions made in previous work [8, 23, 39] to reduce the complexity of Problem (2)-(3): (i) we consider the optimization of one poisoning point at a time, denoted hereafter with xc ; and (ii) we assume that its label yc is initially chosen by the attacker, and kept fixed during the optimization.", "startOffset": 94, "endOffset": 105}, {"referenceID": 37, "context": "Before delving into the technical details, we make the same assumptions made in previous work [8, 23, 39] to reduce the complexity of Problem (2)-(3): (i) we consider the optimization of one poisoning point at a time, denoted hereafter with xc ; and (ii) we assume that its label yc is initially chosen by the attacker, and kept fixed during the optimization.", "startOffset": 94, "endOffset": 105}, {"referenceID": 7, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 18, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 21, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 37, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 29, "context": "For example, this holds if the learning problem L is convex, which implies that all stationary points are global minima [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 18, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 21, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 37, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 37, "context": "As in [39], the idea is to perform several passes over the set of poisoning samples, using Algorithm 1 to optimize each poisoning point at a time, while keeping the other points fixed.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 18, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 21, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 37, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 11, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 12, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 18, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 20, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 28, "context": "The computation of the matrices \u2207x c\u2207wL and \u22072 wL can also be avoided using Hessianvector products [30]:", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "previous work [8, 23, 39], it still requires the inner learning problem to be solved exactly.", "startOffset": 14, "endOffset": 25}, {"referenceID": 21, "context": "previous work [8, 23, 39], it still requires the inner learning problem to be solved exactly.", "startOffset": 14, "endOffset": 25}, {"referenceID": 37, "context": "previous work [8, 23, 39], it still requires the inner learning problem to be solved exactly.", "startOffset": 14, "endOffset": 25}, {"referenceID": 12, "context": "However, as these problems are always solved to a finite accuracy, it may happen that the gradient \u2207x cA is not sufficiently precise, especially if convergence thresholds are too loose [14, 22].", "startOffset": 185, "endOffset": 193}, {"referenceID": 20, "context": "However, as these problems are always solved to a finite accuracy, it may happen that the gradient \u2207x cA is not sufficiently precise, especially if convergence thresholds are too loose [14, 22].", "startOffset": 185, "endOffset": 193}, {"referenceID": 12, "context": "In this work, we overcome this limitation by exploiting back-gradient optimization [14, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "In this work, we overcome this limitation by exploiting back-gradient optimization [14, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 12, "context": "According to [14], this technique allows to compute the desired gradients in the outer problem using the parameterswT obtained from an incomplete optimization of the inner problem (afterT iterations).", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "These are indeed the elements required to compute the gradient of the outer objective with a backward pass (we refer the reader to [22] for more details).", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": ",wT and the required forward derivatives, Domke [14] and Maclaurin et al.", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "[22] proposed to compute them directly during the backward pass, by reversing the steps followed by the learning algorithm to update them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Note finally that, as in [14, 22], the time complexity of our back-gradient descent is O(T ).", "startOffset": 25, "endOffset": 33}, {"referenceID": 20, "context": "Note finally that, as in [14, 22], the time complexity of our back-gradient descent is O(T ).", "startOffset": 25, "endOffset": 33}, {"referenceID": 18, "context": ", not just using a surrogate model trained on the deep features, as in [20]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "The Ransomware data [33] consists of 530 ransomware samples and 549 benign applications.", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "To the best of our knowledge, this has been demonstrated in [6, 27] for evasion attacks (i.", "startOffset": 60, "endOffset": 67}, {"referenceID": 25, "context": "To the best of our knowledge, this has been demonstrated in [6, 27] for evasion attacks (i.", "startOffset": 60, "endOffset": 67}, {"referenceID": 7, "context": "9Note indeed that the validation error only provides a biased estimate of the true classification error, as it is used by the attacker to optimize the poisoning points [8].", "startOffset": 168, "endOffset": 171}, {"referenceID": 19, "context": "2 Handwritten Digit Recognition We consider here the problem of handwritten digit recognition, which involves 10 classes (each corresponding to a digit, from 0 to 9), using theMNIST data [21].", "startOffset": 187, "endOffset": 191}, {"referenceID": 18, "context": ", accounting for all weight updates in each layer (instead of using a surrogate model trained on a frozen deep feature representation [20]).", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "To this end, we consider the convolutional neural network (CNN) proposed in [21] for classification of the MNIST digit data, which requires optimizing more than 450, 000 parameters.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "We use Algorithm 1 to craft each single poisoning point, but, similarly to [39], we optimize them iteratively, making 2 passes over the whole set of poisoning samples.", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "We also use the line search exploited in [39], instead of a fixed gradient step size, to reduce the attack complexity (i.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "Notably, similarly to adversarial test examples, also poisoning samples against deep networks are visually indistinguishable from the initial image (as in [20]), while this is not the case when targeting the LR classifier.", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "This might be due to the specific shape of the decision function learned by the deep network in the input space, as explained in the case of adversarial test examples [15, 36].", "startOffset": 167, "endOffset": 175}, {"referenceID": 34, "context": "This might be due to the specific shape of the decision function learned by the deep network in the input space, as explained in the case of adversarial test examples [15, 36].", "startOffset": 167, "endOffset": 175}, {"referenceID": 10, "context": "5 RELATEDWORK Seminal work on the analysis of supervised learning in the presence of omniscient attackers that can compromise the training data has been presented in [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 16, "context": "5 RELATEDWORK Seminal work on the analysis of supervised learning in the presence of omniscient attackers that can compromise the training data has been presented in [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 17, "context": "The first practical poisoning attacks against two-class classification algorithms have been proposed in [19, 26], in the context of spam filtering and anomaly detection.", "startOffset": 104, "endOffset": 112}, {"referenceID": 24, "context": "The first practical poisoning attacks against two-class classification algorithms have been proposed in [19, 26], in the context of spam filtering and anomaly detection.", "startOffset": 104, "endOffset": 112}, {"referenceID": 7, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 18, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 21, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 37, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 7, "context": "[8] have been the first to demonstrate the vulnerability of SVMs to poisoning attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "[39] have shown how to poison LASSO, ridge regression, and the elastic net.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Finally, Mei and Zhu [23] has systematized such attacks under a unified framework to poison convex learning algorithms with Tikhonov regularizers, 0 10 20 0", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "based on the concept of machine teaching [29, 40].", "startOffset": 41, "endOffset": 49}, {"referenceID": 38, "context": "based on the concept of machine teaching [29, 40].", "startOffset": 41, "endOffset": 49}, {"referenceID": 18, "context": "Note also that, despite recent work [20] has provided a first proof of concept of the existence of adversarial training examples against deep networks, this has been shown on a binary classification task using a surrogate model (attacked with standard KKT-based poisoning).", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": ", adversarial test examples) in [6, 37] against SVMs and NNs, and subsequently in [27] against deep networks.", "startOffset": 32, "endOffset": 39}, {"referenceID": 35, "context": ", adversarial test examples) in [6, 37] against SVMs and NNs, and subsequently in [27] against deep networks.", "startOffset": 32, "endOffset": 39}, {"referenceID": 25, "context": ", adversarial test examples) in [6, 37] against SVMs and NNs, and subsequently in [27] against deep networks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "At the same time, the variability and sophistication of cyberattacks have tremendously increased, making machine learning systems an appealing target for cybercriminals [2, 16].", "startOffset": 169, "endOffset": 176}, {"referenceID": 14, "context": "At the same time, the variability and sophistication of cyberattacks have tremendously increased, making machine learning systems an appealing target for cybercriminals [2, 16].", "startOffset": 169, "endOffset": 176}, {"referenceID": 7, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 18, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 21, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 37, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 0, "context": "To this end, we have extended the commonly-used threat model proposed in [1, 2, 16] by introducing the concept of error specificity, to denote whether the attacker aims to cause specific misclassification errors (i.", "startOffset": 73, "endOffset": 83}, {"referenceID": 1, "context": "To this end, we have extended the commonly-used threat model proposed in [1, 2, 16] by introducing the concept of error specificity, to denote whether the attacker aims to cause specific misclassification errors (i.", "startOffset": 73, "endOffset": 83}, {"referenceID": 14, "context": "To this end, we have extended the commonly-used threat model proposed in [1, 2, 16] by introducing the concept of error specificity, to denote whether the attacker aims to cause specific misclassification errors (i.", "startOffset": 73, "endOffset": 83}, {"referenceID": 7, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 18, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 21, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 37, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 12, "context": "In this work, we have overcome these limitations by proposing a novel poisoning algorithm based on back-gradient optimization [14, 22, 31].", "startOffset": 126, "endOffset": 138}, {"referenceID": 20, "context": "In this work, we have overcome these limitations by proposing a novel poisoning algorithm based on back-gradient optimization [14, 22, 31].", "startOffset": 126, "endOffset": 138}, {"referenceID": 29, "context": "In this work, we have overcome these limitations by proposing a novel poisoning algorithm based on back-gradient optimization [14, 22, 31].", "startOffset": 126, "endOffset": 138}, {"referenceID": 5, "context": "adversarial test examples) [6, 27, 37].", "startOffset": 27, "endOffset": 38}, {"referenceID": 25, "context": "adversarial test examples) [6, 27, 37].", "startOffset": 27, "endOffset": 38}, {"referenceID": 35, "context": "adversarial test examples) [6, 27, 37].", "startOffset": 27, "endOffset": 38}, {"referenceID": 13, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 384, "endOffset": 392}, {"referenceID": 22, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 384, "endOffset": 392}, {"referenceID": 4, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 539, "endOffset": 550}, {"referenceID": 30, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 539, "endOffset": 550}, {"referenceID": 33, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 539, "endOffset": 550}], "year": 2017, "abstractText": "A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we first extend the definition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic differentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradientbased procedures, including neural networks and deep learning architectures. We empirically evaluate its effectiveness on several application examples, including spam filtering, malware detection, and handwritten digit recognition. We finally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across different learning algorithms. CCS CONCEPTS \u2022 Computing Methodologies\u2192Machine Learning;", "creator": "LaTeX with hyperref package"}}}