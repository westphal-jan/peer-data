{"id": "1708.08989", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Deep Residual Bidir-LSTM for Human Activity Recognition Using Wearable Sensors", "abstract": "Human activity recognition (HAR) has become a popular topic in research because of its wide application. With the development of deep learning, new ideas have appeared to address HAR problems. Here, a deep network architecture using residual bidirectional long short-term memory (LSTM) cells is proposed. The advantages of the new network include that a bidirectional connection can concatenate the positive time direction (forward state) and the negative time direction (backward state). Second, residual connections between stacked cells act as highways for gradients, which can pass underlying information directly to the upper layer, effectively avoiding the gradient vanishing problem. Generally, the proposed network shows improvements on both the temporal (using bidirectional cells) and the spatial (residual connections stacked deeply) dimensions, aiming to enhance the recognition rate. When tested with the Opportunity data set and the public domain UCI data set, the accuracy was increased by 4.78% and 3.68%, respectively, compared with previously reported results. Finally, the confusion matrix of the public domain UCI data set was analyzed.", "histories": [["v1", "Tue, 22 Aug 2017 11:02:13 GMT  (1320kb)", "http://arxiv.org/abs/1708.08989v1", null], ["v2", "Thu, 7 Sep 2017 07:36:31 GMT  (1265kb)", "http://arxiv.org/abs/1708.08989v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yu zhao", "rennong yang", "guillaume chevalier", "maoguo gong"], "accepted": false, "id": "1708.08989"}, "pdf": {"name": "1708.08989.pdf", "metadata": {"source": "CRF", "title": "Deep Residual Bidir-LSTM for Human Activity Recognition Using Wearable Sensors", "authors": ["Yu Zhao", "Rennong Yang", "Guillaume Chevalier", "Maoguo Gong"], "emails": [], "sections": [{"heading": null, "text": "Human Activity Detection (HAR) has become a popular topic in research due to its broad application, and with the development of deep learning, new ideas seem to solve HAR problems. Here, a deep network architecture using bidirectional long-term memory cells (LSTM) is proposed. Advantages of the new network include that a bidirectional link can link the positive direction (forward) and the negative direction (backward) of time. Second, residual links between stacked cells act as highways for gradients that can pass information directly to the upper layer, effectively avoiding the problem of gradient disappearance. In general, the proposed network shows improvements in both temporal (with bidirectional cells) and spatial (residual connections that are deeply stacked) dimensions aimed at increasing the detection rate."}, {"heading": "1. Introduction", "text": "In fact, the number of people who are able to move is very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very far, very far, very far, very far, very far, very far, very far, very far, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high"}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Baseline LSTM", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "2.2. Bidirectional LSTM", "text": "The improvement of bidirectional LSTM cells is that the current output is based not only on previous information, but also on subsequent information. For example, it is appropriate to predict a missing word based on the context. Bidirectional LSTM [32] consists of two LSTM cells, and the output is determined by the two together. In Figure 3, there are forward and backward sequences in the hidden layer. (0,1,2...) t t, the hidden layer and the input layer can be defined as follows: () () () U W U W V t t t t t -1h h h t t -1h h h h t t t t t t t yh h h h h g + h g + h + b g + b (3)."}, {"heading": "2.3. Residual Network", "text": "The MSRA team built a 152-layer network that was about eight times as deep as the VGG network [21]. Due to its excellent performance, it took first place in the 2015 ILSVRC competition due to its absolute advantage in image classification, image location and image recognition. As the network deepens, the focus of research shifts to overcoming the obstacles to information and transmission of gradients. MSRA uses residential networks. The basic idea is that it is easier to optimize residual mapping than to optimize the original, non-referenced mapping. An important advantage of residential networks is that they are much easier because the gradients can be guided through the layers more directly with the additional operator, which makes it possible to bypass some layers that would otherwise have been restrictive."}, {"heading": "3. Our Model: Deep Residual Bidir-LSTM Network", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4. Experiments", "text": "It is indeed the case that we will be able to embark on a search for new paths to follow in order to achieve our objectives."}, {"heading": "5. Conclusions", "text": "This paper analyzes the importance of HAR research and provides an overview of emerging methods in the field. LSTM neural networks have been used in many innovations in natural language processing, speech recognition and weather forecasting, and this technology has been adapted to the HAR task. We proposed the novel framework of the Deep-Res-Bidir-LSTM network, which can improve learning ability for faster learning in early education, and the proposed network also guarantees the validity of information transmission through residual connections (in the depth dimension) and bi-directional cells (in the time dimension). In our experiments, the proposed network has been able to improve accuracy by defining and increasing the UCI data by determining the non-binding data sets compared to previous work."}, {"heading": "Acknowledgments", "text": "Our github repository is located at https: / / github.com / guillaume-chevalier / HAR-stacked-residual-bidir-LSTMs.The code is open source and we have tried to make it easily adaptable to new problems using a new configuration file. We also thank all those who have contributed to building and maintaining the open data sets on the UCI Machine Learning Repository; more links [33] [34] are cited in the references."}, {"heading": "34(15) (2013) 2033\u20132042.", "text": "[8] Anguita D, Ghio A, Oneto L, et al. Energy efficient smartphone-based activityrecognition on fixed-point arithmetic, Journal of Universal Computerence. 19 (9) (2013) 1295-1314. [9] Yang J B, Nguyen D. N, San P, et al. [Deep convolutional neural networks onmultichannel time series for human activity recognition, in: International Conference onArtificial Intelligence. AAAI Press, 2015. [10] Ord\u00f3\u00f1ez F J, Roggen D. Deep Convolutional and LSTM Recurrent Neural Networks forMultimodal Wearable Activity Recognition, Sensors. 16 (1) (2016). Wang N, Yeung D Y. Learning a deep compact image representation for visual tracking, in: Advances in Neural Information Processing Systems, 2013."}, {"heading": "10(3) (2014) 1219\u20131229.", "text": "[27] Levy E, David O E, Netanyahu N S. Netanyahu. Genetic Algorithms and Deep Learning for the Automatic Classification of Painters, in: Proceedings of the Annual Conference onGenetic and Evolutionary Computation, 2014. [28] Fornarelli G, Giaquinto A. Adaptive particle swarm optimization for CNN associativemories design, Neurocomputing. 72 (16) (2009) 3851-3862. [29] Syulistyo A R, Purnomo D M J, Rachmadi M F, et al. PARTICLE SWARMOPTIMIZATION (PSO) FOR TRAINING OPTIMIZATION ON CONVOLUTIONALNEURAL NETWORK (CNN), Jurnal Ilmu Komputer dan Informasi. 9 (1) (2016): 52-58. [30] Wiesler S, Richard A, Schluter R, et al."}], "references": [{"title": "Dynamic computation offloading for low-power wearable health monitoring systems", "author": ["H Kalantarian", "C Sideris", "B Mortazavi"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "A survey on wearable sensor-based systems for health monitoring and prognosis", "author": ["A Pantelopoulos", "G. Bourbakis N"], "venue": "IEEE Transactions on Systems Man & Cybernetics Part C Applications & Reviews", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Named data networking-based smart home, ICT Express", "author": ["H Ahmed S", "D. Kim"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Ubiquitous smart home system using android application", "author": ["Kumar", "Shiu"], "venue": "arXiv preprint,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Vision based hand gesture recognition for human computer interaction: a survey, Artificial Intelligence Review", "author": ["S Rautaray", "A. Agrawal"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Hand tracking and gesture recognition system for human\u2013computer interaction using low-cost hardware, Multimedia Tools and Applications", "author": ["S Yeo H", "G Lee B", "H. Lim"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "The Opportunity challenge: A benchmark database for on-body sensor-based activity recognition, Pattern Recognition Letters", "author": ["R Chavarriaga", "H Sagha", "A Calatroni"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Energy efficient smartphone-based activity recognition using fixed-point arithmetic, Journal of Universal Computerence", "author": ["D Anguita", "A Ghio", "L Oneto"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Deep convolutional neural networks on multichannel time series for human activity", "author": ["B Yang J", "N Nguyen M", "P San P"], "venue": "recognition, in: International Conference on Artificial Intelligence. AAAI Press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition, Sensors", "author": ["J Ord\u00f3\u00f1ez F", "D. Roggen"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning a deep compact image representation for visual tracking", "author": ["N Wang", "Y. Yeung D"], "venue": "in: Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C Dong", "C Loy", "K He"], "venue": "in: European Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A Graves", "R Mohamed A", "G. Hinton"], "venue": "in: IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni"], "venue": "arXiv preprint,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Convolutional Neural Networks for Speech Recognition", "author": ["O Abdel-Hamid", "R Mohamed A", "H Jiang"], "venue": "IEEE/ACM Transactions on Audio Speech & Language Processing", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit"], "venue": "in: CoRR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun"], "venue": "arXiv preprint arXiv:", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "E. Hinton G"], "venue": "in: International Conference on Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y Bengio", "P Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Understanding the exploding gradient problem", "author": ["Pascanu", "Razvan", "Tomas Mikolov", "Yoshua Bengio"], "venue": "in: CoRR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Christian Szegedy"], "venue": "arXiv preprint,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "A Public Domain Dataset for Human Activity Recognition using Smartphones", "author": ["Anguita", "Davide"], "venue": "in: ESANN,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Spatial Coherence for Visual Motion Analysis", "author": ["J. Maclean W"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "An improved grid search algorithm and its application in PCA and SVM based face recognition", "author": ["Y Yao", "L Zhang", "Y Liu"], "venue": "Journal of Computational Information Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Genetic algorithms and deep learning for automatic painter classification", "author": ["E Levy", "E David O", "Netanyahu. Netanyahu N S"], "venue": "in: proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Adaptive particle swarm optimization for CNN associative memories design, Neurocomputing", "author": ["G Fornarelli", "A. Giaquinto"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "PARTICLE SWARM OPTIMIZATION (PSO) FOR TRAINING OPTIMIZATION ON CONVOLUTIONAL  NEURAL NETWORK (CNN)", "author": ["R Syulistyo A", "J Purnomo D M", "F Rachmadi M"], "venue": "Jurnal Ilmu Komputer dan Informasi", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Mean-normalized stochastic gradient for large-scale deep learning", "author": ["S Wiesler", "A Richard", "R Schluter"], "venue": "in: IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Jimmy B"], "venue": "arXiv preprint,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Robust discriminative keyword spotting for emotionally colored spontaneous speech using bidirectional LSTM networks", "author": ["M Wollmer", "F Eyben", "J Keshet"], "venue": "in: IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Human activity recognition using smartphone data set, https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartpho nes", "author": ["A David", "L. Oneto"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Opportunity activity recognition", "author": ["R Daniel", "M. Rossi"], "venue": "dataset, https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "It can be used widely, including in health monitoring [1][2], smart homes [3][4], and human\u2013computer interactions [5][6]; for example, LSTM cells are a good choice for solving HAR problems.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "It can be used widely, including in health monitoring [1][2], smart homes [3][4], and human\u2013computer interactions [5][6]; for example, LSTM cells are a good choice for solving HAR problems.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "It can be used widely, including in health monitoring [1][2], smart homes [3][4], and human\u2013computer interactions [5][6]; for example, LSTM cells are a good choice for solving HAR problems.", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "It can be used widely, including in health monitoring [1][2], smart homes [3][4], and human\u2013computer interactions [5][6]; for example, LSTM cells are a good choice for solving HAR problems.", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "It can be used widely, including in health monitoring [1][2], smart homes [3][4], and human\u2013computer interactions [5][6]; for example, LSTM cells are a good choice for solving HAR problems.", "startOffset": 114, "endOffset": 117}, {"referenceID": 5, "context": "It can be used widely, including in health monitoring [1][2], smart homes [3][4], and human\u2013computer interactions [5][6]; for example, LSTM cells are a good choice for solving HAR problems.", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "A public domain benchmark of HAR has been introduced, and different methods of recognition have been analyzed [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "A Multi-Class Hardware-Friendly Support Vector Machine (MC-HF-SVM), which uses fixed-point arithmetic for HAR instead of the typical floating-point arithmetic, has been proposed for sensor data [8].", "startOffset": 194, "endOffset": 197}, {"referenceID": 8, "context": "Unlike the manual filtering features in previous algorithms, a systematic feature learning method that combines feature extraction with CNN training has also been proposed [9].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "of 4% of the F1 score [10]; the effects of parameters on the final result were also analyzed.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "In recent years, deep learning has shown applicability to many fields, such as image processing [11][12], speech recognition [13][14][15], and natural language processing [16][17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "In recent years, deep learning has shown applicability to many fields, such as image processing [11][12], speech recognition [13][14][15], and natural language processing [16][17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "In recent years, deep learning has shown applicability to many fields, such as image processing [11][12], speech recognition [13][14][15], and natural language processing [16][17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "In recent years, deep learning has shown applicability to many fields, such as image processing [11][12], speech recognition [13][14][15], and natural language processing [16][17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "In recent years, deep learning has shown applicability to many fields, such as image processing [11][12], speech recognition [13][14][15], and natural language processing [16][17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "In recent years, deep learning has shown applicability to many fields, such as image processing [11][12], speech recognition [13][14][15], and natural language processing [16][17].", "startOffset": 171, "endOffset": 175}, {"referenceID": 16, "context": "In recent years, deep learning has shown applicability to many fields, such as image processing [11][12], speech recognition [13][14][15], and natural language processing [16][17].", "startOffset": 175, "endOffset": 179}, {"referenceID": 17, "context": "In ILSVRC 2012, AlexNet [18], proposed by Alex Krizhevsky, took first place, and, since then, deep learning has been considered to be applicable to solving real problems and has done so with impressive accuracy.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "LSTM cells, which were first proposed by Juergen Schmidhuber in 1997 [19], are variants of recurrent neural networks (RNNs).", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "LSTM [18] is an extension of recurrent neural networks.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "However, Bengio [20] found that RNN could remember the information for only a short time, because of the vanishing and exploding gradient problems.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The output value is between [0, 1] to allow the multiplication to then happen to let information flow or not; thus, it is considered good practice to initialize these gates to a value of 1, or close to 1, so as to not impair training at the beginning.", "startOffset": 28, "endOffset": 34}, {"referenceID": 31, "context": "Bidirectional LSTM [32] is made up of two LSTM cells, and the output is", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "The MSRA team built a 152-layer network, which was about eight times that of the VGG network [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "On top of a collection of residual connections is a bottleneck where the next layers stop being residual and where a batch normalization is generally applied to normalize and restrict the usage of the feature space represented by the layer [22].", "startOffset": 240, "endOffset": 244}, {"referenceID": 30, "context": "The difference between the predicted value and the real value was then compared with a sigmoid cross entropy loss with L2 weight decay to then back-propagate errors backward into the network layer by layer with the Adam Optimizer [31].", "startOffset": 230, "endOffset": 234}, {"referenceID": 20, "context": "Although residual connections for CNN have been used [21], this method is also available for LSTM.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "To avoid a sudden leveling off in the accuracy during learning, gradient clipping [22] is added with a maximal gradient step norm of 15.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "Batch normalization [23] can also be useful in training residual connections.", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "We tested the Deep-Res-Bidir-LSTM network with the public domain UCI data set [24] and the Opportunity data set [7].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "We tested the Deep-Res-Bidir-LSTM network with the public domain UCI data set [24] and the Opportunity data set [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 23, "context": "There are several open data sets that can be benchmarked, such as the public domain UCI [24], the Opportunity [7], and the KTH data sets [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "There are several open data sets that can be benchmarked, such as the public domain UCI [24], the Opportunity [7], and the KTH data sets [25].", "startOffset": 110, "endOffset": 113}, {"referenceID": 24, "context": "There are several open data sets that can be benchmarked, such as the public domain UCI [24], the Opportunity [7], and the KTH data sets [25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "Additionally, all features were pre-normalized and bounded within [-1, 1].", "startOffset": 66, "endOffset": 73}, {"referenceID": 9, "context": "The NULL class rendered the data set highly unbalanced; thus, following previous research [10], we used a weighted F1 score [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "The NULL class rendered the data set highly unbalanced; thus, following previous research [10], we used a weighted F1 score [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "Many of those 242 features are not useful for HAR; thus, we used only 113 features, such as DeepConvLSTM [10].", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "Such a small standard deviation is often useful in deep learning [30].", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Generally used methods of tuning parameters include experimental methods, grid searches [26], genetic algorithm (GA) [27], and particle swarm optimization (PSO) [28][29].", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "Generally used methods of tuning parameters include experimental methods, grid searches [26], genetic algorithm (GA) [27], and particle swarm optimization (PSO) [28][29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "Generally used methods of tuning parameters include experimental methods, grid searches [26], genetic algorithm (GA) [27], and particle swarm optimization (PSO) [28][29].", "startOffset": 161, "endOffset": 165}, {"referenceID": 28, "context": "Generally used methods of tuning parameters include experimental methods, grid searches [26], genetic algorithm (GA) [27], and particle swarm optimization (PSO) [28][29].", "startOffset": 165, "endOffset": 169}, {"referenceID": 7, "context": "Algorithm Accuracy F1 score MC-SVM [8] 89.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "54% MultiClass Hardware Friendly SVM, or MC-HF-SVM, was proposed by Davide Anguita [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 148, "endOffset": 151}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 173, "endOffset": 176}, {"referenceID": 6, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 187, "endOffset": 190}, {"referenceID": 8, "context": "F1 score with the NULL class of each algorithm with the Opportunity data set Algorithm F1 score LDA [7] 69% QDA [7] 53% NCC [7] 51% 1NN [7] 87% 3NN [7] 85% UP [7] 64% NStar [7] 84% SStar [7] 86% DBN [9] 73.", "startOffset": 199, "endOffset": 202}, {"referenceID": 8, "context": "0% CNN [9] 85.", "startOffset": 7, "endOffset": 10}, {"referenceID": 32, "context": "We also thank everyone who contributed to building and maintaining the openly available data sets on the UCI Machine Learning Repository; other links [33][34] are cited in the References.", "startOffset": 150, "endOffset": 154}, {"referenceID": 33, "context": "We also thank everyone who contributed to building and maintaining the openly available data sets on the UCI Machine Learning Repository; other links [33][34] are cited in the References.", "startOffset": 154, "endOffset": 158}], "year": 2017, "abstractText": "Human activity recognition (HAR) has become a popular topic in research because of its wide application. With the development of deep learning, new ideas have appeared to address HAR problems. Here, a deep network architecture using residual bidirectional long short-term memory (LSTM) cells is proposed. The advantages of the new network include that a bidirectional connection can concatenate the positive time direction (forward state) and the negative time direction (backward state). Second, residual connections between stacked cells act as highways for gradients, which can pass underlying information directly to the upper layer, effectively avoiding the gradient vanishing problem. Generally, the proposed network shows improvements on both the temporal (using bidirectional cells) and the spatial (residual connections stacked deeply) dimensions, aiming to enhance the recognition rate. When tested with the Opportunity data set and the public domain UCI data set, the accuracy was increased by 4.78% and 3.68%, respectively, compared with previously reported results. Finally, the confusion matrix of the public domain UCI data set was analyzed.", "creator": "Microsoft\u00ae Word 2010"}}}