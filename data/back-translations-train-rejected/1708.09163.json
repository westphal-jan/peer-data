{"id": "1708.09163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "An Empirical Study of Discriminative Sequence Labeling Models for Vietnamese Text Processing", "abstract": "This paper presents an empirical study of two widely-used sequence prediction models, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks (LSTMs), on two fundamental tasks for Vietnamese text processing, including part-of-speech tagging and named entity recognition. We show that a strong lower bound for labeling accuracy can be obtained by relying only on simple word-based features with minimal hand-crafted feature engineering, of 90.65\\% and 86.03\\% performance scores on the standard test sets for the two tasks respectively. In particular, we demonstrate empirically the surprising efficiency of word embeddings in both of the two tasks, with both of the two models. We point out that the state-of-the-art LSTMs model does not always outperform significantly the traditional CRFs model, especially on moderate-sized data sets. Finally, we give some suggestions and discussions for efficient use of sequence labeling models in practical applications.", "histories": [["v1", "Wed, 30 Aug 2017 08:32:32 GMT  (66kb)", "http://arxiv.org/abs/1708.09163v1", "To appear in the Proceedings of the 9th International Conference on Knowledge and Systems Engineering (KSE) 2017"]], "COMMENTS": "To appear in the Proceedings of the 9th International Conference on Knowledge and Systems Engineering (KSE) 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["phuong le-hong", "minh pham quang nhat", "thai-hoang pham", "tuan-anh tran", "dang-minh nguyen"], "accepted": false, "id": "1708.09163"}, "pdf": {"name": "1708.09163.pdf", "metadata": {"source": "CRF", "title": "An Empirical Study of Discriminative Sequence Labeling Models for Vietnamese Text Processing", "authors": ["Phuong Le-Hong", "Minh Pham Quang Nhat", "Thai-Hoang Pham", "Tuan-Anh Tran", "Dang-Minh Nguyen"], "emails": [], "sections": [{"heading": null, "text": "In recent years it has been shown that the number of people who are able to do their work, is able to do their work, and that they are able to do their work, and that they are able to do their work, by doing their work. (...) In the last ten years the number of people who are able to do their work has multiplied three-fold. (...) In the last ten years the number of people who are able to do their work has multiplied three-fold. (...) In the last ten years the number of workers who are able to do their work has multiplied. (...) In the last ten years the number of people who are able to do their work has multiplied three-fold. (...) In the last ten years the number of workers who are able to do their work has multiplied. (...) In the last ten years the number of workers has tripled. (... The number of workers has tripled. (...) The number of workers has tripled. (...) The number of people has tripled."}, {"heading": "II. METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Fundamental Tasks", "text": "This subsection gives a brief description of two basic sequence learning tasks examined in this study, part of language labeling and named entities.1) Part-of-Speech Tagging: POS tagging is a typical sequence prediction task in which we are interested in building a model that reads text in some languages and assigns a part of the language to each token (word), such as noun, verb, adjective. Generally, in computer applications, POS taggers use finer-grained POS tags such as common nouns or idioms. For example, each word in the following English sentence is marked with its most likely correct part of the language: Profits / N soared / V at / P Boeing / N Co. / N, Easy / ADV Predictions / V Predictions / N on / P Wall / N Street / N, / O labeling, O labeling, O labeling, O labeling, O labeling."}, {"heading": "B. Discriminative Models", "text": "In this subsection, we will briefly describe two discriminatory sequence models used in this study, including conditional random fields (CRFs) and long short-term memory recurrent neural networks (LSTMs).1) Conditional random fields: Conditional random fields (CRF) [4] is a discriminatory probabilistic framework that models directly conditional probabilities of a tag sequence using a word sequence. Formally, in CRF, the conditional probability of a tag sequence y = (y1, y2,., yT), given word sequence x = (x1, x2,., xT) is defined as a sequence."}, {"heading": "III. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "1) Part-of-Speech Tagging: We conduct Vietnamese Part-of-Speech tagging experiments where the training set contains 9,000 records and the test set 1,165 records. The tagset contains 21 different tags. Further details on the corpus are described in [11].2) Named Entity Recognition: For Entity Recognition experiments, we use the standard NER corpus developed by the Vietnamese Language and Speech Process3 community at the end of 2016. Similar to the CoNLL 2003 NER-Corpus for English, four named entity types are considered, including 1Many interesting details on RNNNs are available online at http: / / people.idsia.ch / ~ juergen / rnn.html2https: / / vlsp.hpda.vn /.vn (http: / / www.pople.idsia.ch / juergen / rnn.mhtl2https / http: / / / www.pbs.nv.vps / http: / http: / / www.np.np.nvn)."}, {"heading": "B. Feature Sets", "text": "Each word can be represented by a single hot, sparse vector of size | V |.2) Word Shapes: In addition to word identities, word forms have proven to be important features for improving predictibility, especially for unknown or rare words. Common Word Shapes used in our experiments are shown in Table II. We used regular expressions to extract this word form Features.3) Word Embeddings: Word Embeddings are designated as important features for improving the predictiveness of words, with the word forms being significant. Word Shapes Featuresures.3) Word Embeddings: Word Embeddings are low dimensionally distributed representation of words. Each word embedding is a real vector of d dimensions in which d is much smaller than the word form."}, {"heading": "C. Evaluation Method", "text": "In the POS task, our system is evaluated on the basis of the marking accuracy of the corresponding data sets. Accuracy is the ratio of the number of tokens correctly marked divided by the total number of tokens in the test set. In the NER task, the performance of our system is measured by the F1 score: F1 = 2 * P * R / (P + R). Precision (P) is the percentage of named entities found by the learning system that are correct predictions. Recall (R) is the percentage of named entities contained in the corpus that are found by the system. A named entity is only correct if it is an exact match with the corresponding entity in the data file. The performance of our system is evaluated by the automatic evaluation script of the shared task CoNLL 2003. 5"}, {"heading": "D. Experimental Settings", "text": "In our experiments with the CRF model, we used CRFsuite [17], an implementation of the linear chain (first-order Markov) CRF. This toolkit allows us to easily integrate both binary and numeric features such as word embedding features. We use the default setting of CRFsuite, which uses the training algorithm L-BFGS [18] and L2 regularization, and the coefficient for L2 regularization is 1.0.The recursive neural networks all have a bidirectional recursive layer with a different number of units whose activation function is tanh. As usual, the output layer uses the Softmax activation function. Multi-class entropy loss function 5http: / / www.cnts.ua.ac.be / conll2003 / ner / is selected when the activation function is tanh."}, {"heading": "E. Results", "text": "This year is the highest in the history of the country."}, {"heading": "IV. DISCUSSION", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "V. RELATED WORK", "text": "This section briefly discusses related work on Vietnamese part-of-the-language labeling and mentions entity recognition based on discriminatory sequence models. In [23], the authors give an empirical study on MEMM for Vietnamese part-of-the-language labeling with different characteristics. Their best model has an identification accuracy of about 93.5% when using the entire VLSP tree base. We see that despite using a smaller data set with short sentences and a very simple feature set with minimal handcrafted word forms, we are able to achieve an identification accuracy of more than 90%. This is a strong threshold for this task when only raw text is available. In the VLSP 2016 workshop, several different systems were proposed for Vietnamese NER. The F score of the best participating system is 88.78% [24] in this common task. This system uses many handcrafted features to enhance the performance of the MCR8 models used in most of the IBM models, with the most of the MCR8 models in the IBM models in 2016."}, {"heading": "VI. CONCLUSION", "text": "We have presented an empirical and comparative study of two discriminatory sequence prediction models of CRFs and LSTMs on two basic tasks of Vietnamese word processing. We have demonstrated the great benefit of integrating word embeddings trained by an uncontrolled learning method into both models. These word embeddings are able to capture semantic similarities that help improve the predictive capability of the models, thereby increasing the accuracy of speech mark and named speech recognition by about 4.0% and 5% respectively. The LSTMs model is slightly better than the CRFs model in terms of accuracy, but the gap is not always significant in moderately large datasets, resulting in the cost of much longer training time. We have also shown for the first time that strong accuracy in both speech mark and named speech mark is lower than in the recognition of language units that we can achieve if we rely only on minimal, manual features."}], "references": [{"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257 \u2013286, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Hidden Markov models and the Baum-Welch algorithm", "author": ["L.R. Welch"], "venue": "IEEE Information Theory Society Newsletter, vol. 53, no. 4, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum entropy Markov models for information extraction and segmentation", "author": ["A. McCallum", "D. Freitag", "F. Pereira"], "venue": "Proceedings of ICML, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "ICML, 2001, pp. 282\u2013289.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Generalized iterative scaling for loglinear models", "author": ["J.N. Darroch", "D. Ratcliff"], "venue": "Annals of Mathematical Statistics, vol. 43, no. 5, pp. 1470\u20131480, 1972.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1972}, {"title": "Sequential conditional generalized iterative scaling", "author": ["J. Goodman"], "venue": "Proceedings of ACL, 2002, pp. 9\u201316.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Numerical Optimization, 2nd ed", "author": ["J. Nocedal", "S.J. Wright"], "venue": "New York: Springer,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Proceedings of ICASSP. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Building a large syntactically-annotated corpus of Vietnamese", "author": ["P.T. Nguyen", "L.V. Xuan", "T.M.H. Nguyen", "V.H. Nguyen", "P. Le- Hong"], "venue": "Proceedings of the 3rd Linguistic Annotation Workshop, ACL- IJCNLP, Suntec City, Singapore, 2009, pp. 182\u2013185.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast dependency parsing using distributed word representations", "author": ["P. Le-Hong", "T.-M.-H. Nguyen", "T.-L. Nguyen", "M.-L. Ha"], "venue": "Trends and Applications in Knowledge Discovery and Data Mining, ser. Lecture Notes in Artificial Intelligence. Springer, 2015, vol. 9441.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving Vietnamese dependency parsing using distributed word representations", "author": ["C. Vu-Manh", "A.-T. Luong", "P. Le-Hong"], "venue": "Proceedings of SoICT. ACM, 2015, pp. 54\u201360.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid approach to word segmentation of Vietnamese texts", "author": ["P. Le-Hong", "T.M.H. Nguyen", "A. Roussanaly", "T.V. Ho"], "venue": "Language and Automata Theory and Applications, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2008, vol. 5196, pp. 240\u2013249.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26, C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 3111\u20133119.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of Workshop at ICLR, Scottsdale, Arizona, USA, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs)", "author": ["N. Okazaki"], "venue": "2007. [Online]. Available: http://www.chokkan.org/software/crfsuite/", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Updating quasi-newton matrices with limited storage", "author": ["J. Nocedal"], "venue": "Mathematics of computation, vol. 35, no. 151, pp. 773\u2013782, 1980.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1980}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, vol. 9, Sardinia, Italy, 2010, pp. 249\u2013256.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "On the effect of the label bias problem in part-of-speech tagging", "author": ["P. Le-Hong", "X.-H. Phan", "T.T. Tran"], "venue": "The 10th IEEE RIVF. Hanoi, Vietnam: IEEE, 2013, pp. 103\u2013108.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF", "author": ["X. Ma", "E. Hovy"], "venue": "Proceedings of the ACL, Berlin, Germany, August 2016, pp. 1064\u20131074.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of ACL, Uppsala, Sweden, 2010, pp. 384\u2013394.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts", "author": ["P. Le-Hong", "A. Roussanaly", "T.M.H. Nguyen", "M. Rossignol"], "venue": "Actes de Traitement Automatique des Langues, Montreal, Canada, 2010, pp. 50\u201361.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Vietnamese named entity recognition using token regular expressions and bidirectional inference", "author": ["P. Le-Hong"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "DSKTLAB-NER: Nested named entity recognition in Vietnamese text", "author": ["T.C.V. Nguyen", "T.S. Pham", "T.H. Vuong", "N.V. Nguyen", "M.V. Tran"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Vietnamese named entity recognition at VLSP 2016 evaluation campaign", "author": ["T.S. Nguyen", "L.M. Nguyen", "X.C. Tran"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Named entity recognition in Vietnamese text", "author": ["T.H. Le", "T.T.T. Nguyen", "T.H. Do", "X.T. Nguyen"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end recurrent neural network models for Vietnamese named entity recognition: Word-level vs. characterlevel", "author": ["T.-H. Pham", "P. Le-Hong"], "venue": "Proceedings of PACLING, Yangon, Myanmar, 2017.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 3, "context": "1) Conditional Random Fields: Conditional Random Fields (CRF) [4] is a discriminative probabilistic framework, which directly model conditional probabilities of a tag sequence given a word sequence.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "A feedback network called Long Short-Term Memory (LSTM) [9] was proposed to overcome these problems.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "We represent the word sequence of a sentence with a bidirectional LSTM [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Further details of the corpus are described in [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "In particular, some previous works have also integrated Vietnamese word embeddings to improve performance [12], [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "In particular, some previous works have also integrated Vietnamese word embeddings to improve performance [12], [13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "The tokenization process follows the method described in [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "We train the Mikolov\u2019s continuous Skip-gram model using the neural network and source code introduced in [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "The continuous skip-gram model itself is described in details in [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "In our experiments with the CRF model, we adopted CRFsuite [17], an implementation of linear-chain (first-order Markov) CRF.", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "We use default setting of CRFsuite in which the training algorithm is L-BFGS [18] and L2 regularization is used.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The Xavier initilizer is used for parameter initialization [19].", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "Because of the space limitation, we did not include maximum-entropy Markov models (MEMM) [3] and hidden Markov models (HMM) [1] in our comparison.", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "Because of the space limitation, we did not include maximum-entropy Markov models (MEMM) [3] and hidden Markov models (HMM) [1] in our comparison.", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "For the comparison between MEMM, HMM and CRF, we can refer to the work [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": "In particular, the work [20] investigated and compared MEMM and CRF.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "We also did not include bidirectional LSTM-CNNsCRF [21], the state-of-the-art end-to-end sequence labeling model, which combine bidirectional LSTM, CNN and CRF.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "In [22], Turian et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [23], the authors give an empirical study of MEMM for Vietnamese part-of-speech tagging with diffferent feature sets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "78% [24] in that shared task.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "Le-Hong [24] ME 88.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "[25] ME 84.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] LSTM 83.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] CRF 78.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Most recently, a more advanced end-to-end system for Vietnamese NER using LSTMs was proposed [28], which achieved an F1 score of 88.", "startOffset": 93, "endOffset": 97}], "year": 2017, "abstractText": "This paper presents an empirical study of two widely-used sequence prediction models, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks (LSTMs), on two fundamental tasks for Vietnamese text processing, including part-of-speech tagging and named entity recognition. We show that a strong lower bound for labeling accuracy can be obtained by relying only on simple word-based features with minimal handcrafted feature engineering, of 90.65% and 86.03% performance scores on the standard test sets for the two tasks respectively. In particular, we demonstrate empirically the surprising efficiency of word embeddings in both of the two tasks, with both of the two models. We point out that the state-of-the-art LSTMs model does not always outperform significantly the traditional CRFs model, especially on moderate-sized data sets. Finally, we give some suggestions and discussions for efficient use of sequence labeling models in practical applications.", "creator": "LaTeX with hyperref package"}}}