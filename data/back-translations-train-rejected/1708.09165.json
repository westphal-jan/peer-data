{"id": "1708.09165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Tensor Networks for Dimensionality Reduction and Large-Scale Optimizations. Part 2 Applications and Future Perspectives", "abstract": "Part 2 of this monograph builds on the introduction to tensor networks and their operations presented in Part 1. It focuses on tensor network models for super-compressed higher-order representation of data/parameters and related cost functions, while providing an outline of their applications in machine learning and data analytics. A particular emphasis is on the tensor train (TT) and Hierarchical Tucker (HT) decompositions, and their physically meaningful interpretations which reflect the scalability of the tensor network approach. Through a graphical approach, we also elucidate how, by virtue of the underlying low-rank tensor approximations and sophisticated contractions of core tensors, tensor networks have the ability to perform distributed computations on otherwise prohibitively large volumes of data/parameters, thereby alleviating or even eliminating the curse of dimensionality. The usefulness of this concept is illustrated over a number of applied areas, including generalized regression and classification (support tensor machines, canonical correlation analysis, higher order partial least squares), generalized eigenvalue decomposition, Riemannian optimization, and in the optimization of deep neural networks. Part 1 and Part 2 of this work can be used either as stand-alone separate texts, or indeed as a conjoint comprehensive review of the exciting field of low-rank tensor networks and tensor decompositions.", "histories": [["v1", "Wed, 30 Aug 2017 08:37:36 GMT  (4847kb,D)", "http://arxiv.org/abs/1708.09165v1", "232 pages"]], "COMMENTS": "232 pages", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["a cichocki", "a-h phan", "q zhao", "n lee", "i v oseledets", "m sugiyama", "d mandic"], "accepted": false, "id": "1708.09165"}, "pdf": {"name": "1708.09165.pdf", "metadata": {"source": "CRF", "title": "Tensor Networks for Dimensionality Reduction and Large-Scale Optimizations", "authors": ["A. Cichocki", "A-H. Phan", "Q. Zhao", "N. Lee", "I.V. Oseledets", "M. Sugiyama", "D. Mandic"], "emails": ["cia@brain.riken.jp", "phan@brain.riken.jp", "zhao@brain.riken.jp", "namgil.lee@riken.jp", "i.oseledets@skolkovotech.ru", "sugi@k.u-tokyo.ac.jp", "d.mandic@imperial.ac.uk"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "1.1 Reshaping or Folding", "text": "The simplest way of tensorizing is by redesigning or folding operations, also known as segmentation (Bousse) et al., 2015, Debals and De Lathauwer, 2015]. This type of tensorization preserves the number of original data entries and their sequential arrangement, since it rearranges only one vector into a matrix or a tensor. Therefore, folding does not require additional disk space. A tensor Y of size I1 is considered a folding of a vector y of length I2 (2)."}, {"heading": "1.2 Tensorization through a Toeplitz/Hankel Tensor", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.2.1 Toeplitz Folding", "text": "The Toeplitz matrix is a structured matrix with constant entries in each diagonal. Toeplitz matrices appear in many signal processing applications, e.g. through covariance matrices in prediction, estimation, recognition, classification, regression, harmonic analysis, speech enhancement, interference cancellation, image restoration, adaptive filtering, blind deconvolution and blind equalization [Bini, 1995, Gray, 2006]. Before we introduce a generalization of a Toeplitz matrix to a Toeplitz tensor, let us first consider the discrete convolution between two vectors x and y of the respective lengths I and L."}, {"heading": "1.2.2 Hankel Folding", "text": "The Hankel matrix and the Hankel tensor have structures similar to the Toeplitz matrix and the tensor (Hankel) and can also be used as linear operators in convolution. Hankel matrix. An I-J matrix of a vector y, the length L = I + J '1, is defined as Y = HI, J (y) = y (1) y (2) y (J) y (2) y (3) \"y\" (J + 1) \"y (J + 1).... y (I) y (I + 1)."}, {"heading": "1.2.3 Quantized Tensorization", "text": "It is important to note that the tensors of the Toeplitz and Hankel tensors typically increase the number of data samples (in the sense that the number of entries of the corresponding tensor is greater than the number of original samples), for example, if the dimensions In = 2 for all n, the tensor thus generated is a quantified order tensor (L '1), while the number of entries of such a tensor from the original size L to 2L' 1. Therefore, quantified tensors are suitable for analyzing short-term signals, especially in multivariate autoregressive models."}, {"heading": "1.2.4 Convolution Tensor", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "1.2.5 QTT Representation of the Convolution Tensor", "text": "sE \"s,\" so srteeSi, \"\" iSe sde srteeSn. \"n sE\" i \"s, sdsa sdsa sdsa\" sla, \"sdsa sdsa\" sla, \"sdsa sdsa sla,\" \"sdsa sda,\" \"sdsa sdsa,\" \"\" sdsa sdsa, \"\" \"sdsa sdsa sdsa,\" \"\" sdsa sdsa sda, \"\" sdsa sdsa, \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\", \"\" \",\" \"\", \"\" \",\" \",\" \",\" \",\", \"\", \"\", \"\", \"\", \",\" \",\" \"\", \"\", \",\", \"\", \"\" \"\" \"\", \",\" \"\" \"\", \"\" \"\", \"\" \"\" \",\" \",\" \"\" \"\" \",\" \",\" \"\", \"\" \"\" \",\" \"\", \"\" \",\" \"\", \",\" \"\", \"\", \",\" \"\", \"\" \",\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\", \"\" \",\" \"\" \",\" \",\" \"\" \",\" \",\" \"\", \"\" \"\", \",\" \"\", \"\" \"\" \",\", \"\" \"\", \"\", \"\" \"\" \",\" \"\", \"\", \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\" \"\" \",\" \",\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\", \"\", \"\" \",\" \"\" \"\" \",\" \",\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \",\" \",\" \"\" \"\" \",\" \"\" \",\" \",\" \"\""}, {"heading": "1.2.6 Low-rank Representation of Hankel and Toeplitz Matrices/Tensors", "text": "The Hankel and Toeplitz tensors are multilinear tensors and can also be applied to the BSS problem, as in (1.4). If the Hankel and Toeplitz tensors of the hidden sources are of low rank, the tensor of the mixture is expressed as the sum of low rank. (1) Hankel and Toeplitz tensors of sums and / or products of exponentials, sinusoids and polynomials will also be of low rank, corresponding to the degree of function considered as a product. More importantly, if Hankel / Toeplitz tensors of two vectors u and products of low rank CP / TT."}, {"heading": "1.3 Tensorization by Means of Lo\u0308wner Matrix (Lo\u0308wner Folding)", "text": "A Lo \u00b6 wner matrix of a vector v P RI + J is formed by a function f (t) at (I + J) different points tx1,.., xI, y1,.., yJu, to givev = [f (x1),., f (xI), f (y1),., f (yJ)] T P RI + J, so that the entries of v are divided into two separate groups, t (xi) uIi = 1 and t (yj) f (yj) uJj = 1. The vector v is then converted into the Lo \u00b6 wner matrix, L P RI\u0435 J, defined by L = [f (xi) \"f (yj) xi\" xi. Lo \u00b6 wner matrices appear as a powerful tool for matching to data (paste form) approximatively."}, {"heading": "1.4.1 Tensor Structures in Constant Modulus Signal Separation", "text": "Another method for generating relatively high order tensors in BSS is to model the estimated signals as the roots of a polynomial. Consider a linear mixing system X = HS with R sources of length K, and I mixtures where the source S module is drawn from a series of given modules. For simplicity, we assume that the binary phase shift signal in telecommunications consists of a sequence of 1 and \"1,\" that it has a constant modulus of the unit. Square phase shift of the keying (QPSK) signals takes one of the values 1, i.e. the 16-QAM signal has three square moduli of 2, 10 and 18. For this BSS problem for single constant modulus signals, Lathauwer [2004] linked the problem of a fourth order or for multi-constant signals."}, {"heading": "1.5 Tensorization by Learning Local Structures", "text": "Unlike the previous tensor horizons, this tensorization approach generates tensors from local blocks (patches) that are similar or closely related. In the example of an image, since the intensities of pixels in a small window are highly correlated, hidden structures that represent relationships between small patches of pixels can be learned in local areas. These structures can then be used to reconstruct the image as a whole in, for example, an application of the image [Phan et al., 2016].For a color RGB image Y of size I-J-3, each block of pixels of size hollyc-3 is called asYr, c = Y (r + h '1, c: c + w '1,:).A small tensor, Zr, c, of size h-3 (2d + 1), consisting of (2d + 1) 2 blocks centered around Yr."}, {"heading": "1.6 Tensorization based on Divergences, Similarities or Information Exchange", "text": "For a set of I data points xi, i = 1, 2,.., I, this type of tensorization generates a non-negative symmetric tensor of magnitude I, whose entries represent similarities or differences between xi1, xi2,.., xiN, where in = 1,..., I, ie Y (i1, i2,..., iN) = d (xi1, xi2,..., xiN). (1.37) 28 Such a metric function can express pairs of distances between the two observations xi and xj. Generally, d (xi1, xi2,..., xiN) can calculate the volume of a convex hull formed by N data points. The tensor thus generated can be extended to (N + 1) tenths sor, where the last mode expresses the change in the data, e.g. by time points or deviations between the classifications and on the basis of tensations and clues."}, {"heading": "1.7 Tensor Structures in Multivariate Polynomial Regression", "text": "Multilinear regression (MPR) is an extension of linear and multilinear regressions that allows us to model the nonlinear interaction between independent variables (1). (1) The importance of the two independent variables in the data, x1 and x2, is then quantified as to how strong the interaction between the two independent variables in the data is, x1 and x2. Observe that the model is still linear in relation to the variables x1 and x2, while it affects the crossterm w12x1x2. The above model may also have more terms, e.g. x21, x1x 2, to describe more complex functional behaviors."}, {"heading": "1.8 Tensor Structures in Vector-variate Regression", "text": "If the observations are vectors or tensors, the model can be expanded without further ado."}, {"heading": "1.9 Tensor Structure in Volterra Models of Nonlinear Systems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.9.1 Discrete Volterra Model", "text": "System identification is a paradigm that aims to provide a mathematical description of a system from the observed system inputs and outputs [Billings, 2013]. In practice, tensors are inherently present in volterra operators that model the system response of a nonlinear system that maps an input signal x (t) to an output signal y (t) in the form (t) = V (x (t) = h0 + H1 (x (t))) + H2 (x (t)) + \u00a8 \u00a8 \u00a8 + Hn (x (t) + \u00a8 \u00a8, where h0 is a constant and Hn (x (t)) is the operator of the ninth order, defined as a generalized linkage of integral volterra cores h (n) (k1) (k1) (kn) (kn) (kn) and the input signal that is Hn (x) = n (n) (kn)."}, {"heading": "H (3)", "text": "It is assumed that the volterra nuclei H (n) = [h (n) i1,..., in] have the same size M in each mode and thus result in a symmetrical tensor. Otherwise, they can be symmetrized. Curse of dimensionality. The output corresponding to the input x is written as the sum of N tensor products (see Figure 1.11), given by y = h0 + N-n-n = 1H (n)."}, {"heading": "1.9.2 Separable Representation of Volterra Kernel", "text": "To deal with the curse of dimensionality in volterra cores, we consider the core H (n) to be divisible, i.e. it can be expressed in a low-level tensor format, e.g. as a tensor or in another suitable tensor network format (for the concept of general separation of variables, see Part 1. The first and simplest divisible volterra model proposed in [Favier et al., 2012] represents the cores by symmetrical tensors of rank Rn in CP format, i.e., that is H (n) = tensor model n. (1,55) For this tensor representation, the identification problem is simplified into the estimation of N-factor matrices, An, the size M-Rn and an offset, h0, so that the number of parameters can be reduced to M-Rn + 1."}, {"heading": "1.9.3 Volterra-based Tensorization for Nonlinear Feature Extraction", "text": "In other words, for a data sample xk, which may be a recorded signal in an experiment or vectorization of an image, a feature extracted from xk through a nonlinear process is called yk = f (xk). Such a restricted (discriminatory) feature extraction can then be called a maximization of the Fisher-Scoremax structure c (y) 2 (yk'y) 2 (yk'y) T, (1.59) where y'ck is the mean feature of the samples in class k, and y'the mean feature of all samples. Next, we model the nonlinear system f (x) through a drunken Volterra system f."}, {"heading": "1.10 Low-rank Tensor Representations of Sinusoid Signals and their Applications to BSS and Harmonic Retrieval", "text": "Harmonic signals are of fundamental importance in many practical applications. This section deals with low-level structures of sinus signals under different tensorization methods. These properties can then be exploited for the 38 blind separation of sinus signals or their modulated variants, e.g. for exponentially declining signals, the examples of which are arex (t) = sin (\u03c9 t + \u03c6), x (t) = t sin (\u03c9 t + \u03c6), (1.62) x (t) = Exp ('\u03b3t) sin (\u03c9 t + \u03c6), x (t) = t Exp (' \u03b3t), (1.63) for t = 1, 2,..., L, \u0394 0."}, {"heading": "1.10.1 Folding - Reshaping of Sinusoid", "text": "The harmonic matrix is a matrix of magnitude I-2 defined by the two variables, the angular frequency \u03c9 and the folding of magnitude I, asU\u03c9, I = 1 0... cos (k\u03c9) sin (k\u03c9)... cos (I '1) \u03c9 (I' 1) \u03c9 (I '1). (1.64) Two-way folding. A matrix of magnitude I-J, folded by a sinusoid signal x (t) of length L = I J, is of rank-2, and can be disassembled asY = Urania, I S UT\u03c9I, J, (1.65) where the folding of magnitude I is invariant, only depends on the phase and takes the form S = [sin (\u03c6) cos (\u03c6) cos I) \"sin.\""}, {"heading": "1.10.2 Toeplitz Matrix and Toeplitz Tensors of Sinusoidal Signals", "text": "Toeplitz matrix of sinusoid (j '1)] = 2 (j' 1).The Toeplitz matrix, Y = 1 (n), of a sinusoid signal, y (t) = 1 (n), y (1), y (2), y (1), y (1), y (1), y (1), y (1), y (2), y (2), y (1), y (1), y (1), (1), where QT is invariant to select the folding length I, and the formQT = 1sin2 (2), ['y (3) y (2) y (2) y (2), y (1), y (1), y (1).The above expression results from the fact that [i) y (i + 1) y (3) y (2) y (2) y (2) y (2)."}, {"heading": "1.10.3 Hankel Matrix and Hankel Tensor of Sinusoidal Signal", "text": "The Hankel tensor of a sinusoid signal y (t) is a TT-Tucker tensor, Y = JG; U1, U2,..., UNK, (1,83), for which the factor matrices are defined in (1,78). The nuclear tensor G is a tensor of the ninth order, where G (1) = H (J1) is a matrix of size 1, 2, while the nuclear tensor G (n), for n = 2,..., N '1, size 2, yy, (1,84), where G (1) = H (J1) has a matrix of size 1, 2, while the nuclear tensor G (n), for n = 2,.., N' 1, size 2, size 2 and two tensors of form."}, {"heading": "1.11 Summary", "text": "This year, we have reached the point where we are able to live in a country where most people are able to move to another world, where they are able to live in that world."}, {"heading": "2.1 Tensor Regression", "text": "Regression is the core of signal processing and machine learning, where the output is typically based on a linear combination of regression coefficients and the input regressor, which can be a vector, a matrix, or a tensor. In this way, regression analyses can be used to predict dependent variables (answers, outputs, estimates) from a set of independent cases of matrix inversion variables (predictors, inputs, regressors) by examining the correlations between these variables and explaining the inherent factors behind the observed patterns. It is also often convenient, especially in poorly positioned cases of matrix inversion inherent in regression, to jointly perform regression and dimensional reduction, e.g. major component regression (Jolliffe, 1982), where regression is performed on a well-positioned one-dimensional subspace."}, {"heading": "2.2 Regularized Tensor Models", "text": "This year, it has come to the point that it is a purely reactionary project, a reactionary, reactionary and reactionary solution."}, {"heading": "2.3 Higher-Order Low-Rank Regression (HOLRR)", "text": "Consider a complete multivariate regression task in which the response has a tensor structure."}, {"heading": "2.4 Kernelized HOLRR", "text": "The HOLRR can be extended to its kernel-regulated version, the kernel-regulated HOLRR (KHOLRR). Note that the HOLRR problem in the attribute space is represented by the comb-regulated minimization problem (s) W = 0,.. (2,24) The tensor W is represented in a tucker format. (Then, the nucleus tensor G: 1RU (1RX), the nucleus tensor G: 1RX (WRK), the nucleus tensor W: 1RU (1RX), the nucleus tensor G: 1RX (U = YRX)."}, {"heading": "2.5 Higher-Order Partial Least Squares (HOPLS)", "text": "In this section, the method of the partial smallest squares (PLS) is briefly introduced, followed by their generalizations to tensors."}, {"heading": "2.5.1 Standard Partial Least Squares", "text": "The principle behind the PLS method is to search for a common set of latent vectors in the independent variable X P RI\u0445 J and the dependent variable Y RI\u0445 M by performing their simultaneous decomposition, with the limitation that the components obtained through such decomposition as much as possible of the covariance between X and Y. LS can be considered (see also Figure 2.2) X = TPT + E = R (2.28) Y = TDCT + F = R \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\""}, {"heading": "2.5.3 HOPLS using Constrained Tucker Model", "text": "An alternatives, more flexible and general multilinear regression model, referred to as \"QQ\" = \"QQ\" = \"QQ\" = \"QQ\" = \"QQ\" (Khao et al., 2011, 2013a), \"P + 1\" (K + 1), \"P + 1\" (K + 1), \"L + 1\" (K + 1), \"L + 1\" (K + 1), \"L + 1\" (K + 1), \"L + 1.,\" \"L + 1.,\" \"L + 1.,\" \"L.,\" \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\"., \".,\".,., \".,.,\"., \".,\"., \".,\".,.,., \".,.,\"., \".,\".,., \".,\".,.,., \".,\".,., \".,.,\"., \".,.,\".,., \".,.,.,\".,., \".,.,.,\".,.,., \".,.,\".,.,., \".,.,..,\".,., \".,\"...,.,..., \".,\".,..,...., \".....,\"..., \".,\".., \".,.,.,..,....,.,....,\"...., \".,\"., \".,...,.......,\"., \".,..,.,....,.,.....,\"., \".,\"., \"......,\"...,.., \".,\".,."}, {"heading": "2.6 Kernel HOPLS", "text": "Next, we present the concept of kernel-based tensor PLS (KTPLS or KHOPLS). (Hou et al., 2016b, Zhao et al., 2013c), as a natural extension of HOPLS to potentially infinite dimensional and nonlinear kernel spaces. Consider that the data tensors, Xm and Ym, can be linked together to form a (N + 1) th-order tensor X. (L + 1) th-order tensors, Xm and Ym. (N + 1) th-order tensor X RMI. (L + 1) th-order tensor I1th-order tensor Y. (L + 1) th-order tensor Y."}, {"heading": "2.7 Kernel Functions in Tensor Learning", "text": "It is not only the way in which people move in the different areas of the world in which they move, but also the way in which they move, and the way in which they move, and the way in which they move, in which they move, in the way in which they move themselves, in which they move themselves, in which they move themselves, in the way in which they move themselves, in which they move themselves, in which they move themselves, in the way in which they, in which they, in which they themselves, in which they themselves, in which they themselves, in which they themselves, in which they themselves, in which they themselves, in which they, in which they, in which they themselves, in which they themselves, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, they, they, in which they, they, in which they, in which they, they, in which they, in which they themselves, in which they, in which they, in which they themselves, in which they, in which they, in which they, in which they, they, in which they, in which they, they, in which they, they, in which they, they, in which they, they, in which they, they, in which they, they, in which they, they, in which they, they, in which they, they, they, in which they, they, in which they, they, in which they, they, in which they, they, in which they, they, they, they, they, in which they, they, they, in which, they, they, they, they, they, they, they, in which, they, they, they, they, they, in which, they, they, they, they, they, in which, they, they, they, they, they, they, they, in which, they, they, they, they, they, they, they, in which, they, they, they, they, they, they, in which, they, they, they, they, they, they, they, they, they, in which, they, they, they, they, they, they, they"}, {"heading": "2.8 Tensor Variate Gaussian Processes (TVGP)", "text": "Gaussian processes (GP) can be considered as a class of probability models that define a distribution over a functional space in which the conclusion is performed directly in the functional space [Rasmussen and Williams, 2006].The GP model for tensor-rated entrance spaces, called Tensor-based Gaussian Processes (Tensor-GP), is designed to take into account the tensor structure of the data [Hou et al., 2015, Zhao et al., 2013c, 2014].In view of a paired dataset of M observations D = t (Xm, ym) | m = 1,., Mu, the tensor inputs for all M-Instances (Cases) are aggregated into one (N + 1) th-order design concatenated tensor X P RMempor I1b, while the targets are collected in the vector y."}, {"heading": "2.9 Support Tensor Machines", "text": "In this section, the Support Vector Machine (SVM) is briefly reviewed, followed by the generalizations of SVM to matrices and tensors."}, {"heading": "2.9.1 Support Vector Machines", "text": "The classic SVM [Cortes and Vapnik, 1995] aims to find a classification hyperplane that maximizes the margin between the \"positive\" measurements and the \"negative\" measurements as shown in Figure 2.7. Consider M training measurements, xm P RL (m = 1,.., M) associated with one of the two class markers of interest, P t + 1, \"1u. The standard SVM, i.e. the soft margin SVM, describes a projection vector w P RL and a bias b P R by minimizing the cost function minutes w, b,.M, quantifiable differences of the theory M (w, b,) = 2} w} 2 + CM margin SVM (wTxm + b)."}, {"heading": "2.9.2 Support Matrix Machines (SMM)", "text": "Remember that the general SVM problem in (2,56) deals with data expressed in a vector form, x = 2. On the other hand, when the data is collected as matrices, X, it is typically first vectorized by Vec (X) and then fed into the SVM. In many classification problems, such as EEG classification, the data is naturally expressed as matrices, the structural information could be used for a better solution. For matrices, we have xW, Wy = tr (WTW), and so intuitively we could consider the following formula for the soft76margin support matrix machine [Luo et al., 2015a] min W, b, 1 2tr (WTW) + C M, s.t.t. synonymous with ym ym (tr (WTXm) + b) the following formula for the soft76margin support matrix machine [Luo et al margin, Xyma, X2015a]."}, {"heading": "2.9.3 Support Tensor Machines (STM)", "text": "This, of course, leads to an extension of the SVM by the Xm P + 1, \"which are associated with the scalar variable.\" There are two possible scenarios: 1), which lead to a continuous series of values leading to the tensor regression problem, and 2), the categorical values, which are a standard classification problem. For the classification case STM [Biswas and Milanfar, 2016, Hao et al., 2005], the following minimization may lead to a problem that is a standard classification problem."}, {"heading": "2.10 Higher Rank Support Tensor Machines (HRSTM)", "text": "The aim of the STM thus formulated is therefore to estimate a series of parameters in the form of the sum of the individual tensors (Kotsia et al., 2012) that define a hyperplane between the classes of data. Advantages of this scheme are twofold: 1. The use of a direct CP representation is intuitively closer to the idea of proper processing of tensory input data, as the data structure can be stored more efficiently; 2. The use of simple CP decompositions allows multiple projections of the input tensor along each mode, resulting in significant improvements in the discriminatory ability of the resulting classification. The corresponding optimization problem can be solved in an iterative manner, e.g. the standard CP decomposition, where at each iteration the parameters are estimated according to the projections along a single tensor mode, solving a typical STM type optimization."}, {"heading": "2.11 Kernel Support Tensor Machines", "text": "The class of tensor machines has recently been extended to the non-linear case, using the kernel framework, and is referred to as the Kernel Support System (KSTR). After we have converted each line of each original tensor (or tensor) into a high-dimensional space, we get the associated points in a new high-dimensional feature space, and then we will have a series of training samples tXm, ymu = 1,.., M, where each training sample, Xm, a data point in RI1 bRI2 and RI2 are two vector spaces associated with Xm. Denote by zmp the p-th line of Xm, we can then use a non-linear mapping function (Xm) to obtain a high-dimensional sample function and define a non-linear space."}, {"heading": "2.12 Tensor Fisher Discriminant Analysis (FDA)", "text": "(1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1.).). (1. (1.).). (1. (1.).). (1.). (1.).). (1.). (1.). (1.). (1.). (1.).). (1.). (1.).). (1. (1.).). (1.).). (1.). (1.). (1.). (1.). (1.). (1.). (1.).). (1.).). (1.).). (1.). (1.). (1.). (1.).).). (1.). (1.).).). (1"}, {"heading": "3.1 Tensor Train (TT/MPS) Splitting and Extraction of Cores", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Extraction of a Single Core and a Single Slice for ALS Algorithms", "text": "For efficient implementation of ALS optimization algorithms (n + 1), it is convenient to first share a TT network that represents a tensor (1), G (2),.., G (N), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G, G (2), G, G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (1), G (1), G (1), G (1), G (1), G (1), G (1), G (2), G (1), G (1), G (2), G (1), G (1), G (2), G (2), G (1), G (1), G (2), G (2), G (2), G (2), G (2), G (2), G (2), G (1), G (2), G (2), G (2), G (1), G (2), G (2, G (1), G (1), G (1), G (2), G (2), G (1), G (2), G (1), G (1), G (2), G (2, G (1), G (2), G (1), G (2), G (1), G (1), G (1), (1), G (1), (1), G (1),"}, {"heading": "3.1.2 Extraction of Two Neighboring Cores for Modified ALS (MALS)", "text": "The modified ALS algorithm, also known as the two-site-density-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-group (DMRG2) algorithm requires the extraction of two algorithms. At that time, people did not know the relationship between tensor network and DMRG. In [O \u00b6 stlund and Rommer, 1995] it was pointed out that the wave function generated by DMRG iteration is a matrix production state. The goal of DMRG was to calculate the basic states (minimum eigenpairs) 92 (a) neighboring cores (Holtz et al, 2012a). Similar to the previous section, the extraction of a block of two adjacent TT cores is based on the following linear equation, n + 1 (n)."}, {"heading": "3.2 Alternating Least Squares (ALS) and Modified ALS (MALS)", "text": "Consider the minimization of a scalar cost structure (energy), J (X), from an Nth-order tensor variable, expressed in TT format as X - xxX (1), X (2),.., X (N) yy. However, the solution is sought in the form of a tensor pull (1), the simultaneous minimization across all cores X (n) is usually2We remember here that G (n) P R Rn '1 In and G (n) (1) P R Rn '1 is the simultaneous minimization across all cores X (n) and the matrification of the TT-core tensor G (n), that G RRn '1, resp. P Rn. \"96 Algorithm 7: Alternating Least Squares (ALS).Input: Cost (energy) function J (X) and an initial guess for anth-order."}, {"heading": "3.3 Tensor Completion for Large-Scale Structured Data", "text": "The goal of the reconstruction is to reconstruct a high-dimensional, structured multiway array for which a large portion of the entries are missing or noisy. (1) The assumptions that a good, low TN rate for an incomplete data type sor Y MR: = tX P RI1- \"The problem of tensor completion exists as the following optimization problem can be formulated:\" XJ (X). \"(X)\" It is the minimum of optimization in XJ (X). \"(X)\" It is the reconstruction sensor in TT format and rTT-R2. \"(X).\" RN. \"(X)\" It is the symbol of reconstruction tensor in TR1, R2. \"(X). (X). (X). (X). (X). (X)."}, {"heading": "3.4 Computing a Few Extremal Eigenvalues and Eigenvectors", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1 TT Network for Computing the Single Smallest Eigenvalue and the Corresponding Eigenvector", "text": "This problem can be formulated as standard symmetrical eigenvalue decomposition (EVD) in formA xk = eigenvalues, k = 1, 2,., K, (3.21), where xk P RI represents the orthonormal eigenvalue decomposition (EVD) and the corresponding eigenvalues of a symmetrical matrix A = eigenvalue problems (K = 1, 2,., K, (3.21), where xk P RI uses the orthonorthonormal eigenvalue decomposition (EVD) and the corresponding eigenvalues of a symmetrical matrix A = eigenvalue problems I.Iterative algorithms for extreme eigenvalue problems often use the Rayleigh quotient (RQ) of a symmetrical matrix as the following cost functions J (x) = R (A) = xTAx = xTx xAx = xAx, xy xx, x \u0445 0. (3.22) Based on the Rayleigh quotient (RQ), the largest, smallest and smallest values are."}, {"heading": "3.4.2 TT Network for Computing Several Extremal Eigenvalues and Corresponding Eigenvectors for Symmetric EVD", "text": "In a more general case, several (say, K) extreme self-values and the corresponding K-eigenvectors of a symmetrical structured matrix A (3) can be calculated. (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4. (4).). (4.). (4.).). (4. (4.). (4.).). (4.). (4.). (4. (4.). (4.).). (4.). (4.). (4.). (4. (4.). (4.).). (4.).). (4. (4.). (4.).). (4.). (4.). (4.).). (4.).). (4.). (4.).). (4.).)"}, {"heading": "3.4.3 Modified ALS (MALS/DMRG2) for Symmetric EVD", "text": "In order to speed up the convergence rate and improve the performance of the ALS / DMRG1 methods, the MALS (2-site DMRG2) scheme allows optimization instead of just one, simultaneously two adjacent TT cores at each micro-iteration process, while the other TT cores remain unchanged (assuming they are known), reducing a large-scale optimization problem to a series of smaller optimization problems, although the MALS subproblems are substantially greater than those for ALS. After local optimization (micro-iteration) on a merged core tensor, X (n, n + 1) to a series of small-scale optimization problems, although the MALS subproblems are split (factored) into two separate TT cores (micro-iteration) on a folded core tensor, X (n, n) in the next step."}, {"heading": "3.4.4 Alternating Minimal Energy Method for EVD (EVAMEn)", "text": "Remember that the standard ALS method cannot adaptively change the TT ranks during the iteration process, while the MALS method achieves this in intermediate steps, by merging and splitting two adjacent TT cores, as shown in the previous section. J (X) is not convex in relation to all elements of the core tensors of X and as a result of ALS being prone to getting stuck in local minimums and suffering from a relatively slow convergence speed. On the other hand, the MALS intermediate steps help alleviate these problems and improve the convergence speed, but there is still no guarantee that the algorithm will be converted to a global minimum. The Alternating Minimal Energy (AMEn) algorithms aim to avoid the problem of convergence to a non-global minimum by evaluating the information about the gradient of a cost function or the value of a residuality through \"residuality.\""}, {"heading": "3.5 TT Networks for Tracking a Few Extreme Singular Values and Singular Vectors in SVD", "text": "Similar to the symmetric EVD problem described in the previous section, the block TT concept can be used to calculate only K largest single values and the corresponding single vectors of a given matrix. (Cichocki, 2013, 2014, Lee and Cichocki, 2015): J (U, V) = tr (UTAV), s.t. UTU = IK, VTV = IK, (3.40), where U P RIE and V P RJB (RJB).K (VTA: U), s.t. UTU = IK, (3.41), where the calculation of K smallest single values and the corresponding single vectors of a given matrix, A P RIE J, can be formulated as the following optimization problem: max U, Vtr (VTA: U), s.t. UTU = IK, where A: P RJK = IK."}, {"heading": "3.6 GEVD and Related Problems using TT Networks", "text": "ii. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i"}, {"heading": "3.6.1 TT Networks for Canonical Correlation Analysis (CCA)", "text": "The method introduced by Hotelling in 1936 can be regarded as a generalization of PCA and is a classical method for determining the relationship between two sets of variables (for modern approaches to CCA, see [Bach and Jordan, 2005, Chu et al., 2013, Cunningham and Ghahramani, 2015]] and references that correlate with each other. Formally, the classical CCA calculates two projection vectors, wx = w (1) x P RI and wy = w (1) x P RL, just as maximize the correlation."}, {"heading": "3.6.2 Tensor CCA for Multiview Data", "text": "The standard matrix CCA model has been generalized to the tensor CCA, which in its simplest form can be formulated as the following optimization problem: tw (n) u (C) 1 w (1) 2 w (2) 2 w (2) \u00a8 \u00a8 n w (N))) (3.67) s.t. w (n) T Cnn w (n) = 1 (n = 1, 2,.., N), where w (n) P RIn are canonical vectors, Cnn P RIn\u043e In are covariance matrices and C P RI1\u0445 I2o (n).N IN is a Nth-order data tensor [Kim and Cipolla, 2009, Kim et al., Luo et al, 2015b].Similar to the standard CCA, the goal is to find the canonical vectors that maximize the correlation function."}, {"heading": "3.7 Two-Way Component Analysis in TT Formats", "text": "Low matrix factorizations with specific constraints can be formulated in the following standard optimization setting [Cichocki, 2011, Cichocki and Zdunek, 2006, 2007, Cichocki et al., 1995, 2009] min A, BJ (A, B) =} X'ABT} 2F, (3,72) where a large-scale data matrix, X P RI\u0445 J, is given and the goal is to illustrate the factor matrices, A P RI\u0445 R and B P RJ\u043e R (with assumptions 128 (a) I1I2INX1X2XNJ 1J 1J TXT + + + \u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5\u03b5I I1 = = = = I2INC11 I1C22CNNINI2 (with assumptions 128 (a) I1II2XNJ 1J 1J 1J TXIII2IINJ (b)."}, {"heading": "3.8 Solving Huge-Scale Systems of Linear Equations", "text": "Solving linear systems of large-scale equations occurs throughout science and technology; e.g., convex optimization, signal processing, finite elements, and machine learning are partly based on approximate solution of linear equations, typically using some additional criteria such as sparseness or smoothness. Consider a vast system of linear algebraic equations in TT format (see also Part 1 [Cichocki et al., 2016]) given by Ax - b (3.73), using A P RI\u0445 J, b P RI, and x P RJ. The goal is to find an approximate solution, x P RJ, in TT format, by imposing additional constraints (regularization terms) such as smoothness, thrift, and / or non-negativity on the vector x. Several innovative TT / HT network solutions to this problem are based on this (see Dolgogov and Dolegov, 2014 and Dolegov Selecanavov, 2014 and Dolegov)."}, {"heading": "3.8.1 Solutions for a Large-scale Linear Least Squares Problem using ALS", "text": "The least squares (LS) solution (often referred to as burr regression) minimizes the following cost functionsJ (x) = Ax'b) 22 + \u03b3} Lx 22 = xTATAx '2xTATb + bTb + \u03b3xTLTLx, (3.74) 131with the Tikhonov regularization term on the right side, while L is the so-called smoothing matrix, typically in the form of the discrete first order or second order derivative matrix, and rL = LTL P RJ\u043e J.Upon neglecting the constant factor, bTb, we come at a simplified formJ (x) = xTATAx' 2xTATb + gxTration systems. (3.75) The approximate TT representation of matrix A and vectors x and b allows the construction of three sor sub-networks, as shown in Figure 3.18."}, {"heading": "3.8.2 Alternating Minimum Energy (AMEn) Algorithm for Solving a Large-scale Linear Least Squares Problems", "text": "The AMEn approach introduced in Section 3.4.4 for the EVD problem was initially developed historically as an efficient solution to large-scale problems of the smallest squares. The main difference from the EVAMEn method for solving eigenvalue problems lies in the definition of the gradient of the cost function and in the calculation of the enrichment nuclei Z (n). The main difference from the EVAMEn method for solving huge systems of linear equations Ax - b, for A P RIX J and b P RI in TT format, with I = I1 I2 \u00a8 ME IN and J = J2 \u00a8 \u00a8 \u00a8 \u00a8 \u00a8 JN in the first preliminary step, we assume that the core X (n) P RRn '1\u00ba Qar. \""}, {"heading": "3.8.3 Multivariate Linear Regression and Regularized Approximate Estimation of Moore-Penrose Pseudo-Inverse", "text": "The approaches described in the two previous sections can be directly extended to regularized multivariate regression (A = n), which is standardized as minimizing the cost functionJ (X) = \"B\" 2F + \"LX\" 2F (3.82) = \"Tr (XTATAX)\" 2 \"Tr (BTAX) +\" Tr (XTLTLX) n, \"with A P\" J (with I J), B P \"K,\" X \"P\" RJ1 \"and L\" (1P \"J) n.\" The goal is to find an approximate solution by imposing additional constraints (e.g. by Tikhonov's regulation). In a specific case, if B = II, for K = I the problem of calculating Moore-Penrose. \""}, {"heading": "3.8.4 Solving Huge-scale LASSO and Related Problems", "text": "One way to impose austerity constraints on a vector or matrix is to replace it with the LASSO approach = = TB components, which can be formulated as the following optimization problem [Boyd et al., 2011, Kim et al., 2015, Tan et al., 2015, Tibshirani, 1996a]: min x x} Ax'b} x (1) x 1) x (3) x (2) x (2) x (2) x (2) x (2) x (2) x) x (2) x) x (2) x (2) x) x (2) x (2) x) x (2) x) x (2) x (2) x). In many applications it is possible to use priority information about a savings profile or savings pattern [El Alaoui et al., 2016]."}, {"heading": "3.9 Truncated Optimization Approach in TT Format", "text": "A wide range of optimization problems can be solved by iterative algorithms, which can generally be called \"-J.\" \"(1). (1). (1). (1). (1). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (3). (3). (3). (3). (3). (3). (3). (3). (3). (). (3). ().). (3). (3)."}, {"heading": "3.10 Riemannian Optimization for Low-Rank Tensor Manifolds", "text": "It is therefore natural to ask whether the Riemannian optimization can also contribute to significantly reducing the costs of obtaining solutions. Furthermore, from a Riemannian geometry point of view, limited optimization problems can often be considered unlimited. It is natural to ask whether the Riemannian optimization can also help to open up new research guidelines in connection with tensor networks; see for example [Ishteva et al., 2011, Kasai and Mishra, 2015, Sato et al., 2017, Zhou et al., 2016].RiemanniJ for optimization."}, {"heading": "3.10.1 Simplest Riemannian Optimization Method: Riemannian Gradient Descent", "text": "The simplest method of optimization is the gradient parentage, which has formXk + 1 = Xk \"\u03b1k\" J (Xk), where \u03b1k \"0 is the step size. However, for a sufficiently small \u03b1, the value of the cost function J (X) will decrease. If multiplicity is a linear subspace, then the solution is quite simple - we just need to take a projected gradient step, i.e. project J (Xk) to this subspace. For a general smooth multiplicity, we can do the same, but the projection of the tangent space at the current iteration point, isXk + 1 = Xk.\""}, {"heading": "3.10.2 Advanced Riemannian Optimization Methods", "text": "Vector Transport. More advanced optimization techniques such as conjugate gradient (CG) type methods and quasi-Newton methods use instructions from previous iteration steps. However, these instructions are in different tangents that differ from the corresponding optimization in Euclidean spaces. To this end, we must apply the concept of vector transport, which plays a crucial role in belt optimization. The idea of vector transport is quite simple: it represents an imaging of one tangent space TXMr, to another tangent space TYMr. For low-rank matrix / tensor manifestations X2, the orthogonal projection onto a tangent space PTYMr 'is an example of vector transport. Riemannian CG method. Figure 3.24 illustrates vector transport during a Riemannian CG iteration that converts a tangent vector vector."}, {"heading": "3.10.3 Riemannian Newton Method", "text": "reD rf\u00fc ide rf\u00fc ide rf\u00fc \u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green \"so for the green for the green\" so for the green for the green."}, {"heading": "3.10.4 Low-Rank TT-Manifold", "text": "Consider a series of tensors of the ninth order in TT formats, where all TT ranks are equal (R1,.., RN '1). This theorem can also be thought of as the intersection of (N' 1) low-order matrix manifolds, since the second TT rank of a tensor X is equal to the matrix rank of its nth mode, which unfolds X\u0103n\u0105n. Therefore, a tensor in TT format can be parameterized in the form X (i1,., iN) = X (1) i1 \u00a8 X (N) iN, where X (n) in P R '1\u0445 Rn columns of the TT cores X (n).153Tangent space of the TT. Tangenent space at a point of the TT multiplicity is defined as a series of tensors of the formation X (i1, i1,., iN)."}, {"heading": "3.10.5 Dynamical Low-Rank Matrix/Tensor Approximation", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3.10.6 Convergence of Riemannian Optimization Algorithms", "text": "The convergence theory for the optimization of non-convex manifolds is much more complicated than the corresponding theory in Euclidean spaces and far from complete. Local convergence results result from the general theory [Absil et al., 2008], however, the important problem of curvature and singular points has not yet been fully addressed, and even if the last point is on the multiplicity, it is not clear whether it is better to take a course on the multiplicity. An attempt to study global convergence has been presented in [Kolesnikov and Kolesnidets, 2016], and even in this case convergence to the spurious local minima is possible in a carefully designed example. Also, convergence should be considered together with a slight approximation to the solution itself. If we are far from having converged, convergence to the spurious local minima is possible, while convergence in the factory \"7 should be observed gradually."}, {"heading": "3.10.7 Exponential Machines for Learning Multiple Interactions", "text": "In this section, we will briefly describe such a promising use case where the results of machine learning tasks with categorical data, where it is important to model properly complex interactions between multiple characteristics for improved performance, are considered. Let's consider the special case of N = 3, the linear model that includes interactions between the characteristics x1, x2, and x3. (xN, m) T is a N-dimensional feature vector and ym is the target value of the m-th object. In the special case of N = 3, the linear model that includes interactions between the characteristics x1, x3, and x3. (x) = w000 + w100x1 + w001x3 + w101x3 + w10x3 + w10x3 + w10x3 x3 + w10x3 x3 x3 x3."}, {"heading": "3.10.8 Future Perspectives of Riemannian Optimization", "text": "There has been a steady growth of interest in Riemannian optimization for machine learning problems, especially in the context of low-rank matrix constraints [Boumal and Absil, 2011, Hosseini and Sra, 2015]. As an alternative, the nuclear standard regularization can be used as it159Algorithm 14: Stochastic Riemannian optimization for learning interactions (ExM) [Novikov et al., 2016] Input: training data t (xm, ym) uMm = 1, desired TT rank tR1,.., RN \"1u, number of iterations K, mini-batch size P, 0 \u0103 C1, 0 \u0103 1, 0 \u0103 1 output: Riemansor W in TT format, which roughly minimizes the loss function (3,118) 1: Rough size 0 = 2: for k = 1,."}, {"heading": "3.11 Software and Computer Simulation Experiments with TNs for Optimization Problems", "text": "The TT toolbox, developed by Oseledets and colleagues, is currently the most comprehensive software for the TT (MPS) toolbox developed for the TT (MPS / MPO) toolbox. TT toolbox supports advanced applications based on solving linear equations (including AMEn algorithms), symmetrical eigenvalue decomposition (EVD), and inverse / psudoinverse of the giant matrices.Related toolbox. The TT toolbox supports the development of linear equations (including AMEn algorithms)."}, {"heading": "3.12 Open Problems and Challenges in Applying TNs for Optimization and Related Problems", "text": "In this chapter we have shown that tensor networks are promising tools for very large-scale optimization problems."}, {"heading": "4.1 A Perspective of Tensor Networks for Deep Learning", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2 Restricted Boltzmann Machines (RBM)", "text": "The RBMs are basic building blocks in a quantum state and are mathematically expressed due to their inherent density matrix.173Probability distributions of a variety of input data, including natural image and speech signals. The RBMs are basic basic building blocks in a class of Deep (Limited) Machines (DBMs) and Deep Belief Nets (DBNs). In such deep neural networks, after the formation of an RBM layer, the values of their hidden units can be treated as data for the formation of higher-level RBMs."}, {"heading": "4.2.1 Matrix Variate Restricted Boltzmann Machines", "text": "The Matrix Variate Restricted Boltzmann Model (MVRBM) has been proposed as a generalization of classical RBM to explicitly model matrix data [Qi et al., 2016], with both input and hidden variables contained in their matrix shapes linked by bilinear transformations. MVRBM has much fewer model parameters than the standard matrix of hidden variables, and thus admires 178 faster training while maintaining a comparable performance to the classical RBM.Let V P RJB M be a binary matrix of visible variables, and H P RKB I be a binary matrix of hidden variables. Given the parameters of a 4th order tensor, W P RIB RJB and bias matrices, A P RJB RK I, which is a free matrix matrix matrix matrix matrix, can be defined."}, {"heading": "4.2.2 Tensor-Variate Restricted Boltzmann Machines", "text": "The tensor-variate RMBs (TvRBMs) are able to capture multiplicative interactions between data modes and latent variables (see e.g. [Nguyen et al., 2015]).The TvRBMs are highly compact, as the number of free parameters only increases linearly with the number of modes, while their multiway factoring modes link data modes and hidden units. Modes interact through hidden units, and TvRBM uses tensor data and the hidden layer to sort a (N + 1) mode.179 + -AVJM + H-B IKVH-W (1) RI K-WJ MIn a TvRBM, the visible units are represented by a Nth-order tensor."}, {"heading": "4.3 Basic Features of Deep Convolutional Neural Networks", "text": "Basic DCNNs are characterized by at least three features: locality, weight distribution (optional), and merging, as explained below: \u2022 Locality refers to the connection of an (artificial) neuron to only adjacent neurons in the previous layer, rather than being fed by the entire layer (this is consistent with biological NNNs). \u2022 Weight distribution reflects the property that different neurons in the same layer that are connected to different neighborhoods in the previous layer often have the same weights. Note that weight distribution, when combined with locality, results in standard convolution6. \u2022 Pooling is essentially an operator that gradually decimates (reduces) the layer size by replacing the local population of neuronal activations in a spatial window with a single value (e.g. by classifying their maxima, averages, or their scaled products."}, {"heading": "4.4 Score Functions for Deep Convolutional Neural Networks", "text": "Consider a multi-level classification task in which the input data, also referred to as local structures or instances, e.g. input fields in images, by the sentence X = tx1,.., xNu, where xn P RS, (n = 1,., N), belongs to one of the different categories (classes) represented by c P t1, 2,., Cu. Such representation is for many high-dimensional data - in images that represent local structures vectorization of patches consisting of S pixels, while in audio data can be represented by spectrograms. For this type of problem, DCNNs can be described by a number of multivariate score functions (x1,., xN) = I1., xN) = I1 \"At the Ixels level,\" in which audio data can be represented by spectrograms. For this type of problems, DCNNs can be described by a number of multidimensional functions."}, {"heading": "4.5 Convolutional Arithmetic Circuits and HT Networks", "text": "In terms of terms, the Convolutionary Arithmetic Circuit (ConvAC) is divided into three parts: (i) the first (input) layer is the representation layer, which converts input vectors (x1,.., xN) into real values; (i) the first (input) layer is the representation layer, the input vectors (x1,..,.., in other words, the representation functions, which consist of many hidden layers, which are the N I measurements (training samples) generated by the representation layer; (iii) the output layer, which can be represented by a matrix; and the second, a key part is a revolutionary arithmetic circuit, which consists of many hidden layers."}, {"heading": "4.6 Convolutional Rectifier NN Using Nonlinear TNs", "text": "Convolutionary arithmetic (ConvACs) uses the standard model (tensor) (tensor) products (11), which are for two tensors, A P RI1\u0445 \u00a8 \u00a8 \u00a8 \u00a8 \"IN\" and B P RJ1\u0435 \"JM,\" as (A) B) i1,..., iN, j1,..., iN, jM = ai1,..., iN, jM, jM. To convert ConvAC tensor models into widely used revolutionary rectifier networks, we must use the generalized (nonlinear) outer products, defined as [Cohen and Shashua, 2016] (A), B) i1, jM = 1, jM = (ai1,..., iN, bj1, jM), (4.27) outer products, defined as [Cohen and Shashua, 2016]."}, {"heading": "4.7 MERA and 2D TNs for a Next Generation of DCNNs", "text": "This year it is more than ever before."}, {"heading": "1 Estimation of Derivatives of GCF", "text": "The GCF of the observation and its derivatives are unknown, but can be estimated on the basis of the first GCF derivatives (u). (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (n) (n) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (n) (u) (n) (n) (n) (u) (u) (u) (u) (n) (u) (u) (u) (u) (u) (n) (u) (u) (u) (n) (u) (u) (u) (u) (u) (u) (u) (u) (u) (n) (u) (u) (n) (u) (u) (u) (u) (u) (u) (n) (u) (u) (u) (u) (u) (u) (u) (u)"}, {"heading": "2 Higher Order Cumulants", "text": "109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) 109 (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109) (109)"}, {"heading": "3 Elementary Core Tensor for the Convolution Tensor", "text": "The elementary core tensor S of size N-2-3-4-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5 is explained as follows: S-1 + iN = # max (N'n + 1, 0) odd n, max (N'n + 3, 0) even n, (A.5) even n, (A.5). The structure of the tensor S based on Sn is explained as follows: S (1,.,., n) odd n, max (N'n, 0) odd n, (S2n) odd sor n,."}, {"heading": "Acknowledgements", "text": "The authors would like to express their sincere thanks to the anonymous critics for their constructive and helpful comments. We also appreciate the helpful comments by Claudius Hubig (Ludwig Maximilians University, Munich) and Victor Lempitsky (Skolkovo Institute of Science and Technology, Moscow), as well as the help in the work of Zhe Sun (Riken BSI). We appreciate the insightful comments and rigorous proofreading of selected chapters by Anthony Constantinides, Ilia Kisil, Giuseppe Calvi, Sithan Kanna, Wilhelm von Rosenberg, Alex Stott, Thayne Thiannithi and Bruno Scalzo Dees (Imperial College London), which was partially supported by the Ministry of Education and Science of the Russian Federation (grant 14,756,31,001) and the EPSRC in Great Britain (grant EP / P008461)."}], "references": [{"title": "Low-rank retractions: A survey and new results", "author": ["P.-A. Absil", "I.V. Oseledets"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Absil and Oseledets.,? \\Q2015\\E", "shortCiteRegEx": "Absil and Oseledets.", "year": 2015}, {"title": "Trust-region methods on Riemannian manifolds", "author": ["P.-A. Absil", "C.G. Baker", "K.A. Gallivan"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Absil et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2007}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2008}, {"title": "An extrinsic look at the Riemannian Hessian", "author": ["P.-A. Absil", "R. Mahony", "J. Trumpf"], "venue": "In Geometric Science of Information,", "citeRegEx": "Absil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2013}, {"title": "Newton\u2019s method on Riemannian manifolds and a geometric model for the human spine", "author": ["R.L. Adler", "J.-P. Dedieu", "J.Y. Margulies", "M. Martens", "M. Shub"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Adler et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Adler et al\\.", "year": 2002}, {"title": "Sparse non-negative generalized PCA with applications to metabolomics", "author": ["G.I. Allen", "M. Maletic-Savatic"], "venue": null, "citeRegEx": "Allen and Maletic.Savatic.,? \\Q2011\\E", "shortCiteRegEx": "Allen and Maletic.Savatic.", "year": 2011}, {"title": "The N-way toolbox for MATLAB", "author": ["C.A. Andersson", "R. Bro"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Andersson and Bro.,? \\Q2000\\E", "shortCiteRegEx": "Andersson and Bro.", "year": 2000}, {"title": "Deep convolutional networks are hierarchical kernel machines", "author": ["F. Anselmi", "L. Rosasco", "C. Tan", "T. Poggio"], "venue": "arXiv preprint arXiv:1508.01084,", "citeRegEx": "Anselmi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2015}, {"title": "On the approximation of functionals of very large hermitian matrices represented as matrix product operators", "author": ["M. August", "M. Ba\u00f1uls", "T. Huckle"], "venue": "CoRR, abs/1610.06086,", "citeRegEx": "August et al\\.,? \\Q2016\\E", "shortCiteRegEx": "August et al\\.", "year": 2016}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Technical report,", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Adaptive near-optimal rank tensor approximation for high-dimensional operator equations", "author": ["M. Bachmayr", "W. Dahmen"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Bachmayr and Dahmen.,? \\Q2015\\E", "shortCiteRegEx": "Bachmayr and Dahmen.", "year": 2015}, {"title": "Iterative methods based on soft thresholding of hierarchical tensors", "author": ["M. Bachmayr", "R. Schneider"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Bachmayr and Schneider.,? \\Q2016\\E", "shortCiteRegEx": "Bachmayr and Schneider.", "year": 2016}, {"title": "Tensor networks and hierarchical tensors for the solution of high-dimensional partial differential equations", "author": ["M. Bachmayr", "R. Schneider", "A. Uschmajew"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Bachmayr et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bachmayr et al\\.", "year": 2016}, {"title": "Algorithm 862: MATLAB tensor classes for fast algorithm prototyping", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Bader and Kolda.,? \\Q2006\\E", "shortCiteRegEx": "Bader and Kolda.", "year": 2006}, {"title": "MATLAB tensor toolbox version", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Bader and Kolda.,? \\Q2015\\E", "shortCiteRegEx": "Bader and Kolda.", "year": 2015}, {"title": "A tensor-based volterra series black-box nonlinear system identification and simulation framework", "author": ["K. Batselier", "Z. Chen", "H. Liu", "N. Wong"], "venue": "In 2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),", "citeRegEx": "Batselier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Batselier et al\\.", "year": 2016}, {"title": "Tensor train alternating linear scheme for MIMO Volterra system identification", "author": ["K. Batselier", "Z. Chen", "N. Wong"], "venue": "CoRR, abs/1607.00127,", "citeRegEx": "Batselier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Batselier et al\\.", "year": 2016}, {"title": "A reduced basis approach for calculation of the Bethe\u2013Salpeter excitation energies by using low-rank tensor factorisations", "author": ["P. Benner", "V. Khoromskaia", "B.N. Khoromskij"], "venue": "Molecular Physics,", "citeRegEx": "Benner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Benner et al\\.", "year": 2016}, {"title": "Tensor spectral clustering for partitioning higher-order network structures", "author": ["A.R. Benson", "D.F. Gleich", "J. Leskovec"], "venue": "In Proceedings of the 2015 SIAM International Conference on Data Mining,", "citeRegEx": "Benson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Benson et al\\.", "year": 2015}, {"title": "Iteration-complexity of gradient, subgradient and proximal point methods on Riemannian manifolds", "author": ["G.C. Bento", "O.P. Ferreira", "J.G. Melo"], "venue": "arXiv preprint arXiv:1609.04869,", "citeRegEx": "Bento et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bento et al\\.", "year": 2016}, {"title": "Algorithms for numerical analysis in high dimensions", "author": ["G. Beylkin", "M.J. Mohlenkamp"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Beylkin and Mohlenkamp.,? \\Q2005\\E", "shortCiteRegEx": "Beylkin and Mohlenkamp.", "year": 2005}, {"title": "Quantum machine learning", "author": ["J. Biamonte", "P. Wittek", "N. Pancotti", "P. Rebentrost", "N. Wiebe", "S. Lloyd"], "venue": "arXiv preprint arXiv:1611.09347,", "citeRegEx": "Biamonte et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Biamonte et al\\.", "year": 2016}, {"title": "A tensor approximation method based on ideal minimal residual formulations for the solution of highdimensional problems", "author": ["M. Billaud-Friess", "A. Nouy", "O. Zahm"], "venue": "ESAIM: Mathematical Modelling and Numerical Analysis,", "citeRegEx": "Billaud.Friess et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Billaud.Friess et al\\.", "year": 2014}, {"title": "Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains", "author": ["S.A. Billings"], "venue": null, "citeRegEx": "Billings.,? \\Q2013\\E", "shortCiteRegEx": "Billings.", "year": 2013}, {"title": "Toeplitz matrices, algorithms and applications", "author": ["D. Bini"], "venue": "ECRIM News Online Edition,,", "citeRegEx": "Bini.,? \\Q1995\\E", "shortCiteRegEx": "Bini.", "year": 1995}, {"title": "Linear support tensor machine: Pedestrian detection in thermal infrared images", "author": ["S.K. Biswas", "P. Milanfar"], "venue": "arXiv preprint arXiv:1609.07878,", "citeRegEx": "Biswas and Milanfar.,? \\Q2016\\E", "shortCiteRegEx": "Biswas and Milanfar.", "year": 2016}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2009}, {"title": "Multigrid Methods for Tensor Structured Markov Chains with Low Rank Approximation", "author": ["M. Bolten", "K. Kahl", "S. Sokolovi\u0107"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Bolten et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bolten et al\\.", "year": 2016}, {"title": "Stochastic gradient descent on Riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Bonnabel.,? \\Q2013\\E", "shortCiteRegEx": "Bonnabel.", "year": 2013}, {"title": "Manopt, a MATLAB toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Boumal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2014}, {"title": "A tensor-based method for large-scale blind source separation using segmentation", "author": ["M. Bouss\u00e9", "O. Debals", "L. De Lathauwer"], "venue": "Technical Report Tech. Report 15-59,", "citeRegEx": "Bouss\u00e9 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouss\u00e9 et al\\.", "year": 2015}, {"title": "Multiway calibration. Multilinear PLS", "author": ["R. Bro"], "venue": "Journal of Chemometrics,", "citeRegEx": "Bro.,? \\Q1996\\E", "shortCiteRegEx": "Bro.", "year": 1996}, {"title": "The density matrix renormalization group for a quantum spin chain at non-zero temperature", "author": ["R.J. Bursill", "T. Xiang", "G.A. Gehring"], "venue": "Journal of Physics: Condensed Matter,", "citeRegEx": "Bursill et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bursill et al\\.", "year": 1996}, {"title": "Computing sparse representations of multidimensional signals using Kronecker bases", "author": ["C. Caiafa", "A. Cichocki"], "venue": "Neural Computaion,", "citeRegEx": "Caiafa and Cichocki.,? \\Q2013\\E", "shortCiteRegEx": "Caiafa and Cichocki.", "year": 2013}, {"title": "Stable, robust, and super\u2013fast reconstruction of tensors using multi-way projections", "author": ["C. Caiafa", "A. Cichocki"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Caiafa and Cichocki.,? \\Q2015\\E", "shortCiteRegEx": "Caiafa and Cichocki.", "year": 2015}, {"title": "Entanglement entropy and quantum field theory", "author": ["P. Calabrese", "J. Cardy"], "venue": "Journal of Statistical Mechanics: Theory and Experiment,", "citeRegEx": "Calabrese and Cardy.,? \\Q2004\\E", "shortCiteRegEx": "Calabrese and Cardy.", "year": 2004}, {"title": "Robust low-rank matrix completion by Riemannian optimization", "author": ["L. Cambier", "P.-A. Absil"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Cambier and Absil.,? \\Q2016\\E", "shortCiteRegEx": "Cambier and Absil.", "year": 2016}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Candes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2008}, {"title": "A family of probabilistic kernels based on information divergence", "author": ["A.B. Chan", "N. Vasconcelos", "P.J. Moreno"], "venue": "Technical report,", "citeRegEx": "Chan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2004}, {"title": "Generalized coherent states, reproducing kernels, and quantum support vector machines", "author": ["R. Chatterjee", "T. Yu"], "venue": "arXiv preprint arXiv:1612.03713,", "citeRegEx": "Chatterjee and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Chatterjee and Yu.", "year": 2016}, {"title": "Preconditioning for accelerated iteratively reweighted least squares in structured sparsity reconstruction", "author": ["C. Chen", "J. Huang", "L. He", "H. Li"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Structured Tensors: Theory and Applications", "author": ["H. Chen"], "venue": "PhD thesis,", "citeRegEx": "Chen.,? \\Q2016\\E", "shortCiteRegEx": "Chen.", "year": 2016}, {"title": "On the equivalence of Restricted Boltzmann Machines and Tensor Network States", "author": ["J. Chen", "S. Cheng", "H. Xie", "L. Wang", "T. Xiang"], "venue": "ArXiv eprints,", "citeRegEx": "Chen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Representations of non-linear systems: the narmax model", "author": ["S. Chen", "S.A. Billings"], "venue": "International Journal of Control,", "citeRegEx": "Chen and Billings.,? \\Q1989\\E", "shortCiteRegEx": "Chen and Billings.", "year": 1989}, {"title": "Parallelized tensor train learning of polynomial classifiers", "author": ["Z. Chen", "K. Batselier", "J.A.K. Suykens", "N. Wong"], "venue": "CoRR, abs/1612.06505,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Numerical methods for high-dimensional probability density function equations", "author": ["H. Cho", "D. Venturi", "G.E. Karniadakis"], "venue": "Journal of Computational Physics,", "citeRegEx": "Cho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2016}, {"title": "DFacTo: Distributed factorization of tensors", "author": ["J.H. Choi", "S. Vishwanathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Choi and Vishwanathan.,? \\Q2014\\E", "shortCiteRegEx": "Choi and Vishwanathan.", "year": 2014}, {"title": "Sparse canonical correlation analysis: New formulation and algorithm", "author": ["D. Chu", "L.-Z. Liao", "M.K. Ng", "X. Zhang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Chu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2013}, {"title": "Tensor decompositions: New concepts in brain data analysis", "author": ["A. Cichocki"], "venue": "Journal of the Society of Instrument and Control Engineers,", "citeRegEx": "Cichocki.,? \\Q2011\\E", "shortCiteRegEx": "Cichocki.", "year": 2011}, {"title": "Era of big data processing: A new approach via tensor networks and tensor decompositions, (invited)", "author": ["A. Cichocki"], "venue": "In Proceedings of the International Workshop on Smart Info-Media Systems in Asia (SISA2013),", "citeRegEx": "Cichocki.,? \\Q2013\\E", "shortCiteRegEx": "Cichocki.", "year": 2013}, {"title": "Tensor networks for big data analytics and large-scale optimization problems", "author": ["A. Cichocki"], "venue": "arXiv preprint arXiv:1407.3124,", "citeRegEx": "Cichocki.,? \\Q2014\\E", "shortCiteRegEx": "Cichocki.", "year": 2014}, {"title": "Multilayer nonnegative matrix factorisation", "author": ["A. Cichocki", "R. Zdunek"], "venue": "Electronics Letters,", "citeRegEx": "Cichocki and Zdunek.,? \\Q2006\\E", "shortCiteRegEx": "Cichocki and Zdunek.", "year": 2006}, {"title": "Regularized alternating least squares algorithms for non-negative matrix/tensor factorization", "author": ["A. Cichocki", "R. Zdunek"], "venue": "In International Symposium on Neural Networks,", "citeRegEx": "Cichocki and Zdunek.,? \\Q2007\\E", "shortCiteRegEx": "Cichocki and Zdunek.", "year": 2007}, {"title": "Multi-layer neural networks with a local adaptive learning rule for blind separation of source signals", "author": ["A. Cichocki", "W. Kasprzak", "S. Amari"], "venue": "In Proc. Int. Symposium Nonlinear Theory and Applications (NOLTA),", "citeRegEx": "Cichocki et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 1995}, {"title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation", "author": ["A. Cichocki", "R. Zdunek", "A.-H. Phan", "S. Amari"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Generalized alpha-beta divergences and their application to rubust nonnegative matrix factorization", "author": ["A. Cichocki", "S. Cruces", "S. Amari"], "venue": "Entropy, 13:134\u2013170,", "citeRegEx": "Cichocki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2011}, {"title": "Log-determinant divergences revisited: Alpha-beta and gamma log-det", "author": ["A. Cichocki", "S. Cruces", "S. Amari"], "venue": "divergences. Entropy,", "citeRegEx": "Cichocki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2015}, {"title": "Inductive bias of deep convolutional networks through pooling geometry", "author": ["N. Cohen", "A. Shashua"], "venue": "CoRR, abs/1605.06743,", "citeRegEx": "Cohen and Shashua.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Shashua.", "year": 2016}, {"title": "Convolutional rectifier networks as generalized tensor decompositions", "author": ["N. Cohen", "A. Shashua"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Cohen and Shashua.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Shashua.", "year": 2016}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["N. Cohen", "O. Sharir", "A. Shashua"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Blind identification of under-determined mixtures based on the characteristic function", "author": ["P. Comon", "M. Rajih"], "venue": "Signal Processing,", "citeRegEx": "Comon and Rajih.,? \\Q2006\\E", "shortCiteRegEx": "Comon and Rajih.", "year": 2006}, {"title": "A Tensor-Train accelerated solver for integral equations in complex geometries", "author": ["E. Corona", "A. Rahimian", "D. Zorin"], "venue": "arXiv preprint arXiv:1511.06029,", "citeRegEx": "Corona et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Corona et al\\.", "year": 2015}, {"title": "Linear dimensionality reduction: Survey, insights, and generalizations", "author": ["J.P. Cunningham", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cunningham and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Cunningham and Ghahramani.", "year": 2015}, {"title": "Hierarchical Tucker tensor optimization \u2013 Applications to tensor completion", "author": ["C. Da Silva", "F.J. Herrmann"], "venue": "In Proc. 10th International Conference on Sampling Theory and Applications,", "citeRegEx": "Silva and Herrmann.,? \\Q2013\\E", "shortCiteRegEx": "Silva and Herrmann.", "year": 2013}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Stochastic and deterministic tensorization for blind signal separation", "author": ["O. Debals", "L. De Lathauwer"], "venue": "Proceedings of the 12th International Conference Latent Variable Analysis and Signal Separation,", "citeRegEx": "Debals and Lathauwer.,? \\Q2015\\E", "shortCiteRegEx": "Debals and Lathauwer.", "year": 2015}, {"title": "L\u00f6wner-based blind signal separation of rational functions with applications", "author": ["O. Debals", "M. Van Barel", "L. De Lathauwer"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Debals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Debals et al\\.", "year": 2016}, {"title": "Analytical multi-modulus algorithms based on coupled canonical polyadic decompositions", "author": ["O. Debals", "M. Sohail", "L. De Lathauwer"], "venue": "Technical report,", "citeRegEx": "Debals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Debals et al\\.", "year": 2016}, {"title": "TT-GMRES: Solution to a linear system in the structured tensor format", "author": ["S.V. Dolgov"], "venue": "Russian Journal of Numerical Analysis and Mathematical Modelling,", "citeRegEx": "Dolgov.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov.", "year": 2013}, {"title": "Tensor Product Methods in Numerical Simulation of Highdimensional Dynamical Problems", "author": ["S.V. Dolgov"], "venue": "PhD thesis, Faculty of Mathematics and Informatics,", "citeRegEx": "Dolgov.,? \\Q2014\\E", "shortCiteRegEx": "Dolgov.", "year": 2014}, {"title": "Two-level QTT-Tucker format for optimized tensor calculus", "author": ["S.V. Dolgov", "B.N. Khoromskij"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Dolgov and Khoromskij.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov and Khoromskij.", "year": 2013}, {"title": "Simultaneous state-time approximation of the chemical master equation using tensor product formats", "author": ["S.V. Dolgov", "B.N. Khoromskij"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Dolgov and Khoromskij.,? \\Q2015\\E", "shortCiteRegEx": "Dolgov and Khoromskij.", "year": 2015}, {"title": "Alternating minimal energy methods for linear systems in higher dimensions. Part I: SPD systems", "author": ["S.V. Dolgov", "D.V. Savostyanov"], "venue": "arXiv preprint arXiv:1301.6068,", "citeRegEx": "Dolgov and Savostyanov.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov and Savostyanov.", "year": 2013}, {"title": "Alternating minimal energy methods for linear systems in higher dimensions. Part II: Faster algorithm and application to nonsymmetric systems", "author": ["S.V. Dolgov", "D.V. Savostyanov"], "venue": "arXiv preprint arXiv:1304.1222,", "citeRegEx": "Dolgov and Savostyanov.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov and Savostyanov.", "year": 2013}, {"title": "Alternating minimal energy methods for linear systems in higher dimensions", "author": ["S.V. Dolgov", "D.V. Savostyanov"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Dolgov and Savostyanov.,? \\Q2014\\E", "shortCiteRegEx": "Dolgov and Savostyanov.", "year": 2014}, {"title": "Computation of extreme eigenvalues in higher dimensions using block tensor train format", "author": ["S.V. Dolgov", "B.N. Khoromskij", "I.V. Oseledets", "D.V. Savostyanov"], "venue": "Computer Physics Communications,", "citeRegEx": "Dolgov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dolgov et al\\.", "year": 2014}, {"title": "Fast tensor product solvers for optimization problems with fractional differential equations as constraints", "author": ["S.V. Dolgov", "J.W. Pearson", "D.V. Savostyanov", "M. Stoll"], "venue": "Applied Mathematics and Computation,", "citeRegEx": "Dolgov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dolgov et al\\.", "year": 2016}, {"title": "Colloquium: Area laws for the entanglement entropy", "author": ["J. Eisert", "M. Cramer", "M.B. Plenio"], "venue": "Reviews of Modern Physics,", "citeRegEx": "Eisert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eisert et al\\.", "year": 2010}, {"title": "Asymptotic behavior of `p-based Laplacian regularization in semisupervised learning", "author": ["A. El Alaoui", "X. Cheng", "A. Ramdas", "M.J. Wainwright", "M.I. Jordan"], "venue": "arXiv e-prints", "citeRegEx": "Alaoui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alaoui et al\\.", "year": 2016}, {"title": "A new metric for probability distributions", "author": ["D.M. Endres", "J.E. Schindelin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Endres and Schindelin.,? \\Q2003\\E", "shortCiteRegEx": "Endres and Schindelin.", "year": 2003}, {"title": "Algorithms for entanglement renormalization", "author": ["G. Evenbly", "G. Vidal"], "venue": "Physical Review B,", "citeRegEx": "Evenbly and Vidal.,? \\Q2009\\E", "shortCiteRegEx": "Evenbly and Vidal.", "year": 2009}, {"title": "Tensor network renormalization yields the multiscale entanglement renormalization Ansatz", "author": ["G. Evenbly", "G. Vidal"], "venue": "Physical Review Letters,", "citeRegEx": "Evenbly and Vidal.,? \\Q2015\\E", "shortCiteRegEx": "Evenbly and Vidal.", "year": 2015}, {"title": "Entanglement renormalization and wavelets", "author": ["G. Evenbly", "S.R. White"], "venue": "Physical Review Letters,", "citeRegEx": "Evenbly and White.,? \\Q2016\\E", "shortCiteRegEx": "Evenbly and White.", "year": 2016}, {"title": "Representation and design of wavelets using unitary circuits", "author": ["G. Evenbly", "S.R. White"], "venue": "arXiv e-prints,", "citeRegEx": "Evenbly and White.,? \\Q2016\\E", "shortCiteRegEx": "Evenbly and White.", "year": 2016}, {"title": "Nonlinear system modeling and identification using Volterra-PARAFAC models", "author": ["G. Favier", "A.Y. Kibangou", "T. Bouilloc"], "venue": "International Journal of Adaptive Control and Signal Processing,", "citeRegEx": "Favier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Favier et al\\.", "year": 2012}, {"title": "An introduction to restricted Boltzmann machines", "author": ["A. Fischer", "C. Igel"], "venue": "In Iberoamerican Congress on Pattern Recognition,", "citeRegEx": "Fischer and Igel.,? \\Q2012\\E", "shortCiteRegEx": "Fischer and Igel.", "year": 2012}, {"title": "A Mathematical Introduction to Compressive Sensing", "author": ["S. Foucart", "H. Rauhut"], "venue": null, "citeRegEx": "Foucart and Rauhut.,? \\Q2013\\E", "shortCiteRegEx": "Foucart and Rauhut.", "year": 2013}, {"title": "Sparse inverse covariance estimation with the graphical", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Lasso. Biostatistics,", "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Kernel support tensor regression", "author": ["C. Gao", "X.-J. Wu"], "venue": "Procedia Engineering,", "citeRegEx": "Gao and Wu.,? \\Q2012\\E", "shortCiteRegEx": "Gao and Wu.", "year": 2012}, {"title": "Ultimate tensorization: compressing convolutional and FC layers", "author": ["T. Garipov", "D. Podoprikhin", "A. Novikov", "D.P. Vetrov"], "venue": "alike. CoRR,", "citeRegEx": "Garipov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garipov et al\\.", "year": 2016}, {"title": "Constrained optimization with low-rank tensors and applications to parametric problems with PDEs", "author": ["S. Garreis", "M. Ulbrich"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Garreis and Ulbrich.,? \\Q2017\\E", "shortCiteRegEx": "Garreis and Ulbrich.", "year": 2017}, {"title": "Multilinear PageRank", "author": ["D.F. Gleich", "L.-H. Lim", "Y. Yu"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Gleich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gleich et al\\.", "year": 2015}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Hierarchical singular value decomposition of tensors", "author": ["L. Grasedyck"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Grasedyck.,? \\Q2010\\E", "shortCiteRegEx": "Grasedyck.", "year": 2010}, {"title": "A literature survey of low-rank tensor approximation techniques", "author": ["L. Grasedyck", "D. Kessner", "C. Tobler"], "venue": "GAMM-Mitteilungen,", "citeRegEx": "Grasedyck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grasedyck et al\\.", "year": 2013}, {"title": "Variants of alternating least squares tensor completion in the tensor train format", "author": ["L. Grasedyck", "M. Kluge", "S. Kr\u00e4mer"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Grasedyck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grasedyck et al\\.", "year": 2015}, {"title": "Quantum state tomography via compressed sensing", "author": ["D. Gross", "Y.-K. Liu", "S.T. Flammia", "S. Becker", "J. Eisert"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "Gross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gross et al\\.", "year": 2010}, {"title": "A linear support higher-order tensor machine for classification", "author": ["Z. Hao", "L. He", "B. Chen", "X. Yang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Hao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hao et al\\.", "year": 2013}, {"title": "Multilinear tensor regression for longitudinal relational data", "author": ["P.D. Hoff"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Hoff.,? \\Q2015\\E", "shortCiteRegEx": "Hoff.", "year": 2015}, {"title": "The alternating linear scheme for tensor optimization in the tensor train format", "author": ["S. Holtz", "T. Rohwedder", "R. Schneider"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Holtz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Holtz et al\\.", "year": 2012}, {"title": "On manifolds of tensors of fixed TT-rank", "author": ["S. Holtz", "T. Rohwedder", "R. Schneider"], "venue": "Numerische Mathematik,", "citeRegEx": "Holtz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Holtz et al\\.", "year": 2012}, {"title": "Matrix manifold optimization for Gaussian mixtures", "author": ["R. Hosseini", "S. Sra"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hosseini and Sra.,? \\Q2015\\E", "shortCiteRegEx": "Hosseini and Sra.", "year": 2015}, {"title": "Tensor-based Regression Models and Applications", "author": ["M. Hou"], "venue": "PhD thesis,", "citeRegEx": "Hou.,? \\Q2017\\E", "shortCiteRegEx": "Hou.", "year": 2017}, {"title": "Online local Gaussian processes for tensor-variate regression: Application to fast reconstruction of limb movements from brain signal", "author": ["M. Hou", "Y. Wang", "B. Chaib-draa"], "venue": "In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2015}, {"title": "Common and discriminative subspace kernel-based multiblock tensor partial least squares regression", "author": ["M. Hou", "Q. Zhao", "B. Chaib-draa", "A. Cichocki"], "venue": "In Proc. of Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2016}, {"title": "The analysis of two-way functional data using two-way regularized singular value decompositions", "author": ["J.Z. Huang", "H. Shen", "A. Buja"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "A generalized Lanczos method for systematic optimization of tensor network states", "author": ["R.-Z. Huang", "H.-J. Liao", "Z.-Y. Liu", "H.-D. Xie", "Z.-Y. Xie", "H.-H. Zhao", "J. Chen", "T. Xiang"], "venue": "ArXiv e-prints,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Strictly singlesite DMRG algorithm with subspace expansion", "author": ["C. Hubig", "I.P. McCulloch", "U. Schollw\u00f6ck", "F.A. Wolf"], "venue": "Physical Review B,", "citeRegEx": "Hubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hubig et al\\.", "year": 2015}, {"title": "Generic construction of efficient matrix product operators", "author": ["C. Hubig", "I.P. McCulloch", "U. Schollw\u00f6ck"], "venue": "Phys. Rev. B,", "citeRegEx": "Hubig et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hubig et al\\.", "year": 2017}, {"title": "Subspace iteration methods in terms of matrix product states. PAMM", "author": ["T. Huckle", "K. Waldherr"], "venue": null, "citeRegEx": "Huckle and Waldherr.,? \\Q2012\\E", "shortCiteRegEx": "Huckle and Waldherr.", "year": 2012}, {"title": "Doubly decomposing nonparametric tensor regression", "author": ["M. Imaizumi", "K. Hayashi"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Imaizumi and Hayashi.,? \\Q2016\\E", "shortCiteRegEx": "Imaizumi and Hayashi.", "year": 2016}, {"title": "Robust multilinear principal component analysis", "author": ["K. Inoue", "K. Hara", "K. Urahama"], "venue": "In IEEE 12th International Conference on Computer Vision,", "citeRegEx": "Inoue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Inoue et al\\.", "year": 2009}, {"title": "Best low multilinear rank approximation of higher-order tensors, based on the Riemannian trust-region scheme", "author": ["M. Ishteva", "P.A. Absil", "S. Van Huffel", "L. De Lathauwer"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Ishteva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ishteva et al\\.", "year": 2011}, {"title": "Haten2: Billionscale tensor decompositions", "author": ["I. Jeon", "E. Papalexakis", "U. Kang", "C. Faloutsos"], "venue": "In Proceedings of the 31st IEEE International Conference on Data Engineering (ICDE),", "citeRegEx": "Jeon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2015}, {"title": "Mining billionscale tensors: Algorithms and discoveries", "author": ["I. Jeon", "E.E. Papalexakis", "C. Faloutsos", "L. Sael", "U. Kang"], "venue": "The VLDB Journal,", "citeRegEx": "Jeon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2016}, {"title": "A note on the use of principal components in regression", "author": ["I. Jolliffe"], "venue": "Applied Statistics,", "citeRegEx": "Jolliffe.,? \\Q1982\\E", "shortCiteRegEx": "Jolliffe.", "year": 1982}, {"title": "Tensor low-rank and sparse light field photography", "author": ["M.H. Kamal", "B. Heshmat", "R. Raskar", "P. Vandergheynst", "G. Wetzstein"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Kamal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kamal et al\\.", "year": 2016}, {"title": "GigaTensor: Scaling tensor analysis up by 100 times \u2013 algorithms and discoveries", "author": ["U. Kang", "E.E. Papalexakis", "A. Harpale", "C. Faloutsos"], "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "Kang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2012}, {"title": "Uni10: An open-source library for tensor network algorithms", "author": ["Y.-J. Kao", "Y.-D. Hsieh", "P. Chen"], "venue": "In Journal of Physics: Conference Series,", "citeRegEx": "Kao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kao et al\\.", "year": 2015}, {"title": "Parallel algorithms for tensor completion in the CP format", "author": ["L. Karlsson", "D. Kressner", "A. Uschmajew"], "venue": "Parallel Computing,", "citeRegEx": "Karlsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karlsson et al\\.", "year": 2016}, {"title": "Riemannian preconditioning for tensor completion", "author": ["H. Kasai", "B. Mishra"], "venue": "arXiv preprint arXiv:1506.02159,", "citeRegEx": "Kasai and Mishra.,? \\Q2015\\E", "shortCiteRegEx": "Kasai and Mishra.", "year": 2015}, {"title": "Multilevel Toeplitz matrices generated by tensor-structured vectors and convolution with logarithmic complexity", "author": ["V.A. Kazeev", "B.N. Khoromskij", "E.E. Tyrtyshnikov"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Kazeev et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kazeev et al\\.", "year": 2013}, {"title": "Fast tensor method for summation of long-range potentials on 3D lattices with defects", "author": ["V. Khoromskaia", "B.N. Khoromskij"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Khoromskaia and Khoromskij.,? \\Q2016\\E", "shortCiteRegEx": "Khoromskaia and Khoromskij.", "year": 2016}, {"title": "O(d log N)-quantics approximation of N-d tensors in high-dimensional numerical modeling", "author": ["B.N. Khoromskij"], "venue": "Constructive Approximation,", "citeRegEx": "Khoromskij.,? \\Q2011\\E", "shortCiteRegEx": "Khoromskij.", "year": 2011}, {"title": "Tensors-structured numerical methods in scientific computing: Survey on recent advances", "author": ["B.N. Khoromskij"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Khoromskij.,? \\Q2011\\E", "shortCiteRegEx": "Khoromskij.", "year": 2011}, {"title": "Superfast wavelet transform using quanticsTT approximation. I. application to Haar wavelets", "author": ["B.N. Khoromskij", "S. Miao"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Khoromskij and Miao.,? \\Q2014\\E", "shortCiteRegEx": "Khoromskij and Miao.", "year": 2014}, {"title": "Quantics-TT collocation approximation of parameter-dependent and stochastic elliptic PDEs", "author": ["B.N. Khoromskij", "I. Oseledets"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Khoromskij and Oseledets.,? \\Q2010\\E", "shortCiteRegEx": "Khoromskij and Oseledets.", "year": 2010}, {"title": "Tensor-structured Galerkin approximation of parametric and stochastic elliptic PDEs", "author": ["B.N. Khoromskij", "C. Schwab"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Khoromskij and Schwab.,? \\Q2011\\E", "shortCiteRegEx": "Khoromskij and Schwab.", "year": 2011}, {"title": "Efficient computation of highly oscillatory integrals by using QTT tensor approximation", "author": ["B.N. Khoromskij", "A. Veit"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Khoromskij and Veit.,? \\Q2016\\E", "shortCiteRegEx": "Khoromskij and Veit.", "year": 2016}, {"title": "Desingularization of bounded-rank matrix sets", "author": ["V. Khrulkov", "I. Oseledets"], "venue": "arXiv preprint arXiv:1612.03973,", "citeRegEx": "Khrulkov and Oseledets.,? \\Q2016\\E", "shortCiteRegEx": "Khrulkov and Oseledets.", "year": 2016}, {"title": "New robust Lasso method based on ranks", "author": ["H.-J. Kim", "E. Ollila", "V. Koivunen"], "venue": "In 23rd European Signal Processing Conference (EUSIPCO),", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Canonical correlation analysis of video volume tensors for action categorization and detection", "author": ["T.K. Kim", "R. Cipolla"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kim and Cipolla.,? \\Q2009\\E", "shortCiteRegEx": "Kim and Cipolla.", "year": 2009}, {"title": "Tensor canonical correlation analysis for action classification", "author": ["T.K. Kim", "S.F. Wong", "R. Cipolla"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Dynamical low rank approximation", "author": ["O. Koch", "C. Lubich"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Koch and Lubich.,? \\Q2007\\E", "shortCiteRegEx": "Koch and Lubich.", "year": 2007}, {"title": "Dynamical tensor approximation", "author": ["O. Koch", "C. Lubich"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Koch and Lubich.,? \\Q2010\\E", "shortCiteRegEx": "Koch and Lubich.", "year": 2010}, {"title": "Pymanopt: A Python Toolbox for manifold optimization using automatic differentiation", "author": ["N. Koep", "S. Weichwald"], "venue": "arXiv preprint arXiv:1603.03236,", "citeRegEx": "Koep and Weichwald.,? \\Q2016\\E", "shortCiteRegEx": "Koep and Weichwald.", "year": 2016}, {"title": "Trace optimization and eigenproblems in dimension reduction methods", "author": ["E. Kokiopoulou", "J. Chen", "Y. Saad"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Kokiopoulou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kokiopoulou et al\\.", "year": 2011}, {"title": "Convergence analysis of projected fixed-point iteration on a low-rank matrix manifold", "author": ["D. Kolesnikov", "I.V. Oseledets"], "venue": "arXiv preprint arXiv:1604.02111,", "citeRegEx": "Kolesnikov and Oseledets.,? \\Q2016\\E", "shortCiteRegEx": "Kolesnikov and Oseledets.", "year": 2016}, {"title": "Higher rank support tensor machines for visual recognition", "author": ["I. Kotsia", "W. Guo", "I. Patras"], "venue": "Pattern Recognition,", "citeRegEx": "Kotsia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kotsia et al\\.", "year": 2012}, {"title": "Low-rank tensor methods for communicating Markov processes", "author": ["D. Kressner", "F. Macedo"], "venue": "In Quantitative Evaluation of Systems,", "citeRegEx": "Kressner and Macedo.,? \\Q2014\\E", "shortCiteRegEx": "Kressner and Macedo.", "year": 2014}, {"title": "Low-rank tensor Krylov subspace methods for parametrized linear systems", "author": ["D. Kressner", "C. Tobler"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kressner and Tobler.,? \\Q2011\\E", "shortCiteRegEx": "Kressner and Tobler.", "year": 2011}, {"title": "Preconditioned low-rank methods for highdimensional elliptic PDE eigenvalue problems", "author": ["D. Kressner", "C. Tobler"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Kressner and Tobler.,? \\Q2011\\E", "shortCiteRegEx": "Kressner and Tobler.", "year": 2011}, {"title": "Algorithm 941: HTucker\u2013A MATLAB toolbox for tensors in hierarchical Tucker format", "author": ["D. Kressner", "C. Tobler"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Kressner and Tobler.,? \\Q2014\\E", "shortCiteRegEx": "Kressner and Tobler.", "year": 2014}, {"title": "On low-rank approximability of solutions to high-dimensional operator equations and eigenvalue problems", "author": ["D. Kressner", "A. Uschmajew"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Kressner and Uschmajew.,? \\Q2016\\E", "shortCiteRegEx": "Kressner and Uschmajew.", "year": 2016}, {"title": "Low-rank tensor methods with subspace correction for symmetric eigenvalue problems", "author": ["D. Kressner", "M. Steinlechner", "A. Uschmajew"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Kressner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kressner et al\\.", "year": 2014}, {"title": "Low-rank tensor completion by Riemannian optimization", "author": ["D. Kressner", "M. Steinlechner", "B. Vandereycken"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Kressner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kressner et al\\.", "year": 2014}, {"title": "Preconditioned lowrank Riemannian optimization for linear systems with tensor product structure", "author": ["D. Kressner", "M. Steinlechner", "B. Vandereycken"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Kressner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kressner et al\\.", "year": 2016}, {"title": "Volterrafaces: Discriminant analysis using volterra kernels", "author": ["R. Kumar", "A. Banerjee", "B.C. Vemuri"], "venue": "In 2009 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Estimating the parameters of exponentially damped sinusoids and pole-zero modelling in noise", "author": ["R. Kumaresan", "D.W. Tufts"], "venue": "IEEE Trans. Acoust. Speech Signal Processing,", "citeRegEx": "Kumaresan and Tufts.,? \\Q1982\\E", "shortCiteRegEx": "Kumaresan and Tufts.", "year": 1982}, {"title": "Algebraic techniques for the blind deconvolution of constant modulus signals", "author": ["L. De Lathauwer"], "venue": "In The 2004 12th European Signal Processing Conference,", "citeRegEx": "Lathauwer.,? \\Q2004\\E", "shortCiteRegEx": "Lathauwer.", "year": 2004}, {"title": "Fast convolutional neural networks using group-wise brain damage", "author": ["V. Lebedev", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1506.02515,", "citeRegEx": "Lebedev and Lempitsky.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev and Lempitsky.", "year": 2015}, {"title": "Tensor conjugate-gradient-type method for Rayleigh quotient minimization in block QTT-format", "author": ["O.S. Lebedeva"], "venue": "Russian Journal of Numerical Analysis and Mathematical Modelling,", "citeRegEx": "Lebedeva.,? \\Q2011\\E", "shortCiteRegEx": "Lebedeva.", "year": 2011}, {"title": "Discriminant analysis for multiway data", "author": ["G. Lechuga", "L. Le Brusquet", "V. Perlbarg", "L. Puybasset", "D. Galanaud", "A. Tenenhaus"], "venue": "Springer Proceedings in Mathematics and Statistics,", "citeRegEx": "Lechuga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lechuga et al\\.", "year": 2015}, {"title": "Biclustering via sparse singular value decomposition", "author": ["M. Lee", "H. Shen", "J.Z. Huang", "J.S. Marron"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Estimating a few extreme singular values and vectors for large-scale matrices in Tensor Train format", "author": ["N. Lee", "A. Cichocki"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lee and Cichocki.,? \\Q2015\\E", "shortCiteRegEx": "Lee and Cichocki.", "year": 2015}, {"title": "Tensor train decompositions for higher-order regression with LASSO penalties", "author": ["N. Lee", "A. Cichocki"], "venue": "In Workshop on Tensor Decompositions and Applications (TDA2016),", "citeRegEx": "Lee and Cichocki.,? \\Q2016\\E", "shortCiteRegEx": "Lee and Cichocki.", "year": 2016}, {"title": "Regularized computation of approximate pseudoinverse of large matrices using low-rank tensor train decompositions", "author": ["N. Lee", "A. Cichocki"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lee and Cichocki.,? \\Q2016\\E", "shortCiteRegEx": "Lee and Cichocki.", "year": 2016}, {"title": "Multilinear discriminant analysis for higher-order tensor data classification", "author": ["Q. Li", "D. Schonfeld"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Li and Schonfeld.,? \\Q2014\\E", "shortCiteRegEx": "Li and Schonfeld.", "year": 2014}, {"title": "Tucker tensor regression and neuroimaging analysis", "author": ["X. Li", "H. Zhou", "L. Li"], "venue": "arXiv preprint arXiv:1304.5637,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Experimental realization of a quantum support vector machine", "author": ["Z. Li", "X. Liu", "N. Xu", "J. Du"], "venue": "Physical Review Letters,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Tensor methods for parameter estimation and bifurcation analysis of stochastic reaction networks", "author": ["S. Liao", "T. Vejchodsk\u00fd", "R. Erban"], "venue": "Journal of the Royal Society Interface,", "citeRegEx": "Liao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2015}, {"title": "Why does deep and cheap learning work so well", "author": ["H.W. Lin", "M. Tegmark"], "venue": "ArXiv e-prints,", "citeRegEx": "Lin and Tegmark.,? \\Q2016\\E", "shortCiteRegEx": "Lin and Tegmark.", "year": 2016}, {"title": "A low-rank approach to the computation of path integrals", "author": ["M.S. Litsarev", "I.V. Oseledets"], "venue": "Journal of Computational Physics,", "citeRegEx": "Litsarev and Oseledets.,? \\Q2016\\E", "shortCiteRegEx": "Litsarev and Oseledets.", "year": 2016}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Dynamical approximation of hierarchical Tucker and tensor-train tensors", "author": ["C. Lubich", "T. Rohwedder", "R. Schneider", "B. Vandereycken"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lubich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lubich et al\\.", "year": 2013}, {"title": "Time integration of tensor trains", "author": ["C. Lubich", "I.V. Oseledets", "B. Vandereycken"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Lubich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lubich et al\\.", "year": 2015}, {"title": "A projector-splitting integrator for dynamical low-rank approximation", "author": ["C. Lubich", "I.V. Oseledets"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Lubich and Oseledets.,? \\Q2014\\E", "shortCiteRegEx": "Lubich and Oseledets.", "year": 2014}, {"title": "Support matrix machines", "author": ["L. Luo", "Y. Xie", "Z. Zhang", "W.-J. Li"], "venue": "In The International Conference on Machine Learning (ICML),", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Tensor canonical correlation analysis for multi-view dimension reduction", "author": ["Y. Luo", "D. Tao", "K. Ramamohanarao", "C. Xu", "Y. Wen"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Computing inner eigenvalues of matrices in tensor train matrix format", "author": ["T. Mach"], "venue": "In Numerical Mathematics and Advanced Applications", "citeRegEx": "Mach.,? \\Q2011\\E", "shortCiteRegEx": "Mach.", "year": 2011}, {"title": "Wave-packet dynamics within the multiconfiguration Hartree framework: General aspects and application to NOCl", "author": ["U. Manthe", "H.-D. Meyer", "L.S. Cederbaum"], "venue": "Journal of Chemical Physics,", "citeRegEx": "Manthe et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Manthe et al\\.", "year": 1992}, {"title": "Analytic optimization of a MERA network and its relevance to quantum integrability and wavelet", "author": ["H. Matsueda"], "venue": "arXiv preprint arXiv:1608.02205,", "citeRegEx": "Matsueda.,? \\Q2016\\E", "shortCiteRegEx": "Matsueda.", "year": 2016}, {"title": "Deep vs. shallow networks: An approximation theory perspective", "author": ["H. Mhaskar", "T. Poggio"], "venue": "Analysis and Applications,", "citeRegEx": "Mhaskar and Poggio.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar and Poggio.", "year": 2016}, {"title": "A Kullback-Leibler divergence based kernel for SVM classification in multimedia applications", "author": ["P.J. Moreno", "P. Ho", "N. Vasconcelos"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Moreno et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Moreno et al\\.", "year": 2003}, {"title": "Tensor-variate Restricted Boltzmann Machines", "author": ["T.D. Nguyen", "T. Tran", "D.Q. Phung", "S. Venkatesh"], "venue": "In AAAI,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Regularized group regression methods for genomic prediction: Bridge, MCP, SCAD, Group Bridge, Group Lasso, Sparse Group Lasso, Group MCP and Group SCAD", "author": ["J. Ogutu", "H. Piepho"], "venue": "In BMC Proceedings,", "citeRegEx": "Ogutu and Piepho.,? \\Q2014\\E", "shortCiteRegEx": "Ogutu and Piepho.", "year": 2014}, {"title": "A practical introduction to tensor networks: Matrix product states and projected entangled pair states", "author": ["R. Or\u00fas"], "venue": "Annals of Physics,", "citeRegEx": "Or\u00fas.,? \\Q2014\\E", "shortCiteRegEx": "Or\u00fas.", "year": 2014}, {"title": "Infinite time-evolving block decimation algorithm beyond unitary evolution", "author": ["R. Or\u00fas", "G. Vidal"], "venue": "Physical Review B,", "citeRegEx": "Or\u00fas and Vidal.,? \\Q2008\\E", "shortCiteRegEx": "Or\u00fas and Vidal.", "year": 2008}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets.", "year": 2011}, {"title": "Constructive representation of functions in low-rank tensor formats", "author": ["I.V. Oseledets"], "venue": "Constructive Approximation,", "citeRegEx": "Oseledets.,? \\Q2012\\E", "shortCiteRegEx": "Oseledets.", "year": 2012}, {"title": "Solution of linear systems and matrix inversion in the TT-format", "author": ["I.V. Oseledets", "S.V. Dolgov"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets and Dolgov.,? \\Q2012\\E", "shortCiteRegEx": "Oseledets and Dolgov.", "year": 2012}, {"title": "Tucker dimensionality reduction of three-dimensional arrays in linear time", "author": ["I.V. Oseledets", "D.V. Savostianov", "E.E. Tyrtyshnikov"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Oseledets et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oseledets et al\\.", "year": 2008}, {"title": "URL https:// github.com/oseledets/TT-Toolbox", "author": ["I.V. Oseledets", "S.V. Dolgov", "V.A. Kazeev", "D. Savostyanov", "O. Lebedeva", "P. Zhlobich", "T. Mach", "L. Song"], "venue": "TT-Toolbox,", "citeRegEx": "Oseledets et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Oseledets et al\\.", "year": 2012}, {"title": "Thermodynamic limit of density matrix renormalization", "author": ["S. \u00d6stlund", "S. Rommer"], "venue": "Physical Review Letters,", "citeRegEx": "\u00d6stlund and Rommer.,? \\Q1995\\E", "shortCiteRegEx": "\u00d6stlund and Rommer.", "year": 1995}, {"title": "Exponential data fitting using multilinear algebra: the single-channel and multi-channel case", "author": ["J.M. Papy", "L. De Lathauwer", "S. Van Huffel"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Papy et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Papy et al\\.", "year": 2005}, {"title": "Tensor networks for latent variable analysis. Part I: Algorithms for tensor train decomposition", "author": ["A.-H. Phan", "A. Cichocki", "A. Uschmajew", "P. Tichavsky", "G. Luta", "D. Mandic"], "venue": "ArXiv e-prints,", "citeRegEx": "Phan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Phan et al\\.", "year": 2016}, {"title": "Tensor networks for latent variable analysis. Part 2: Blind source separation and hamonic retrieval, submitted to ArXiv e-prints, 2017", "author": ["A.-H. Phan", "A. Cichocki", "A. Uschmajew", "P. Tichavsk\u00fd", "G. Luta", "D.P. Mandic"], "venue": null, "citeRegEx": "Phan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Phan et al\\.", "year": 2017}, {"title": "Tensor decompositions for feature extraction and classification of high dimensional datasets", "author": ["A.H. Phan", "A. Cichocki"], "venue": "Nonlinear Theory and its Applications, IEICE,", "citeRegEx": "Phan and Cichocki.,? \\Q2010\\E", "shortCiteRegEx": "Phan and Cichocki.", "year": 2010}, {"title": "TENSORBOX: MATLAB package for tensor decomposition", "author": ["A.H. Phan", "P. Tichavsk\u00fd", "A. Cichocki"], "venue": "arXiv preprint arXiv:1601.01083,", "citeRegEx": "Phan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Phan et al\\.", "year": 2012}, {"title": "Variational numerical renormalization group: Bridging the gap between NRG and density matrix renormalization group", "author": ["I. Pi\u017eorn", "F. Verstraete"], "venue": "Physical Review Letters,", "citeRegEx": "Pi\u017eorn and Verstraete.,? \\Q2012\\E", "shortCiteRegEx": "Pi\u017eorn and Verstraete.", "year": 2012}, {"title": "Why and when can deep\u2013but not shallow\u2013networks avoid the curse of dimensionality: A review", "author": ["T. Poggio", "H. Mhaskar", "L. Rosasco", "B. Miranda", "Q.L. Liao"], "venue": "arXiv preprint arXiv:1611.00740,", "citeRegEx": "Poggio et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2016}, {"title": "Matrix variate RBM and its applications", "author": ["G. Qi", "Y. Sun", "J. Gao", "Y. Hu", "J. Li"], "venue": "arXiv preprint arXiv:1601.00722,", "citeRegEx": "Qi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2016}, {"title": "Hankel tensors: Associated Hankel matrices and Vandermonde decomposition", "author": ["L. Qi"], "venue": "Commun Math Sci,", "citeRegEx": "Qi.,? \\Q2015\\E", "shortCiteRegEx": "Qi.", "year": 2015}, {"title": "Higher-order low-rank regression", "author": ["G. Rabusseau", "H. Kadri"], "venue": "arXiv preprint arXiv:1602.06863,", "citeRegEx": "Rabusseau and Kadri.,? \\Q2016\\E", "shortCiteRegEx": "Rabusseau and Kadri.", "year": 2016}, {"title": "Convex regularization for high-dimensional tensor regression", "author": ["G. Raskutti", "M. Yuan"], "venue": "arXiv preprint arXiv:1512.01215,", "citeRegEx": "Raskutti and Yuan.,? \\Q2015\\E", "shortCiteRegEx": "Raskutti and Yuan.", "year": 2015}, {"title": "Gaussian Processes for Machine Learning, volume 1", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Low rank tensor recovery via iterative hard thresholding", "author": ["H. Rauhut", "R. Schneider", "Z. Stojanac"], "venue": "arXiv preprint arXiv:1602.05217,", "citeRegEx": "Rauhut et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rauhut et al\\.", "year": 2016}, {"title": "Quantum support vector machine for big data classification", "author": ["P. Rebentrost", "M. Mohseni", "S. Lloyd"], "venue": "Physical Review letters,", "citeRegEx": "Rebentrost et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rebentrost et al\\.", "year": 2014}, {"title": "Nested expectation propagation for Gaussian process classification with a multinomial probit likelihood", "author": ["J. Riihim\u00e4ki", "P. Jyl\u00e4nki", "A. Vehtari"], "venue": "arXiv preprint arXiv:1207.3649,", "citeRegEx": "Riihim\u00e4ki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Riihim\u00e4ki et al\\.", "year": 2012}, {"title": "Kernel partial least squares regression in reproducing kernel Hilbert space", "author": ["R. Rosipal", "L.J. Trejo"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Rosipal and Trejo.,? \\Q2002\\E", "shortCiteRegEx": "Rosipal and Trejo.", "year": 2002}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of The 12th International Conference on Artificial Intelligence and Statistics (AISTATS\u201909),", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Riemannian stochastic variance reduced gradient", "author": ["H. Sato", "H. Kasai", "B. Mishra"], "venue": "arXiv preprint arXiv:1702.05594,", "citeRegEx": "Sato et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sato et al\\.", "year": 2017}, {"title": "Exact NMR simulation of protein-size spin systems using tensor train formalism", "author": ["D.V. Savostyanov", "S.V. Dolgov", "J.M. Werner", "I. Kuprov"], "venue": "Physical Review B,", "citeRegEx": "Savostyanov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Savostyanov et al\\.", "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Deeper and cheaper machine learning [top tech 2017", "author": ["D. Schneider"], "venue": "IEEE Spectrum,", "citeRegEx": "Schneider.,? \\Q2017\\E", "shortCiteRegEx": "Schneider.", "year": 2017}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "The density-matrix renormalization group in the age of matrix product states", "author": ["U. Schollw\u00f6ck"], "venue": "Annals of Physics,", "citeRegEx": "Schollw\u00f6ck.,? \\Q2011\\E", "shortCiteRegEx": "Schollw\u00f6ck.", "year": 2011}, {"title": "Matrix product state algorithms: DMRG, TEBD and relatives", "author": ["U. Schollw\u00f6ck"], "venue": "In Strongly Correlated Systems,", "citeRegEx": "Schollw\u00f6ck.,? \\Q2013\\E", "shortCiteRegEx": "Schollw\u00f6ck.", "year": 2013}, {"title": "An introduction to quantum machine learning", "author": ["M. Schuld", "I. Sinayskiy", "F. Petruccione"], "venue": "Contemporary Physics,", "citeRegEx": "Schuld et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schuld et al\\.", "year": 2015}, {"title": "Higher-order Boltzmann machines", "author": ["T.J. Sejnowski"], "venue": "In AIP Conference Proceedings 151 on Neural Networks for Computing,", "citeRegEx": "Sejnowski.,? \\Q1987\\E", "shortCiteRegEx": "Sejnowski.", "year": 1987}, {"title": "Online learning in the manifold of low-rank matrices", "author": ["U. Shalit", "D. Weinshall", "G. Chechik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shalit et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalit et al\\.", "year": 2010}, {"title": "Tensorial mixture models", "author": ["O. Sharir", "R. Tamari", "N. Cohen", "A. Shashua"], "venue": "CoRR, abs/1610.04167,", "citeRegEx": "Sharir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharir et al\\.", "year": 2016}, {"title": "Higher order orthogonal iteration of tensors (HOOI) and its relation to PCA and GLRAM", "author": ["B. Sheehan", "Y. Saad"], "venue": "In Proceedings of the 2007 SIAM International Conference on Data Mining,", "citeRegEx": "Sheehan and Saad.,? \\Q2007\\E", "shortCiteRegEx": "Sheehan and Saad.", "year": 2007}, {"title": "Fully scalable methods for distributed tensor factorization", "author": ["K. Shin", "L. Sael", "U. Kang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Shin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2017}, {"title": "A kernel-based framework to tensorial data analysis", "author": ["M. Signoretto", "L. De Lathauwer", "J.A.K. Suykens"], "venue": "Neural Networks,", "citeRegEx": "Signoretto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2011}, {"title": "Classification of multichannel signals with cumulant-based kernels", "author": ["M. Signoretto", "E. Olivetti", "L. De Lathauwer", "J.A.K. Suykens"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Signoretto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2012}, {"title": "Support vector regression machines", "author": ["A. Smola", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Smola and Vapnik.,? \\Q1997\\E", "shortCiteRegEx": "Smola and Vapnik.", "year": 1997}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "In Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Riemannian Optimization for Solving High-Dimensional Problems with Low-Rank Tensor Structure", "author": ["M. Steinlechner"], "venue": "PhD thesis, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne,", "citeRegEx": "Steinlechner.,? \\Q2016\\E", "shortCiteRegEx": "Steinlechner.", "year": 2016}, {"title": "Riemannian optimization for high-dimensional tensor completion", "author": ["M. Steinlechner"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Steinlechner.,? \\Q2016\\E", "shortCiteRegEx": "Steinlechner.", "year": 2016}, {"title": "Supervised learning with quantuminspired tensor networks", "author": ["E.M. Stoudenmire", "D.J. Schwab"], "venue": "arXiv preprint arXiv:1605.05775,", "citeRegEx": "Stoudenmire and Schwab.,? \\Q2016\\E", "shortCiteRegEx": "Stoudenmire and Schwab.", "year": 2016}, {"title": "ITensor Library Release v0.2.5", "author": ["E.M. Stoudenmire", "S.R. White"], "venue": "Technical report,", "citeRegEx": "Stoudenmire and White.,? \\Q2014\\E", "shortCiteRegEx": "Stoudenmire and White.", "year": 2014}, {"title": "Sparse low-rank tensor response regression", "author": ["W.W. Sun", "L. Li"], "venue": "arXiv preprint arXiv:1609.04523,", "citeRegEx": "Sun and Li.,? \\Q2016\\E", "shortCiteRegEx": "Sun and Li.", "year": 2016}, {"title": "Heterogeneous tensor decomposition for clustering via manifold optimization", "author": ["Y. Sun", "J. Gao", "X. Hong", "B. Mishra", "B. Yin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Least squares support vector machine classifiers", "author": ["J.A.K. Suykens", "J. Vandewalle"], "venue": "Neural Processing Letters,", "citeRegEx": "Suykens and Vandewalle.,? \\Q1999\\E", "shortCiteRegEx": "Suykens and Vandewalle.", "year": 1999}, {"title": "Matching pursuit Lasso Part I and II: Sparse recovery over big dictionary", "author": ["M. Tan", "I. Tsang", "L. Wang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "Normalized iterative hard thresholding for matrix completion", "author": ["J. Tanner", "K. Wei"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Tanner and Wei.,? \\Q2013\\E", "shortCiteRegEx": "Tanner and Wei.", "year": 2013}, {"title": "Supervised tensor learning", "author": ["D. Tao", "X. Li", "W. Hu", "S. Maybank", "X. Wu"], "venue": "In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM\u201905),", "citeRegEx": "Tao et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tao et al\\.", "year": 2005}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Multikernel least mean squares algorithm", "author": ["F.A. Tobar", "S.-Y. Kung", "D. P Mandic"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Tobar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tobar et al\\.", "year": 2014}, {"title": "Convex tensor decomposition via structured Schatten norm regularization", "author": ["R. Tomioka", "T. Suzuki"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Tomioka and Suzuki.,? \\Q2013\\E", "shortCiteRegEx": "Tomioka and Suzuki.", "year": 2013}, {"title": "The geometry of algorithms using hierarchical tensors", "author": ["A. Uschmajew", "B. Vandereycken"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Uschmajew and Vandereycken.,? \\Q2013\\E", "shortCiteRegEx": "Uschmajew and Vandereycken.", "year": 2013}, {"title": "Statistical Learning Theory, volume 1", "author": ["V.N. Vapnik", "V. Vapnik"], "venue": null, "citeRegEx": "Vapnik and Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Vapnik.", "year": 1998}, {"title": "A randomized block sampling approach to Canonical Polyadic decomposition of large-scale tensors", "author": ["N. Vervliet", "L. De Lathauwer"], "venue": "IEEE Transactions on Selected Topics Signal Processing,", "citeRegEx": "Vervliet and Lathauwer.,? \\Q2016\\E", "shortCiteRegEx": "Vervliet and Lathauwer.", "year": 2016}, {"title": "Efficient classical simulation of slightly entangled quantum computations", "author": ["G. Vidal"], "venue": "Physical Review Letters,", "citeRegEx": "Vidal.,? \\Q2003\\E", "shortCiteRegEx": "Vidal.", "year": 2003}, {"title": "Class of quantum many-body states that can be efficiently simulated", "author": ["G. Vidal"], "venue": "Physical Review Letters,", "citeRegEx": "Vidal.,? \\Q2008\\E", "shortCiteRegEx": "Vidal.", "year": 2008}, {"title": "Transfer-matrix density-matrix renormalizationgroup theory for thermodynamics of one-dimensional quantum systems", "author": ["X. Wang", "T. Xiang"], "venue": "Physical Review B,", "citeRegEx": "Wang and Xiang.,? \\Q1997\\E", "shortCiteRegEx": "Wang and Xiang.", "year": 1997}, {"title": "Fast and guaranteed tensor decomposition via sketching", "author": ["Y. Wang", "H.-Y. Tung", "A. Smola", "A. Anandkumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Tensor displays: Compressive light field synthesis using multilayer displays with directional backlighting", "author": ["G. Wetzstein", "D. Lanman", "M. Hirsch", "R. Raskar"], "venue": "ACM Transaction on Graphics,", "citeRegEx": "Wetzstein et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wetzstein et al\\.", "year": 2012}, {"title": "Density matrix formulation for quantum renormalization groups", "author": ["S.R. White"], "venue": "Physical Review Letters,", "citeRegEx": "White.,? \\Q1992\\E", "shortCiteRegEx": "White.", "year": 1992}, {"title": "Density matrix renormalization group algorithms with a single center site", "author": ["S.R. White"], "venue": "Physical Review B,", "citeRegEx": "White.,? \\Q2005\\E", "shortCiteRegEx": "White.", "year": 2005}, {"title": "Multitask learning meets tensor factorization: Task imputation via convex optimization", "author": ["K. Wimalawarne", "M. Sugiyama", "R. Tomioka"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wimalawarne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2014}, {"title": "Theoretical and experimental analyses of tensor-based regression and classification", "author": ["K. Wimalawarne", "R. Tomioka", "M. Sugiyama"], "venue": "Neural Computation,", "citeRegEx": "Wimalawarne et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2016}, {"title": "A Penalized Matrix Decomposition, and its Applications", "author": ["D.M. Witten"], "venue": "PhD dissertation,", "citeRegEx": "Witten.,? \\Q2010\\E", "shortCiteRegEx": "Witten.", "year": 2010}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2009}, {"title": "General tensor spectral co-clustering for higher-order data", "author": ["T. Wu", "A.R. Benson", "D.F. Gleich"], "venue": "arXiv preprint arXiv:1603.00395,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Density-matrix renormalization", "author": ["T. Xiang", "X. Wang"], "venue": "Lecture Notes in Physics,", "citeRegEx": "Xiang and Wang.,? \\Q1999\\E", "shortCiteRegEx": "Xiang and Wang.", "year": 1999}, {"title": "Deep multi-task representation learning: A tensor factorisation approach", "author": ["Y. Yang", "T. Hospedales"], "venue": "arXiv preprint arXiv:1605.06391,", "citeRegEx": "Yang and Hospedales.,? \\Q2016\\E", "shortCiteRegEx": "Yang and Hospedales.", "year": 2016}, {"title": "Blind source separation via the second characteristic function", "author": ["A. Yeredor"], "venue": "Signal Processing,", "citeRegEx": "Yeredor.,? \\Q2000\\E", "shortCiteRegEx": "Yeredor.", "year": 2000}, {"title": "Smooth PARAFAC decomposition for tensor completion", "author": ["T. Yokota", "Q. Zhao", "A. Cichocki"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Yokota et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yokota et al\\.", "year": 2016}, {"title": "Learning from multiway data: Simple and efficient tensor regression", "author": ["R. Yu", "E.Y. Liu", "U.S.C. Edu"], "venue": "Proceedings of the 33rd International Conference on Mahince Learning (ICML),", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Low-rank tensor constrained multiview subspace clustering", "author": ["C. Zhang", "H. Fu", "S. Liu", "G. Liu", "X. Cao"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Subspace methods with local refinements for eigenvalue computation using low-rank tensor-train format", "author": ["J. Zhang", "Z. Wen", "Y. Zhang"], "venue": "Journal of Scientific Computing,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Enabling high-dimensional hierarchical uncertainty quantification by ANOVA and tensor-train decomposition", "author": ["Z. Zhang", "X. Yang", "I.V. Oseledets", "G.E. Karniadakis", "L. Daniel"], "venue": "IEEE Transactions on ComputerAided Design of Integrated Circuits and Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Multilinear subspace regression: An orthogonal tensor decomposition approach", "author": ["Q. Zhao", "C.F. Caiafa", "D.P. Mandic", "L. Zhang", "T. Ball", "A. Schulze-Bonhage", "A. Cichocki"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Higher order partial least squares (HOPLS): A generalized multilinear regression method", "author": ["Q. Zhao", "C. Caiafa", "D.P. Mandic", "Z.C. Chao", "Y. Nagasaka", "N. Fujii", "L. Zhang", "A. Cichocki"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "A tensor-variate Gaussian process for classification of multidimensional structured data", "author": ["Q. Zhao", "L. Zhang", "A. Cichocki"], "venue": "In Proceedings of the Twenty-Seven AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Kernelization of tensor-based models for multiway data analysis: Processing of multidimensional structured data", "author": ["Q. Zhao", "G. Zhou", "T. Adali", "L. Zhang", "A. Cichocki"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Tensor-variate Gaussian processes regression and its application to video surveillance", "author": ["Q. Zhao", "G. Zhou", "L. Zhang", "A. Cichocki"], "venue": "In Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Bayesian CP factorization of incomplete tensors with automatic rank determination", "author": ["Q. Zhao", "L. Zhang", "A. Cichocki"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Bayesian robust tensor factorization for incomplete multiway data", "author": ["Q. Zhao", "G. Zhou", "L. Zhang", "A. Cichocki", "S.I. Amari"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Least squares twin support tensor machine for classification", "author": ["X.B. Zhao", "H.F. Shi", "M. Lv", "L. Jing"], "venue": "Journal of Information & Computational Science,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "DinTucker: Scaling up Gaussian process models on large multidimensional arrays", "author": ["S. Zhe", "Y. Qi", "Y. Park", "Z. Xu", "I. Molloy", "S. Chari"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhe et al\\.", "year": 2016}, {"title": "Distributed flexible nonlinear tensor factorization", "author": ["S. Zhe", "P. Wang", "K.-C. Lee", "Z. Xu", "J. Yang", "Y. Park", "Y. Qi"], "venue": "arXiv preprint arXiv:1604.07928,", "citeRegEx": "Zhe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhe et al\\.", "year": 2016}, {"title": "TDALAB: Tensor Decomposition Laboratory", "author": ["G. Zhou", "A. Cichocki"], "venue": "http://bsp.brain.riken.jp/TDALAB/,", "citeRegEx": "Zhou and Cichocki.,? \\Q2013\\E", "shortCiteRegEx": "Zhou and Cichocki.", "year": 2013}, {"title": "Tensor regression with applications in neuroimaging data analysis", "author": ["H. Zhou", "L. Li", "H. Zhu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Riemannian tensor completion with side information", "author": ["T. Zhou", "H. Qian", "Z. Shen", "C. Xu"], "venue": "arXiv preprint arXiv:1611.03993,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Fundamental modular region, Boltzmann factor and area law in lattice theory", "author": ["D. Zwanziger"], "venue": "Nuclear Physics B,", "citeRegEx": "Zwanziger.,? \\Q1994\\E", "shortCiteRegEx": "Zwanziger.", "year": 1994}], "referenceMentions": [{"referenceID": 48, "context": "Cichocki et al. Please make reference to: A. Cichocki, A.H. Phan, Q. Zhao, N. Lee, I. Oseledets, and D.P. Mandic (2017), \u201cTensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 2 Applications and Future perspectives\u201d, Foundations and Trends in Machine Learning: Vol.", "startOffset": 0, "endOffset": 120}, {"referenceID": 185, "context": "[Papy et al., 2005] An Nth-order Hankel tensor of size I1 \u02c6 I2 \u02c6  \u0308  \u0308  \u0308 \u02c6 IN , which is represented by Y = HI1,.", "startOffset": 0, "endOffset": 19}, {"referenceID": 121, "context": "The convolution tensor can then be represented in a QTT format of rank-2 [Kazeev et al., 2013] with core tensors C(2) =  \u0308  \u0308  \u0308 = C(D) = S, C(1) = S(1, :, :, :, :), and the last core tensor C(D+1) = [ 0 1 1 0 ] which is of size 2\u02c6 1\u02c6 1\u02c6 2\u02c6 1.", "startOffset": 73, "endOffset": 94}, {"referenceID": 193, "context": "Moreover, the tensor Y also admits a symmetric CP decomposition with Vandermonde structured factor matrix [Qi, 2015] Y = diagN(\u03bb)\u02c61 V T \u02c62 VT  \u0308  \u0308  \u0308 \u02c6N VT , (1.", "startOffset": 106, "endOffset": 116}, {"referenceID": 41, "context": "12)), the Vandermonde decomposition of the Hankel tensor Y becomes a Vandermonde factorization of y [Chen, 2016], given by", "startOffset": 100, "endOffset": 112}, {"referenceID": 249, "context": "In practice, the GCF of the observation and its derivatives are unknown, but can be estimated from the sample first GCF [Yeredor, 2000].", "startOffset": 120, "endOffset": 135}, {"referenceID": 147, "context": "For this BSS problem for single constant modulus signals, Lathauwer [2004] linked the problem to CP decomposition of a fourth-order tensor.", "startOffset": 58, "endOffset": 75}, {"referenceID": 66, "context": "For multi-constant modulus signals, Debals et al. [2016b] established a link to a coupled CP decomposition.", "startOffset": 36, "endOffset": 58}, {"referenceID": 186, "context": ", an application of image denoising [Phan et al., 2016].", "startOffset": 36, "endOffset": 55}, {"referenceID": 43, "context": ", xN , the MPR can be written as a tensor-vector product as [Chen and Billings, 1989]", "startOffset": 60, "endOffset": 85}, {"referenceID": 44, "context": "For example, the weight tensor W can be constrained to be in low rank TT-format [Chen et al., 2016].", "startOffset": 80, "endOffset": 99}, {"referenceID": 221, "context": ", 2016], the TT/MPS tensor format [Stoudenmire and Schwab, 2016], or the hierarchical Tucker tensor format [Cohen and Shashua, 2016].", "startOffset": 34, "endOffset": 64}, {"referenceID": 57, "context": ", 2016], the TT/MPS tensor format [Stoudenmire and Schwab, 2016], or the hierarchical Tucker tensor format [Cohen and Shashua, 2016].", "startOffset": 107, "endOffset": 132}, {"referenceID": 23, "context": "1 Discrete Volterra Model System identification is a paradigm which aims to provide a mathematical description of a system from the observed system inputs and outputs [Billings, 2013].", "startOffset": 167, "endOffset": 183}, {"referenceID": 84, "context": "The first and simplest separable Volterra model, proposed in [Favier et al., 2012], represents the kernels by symmetric tensors of rank Rn in the CP format, that is H(n) = I\u02c61 An \u02c62 An  \u0308  \u0308  \u0308 \u02c6n An .", "startOffset": 61, "endOffset": 82}, {"referenceID": 147, "context": "3 Volterra-based Tensorization for Nonlinear Feature Extraction Consider nonlinear feature extraction in a supervised learning system, such that the extracted features maximize the Fisher score [Kumar et al., 2009].", "startOffset": 194, "endOffset": 214}, {"referenceID": 147, "context": "To this end, Kumar et al. [2009] suggested to split the data into small patches.", "startOffset": 13, "endOffset": 33}, {"referenceID": 186, "context": ", 2), that is, through the minimization [Phan et al., 2016] min }Y \u0301 X1  \u0301 X2  \u0301  \u0308  \u0308  \u0308  \u0301 XP}F .", "startOffset": 40, "endOffset": 59}, {"referenceID": 148, "context": ", singular value decomposition of the Hankel-type matrix as in the Kumaresan-Tufts (KT) method [Kumaresan and Tufts, 1982].", "startOffset": 95, "endOffset": 122}, {"referenceID": 187, "context": "For a detailed derivation of these algorithms, see [Phan et al., 2017].", "startOffset": 51, "endOffset": 70}, {"referenceID": 115, "context": ", principal component regression (PCR) [Jolliffe, 1982], whereby regression is performed on a well-posed low-dimensional subspace defined through most significant principal components.", "startOffset": 39, "endOffset": 55}, {"referenceID": 217, "context": "A well established and important supervised learning technique is linear or nonlinear Support Vector Regression (SVR) [Smola and Vapnik, 1997], which allows for the modeling of streaming data and is quite closely related to Support Vector Machines (SVM) [Cortes and Vapnik, 1995].", "startOffset": 118, "endOffset": 142}, {"referenceID": 228, "context": "Standard support vector regression techniques have been naturally extended to Tensor Regression (TR) or Support Tensor Machine (STM) methods [Tao et al., 2005].", "startOffset": 141, "endOffset": 159}, {"referenceID": 266, "context": "5) where \u201c \u030b\u201d denotes the outer product of vectors, leads to a generalized linear model (GLM), called the CP tensor regression [Zhou et al., 2013].", "startOffset": 127, "endOffset": 146}, {"referenceID": 98, "context": "6) we obtain Tucker tensor regression [Hoff, 2015, Li et al., 2013, Yu et al., 2016]. An alternative form of the multilinear Tucker regression model, proposed by Hoff [2015], assumes that the replicated observations tXm, Ymu m=1 are stacked in concatenated tensors X P RI1\u02c6 \u0308 \u0308 \u0308\u02c6IN\u02c6M and Y P RJ1\u02c6 \u0308 \u0308 \u0308\u02c6JN\u02c6M, which admit the following model Y = X\u02c61 W1 \u02c62 W2  \u0308  \u0308  \u0308 \u02c6N WN \u02c6N+1 DM + E, (2.", "startOffset": 39, "endOffset": 174}, {"referenceID": 102, "context": "2) is represented by a low-rank HT decomposition, this is referred to as the H-Tucker tensor regression [Hou, 2017].", "startOffset": 104, "endOffset": 115}, {"referenceID": 195, "context": "8), including the multi-response regression, vector autoregressive model and pair-wise interaction tensor model (see [Raskutti and Yuan, 2015] and references therein).", "startOffset": 117, "endOffset": 142}, {"referenceID": 111, "context": ", 2015], multilinear robust principal component analysis [Inoue et al., 2009], and subspace clustering [Zhang et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 251, "context": "properties of data itself, low-rank regularization can be also applied to learning coefficients in regression and classification [Yu et al., 2016].", "startOffset": 129, "endOffset": 146}, {"referenceID": 243, "context": "The low-rank constraint for the tensor W can also be formulated through the tensor norm, in the form [Wimalawarne et al., 2016] min W,b (J(X, y | W, b) + \u03bb}W}), (2.", "startOffset": 101, "endOffset": 127}, {"referenceID": 163, "context": "One of the important and useful tensor norms is the tensor nuclear norm [Liu et al., 2013] or the (overlapped) trace norm [Wimalawarne et al.", "startOffset": 72, "endOffset": 90}, {"referenceID": 242, "context": ", 2013] or the (overlapped) trace norm [Wimalawarne et al., 2014], which can be defined for a tensor W P RI1\u02c6 \u0308 \u0308 \u0308\u02c6IN as }W} \u030a = N", "startOffset": 39, "endOffset": 65}, {"referenceID": 231, "context": "Recently Tomioka and Suzuki [2013] proposed the latent trace norm of a tensor, which takes a mixture of N latent tensors, W(n), and regularizes each of them separately, as in (2.", "startOffset": 9, "endOffset": 35}, {"referenceID": 242, "context": "To deal with this problem, the scaled latent norm was proposed by Wimalawarne et al. [2014] which is defined as", "startOffset": 66, "endOffset": 92}, {"referenceID": 31, "context": "2 The N-way PLS Method The multi-way PLS (called N-way PLS) proposed by Bro [1996] is a simple extension of the standard PLS.", "startOffset": 72, "endOffset": 83}, {"referenceID": 200, "context": ", \u03a6(1)\u03a6 T (1)\u03a8(1)\u03a8 T (1)tr = \u03bbtr and ur = \u03a8(1)\u03a8 T (1)tr [Rosipal and Trejo, 2002].", "startOffset": 56, "endOffset": 81}, {"referenceID": 206, "context": "7 Kernel Functions in Tensor Learning Kernel functions can be considered as a means for defining a new topology which implies a priori knowledge about the invariance in the input space [Sch\u00f6lkopf and Smola, 2002] (see Figure 2.", "startOffset": 185, "endOffset": 212}, {"referenceID": 230, "context": "Kernel algorithms can also be used for estimation of vector-valued nonlinear and nonstationary signals [Tobar et al., 2014].", "startOffset": 103, "endOffset": 123}, {"referenceID": 215, "context": "One similarity measure between matrices is the so called Chordal distance, which is a projection of the Frobenius norm on a Grassmannian manifolds [Signoretto et al., 2011].", "startOffset": 147, "endOffset": 172}, {"referenceID": 215, "context": "It should be emphasized that such a tensor kernel ensures (provides) rotation and reflection invariance for elements on the Grassmann manifold [Signoretto et al., 2011].", "startOffset": 143, "endOffset": 168}, {"referenceID": 173, "context": "One simple and very useful information divergence is the standard symmetric Kullback-Leibler (sKL) divergence [Moreno et al., 2003], expressed as", "startOffset": 110, "endOffset": 131}, {"referenceID": 207, "context": "It should be emphasized that such a tensor kernel ensures (provides) rotation and reflection invariance for elements on the Grassmann manifold [Signoretto et al., 2011]. Zhao et al. [2013c] proposed a whole family of probabilistic product kernels based on generative models.", "startOffset": 144, "endOffset": 190}, {"referenceID": 196, "context": "8 Tensor Variate Gaussian Processes (TVGP) Gaussian processes (GP) can be considered as a class of probabilistic models which specify a distribution over a function space, where the inference is performed directly in the function space [Rasmussen and Williams, 2006].", "startOffset": 236, "endOffset": 266}, {"referenceID": 253, "context": ", M, was investigated by Zhao et al. [2013b]. All class labels are collected in the M\u02c6 1 target vector y, and all tensors are concatenated in an (N + 1)th-order tensor X of size M\u02c6 I1\u02c6 \u0308  \u0308  \u0308\u02c6 IN .", "startOffset": 25, "endOffset": 45}, {"referenceID": 110, "context": "The additive multiplicative nonparametric regression (AMNR) model constructs f as the sum of local functions which take the components of a rank-one tensor as inputs [Imaizumi and Hayashi, 2016].", "startOffset": 166, "endOffset": 194}, {"referenceID": 233, "context": "Remark 4: According to the statistical learning theory [Vapnik and Vapnik, 1998], SVM-based learning performs well when the number of training measurements is larger than the complexity of the model.", "startOffset": 55, "endOffset": 80}, {"referenceID": 97, "context": "problem can be formulated, as follows [Hao et al., 2013]:", "startOffset": 38, "endOffset": 56}, {"referenceID": 97, "context": "Then, the inner product of Xi and Xj is given by Hao et al. [2013] xXi, Xjy \u00ab R", "startOffset": 49, "endOffset": 67}, {"referenceID": 138, "context": "10 Higher Rank Support Tensor Machines (HRSTM) Higher Rank STMs (HRSTM) aim to estimate a set of parameters in the form of the sum of rank-one tensors [Kotsia et al., 2012], which defines a", "startOffset": 151, "endOffset": 172}, {"referenceID": 88, "context": "11 Kernel Support Tensor Machines The class of support tensor machines has been recently extended to the nonlinear case by using the kernel framework, and is referred to as kernel support tensor regression (KSTR) [Gao and Wu, 2012].", "startOffset": 213, "endOffset": 231}, {"referenceID": 152, "context": "91) The regularized Multiway Fisher Discriminant Analysis (MFDA) [Lechuga et al., 2015] aims to impose the structural constraints in such a way that the weight vector w will be decomposed as w = wN b  \u0308  \u0308  \u0308 bw1.", "startOffset": 65, "endOffset": 87}, {"referenceID": 188, "context": "The Linear Discriminant Analysis (LDA) can also be extended to tensor data, which is referred to as Higher Order Discriminant Analysis (HODA) [Phan and Cichocki, 2010] and Multilinear Discriminant Analysis (MDA) [Li and Schonfeld, 2014].", "startOffset": 142, "endOffset": 167}, {"referenceID": 157, "context": "The Linear Discriminant Analysis (LDA) can also be extended to tensor data, which is referred to as Higher Order Discriminant Analysis (HODA) [Phan and Cichocki, 2010] and Multilinear Discriminant Analysis (MDA) [Li and Schonfeld, 2014].", "startOffset": 212, "endOffset": 236}, {"referenceID": 154, "context": ", 2016], SVD [Lee and Cichocki, 2015], solutions of overdetermined and undetermined systems of linear algebraic equations [Dolgov and Savostyanov, 2014, Oseledets and Dolgov, 2012], the Moore\u2013 Penrose pseudo-inverse of structured matrices [Lee and Cichocki, 2016b], and LASSO regression problems [Lee and Cichocki, 2016a].", "startOffset": 13, "endOffset": 37}, {"referenceID": 184, "context": "In [\u00d6stlund and Rommer, 1995] pointed out that the wave function generated by the DMRG iteration is a matrix product state.", "startOffset": 3, "endOffset": 29}, {"referenceID": 239, "context": "2 Extraction of Two Neighboring Cores for Modified ALS (MALS) The Modified ALS algorithm, also called the two-site Density Matrix Renormalization Group (DMRG2) algorithm1 requires the extraction of two 1The DMRG algorithm was first proposed by White [1992]. At that time, people did not know the relation between tensor network and the DMRG.", "startOffset": 244, "endOffset": 257}, {"referenceID": 177, "context": "As a matter of fact, quantum physicists use rather the time-evolving block decimation (TEBD) algorithm in order to optimize matrix product states (TT/MPS) (for more detail see Or\u00fas [2014], Orus and Vidal [2008], Vidal [2003].", "startOffset": 176, "endOffset": 188}, {"referenceID": 177, "context": "As a matter of fact, quantum physicists use rather the time-evolving block decimation (TEBD) algorithm in order to optimize matrix product states (TT/MPS) (for more detail see Or\u00fas [2014], Orus and Vidal [2008], Vidal [2003].", "startOffset": 176, "endOffset": 211}, {"referenceID": 177, "context": "As a matter of fact, quantum physicists use rather the time-evolving block decimation (TEBD) algorithm in order to optimize matrix product states (TT/MPS) (for more detail see Or\u00fas [2014], Orus and Vidal [2008], Vidal [2003].)", "startOffset": 176, "endOffset": 225}, {"referenceID": 74, "context": "Algorithm 10: One full sweep of the ALS algorithm for symmetric EVD [Dolgov and Savostyanov, 2014] Input: A symmetric matrix A P RI\u02c6I , and initial guesses for X P RI1 I2 \u0308 \u0308 \u0308IN\u02c6K in block-1 TT format and with right orthogonal cores X(2), .", "startOffset": 68, "endOffset": 98}, {"referenceID": 74, "context": "The Alternating Minimal Energy (AMEn) algorithm aims to avoid the problem of convergence to non-global minimum by exploiting the information about the gradient of a cost function or information about the value of a current residual by \u201cenriching\u201d the TT cores with additional information during the iteration process [Dolgov and Savostyanov, 2014].", "startOffset": 317, "endOffset": 347}, {"referenceID": 76, "context": "Such an enrichment was efficiently implemented first by Dolgov and Savostyanov to solve symmetric as well as nonsymmetric linear systems [Dolgov et al., 2016] and was later extended to symmetric EVD in [Kressner et al.", "startOffset": 137, "endOffset": 158}, {"referenceID": 107, "context": ", 2014a] and [Hubig et al., 2015].", "startOffset": 13, "endOffset": 33}, {"referenceID": 74, "context": "The concept was proposed by White [2005] as a corrected one-side DMRG1 method, while [Dolgov and Savostyanov, 2014] proposed a significantly improved AMEn algorithm for solving large scale systems of linear equations together with theoretical convergence analysis.", "startOffset": 85, "endOffset": 115}, {"referenceID": 107, "context": ", 2014a] and [Hubig et al., 2015] developed AMEn type methods for solving large scale eigenvalue problems.", "startOffset": 13, "endOffset": 33}, {"referenceID": 68, "context": "The Alternating Minimal Energy (AMEn) algorithm aims to avoid the problem of convergence to non-global minimum by exploiting the information about the gradient of a cost function or information about the value of a current residual by \u201cenriching\u201d the TT cores with additional information during the iteration process [Dolgov and Savostyanov, 2014]. In that sense, the AMEn can be considered as a subspace correction technique. Such an enrichment was efficiently implemented first by Dolgov and Savostyanov to solve symmetric as well as nonsymmetric linear systems [Dolgov et al., 2016] and was later extended to symmetric EVD in [Kressner et al., 2014a] and [Hubig et al., 2015]. Similar to the ALS method, at each iteration step, the AMEn algorithm updates a single core tensor (see Figure 3.11). Then, it concatenates the updated TT core X(n) with a core tensor Z(n) obtained from the residual vector. At each micro-iteration, only one core tensor of the solution is enriched by the core tensor computed based on the gradient vector. By concatenating two core tensors X(n) and Z(n), the AMEn can achieve global convergence, while maintaining the computational and storage complexities as low as those of the standard ALS [Dolgov and Khoromskij, 2015, Dolgov and Savostyanov, 2014]. The concatenation step of the AMEn is also called the (core) enrichment, the basis expansion, or the local subspace correction. The concept was proposed by White [2005] as a corrected one-side DMRG1 method, while [Dolgov and Savostyanov, 2014] proposed a significantly improved AMEn algorithm for solving large scale systems of linear equations together with theoretical convergence analysis.", "startOffset": 318, "endOffset": 1453}, {"referenceID": 2, "context": "This corresponds to an orthogonal projection of the gradient of J(X) = tr(XTAX) onto the tangent space of the Stiefel manifold [Absil et al., 2008].", "startOffset": 127, "endOffset": 147}, {"referenceID": 107, "context": "Alternatively, [Hubig et al., 2015] developed a more efficient algorithm, whereby instead of using the exact residual AX  \u0301 X\u039b to compute the enrichment term, which is computationally expensive, they show that it is sufficient to exploit only the AX term.", "startOffset": 15, "endOffset": 35}, {"referenceID": 107, "context": "39) for eigenvalue problems, with K = 1 eigenvalue [Hubig et al., 2015].", "startOffset": 51, "endOffset": 71}, {"referenceID": 74, "context": "Furthermore, it has been found that even a rough approximation to the residual / gradient for the enrichment Z(n) will lead to a faster convergence of the algorithm [Dolgov and Savostyanov, 2014].", "startOffset": 165, "endOffset": 195}, {"referenceID": 154, "context": "12 (see [Lee and Cichocki, 2015] for detail and computer simulation experiments).", "startOffset": 8, "endOffset": 32}, {"referenceID": 246, "context": "For example, in spectral co-clustering, only one eigenvector (called Fiedler eigenvector) needs to be computed, which corresponds to the second smallest generalized eigenvalue [Wu et al., 2016].", "startOffset": 176, "endOffset": 193}, {"referenceID": 2, "context": "52) into the trace operator, to give the following optimization problem [Absil et al., 2008] min VPRI\u02c6K tr(VTAV(VTBV) \u03011).", "startOffset": 72, "endOffset": 92}, {"referenceID": 62, "context": "Also, by a change in the variables, W = B1/2V, the GEVD reduces to the standard symmetric EVD [Cunningham and Ghahramani, 2015] min WPRI\u02c6K tr(WTB \u03011/2AB \u03011/2W), s.", "startOffset": 94, "endOffset": 127}, {"referenceID": 62, "context": "An alternative approach is to use the orthogonal CCA model, which can be formulated as [Cunningham and Ghahramani, 2015]", "startOffset": 87, "endOffset": 120}, {"referenceID": 74, "context": "4 for the EVD problem, has been developed first historically as an efficient solution to large-scale least squares problems [Dolgov and Savostyanov, 2014].", "startOffset": 124, "endOffset": 154}, {"referenceID": 74, "context": "Next, for building an enrichment, Z(n) P RRn \u03011\u02c6Jn\u02c6Qn , [Dolgov and Savostyanov, 2014] considered an approximation to the partially projected gradient (X\u0103n bL IJn \u0308 \u0308 \u0308JN ) T(ATAx \u0301ATb) \u2013 vec ( xxZ(n), R(n+1), .", "startOffset": 56, "endOffset": 86}, {"referenceID": 74, "context": "Algorithm 12: AMEn for linear systems Ax \u2013 b [Dolgov and Savostyanov, 2014] Input: Matrix A P RI\u02c6J , with I \u011b J, vector b P RI , initial guesses for x P RJ and residual pr = AT(Ax \u0301 b) P RJ in the TT format Output: Approximate solution x in the TT format X = xxX(1), X(2), .", "startOffset": 45, "endOffset": 75}, {"referenceID": 40, "context": "When the sparsity structures are overlapping, the problem can be reformulated as the Group LASSO problem [Chen et al., 2014] min x }Ax \u0301 b}2 + \u03b3}G\u03a6x}2,1, (3.", "startOffset": 105, "endOffset": 124}, {"referenceID": 176, "context": "Other generalizations of the standard LASSO include the Block LASSO, Fused LASSO, Elastic Net and Bridge regression algorithms [Ogutu and Piepho, 2014].", "startOffset": 127, "endOffset": 151}, {"referenceID": 37, "context": "A simple approach in this direction would be to apply Iteratively Reweighted Least Squares (IRLS) methods10 [Candes et al., 2008], whereby the `1-norm is replaced by the reweighted `2-norm (see Figure 3.", "startOffset": 108, "endOffset": 129}, {"referenceID": 37, "context": "93) where the diagonal elements are wj = [|xj| + \u03b52]q/2 \u03011, and \u03b5 \u0105 0 is a very small number needed to avoid divergence for a small xj [Candes et al., 2008].", "startOffset": 135, "endOffset": 156}, {"referenceID": 40, "context": "Similarly, for the non-overlapping group LASSO [Chen et al., 2014] }x}q,1 = \u00ff", "startOffset": 47, "endOffset": 66}, {"referenceID": 2, "context": "\u2022 \u2207J(xk)9Axk  \u0301 xk\u03bbk with \u03bbk = xk Axk, for the Rayleigh quotient J(x) = xTAx with }x}2 = 1, which is an orthogonal projection of the gradient of J onto the tangent space of a sphere, or a Stiefel manifold in general [Absil et al., 2008].", "startOffset": 216, "endOffset": 236}, {"referenceID": 12, "context": "a soft thresholding scheme [Bachmayr et al., 2016].", "startOffset": 27, "endOffset": 50}, {"referenceID": 22, "context": ", Richardson, CG GMRES) and combined with CP format [Beylkin and Mohlenkamp, 2005, Khoromskij and Schwab, 2011], CP and Tucker formats [Billaud-Friess et al., 2014], TT format [Dolgov, 2013, Khoromskij and Oseledets, 2010], HT format [Bachmayr and Dahmen, 2015, Bachmayr and Schneider, 2016, Kressner and Tobler, 2011a], and a subspace projection method combined with HT format [Ballani and Grasedyck, 2013]; \u2022 A subspace projection method combined with HT format [Ballani and Grasedyck, 2013]; \u2022 Computing extreme eigenvalues and corresponding eigenvectors by Lanczos method combined with TT format [Huang et al.", "startOffset": 135, "endOffset": 164}, {"referenceID": 151, "context": ", 2016, Huckle and Waldherr, 2012], preconditioned inverse iteration with TT format [Mach, 2013], block CG method (LOBPCG) combined with TT format [Lebedeva, 2011] and with HT format [Kressner and Tobler, 2011b]; \u2022 Iterative hard thresholding for low-rank matrix/tensor completion [Foucart and Rauhut, 2013, Tanner and Wei, 2013].", "startOffset": 147, "endOffset": 163}, {"referenceID": 12, "context": "Moreover, theoretical global convergence properties hold under some restricted conditions (see [Bachmayr et al., 2016] for more detail).", "startOffset": 95, "endOffset": 118}, {"referenceID": 26, "context": "In addition, due to the analogy between low-rank truncation and sparse signal estimation techniques, truncated iteration-type algorithms are also suitable for largescale compressed sensing [Blumensath and Davies, 2009].", "startOffset": 189, "endOffset": 218}, {"referenceID": 68, "context": "For example, in the GMRES method [Dolgov, 2013], even if the solution vector and the right-hand side vector are well approximated by the TT format, the residuals and Krylov vectors involved in intermediate iterations usually have high TT ranks.", "startOffset": 33, "endOffset": 47}, {"referenceID": 232, "context": "For example, [Uschmajew and Vandereycken, 2013] developed the manifold structure for the HT tensors, while Lubich et al.", "startOffset": 13, "endOffset": 47}, {"referenceID": 98, "context": "If the inner product of two tensors induces a Euclidean metric on the embedding space RI1\u02c6 \u0308 \u0308 \u0308\u02c6IN , then the above submanifold Mr is a Riemannian manifold [Holtz et al., 2012b, Steinlechner, 2016a, Uschmajew and Vandereycken, 2013]. Similar results are also available for the more general hierarchical Tucker models. For example, [Uschmajew and Vandereycken, 2013] developed the manifold structure for the HT tensors, while Lubich et al. [2013] developed the concept of dynamical low-rank approximation for both HT and TT formats.", "startOffset": 158, "endOffset": 447}, {"referenceID": 2, "context": "The fundamental and basic algorithms for Riemannian optimization [Absil et al., 2008] are quite attractive, but are still not widely used as they are more technically complicated than, e.", "startOffset": 65, "endOffset": 85}, {"referenceID": 29, "context": "The MANOPT package [Boumal et al., 2014] (for a Python version see [Koep and Weichwald, 2016]) provides a useful interface for many standard matrix Riemannian optimization techniques, however, extensions for tensors (especially in high dimensions) are not easy and should be performed very carefully.", "startOffset": 19, "endOffset": 40}, {"referenceID": 135, "context": ", 2014] (for a Python version see [Koep and Weichwald, 2016]) provides a useful interface for many standard matrix Riemannian optimization techniques, however, extensions for tensors (especially in high dimensions) are not easy and should be performed very carefully.", "startOffset": 34, "endOffset": 60}, {"referenceID": 146, "context": "105) and [Kressner et al., 2016].", "startOffset": 9, "endOffset": 32}, {"referenceID": 4, "context": "Definition 1 [Adler et al., 2002] A mapping RX is called the retraction, if 1.", "startOffset": 13, "endOffset": 33}, {"referenceID": 0, "context": "For low-rank matrices and tensors with fixed TT-ranks, the simplest retraction is provided by respective SVD and TT-SVD algorithms, however, there are many other types of retractions; for more detail we refer to the survey [Absil and Oseledets, 2015].", "startOffset": 223, "endOffset": 250}, {"referenceID": 2, "context": "[2016] the Polak-Ribiere update formula [Nocedal and Wright, 2006] can be adapted to Riemannian optimization [Absil et al., 2008].", "startOffset": 109, "endOffset": 129}, {"referenceID": 141, "context": "To this end, as suggested in Kressner et al. [2016] the Polak-Ribiere update formula [Nocedal and Wright, 2006] can be adapted to Riemannian optimization [Absil et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 2, "context": "The local search direction, \u03bek, is determined from the correction equation [Absil et al., 2008] as HXk \u03bek = \u0301PTXkMr\u2207J(Xk), (3.", "startOffset": 75, "endOffset": 95}, {"referenceID": 1, "context": "Recently, in order to achieve a better global convergence, an alternative version of the Riemannian Newton method called the Trust-Region scheme was developed by Absil et al. [2007], Boumal and Absil [2011], Ishteva et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 1, "context": "Recently, in order to achieve a better global convergence, an alternative version of the Riemannian Newton method called the Trust-Region scheme was developed by Absil et al. [2007], Boumal and Absil [2011], Ishteva et al.", "startOffset": 162, "endOffset": 207}, {"referenceID": 1, "context": "Recently, in order to achieve a better global convergence, an alternative version of the Riemannian Newton method called the Trust-Region scheme was developed by Absil et al. [2007], Boumal and Absil [2011], Ishteva et al. [2011]. Retraction and vector transport are critical operations to the success of sophisticated Riemannian optimization algorithms, such as Riemannian CG, Newton and/or quasi-Newton methods.", "startOffset": 162, "endOffset": 230}, {"referenceID": 170, "context": "Indeed, some fundamental results can be found in the quantum molecular dynamics community which has used the so-called Multi Configurational Time-Dependent Hartree (MCTDH) method already since the 1990s [Manthe et al., 1992].", "startOffset": 203, "endOffset": 224}, {"referenceID": 133, "context": "mathematics by Koch and Lubich [2007], its roots are in quantum mechanics and can be traced back to the so-called Dirac-Frenkel principle.", "startOffset": 15, "endOffset": 38}, {"referenceID": 134, "context": "The original approach of Koch and Lubich [2007] (later generalized to the Tucker and TT models [Koch and Lubich, 2010]) is to write down ordinary differential equations for the parameters U(t), S(t), V(t) of the SVD like decomposition in the form Y(t) = U(t)S(t)VT(t).", "startOffset": 95, "endOffset": 118}, {"referenceID": 166, "context": "This leads to a much better convergence, and moreover an exactness result [Lubich and Oseledets, 2014] can be proven if A1 and A0 are on the manifold, and Y0 = A0, then this scheme is exact.", "startOffset": 74, "endOffset": 102}, {"referenceID": 0, "context": "Through the link with the Riemannian optimization, the projectorsplitting scheme can be also used here, which can be viewed as a secondorder retraction [Absil and Oseledets, 2015].", "startOffset": 152, "endOffset": 179}, {"referenceID": 132, "context": "The original approach of Koch and Lubich [2007] (later generalized to the Tucker and TT models [Koch and Lubich, 2010]) is to write down ordinary differential equations for the parameters U(t), S(t), V(t) of the SVD like decomposition in the form Y(t) = U(t)S(t)VT(t).", "startOffset": 25, "endOffset": 48}, {"referenceID": 132, "context": "The original approach of Koch and Lubich [2007] (later generalized to the Tucker and TT models [Koch and Lubich, 2010]) is to write down ordinary differential equations for the parameters U(t), S(t), V(t) of the SVD like decomposition in the form Y(t) = U(t)S(t)VT(t). The second approach is also straightforward: apply any time integration scheme to the equation (3.116). In this case, a standard method will yield the solution which is not on the manifold, and a retraction would be needed. In Lubich and Oseledets [2014] a simple and efficient solution to this problem, referred to as the projector-splitting scheme, was proposed, based on the special structure of the manifold.", "startOffset": 25, "endOffset": 524}, {"referenceID": 164, "context": "The generalization to the TT network was proposed by Lubich et al. [2015], and can be implemented within the framework of sweeping algorithms, allowing for the efficient TT-approximation of dynamical systems and solution of optimization problems with non-quadratic functionals.", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": "Local convergence results follow from the general theory [Absil et al., 2008], however the important problem of the curvature and singular points is not yet fully addressed.", "startOffset": 57, "endOffset": 77}, {"referenceID": 129, "context": "One way forward is to look for the desingularization [Khrulkov and Oseledets, 2016], another technique is to employ the concept of tangent bundle.", "startOffset": 53, "endOffset": 83}, {"referenceID": 137, "context": "An attempt to study the global convergence was presented in [Kolesnikov and Oseledets, 2016], and even in this case, convergence to the spurious local minima is possible in a carefully designed example.", "startOffset": 60, "endOffset": 92}, {"referenceID": 137, "context": "This was observed experimentally in [Kolesnikov and Oseledets, 2016] in the form of a \u201cstaircase\u201d convergence.", "startOffset": 36, "endOffset": 68}, {"referenceID": 174, "context": "To deal with the exponentially large number of parameters, Novikov et al. [2016], Stoudenmire and Schwab [2016] proposed the so-called Exponential Machines (ExM), where a large tensor of parameters is represented compactly in the TT format in order to provide low-rank regularization of the model and a reduction of the problem to a manageable scale.", "startOffset": 59, "endOffset": 81}, {"referenceID": 174, "context": "To deal with the exponentially large number of parameters, Novikov et al. [2016], Stoudenmire and Schwab [2016] proposed the so-called Exponential Machines (ExM), where a large tensor of parameters is represented compactly in the TT format in order to provide low-rank regularization of the model and a reduction of the problem to a manageable scale.", "startOffset": 59, "endOffset": 112}, {"referenceID": 197, "context": ", 2016], but a recent negative result shows that it is not possible to provide a good convex surrogate for the TT-manifold [Rauhut et al., 2016], thus making the Riemannian optimization the most promising tool for low-rank constrained optimization.", "startOffset": 123, "endOffset": 144}, {"referenceID": 135, "context": "The Pymanopt [Koep and Weichwald, 2016] is the first step in this direction, but it is still quite far from big-data problems, since it works with full matrices even for a low-rank manifold, and in the tensor case that would be prohibitive.", "startOffset": 13, "endOffset": 39}, {"referenceID": 183, "context": "com/oseledets/ttpy) for PYTHON is currently the most complete software for the TT (MPS/MPO) and QTT networks [Oseledets et al., 2012].", "startOffset": 109, "endOffset": 133}, {"referenceID": 6, "context": "For standard TDs (CPD, Tucker models) the Tensor Toolbox for MATLAB, originally developed by Kolda and Bader, provides several general-purpose functions and special facilities for handling sparse, dense, and structured TDs [Bader and Kolda, 2006, 2015], while the N-Way Toolbox for MATLAB, by Andersson and Bro, was developed mostly for Chemometrics applications [Andersson and Bro, 2000].", "startOffset": 363, "endOffset": 388}, {"referenceID": 140, "context": "Related and complementary algorithms implemented by Kressner et al. [2014a] are available within the MATLAB TTeMPS Toolbox (http://anchp.", "startOffset": 52, "endOffset": 76}, {"referenceID": 142, "context": "html) focuses mostly on HT type of tensor networks [Kressner and Tobler, 2014], which avoid explicit computation of the SVDs when truncating a tensor which is already in a HT format [Espig et al.", "startOffset": 51, "endOffset": 78}, {"referenceID": 118, "context": "The library is aimed towards more complex tensor networks such as PEPS and MERA [Kao et al., 2015].", "startOffset": 80, "endOffset": 98}, {"referenceID": 47, "context": ", 2012, Zhou and Cichocki, 2013]. The Hierarchical Tucker toolbox by Kressner and Tobler (http://www. sam.math.ethz.ch/NLAgroup/htucker_toolbox.html) focuses mostly on HT type of tensor networks [Kressner and Tobler, 2014], which avoid explicit computation of the SVDs when truncating a tensor which is already in a HT format [Espig et al., 2012, Grasedyck et al., 2013, Kressner and Tobler, 2014]. In quantum physics and chemistry, a number of software packages have been developed in the context of DMRG techniques for simulating quantum networks. One example is the intelligent Tensor (iTensor) by Stoudenmire and White [2014], an open source C++ library for rapid development of tensor network algorithms.", "startOffset": 17, "endOffset": 630}, {"referenceID": 54, "context": "Sparse Nonnegative Matrix Factorization (NMF) [Cichocki et al., 2009], given by min A,B }X \u0301AB}F + \u03b31}A}1 + \u03b32}B}1, s.", "startOffset": 46, "endOffset": 69}, {"referenceID": 87, "context": "Sparse Inverse Covariance Selection [Friedman et al., 2008], given by min XPSN\u02c6N ++ tr(CxX) \u0301 log det X + \u03b3}X}1, s.", "startOffset": 36, "endOffset": 59}, {"referenceID": 244, "context": "Useful examples of penalty functions which promote sparsity and smoothness are [Witten, 2010]", "startOffset": 79, "endOffset": 93}, {"referenceID": 105, "context": "Two-way functional PCA/SVD [Huang et al., 2009] in the form max u,v tuTAv \u0301 \u03b3 2 P1(u)P2(v)u, s.", "startOffset": 27, "endOffset": 47}, {"referenceID": 153, "context": "Sparse SVD [Lee et al., 2010], given by", "startOffset": 11, "endOffset": 29}, {"referenceID": 5, "context": "Generalized nonnegative SPCA [Allen and Maletic-Savatic, 2011] which solves max u,v tuTARv \u0301 \u03b1}v}1u, s.", "startOffset": 29, "endOffset": 62}, {"referenceID": 203, "context": ", a system which simulates the structure of biological molecule [Savostyanov et al., 2014].", "startOffset": 64, "endOffset": 90}, {"referenceID": 40, "context": "1 A Perspective of Tensor Networks for Deep Learning Several research groups have recently investigated the application of tensor decompositions to simplify DNNs and to establish links between deep learning and low-rank tensor networks [Chen et al., 2017, Cohen et al., 2016, Lebedev and Lempitsky, 2015, Novikov et al., 2015, Poggio et al., 2016, Yang and Hospedales, 2016]. For example, Chen et al. [2017] presented a general and constructive connection between Restricted Boltzmann Machines (RBM) and TNs, together with the correspondence between", "startOffset": 237, "endOffset": 408}, {"referenceID": 35, "context": "We therefore conjecture that realistic datasets in most successful machine learning applications have relatively low entanglement entropies [Calabrese and Cardy, 2004].", "startOffset": 140, "endOffset": 167}, {"referenceID": 42, "context": "The TN can be simplified through conversion to TT/MPS formats [Chen et al., 2017].", "startOffset": 62, "endOffset": 81}, {"referenceID": 169, "context": "3: Graphical illustrations of the construction of Deep (restricted) Boltzmann Machines (DBMs) and Standard Deep Belief Networks (DBNs). Both use initialization schemes based on greedy layer-wise training of Restricted Boltzmann Machines (RBMs) (left panel), i.e., they are both probabilistic graphical models consisting of stacked layers of RBMs. Although DBNs and DBMs look diagrammatically quite similar, they are qualitatively very different. The main difference is in how the hidden layers are connected. In a DBM, the connection between all layers is undirected, thus each pair of layers forms an RBM, while a DBN has undirected connections between its top two layers and downward directed connections between all its lower layers (see Salakhutdinov and Hinton [2009] for detail).", "startOffset": 78, "endOffset": 773}, {"referenceID": 85, "context": "\u201cexperts\u201d, in which a number of experts for individual observations are combined multiplicatively [Fischer and Igel, 2012].", "startOffset": 98, "endOffset": 122}, {"referenceID": 42, "context": "2(c) [Chen et al., 2017] shows that such a tensor network model comprises a set of interconnected diagonal core tensors: Kth-order tensors \u039b v = diagK(1, e am) and Mth-order tensors \u039b (k) h = diagM(1, e bk), all of sizes 2 \u02c6 2 \u02c6  \u0308  \u0308  \u0308 \u02c6 2.", "startOffset": 5, "endOffset": 24}, {"referenceID": 192, "context": "1 Matrix Variate Restricted Boltzmann Machines The matrix-variate restricted Boltzmann machine (MVRBM) model was proposed as a generalization of the classic RBM to explicitly model matrix data [Qi et al., 2016], whereby both input and hidden variables are in their matrix forms which are connected by bilinear transforms.", "startOffset": 193, "endOffset": 210}, {"referenceID": 85, "context": "For the corresponding learning algorithms see Fischer and Igel [2012], Qi et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 85, "context": "For the corresponding learning algorithms see Fischer and Igel [2012], Qi et al. [2016], Salakhutdinov and Hinton [2009].", "startOffset": 46, "endOffset": 88}, {"referenceID": 85, "context": "For the corresponding learning algorithms see Fischer and Igel [2012], Qi et al. [2016], Salakhutdinov and Hinton [2009].", "startOffset": 46, "endOffset": 121}, {"referenceID": 174, "context": "[Nguyen et al., 2015]).", "startOffset": 0, "endOffset": 21}, {"referenceID": 57, "context": ", I), to template input patches, thereby creating I feature maps [Cohen and Shashua, 2016].", "startOffset": 65, "endOffset": 90}, {"referenceID": 57, "context": ", x(I)u [Cohen and Shashua, 2016].", "startOffset": 8, "endOffset": 33}, {"referenceID": 191, "context": "For example, it was shown recently how NNs with a univariate rectified linear unit (ReLU) nonlinearity may perform multivariate function approximation [Poggio et al., 2016].", "startOffset": 151, "endOffset": 172}, {"referenceID": 191, "context": "For example, it was shown recently how NNs with a univariate rectified linear unit (ReLU) nonlinearity may perform multivariate function approximation [Poggio et al., 2016]. As discussed in Part 1 and in Chapter 1 the main idea is to employ a low-rank tensor network representation to approximate and interpolate a multivariate function hc(x1, . . . , xN) of N variables by a finite sum of separated products of simpler functions (i.e., via sparsely interconnected core tensors). Tensorial Mixture Model (TMM). Recently, the model in (4.15) has been extended to a corresponding probabilistic model, referred to as the Tensorial Mixture Model (TMM), given by Sharir et al. [2016] P(x1, x2, .", "startOffset": 152, "endOffset": 679}, {"referenceID": 212, "context": "The TMM based on the HT decompositions is promising for multiple classification problems with partially missing training data, and potentially even regardless of the distribution of missing data [Sharir et al., 2016].", "startOffset": 195, "endOffset": 216}, {"referenceID": 59, "context": ", I, map each local patch xn into a feature space of dimension I; (ii) the second, a key part, is a convolutional arithmetic circuit consisting of many hidden layers that takes the N I measurements (training samples) generated by the representation layer; (iii) the output layer, which can be represented by a matrix, W(L) P RR\u02c6C, which computes C different score functions hc [Cohen et al., 2016].", "startOffset": 377, "endOffset": 397}, {"referenceID": 57, "context": "In order to convert ConvAC tensor models to widely used convolutional rectifier networks, we need to employ the generalized (nonlinear) outer products, defined as [Cohen and Shashua, 2016] (A \u030b\u03c1 B)i1,.", "startOffset": 163, "endOffset": 188}, {"referenceID": 57, "context": "Example 11 Consider a generalized CP decomposition, which corresponds to a shallow rectifier network in the form [Cohen and Shashua, 2016]", "startOffset": 113, "endOffset": 138}, {"referenceID": 80, "context": "7 MERA and 2D TNs for a Next Generation of DCNNs The Multiscale Entanglement Renormalization Ansatz (MERA) tensor network was first introduced by Vidal [2008], and for this network numerical algorithms to minimize some specific cost functions or local Hamiltonians used in quantum physics already exist [Evenbly and Vidal, 2009].", "startOffset": 303, "endOffset": 328}, {"referenceID": 81, "context": "The MERA is a relatively new tensor network, widely investigated in quantum physics based on variational Ansatz, since it is capable of capturing many of the key complex physical properties of strongly correlated ground states [Evenbly and Vidal, 2015].", "startOffset": 227, "endOffset": 252}, {"referenceID": 230, "context": "7 MERA and 2D TNs for a Next Generation of DCNNs The Multiscale Entanglement Renormalization Ansatz (MERA) tensor network was first introduced by Vidal [2008], and for this network numerical algorithms to minimize some specific cost functions or local Hamiltonians used in quantum physics already exist [Evenbly and Vidal, 2009].", "startOffset": 146, "endOffset": 159}, {"referenceID": 81, "context": "The key properties of MERA can be summarized, as follows [Evenbly and Vidal, 2015]: \u2022 MERA can capture scale-invariance in input data; \u2022 It reproduces a polynomial decay of correlations between inputs, in contrast to HT or TT networks which reproduce only exponential decay of correlations; \u2022 MERA has the ability to compress tensor data much better than TT/HT tensor networks; \u2022 It reproduces a logarithmic correction to the area law, therefore MERA is more powerful tensor network than HT/TTNS or TT/TC networks; \u2022 MERA can be efficiently contracted due to unitary constraints imposed on core tensors.", "startOffset": 57, "endOffset": 82}], "year": 2017, "abstractText": "Part 2 of this monograph builds on the introduction to tensor networks and their operations presented in Part 1. It focuses on tensor network models for super-compressed higher-order representation of data/parameters and related cost functions, while providing an outline of their applications in machine learning and data analytics. A particular emphasis is on the tensor train (TT) and Hierarchical Tucker (HT) decompositions, and their physically meaningful interpretations which reflect the scalability of the tensor network approach. Through a graphical approach, we also elucidate how, by virtue of the underlying low-rank tensor approximations and sophisticated contractions of core tensors, tensor networks have the ability to perform distributed computations on otherwise prohibitively large volumes of data/parameters, thereby alleviating or even eliminating the curse of dimensionality. The usefulness of this concept is illustrated over a number of applied areas, including generalized regression and classification (support tensor machines, canonical correlation analysis, higher order partial least squares), generalized eigenvalue decomposition, Riemannian optimization, and in the optimization of deep neural networks. Part 1 and Part 2 of this work can be used either as stand-alone separate texts, or indeed as a conjoint comprehensive review of the exciting field of low-rank tensor networks and tensor decompositions.", "creator": "LaTeX with hyperref package"}}}