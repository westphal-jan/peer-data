{"id": "1708.09217", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Look-ahead Attention for Generation in Neural Machine Translation", "abstract": "The attention model has become a standard component in neural machine translation (NMT) and it guides translation process by selectively focusing on parts of the source sentence when predicting each target word. However, we find that the generation of a target word does not only depend on the source sentence, but also rely heavily on the previous generated target words, especially the distant words which are difficult to model by using recurrent neural networks. To solve this problem, we propose in this paper a novel look-ahead attention mechanism for generation in NMT, which aims at directly capturing the dependency relationship between target words. We further design three patterns to integrate our look-ahead attention into the conventional attention model. Experiments on NIST Chinese-to-English and WMT English-to-German translation tasks show that our proposed look-ahead attention mechanism achieves substantial improvements over state-of-the-art baselines.", "histories": [["v1", "Wed, 30 Aug 2017 11:27:02 GMT  (360kb,D)", "http://arxiv.org/abs/1708.09217v1", "12 pages, 5 figures"]], "COMMENTS": "12 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["long zhou", "jiajun zhang", "chengqing zong"], "accepted": false, "id": "1708.09217"}, "pdf": {"name": "1708.09217.pdf", "metadata": {"source": "CRF", "title": "Look-ahead Attention for Generation in Neural Machine Translation", "authors": ["Long Zhou", "Jiajun Zhang", "Chengqing Zong"], "emails": ["long.zhou@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "Unlike traditional statistical machine translation (SMT), which includes several separately tuned components, NMT relies on a single and large neural network to directly transform the source set into an associated target set. Typically, NMT adopts the encoder decoder architecture, which consists of two recurring neural networks; the encoder network models the semantics of the source set and transforms the source set into context vector representation from which the decoder network generates the target translation word for word."}, {"heading": "2 Neural Machine Translation", "text": "Our framework, which integrates the predictive attention mechanism in NMT, can be applied to any conventional attention model. Without loss of universality, we use the enhanced attention-based NMT used by Luong et al. [16], which uses stacked LSTM layers for encoders and decoders, as shown in Figure 2.The NMT first uses the source sentence X = (x1, x2,..., xm) in a sequence of context vector representation C = (h1, h2,..., hm), whose size variants 1 http: / nlp.stanford.edu: 8080 / parser / index.jsp. with reference to the source sentence length. Then the NMT decodes from the context vector representation C and generates target translations Y = (y1, y2,..., yn) one word at a time by maximizing the probability of p (yj | lty,.C)."}, {"heading": "3 Model Description", "text": "Learning long-distance dependencies is a key challenge in machine translation. Although the attention model presented above has shown its effectiveness in NMT, it does not take into account the dependency relationship between target words. In order to reduce the burden of the LSTM or GRU, to maintain target-side long-distance dependencies, we are designing a novel predictive attention mechanism that directly links the current target word to the previously generated target words. In this section, we will explain three proposed approaches to integrating predictive attention into the generation of attention-based NMT."}, {"heading": "3.1 Concatenation Pattern", "text": "Figure 3 (b) illustrates the concatenation pattern of the predictive attention mechanism. We not only calculate the attention between the current hidden target state and the hidden source state, but also calculate the attention between the current hidden target state and previous hidden target states. The predictive attention output in the time period j is calculated as follows: cdj = j \u2212 1 \u2211 i = 1 ATT (slj, s l i) \u00b7 sli (10), where ATT (slj, s l i) is a normalized element. Specifically, given the hidden target state slj, the source-side context vector representation cj and the target-side context vector representation c d j) (11), we use a concatenation layer to combine the information to create a hidden attention state as follows: tfinalj = tanh (Wc [s l j; j; j; j + b)."}, {"heading": "3.2 Enc-Dec Pattern", "text": "The Enc-Dec pattern is a simple way to get predictive attention that considers source-side context vector representation and target-side context vector representation to be equally important. Unlike the concatenation pattern, Enc-Dec pattern uses a hierarchical architecture to integrate predictive attention, as shown in Figure 3 (c). Once we get the attention-hidden state of conventional attention-based NMTs, we can use a predictive attention mechanism to update the previous attention-hidden state. In detail, the model first calculates the attention-hidden state of conventional attention-based NMTs as an equation."}, {"heading": "3.3 Dec-Enc Pattern", "text": "The Dec-Enc pattern is the opposite of the Enc-Dec pattern and uses an attention mechanism that helps the model align with the source words. Figure 3 (d) shows this pattern. We first calculate predictive attention performance as equation 10, and the hidden attention state is calculated by: tdj = tanh (Wc1 [s l j; c e j] + b) (14). Finally, we can calculate the attention between the hidden attention state tdj and the hidden state of the source to obtain the last hidden attention state: tfinalj = tanh (Wc2 [t d j; c e j] + b2) (15) cej = m \u2211 i = 1 ATT (tdj, h l i) \u00b7 hli (16), where hli is the source-side hidden state on the top level."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We conduct our experiments with the NIST translation tasks Chinese-English and the WMT14 translation tasks English-German. The evaluation metric is BLEU [21] as calculated by the multi-blue.perl script. For Chinese-English, our training data consists of 630K sentence pairs extracted from LDC corpus2. We use NIST 2003 (MT03) data sets Chinese-English as validation set, NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) data sets as test sets. In addition, the 10M Xinhua portion of the Gigaword corpus in the training language model for SMT is used. For English-German, we use the same subset of the WMT 2006 (MT06) data set as a test set [16,25,34]."}, {"heading": "4.2 Training Details", "text": "We build the models described, modified by the Zoph RNN4 toolkit, written in C + + / CUDA, which provides efficient training across multiple GPUs. Our training procedures and hyperparameter selections are similar to those used by Luong et al. [16]. In the NMT architecture, as illustrated in Figure 2, the encoder has three stacked LSTM layers, including a bidirectional layer followed by a global attention layer, and the decoder contains two stacked LSTM layers followed by the Softmax layer. Specifically, we limit the source and target vocabulary to the most common 30K words for Chinese-English and 50K words for English-German. The word embedding dimension and the size of hidden layers are all set to 1000. Parameter optimization is done using stochastic descent (SGD), and we set the learning rate at the beginning to halve the perplexity during development and the 0.1."}, {"heading": "4.3 Results on Chinese-English Translation", "text": "In fact, most of them are a type of conspiracy theory that sees itself as able to change the world without being able to change the world."}, {"heading": "4.4 Results on English-German Translation", "text": "We evaluate our model using the WMT14 English to German translation tasks, the results of which are shown in Table 2. We note that our proposed look-ahead attention NMT model also achieves significant improvements in accuracy for large-format English-German translations. Furthermore, we compare our NMT systems with various other systems, including Zhou et al. [34], which use a much deeper neural network. Luong et al. [16] achieves a BLEU score of 19.00 with a 4-layer-deep encoder decoder model. Shen et al. [25] obtain the BLEU score of 18.02 with MRI techniques. For this work, 10 use our Enc-Dec-ahead attention NMT model with two layers 20.36 BLEU scores, which corresponds to the BLEU score of Zhou et al. [34] in relation to BLEU."}, {"heading": "5 Related Work", "text": "Most of the existing approaches and models focus mainly on the development of better attention models [16,19,20,28,18], better strategies for dealing with rare and unknown words [17,14,24], the use of large-scale monolingual data [3,23,33] and the integration of SMT techniques [25,7,35,30]. Our goal in this paper is to design an intelligent attention mechanism to model the dependence relationship between target words. Tu et al. [28] and Mi et al. [19] proposed to add a coverage vector to attention models to address the problem of repetition and dropping of translations. Cohn et al. [6] expanded the attention model to include known features in traditional SMT. Unlike previous work, which mainly focused attention models on predicting the orientation of a target word in relation to source words, we focus on a direct relationship between target words to bridge the dependency."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel look-ahead attention mechanism for the generation of NMT, which aims to directly capture the dependence relationship between target words over long distances. In addition, the look-ahead attention model is based not only on the initial words, but also on the previously generated words when generating the next target word. Furthermore, we present and examine three patterns to integrate our proposed predictive attention into the traditional attention model. Experiments on translation tasks in Chinese-English and English-German show that our proposed model achieves significant BLEU score gains over strong SMT baselines and a state-of-the-art NMT baseline."}, {"heading": "Acknowledgments", "text": "The research was funded by the Natural Science Foundation of China under grant numbers 61673380, 61402478 and 61403379."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of ICLR 2015", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["J. Cheng", "L. Dong", "M. Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning for neural machine translation", "author": ["Y. Cheng", "W. Xu", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["D. Chiang"], "venue": "In Proceedings of ACL 2005", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning phrase representations using RNN encoderCdecoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of EMNLP 2014", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["T. Cohn", "C.D.V. Hoang", "E. Vymolova", "K. Yao", "C. Dyer", "G. Haffari"], "venue": "arXiv preprint arXiv:1601.01085", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved neural machine translation with SMT features", "author": ["W. He", "Z. He", "H. Wu", "H. Wang"], "venue": "In Proceedings of AAAI 2016", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory, vol", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "9. MIT Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["M. Junczys-Dowmunt", "T. Dwojak", "H. Hoang"], "venue": "In Proceedings of IWSLT 2016", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In Proceedings of EMNLP 2013", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R Zens"], "venue": "Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Six challanges for neural machine translation", "author": ["P. Koehn", "R. Knowles"], "venue": "arXiv preprint arXiv:1706.03872", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of ACL-NAACL 2013", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards zero unknown word in neural machine translation", "author": ["X. Li", "J. Zhang", "C. Zong"], "venue": "In Proceedings of IJCAI 2016", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A structured self-attentive sentence embedding", "author": ["Z. Lin", "M. Feng", "Santos", "C.N.d.", "M. Yu", "B. Xiang", "B. Zhou", "Y. Bengio"], "venue": "arXiv preprint arXiv:1703.03130", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.T. Luong", "H. Pham", "C.D. Manning"], "venue": "In Proceedings of EMNLP 2015", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["M.T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "In Proceedings of ACL 2015", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive attention for neural machine translation", "author": ["F. Meng", "Z. Lu", "H. Li", "Q. Liu"], "venue": "In Proceedings of COLING 2016", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A coverage embedding model for neural machine translation", "author": ["H. Mi", "B. Sankaran", "Z. Wang", "N. Ge", "A. Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Supervised attentions for neural machine translation", "author": ["H. Mi", "Z. Wang", "N. Ge", "A. Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Bleu: a methof for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "In Proceedings of ACL 2002", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "A deep reinforced model for abstractive summarization", "author": ["R. Paulus", "C. Xiong", "R. Socher"], "venue": "arXiv preprint arXiv:1705.04304", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Improving neural machine translation models with monolingual data", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Proceedings of NIPS 2014", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Context gates for neural machine translation", "author": ["Z. Tu", "Y. Liu", "Z. Lu", "X. Liu", "H. Li"], "venue": "arXiv preprint arXiv:1608.06043", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention is all you need", "author": ["A. Vawani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N.Gomez", "L. Kaiser", "I. Polosukhin"], "venue": "arXiv preprint arXiv:1706.03762", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation advised by statistical machine translation", "author": ["X. Wang", "Z. Lu", "Z. Tu", "H. Li", "D. Xiong", "M. Zhang"], "venue": "In Proceedings of AAAI 2017", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Sequence-to-dependency neural machine translation", "author": ["S. Wu", "D. Zhang", "N. Yang", "M. Li", "M. Zhou"], "venue": "In Proceedings of ACL 2017", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Tree-based translation without using parse trees", "author": ["F. Zhai", "J. Zhang", "Y. Zhou", "C Zong"], "venue": "In Proceedings of COLING 2012", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["J. Zhang", "C. Zong"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["J. Zhou", "Y. Cao", "X. Wang", "P. Li", "W. Xu"], "venue": "arXiv preprint arXiv:1606.04199", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural system combination for machine translation", "author": ["L. Zhou", "W. Hu", "J. Zhang", "C. Zong"], "venue": "In Proceedings of ACL 2017", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 9, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 25, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 0, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 8, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 12, "context": "Unlike traditional statistical machine translation (SMT) [13,4,32] which contains multiple separately tuned components, NMT builds upon a single and large neural network to directly map source sentence to associated target sentence.", "startOffset": 57, "endOffset": 66}, {"referenceID": 3, "context": "Unlike traditional statistical machine translation (SMT) [13,4,32] which contains multiple separately tuned components, NMT builds upon a single and large neural network to directly map source sentence to associated target sentence.", "startOffset": 57, "endOffset": 66}, {"referenceID": 31, "context": "Unlike traditional statistical machine translation (SMT) [13,4,32] which contains multiple separately tuned components, NMT builds upon a single and large neural network to directly map source sentence to associated target sentence.", "startOffset": 57, "endOffset": 66}, {"referenceID": 4, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 191, "endOffset": 197}, {"referenceID": 11, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 191, "endOffset": 197}, {"referenceID": 15, "context": "[16], which utilizes stacked LSTM layers for both encoder and decoder as illustrated in Figure 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "If k = 1, sj will be calculated by combining tj\u22121 as feed input [16]: sj = LSTM(s 1 j\u22121, yj\u22121, tj\u22121) (8)", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "The evaluation metric is BLEU [21] as calculated by the multi-blue.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "For English-German, to compare with the results reported by previous work [16,25,34], we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 84}, {"referenceID": 24, "context": "For English-German, to compare with the results reported by previous work [16,25,34], we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 84}, {"referenceID": 33, "context": "For English-German, to compare with the results reported by previous work [16,25,34], we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 84}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Moses-1 [11] is the state-of-the-art phrase-based SMT system with the default configuration and a 4-gram language model trained on the target portion of training data.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "[1] to group sentences of similar lengths together and compute a BLEU score per group, as demonstrated in Figure 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27] to improve translation quality and instead we remain it as our future work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] which use a much deeper neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] achieves BLEU score of 19.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] obtained the BLEU score of 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] LSTM with 4 layers+dropout+local att.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Gated RNN with search + MRT 50K 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] LSTM with 16 layers + F-F connections 160K 20.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] in term of BLEU.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] employ a much larger depth as well as vocabulary size to obtain their best results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 18, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 19, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 27, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 17, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 16, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 165, "endOffset": 175}, {"referenceID": 13, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 165, "endOffset": 175}, {"referenceID": 23, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 165, "endOffset": 175}, {"referenceID": 2, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 217, "endOffset": 226}, {"referenceID": 22, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 217, "endOffset": 226}, {"referenceID": 32, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 217, "endOffset": 226}, {"referenceID": 24, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 6, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 34, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 29, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 27, "context": "[28] and Mi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposed to extend attention models with a coverage vector in order to attack the problem of repeating and dropping translations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] augmented the attention model with well-known features in traditional SMT.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31] lately proposed a sequence-to-dependency NMT method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] proposed a new simple network architecture, Transformer, based solely on attention mechanisms with multi-headed self attention.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] presented a self-attention mechanism which extracts different aspects of the sentence into multiple vector representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "And the self-attention model has been used successfully in some tasks including abstractive summarization and reading comprehension[22,2].", "startOffset": 131, "endOffset": 137}, {"referenceID": 1, "context": "And the self-attention model has been used successfully in some tasks including abstractive summarization and reading comprehension[22,2].", "startOffset": 131, "endOffset": 137}], "year": 2017, "abstractText": "The attention model has become a standard component in neural machine translation (NMT) and it guides translation process by selectively focusing on parts of the source sentence when predicting each target word. However, we find that the generation of a target word does not only depend on the source sentence, but also rely heavily on the previous generated target words, especially the distant words which are difficult to model by using recurrent neural networks. To solve this problem, we propose in this paper a novel look-ahead attention mechanism for generation in NMT, which aims at directly capturing the dependency relationship between target words. We further design three patterns to integrate our look-ahead attention into the conventional attention model. Experiments on NIST Chinese-to-English and WMT English-to-German translation tasks show that our proposed look-ahead attention mechanism achieves substantial improvements over state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}