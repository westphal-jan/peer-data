{"id": "1708.09450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events", "abstract": "Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topic-specific datasets enables learning finer-grained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent.", "histories": [["v1", "Wed, 30 Aug 2017 20:01:34 GMT  (29kb)", "http://arxiv.org/abs/1708.09450v1", "SIGDIAL 2016"]], "COMMENTS": "SIGDIAL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["elahe rahimtoroghi", "ernesto hernandez", "marilyn a walker"], "accepted": false, "id": "1708.09450"}, "pdf": {"name": "1708.09450.pdf", "metadata": {"source": "CRF", "title": "Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events", "authors": ["Elahe Rahimtoroghi", "Ernesto Hernandez"], "emails": ["elahe@soe.ucsc.edu,", "eherna23@ucsc.edu,", "mawalker@ucsc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.09 450v 1 [cs.C L] 30 Aug 201 7Civilian media are provided by ordinary people who tell stories about their daily lives. We are developing and testing a novel method of learning fine-grained common sense from these stories of contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for understanding text and history, information extraction, question-solving and text summary. We are testing and comparing different methods of learning contingency relationships and comparing what is learned from topic-sorted story collections with stories from general areas. Our experiments show that the use of topical data sets enables the learning of fine-grained knowledge of events and leads to significant improvements over the basics. An evaluation on Amazon Mechanical Turk shows that 82% of the relationships between events we learn from themed stories are rated contingent."}, {"heading": "1 Introduction", "text": "The original idea behind the scripts Schank introduced was to capture knowledge of the fine-grained events of everyday experience, such as opening a refrigerator that allows food to be prepared, or the event of getting out of bed triggered by an alarm that goes off (Schank et al., 1977).This idea motivated previous work to investigate whether healthy knowledge of events can be learned from the text, but few learn from data other than from the news wire (Hu et al., 2013; Manshadi et al., 2008; Beamer and Girju, 2009).News articles (obviously) cover newsworthy topics such as bombings, explosions, war, and the killing of knowledge limited to these types of events. However, much of the user-generated content on social media is provided by ordinary people who tell stories about their daily lives. These stories are rich in common sense knowledge that includes, for example, the story of camping."}, {"heading": "2 A Corpus of Everyday Events", "text": "Our database is drawn from the Spinn3r Corpus of millions of blog posts (Burton et al., 2009; Gordon and Swanson, 2009; Gordon et al., 2012). We suspect that personal stories are a valuable resource for learning healthy knowledge about the relationships between everyday events, and that finergrained knowledge can be learned from topic-sorted stories (Riaz and Girju, 2010) that share a particular topic, so we construct two different types of stories: general-domain set. We created a random subset of the Spinn3r Corpus of personal blog domains: livejournal.com, wordpress.com, blogspot.com, spaces.live.com, typepad.com, travelpod.com. This set consists of 4,200 stories that were not selected for a particular topic. We produced a dataset by filtering the corpus using a bootstrapping method to create theme-specific themes."}, {"heading": "3 Learning Contingency Relation between Narrative Events", "text": "In this section, we describe our representation of events in narratives and our methods for modelling random relationships between events."}, {"heading": "3.1 Event Representation", "text": "In previous work, various representations of the event structure have been proposed, such as individual verbs and verbs with two or more arguments. Verbs are used as a central reference to an event in a narrative. However, other units associated with the verb also play a strong role in conveying the meaning of the event. In (Pichotta and Mooney, 2014) it is shown that the representation of multiple arguments is richer than the previous ones and is able to capture interactions between several events. We use a representation that, in addition to the subject and the direct object, includes the verb pichotta in the event structure and define an event as a verb with its dependency relationships as follows: Verb Lemma (subj: Subject Lemma, dobj: Direct Object Lemma, prt: Particle) Table 3 shows sample sentences describing an event from the topic of camping, together with its event structure. The examples show how arguments frequently change the meaning of an event. In row 1 the direct object and particle."}, {"heading": "3.2 Causal Potential Method", "text": "We define a contingent pair of events as a sequence of two events (e1, e2), so that e1 and e2 likely occur together in the given order, and e2 depends on e1. We apply an unattended distribution measure called causal potential to induce the contingency relationship between two events. Causal potential (CP) was introduced by Beamer and Girju (2009) to measure the tendency of an event pair, to encode a causal relationship in which event pairs with high CP are more likely to occur in a causal context. We calculate CP for each pair of adjacent events in each topic-specific dataset. We used a 2-skip Bigram model that considers two events to be adjacent when the second event occurs within two or fewer events. We use skip-2 Bigram to capture the fact that two related events often cannot be separated by an essential event."}, {"heading": "3.3 Baseline Methods", "text": "Our previous work on the modeling of contingency relationships in film scripts compared the causal potential with methods used in previous work: Bigram event models (Manshadi et al., 2008) and Pointwise Mutual Information (PMI) (Chambers and Jurafsky, 2008), and the evaluations showed that CP achieves better results (Hu et al., 2013). In this work, we use CP to induce contingency relationships between events, and apply three other models as a basis for comparison: Event-Unigram. This method results in a distribution of normalized frequencies for events. Event-Bigram. We calculate the Bigram probability of each pair of adjacent events using the Skip-2-Bigram model using the Maximum Likelihood Estimation (MLE) from our datasets: P (e2 | e1) = Count (e1, e2) Count (Event-SCP) (2) Proximate Event Metric on the Constimation (MLE) of this event (2) is the precursor data set (P) of the (P)."}, {"heading": "4 Evaluation Experiments", "text": "We performed three sets of experiments to evaluate different aspects of our work. First, we compare the content of our topic-specific event pairs with current state-of-the-art event collections to show that the fine-grained knowledge we learned about everyday events is not present in previous work focused on the news genre. Second, we perform an automatic evaluation test, which is performed according to the model of the COPA task (Roemmele et al., 2011) on a predetermined test set to evaluate the event pair collections that we have extracted from both general domain and topicSpecific data sets with respect to contingency relationships. We assume that the contingent event pairs can be used as basic elements to generate coherent event chains and narrative schemes. Therefore, in the third part of the experiments, we extract topical eventual pairs from our Topicfic Turn and Amazon specific data pairs to evaluate their topicality."}, {"heading": "4.1 Comparison to Rel-gram Tuple Collections", "text": "We chose Rel-gram tuples (Balasubramanian et al., 2013) for comparison because it is the most relevant previous work for us: they generate pairs of relative tuples of events, called rel-grams, with random statistics based on symmetric conditional probabilities described in Sec 3.3. In addition, the rel-grams are publicly available through an online search interface3 and their evaluations, which show that their method exceeds the previous state of the art for generating events. Their work focuses on news articles and does not take into account the causal relationship between events for generating event schemes. We compare the content of what we have learned from our topic-specific corpus to the rel-gram tuples to show that the financed type of knowledge we find in their events is not found."}, {"heading": "4.2 Automatic Two-Choice Test", "text": "This year, the number of job-related redundancies has tripled compared to the previous year, and the number of job-related redundancies has tripled compared to the previous year."}, {"heading": "4.3 Topic-Indicative Contingent Event Pairs", "text": "We identify random event pairs that are highly indicative of a particular topic. We assume that these event pairs serve as the building blocks of coherent event chains and narrative schemes because they encode contingency relationships and correspond to a specific topic. We evaluate the pairs on Amazon Mechanical Turk (AMT). To identify event sequences that have a strong correlation to a topic, we apply two filtering methods. First, we select the common pairs for each topic and remove the ontology that occurs less than 5 times in the corpus. Second, we use the indicative event patterns for each topic and extract the pairs that contain at least one of these patterns. Indicative event patterns are automatically generated during the boot strapping phase and mapped to their respective events."}, {"heading": "5 Discussion and Conclusions", "text": "We learned fine-grained common sense knowledge about contingent relationships between everyday events from personal stories written by ordinary people. We applied a semi-supervised bootstrapping approach that used event patterns to create thematically sorted stories, and evaluated our methods using a set of generic narratives as well as two topic-specific datasets. We developed a new method for learning random relationships between events that is tailored to the \"oral narrative\" of blog stories. Our evaluations suggest that a method that works well in the news genre does not produce coherent results to personal stories (comparing the event SCP baseline with the causal potential). We modeled the contingency (causal and conditional) relationship between events from each dataset using causal potential and rated questions that are automatically generated from a pre-held test set. Results show significant improvements over the event-event and the unified message relationship (the event-P)."}], "references": [{"title": "Generating coherent event schemas at scale", "author": ["Niranjan Balasubramanian", "Stephen Soderland", "Mausam", "Oren Etzioni."], "venue": "EMNLP. pages 1721\u20131731.", "citeRegEx": "Balasubramanian et al\\.,? 2013", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2013}, {"title": "Using a bigram event model to predict causal potential", "author": ["Brandon Beamer", "Roxana Girju."], "venue": "Computational Linguistics and Intelligent Text Processing, Springer, pages 430\u2013441.", "citeRegEx": "Beamer and Girju.,? 2009", "shortCiteRegEx": "Beamer and Girju.", "year": 2009}, {"title": "The ICWSM 2009 Spinn3r dataset", "author": ["Kevin Burton", "Akshay Java", "Ian Soboroff."], "venue": "Proceedings of the Third Annual Conference on Weblogs and Social Media (ICWSM 2009).", "citeRegEx": "Burton et al\\.,? 2009", "shortCiteRegEx": "Burton et al\\.", "year": 2009}, {"title": "Unsupervised learning of narrative event chains", "author": ["Nathanael Chambers", "Dan Jurafsky."], "venue": "Proceedings of ACL-08: HLT pages 789\u2013797.", "citeRegEx": "Chambers and Jurafsky.,? 2008", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["Nathanael Chambers", "Dan Jurafsky."], "venue": "Proceedings of the 47th Annual Meeting of the ACL. pages 602\u2013610.", "citeRegEx": "Chambers and Jurafsky.,? 2009", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2009}, {"title": "Verbocean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "EMNLP. volume 4, pages 33\u201340.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Minimally supervised event causality identification", "author": ["Quang Xuan Do", "Yee Seng Chan", "Dan Roth."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 294\u2013303.", "citeRegEx": "Do et al\\.,? 2011", "shortCiteRegEx": "Do et al\\.", "year": 2011}, {"title": "Automatic detection of causal relations for question answering", "author": ["Roxana Girju."], "venue": "Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering-Volume 12. Association for Computational Linguistics, pages 76\u201383.", "citeRegEx": "Girju.,? 2003", "shortCiteRegEx": "Girju.", "year": 2003}, {"title": "Identifying personal stories in millions of weblog entries", "author": ["Andrew Gordon", "Reid Swanson."], "venue": "Third International Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA.", "citeRegEx": "Gordon and Swanson.,? 2009", "shortCiteRegEx": "Gordon and Swanson.", "year": 2009}, {"title": "Different strokes of different folks: Searching for health narratives in weblogs", "author": ["Andrew S Gordon", "Christopher Wienberg", "Sara Owsley Sood."], "venue": "Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International", "citeRegEx": "Gordon et al\\.,? 2012", "shortCiteRegEx": "Gordon et al\\.", "year": 2012}, {"title": "Unsupervised induction of contingent event pairs from film scenes", "author": ["Zhichao Hu", "Elahe Rahimtoroghi", "Larissa Munishkina", "Reid Swanson", "Marilyn A Walker."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. pages 370\u2013", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Plot units and narrative summarization", "author": ["Wendy G Lehnert."], "venue": "Cognitive Science 5(4):293\u2013331.", "citeRegEx": "Lehnert.,? 1981", "shortCiteRegEx": "Lehnert.", "year": 1981}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky."], "venue": "ACL (System Demonstrations). pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Learning a probabilistic model of event sequences from internet weblog stories", "author": ["Mehdi Manshadi", "Reid Swanson", "Andrew S Gordon."], "venue": "Proceedings of the 21st FLAIRS Conference.", "citeRegEx": "Manshadi et al\\.,? 2008", "shortCiteRegEx": "Manshadi et al\\.", "year": 2008}, {"title": "Generative event schema induction with entity disambiguation", "author": ["Kiem-Hieu Nguyen", "Xavier Tannier", "Olivier Ferret", "Romaric Besan\u00e7on."], "venue": "Proceedings of the 53rd annual meeting of the Association for Computational Linguistics (ACL-15).", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Statistical script learning with multi-argument events", "author": ["Karl Pichotta", "Raymond J Mooney."], "venue": "EACL 2014 page 220.", "citeRegEx": "Pichotta and Mooney.,? 2014", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2014}, {"title": "The penn discourse treebank 2.0", "author": ["R. Prasad", "N. Dinesh", "A. Lee", "E. Miltsakaki", "L. Robaldo", "A. Joshi", "B. Webber"], "venue": "In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Minimal narrative annotation schemes and their applications", "author": ["Elahe Rahimtoroghi", "Thomas Corcoran", "Reid Swanson", "Marilyn A. Walker", "Kenji Sagae", "Andrew S. Gordon."], "venue": "7th Workshop on Intelligent Narrative Technologies. Milwaukee, WI.", "citeRegEx": "Rahimtoroghi et al\\.,? 2014", "shortCiteRegEx": "Rahimtoroghi et al\\.", "year": 2014}, {"title": "Another look at causality: Discovering scenario-specific contingency relationships with no supervision", "author": ["Mehwish Riaz", "Roxana Girju."], "venue": "Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on. IEEE, pages 361\u2013368.", "citeRegEx": "Riaz and Girju.,? 2010", "shortCiteRegEx": "Riaz and Girju.", "year": 2010}, {"title": "Automatically generating extraction patterns from untagged text", "author": ["Ellen Riloff."], "venue": "Proceedings of the national conference on artificial intelligence. pages 1044\u2013 1049.", "citeRegEx": "Riloff.,? 1996", "shortCiteRegEx": "Riloff.", "year": 1996}, {"title": "Learning textual graph patterns to detect causal event relations", "author": ["Bryan Rink", "Cosmin Adrian Bejan", "Sanda M Harabagiu."], "venue": "FLAIRS Conference.", "citeRegEx": "Rink et al\\.,? 2010", "shortCiteRegEx": "Rink et al\\.", "year": 2010}, {"title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning", "author": ["Melissa Roemmele", "Cosmin Adrian Bejan", "Andrew S Gordon"], "venue": null, "citeRegEx": "Roemmele et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Roemmele et al\\.", "year": 2011}, {"title": "Scripts Plans Goals", "author": ["R Schank", "Robert Abelson", "Roger C Schank."], "venue": "Lea.", "citeRegEx": "Schank et al\\.,? 1977", "shortCiteRegEx": "Schank et al\\.", "year": 1977}, {"title": "Identifying narrative clause types in personal stories", "author": ["Reid Swanson", "Elahe Rahimtoroghi", "Thomas Corcoran", "Marilyn A Walker."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue.", "citeRegEx": "Swanson et al\\.,? 2014", "shortCiteRegEx": "Swanson et al\\.", "year": 2014}, {"title": "Identifying subjective characters in narrative", "author": ["Janyce M Wiebe."], "venue": "Proceedings of the 13th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, pages 401\u2013406.", "citeRegEx": "Wiebe.,? 1990", "shortCiteRegEx": "Wiebe.", "year": 1990}], "referenceMentions": [{"referenceID": 22, "context": "The original idea behind scripts as introduced by Schank was to capture knowledge about the finegrained events of everyday experience, such as opening a fridge enabling preparing food, or the event of getting out of bed being triggered by an alarm going off (Schank et al., 1977).", "startOffset": 258, "endOffset": 279}, {"referenceID": 4, "context": "simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013).", "startOffset": 78, "endOffset": 137}, {"referenceID": 0, "context": "simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013).", "startOffset": 78, "endOffset": 137}, {"referenceID": 8, "context": "An advantage is that they tend to be told in chronological order (Gordon and Swanson, 2009), and temporal order between events is a strong cue to contingency (Prasad et al.", "startOffset": 65, "endOffset": 91}, {"referenceID": 16, "context": "An advantage is that they tend to be told in chronological order (Gordon and Swanson, 2009), and temporal order between events is a strong cue to contingency (Prasad et al., 2008; Beamer and Girju, 2009).", "startOffset": 158, "endOffset": 203}, {"referenceID": 1, "context": "An advantage is that they tend to be told in chronological order (Gordon and Swanson, 2009), and temporal order between events is a strong cue to contingency (Prasad et al., 2008; Beamer and Girju, 2009).", "startOffset": 158, "endOffset": 203}, {"referenceID": 17, "context": "However, their structure is more similar to oral narrative than to newswire (Rahimtoroghi et al., 2014; Swanson et al., 2014).", "startOffset": 76, "endOffset": 125}, {"referenceID": 23, "context": "However, their structure is more similar to oral narrative than to newswire (Rahimtoroghi et al., 2014; Swanson et al., 2014).", "startOffset": 76, "endOffset": 125}, {"referenceID": 13, "context": "Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014).", "startOffset": 54, "endOffset": 190}, {"referenceID": 14, "context": "Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014).", "startOffset": 54, "endOffset": 190}, {"referenceID": 0, "context": "Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014).", "startOffset": 54, "endOffset": 190}, {"referenceID": 15, "context": "Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014).", "startOffset": 54, "endOffset": 190}, {"referenceID": 11, "context": "Historically, work on scripts explicitly modeled causality (Lehnert, 1981) inter alia.", "startOffset": 59, "endOffset": 74}, {"referenceID": 10, "context": "Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004).", "startOffset": 226, "endOffset": 342}, {"referenceID": 6, "context": "Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004).", "startOffset": 226, "endOffset": 342}, {"referenceID": 7, "context": "Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004).", "startOffset": 226, "endOffset": 342}, {"referenceID": 18, "context": "Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004).", "startOffset": 226, "endOffset": 342}, {"referenceID": 20, "context": "Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004).", "startOffset": 226, "endOffset": 342}, {"referenceID": 5, "context": "Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004).", "startOffset": 226, "endOffset": 342}, {"referenceID": 1, "context": "We apply Causal Potential (Beamer and Girju, 2009) to model the contingency relation be-", "startOffset": 26, "endOffset": 50}, {"referenceID": 2, "context": "Our dataset is drawn from the Spinn3r corpus of millions of blog posts (Burton et al., 2009; Gordon and Swanson, 2009; Gordon et al., 2012).", "startOffset": 71, "endOffset": 139}, {"referenceID": 8, "context": "Our dataset is drawn from the Spinn3r corpus of millions of blog posts (Burton et al., 2009; Gordon and Swanson, 2009; Gordon et al., 2012).", "startOffset": 71, "endOffset": 139}, {"referenceID": 9, "context": "Our dataset is drawn from the Spinn3r corpus of millions of blog posts (Burton et al., 2009; Gordon and Swanson, 2009; Gordon et al., 2012).", "startOffset": 71, "endOffset": 139}, {"referenceID": 18, "context": "We hypothesize that personal stories are a valuable resource to learn common-sense knowledge about relations between everyday events and that finergrained knowledge can be learned from topic-sorted stories (Riaz and Girju, 2010) that share a particular theme, so we construct two different sets of stories: General-Domain Set.", "startOffset": 206, "endOffset": 228}, {"referenceID": 19, "context": "We apply AutoSlog-TS, a semi-supervised algorithm that learns narrative event-patterns to bootstrap a collection of stories on the same theme (Riloff, 1996).", "startOffset": 142, "endOffset": 156}, {"referenceID": 15, "context": "In (Pichotta and Mooney, 2014) it is shown", "startOffset": 3, "endOffset": 30}, {"referenceID": 12, "context": "We parse each sentence and extract every verb lemma with its arguments using Stanford dependencies (Manning et al., 2014).", "startOffset": 99, "endOffset": 121}, {"referenceID": 17, "context": "We use skip-2 bigram in order to capture the fact that two related events may often be separated by a non-essential event, because of the oral-narrative nature of our data (Rahimtoroghi et al., 2014).", "startOffset": 172, "endOffset": 199}, {"referenceID": 24, "context": "We filter out clauses that tend to be associated with private states (Wiebe, 1990).", "startOffset": 69, "endOffset": 82}, {"referenceID": 1, "context": "Causal Potential (CP) was introduced by Beamer and Girju (2009) as a way to measure the tendency of an event pair to encode a causal relation, where event pairs with high CP have a higher probability of occurring in a causal context.", "startOffset": 40, "endOffset": 64}, {"referenceID": 13, "context": "Our previous work on modeling contingency relations in film scripts data compared Causal Potential to methods used in previous work: Bigram event models (Manshadi et al., 2008) and Pointwise Mutual Information (PMI) (Chambers and Jurafsky, 2008) and the evaluations showed that CP obtains better results (Hu et al.", "startOffset": 153, "endOffset": 176}, {"referenceID": 3, "context": ", 2008) and Pointwise Mutual Information (PMI) (Chambers and Jurafsky, 2008) and the evaluations showed that CP obtains better results (Hu et al.", "startOffset": 47, "endOffset": 76}, {"referenceID": 10, "context": ", 2008) and Pointwise Mutual Information (PMI) (Chambers and Jurafsky, 2008) and the evaluations showed that CP obtains better results (Hu et al., 2013).", "startOffset": 135, "endOffset": 152}, {"referenceID": 0, "context": "We use the Symmetric Conditional Probability between event tuples (Rel-grams) used in (Balasubramanian et al., 2013) as another baseline method.", "startOffset": 86, "endOffset": 116}, {"referenceID": 21, "context": "Second, we run an automatic evaluation test, modeled after the COPA task (Roemmele et al., 2011), on a held-out test set to evaluate the event pair collections that we have extracted from both General-Domain and TopicSpecific datasets, in terms of contingency relations.", "startOffset": 73, "endOffset": 96}, {"referenceID": 0, "context": "We chose Rel-gram tuples (Balasubramanian et al., 2013) for comparison since it is the most relevant previous work to us: they generate pairs of relational tuples of events, called Rel-grams using cooccurrence statistics based on Symmetric Conditional Probability described in Sec 3.", "startOffset": 25, "endOffset": 55}, {"referenceID": 21, "context": "For evaluating our contingent event pair collections we have automatically generated a set of two-choice questions along with the answers, modeled after the COPA task (Roemmele et al., 2011).", "startOffset": 167, "endOffset": 190}], "year": 2017, "abstractText": "Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topicspecific datasets enables learning finergrained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent.", "creator": "LaTeX with hyperref package"}}}