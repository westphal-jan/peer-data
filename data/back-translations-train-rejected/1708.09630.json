{"id": "1708.09630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Optimal Distributed Control of Multi-agent Systems in Contested Environments via Reinforcement Learning", "abstract": "This paper presents a model-free reinforcement learning (RL) based distributed control protocol for leader-follower multi-agent systems. Although RL has been successfully used to learn optimal control protocols for multi-agent systems, the effects of adversarial inputs are ignored. It is shown in this paper, however, that their adverse effects can propagate across the network and impact the learning outcome of other intact agents. To alleviate this problem, a unified RL-based distributed control frameworks is developed for both homogeneous and heterogeneous multi-agent systems to prevent corrupted sensory data from propagating across the network. To this end, only the leader communicates its actual sensory information and other agents estimate the leader state using a distributed observer and communicate this estimation to their neighbors to achieve consensus on the leader state. The observer cannot be physically affected by any adversarial input. To further improve resiliency, distributed H-infinity control protocols are designed to attenuate the effect of the adversarial inputs on the compromised agent itself. An off-policy RL algorithm is developed to learn the solutions of the game algebraic Riccati equations arising from solving the H-infinity control problem. No knowledge of the agent dynamics is required and it is shown that the proposed RL-based H-infinity control protocol is resilient against adversarial inputs.", "histories": [["v1", "Thu, 31 Aug 2017 09:21:08 GMT  (5160kb,D)", "http://arxiv.org/abs/1708.09630v1", null], ["v2", "Sat, 30 Sep 2017 13:52:23 GMT  (4789kb,D)", "http://arxiv.org/abs/1708.09630v2", null]], "reviews": [], "SUBJECTS": "cs.MA cs.LG cs.SY", "authors": ["rohollah moghadam", "hamidreza modares"], "accepted": false, "id": "1708.09630"}, "pdf": {"name": "1708.09630.pdf", "metadata": {"source": "CRF", "title": "Optimal Distributed Control of Multi-agent Systems in Contested Environments via Reinforcement Learning", "authors": ["Rohollah Moghadam"], "emails": ["moghadamr@mst.edu,", "modaresh@mst.edu)."], "sections": [{"heading": null, "text": "It is not just the way in which the control of airplanes and satellites, and more [1]. However, due to their networked nature, adversarial interventions such as interference or attacks on sensors and actuators can significantly impair their performance. In a contested environment with adversarial inputs, corrupt data communicated by a single compromised agent and used by neighbors for learning, the entire network can be misdirected to a misunderstanding of the environment, and consequently will not cause emerging behavior or misconduct. Design of optimal control protocols that have an ability to learn the uncertainties online has attracted considerable attention."}], "references": [{"title": "Consensus and cooperation in networked multi-agent systems", "author": ["R. Olfati-Saber", "J.A. Fax", "R.M. Murray"], "venue": "Proceedings of the IEEE, vol. 95, pp. 215\u2013233, Jan 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Coordination of groups of mobile autonomous agents using nearest neighbor rules", "author": ["A. Jadbabaie", "J. Lin", "A.S. Morse"], "venue": "IEEE Transactions on Automatic Control, vol. 48, pp. 988\u20131001, June 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Information flow and cooperative control of vehicle formations", "author": ["J. Fax", "R. Murray"], "venue": "IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1465\u20131476, 2004. cited By 2455.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Information consensus in multivehicle cooperative control", "author": ["W. Ren", "R.W. Beard", "E.M. Atkins"], "venue": "IEEE Control Systems, vol. 27, pp. 71\u201382, April 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Approximate Dynamic Programming: Solving the curses of dimensionality, vol. 703", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A Menu of Designs for Reinforcement Learning Over Time, pp. 67\u201395", "author": ["W.T. Miller", "R.S. Sutton", "P.J. Werbos"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers", "author": ["F.L. Lewis", "D. Vrabie", "K.G. Vamvoudakis"], "venue": "IEEE Control Systems, vol. 32, no. 6, pp. 76\u2013105, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive dynamic programming: An introduction", "author": ["F.Y. Wang", "H. Zhang", "D. Liu"], "venue": "IEEE Computational Intelligence Magazine, vol. 4, pp. 39\u201347, May 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning in continuous time and space", "author": ["K. Doya"], "venue": "Neural computation, vol. 12, no. 1, pp. 219\u2013245, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems", "author": ["D. Liu", "Q. Wei"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 621\u2013634, March 2014.  14", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Decentralized stabilization for a class of continuous-time nonlinear interconnected systems using online learning optimal control approach", "author": ["D. Liu", "D. Wang", "H. Li"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 418\u2013428, Feb 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Iterative adaptive dynamic programming for solving unknown nonlinear zero-sum game based on online data", "author": ["Y. Zhu", "D. Zhao", "X. Li"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 28, pp. 714\u2013725, March 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Reinforcement q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics", "author": ["B. Kiumarsi", "F.L. Lewis", "H. Modares", "A. Karimpour", "M.-b. Naghibi- Sistani"], "venue": "Automatica, vol. 50, no. 4, pp. 1167\u20131175, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive learning in tracking control based on the dual critic network design", "author": ["Z. Ni", "H. He", "J. Wen"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, pp. 913\u2013928, June 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Using reinforcement learning techniques to solve continuous-time non-linear optimal tracking problem without system dynamics", "author": ["Y. Zhu", "D. Zhao", "X. Li"], "venue": "IET Control Theory Applications, vol. 10, no. 12, pp. 1339\u20131347, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Actor-critic-based optimal tracking for partially unknown nonlinear discrete-time systems", "author": ["B. Kiumarsi", "F.L. Lewis"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 1, pp. 140\u2013151, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Online solution of nonlinear two-player zero-sum games using synchronous policy iteration", "author": ["K.G. Vamvoudakis", "F.L. Lewis"], "venue": "International Journal of Robust and Nonlinear Control, vol. 22, no. 13, pp. 1460\u20131483, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-agent differential graphical games: Nash online adaptive learning solutions", "author": ["M.I. Abouheaf", "F.L. Lewis"], "venue": "52nd IEEE Conference on Decision and Control, pp. 5803\u20135809, Dec 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["L. Buoniu", "R.B. hatska", "B.D. Schutter"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, pp. 156\u2013172, March 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "H\u221e tracking control of completely unknown continuous-time systems via off-policy reinforcement learning", "author": ["H. Modares", "F.L. Lewis", "Z.P. Jiang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, pp. 2550\u20132562, Oct 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Off-policy reinforcement learning for h\u221e control design", "author": ["B. Luo", "H.N. Wu", "T. Huang"], "venue": "IEEE Transactions on Cybernetics, vol. 45, pp. 65\u2013 76, Jan 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "An iterative adaptive dynamic programming method for solving a class of nonlinear zero-sum differential games", "author": ["H. Zhang", "Q. Wei", "D. Liu"], "venue": "Automatica, vol. 47, no. 1, pp. 207\u2013214, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Online solution of nonquadratic two-player zero-sum games arising in the h control of constrained input systems", "author": ["H. Modares", "F.L. Lewis", "M.-B.N. Sistani"], "venue": "International Journal of Adaptive Control and Signal Processing, vol. 28, no. 3-5, pp. 232\u2013254, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural network based online simultaneous policy update algorithm for solving the hji equation in nonlinear control", "author": ["H.N. Wu", "B. Luo"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, pp. 1884\u20131895, Dec 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1884}, {"title": "Adaptive dynamic programming for online solution of a zero-sum differential game", "author": ["D. Vrabie", "F. Lewis"], "venue": "Journal of Control Theory and Applications, vol. 9, no. 3, pp. 353\u2013360, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Integral reinforcement learning for linear continuous-time zero-sum games with completely unknown dynamics", "author": ["H. Li", "D. Liu", "D. Wang"], "venue": "IEEE Transactions on Automation Science and Engineering, vol. 11, pp. 706\u2013714, July 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection in adversarial environments", "author": ["K.G. Vamvoudakis", "J.P. Hespanha", "B. Sinopoli", "Y. Mo"], "venue": "IEEE Transactions on Automatic Control, vol. 59, pp. 3209\u20133223, Dec 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning consensus in adversarial environments", "author": ["K.G. Vamvoudakis", "L.R.G. Carrillo", "J.P. Hespanha"], "venue": "SPIE Defense, Security, and Sensing, pp. 87410K\u201387410K, International Society for Optics and Photonics, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Safe and secure networked control systems under denial-of-service attacks", "author": ["S. Amin", "A.A. C\u00e1rdenas", "S.S. Sastry"], "venue": "International Workshop on Hybrid Systems: Computation and Control, pp. 31\u201345, Springer, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A secure control framework for resource-limited adversaries", "author": ["A. Teixeira", "I. Shames", "H. Sandberg", "K.H. Johansson"], "venue": "Automatica, vol. 51, pp. 135\u2013148, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed consensus tracking for multiagent systems under two types of attacks", "author": ["Z. Feng", "G. Hu", "G. Wen"], "venue": "International Journal of Robust and Nonlinear Control, vol. 26, no. 5, pp. 896\u2013918, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Security of interdependent and identical networked control systems", "author": ["S. Amin", "G.A. Schwartz", "S.S. Sastry"], "venue": "Automatica, vol. 49, no. 1, pp. 186\u2013192, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Consensus computation in unreliable networks: A system theoretic approach", "author": ["F. Pasqualetti", "A. Bicchi", "F. Bullo"], "venue": "IEEE Transactions on Automatic Control, vol. 57, pp. 90\u2013104, Jan 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Attack detection and identification in cyber-physical systems", "author": ["F. Pasqualetti", "F. Drfler", "F. Bullo"], "venue": "IEEE Transactions on Automatic Control, vol. 58, pp. 2715\u20132729, Nov 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed function calculation via linear iterative strategies in the presence of malicious agents", "author": ["S. Sundaram", "C.N. Hadjicostis"], "venue": "IEEE Transactions on Automatic Control, vol. 56, no. 7, pp. 1495\u20131508, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting integrity attacks on scada systems", "author": ["Y. Mo", "R. Chabukswar", "B. Sinopoli"], "venue": "IEEE Transactions on Control Systems Technology, vol. 22, pp. 1396\u20131407, July 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Cyber-physical attacks and defences in the smart grid: a survey", "author": ["H. He", "J. Yan"], "venue": "IET Cyber-Physical Systems: Theory & Applications, vol. 1, no. 1, pp. 13\u201327, 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Secure consensus control for multiagent systems with attacks and communication delays", "author": ["Y. Wu", "X. He"], "venue": "IEEE/CAA Journal of Automatica Sinica, vol. 4, pp. 136\u2013142, Jan 2017.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed l2-gain output-feedback control of homogeneous and heterogeneous systems", "author": ["Q. Jiao", "H. Modares", "F.L. Lewis", "S. Xu", "L. Xie"], "venue": "Automatica, vol. 71, pp. 361\u2013368, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust h-infinity group consensus for interacting clusters of integrator agents", "author": ["J. Qin", "Q. Ma", "W.X. Zheng", "H. Gao", "Y. Kang"], "venue": "IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1\u20131, 2017.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed robust fixedtime consensus for nonlinear and disturbed multiagent systems", "author": ["H. Hong", "W. Yu", "G. Wen", "X. Yu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. PP, no. 99, pp. 1\u201310, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Consensus achievement of multi-agent systems with directed and switching topology networks", "author": ["I. Saboori", "K. Khorasani"], "venue": "IEEE Transactions on Automatic Control, vol. 59, pp. 3104\u20133109, Nov 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed h and h 2 consensus control in directed networks", "author": ["J. Wang", "Z. Duan", "Z. Li", "G. Wen"], "venue": "IET Control Theory & Applications, vol. 8, no. 3, pp. 193\u2013201, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "On h\u221e and h2 performance regions of multi-agent systems", "author": ["Z. Li", "Z. Duan", "G. Chen"], "venue": "Automatica, vol. 47, no. 4, pp. 797\u2013803, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed robust h consensus control in directed networks of agents with time-delay", "author": ["P. Lin", "Y. Jia", "L. Li"], "venue": "Systems & Control Letters, vol. 57, no. 8, pp. 643\u2013653, 2008.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Disturbance rejection of multi-agent systems: A reinforcement learning differential game approach", "author": ["Q. Jiao", "H. Modares", "S. Xu", "F.L. Lewis", "K.G. Vamvoudakis"], "venue": "2015 American Control Conference (ACC), pp. 737\u2013742, July 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal linear-consensus algorithms: An lqr perspective", "author": ["Y. Cao", "W. Ren"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, pp. 819\u2013830, June 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "A Survey of Modern Algebra", "author": ["G. Birkhoff", "S. Lane"], "venue": "AKP classics, Taylor & Francis,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1977}, {"title": "Handbook of mathematical functions: with formulas, graphs, and mathematical tables, vol. 55", "author": ["M. Abramowitz", "I.A. Stegun"], "venue": "Courier Corporation,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1964}, {"title": "Decentralized synchronization of uncertain nonlinear systems with a reputation algorithm", "author": ["J.R. Klotz", "A. Parikh", "T.H. Cheng", "W.E. Dixon"], "venue": "IEEE Transactions on Control of Network Systems, vol. PP, no. 99, pp. 1\u20131, 2016.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Resilient asymptotic consensus in robust networks", "author": ["H.J. LeBlanc", "H. Zhang", "X. Koutsoukos", "S. Sundaram"], "venue": "IEEE Journal on Selected Areas in Communications, vol. 31, pp. 766\u2013781, April 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed secure average consensus for linear multi-agent systems under dos attacks", "author": ["Z. Feng", "G. Hu"], "venue": "2017 American Control Conference (ACC), pp. 2261\u20132266, May 2017.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2017}, {"title": "Resilient synchronization in robust networked multi-agent systems", "author": ["H.J. LeBlanc", "X.D. Koutsoukos"], "venue": "Proceedings of the 16th International Conference on Hybrid Systems: Computation and Control, HSCC \u201913, (New York, NY, USA), pp. 21\u201330, ACM, 2013.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal design for synchronization of cooperative systems: State feedback, observer and output feedback", "author": ["H. Zhang", "F.L. Lewis", "A. Das"], "venue": "IEEE Transactions on Automatic Control, vol. 56, pp. 1948\u20131952, Aug 2011.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1948}, {"title": "Cooperative control of multi-agent systems: optimal and adaptive design approaches", "author": ["F.L. Lewis", "H. Zhang", "K. Hengster-Movric", "A. Das"], "venue": "Springer Science & Business Media,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Resilient distributed control in the presence of misbehaving agents in networked control systems", "author": ["W. Zeng", "M.Y. Chow"], "venue": "IEEE Transactions on Cybernetics, vol. 44, pp. 2038\u20132049, Nov 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonlinear control systems", "author": ["A. Isidori"], "venue": "Springer Science & Business Media,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2013}, {"title": "Optimal model-free output synchronization of heterogeneous systems using off-policy reinforcement learning", "author": ["H. Modares", "S.P. Nageshrao", "G.A.D. Lopes", "R. Babu\u0161ka", "F.L. Lewis"], "venue": "Automatica, vol. 71, pp. 334\u2013341, 2016.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Stable adaptive neuro-control design via lyapunov function derivative estimation", "author": ["G.A. Rovithakis"], "venue": "Automatica, vol. 37, no. 8, pp. 1213\u2013 1221, 2001.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2001}, {"title": "A distributed control approach to a robust output regulation problem for multi-agent linear systems", "author": ["X. Wang", "Y. Hong", "J. Huang", "Z.P. Jiang"], "venue": "IEEE Transactions on Automatic Control, vol. 55, pp. 2891\u2013 2895, Dec 2010.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Event-triggered projected luenberger observer for linear systems under sparse sensor attacks", "author": ["Y. Shoukry", "P. Tabuada"], "venue": "53rd IEEE Conference on Decision and Control, pp. 3548\u20133553, Dec 2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Secure and robust state estimation under sensor attacks, measurement noises, and process disturbances: Observerbased combinatorial approach", "author": ["C. Lee", "H. Shim", "Y. Eun"], "venue": "2015 European Control Conference (ECC), pp. 1872\u20131877, July 2015.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2015}, {"title": "Secure state estimation for cyber physical systems under sensor attacks: A satisfiability modulo theory approach", "author": ["Y. Shoukry", "P. Nuzzo", "A. Puggelli", "A.L. Sangiovanni-Vincentelli", "S.A. Seshia", "P. Tabuada"], "venue": "IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1\u20131, 2017.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2017}, {"title": "Event-triggered state observers for sparse sensor noise/attacks", "author": ["Y. Shoukry", "P. Tabuada"], "venue": "IEEE Transactions on Automatic Control, vol. 61, pp. 2079\u20132091, Aug 2016.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to event-triggered and self-triggered control", "author": ["W.P.M.H. Heemels", "K.H. Johansson", "P. Tabuada"], "venue": "2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp. 3270\u20133285, Dec 2012.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Distributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]\u2013[4].", "startOffset": 295, "endOffset": 298}, {"referenceID": 3, "context": "Introduction Distributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]\u2013[4].", "startOffset": 299, "endOffset": 302}, {"referenceID": 8, "context": "Reinforcement learning (RL) [5]\u2013[10], inspired by learning", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 15, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 248, "endOffset": 252}, {"referenceID": 18, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 253, "endOffset": 257}, {"referenceID": 19, "context": "RL-based H\u221e control is considered to attenuate the effect of disturbances in [21]\u2013[27], and to mitigate attacks in [28] for single-agent systems.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "RL-based H\u221e control is considered to attenuate the effect of disturbances in [21]\u2013[27], and to mitigate attacks in [28] for single-agent systems.", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "RL-based H\u221e control is considered to attenuate the effect of disturbances in [21]\u2013[27], and to mitigate attacks in [28] for single-agent systems.", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "Attacks on multi-agent systems have been investigated by several researchers [29]\u2013[39].", "startOffset": 77, "endOffset": 81}, {"referenceID": 37, "context": "Attacks on multi-agent systems have been investigated by several researchers [29]\u2013[39].", "startOffset": 82, "endOffset": 86}, {"referenceID": 38, "context": "Besides, the H\u221e control of multi-agent systems is considered in [40]\u2013[46] to attenuate the effects of disturbances on agents.", "startOffset": 64, "endOffset": 68}, {"referenceID": 44, "context": "Besides, the H\u221e control of multi-agent systems is considered in [40]\u2013[46] to attenuate the effects of disturbances on agents.", "startOffset": 69, "endOffset": 73}, {"referenceID": 45, "context": "with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.", "startOffset": 39, "endOffset": 43}, {"referenceID": 46, "context": "with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.", "startOffset": 45, "endOffset": 49}, {"referenceID": 47, "context": "1) Cayley-Hamilton Theorem [49]: The matrix exponential eAt with A \u2208 Rn\u00d7n can be written by", "startOffset": 27, "endOffset": 31}, {"referenceID": 48, "context": "2) Binomial Theorem [50]: For a positive integer n, one has", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "This is in contrast to existing attack mitigation methods [34], [51]\u2013[53] in which agents discard their neighbor information based on the discrepancy between their values.", "startOffset": 58, "endOffset": 62}, {"referenceID": 49, "context": "This is in contrast to existing attack mitigation methods [34], [51]\u2013[53] in which agents discard their neighbor information based on the discrepancy between their values.", "startOffset": 64, "endOffset": 68}, {"referenceID": 51, "context": "This is in contrast to existing attack mitigation methods [34], [51]\u2013[53] in which agents discard their neighbor information based on the discrepancy between their values.", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": "This is a commonplace assumption in the literature [32], [54].", "startOffset": 51, "endOffset": 55}, {"referenceID": 52, "context": "This is a commonplace assumption in the literature [32], [54].", "startOffset": 57, "endOffset": 61}, {"referenceID": 53, "context": "Define the local neighborhood state tracking error ei \u2208 Rn for agent i as [55]", "startOffset": 74, "endOffset": 78}, {"referenceID": 53, "context": "[55] shows how c and K can be designed by solving an Algebraic Riccati Equation (ARE) to assure synchronization of all agents to the leader.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "It was shown in [55] that in the absence of the adversarial input \u03c9i(t), if the controller ui(t) in (7) is designed to make the local neighborhood tracking error (6) zero, it guarantees that (8) is satisfied and, therefore, the synchronization problem is solved.", "startOffset": 16, "endOffset": 20}, {"referenceID": 54, "context": "[56] Let \u03a3 be a diagonal matrix with at least one nonzero positive element, and L be the Laplacian matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Existing H\u221e disturbance attenuation techniques for multi-agent systems [40] aim to minimize the effect of the disturbance on the local neighborhood tracking error.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.", "startOffset": 93, "endOffset": 97}, {"referenceID": 52, "context": "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.", "startOffset": 99, "endOffset": 103}, {"referenceID": 55, "context": "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.", "startOffset": 105, "endOffset": 109}, {"referenceID": 54, "context": "[56] Consider the dynamic observer defined in (34a).", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "As stated in Lemma 2, \u03c3 \u2192 0 and therefore, based on converse Lyapunov theorem [58], there exists a smooth positive definite function V(\u03c3) such that", "startOffset": 78, "endOffset": 82}, {"referenceID": 56, "context": "Based on LaSalles invariance principle [58], as t \u2192 \u221e all trajectories of (37) and (43) converge to the largest invariant subset of points where V\u0307(\u03c3, \u00ea) = V\u0307\u03c3(\u03c3) = 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "It is shown in [21] that if (55) is satisfied, then A + BKxi is Hurwitz.", "startOffset": 15, "endOffset": 19}, {"referenceID": 57, "context": "The rest of the proof is similar to [60], and therefore, is omitted.", "startOffset": 36, "endOffset": 40}, {"referenceID": 46, "context": "Moreover, designing the control protocol (7) in an optimal manner by minimizing the performance function [48] as", "startOffset": 105, "endOffset": 109}, {"referenceID": 58, "context": "Similar to [61], one can show that L\u0307i 6 0 which indicates that the tracking error (73) converges to zero, if we design the following adaptation laws to update the neural network weights \uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 \u1e86ai = \u2212kaiWai + kiS a(ei, xi) \u1e86bi = \u2212kbiWbi + kiS b(ei)ui \u1e86ci = \u2212kciWci \u2212 kiS c(ei)\u1e59i (80)", "startOffset": 11, "endOffset": 15}, {"referenceID": 58, "context": "It is shown in [61] that the control law (81) with updating laws (80) guarantee the uniform ultimate boundedness of the error ei.", "startOffset": 15, "endOffset": 19}, {"referenceID": 58, "context": "Moreover, [61] suggested a resetting method to keep away the value of \u2223\u2223\u2223WT bi S b(ei)\u2223\u2223\u2223 from zero.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "Model-free off-policy RL for solving optimal output regulation In order to find the optimal gain (54) without the requirement of the knowledge of the system dynamics, offpolicy RL algorithm [21] is used in this subsection.", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "\u1e8bi = [ 0 \u22124 1 0 ] xi + [ 1 0 ] ui + [ 1 0 ] \u03c9i,", "startOffset": 23, "endOffset": 30}, {"referenceID": 0, "context": "\u1e8bi = [ 0 \u22124 1 0 ] xi + [ 1 0 ] ui + [ 1 0 ] \u03c9i,", "startOffset": 36, "endOffset": 43}, {"referenceID": 0, "context": "yi = [1 0] xi , i = 1, .", "startOffset": 5, "endOffset": 10}, {"referenceID": 0, "context": "\u1e8b0 = [ 0 \u22124 1 0 ] x0, y0 = [1 0] x0 (86) Algorithm 1.", "startOffset": 27, "endOffset": 32}, {"referenceID": 0, "context": "C2,4 = [ 1 0 0 ] (90)", "startOffset": 7, "endOffset": 16}, {"referenceID": 0, "context": "S = [ 0 1 \u22121 0 ] , F = [ 1 0 ] (91)", "startOffset": 23, "endOffset": 30}, {"referenceID": 59, "context": "The standard control protocol proposed in [62] is used to solve the synchronization problem for heterogeneous multiagent systems as \uf8f1\uf8f4\uf8f2\uf8f4\uf8f3 ui = Kizi \u017ci = \u1e201izi + \u1e202ieiv (92)", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "5 0 ] , G2 = [ 0 1 ] (94)", "startOffset": 13, "endOffset": 20}, {"referenceID": 60, "context": "The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]\u2013[65].", "startOffset": 181, "endOffset": 185}, {"referenceID": 62, "context": "The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]\u2013[65].", "startOffset": 186, "endOffset": 190}, {"referenceID": 63, "context": "Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.", "startOffset": 32, "endOffset": 36}, {"referenceID": 64, "context": "Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "This paper presents a model-free reinforcement learning (RL) based distributed control protocol for leaderfollower multi-agent systems. Although RL has been successfully used to learn optimal control protocols for multi-agent systems, the effects of adversarial inputs are ignored. It is shown in this paper, however, that their adverse effects can propagate across the network and impact the learning outcome of other intact agents. To alleviate this problem, a unified RL-based distributed control frameworks is developed for both homogeneous and heterogeneous multi-agent systems to prevent corrupted sensory data from propagating across the network. To this end, only the leader communicates its actual sensory information and other agents estimate the leader state using a distributed observer and communicate this estimation to their neighbors to achieve consensus on the leader state. The observer cannot be physically affected by any adversarial input. To further improve resiliency, distributed H\u221e control protocols are designed to attenuate the effect of the adversarial inputs on the compromised agent itself. An off-policy RL algorithm is developed to learn the solutions of the game algebraic Riccati equations arising from solving the H\u221e control problem. No knowledge of the agent\u2019s dynamics is required and it is shown that the proposed RL-based H\u221e control protocol is resilient against adversarial inputs.", "creator": "LaTeX with hyperref package"}}}