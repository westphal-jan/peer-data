{"id": "1708.09666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Generating Video Descriptions with Topic Guidance", "abstract": "Generating video descriptions in natural language (a.k.a. video captioning) is a more challenging task than image captioning as the videos are intrinsically more complicated than images in two aspects. First, videos cover a broader range of topics, such as news, music, sports and so on. Second, multiple topics could coexist in the same video. In this paper, we propose a novel caption model, topic-guided model (TGM), to generate topic-oriented descriptions for videos in the wild via exploiting topic information. In addition to predefined topics, i.e., category tags crawled from the web, we also mine topics in a data-driven way based on training captions by an unsupervised topic mining model. We show that data-driven topics reflect a better topic schema than the predefined topics. As for testing video topic prediction, we treat the topic mining model as teacher to train the student, the topic prediction model, by utilizing the full multi-modalities in the video especially the speech modality. We propose a series of caption models to exploit topic guidance, including implicitly using the topics as input features to generate words related to the topic and explicitly modifying the weights in the decoder with topics to function as an ensemble of topic-aware language decoders. Our comprehensive experimental results on the current largest video caption dataset MSR-VTT prove the effectiveness of our topic-guided model, which significantly surpasses the winning performance in the 2016 MSR video to language challenge.", "histories": [["v1", "Thu, 31 Aug 2017 11:17:53 GMT  (5086kb,D)", "http://arxiv.org/abs/1708.09666v1", "Appeared at ICMI 2017"], ["v2", "Mon, 4 Sep 2017 11:38:38 GMT  (5086kb,D)", "http://arxiv.org/abs/1708.09666v2", "Appeared at ICMR 2017"]], "COMMENTS": "Appeared at ICMI 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["shizhe chen", "jia chen", "qin jin"], "accepted": false, "id": "1708.09666"}, "pdf": {"name": "1708.09666.pdf", "metadata": {"source": "META", "title": "Generating Video Descriptions with Topic Guidance", "authors": ["Shizhe Chen", "Jia Chen", "Qin Jin"], "emails": ["cszhe1@ruc.edu.cn", "jiac@cs.cmu.edu", "qjin@ruc.edu.cn"], "sections": [{"heading": null, "text": "Keywords Video subtitling; Data-driven topics; Multimodalities; Teacher students learning"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world in which they will be able to move."}, {"heading": "2 Related Works", "text": "In fact, most of them will be able to play by the rules that they have established in recent years, and they will be able to play by the rules that they have set for their policies."}, {"heading": "3 Prede ned vs. Data-driven Topics", "text": "Our previous study [5] showed that the use of pre-made themes such as category tags can significantly increase the performance of video subtitles. In this section, we analyze the qualities of these pre-made themes and suggest a data-driven approach to develop additional themes for the subtitle task."}, {"heading": "3.1 Prede ned Topics: Category Tags", "text": "Each video clip in the MSR VTT dataset contains a pre-made category tag, which is derived from the metadata of the video. e Category tag distribution is shown in Figure 3. e Prefabricated category tags correspond to the variety of video themes, but there are essentially three disadvantages of this: (1) Inaccurate category labels: e Prefabricated category labels contain a certain number of caption errors, as shown in the examples in Figure 4, which severely impair the performance of captions. (2) Exclusive theme allocations: e users can assign only one of the category labels. Such a hot topic presentation cannot reflect the variety of themes within the video. (3) Suboptimal theme scheme: a) Ambiguous category de nition. For example, the \"People\" category is too general to be classified. b) Overlaps between the \"Food\" and \"Cooking\" categories cover nearly similar videos."}, {"heading": "3.2 Data-driven Topics", "text": "To overcome the drawbacks of pre-made category tags, we propose a data-driven method to generate a more appropriate set of video themes. e human generated groundtruth captions provide us with rich and precise annotations of the videos, which also affect a more task-related distribution of the videos. We note that the multiple human-generated ground truth captions sometimes do not even match for the same video. Thus, Figure 1 shows a sample video and its basic truth captions describing the video from various aspects, including detailed image content, language content and general video content. Such an example shows that a video usuallyTable 1: Examples of some data-driven themes with their representative words and common occurrences with the occurring categories."}, {"heading": "3.3 Relation betweenPrede nedTopics andData-driven Topics", "text": "We examine the relationship between the prefabricated topics and the data-driven topics based on their simultaneous occurrence in videos. For each video in the training set, we assign the corresponding prefabricated category tag and data-driven topic distribution from the LDA model. To simplify the calculation of simultaneous occurrence, we assign the most likely topic to each video in the training set. Table 1 shows the simultaneous occurrence of some prefabricated categories and data-driven topics. We see that some prefabricated categories are divided into different topics. For example, the music category is mainly divided into theme 1 of dancing and theme 3 of singing. Some content-related categories are grouped into one topic, i.e. theme 13, consisting of food and cooking categories. And categories that have similar content are also grouped together. For example, news, education and science categories are grouped into one, as most of the descriptions under these categories are \"someone talks about something.\" In summary, the data-driven topics are more promising than the video content."}, {"heading": "4 Topic Guidance Model", "text": "In this section, we present our solutions to the following two problems: 1) how to automatically predict themes for testing videos with multimodal modalities; 2) how to maximize the effects of the theme information for captions generation."}, {"heading": "4.1 Multimodal Features", "text": "We extract characteristics from the image, motion, auction and language modalities to the content of videos.Image modality: E-image modality restores the static content of videos. We extract activations from the penultimate layers of the capture resource [23], which were preschooled on the ImageNet as image object characteristics, and the penultimate layers of the resource [24] pretrainedure Figure 5: e Framework for topic prediction. We treat the problem from the perspective of a teacher / student. The e-teacher theme mining model is used to guide the two student prediction models to learn on the basis of general multimodalities and language modality."}, {"heading": "4.2 Topic Prediction", "text": "For prede ned topics, i.e. category day, we train a standard of a hidden layer neural network with cross-entropy loss for predicting. e inputs of this neural network are the multimodal features as in Section 4.1.For data-driven topics, since there is no direct topic class label, we use the topic distribution generated from the topic mining LDA model in Section 3.2. We take a teacher-student learning perspective [6] to learn the data-driven topic prediction model. To the topic mining is considered as a teacher and the topic prediction model is considered as a student. e teacher, topic mining model, is educated in unguarded style, i.e. topic to guide the students, topic prediction model to learn."}, {"heading": "4.3 Caption Models with Topic Guidance", "text": "Suppose we have several pairs of video sets (V, y) in the video sequence in which we have the EWS (D, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "Dataset: e MSR-VTT corpus [15] is currently the largest video-to-speech dataset with a variety of video content. It consists of 10,000 video clips with 20 man-made captions per clip. Each video also includes a predefined category tag, which is one of the 20 popular video categories in web videos. By default, we use 6,513 videos for training, 497 videos for validation, and the remaining 2,990 for testing purposes. Data pre-processing: We convert all descriptions to lowercase letters and remove all punctuations. We add sentence start < BOS > and end of sentence tag < EOS > to our vocabulary. Words that occur more than twice are selected, resulting in a vocabulary of size 10,868. The maximum length of a generated caption is set to 30 captions."}, {"heading": "5.2 Evaluation", "text": "Table 2 presents the performance of captions on pre-built category tag tests and the data-driven themes using their respective best caption model with theme guidance. We can see that the predicted topic guidances (the second to fourth row) significantly improve the performance of the multimodal baseline (the first row). Since the data-driven themes are predicted in tests as shown in Figure 5, we also use the predicted category tags for fair comparison. Even compared to the category guidelines assigned by video uploaders, the predicted data-driven themes outperform the predicted category tags on all four rating variables, and the student's t-test shows that the improvement is significant with the p-value < 0.01 on the p-value < < 0.002. Even compared to the category tags assigned by video uploaders, the predicted data-level test increases the performance of the metric with the metric."}, {"heading": "5.3 Ablation Experiments", "text": "InteractiveCaptionwithManuallyAnnotatedTopic: Our model o ers the exibility of manually assigned themes to the video. It means that we can interactively comment on the themes for testing videos based on the relevance between video content and the representative words in themes to generate more captions. Results in Table 4and examples in Figure 7 presents the caption performance with the commented themes. Both show that with more accurate themes the caption theme can be further improved.In Uence of Multi-modalities: As an implicit assessment we can evaluate the topic prediction performance with the commented themes on the dynam. e prediction accuracies with various modalities are evaluated in Figure 8. e performance of the Aural and Speech modality are only based on videos with the appropriate modalities."}, {"heading": "6 Conclusions", "text": "Our experimental results show that the topic information is very useful to guide the caption model for more thematic description generations, and topics that are automatically degraded in a data-driven manner are superior to the topic guidance found. Multimodal features, especially the features of the language modality, are critical to predicting themes for testing videos. Our proposed topic-driven model, which acts as an interplay of theme-aware speech decoders, can use the topic information more effectively than other caption models. It signals a significant improvement in basic multimodal performance on the largest data set currently available on the MSR-VTT video channels, exceeding the winning performance of the 2016 MSR video language challenge. In future work, we will continue to improve prediction performance and jointly learn the topic until the end of the caption."}, {"heading": "7 Acknowledgments", "text": "is supported by the National Key Research and Development Plan under grant number 2016YFB1001202."}], "references": [{"title": "Phrase-based image captioning", "author": ["R\u00e9mi Lebret", "Pedro H.O. Pinheiro", "Ronan Collobert"], "venue": "In ICML, pages 2085\u20132094,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Show, a\u008aend and tell: Neural image caption generation with visual a\u008aention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Image captioning with semantic a\u008aention", "author": ["\u008banzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Describing videos using multi-modal fusion", "author": ["Qin Jin", "Jia Chen", "Shizhe Chen", "Yifan Xiong", "Alexander Hauptmann"], "venue": "In ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Do deep nets really need to be deep", "author": ["Lei Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan \u008cater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In ICCV,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Je\u0082 Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": "Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Qi Wu", "Chunhua Shen", "Lingqiao Liu", "Anthony Dick", "Anton van den Hengel"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pa\u0088ern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Semantic compositional networks for visual captioning", "author": ["Zhe Gan", "Chuang Gan", "Xiaodong He", "Yunchen Pu", "Kenneth Tran", "Jianfeng Gao", "Lawrence Carin", "Li Deng"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["Pradipto Das", "Chenliang Xu", "Richard F. Doell", "Jason J. Corso"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Msr-v\u008a: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher J. Pal", "Hugo Larochelle", "Aaron C. Courville"], "venue": "In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Je\u0082rey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Video description generation using audio and visual cues", "author": ["Qin Jin", "Junwei Liang"], "venue": "In ICMR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Multimodal video description", "author": ["Vasili Ramanishka", "Abir Das", "Dong Huk Park", "Subhashini Venugopalan", "Lisa Anne Hendricks", "Marcus Rohrbach", "Kate Saenko"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Inceptionv4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Io\u0082e", "Vincent Vanhoucke", "Alex Alemi"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Places: An image database for deep scene understanding", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Antonio Torralba", "Aude Oliva"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri"], "venue": "In ICCV,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["Steven Davis", "Paul Mermelstein"], "venue": "IEEE transactions on acoustics, speech, and signal processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1980}, {"title": "So\u0089ening quantization in bag-of-audiowords", "author": ["Stephanie Pancoast", "Murat Akbacak"], "venue": "In ICASSP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Image classi\u0080cation with the \u0080sher vector: \u008ceory and practice", "author": ["Jorge S\u00e1nchez", "Florent Perronnin", "\u008comas Mensink", "Jakob Verbeek"], "venue": "International journal of computer vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["Geo\u0082rey Hinton", "Oriol Vinyals", "Je\u0082 Dean"], "venue": "Computer Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Guiding the long-short term memory model for image caption generation", "author": ["Xu Jia", "Efstratios Gavves", "Basura Fernando", "Tinne Tuytelaars"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Unsupervised learning of image transformations", "author": ["R Memisevic", "G Hinton"], "venue": "In CVPR, pages", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Meteor universal: Language speci\u0080c translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Citeseer,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Cider: Consensusbased image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In CVPR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 1, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 2, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 3, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 4, "context": "In our previous study [5], we have utilized the prede\u0080ned topics, the category tags crawled from video meta-data during data collection, to improve the captioning performance.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "For the \u0080rst question, we take a teacher-student learning perspective [6] to train the data-driven topic prediction model.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "For the second question, we propose a novel topic-guided model (TGM) to employ the predicted topics, which is based on the encoderdecoder framework [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "[1] predict phrases with a bilinear model and generate descriptions using simple syntax statistics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] use the Conditional Random Field to learn object and activity labels from the video.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "More recently, researches have been focusing on the second direction of encoder-decoder framework [7] which generates sentences based on image/video features in an end-to-end manner.", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "[2] utilize the LSTM to generate sentences with CNN features extracted from the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] transfer knowledge from image caption models with the encoder to perform mean pooling over frame CNN features for video captioning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] explicitly embed the sentences and the videos into a common space in addition to the video description generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] directly generate image captions based on the detected semantic concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] propose to selectively a\u008aend to concept proposals in the decoder.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] propose the semantic compositional networks (SCN) which works as an ensemble of concept-dependent language decoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "2) \u008ce topics contain more additional information than semantic objects such as from the motion, aural and speech modalities; and 3) \u008ce prediction accuracy is very important for the model as shown in [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 12, "context": "Previous works have explored generating descriptions for narrow-domain videos such as YouCook [13] and TACoS [14], whose vocabularies and expressions are similar in the dataset.", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "However, for open-domain videos with various topics such as the MSR-VTT dataset [15], Jin et al.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "[5] exploit the prede\u0080ned video categories in the encoder and signi\u0080cantly improve the captioning performance, which results in their winning of the MSR video to language challenge [16].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[17] explore the temporal structure with local C3D features and global temporal a\u008aention mechanism.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] propose the sequence to sequence structure which utilizes the LSTM as encoder to capture the temporal dynamics of videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] further propose the hierarchical RNN encoder as well as the temporal-spatial a\u008aention.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5, 20] and Ramanishka et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[5, 20] and Ramanishka et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[21] integrate the visual and aural features in the encoder by early fusion and show that the multimodal fusion was bene\u0080cial to improve captioning performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Our previous study [5] has shown that using prede\u0080ned topics such as category tags can signi\u0080cantly boost video captioning performance.", "startOffset": 19, "endOffset": 22}, {"referenceID": 19, "context": "eration process of Latent Dirichlet Allocation (LDA) model [22]: 1.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "We extract activations from the penultimate layers of the inception-resnet [23] pre-trained on the ImageNet as image object features, and the penultimate layers of the resnet [24] pre-trained Figure 5: \u0087e framework for topic prediction.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "We extract activations from the penultimate layers of the inception-resnet [23] pre-trained on the ImageNet as image object features, and the penultimate layers of the resnet [24] pre-trained Figure 5: \u0087e framework for topic prediction.", "startOffset": 175, "endOffset": 179}, {"referenceID": 22, "context": "on the places365 [25] as image scene features, the dimensionality of which are 1536 and 2048 respectively.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "We extract features from the C3D model [26] pretrained on the Sports-1M dataset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [27] as the basic low-level descriptors.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "Two encoding strategies, Bag-of-AudioWords [28] and Fisher Vector [29], are used to aggregate MFCC frames into one video-level feature vector, with dimensionality of 1024 and 624 respectively.", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "Two encoding strategies, Bag-of-AudioWords [28] and Fisher Vector [29], are used to aggregate MFCC frames into one video-level feature vector, with dimensionality of 1024 and 624 respectively.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "We take a teacher-student learning perspective [6] to train the data-driven topic prediction model.", "startOffset": 47, "endOffset": 50}, {"referenceID": 27, "context": "dark knowledge [31], rather than output label from the teacher model.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "Since the captioning output is the sequential words, we utilize the LSTM [32] recurrent neural networks as our language decoder:", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "To enhance the topic guidance, we concatenate the topic distribution z with the word embeddingwt\u22121 as the input to the LSTM every step, which is similar to the gLSTM proposed in [33]:", "startOffset": 178, "endOffset": 182}, {"referenceID": 11, "context": "[12], therefore, we further propose the topic-guided model (TGM) that explicitly functions as an ensemble of topic-aware language decoders to capture di\u0082erent sentence distributions for each topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "So the ideas in [34] are used to share parameters by factorizingW (z) as follows:", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Dataset: \u008ce MSR-VTT corpus [15] is currently the largest video to language dataset with a wide variety of video contents.", "startOffset": 27, "endOffset": 31}, {"referenceID": 31, "context": "5 on the input and output of LSTM and use ADAM algorithm [35] with learning rate of 10\u22124.", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 60, "endOffset": 64}, {"referenceID": 34, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "Our proposed TGM model also achieves be\u008aer performance than the winning performance in 2016 MSR video to language challenge [5], where we use multimodal features and select best models by the prede\u0080ned categories.", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "v2t navigator [5] 0.", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Generating video descriptions in natural language (a.k.a. video captioning) is a more challenging task than image captioning as the videos are intrinsically more complicated than images in two aspects. First, videos cover a broader range of topics, such as news, music, sports and so on. Second, multiple topics could coexist in the same video. In this paper, we propose a novel caption model, topic-guided model (TGM), to generate topic-oriented descriptions for videos in the wild via exploiting topic information. In addition to prede\u0080ned topics, i.e., category tags crawled from the web, we also mine topics in a data-driven way based on training captions by an unsupervised topic mining model. We show that data-driven topics re\u0083ect a be\u008aer topic schema than the prede\u0080ned topics. As for testing video topic prediction, we treat the topic mining model as teacher to train the student, the topic prediction model, by utilizing the full multi-modalities in the video especially the speech modality. We propose a series of caption models to exploit topic guidance, including implicitly using the topics as input features to generate words related to the topic and explicitly modifying the weights in the decoder with topics to function as an ensemble of topic-aware language decoders. Our comprehensive experimental results on the current largest video caption dataset MSR-VTT prove the e\u0082ectiveness of our topic-guided model, which signi\u0080cantly surpasses the winning performance in the 2016 MSR video to language challenge.", "creator": "LaTeX with hyperref package"}}}